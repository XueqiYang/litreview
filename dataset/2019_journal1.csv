"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,"Reference Count","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Lung and Pancreatic Tumor Characterization in the Deep Learning Era: Novel Supervised and Unsupervised Learning Approaches","S. Hussein; P. Kandel; C. W. Bolan; M. B. Wallace; U. Bagci","Center for Advanced Machine Learning, Symantec Corporation, Atlanta, GA, USA; Mayo Clinic, Jacksonville, FL, USA; Mayo Clinic, Jacksonville, FL, USA; Mayo Clinic, Jacksonville, FL, USA; Center for Research in Computer Vision, University of Central Florida, Orlando, FL, USA","IEEE Transactions on Medical Imaging","","2019","38","8","1777","1787","Risk stratification (characterization) of tumors from radiology images can be more accurate and faster with computer-aided diagnosis (CAD) tools. Tumor characterization through such tools can also enable non-invasive cancer staging, prognosis, and foster personalized treatment planning as a part of precision medicine. In this paper, we propose both supervised and unsupervised machine learning strategies to improve tumor characterization. Our first approach is based on supervised learning for which we demonstrate significant gains with deep learning algorithms, particularly by utilizing a 3D convolutional neural network and transfer learning. Motivated by the radiologists' interpretations of the scans, we then show how to incorporate task-dependent feature representations into a CAD system via a graph-regularized sparse multi-task learning framework. In the second approach, we explore an unsupervised learning algorithm to address the limited availability of labeled training data, a common problem in medical imaging applications. Inspired by learning from label proportion approaches in computer vision, we propose to use proportion-support vector machine for characterizing tumors. We also seek the answer to the fundamental question about the goodness of “deep features” for unsupervised tumor classification. We evaluate our proposed supervised and unsupervised learning algorithms on two different tumor diagnosis challenges: lung and pancreas with 1018 CT and 171 MRI scans, respectively, and obtain the state-of-the-art sensitivity and specificity results in both problems.","","","10.1109/TMI.2019.2894349","University of Central Florida-Mayo Clinic Jacksonville seed grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624570","Unsupervised learning;lung cancer;3D CNN;IPMN;pancreatic cancer","Lung;Unsupervised learning;Tumors;Cancer;Three-dimensional displays;Feature extraction;Deep learning","biomedical MRI;cancer;computerised tomography;convolutional neural nets;image classification;image representation;lung;medical image processing;supervised learning;support vector machines;tumours;unsupervised learning","deep learning era;radiology images;computer-aided diagnosis tools;noninvasive cancer staging;foster personalized treatment planning;deep learning algorithms;transfer learning;task-dependent feature representations;CAD system;graph-regularized sparse multitask learning framework;medical imaging applications;label proportion approaches;proportion-support vector machine;deep features;unsupervised tumor classification;supervised learning algorithms;unsupervised learning algorithms;tumor diagnosis;pancreatic tumor characterization;lung tumor characterization;MRI scans;CT scans;3D convolutional neural network","","","49","","","","","IEEE","IEEE Journals"
"Adversarial Examples: Attacks and Defenses for Deep Learning","X. Yuan; P. He; Q. Zhu; X. Li","National Science Foundation Center for Big Learning, University of Florida, Gainesville, FL, USA; National Science Foundation Center for Big Learning, University of Florida, Gainesville, FL, USA; National Science Foundation Center for Big Learning, University of Florida, Gainesville, FL, USA; National Science Foundation Center for Big Learning, University of Florida, Gainesville, FL, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","9","2805","2824","With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks (DNNs) have been recently found vulnerable to well-designed input samples called adversarial examples. Adversarial perturbations are imperceptible to human but can easily fool DNNs in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying DNNs in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for DNNs, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples. In addition, three major challenges in adversarial examples and the potential solutions are discussed.","","","10.1109/TNNLS.2018.2886017","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611298","Adversarial examples;deep learning (DL);deep neural network (DNN);security","Task analysis;Security;Data models;Deep learning;Feature extraction;Computer architecture;Computational modeling","learning (artificial intelligence);neural nets;security of data","safety-critical environments;adversarial examples;deep learning;DNN","","17","141","","","","","IEEE","IEEE Journals"
"Transferable Representation Learning with Deep Adaptation Networks","M. Long; Y. Cao; Z. Cao; J. Wang; M. I. Jordan","School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; Department of EECS, Department of Statistics, University of California, Berkeley, Berkeley, CA, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","12","3071","3085","Domain adaptation studies learning algorithms that generalize across source domains and target domains that exhibit different distributions. Recent studies reveal that deep neural networks can learn transferable features that generalize well to similar novel tasks. However, as deep features eventually transition from general to specific along the network, feature transferability drops significantly in higher task-specific layers with increasing domain discrepancy. To formally reduce the effects of this discrepancy and enhance feature transferability in task-specific layers, we develop a novel framework for deep adaptation networks that extends deep convolutional neural networks to domain adaptation problems. The framework embeds the deep features of all task-specific layers into reproducing kernel Hilbert spaces (RKHSs) and optimally matches different domain distributions. The deep features are made more transferable by exploiting low-density separation of target-unlabeled data in very deep architectures, while the domain discrepancy is further reduced via the use of multiple kernel learning that enhances the statistical power of kernel embedding matching. The overall framework is cast in a minimax game setting. Extensive empirical evidence shows that the proposed networks yield state-of-the-art results on standard visual domain-adaptation benchmarks.","","","10.1109/TPAMI.2018.2868685","National Key R&D Program of China; National Natural Science Foundation of China; DARPA Program on Lifelong Learning Machines; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454781","Domain adaptation;deep learning;convolutional neural network;two-sample test;multiple kernel learning","Task analysis;Learning systems;Adaptation models;Convolutional neural networks;Deep learning","convolutional neural nets;feature extraction;game theory;generalisation (artificial intelligence);Hilbert spaces;learning (artificial intelligence);minimax techniques","deep features;feature transferability;domain discrepancy;deep adaptation networks;deep convolutional neural networks;domain distributions;deep architectures;multiple kernel learning;transferable representation learning;deep neural networks;transferable feature learning;visual domain-adaptation;learning algorithms;generalization;reproducing kernel Hilbert spaces;kernel embedding matching;minimax game","","8","56","","","","","IEEE","IEEE Journals"
"Deep Matrix Factorization With Implicit Feedback Embedding for Recommendation System","B. Yi; X. Shen; H. Liu; Z. Zhang; W. Zhang; S. Liu; N. Xiong","National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, China; National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, China; National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, China; National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, China; National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, China; National Engineering Research Center for E-Learning and the National Engineering Laboratory for Educational Big Data, Central China Normal University, Wuhan, China; National Engineering Laboratory for Educational Big Data, Central China Normal University, Wuhan, China","IEEE Transactions on Industrial Informatics","","2019","15","8","4591","4601","Automatic recommendation has become an increasingly relevant problem to industries, which allows users to discover new items that match their tastes and enables the system to target items to the right users. In this paper, we propose a deep learning (DL) based collaborative filtering framework, namely, deep matrix factorization (DMF), which can integrate any kind of side information effectively and handily. In DMF, two feature transforming functions are built to directly generate latent factors of users and items from various input information. As for the implicit feedback that is commonly used as input of recommendation algorithms, implicit feedback embedding (IFE) is proposed. IFE converts the high-dimensional and sparse implicit feedback information into a low-dimensional real-valued vector retaining primary features. Using IFE could reduce the scale of model parameters conspicuously and increase model training efficiency. Experimental results on five public databases indicate that the proposed method performs better than the state-of-the-art DL-based recommendation algorithms on both accuracy and training efficiency in terms of quantitative assessments.","","","10.1109/TII.2019.2893714","National Key R&D Program of China; CCNU; Cultivating Excellent Doctoral Dissertations Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8616805","Collaborative filtering (CF);deep learning (DL);matrix factorization (MF);recommendation system;representation learning","Training;Negative feedback;Deep learning;Motion pictures;Feature extraction;Big Data;Electronic learning","collaborative filtering;learning (artificial intelligence);matrix decomposition;recommender systems","feature transforming functions;implicit feedback embedding;IFE;high-dimensional feedback information;sparse implicit feedback information;low-dimensional real-valued vector;deep matrix factorization;DMF;automatic recommendation system;deep learning based collaborative filtering framework;users latent factors;DL-based recommendation algorithms","","1","46","Traditional","","","","IEEE","IEEE Journals"
"Deep Transfer Learning for Intelligent Cellular Traffic Prediction Based on Cross-Domain Big Data","C. Zhang; H. Zhang; J. Qiao; D. Yuan; M. Zhang","Shandong Provincial Key Laboratory of Wireless Communication Technologies, Shandong University, Jinan, China; Shandong Provincial Key Laboratory of Wireless Communication Technologies, Shandong University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; Shandong Provincial Key Laboratory of Wireless Communication Technologies, Shandong University, Jinan, China; Shandong Provincial Key Laboratory of Wireless Communication Technologies, Shandong University, Jinan, China","IEEE Journal on Selected Areas in Communications","","2019","37","6","1389","1401","Machine (deep) learning-enabled accurate traffic modeling and prediction is an indispensable part for future big data-driven intelligent cellular networks, since it can help autonomic network control and management as well as service provisioning. Along this line, this paper proposes a novel deep learning architecture, namely Spatial-Temporal Cross-domain neural Network (STCNet), to effectively capture the complex patterns hidden in cellular data. By adopting a convolutional long short-term memory network as its subcomponent, STCNet has a strong ability in modeling spatial-temporal dependencies. Besides, three kinds of cross-domain datasets are actively collected and modeled by STCNet to capture the external factors that affect traffic generation. As diversity and similarity coexist among cellular traffic from different city functional zones, a clustering algorithm is put forward to segment city areas into different groups, and consequently, a successive inter-cluster transfer learning strategy is designed to enhance knowledge reuse. In addition, the knowledge transferring among different kinds of cellular traffic is also explored with the proposed STCNet model. The effectiveness of STCNet is validated through real-world cellular traffic datasets using three kinds of evaluation metrics. The experimental results demonstrate that STCNet outperforms the state-of-the-art algorithms. In particular, the transfer learning based on STCNet brings about 4%~13% extra performance improvements.","","","10.1109/JSAC.2019.2904363","National Science Fund of China for Excellent Young Scholars; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667446","Cellular traffic prediction;big data;deep learning;intelligent traffic management","Predictive models;Correlation;Deep learning;Urban areas;Big Data;Wireless communication;Spatiotemporal phenomena","Big Data;cellular radio;learning (artificial intelligence);neural nets;pattern clustering;telecommunication computing;telecommunication traffic","intelligent cellular traffic prediction;autonomic network control;deep learning architecture;real-world cellular traffic datasets;convolutional long short-term memory network;knowledge transfer;inter-cluster transfer learning strategy;city functional zones;big data-driven intelligent cellular networks;spatial-temporal cross-domain neural network;STCNet","","3","43","","","","","IEEE","IEEE Journals"
"Toward Knowledge as a Service Over Networks: A Deep Learning Model Communication Paradigm","Z. Chen; L. Duan; S. Wang; Y. Lou; T. Huang; D. O. Wu; W. Gao","National Engineering Laboratory of Video Technology, the School of Electronics Engineering and Computer Science, Peking University, Beijing, China; National Engineering Laboratory of Video Technology, the School of Electronics Engineering and Computer Science, Peking University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong; National Engineering Laboratory of Video Technology, the School of Electronics Engineering and Computer Science, Peking University, Beijing, China; National Engineering Laboratory of Video Technology, the School of Electronics Engineering and Computer Science, Peking University, Beijing, China; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; National Engineering Laboratory of Video Technology, the School of Electronics Engineering and Computer Science, Peking University, Beijing, China","IEEE Journal on Selected Areas in Communications","","2019","37","6","1349","1363","The advent of artificial intelligence and Internet of Things has led to the seamless transition turning the big data into the big knowledge. The deep learning models, which assimilate knowledge from large-scale data, can be regarded as an alternative but promising modality of knowledge for artificial intelligence services. Yet, the compression, storage, and communication of the deep learning models towards better knowledge services, especially over networks, pose a set of challenging problems on both industrial and academic realms. This paper presents the deep learning model communication paradigm based on multiple model compression, which greatly exploits the redundancy among multiple deep learning models in different application scenarios. We analyze the potential and demonstrate the promise of the compression strategy for deep learning model communication through a set of experiments. Moreover, the interoperability in deep learning model communication, which is enabled based on the standardization of compact deep learning model representation, is also discussed and envisioned.","","","10.1109/JSAC.2019.2904360","National Key Research and Development Program of China; National Natural Science Foundation of China; Shenzhen Municipal Science and Technology Program; National Research Foundation Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667347","Deep learning;deep learning model communication;neural network compression;knowledge centric network","Deep learning;Knowledge engineering;Redundancy;Computational modeling;Data models;Task analysis;Neural networks","Big Data;cloud computing;Internet of Things;learning (artificial intelligence);open systems","big knowledge;artificial intelligence services;deep learning model communication paradigm;multiple model compression;multiple deep learning models;compact deep learning model representation;knowledge as a service","","1","69","","","","","IEEE","IEEE Journals"
"Person Reidentification via Structural Deep Metric Learning","X. Yang; P. Zhou; M. Wang","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Automation, Northwestern Polytechnical University, Xi’an, China.; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","10","2987","2998","Despite the promising progress made in recent years, person reidentification (re-ID) remains a challenging task due to the complex variations in human appearances from different camera views. This paper proposes to tackle this task by jointly learning feature representation and distance metric in an end-to-end manner. Existing deep metric learning-based re-ID methods usually encounter the following two weaknesses: 1) most works based on pairwise or triplet constraints often suffer from slow convergence and poor local optima, partially because they use very limited samples for each update and 2) hard negative sample mining has been widely applied in existing works. However, hard positive samples, which also contribute to the training of network, have not received enough attention. To alleviate these problems, we develop a novel structural metric learning objective for person re-ID, in which each positive pair is allowed to be compared against all negative pairs in a minibatch and each positive pair is adaptively assigned a hardness-aware weight to modulate its contribution. The introduced positive pair weighting strategy enables the algorithm to focus more on the hard positive samples. Furthermore, we propose to enhance the proposed loss function by adding a global loss term to reduce the variances of positive/negative pair distances, which is able to improve the generalization capability of the network model. By this approach, person images can be nonlinearly mapped into a low-dimensional embedding space where similar samples are kept closer and dissimilar samples are pushed farther apart. We implement the proposed algorithm using the inception architecture and evaluate it on three large-scale re-ID data sets. Experiment results demonstrate that our approach is able to outperform most state of the arts while using much lower dimensional deep features.","","","10.1109/TNNLS.2018.2861991","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445716","Computer vision;deep metric learning;deep neural network;machine learning;person re-identification (re-ID)","Measurement;Task analysis;Learning systems;Cameras;Training;Convergence;Computer vision","image representation;image sensors;learning (artificial intelligence);network theory (graphs)","person re-ID;negative pairs;hardness-aware weight;introduced positive pair weighting strategy;hard positive samples;person images;dissimilar samples;large-scale re-ID data sets;lower dimensional deep features;person reidentification;structural deep metric learning;complex variations;human appearances;feature representation;pairwise;poor local optima;structural metric learning objective;deep metric learning-based re-ID methods","","","82","","","","","IEEE","IEEE Journals"
"Active Transfer Learning Network: A Unified Deep Joint Spectral–Spatial Feature Learning Model for Hyperspectral Image Classification","C. Deng; Y. Xue; X. Liu; C. Li; D. Tao","School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; School of Electronic Engineering, Xidian University, Xi’an, China; UBTECH Sydney Artificial Intelligence Centre","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","3","1741","1754","Deep learning has recently attracted significant attention in the field of hyperspectral images (HSIs) classification. However, the construction of an efficient deep neural network mostly relies on a large number of labeled samples being available. To address this problem, this paper proposes a unified deep network, combined with active transfer learning (TL) that can be well-trained for HSIs classification using only minimally labeled training data. More specifically, deep joint spectral-spatial feature is first extracted through hierarchical stacked sparse autoencoder (SSAE) networks. Active TL is then exploited to transfer the pretrained SSAE network and the limited training samples from the source domain to the target domain, where the SSAE network is subsequently fine-tuned using the limited labeled samples selected from both source and target domains by the corresponding active learning (AL) strategies. The advantages of our proposed method are threefold: 1) the network can be effectively trained using only limited labeled samples with the help of novel AL strategies; 2) the network is flexible and scalable enough to function across various transfer situations, including cross data set and intraimage; and 3) the learned deep joint spectral-spatial feature representation is more generic and robust than many joint spectral-spatial feature representations. Extensive comparative evaluations demonstrate that our proposed method significantly outperforms many state-of-the-art approaches, including both traditional and deep network-based methods, on three popular data sets.","","","10.1109/TGRS.2018.2868851","National Natural Science Foundation of China; Key R&D Program-The Key Industry Innovation Chain of Shaanxi; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520902","Active learning (AL);deep learning;hyperspectral image (HSI) classification;multiple-feature representation;stacked sparse autoencoder (SSAE);transfer learning (TL)","Feature extraction;Training;Loss measurement;Hyperspectral imaging;Training data","feature extraction;geophysical image processing;image classification;learning (artificial intelligence);neural nets","active transfer learning network;unified deep joint spectral-spatial feature learning model;hyperspectral image classification;deep learning;hyperspectral images classification;efficient deep neural network;labeled samples;unified deep network;HSIs classification;training data;hierarchical stacked sparse autoencoder networks;active TL;pretrained SSAE network;training samples;source domain;target domain;target domains;corresponding active learning strategies;transfer situations;learned deep joint spectral-spatial feature representation;joint spectral-spatial feature representations;traditional network-based methods;deep network-based methods","","4","54","","","","","IEEE","IEEE Journals"
"Robust Hierarchical Deep Learning for Vehicular Management","Q. Wang; J. Wan; X. Li","School of Computer Science, and Center for Optical Imagery Analysis and Learning, Northwestern Polytechnical University, Xi’an, China; Video, Image, and Sound Analysis Lab, Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong; School of Computer Science, and Center for Optical Imagery Analysis and Learning, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Vehicular Technology","","2019","68","5","4148","4156","Congestion detection is an important aspect of vehicular management. However, most of the existing algorithms are insufficient for real applications. Traditional features are not discriminative which results in rather poor performance under complex scenarios. The deep features can better represent high-level information, but the training of deep network for regression is difficult. To promote the congestion detection, a robust hierarchical deep learning is proposed for the task. In this method, a deep network is designed for hierarchical semantic feature extraction. Different from traditional deep regression networks, which usually directly utilize mean squared error as loss function, a robust metric learning is employed to effectively train the network. Based on this, multiple networks are combined together to further improve the generalization ability. Extensive experiments are conducted and the proposed model is confirmed to be effective.","","","10.1109/TVT.2018.2883046","National Key R&D Program of China; National Natural Science Foundation of China; Natural Science Foundation of Shaanxi Province; Fundamental Research Funds for the Central Universities; Open Research Fund of Key Laboratory of Spectral Imaging Technology Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543594","Deep learning;traffic surveillance;regression;congestion detection;crowd counting;ensemble learning;metric learning","Feature extraction;Measurement;Videos;Task analysis;Training;Convolutional neural networks","feature extraction;generalisation (artificial intelligence);knowledge representation;learning (artificial intelligence);mean square error methods;neural nets;traffic engineering computing","vehicular management;deep features;high-level information;deep network;congestion detection;robust hierarchical deep learning;hierarchical semantic feature extraction;robust metric learning;deep regression networks;mean squared error;loss function;generalization ability","","7","52","","","","","IEEE","IEEE Journals"
"Deep Learning Ensemble for Hyperspectral Image Classification","Y. Chen; Y. Wang; Y. Gu; X. He; P. Ghamisi; X. Jia","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Higher Education Key Lab for Measure & Control Technology and Instrumentations of Heilongjiang, Harbin University of Science and Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; School of Engineering and Information Technology, The University of New South Wales, Canberra, ACT, Australia","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","6","1882","1897","Deep learning models, especially deep convolutional neural networks (CNNs), have been intensively investigated for hyperspectral image (HSI) classification due to their powerful feature extraction ability. In the same manner, ensemble-based learning systems have demonstrated high potential to effectively perform supervised classification. In order to boost the performance of deep learning-based HSI classification, the idea of deep learning ensemble framework is proposed here, which is loosely based on the integration of deep learning model and random subspace-based ensemble learning. Specifically, two deep learning ensemble-based classification methods (i.e., CNN ensemble and deep residual network ensemble) are proposed. CNNs or deep residual networks are used as individual classifiers and random subspaces contribute to diversify the ensemble system in a simple yet effective manner. Moreover, to further improve the classification accuracy, transfer learning is investigated in this study to transfer the learnt weights from one individual classifier to another (i.e., CNNs). This mechanism speeds up the learning stage. Experimental results with widely used hyperspectral datasets indicate that the proposed deep learning ensemble system provides competitive results compared with state-of-the-art methods in terms of classification accuracy. The combination of deep learning and ensemble learning provides a significant potential for reliable HSI classification.","","","10.1109/JSTARS.2019.2915259","National Natural Science Foundation of China; State Key Laboratory of Frozen Soil Engineering; High Potential Program” of Helmholtz-Zentrum Dresden-Rossendorf; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721080","Convolutional neural network (CNN);deep learning;ensemble;hyperspectral imagery classification;random subspace","Deep learning;Feature extraction;Hyperspectral imaging;Radio frequency;Neural networks","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence)","hyperspectral image classification;deep convolutional neural networks;ensemble-based learning systems;supervised classification;deep learning-based HSI classification;random subspace-based ensemble learning;deep learning ensemble-based classification methods;deep residual network ensemble;transfer learning;CNN;feature extraction ability","","","53","Traditional","","","","IEEE","IEEE Journals"
"Multi-Modal Object Tracking and Image Fusion With Unsupervised Deep Learning","N. LaHaye; J. Ott; M. J. Garay; H. M. El-Askary; E. Linstead","Computational and Data Sciences Department, Chapman University, Orange, CA, USA; Computational and Data Sciences Department, Chapman University, Orange, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Center of Excellence in Earth Systems Modeling and Observations and the Schmid College of Science and Technology, Chapman University, Orange, CA, USA; Schmid College of Science and Technology, Machine Learning and Assistive Technologies Lab, Chapman University, Orange, CA, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","8","3056","3066","The number of different modalities for remote sensors continues to grow, bringing with it an increase in the volume and complexity of the data being collected. Although these datasets individually provide valuable information, in aggregate they provide additional opportunities to discover meaningful patterns on a large scale. However, the ability to combine and analyze disparate datasets is challenged by the potentially vast parameter space that results from aggregation. Each dataset in itself requires instrument-specific and dataset-specific knowledge. If the intention is to use multiple, diverse datasets, one needs an understanding of how to translate and combine these parameters in an efficient and effective manner. While there are established techniques for combining datasets from specific domains or platforms, there is no generic, automated method that can address the problem in general. Here, we discuss the application of deep learning to track objects across different image-like data-modalities, given data in a similar spatio-temporal range, and automatically co-register these images. Using deep belief networks combined with unsupervised learning methods, we are able to recognize and separate different objects within image-like data in a structured manner, thus making progress toward the ultimate goal of a generic tracking and fusion pipeline requiring minimal human intervention.","","","10.1109/JSTARS.2019.2920234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8738825","Big data applications;clustering;computer vision;deep belief networks (DBNs);deep learning","Instruments;Remote sensing;Earth;Data models;Deep learning;Cameras;Data integration","belief networks;image fusion;object detection;object tracking;pattern clustering;unsupervised learning","multimodal object tracking;image fusion;unsupervised deep learning;remote sensors;disparate datasets;dataset-specific knowledge;multiple datasets;diverse datasets;data-modalities;deep belief networks;unsupervised learning methods;generic tracking;fusion pipeline","","","33","Traditional","","","","IEEE","IEEE Journals"
"Deep Manifold Structure Transfer for Action Recognition","C. Li; B. Zhang; C. Chen; Q. Ye; J. Han; G. Guo; R. Ji","School of Mechanical Electronic and Information Engineering, China University of Mining and Technology, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Department of Electrical and Computer Engineering, University of North Carolina at Charlotte, Charlotte, NC, USA; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Computing and Communications, Lancaster University, Lancaster, U.K.; Institute of Deep Learning, Baidu Research and National Engineering Laboratory for Deep Learning Technology and Application, Beijing, China; School of Information Science and Engineering, Xiamen University, Xiamen, China","IEEE Transactions on Image Processing","","2019","28","9","4646","4658","While intrinsic data structure in subspace provides useful information for visual recognition, it has not yet been well studied in deep feature learning for action recognition. In this paper, we introduce a new spatio-temporal manifold network (STMN) that leverages data manifold structures to regularize deep action feature learning, aiming at simultaneously minimizing the intra-class variations of learned deep features and alleviating the over-fitting problem. To this end, the manifold prior is imposed from the top layer of a convolutional neural network (CNN) and propagated across convolutional layers during forward-backward propagation. The observed correspondence of manifold structures in the data space and feature space validates that the manifold priori can be transferred across the CNN layers. The STMN theoretically recasts the problem of transferring the data structure prior into the deep learning architectures as a projection over the manifold via an embedding method, which can be easily solved by an alternating direction method of multipliers and backward propagation (ADMM-BP) algorithm. The STMN is generic in the sense that it can be plugged into various backbone architectures to learn more discriminative representation for action recognition. The extensive experimental results show that our method achieves comparable or even better performance compared with the state-of-the-art approaches on four benchmark datasets.","","","10.1109/TIP.2019.2912357","National Natural Science Foundation of China; National Basic Research Program of China; Shenzhen Science and Technology Program; Shenzhen Peacock Plan; National Key R&D Plan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8699101","Action recognition;manifold;alternating direction method of multipliers;backward propagation;ADMM-BP","Manifolds;Three-dimensional displays;Deep learning;Data structures;Streaming media;Backpropagation","backpropagation;convolutional neural nets;feature extraction;image recognition;image representation;minimisation","intraclass variations minimization;alternating direction method of multipliers and backward propagation algorithm;ADMM-BP algorithm;deep learning architectures;CNN layers;forward-backward propagation;convolutional layers;convolutional neural network;deep action feature learning;STMN;spatio-temporal manifold network;action recognition;deep manifold structure transfer","","","69","","","","","IEEE","IEEE Journals"
"Deep Transfer Low-Rank Coding for Cross-Domain Learning","Z. Ding; Y. Fu","Department of Computer, Information and Technology, Indiana University—Purdue University Indianapolis, Indianapolis, IN, USA; Department of Electrical and Computer Engineering, College of Computer and Information Science, Northeastern University, Boston, MA, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","6","1768","1779","Transfer learning has attracted great attention to facilitate the sparsely labeled or unlabeled target learning by leveraging previously well-established source domain through knowledge transfer. Recent activities on transfer learning attempt to build deep architectures to better fight off cross-domain divergences by extracting more effective features. However, its generalizability would decrease greatly due to the domain mismatch enlarges, particularly at the top layers. In this paper, we develop a novel deep transfer low-rank coding based on deep convolutional neural networks, where we investigate multilayer low-rank coding at the top task-specific layers. Specifically, multilayer common dictionaries shared across two domains are obtained to bridge the domain gap such that more enriched domain-invariant knowledge can be captured through a layerwise fashion. With rank minimization on the new codings, our model manages to preserve the global structures across source and target, and thus, similar samples of two domains tend to gather together for effective knowledge transfer. Furthermore, domain/classwise adaption terms are integrated to guide the effective coding optimization in a semisupervised manner, so the marginal and conditional disparities of two domains will be alleviated. Experimental results on three visual domain adaptation benchmarks verify the effectiveness of our proposed approach on boosting the recognition performance for the target domain, by comparing it with other state-of-the-art deep transfer learning.","","","10.1109/TNNLS.2018.2874567","National Science Foundation; Army Research Office; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8513988","Deep learning;low-rank coding;transfer learning","Encoding;Dictionaries;Nonhomogeneous media;Feature extraction;Task analysis;Knowledge transfer;Adaptation models","convolutional neural nets;learning (artificial intelligence)","effective knowledge transfer;domain/classwise adaption terms;effective coding optimization;visual domain adaptation benchmarks;target domain;state-of-the-art deep transfer learning;deep transfer low-rank coding;sparsely labeled unlabeled target learning;well-established source domain;transfer learning attempt;deep architectures;cross-domain divergences;domain mismatch enlarges;deep convolutional neural networks;domain gap;enriched domain-invariant knowledge;cross-domain learning","","","60","","","","","IEEE","IEEE Journals"
"Learning Deep Binary Descriptor with Multi-Quantization","Y. Duan; J. Lu; Z. Wang; J. Feng; J. Zhou","Department of Automation, State Key Lab of Intelligent Technologies and Systems and Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems and Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems and Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems and Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems and Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","8","1924","1938","In this paper, we propose an unsupervised feature learning method called deep binary descriptor with multi-quantization (DBD-MQ) for visual analysis. Existing learning-based binary descriptors such as compact binary face descriptor (CBFD) and DeepBit utilize the rigid sign function for binarization despite of data distributions, which usually suffer from severe quantization loss. In order to address the limitation, we propose a deep multi-quantization network to learn a data-dependent binarization in an unsupervised manner. More specifically, we design a K-Autoencoders (KAEs) network to jointly learn the parameters of feature extractor and the binarization functions under a deep learning framework, so that discriminative binary descriptors can be obtained with a fine-grained multi-quantization. As DBD-MQ simply allocates the same number of quantizers to each real-valued feature dimension ignoring the elementwise diversity of informativeness, we further propose a deep competitive binary descriptor with multi-quantization (DCBD-MQ) method to learn optimal allocation of bits with the fixed binary length in a competitive manner, where informative dimensions gain more bits for complete representation. Moreover, we present a similarity-aware binary encoding strategy based on the earth mover's distance of Autoencoders, so that elements that are quantized into similar Autoencoders will have smaller Hamming distances. Extensive experimental results on six widely-used datasets show that our DBD-MQ and DCBD-MQ outperform most state-of-the-art unsupervised binary descriptors.","","","10.1109/TPAMI.2018.2858760","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; National 1000 Young Talents Plan Program; Shenzhen Fundamental Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417979","Binary descriptor;unsupervised learning;deep learning;competitive learning;multi-quantization;K-Autoencoders","Binary codes;Quantization (signal);Learning systems;Machine learning;Encoding;Resource management;Feature extraction","data compression;feature extraction;image coding;image representation;neural nets;unsupervised learning","unsupervised feature learning method;deep multiquantization network;data-dependent binarization;K-Autoencoders network;binarization functions;deep learning framework;discriminative binary descriptors;fine-grained multiquantization;DBD-MQ;DCBD-MQ;fixed binary length;similarity-aware binary encoding strategy;real-valued feature dimension;KAE network;feature extractor;deep competitive binary descriptor with multiquantization method;optimal bit allocation;informative dimensions;earth mover's distance;Hamming distances","","2","76","","","","","IEEE","IEEE Journals"
"Generalization and Expressivity for Deep Nets","S. Lin","Department of Mathematics, Wenzhou University, Wenzhou, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","5","1392","1406","Along with the rapid development of deep learning in practice, theoretical explanations for its success become urgent. Generalization and expressivity are two widely used measurements to quantify theoretical behaviors of deep nets. The expressivity focuses on finding functions expressible by deep nets but cannot be approximated by shallow nets with similar number of neurons. It usually implies the large capacity. The generalization aims at deriving fast learning rate for deep nets. It usually requires small capacity to reduce the variance. Different from previous studies on deep nets, pursuing either expressivity or generalization, we consider both the factors to explore theoretical advantages of deep nets. For this purpose, we construct a deep net with two hidden layers possessing excellent expressivity in terms of localized and sparse approximation. Then, utilizing the well known covering number to measure the capacity, we find that deep nets possess excellent expressive power (measured by localized and sparse approximation) without essentially enlarging the capacity of shallow nets. As a consequence, we derive near-optimal learning rates for implementing empirical risk minimization on deep nets. These results theoretically exhibit advantages of deep nets from the learning theory viewpoint.","","","10.1109/TNNLS.2018.2868980","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8475035","Deep learning;expressivity;generalization;learning theory;localized approximation","Neurons;Sparse representation;Biological neural networks;Machine learning;Learning systems;Power measurement;Risk management","approximation theory;learning (artificial intelligence);minimisation;neural nets;set theory;sparse matrices","deep net;shallow nets;empirical risk minimization;sparse approximation;near-optimal learning rates;learning theory viewpoint;deep learning","","","48","","","","","IEEE","IEEE Journals"
"Deep Ensemble Machine for Video Classification","J. Zheng; X. Cao; B. Zhang; X. Zhen; X. Su","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","2","553","565","Video classification has been extensively researched in computer vision due to its wide spread applications. However, it remains an outstanding task because of the great challenges in effective spatial-temporal feature extraction and efficient classification with high-dimensional video representations. To address these challenges, in this paper, we propose an end-to-end learning framework called deep ensemble machine (DEM) for video classification. Specifically, to establish effective spatio-temporal features, we propose using two deep convolutional neural networks (CNNs), i.e., vision and graphics group and C3-D to extract heterogeneous spatial and temporal features for complementary representations. To achieve efficient classification, we propose ensemble learning based on random projections aiming to transform high-dimensional features into a set of lower dimensional compact features in subspaces; an ensemble of classifiers is trained on the subspaces and combined with a weighting layer during the backpropagation. To further enhance the performance, we introduce rectified linear encoding (RLE) inspired from error-correcting output coding to encode the initial outputs of classifiers, followed by a softmax layer to produce the final classification results. DEM combines the strengths of deep CNNs and ensemble learning, which establishes a new end-to-end learning architecture for more accurate and efficient video classification. We show the great effectiveness of DEM by extensive experiments on four data sets for diverse video classification tasks including action recognition and dynamic scene classification. Results have shown that DEM achieves high performance on all tasks with an improvement of up to 13% on CIFAR10 data set over the baseline model.","","","10.1109/TNNLS.2018.2844464","National Key Research and Development Program of China; National Science Fund for Distinguished Young Scholars; National Natural Science Foundation of China; Foundation for Innovative Research Groups of the National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8402255","Action recognition;deep learning;dynamic scene classification;ensemble learning;random projection;rectified linear encoding (RLE);video classification","Feature extraction;Machine learning;Task analysis;Encoding;Computer architecture;Learning systems;Computational modeling","backpropagation;computer vision;convolutional neural nets;feature extraction;image classification;image representation;learning (artificial intelligence);video signal processing","DEM;deep CNNs;end-to-end learning architecture;deep ensemble machine;spatial-temporal feature extraction;deep convolutional neural networks;action recognition;scene classification;video representations;deep ensemble learning;computer vision;video classification tasks;rectified linear encoding;RLE;softmax layer","","4","71","","","","","IEEE","IEEE Journals"
"Deep Learning for Hyperspectral Image Classification: An Overview","S. Li; W. Song; L. Fang; Y. Chen; P. Ghamisi; J. A. Benediktsson","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Department of Information Engineering, School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Helmholtz-Zentrum Dresden-Rossendorf (HZDR), Helmholtz Institute Freiberg for Resource Technology (HIF), Exploration, Freiberg, Germany; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavk, Iceland","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","9","6690","6709","Hyperspectral image (HSI) classification has become a hot topic in the field of remote sensing. In general, the complex characteristics of hyperspectral data make the accurate classification of such data challenging for traditional machine learning methods. In addition, hyperspectral imaging often deals with an inherently nonlinear relation between the captured spectral information and the corresponding materials. In recent years, deep learning has been recognized as a powerful feature-extraction tool to effectively address nonlinear problems and widely used in a number of image processing tasks. Motivated by those successful applications, deep learning has also been introduced to classify HSIs and demonstrated good performance. This survey paper presents a systematic review of deep learning-based HSI classification literatures and compares several strategies for this topic. Specifically, we first summarize the main challenges of HSI classification which cannot be effectively overcome by traditional machine learning methods, and also introduce the advantages of deep learning to handle these problems. Then, we build a framework that divides the corresponding works into spectral-feature networks, spatial-feature networks, and spectral-spatial-feature networks to systematically review the recent achievements in deep learning-based HSI classification. In addition, considering the fact that available training samples in the remote sensing field are usually very limited and training deep networks require a large number of samples, we include some strategies to improve classification performance, which can provide some guidelines for future studies on this topic. Finally, several representative deep learning-based classification methods are conducted on real HSIs in our experiments.","","","10.1109/TGRS.2019.2907932","National Natural Science Foundation of China; Science and Technology Plan Project Fund of Hunan Province; Science and Technology Talents Program of Hunan Association for Science and Technology; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8697135","Classification;deep learning;feature extraction;hyperspectral image (HSI)","Feature extraction;Deep learning;Hyperspectral imaging;Training;Logistics","feature extraction;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);remote sensing","hyperspectral image classification;hyperspectral data;traditional machine learning methods;image processing tasks;deep learning-based HSI classification literatures;spectral-spatial-feature networks;deep networks;classification performance;representative deep learning-based classification methods;feature-extraction tool;remote sensing;spectral information","","","119","","","","","IEEE","IEEE Journals"
"A Deep Reinforcement Learning Network for Traffic Light Cycle Control","X. Liang; X. Du; G. Wang; Z. Han","Department of Computer Science, New Jersey Institute of Technology, Newark, NJ, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; Department of Computer Science, New Jersey Institute of Technology, Newark, NJ, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA","IEEE Transactions on Vehicular Technology","","2019","68","2","1243","1253","Existing inefficient traffic light cycle control causes numerous problems, such as long delay and waste of energy. To improve efficiency, taking real-time traffic information as an input and dynamically adjusting the traffic light duration accordingly is a must. Existing works either split the traffic signal into equal duration or only leverage limited traffic information. In this paper, we study how to decide the traffic signal duration based on the collected data from different sensors. We propose a deep reinforcement learning model to control the traffic light cycle. In the model, we quantify the complex traffic scenario as states by collecting traffic data and dividing the whole intersection into small grids. The duration changes of a traffic light are the actions, which are modeled as a high-dimension Markov decision process. The reward is the cumulative waiting time difference between two cycles. To solve the model, a convolutional neural network is employed to map states to rewards. The proposed model incorporates multiple optimization elements to improve the performance, such as dueling network, target network, double Q-learning network, and prioritized experience replay. We evaluate our model via simulation on a Simulation of Urban MObility simulator. Simulation results show the efficiency of our model in controlling traffic lights.","","","10.1109/TVT.2018.2890726","National Science Foundation; Air Force Office of Scientific Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600382","Reinforcement learning;deep learning;traffic light control;vehicular network","Reinforcement learning;Deep learning;Real-time systems;Control systems;Roads;Computational modeling;Adaptation models","control engineering computing;convolutional neural nets;learning (artificial intelligence);Markov processes;optimisation;road traffic control;sensor fusion;traffic engineering computing","sensor data;traffic light cycle control;optimization elements;Markov decision process;traffic signal duration;traffic light duration;real-time traffic information;deep reinforcement learning network;convolutional neural network;cumulative waiting time difference;deep reinforcement learning model","","4","33","","","","","IEEE","IEEE Journals"
"Optimizing Autoencoders for Learning Deep Representations From Health Data","C. Zhou; Y. Jia; M. Motani","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Journal of Biomedical and Health Informatics","","2019","23","1","103","111","Analyzing patients' health data using machine learning techniques can improve both patient outcomes and hospital operations. However, heterogeneous patient data (e.g., vital signs) and inefficient feature learning methods affect the implementation of machine learning-based patient data analysis. In this paper, we present a novel unsupervised deep learning-based feature learning (DFL) framework to automatically learn compact representations from patient health data for efficient clinical decision making. Real-world pneumonia patient data from the National University Hospital in Singapore are collected and analyzed to evaluate the performance of DFL. Furthermore, publicly available electroencephalogram data are extracted from the UCI Machine Learning Repository to test and support our findings. Using both data sets, we compare the performance of DFL to that of several popular feature learning methods and demonstrate its advantages.","","","10.1109/JBHI.2018.2856820","Ministry of Education - Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8412086","Machine learning;feature learning;deep learning;unsupervised learning;classification;patient health data","Feature extraction;Data mining;Training;Noise reduction;Machine learning;Interpolation;Hospitals","data analysis;decision making;diseases;electroencephalography;medical information systems;neural nets;pattern classification;unsupervised learning","real-world pneumonia patient data;National University Hospital;UCI Machine Learning Repository;Singapore;unsupervised deep learning-based feature learning;patients health data analysis;deep representation learning;autoencoder optimization;electroencephalogram data;clinical decision making;machine learning","","1","32","","","","","IEEE","IEEE Journals"
"Securing Deep Learning Based Edge Finger Vein Biometrics With Binary Decision Diagram","W. Yang; S. Wang; J. Hu; G. Zheng; J. Yang; C. Valli","Security Research Institute, School of Science, Edith Cowan University, WA, Australia; School of Engineering and Mathematical Sciences, La Trobe University, VIC, Australia; School of Engineering and Information Technology, University of New South Wales at the Australian Defence Force Academy (UNSW@ADFA), Canberra, ACT, Australia; Security Research Institute, School of Science, Edith Cowan University, WA, Australia; College of Computer Science and Information Engineering, Tianjin University of Science and Technology, Tianjin, China; Security Research Institute, School of Science, Edith Cowan University, WA, Australia","IEEE Transactions on Industrial Informatics","","2019","15","7","4244","4253","With built-in artificial intelligence (AI), edge devices, e.g., smart cameras, can perform tasks like detecting and tracking individuals, which is referred to as edge biometrics. As a driving force for AI, machine/deep learning plays a critical role in edge biometrics. Machine/deep learning based edge biometric systems outperform their nonmachine learning counterpart. However, research shows that artificial neural networks, e.g., convolutional neural networks, are invertible such that adversaries can obtain a certain amount of information about the original inputs/templates. This information leakage is not tolerable for biometric systems because biometric data in the original (raw) templates cannot be reset or replaced. Once compromised, they are lost forever. Therefore, how to prevent original biometric templates from being attacked through inverting deep neural networks is a pressing, but unsolved issue, for deep learning based biometric recognition. To address the issue, in this paper, we develop a novel biometric template protection algorithm using the binary decision diagram (BDD) for deep learning based finger-vein biometric systems. The proposed algorithm is capable of creating a new noninvertible version of the original finger-vein template, which is stacked with an artificial neural network-the multilayer extreme learning machine (ML-ELM) to generate a privacy-preserving finger-vein recognition system, named BDD-ML-ELM. The proposed BDD-ML-ELM ensures the safety of the original finger-vein template even if its transformed version is compromised. The transformed template, if compromised, can be revoked and replaced with another new version by simply changing the user-specific keys. Therefore, the BDD-ML-ELM has a clear advantage over the existing machine/deep learning based biometric systems, whose raw biometric templates are vulnerable when the artificial neural network suffers an inversion attack.","","","10.1109/TII.2019.2900665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648285","Artificial intelligence (AI);binary decision diagram (BDD);biometric template protection;edge biometrics;edge computing;finger vein;machine/deep learning","Biometrics (access control);Feature extraction;Deep learning;Security;Artificial neural networks","binary decision diagrams;biometrics (access control);data privacy;feature extraction;fingerprint identification;learning (artificial intelligence);vein recognition","edge finger vein biometrics;binary decision diagram;artificial intelligence;edge devices;edge biometric systems;nonmachine learning counterpart;convolutional neural networks;biometric data;deep neural networks;biometric recognition;deep learning based finger-vein biometric systems;privacy-preserving finger-vein recognition system;transformed template;artificial neural network;machine/deep learning;biometric templates;BDD-ML-ELM system;finger-vein template;biometric template protection algorithm","","","33","","","","","IEEE","IEEE Journals"
"Deep Learning-Based Channel Estimation and Equalization Scheme for FBMC/OQAM Systems","X. Cheng; D. Liu; C. Wang; S. Yan; Z. Zhu","College of Information Science and Engineering, China University of Petroleum, Beijing; College of Information Science and Engineering, China University of Petroleum, Beijing; College of Information Science and Engineering, China University of Petroleum, Beijing; College of Information Science and Engineering, China University of Petroleum, Beijing; College of Information Science and Engineering, China University of Petroleum, Beijing","IEEE Wireless Communications Letters","","2019","8","3","881","884","Filter bank multicarrier (FBMC) modulation is a promising candidate modulation method for future communication systems. However, FBMC systems cannot directly use channel estimation methods proposed for orthogonal frequency-division multiplexing systems due to its inherent imaginary interference. In this letter, we propose a channel estimation and equalization scheme based on deep learning (DL-CE) for FBMC systems. In the DL-CE scheme, the channel state information and the constellation demapping method are learned by a deep neural networks model, and then the distorted frequency-domain sequences are equalized implicitly to obtain binary bits directly. Simulation results show that the proposed DL-CE scheme achieves state-of-the-art performance on channel estimation and equalization.","","","10.1109/LWC.2019.2898437","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8637720","Filter bank multicarrier;OQAM;channel estimation;deep learning;deep neural networks","Channel estimation;Interference;Quadrature amplitude modulation;Neurons;Training;Deep learning;OFDM","channel bank filters;channel estimation;equalisers;learning (artificial intelligence);neural nets;OFDM modulation;quadrature amplitude modulation;radiofrequency interference;telecommunication computing;wireless channels","DL-CE scheme;channel state information;constellation demapping method;deep neural networks model;distorted frequency-domain sequences;filter bank multicarrier modulation;future communication systems;channel estimation methods;orthogonal frequency-division multiplexing systems;imaginary interference;FBMC system;OQAM system;deep learning-based channel estimation scheme;deep learning-based channel equalization scheme","","3","15","","","","","IEEE","IEEE Journals"
"Reply to “Comments on ‘Traffic Sign Recognition Using Kernel Extreme Learning Machines With Deep Perceptual Features”’","Y. Zeng","College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China","IEEE Transactions on Intelligent Transportation Systems","","2019","20","10","3762","3764","This paper addresses the questions and concerns raised in the comment paper entitled “Comments on `Traffic Sign Recognition Using Kernel Extreme Leaning Machines With Deep Perceptual Features”'. It is analyzed that the major questions and concerns in the comment paper are due to some misunderstanding on the statements and the main contributions of the original paper by Zeng et al.. The main clarifications are as follows: (1) The weight update rule used by Zeng et al. is a standard form of kernel extreme learning machine (KELM) but not the rule of reduced KELM mentioned by the comment paper. (2) The aim and main contribution of the original paper by Zeng et al. are to improve the classification precision of traffic sign recognition (TSR) based on convolutional neural networks without dramatically increasing the scale or complexity of the deep neural network model, which leads to a relatively less training cost. The authors of the comment paper focused on the reduction of KELM's computational costs but it is beyond the topic of the original paper. In addition, the reduction of KELM's computational costs has been well studied in the literature. (3) The supplementary experiment is provided to show that the TSR method proposed by Zeng et al. is valid whether or not the training samples are much more than the needed kernels.","","","10.1109/TITS.2019.2933410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798758","Traffic sign recognition;extreme learning machines;kernel extreme learning machines;convolutional neural networks;deep learning","Kernel;Training;Complexity theory;Computational efficiency;Computational modeling;Convolutional neural networks","image recognition;learning (artificial intelligence);neural nets;traffic engineering computing","traffic sign recognition;deep perceptual features;kernel extreme learning machine;deep neural network model;KELM;convolutional neural networks","","","15","","","","","IEEE","IEEE Journals"
"Surface-Electromyography-Based Gesture Recognition by Multi-View Deep Learning","W. Wei; Q. Dai; Y. Wong; Y. Hu; M. Kankanhalli; W. Geng","State Key Laboratory of CAD&CG, College of Computer Science and TechnologyZhejiang University; State Key Laboratory of CAD&CG, College of Computer Science and TechnologyZhejiang University; School of ComputingNational University of Singapore; State Key Laboratory of CAD&CG, College of Computer Science and TechnologyZhejiang University; School of ComputingNational University of Singapore; State Key Laboratory of CAD&CG, College of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Biomedical Engineering","","2019","66","10","2964","2973","Gesture recognition using sparse multichannel surface electromyography (sEMG) is a challenging problem, and the solutions are far from optimal from the point of view of muscle-computer interface. In this paper, we address this problem from the context of multi-view deep learning. A novel multi-view convolutional neural network (CNN) framework is proposed by combining classical sEMG feature sets with a CNN-based deep learning model. The framework consists of two parts. In the first part, multi-view representations of sEMG are modeled in parallel by a multistream CNN, and a performance-based view construction strategy is proposed to choose the most discriminative views from classical feature sets for sEMG-based gesture recognition. In the second part, the learned multi-view deep features are fused through a view aggregation network composed of early and late fusion subnetworks, taking advantage of both early and late fusion of learned multi-view deep features. Evaluations on 11 sparse multichannel sEMG databases as well as five databases with both sEMG and inertial measurement unit data demonstrate that our multi-view framework outperforms single-view methods on both unimodal and multimodal sEMG data streams.","","","10.1109/TBME.2019.2899222","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; National Research Foundation; Prime Minister's Office, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641445","Surface electromyography;muscle-computer interface;human-computer interface;multi-view learning;deep learning","Gesture recognition;Deep learning;Feature extraction;Databases;Electromyography;Human computer interaction;Discrete wavelet transforms","convolutional neural nets;electromyography;gesture recognition;learning (artificial intelligence);medical image processing","surface-electromyography-based gesture recognition;multiview deep learning;sparse multichannel surface electromyography;classical sEMG feature;CNN-based deep learning model;sparse multichannel sEMG databases;multiview convolutional neural network framework","","2","46","Traditional","","","","IEEE","IEEE Journals"
"Deep Learning and Handcrafted Method Fusion: Higher Diagnostic Accuracy for Melanoma Dermoscopy Images","J. R. Hagerty; R. J. Stanley; H. A. Almubarak; N. Lama; R. Kasmi; P. Guo; R. J. Drugge; H. S. Rabinovitz; M. Oliviero; W. V. Stoecker","S&A Technologies, Rolla, MO, USA; Missouri University of Science & Technology, Rolla, MO, USA; Missouri University of Science & Technology, Rolla, MO, USA; Missouri University of Science & Technology, Rolla, MO, USA; University of Bejaia, Bejaia, Algeria; Missouri University of Science & Technology, Rolla, MO, USA; Sheard and Drugge, Stamford, CT, USA; Plantation Skin and Cancer Associates, Plantation, FL, USA; Plantation Skin and Cancer Associates, Plantation, FL, USA; S&A Technologies, Rolla, MO, USA","IEEE Journal of Biomedical and Health Informatics","","2019","23","4","1385","1391","This paper presents an approach that combines conventional image processing with deep learning by fusing the features from the individual techniques. We hypothesize that the two techniques, with different error profiles, are synergistic. The conventional image processing arm uses three handcrafted biologically inspired image processing modules and one clinical information module. The image processing modules detect lesion features comparable to clinical dermoscopy information-atypical pigment network, color distribution, and blood vessels. The clinical module includes information submitted to the pathologist- patient age, gender, lesion location, size, and patient history. The deep learning arm utilizes knowledge transfer via a ResNet-50 network that is repurposed to predict the probability of melanoma classification. The classification scores of each individual module from both processing arms are then ensembled utilizing logistic regression to predict an overall melanoma probability. Using cross-validated results of melanoma classification measured by area under the receiver operator characteristic curve (AUC), classification accuracy of 0.94 was obtained for the fusion technique. In comparison, the ResNet-50 deep learning based classifier alone yields an AUC of 0.87 and conventional image processing based classifier yields an AUC of 0.90. Further study of fusion of conventional image processing techniques and deep learning is warranted.","","","10.1109/JBHI.2019.2891049","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8601319","Melanoma;dermoscopy;deep learning;classifier;transfer learning","Image color analysis;Lesions;Melanoma;Hair;Deep learning;Biomedical imaging","biomedical optical imaging;cancer;feature extraction;image classification;image colour analysis;image fusion;image segmentation;image texture;learning (artificial intelligence);medical image processing;patient diagnosis;regression analysis;skin","individual module;melanoma probability;melanoma classification;classification accuracy;fusion technique;ResNet-50 deep learning;conventional image processing based classifier;conventional image processing techniques;higher diagnostic accuracy;melanoma dermoscopy images;individual techniques;different error profiles;conventional image processing arm;image processing modules;clinical information module;lesion features;clinical dermoscopy information-atypical pigment network;deep learning arm;ResNet-50 network;handcrafted method fusion","","1","46","","","","","IEEE","IEEE Journals"
"Deep Multigrained Cascade Forest for Hyperspectral Image Classification","X. Liu; R. Wang; Z. Cai; Y. Cai; X. Yin","School of Automation, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","10","8169","8183","Currently, deep neural networks (DNNs) are an important method for handling hyperspectral image (HSI) classification because of their good performance in image processing. However, DNNs' performance depends on a massive number of training data and hyperparameters that are carefully fine-tuned, which results in structural complexity and a time-consuming process. Deep forest is a novel deep learning method that does not need much training data and has a simple structure. In this paper, we first design a deep forest for spectral-based HSI classification and then propose an improved deep forest algorithm, named deep multigrained cascade forest (dgcForest), for spatial-based HSI classification. On the one hand, the cascade forest in dgcForest is used in multigrained scanning, which enhances the performance; on the other hand, a pooling layer is added after the multigrained scanning to reduce the output dimensions. To demonstrate that our proposed algorithm presents a good performance in HSI classification, we analyze the hyperparameters of deep forest and dgcForest and compare them with other methods on the biased and unbiased data sets, which illustrates that our method is superior to other state-of-the-art deep learning methods.","","","10.1109/TGRS.2019.2918587","National Natural Science Foundation of China; Natural Science Foundation of Hubei Province; China University of Geosciences, Wuhan; Key Research and Development Project of Hainan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736489","Deep forest;deep learning;hyperspectral image (HSI) classification;machine learning","Forestry;Deep learning;Hyperspectral imaging;Vegetation;Feature extraction;Geology;Decision trees","geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);neural nets","hyperspectral image classification;deep neural networks;image processing;deep learning method;spectral-based HSI classification;dgcForest;spatial-based HSI classification;multigrained scanning;deep forest algorithm;deep multigrained cascade forest","","","46","","","","","IEEE","IEEE Journals"
"Deep Coupled-Representation Learning for Sparse Linear Inverse Problems With Side Information","E. Tsiligianni; N. Deligiannis","Department of Electronics and Informatics, Vrije Universiteit Brussel, Brussels, Belgium; Department of Electronics and Informatics, Vrije Universiteit Brussel, Brussels, Belgium","IEEE Signal Processing Letters","","2019","26","12","1768","1772","In linear inverse problems, the goal is to recover a target signal from undersampled, incomplete or noisy linear measurements. Typically, the recovery relies on complex numerical optimization methods; recent approaches perform an unfolding of a numerical algorithm into a neural network form, resulting in a substantial reduction of the computational complexity. In this letter, we consider the recovery of a target signal with the aid of a correlated signal, the so-called side information (SI), and propose a deep unfolding model that incorporates SI. The proposed model is used to learn coupled representations of correlated signals from different modalities, enabling the recovery of multi-modal data at a low computational cost. As such, our work introduces the first deep unfolding method with SI, which actually comes from a different modality. We apply our model to reconstruct near-infrared images from undersampled measurements given RGB images as SI. Experimental results demonstrate the superior performance of the proposed framework against single-modal deep learning methods that do not use SI, multi-modal deep learning designs, and optimization algorithms.","","","10.1109/LSP.2019.2929869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844082","Inverse problems;coupled representations;representation learning;designing deep neural networks","Neural networks;Inverse problems;Deep learning;Computational modeling;Sparse representation;Signal processing algorithms;Dictionaries","computational complexity;image reconstruction;image representation;inverse problems;learning (artificial intelligence);neural nets;optimisation","optimization algorithms;sparse linear inverse problems;target signal;undersampled linear measurements;incomplete linear measurements;noisy linear measurements;complex numerical optimization methods;numerical algorithm;neural network form;computational complexity;correlated signal;RGB images;near-infrared image reconstruction;deep coupled-representation learning;single-modal deep learning methods;undersampled measurements;deep unfolding method;low computational cost;multimodal data;coupled representations;deep unfolding model","","1","32","IEEE","","","","IEEE","IEEE Journals"
"A Deep Learning Approach for Microwave and Millimeter-Wave Radiometer Calibration","M. Ogut; X. Bosch-Lluis; S. C. Reising","Microwave Systems Laboratory, Colorado State University, Fort Collins, CO, USA; Microwave Systems Laboratory, Colorado State University, Fort Collins, CO, USA; Microwave Systems Laboratory, Colorado State University, Fort Collins, CO, USA","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","8","5344","5355","Deep learning artificial neural network techniques can be applied for on-orbit calibration of microwave and millimeter-wave radiometer spaceborne instruments, including those for small satellites. The noise-wave model has been employed for noise characterization and validation of the proposed deep learning calibration technique for a synthetically generated Dicke-switching radiometer. The developed deep learning neural network radiometer calibrator produces high accuracy estimates of antenna temperatures from the measurements of radiometer output voltage and thermistor readings. Tests with noise-free and noisy samples of the developed model have shown that the proposed calibration method does not add any significant noise to the radiometer calibration. The performance of the proposed method does not degrade with increased nonlinearity for a radiometer, while nonlinearity is a challenging issue for conventional calibration techniques. The deep learning calibration model learns the radiometer noise characteristics from radiometer prelaunch measurements during thermal vacuum chamber testing. The neural network calibrator proposed in this paper has self-learning capability during the on-orbit operation of a radiometer that can be used to improve the performance of on-orbit calibration. The proposed technique is demonstrated by comparing the residual uncertainty of the deep learning calibration with the theoretical value. No numerical study is presented to compare the performance with conventional calibration techniques. The new method may be solely applied to calibrate the radiometer or applied along with conventional calibration techniques.","","","10.1109/TGRS.2019.2899110","National Aeronautics and Space Administration; Earth Science Division Technology Office; Instrument Incubator Program; Earth Science Division Flight Venture Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661787","Calibration;CubeSats;deep learning;microwave radiometer;millimeter-wave radiometer;neural network;noise-wave model","Microwave radiometry;Calibration;CubeSat;Deep learning;Antenna measurements;Microwave theory and techniques;Temperature measurement","calibration;computerised instrumentation;learning (artificial intelligence);microwave detectors;microwave measurement;millimetre wave detectors;millimetre wave measurement;neural nets;numerical analysis;radiometers;temperature measurement;temperature sensors;thermistors","radiometer noise characteristics;radiometer prelaunch measurements;self-learning capability;on-orbit calibration;millimeter-wave radiometer calibration;millimeter-wave radiometer spaceborne instruments;noise-wave model;deep learning calibration technique;Dicke-switching radiometer;deep learning neural network radiometer calibrator;artificial neural network calibrator techniques;microwave radiometer calibration;microwave radiometer spaceborne instruments;antenna temperature estimation;radiometer output voltage measurement;thermistor readings;thermal vacuum chamber testing","","","37","","","","","IEEE","IEEE Journals"
"Utilizing Unlabeled Data to Detect Electricity Fraud in AMI: A Semisupervised Deep Learning Approach","T. Hu; Q. Guo; X. Shen; H. Sun; R. Wu; H. Xi","Tsinghua-Berkeley Shenzhen Institute, Shenzhen, China; Tsinghua-Berkeley Shenzhen Institute, Shenzhen, China; Tsinghua-Berkeley Shenzhen Institute, Shenzhen, China; Tsinghua-Berkeley Shenzhen Institute, Shenzhen, China; Tsinghua-Berkeley Shenzhen Institute, Shenzhen, China; Tsinghua-Berkeley Shenzhen Institute, Shenzhen, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","11","3287","3299","As nontechnical losses in power systems have recently become a global concern, electricity fraud detection models attracted increasing academic interest. The wide application of smart meters has offered more possibility to detecting fraud from user's consumption patterns. However, the performances of existing consumption-based electricity fraud detection models are still not satisfactory enough for practice, partly due to their limited ability to handle high-dimensional data. In this paper, a deep-learning-based model is developed for detecting electricity fraud in the advanced metering infrastructure, namely, the multitask feature extracting fraud detector (MFEFD). The deep architecture has brought MFEFD a powerful ability to handle high-dimensional input, through which consumption patterns inside load profiles can be effectively extracted. Another challenge is that the insufficiency of labeled data has restricted the generalization of existing models since they are mostly based on supervised learning and labeled data. MFEFD is trained in a semisupervised manner, in which multitask training was implemented to combine the supervised and unsupervised training, so that both the knowledge from unlabeled and labeled data can be made use of. Real-world-data-based case studies have demonstrated MFEFD's high detection performance, robustness, privacy preservation, and practicability.","","","10.1109/TNNLS.2018.2890663","National Key Research and Development Program of China (Basic Research Class); State Grid Corporation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629365","Deep learning;electricity fraud detection;multitask learning;nontechnical losses (NTL);semisupervised training","Training;Load modeling;Hardware;Smart meters;Data models;Feature extraction;Meters","feature extraction;fraud;learning (artificial intelligence);power consumption;power engineering computing;power meters;power system measurement","consumption patterns;high-dimensional data;deep-learning-based model;advanced metering infrastructure;fraud detector;deep architecture;high-dimensional input;supervised learning;real-world-data-based case studies;unlabeled data;detect electricity fraud;semisupervised deep learning approach;nontechnical losses;power systems;smart meters;MFEFD high detection performance;consumption-based electricity fraud detection models","","1","43","","","","","IEEE","IEEE Journals"
"Multi-Scale and Multi-Task Deep Learning Framework for Automatic Road Extraction","X. Lu; Y. Zhong; Z. Zheng; Y. Liu; J. Zhao; A. Ma; J. Yang","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","9362","9377","Road detection and centerline extraction from very high-resolution (VHR) remote sensing imagery are of great significance in various practical applications. Road detection and centerline extraction operations depend on each other, to a certain extent. The road detection constrains the appearance of the centerline, and the centerline enhances the linear features of the road detection. However, most of the previous works have addressed these two tasks separately and have not considered the symbiotic relationship between them, making it difficult to obtain smooth and complete roads. In this paper, a novel multi-scale and multi-task deep learning framework for automatic road extraction (MSMT-RE) is proposed to build the relationship between them and simultaneously complete the road detection and centerline extraction tasks. U-Net is selected as the basic network for multi-task learning due to its strong ability to preserve spatial details. Multi-scale feature integration is also applied in the framework to increase the robustness of the feature extraction. Meanwhile, an adaptive loss function is introduced to solve the problems of roads taking up a small percentage of the training samples, and the fact that the positive samples of the two tasks are unbalanced. Finally, experiments were conducted on two public road data sets and two large images from Google Earth, and the proposed framework was compared with other state-of-the-art deep learning-based road extraction methods, both quantitatively and qualitatively. The proposed approach outperformed all the compared methods, confirming its advantages in automatic road extraction.","","","10.1109/TGRS.2019.2926397","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Natural Science Foundation of Hubei Province; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792386","Adaptive loss function;centerline extraction;deep learning framework;multi-scale and multi-task;remote sensing;road detection","Roads;Feature extraction;Remote sensing;Task analysis;Deep learning;Training;Labeling","feature extraction;geophysical image processing;image resolution;learning (artificial intelligence);object detection;remote sensing;roads","multitask deep learning framework;automatic road extraction;road detection;centerline extraction operations;multiscale feature integration;feature extraction;public road data sets;multiscale deep learning framework;very high-resolution remote sensing imagery;adaptive loss function;deep learning-based road extraction methods","","","50","","","","","IEEE","IEEE Journals"
"Learning Deep Features for One-Class Classification","P. Perera; V. M. Patel","Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA","IEEE Transactions on Image Processing","","2019","28","11","5450","5463","We present a novel deep-learning-based approach for one-class transfer learning in which labeled data from an unrelated task is used for feature learning in one-class classification. The proposed method operates on top of a convolutional neural network (CNN) of choice and produces descriptive features while maintaining a low intra-class variance in the feature space for the given class. For this purpose two loss functions, compactness loss and descriptiveness loss, are proposed along with a parallel CNN architecture. A template matching-based framework is introduced to facilitate the testing process. Extensive experiments on publicly available anomaly detection, novelty detection, and mobile active authentication datasets show that the proposed deep one-class (DOC) classification method achieves significant improvements over the state-of-the-art.","","","10.1109/TIP.2019.2917862","Office of Naval Research Global; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721681","One-class classification;anomaly detection;novelty detection;deep learning","Feature extraction;Training;Task analysis;Training data;Anomaly detection;Authentication;Deep learning","convolutional neural nets;image classification;image matching;learning (artificial intelligence)","deep features;deep-learning-based approach;one-class transfer learning;feature learning;feature space;compactness loss;descriptiveness loss;parallel CNN architecture;template matching-based framework;publicly available anomaly detection;deep one-class classification;intraclass variance;DOC classification method;convolutional neural network;novelty detection;mobile active authentication datasets","","","49","","","","","IEEE","IEEE Journals"
"Knowledge-Driven Service Offloading Decision for Vehicular Edge Computing: A Deep Reinforcement Learning Approach","Q. Qi; J. Wang; Z. Ma; H. Sun; Y. Cao; L. Zhang; J. Liao","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; EBUPT Information Technology Company, Ltd., Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Vehicular Technology","","2019","68","5","4192","4203","The smart vehicles construct Internet of Vehicle (IoV), which can execute various intelligent services. Although the computation capability of a vehicle is limited, multi-type of edge computing nodes provide heterogeneous resources for intelligent vehicular services. When offloading the complex service to the vehicular edge computing node, the decision for its destination should be considered according to numerous factors. This paper mostly formulate the offloading decision as a resource scheduling problem with single or multiple objective function and constraints, where some customized heuristics algorithms are explored. However, offloading multiple data dependence tasks in a complex service is a difficult decision, as an optimal solution must understand the resource requirement, the access network, the user mobility, and importantly the data dependence. Inspired by recent advances in machine learning, we propose a knowledge driven (KD) service offloading decision framework for IoV, which provides the optimal policy directly from the environment. We formulate the offloading decision for the multiple tasks as a long-term planning problem, and explore the recent deep reinforcement learning to obtain the optimal solution. It can scruple the future data dependence of the following tasks when making decision for a current task from the learned offloading knowledge. Moreover, the framework supports the pre-training at the powerful edge computing node and continually online learning when the vehicular service is executed, so that it can adapt the environment changes and can learn policy that are sensible in foresight. The simulation results show that KD service offloading decision converges quickly, adapts to different conditions, and outperforms a greedy offloading decision algorithm.","","","10.1109/TVT.2019.2894437","National Natural Science Foundation of China; Beijing Municipal Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620546","Internet of Vehicle;service offloading decision;multi-task;knowledge driven;deep reinforcement learning","Edge computing;Task analysis;Computational modeling;Optimization;Deep learning;Reinforcement learning;Servers","greedy algorithms;Internet;learning (artificial intelligence);scheduling;traffic engineering computing","smart vehicles;IoV;intelligent services;computation capability;edge computing nodes;heterogeneous resources;intelligent vehicular services;complex service;vehicular edge computing node;resource scheduling problem;multiple objective function;customized heuristics algorithms;multiple data dependence tasks;resource requirement;machine learning;optimal policy;multiple tasks;long-term planning problem;recent deep reinforcement learning;future data dependence;making decision;current task;learned offloading knowledge;continually online learning;vehicular service;KD service offloading decision;greedy offloading decision algorithm;edge computing node;knowledge driven service offloading decision framework","","5","37","","","","","IEEE","IEEE Journals"
"Real-Time Deep Learning at the Edge for Scalable Reliability Modeling of Si-MOSFET Power Electronics Converters","M. Baharani; M. Biglarbegian; B. Parkhideh; H. Tabkhi","Electrical and Computer Engineering Department, Energy Production and Infrastructure Center, University of North Carolina at Charlotte, Charlotte, NC, USA; Electrical and Computer Engineering Department, Energy Production and Infrastructure Center, University of North Carolina at Charlotte, Charlotte, NC, USA; Electrical and Computer Engineering Department, Energy Production and Infrastructure Center, University of North Carolina at Charlotte, Charlotte, NC, USA; Electrical and Computer Engineering Department, Energy Production and Infrastructure Center, University of North Carolina at Charlotte, Charlotte, NC, USA","IEEE Internet of Things Journal","","2019","6","5","7375","7385","With the significant growth of advanced high-frequency power converters, online monitoring and active reliability assessment of power electronic devices are extremely crucial. This paper presents a transformative approach, named deep learning reliability awareness of converters at the edge (Deep RACE), for real-time reliability modeling and prediction of high-frequency MOSFET power electronic converters. Deep RACE offers a holistic solution which comprises algorithm advances, and full system integration (from the cloud down to the edge node) to create a near real-time reliability awareness. On the algorithm side, this paper proposes a deep learning algorithmic solution based on stacked long short-term memory for collective reliability training and inference across collective MOSFET converters based on device resistance changes. Deep RACE also proposes an integrative edge-to-cloud solution to offer a scalable decentralized devices-specific reliability monitoring, awareness, and modeling. The MOSFET convertors are Internet-of-Things (IoT) devices which have been empowered with edge real-time deep learning processing capabilities. The proposed Deep RACE solution has been prototyped and implemented through learning from MOSFET data set provided by NASA. Our experimental results show an average miss prediction of 8.9% over five different devices which is a much higher accuracy compared to well-known classical approaches (Kalman filter and particle filter). Deep RACE only requires 26-ms processing time and 1.87-W computing power on edge IoT device.","","","10.1109/JIOT.2019.2896174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629973","Deep learning;high-frequency power converter;long short-term memory (LSTM);MOSFET;reliability modeling","Reliability;Real-time systems;Deep learning;MOSFET;Power electronics;Predictive models;Degradation","electronic engineering computing;Internet of Things;Kalman filters;MOSFET;neural nets;power aware computing;power convertors;power electronics;real-time systems;reliability","scalable decentralized devices-specific reliability monitoring;MOSFET convertors;Internet-of-Things devices;real-time deep learning processing capabilities;Deep RACE solution;MOSFET data;1.87-W computing power;edge IoT device;scalable reliability modeling;advanced high-frequency power converters;active reliability assessment;power electronic devices;real-time reliability modeling;high-frequency MOSFET power electronic converters;edge node;real-time reliability awareness;deep learning algorithmic solution;collective reliability training;collective MOSFET converters;device resistance changes;edge-to-cloud solution;Si-MOSFET power electronics converters;deep learning reliability awareness;Kalman filter;particle filter","","1","36","","","","","IEEE","IEEE Journals"
"Learning With Annotation of Various Degrees","J. T. Zhou; M. Fang; H. Zhang; C. Gong; X. Peng; Z. Cao; R. S. M. Goh","Institute of High Performance Computing, A*STAR, Singapore; AI Lab, Shenzhen, China; Artificial Intelligence Initiative, A*STAR, Singapore; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; College of Computer Science, Sichuan University, Chengdu, China; School of Automation, Huazhong University of Science and Technology, Wuhan, China; Institute of High Performance Computing, A*STAR, Singapore","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","9","2794","2804","In this paper, we study a new problem in the scenario of sequences labeling. To be exact, we consider that the training data are with annotation of various degrees, namely, fully labeled, unlabeled, and partially labeled sequences. The learning with fully un/labeled sequence refers to the standard setting in traditional un/supervised learning, and the proposed partially labeling specifies the subject that the element does not belong to. The partially labeled data are cheaper to obtain compared with the fully labeled data though it is less informative, especially when the tasks require a lot of domain knowledge. To solve such a practical challenge, we propose a novel deep conditional random field (CRF) model which utilizes an end-to-end learning manner to smoothly handle fully/un/partially labeled sequences within a unified framework. To the best of our knowledge, this could be one of the first works to utilize the partially labeled instance for sequence labeling, and the proposed algorithm unifies the deep learning and CRF in an end-to-end framework. Extensive experiments show that our method achieves state-of-the-art performance in two sequence labeling tasks on some popular data sets.","","","10.1109/TNNLS.2018.2885854","Singapore Government’s Research, Innovation and Enterprise 2020 Plan (Advanced Manufacturing and Engineering Domain) through programmatic; National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; NSF of Jiangsu Province; Summit of the Six Top Talents Program; Lift Program for Young Talents of Jiangsu Province; CAST Lift Program for Young Talents; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611308","Deep conditional random field (CRF);incomplete annotation;partially labeled data;sequence labeling","Labeling;Supervised learning;Hidden Markov models;Training data;Semisupervised learning;Neural networks","data handling;statistical analysis;supervised learning;unsupervised learning","partially labeled data;fully labeled data;deep conditional random field model;end-to-end learning manner;partially labeled instance;deep learning;training data;partially labeling;sequence labeling;unsupervised learning;supervised learning;deep CRF model;various degree annotation","","5","61","","","","","IEEE","IEEE Journals"
"Imbalanced Deep Learning by Minority Class Incremental Rectification","Q. Dong; S. Gong; X. Zhu","Queen Mary University of London, London, United Kingdom; Queen Mary University of London, London, United Kingdom; Vision Semantics Ltd., London, United Kingdom","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","6","1367","1381","Model learning from class imbalanced training data is a long-standing and significant challenge for machine learning. In particular, existing deep learning methods consider mostly either class balanced data or moderately imbalanced data in model training, and ignore the challenge of learning from significantly imbalanced training data. To address this problem, we formulate a class imbalanced deep learning model based on batch-wise incremental minority (sparsely sampled) class rectification by hard sample mining in majority (frequently sampled) classes during model training. This model is designed to minimise the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes in an iterative batch-wise learning process. To that end, we introduce a Class Rectification Loss (CRL) function that can be deployed readily in deep network architectures. Extensive experimental evaluations are conducted on three imbalanced person attribute benchmark datasets (CelebA, X-Domain, DeepFashion) and one balanced object category benchmark dataset (CIFAR-100). These experimental results demonstrate the performance advantages and model scalability of the proposed batch-wise incremental minority class rectification model over the existing state-of-the-art models for addressing the problem of imbalanced data learning.","","","10.1109/TPAMI.2018.2832629","China Scholarship Council; Society Newton Advanced Fellowship Programme; Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8353718","Class imbalanced deep learning;multi-label learning;inter-class boundary rectification;hard sample mining;facial attribute recognition;clothing attribute recognition;person attribute recognition","Training data;Data models;Machine learning;Data mining;Training;Computational modeling;Benchmark testing","clothing;data mining;face recognition;iterative methods;learning (artificial intelligence);neural net architecture;pattern classification;sparse matrices","class balanced data;model training;class imbalanced deep learning;hard sample mining;majority classes;sparsely sampled boundaries;iterative batch-wise learning process;deep network architectures;class rectification loss function;minority class incremental rectification;machine learning;class imbalanced training data;imbalanced data learning;batch-wise incremental minority class rectification model;model scalability","","5","83","","","","","IEEE","IEEE Journals"
"Object Detection With Deep Learning: A Review","Z. Zhao; P. Zheng; S. Xu; X. Wu","College of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; College of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; College of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","11","3212","3232","Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.","","","10.1109/TNNLS.2018.2876865","National Natural Science Foundation of China; National Key Research and Development Program of China; Anhui Natural Science Funds for Distinguished Young Scholar; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8627998","Deep learning;neural network;object detection","Object detection;Deep learning;Task analysis;Feature extraction;Computer architecture;Training;Neural networks","convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);neural net architecture;object detection","salient object detection;face detection;pedestrian detection;neural network;video analysis;image understanding;image features;network architecture;deep learning-based object detection;scene classifiers;convolutional neural network","","18","230","","","","","IEEE","IEEE Journals"
"Arterial Spin Labeling Images Synthesis From sMRI Using Unbalanced Deep Discriminant Learning","W. Huang; M. Luo; X. Liu; P. Zhang; H. Ding; W. Xue; D. Ni","School of Information Engineering, Nanchang University, Nanchang, China; School of Information Engineering, Nanchang University, Nanchang, China; School of Information Engineering, Nanchang University, Nanchang, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Health Science Center, School of Biomedical Engineering, Shenzhen University, Shenzhen, China; Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Health Science Center, School of Biomedical Engineering, Shenzhen University, Shenzhen, China; Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Health Science Center, School of Biomedical Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Medical Imaging","","2019","38","10","2338","2351","Adequate medical images are often indispensable in contemporary deep learning-based medical imaging studies, although the acquisition of certain image modalities may be limited due to several issues including high costs and patients issues. However, thanks to recent advances in deep learning techniques, the above tough problem can be substantially alleviated by medical images synthesis, by which various modalities including T1/T2/DTI MRI images, PET images, cardiac ultrasound images, retinal images, and so on, have already been synthesized. Unfortunately, the arterial spin labeling (ASL) image, which is an important fMRI indicator in dementia diseases diagnosis nowadays, has never been comprehensively investigated for the synthesis purpose yet. In this paper, ASL images have been successfully synthesized from structural magnetic resonance images for the first time. Technically, a novel unbalanced deep discriminant learning-based model equipped with new ResNet sub-structures is proposed to realize the synthesis of ASL images from structural magnetic resonance images. The extensive experiments have been conducted. Comprehensive statistical analyses reveal that: 1) this newly introduced model is capable to synthesize ASL images that are similar towards real ones acquired by actual scanning; 2) synthesized ASL images obtained by the new model have demonstrated outstanding performance when undergoing rigorous tests of region-based and voxel-based corrections of partial volume effects, which are essential in ASL images processing; and 3) it is also promising that the diagnosis performance of dementia diseases can be significantly improved with the help of synthesized ASL images obtained by the new model, based on a multi-modal MRI dataset containing 355 demented patients in this paper.","","","10.1109/TMI.2019.2906677","National Natural Science Foundation of China; Natural Science Foundation of Jiangxi Province; Natural Science Basic Research Plan in Shaanxi Province of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672606","Magnetic resonance imaging (MRI);brain;machine learning","Deep learning;Dementia;Biomedical imaging;Medical diagnosis;Magnetic resonance imaging","biodiffusion;biomedical MRI;blood vessels;diseases;image segmentation;learning (artificial intelligence);medical image processing;neurophysiology","multimodal MRI dataset;dementia diseases;statistical analyses;ResNet sub-structures;ASL image processing;synthesized ASL images;deep discriminant learning-based model;structural magnetic resonance images;arterial spin labeling image;retinal images;cardiac ultrasound images;PET images;medical image synthesis;deep learning techniques;image modalities;contemporary deep learning-based medical imaging studies;adequate medical images;sMRI using unbalanced deep discriminant learning;arterial spin labeling images","","1","45","","","","","IEEE","IEEE Journals"
"Online Deep Reinforcement Learning for Computation Offloading in Blockchain-Empowered Mobile Edge Computing","X. Qiu; L. Liu; W. Chen; Z. Hong; Z. Zheng","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Vehicular Technology","","2019","68","8","8050","8062","Offloading computation-intensive tasks (e.g., blockchain consensus processes and data processing tasks) to the edge/cloud is a promising solution for blockchain-empowered mobile edge computing. However, the traditional offloading approaches (e.g., auction-based and game-theory approaches) fail to adjust the policy according to the changing environment and cannot achieve long-term performance. Moreover, the existing deep reinforcement learning-based offloading approaches suffer from the slow convergence caused by high-dimensional action space. In this paper, we propose a new model-free deep reinforcement learning-based online computation offloading approach for blockchain-empowered mobile edge computing in which both mining tasks and data processing tasks are considered. First, we formulate the online offloading problem as a Markov decision process by considering both the blockchain mining tasks and data processing tasks. Then, to maximize long-term offloading performance, we leverage deep reinforcement learning to accommodate highly dynamic environments and address the computational complexity. Furthermore, we introduce an adaptive genetic algorithm into the exploration of deep reinforcement learning to effectively avoid useless exploration and speed up the convergence without reducing performance. Finally, our experimental results demonstrate that our algorithm can converge quickly and outperform three benchmark policies.","","","10.1109/TVT.2019.2924015","National Key Research and Development Plan; National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; Program for Guangdong Introducing Innovative and Entrepreneurial Teams; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8742672","Online computation offloading;blockchain;mobile edge computing;deep reinforcement learning","Task analysis;Blockchain;Reinforcement learning;Data processing;Servers;Convergence;Edge computing","cloud computing;convergence;cryptography;data mining;decision theory;distributed databases;genetic algorithms;learning (artificial intelligence);Markov processes;mobile computing","online deep reinforcement learning;blockchain-empowered mobile edge computing;blockchain consensus processes;data processing tasks;online offloading problem;blockchain mining tasks;long-term offloading performance;computational complexity;deep reinforcement learning;model-free deep reinforcement learning-based online computation offloading approach;deep reinforcement learning-based offloading approach;cloud computing;high-dimensional action space;Markov decision process;highly dynamic environments;adaptive genetic algorithm;convergence","","2","44","Traditional","","","","IEEE","IEEE Journals"
"Comparing Task Simplifications to Learn Closed-Loop Object Picking Using Deep Reinforcement Learning","M. Breyer; F. Furrer; T. Novkovic; R. Siegwart; J. Nieto","Autonomous Systems Lab, ETH, Zurich, Switzerland; Autonomous Systems Lab, ETH, Zurich, Switzerland; Autonomous Systems Lab, ETH, Zurich, Switzerland; Autonomous Systems Lab, ETH, Zurich, Switzerland; Autonomous Systems Lab, ETH, Zurich, Switzerland","IEEE Robotics and Automation Letters","","2019","4","2","1549","1556","Enabling autonomous robots to interact in unstructured environments with dynamic objects requires manipulation capabilities that can deal with clutter, changes, and objects' variability. This letter presents a comparison of different reinforcement learning-based approaches for object picking with a robotic manipulator. We learn closed-loop policies mapping depth camera inputs to motion commands and compare different approaches to keep the problem tractable, including reward shaping, curriculum learning, and using a policy pre-trained on a task with a reduced action set to warm-start the full problem. For efficient and more flexible data collection, we train in simulation and transfer the policies to a real robot. We show that using curriculum learning, policies learned with a sparse reward formulation can be trained at similar rates as with a shaped reward. These policies result in success rates comparable to the policy initialized on the simplified task. We could successfully transfer these policies to the real robot with only minor modifications of the depth image filtering. We found that using a heuristic to warm-start the training was useful to enforce desired behavior, while the policies trained from scratch using a curriculum learned better to cope with unseen scenarios where objects are removed.","","","10.1109/LRA.2019.2896467","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; Luxembourg National Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630008","Grasping;visual servoing;deep reinforcement learning;curriculum learning","Task analysis;Training;Robots;Grippers;Cameras;Data models;Reinforcement learning","closed loop systems;image filtering;learning (artificial intelligence);manipulators;motion control;robot programming;robot vision","task simplifications;object picking;deep reinforcement learning;manipulation capabilities;robotic manipulator;closed-loop policies;reward shaping;curriculum learning;autonomous robots;reinforcement learning;data collection;depth camera inputs;depth image filtering","","","46","","","","","IEEE","IEEE Journals"
"Distributed Deep Learning at the Edge: A Novel Proactive and Cooperative Caching Framework for Mobile Edge Networks","Y. M. Saputra; D. T. Hoang; D. N. Nguyen; E. Dutkiewicz; D. Niyato; D. I. Kim","School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Wireless Communications Letters","","2019","8","4","1220","1223","We propose two novel proactive cooperative caching approaches using deep learning (DL) to predict users' content demand in a mobile edge caching network. In the first approach, a content server (CS) takes responsibilities to collect information from all mobile edge nodes (MENs) in the network and then performs the proposed DL algorithm to predict the content demand for the whole network. However, such a centralized approach may disclose the private information because MENs have to share their local users' data with the CS. Thus, in the second approach, we propose a novel distributed deep learning (DDL)-based framework. The DDL allows MENs in the network to collaborate and exchange information to reduce the error of content demand prediction without revealing the private information of mobile users. Through simulation results, we show that our proposed approaches can enhance the accuracy by reducing the root mean squared error (RMSE) up to 33.7% and reduce the service delay by 47.4% compared with other machine learning algorithms.","","","10.1109/LWC.2019.2912365","National Research Foundation of Korea; National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693954","Mobile edge caching;distributed deep learning;proactive and cooperative caching","Cooperative caching;Deep learning;Computational modeling;Prediction algorithms;Predictive models;Servers;Delays","cooperative communication;learning (artificial intelligence);mean square error methods;mobile communication","cooperative caching framework;mobile edge networks;proactive cooperative caching approaches;mobile edge caching network;content server;mobile edge nodes;private information;deep learning-based framework;DDL;content demand prediction;mobile users;machine learning algorithms;distributed deep learning","","","11","","","","","IEEE","IEEE Journals"
"Recognizing Global Reservoirs From Landsat 8 Images: A Deep Learning Approach","W. Fang; C. Wang; X. Chen; W. Wan; H. Li; S. Zhu; Y. Fang; B. Liu; Y. Hong","Institute of Remote Sensing and GIS, School of Earth and Space Sciences, Peking University, Beijing, China; Department of Hydraulic Engineering, Tsinghua University, Beijing, China; Institute of Remote Sensing and GIS, School of Earth and Space Sciences, Peking University, Beijing, China; Institute of Remote Sensing and GIS, School of Earth and Space Sciences, Peking University, Beijing, China; Institute of Remote Sensing and GIS, School of Earth and Space Sciences, Peking University, Beijing, China; Department of Hydraulic Engineering, Tsinghua University, Beijing, China; Department of Hydraulic Engineering, Tsinghua University, Beijing, China; Institute of Remote Sensing and GIS, School of Earth and Space Sciences, Peking University, Beijing, China; Institute of Remote Sensing and GIS, School of Earth and Space Sciences, Peking University, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","9","3168","3177","Man-made reservoirs are key components of terrestrial hydrological systems. Identifying the location and number of reservoirs is the premise for studying the impact of human activities on water resources and environmental changes. While complete bottom-up censuses can provide a comprehensive view of the reservoir landscape, they are time-consuming and laborious and are thus infeasible on a global scale. Moreover, it is challenging to distinguish man-made reservoirs from natural lakes in remote sensing images. This study proposes a convolutional neural network (CNN)-based framework to recognize global reservoirs from Landsat 8 imageries. On the basis of the HydroLAKES dataset, a Landsat 8 cloud-free mosaic of 2017 was clipped for each feature (reservoir or lake) and was resized into 224 × 224 patches, which were collected as training and testing samples. Compared to other deep learning methods (Alexnet and VGG) and state-of-the-art traditional machine learning methods (support vector machine, random forest, gradient boosting, and bag-of-visual-words), we found that fine-tuning the pretrained CNN model, ResNet-50, could reach the highest accuracy (91.45%). Application cases in Kansas (USA, North America), Mpumalanga (South Africa, Africa), and Kostanay (Kazakhstan, Asia) resulted in classification accuracies of better than 99%, which showed the applicability of the proposed ResNet-50 model to the extraction of reservoirs from a vast amount of moderate resolution images. The framework that was developed in this paper is the first attempt to combine remote sensing big data and the deep learning technique to the recognition of reservoirs at a global scale.","","","10.1109/JSTARS.2019.2929601","Key R&D Program of Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8804226","Convolutional neural network (CNN);deep learning;Landsat;object recognition;reservoir","Reservoirs;Remote sensing;Earth;Lakes;Training;Artificial satellites;Deep learning","geophysical image processing;hydrological techniques;image classification;image recognition;image segmentation;lakes;learning (artificial intelligence);neural nets;remote sensing;support vector machines;water resources","Landsat 8 cloud-free mosaic;pretrained CNN model;moderate resolution images;deep learning technique;global scale;global reservoirs;Landsat 8 images;deep learning approach;man-made reservoirs;terrestrial hydrological systems;human activities;water resources;environmental changes;reservoir landscape;natural lakes;remote sensing images;convolutional neural network-based framework;Landsat 8 imageries;traditional machine learning methods;ResNet-50 model;reservoir extraction","","","60","Traditional","","","","IEEE","IEEE Journals"
"iRAF: A Deep Reinforcement Learning Approach for Collaborative Mobile Edge Computing IoT Networks","J. Chen; S. Chen; Q. Wang; B. Cao; G. Feng; J. Hu","National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, China; Institute of Network Technology, Beijing University of Posts and Telecommunications, Beijing; National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, China","IEEE Internet of Things Journal","","2019","6","4","7011","7024","Recently, as the development of artificial intelligence (AI), data-driven AI methods have shown amazing performance in solving complex problems to support the Internet of Things (IoT) world with massive resource-consuming and delay-sensitive services. In this paper, we propose an intelligent resource allocation framework (iRAF) to solve the complex resource allocation problem for the collaborative mobile edge computing (CoMEC) network. The core of iRAF is a multitask deep reinforcement learning algorithm for making resource allocation decisions based on network states and task characteristics, such as the computing capability of edge servers and devices, communication channel quality, resource utilization, and latency requirement of the services, etc. The proposed iRAF can automatically learn the network environment and generate resource allocation decision to maximize the performance over latency and power consumption with self-play training. iRAF becomes its own teacher: a deep neural network (DNN) is trained to predict iRAF's resource allocation action in a self-supervised learning manner, where the training data is generated from the searching process of Monte Carlo tree search (MCTS) algorithm. A major advantage of MCTS is that it will simulate trajectories into the future, starting from a root state, to obtain a best action by evaluating the reward value. Numerical results show that our proposed iRAF achieves 59.27% and 51.71% improvement on service latency performance compared with the greedy-search and the deep Q-learning-based methods, respectively.","","","10.1109/JIOT.2019.2913162","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698845","Collaborative mobile edge computing (CoMEC);deep reinforcement learning (DRL);intelligent resource allocation framework (iRAF);Internet of Things (IoT);Monte Carlo tree search (MCTS)","Servers;Resource management;Task analysis;Internet of Things;Collaboration;Delays;Reinforcement learning","groupware;Internet of Things;learning (artificial intelligence);mobile computing;Monte Carlo methods;neural nets;resource allocation;tree searching","collaborative mobile edge computing IoT networks;artificial intelligence;data-driven AI methods;massive resource-consuming;delay-sensitive services;intelligent resource allocation framework;complex resource allocation problem;collaborative mobile edge computing network;multitask deep reinforcement learning algorithm;resource allocation decision;computing capability;edge servers;resource utilization;network environment;deep neural network;iRAF's resource allocation action;self-supervised learning manner;Monte Carlo tree search algorithm;service latency performance;Q-learning-based methods;deep reinforcement learning;multitask deep reinforcement learning","","","44","","","","","IEEE","IEEE Journals"
"Deep Learning: Current and Emerging Applications in Medicine and Technology","A. Akay; H. Hess","Department of Biomedical Engineering, Columbia University, New York, NY, USA; Department of Biomedical Engineering, Columbia University, New York, NY, USA","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","906","920","Machine learning is enabling researchers to analyze and understand increasingly complex physical and biological phenomena in traditional fields such as biology, medicine, and engineering and emerging fields like synthetic biology, automated chemical synthesis, and biomanufacturing. These fields require new paradigms toward understanding increasingly complex data and converting such data into medical products and services for patients. The move toward deep learning and complex modeling is an attempt to bridge the gap between acquiring massive quantities of complex data, and converting such data into practical insights. Here, we provide an overview of the field of machine learning, its current applications and needs in traditional and emerging fields, and discuss an illustrative attempt at using deep learning to understand swarm behavior of molecular shuttles.","","","10.1109/JBHI.2019.2894713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624419","Deep learning;machine learning;neural networks;informatics","Drugs;Task analysis;Deep learning;Computer architecture;Biology","bioinformatics;data handling;learning (artificial intelligence);medical computing;molecular biophysics","medicine;machine learning;biological phenomena;engineering;synthetic biology;automated chemical synthesis;complex data;medical products;services;deep learning;complex modeling;emerging applications;complex physical phenomena;biomanufacturing;swarm behavior;molecular shuttles","","3","224","","","","","IEEE","IEEE Journals"
"Deep Transfer Learning Based on Sparse Autoencoder for Remaining Useful Life Prediction of Tool in Manufacturing","C. Sun; M. Ma; Z. Zhao; S. Tian; R. Yan; X. Chen","School of Mechanical Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; Department of Mechanical Engineering, University of Massachusetts Lowell, MA, USA; School of Mechanical Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; School of Mechanical Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; School of Mechanical Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; School of Mechanical Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China","IEEE Transactions on Industrial Informatics","","2019","15","4","2416","2425","Deep learning with ability to feature learning and nonlinear function approximation has shown its effectiveness for machine fault prediction. While, how to transfer a deep network trained by historical failure data for prediction of a new object is rarely researched. In this paper, a deep transfer learning (DTL) network based on sparse autoencoder (SAE) is presented. In the DTL method, three transfer strategies, that is, weight transfer, transfer learning of hidden feature, and weight update, are used to transfer an SAE trained by historical failure data to a new object. By these strategies, prediction of the new object without supervised information for training is achieved. Moreover, the learned features by deep transfer network for the new object share joint and similar characteristic to that of historical failure data, which is beneficial to accurate prediction. Case study on remaining useful life (RUL) prediction of cutting tool is performed to validate effectiveness of the DTL method. An SAE network is first trained by run-to-failure data with RUL information of a cutting tool in an off-line process. The trained network is then transferred to a new tool under operation for on-line RUL prediction. The prediction result with high accuracy shows advantage of the DTL method for RUL prediction.","","","10.1109/TII.2018.2881543","National Natural Science Foundation of China; Absorb Outcome Transformation Project of Science and Technology Department in Shaanxi Province of China; Postdoctoral Science Foundation of China; National Key Research Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540073","Deep learning;deep transfer learning (DTL);remaining useful life (RUL) prediction;sparse autoencoder (SAE);tool;transfer learning","Tools;Feature extraction;Hidden Markov models;Monitoring;Predictive models;Fault diagnosis","condition monitoring;cutting tools;fault diagnosis;function approximation;learning (artificial intelligence);manufacturing processes;neural nets;production engineering computing;remaining life assessment","weight update;cutting tool;DTL method;SAE network;run-to-failure data;sparse autoencoder;learning function approximation;nonlinear function approximation;machine fault prediction;deep transfer learning network;weight transfer;remaining useful life prediction;RUL prediction;transfer learning of hidden feature;manufacturing tool","","6","52","","","","","IEEE","IEEE Journals"
"Deep Learning for Distributed Optimization: Applications to Wireless Resource Management","H. Lee; S. H. Lee; T. Q. S. Quek","Department of Information and Communications Engineering, Pukyong National University, Busan, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore","IEEE Journal on Selected Areas in Communications","","2019","37","10","2251","2266","This paper studies a deep learning (DL) framework to solve distributed non-convex constrained optimizations in wireless networks where multiple computing nodes, interconnected via backhaul links, desire to determine an efficient assignment of their states based on local observations. Two different configurations are considered: First, an infinite-capacity backhaul enables nodes to communicate in a lossless way, thereby obtaining the solution by centralized computations. Second, a practical finite-capacity backhaul leads to the deployment of distributed solvers equipped along with quantizers for communication through capacity-limited backhaul. The distributed nature and the non-convexity of the optimizations render the identification of the solution unwieldy. To handle them, deep neural networks (DNNs) are introduced to approximate an unknown computation for the solution accurately. In consequence, the original problems are transformed to training tasks of the DNNs subject to non-convex constraints where existing DL libraries fail to extend straightforwardly. A constrained training strategy is developed based on the primal-dual method. For distributed implementation, a novel binarization technique at the output layer is developed for quantization at each node. Our proposed distributed DL framework is examined in various network configurations of wireless resource management. Numerical results verify the effectiveness of our proposed approach over existing optimization techniques.","","","10.1109/JSAC.2019.2933890","National Research Foundation of Korea; Institute of Information & Communications Technology Planning & Evaluation (IITP) Grant funded by the Korea Government (MSIT) [High Accurate Positioning Enabled MIMO Transmission and Network Technologies for Next 5G-V2X (vehicle-to-everything) Services]; SUTD-ZJU Research Collaboration; SUTD AI Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792179","Deep neural network;distributed deep learning;primal-dual method;wireless resource management","Training;Optimization;Wireless networks;Resource management;Power control;Deep learning","cellular radio;concave programming;convex programming;learning (artificial intelligence);mobility management (mobile radio);neural nets;telecommunication computing","distributed optimization;wireless resource management;deep learning framework;nonconvex constrained optimizations;wireless networks;multiple computing nodes;backhaul links;local observations;infinite-capacity backhaul;centralized computations;practical finite-capacity backhaul;distributed solvers;capacity-limited backhaul;deep neural networks;training tasks;DNNs subject;nonconvex constraints;constrained training strategy;distributed implementation;quantization;distributed DL framework;network configurations;optimization techniques;unknown computation","","1","40","","","","","IEEE","IEEE Journals"
"Deep Learning-Based Multiple Object Visual Tracking on Embedded System for IoT and Mobile Edge Computing Applications","B. Blanco-Filgueira; D. García-Lesta; M. Fernández-Sanjurjo; V. M. Brea; P. López","Centro Singular de Investigación en Tecnoloxías da Información, Universidade de Santiago de Compostela, Santiago de Compostela, Spain; Centro Singular de Investigación en Tecnoloxías da Información, Universidade de Santiago de Compostela, Santiago de Compostela, Spain; Centro Singular de Investigación en Tecnoloxías da Información, Universidade de Santiago de Compostela, Santiago de Compostela, Spain; Centro Singular de Investigación en Tecnoloxías da Información, Universidade de Santiago de Compostela, Santiago de Compostela, Spain; Centro Singular de Investigación en Tecnoloxías da Información, Universidade de Santiago de Compostela, Santiago de Compostela, Spain","IEEE Internet of Things Journal","","2019","6","3","5423","5431","Compute and memory demands of state-of-the-art deep learning methods are still a shortcoming that must be addressed to make them useful at Internet of Things (IoT) end-nodes. In particular, recent results depict a hopeful prospect for image processing using convolutional neural networks, CNNs, but the gap between software and hardware implementations is already considerable for IoT and mobile edge computing applications due to their high power consumption. This proposal performs low-power and real time deep learning-based multiple object visual tracking implemented on an NVIDIA Jetson TX2 development kit. It includes a camera and wireless connection capability and it is battery powered for mobile and outdoor applications. A collection of representative sequences captured with the on-board camera, dETRUSC video dataset, is used to exemplify the performance of the proposed algorithm and to facilitate benchmarking. The results in terms of power consumption and frame rate demonstrate the feasibility of deep learning algorithms on embedded platforms although more effort in the joint algorithm and hardware design of CNNs is needed.","","","10.1109/JIOT.2019.2902141","Spanish Government project MICINN (FEDER); Consellería de Cultura, Educación e Ordenación Universitaria, Xunta de Galicia; European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653851","Deep learning;edge computing;foreground segmentation;Internet of Things (IoT) node;visual tracking","Graphics processing units;Hardware;Object tracking;Deep learning;Internet of Things;Real-time systems;Cameras","convolutional neural nets;embedded systems;image capture;image sequences;Internet of Things;learning (artificial intelligence);object detection;object tracking","real-time deep learning-based multiple object visual tracking;software implementations;Internet of Things end-nodes;deep learning algorithms;frame rate;outdoor applications;mobile applications;wireless connection capability;camera;NVIDIA Jetson TX2 development kit;hardware implementations;convolutional neural networks;memory demands;mobile edge computing applications;IoT;embedded system","","2","50","","","","","IEEE","IEEE Journals"
"Self Paced Deep Learning for Weakly Supervised Object Detection","E. Sangineto; M. Nabi; D. Culibrk; N. Sebe","Department of Information Engineering and Computer Science (DISI), University of Trento, Trento, TN, Italy; Department of Information Engineering and Computer Science (DISI), University of Trento, Trento, TN, Italy; Department of Industrial Engineering and Management, University of Novi Sad, Novi Sad, Serbia; Department of Information Engineering and Computer Science (DISI), University of Trento, Trento, TN, Italy","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","3","712","725","In a weakly-supervised scenario object detectors need to be trained using image-level annotation alone. Since bounding-box-level ground truth is not available, most of the solutions proposed so far are based on an iterative, Multiple Instance Learning framework in which the current classifier is used to select the highest-confidence boxes in each image, which are treated as pseudo-ground truth in the next training iteration. However, the errors of an immature classifier can make the process drift, usually introducing many of false positives in the training dataset. To alleviate this problem, we propose in this paper a training protocol based on the self-paced learning paradigm. The main idea is to iteratively select a subset of images and boxes that are the most reliable, and use them for training. While in the past few years similar strategies have been adopted for SVMs and other classifiers, we are the first showing that a self-paced approach can be used with deep-network-based classifiers in an end-to-end training pipeline. The method we propose is built on the fully-supervised Fast-RCNN architecture and can be applied to similar architectures which represent the input image as a bag of boxes. We show state-of-the-art results on Pascal VOC 2007, Pascal VOC 2010 and ILSVRC 2013. On ILSVRC 2013 our results based on a low-capacity AlexNet network outperform even those weakly-supervised approaches which are based on much higher-capacity networks.","","","10.1109/TPAMI.2018.2804907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8289350","Weakly supervised learning;object detection;self-paced learning;curriculum learning;deep learning;training protocol","Training;Protocols;Object detection;Reliability;Task analysis;Machine learning;Detectors","convolutional neural nets;image classification;learning (artificial intelligence);object detection;protocols","weakly supervised object detection;weakly-supervised scenario object detectors;image-level annotation;bounding-box-level ground truth;current classifier;highest-confidence boxes;immature classifier;process drift;false positives;training dataset;training protocol;self-paced approach;deep-network-based classifiers;end-to-end training pipeline;fully-supervised Fast-RCNN architecture;input image;weakly-supervised approaches;multiple instance learning framework;self paced deep learning","","6","45","","","","","IEEE","IEEE Journals"
"Deep-Learning-Based Millimeter-Wave Massive MIMO for Hybrid Precoding","H. Huang; Y. Song; J. Yang; G. Gui; F. Adachi","Key Laboratory of Broadband Wireless Communication and Sensor Network Technology, Nanjing University of Posts and Telecommunications, Ministry of Education, Nanjing, China; Key Laboratory of Broadband Wireless Communication and Sensor Network Technology, Nanjing University of Posts and Telecommunications, Ministry of Education, Nanjing, China; Key Laboratory of Broadband Wireless Communication and Sensor Network Technology, Nanjing University of Posts and Telecommunications, Ministry of Education, Nanjing, China; Key Laboratory of Broadband Wireless Communication and Sensor Network Technology, Nanjing University of Posts and Telecommunications, Ministry of Education, Nanjing, China; Research Organization of Electrical Communication, Tohoku University, Sendai, Japan","IEEE Transactions on Vehicular Technology","","2019","68","3","3027","3032","Millimeter wave (mmWave) massive multiple-input multiple-output (MIMO) has been regarded to be an emerging solution for the next generation of communications, in which hybrid analog and digital precoding is an important method for reducing the hardware complexity and energy consumption associated with mixed signal components. However, the fundamental limitations of the existing hybrid precoding schemes are that they have high-computational complexity and fail to fully exploit the spatial information. To overcome these limitations, this paper proposes a deep-learning-enabled mmWave massive MIMO framework for effective hybrid precoding, in which each selection of the precoders for obtaining the optimized decoder is regarded as a mapping relation in the deep neural network (DNN). Specifically, the hybrid precoder is selected through training based on the DNN for optimizing precoding process of the mmWave massive MIMO. Additionally, we present extensive simulation results to validate the excellent performance of the proposed scheme. The results exhibit that the DNN-based approach is capable of minimizing the bit error ratio and enhancing the spectrum efficiency of the mmWave massive MIMO, which achieves better performance in hybrid precoding compared with conventional schemes while substantially reducing the required computational complexity.","","","10.1109/TVT.2019.2893928","Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8618345","Millimeter wave (mmWave);massive multiple-input multiple-output (MIMO);deep learning;hybrid precoding","Precoding;MIMO communication;Deep learning;Radio frequency;Neural networks;Computational complexity","computational complexity;decoding;learning (artificial intelligence);MIMO communication;neural nets;precoding;telecommunication computing","hybrid precoder;precoding process;DNN-based approach;millimeter wave massive multiple-input multiple-output;high-computational complexity;deep-learning-enabled mmWave massive MIMO framework;deep neural network;spectrum efficiency;bit error ratio minimization;mapping relation;spatial information;energy consumption;mixed signal components;hardware complexity reduction;millimeter wave massive multiple-input multiple-output framework;deep-learning-based millimeter-wave massive MIMO framework;hybrid precoding schemes;hybrid analog-digital precoding","","70","25","","","","","IEEE","IEEE Journals"
"Deep Learning Architecture for Estimating Hourly Ground-Level PM2.5 Using Satellite Remote Sensing","Y. Sun; Q. Zeng; B. Geng; X. Lin; B. Sude; L. Chen","Chinese Research Academy of Environmental Sciences, Institute of Environmental Ecology, Beijing, China; College of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; Chinese Academy of Social Sciences, Institute for Urban and Environment Studies, Beijing, China; College of Geography and Environment Science, Zhejiang Normal University, Zhejiang, China; Chinese Research Academy of Environmental Sciences, Institute of Environmental Ecology, Beijing, China; Chinese Academy of Sciences, Aerospace Information Research Institute, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","9","1343","1347","The prediction of PM2.5 concentration is a canonical predictive challenge due to the distribution of $PM_{2.5}$ appears serious spatiotemporal variability at multiple scales. Currently, using satellite-based remote sensing data to estimate ground-level PM2.5 is a promising method for providing spatiotemporal continuous information of PM2.5. In this letter, we proposed a deep neural network (DNN)-based PM2.5 prediction model to capture the spatiotemporal variability of ground-level PM2.5 using the remote sensing aerosol optical depth (AOD) data from the Himawari-8 satellite along with the conventional meteorological observation variables (denoted as PM25-DNN). The PM25-DNN model was trained and tested using the data from Beijing-Tianjin-Hebei region of China in 2017, and we compared the prediction performance between the PM25-DNN and the current state-of-the-art methods in this field. The results show that the PM25-DNN outperforms the other models with the cross-validated coefficient of determination ($\text{R}^{2}$ ), root-mean-square error (RMSE), mean prediction error (MPE), and relative prediction error (RPE) were 0.84, $19.9~\mu \text{g}/\text{m}^{3}$ , $11.89~\mu \text{g}/\text{m}^{3}$ , and 41.21%, respectively. Then, the trained PM25-DNN model was applied to estimate the hourly gridded PM2.5 with 1-km spatial resolution. Our results indicate that the DNN architecture can capture the essential spatiotemporal distribution associated with PM2.5 only using AOD data and conventional meteorological observational variables without more handcrafted features. The proposed PM25-DNN model can greatly improve the accuracy of PM2.5 estimation, and it provides a new perspective for PM2.5 monitoring using end-to-end deep learning method.","","","10.1109/LGRS.2019.2900270","National Basic Research Program of China (973 Program); China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8685685","Aerosol optical depth (AOD);deep learning;Himawari-8;PM ₂.₅;satellite remote sensing","Spatiotemporal phenomena;Atmospheric modeling;Monitoring;Predictive models;Data models;Deep learning;Biological system modeling","aerosols;atmospheric optics;atmospheric techniques;learning (artificial intelligence);neural nets;remote sensing","satellite remote sensing;canonical predictive challenge;satellite-based remote sensing data;spatiotemporal continuous information;deep neural network-based PM2.5 prediction model;remote sensing aerosol optical depth data;Himawari-8 satellite;conventional meteorological observation variables;prediction performance;relative prediction error;trained PM25-DNN model;DNN architecture;essential spatiotemporal distribution;conventional meteorological observational variables;end-to-end deep learning method;spatiotemporal variability;deep learning architecture;hourly ground-level PM2.5 ;Beijing-Tianjin-Hebei region;China;AD 2017;current state-of-the-art methods;cross-validated coefficient","","","19","","","","","IEEE","IEEE Journals"
"Regularizing Deep Neural Networks by Enhancing Diversity in Feature Extraction","B. O. Ayinde; T. Inanc; J. M. Zurada","Department of Electrical and Computer Engineering, University of Louisville, Louisville, KY, USA; Department of Electrical and Computer Engineering, University of Louisville, Louisville, KY, USA; Department of Electrical and Computer Engineering, University of Louisville, Louisville, KY, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","9","2650","2661","This paper proposes a new and efficient technique to regularize the neural network in the context of deep learning using correlations among features. Previous studies have shown that oversized deep neural network models tend to produce a lot of redundant features that are either the shifted version of one another or are very similar and show little or no variations, thus resulting in redundant filtering. We propose a way to address this problem and show that such redundancy can be avoided using regularization and adaptive feature dropout mechanism. We show that regularizing both negative and positive correlated features according to their differentiation and based on their relative cosine distances yields network extracting dissimilar features with less overfitting and better generalization. This concept is illustrated with deep multilayer perceptron, convolutional neural network, sparse autoencoder, gated recurrent unit, and long short-term memory on MNIST digits recognition, CIFAR-10, ImageNet, and Stanford Natural Language Inference data sets.","","","10.1109/TNNLS.2018.2885972","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603826","Cosine similarity;deep learning;feature clustering;feature correlation;redundancy elimination;regularization","Feature extraction;Training;Neural networks;Correlation;Redundancy;Decorrelation;Deep learning","convolutional neural nets;feature extraction;learning (artificial intelligence);multilayer perceptrons;recurrent neural nets","convolutional neural network;deep neural networks;feature extraction;deep learning;oversized deep neural network models;redundant features;redundant filtering;redundancy;adaptive feature dropout mechanism;positive correlated features;deep multilayer perceptron;negative correlated features;cosine distances;dissimilar features extraction;sparse autoencoder;gated recurrent unit;long short-term memory;MNIST digits recognition;CIFAR-10;ImageNet;Stanford natural language inference data set","","1","47","","","","","IEEE","IEEE Journals"
"Deep Multi-View Learning Using Neuron-Wise Correlation-Maximizing Regularizers","K. Jia; J. Lin; M. Tan; D. Tao","School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Information Technologies, The University of Sydney, NSW, Australia","IEEE Transactions on Image Processing","","2019","28","10","5121","5134","Many machine learning problems are concerned with discovering or associating common patterns in data of multiple views or modalities. Multi-view learning is one of the methods to achieve such goals. Recent methods propose deep multi-view networks via adaptation of generic deep neural networks (DNNs), which concatenate features of individual views at intermediate network layers (i.e., fusion layers). In this paper, we study the problem of multi-view learning in such end-to-end networks. We take a regularization approach via multi-view learning criteria, and propose a novel, effective, and efficient neuron-wise correlation-maximizing regularizer. We implement our proposed regularizers collectively as a correlation-regularized network layer (CorrReg). CorrReg can be applied to either fully-connected or convolutional fusion layers, simply by replacing them with their CorrReg counterparts. By partitioning neurons of a hidden layer in generic DNNs into multiple subsets, we also consider a multi-view feature learning perspective of generic DNNs. Such a perspective enables us to study deep multi-view learning in the context of regularized network training, for which we present control experiments of benchmark image classification to show the efficacy of our proposed CorrReg. To investigate how CorrReg is useful for practical multi-view learning problems, we conduct experiments of RGB-D object/scene recognition and multi-view-based 3D object recognition, using networks with fusion layers that concatenate intermediate features of individual modalities or views for subsequent classification. Applying CorrReg to fusion layers of these networks consistently improves classification performance. In particular, we achieve the new state of the art on the benchmark RGB-D object and RGB-D scene datasets. We make the implementation of CorrReg publicly available.","","","10.1109/TIP.2019.2912356","National Natural Science Foundation of China; Program for Guangdong Introducing Innovative and Entrepreneurial Teams; Guangdong Provincial Scientific and Technological Funds; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8708947","Multi-view learning;deep learning;regularization;normalization;canonical correlation analysis","Training;Neurons;Task analysis;Correlation;Benchmark testing;Object recognition;Feature extraction","feature extraction;image classification;image colour analysis;image fusion;learning (artificial intelligence);object recognition","deep multiview learning;regularized network training;CorrReg;practical multiview learning problems;multiview-based 3D object recognition;individual modalities;neuron-wise correlation-maximizing regularizers;machine learning problems;deep multiview networks;generic deep neural networks;intermediate network layers;end-to-end networks;multiview learning criteria;efficient neuron-wise correlation-maximizing regularizer;correlation-regularized network layer;convolutional fusion layers;generic DNN;RGB-D object recognition;RGB-D scene recognition","","","70","","","","","IEEE","IEEE Journals"
"Multi-Scale Deep Reinforcement Learning for Real-Time 3D-Landmark Detection in CT Scans","F. Ghesu; B. Georgescu; Y. Zheng; S. Grbic; A. Maier; J. Hornegger; D. Comaniciu","Siemens Healthineers, Medical Imaging Technologies, Princeton, NJ, USA; Siemens Healthineers, Medical Imaging Technologies, Princeton, NJ; Siemens Healthineers, Medical Imaging Technologies, Princeton, NJ; Siemens Healthineers, Medical Imaging Technologies, Princeton, NJ; Pattern Recognition Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Pattern Recognition Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Siemens Healthineers, Medical Imaging Technologies, Princeton, NJ","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","1","176","189","Robust and fast detection of anatomical structures is a prerequisite for both diagnostic and interventional medical image analysis. Current solutions for anatomy detection are typically based on machine learning techniques that exploit large annotated image databases in order to learn the appearance of the captured anatomy. These solutions are subject to several limitations, including the use of suboptimal feature engineering techniques and most importantly the use of computationally suboptimal search-schemes for anatomy detection. To address these issues, we propose a method that follows a new paradigm by reformulating the detection problem as a behavior learning task for an artificial agent. We couple the modeling of the anatomy appearance and the object search in a unified behavioral framework, using the capabilities of deep reinforcement learning and multi-scale image analysis. In other words, an artificial agent is trained not only to distinguish the target anatomical object from the rest of the body but also how to find the object by learning and following an optimal navigation path to the target object in the imaged volumetric space. We evaluated our approach on 1487 3D-CT volumes from 532 patients, totaling over 500,000 image slices and show that it significantly outperforms state-of-the-art solutions on detecting several anatomical structures with no failed cases from a clinical acceptance perspective, while also achieving a 20-30 percent higher detection accuracy. Most importantly, we improve the detection-speed of the reference methods by 2-3 orders of magnitude, achieving unmatched real-time performance on large 3D-CT scans.","","","10.1109/TPAMI.2017.2782687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8187667","Deep learning;deep reinforcement learning;medical image analysis;multi-scale;scale-space modeling;three-dimensional (3D) object detection;real-time detection;intelligent localization","Machine learning;Biomedical imaging;Search problems;Training;Three-dimensional displays;Real-time systems","computerised tomography;image registration;image segmentation;learning (artificial intelligence);medical image processing;visual databases","multiscale deep reinforcement learning;real-time 3D-landmark detection;anatomical structures;diagnostic image analysis;interventional medical image analysis;current solutions;anatomy detection;machine learning;exploit large annotated image databases;captured anatomy;suboptimal feature engineering techniques;computationally suboptimal search-schemes;detection problem;behavior learning task;artificial agent;anatomy appearance;object search;unified behavioral framework;multiscale image analysis;target anatomical object;target object;imaged volumetric space;3D-CT volumes;detection-speed;3D-CT scans","","3","64","","","","","IEEE","IEEE Journals"
"Data-Driven Deep Learning for Automatic Modulation Recognition in Cognitive Radios","Y. Wang; M. Liu; J. Yang; G. Gui","Key Laboratory of Ministry of Education in Broadband Wireless Communication and Sensor Network Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; Key Laboratory of Ministry of Education in Broadband Wireless Communication and Sensor Network Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; Key Laboratory of Ministry of Education in Broadband Wireless Communication and Sensor Network Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; Key Laboratory of Ministry of Education in Broadband Wireless Communication and Sensor Network Technology, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Transactions on Vehicular Technology","","2019","68","4","4074","4077","Automatic modulation recognition (AMR) is an essential and challenging topic in the development of the cognitive radio (CR), and it is a cornerstone of CR adaptive modulation and demodulation capabilities to sense and learn environments and make corresponding adjustments. AMR is essentially a classification problem, and deep learning achieves outstanding performances in various classification tasks. So, this paper proposes a deep learning-based method, combined with two convolutional neural networks (CNNs) trained on different datasets, to achieve higher accuracy AMR. A CNN is trained on samples composed of in-phase and quadrature component signals, otherwise known as in-phase and quadrature samples, to distinguish modulation modes, that are relatively easy to identify. We adopt dropout instead of pooling operation to achieve higher recognition accuracy. A CNN based on constellation diagrams is also designed to recognize modulation modes that are difficult to distinguish in the former CNN, such as 16 quadratic-amplitude modulation (QAM) and 64 QAM, demonstrating the ability to classify QAM signals even in scenarios with a low signal-to-noise ratio.","","","10.1109/TVT.2019.2900460","Priority Academic Program Development of Jiangsu Higher Education Institutions; Nanjing University of Posts and Telecommunications; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8645696","Automatic modulation recognition (AMR);cognitive radio (CR);deep learning;convolutional neural network (CNN);in-phase and quadrature (IQ) samples;constellation diagrams","Constellation diagram;Feature extraction;Task analysis;Training;Deep learning;Signal to noise ratio","adaptive modulation;cognitive radio;convolutional neural nets;learning (artificial intelligence);quadrature amplitude modulation;signal classification;telecommunication computing","data-driven deep learning;automatic modulation recognition;cognitive radio;classification tasks;deep learning-based method;convolutional neural networks;higher accuracy AMR;CNN;quadrature samples;modulation modes;higher recognition accuracy;quadratic-amplitude modulation;classification problem;CR adaptive demodulation capability;CR adaptive modulation capability;in-phase component signal;quadrature component signal;constellation diagrams;low signal-to-noise ratio","","70","13","","","","","IEEE","IEEE Journals"
"Planning Approximate Exploration Trajectories for Model-Free Reinforcement Learning in Contact-Rich Manipulation","S. Hoppe; Z. Lou; D. Hennes; M. Toussaint","Bosch Corporate Research, Stuttgart, Germany; Bosch Corporate Research, Stuttgart, Germany; Machine Learning and Robotics Lab, University of Stuttgart, Stuttgart, Germany; Machine Learning and Robotics Lab, University of Stuttgart, Stuttgart, Germany","IEEE Robotics and Automation Letters","","2019","4","4","4042","4047","Recent progress in deep reinforcement learning has enabled simulated agents to learn complex behavior policies from scratch, but their data complexity often prohibits real-world applications. The learning process can be sped up by expert demonstrations but those can be costly to acquire. We demonstrate that it is possible to employ model-free deep reinforcement learning combined with planning to quickly generate informative data for a manipulation task. In particular, we use an approximate trajectory optimization approach for global exploration based on an upper confidence bound of the advantage function. The advantage is approximated by a network for Q-learning with separately updated streams for state value and advantage that allows ensembles to approximate model uncertainty for one stream only. We evaluate our method on new extensions to the classical peg-in-hole task, one of which is only solvable by active usage of contacts between peg tips and holes. The experimental evaluation suggests that our method explores more relevant areas of the environment and finds exemplar solutions faster-both on a real robot and in simulation. Combining our exploration with learning from demonstration outperforms state-of-the-art model-free reinforcement learning in terms of convergence speed for contact-rich manipulation tasks.","","","10.1109/LRA.2019.2928212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8760436","Deep learning in robotics and automation;dexterous manipulation;learning and adaptive systems","Reinforcement learning;Task analysis;Robots;Uncertainty;Data models;Trajectory optimization","learning (artificial intelligence);path planning","peg-in-hole task;peg tips;contact-rich manipulation tasks;planning approximate exploration trajectories;simulated agents;complex behavior policies;data complexity;real-world applications;learning process;model-free deep reinforcement learning;informative data;manipulation task;approximate trajectory optimization approach;global exploration;separately updated streams;approximate model uncertainty","","","28","Traditional","","","","IEEE","IEEE Journals"
"Spatial–Spectral Fusion by Combining Deep Learning and Variational Model","H. Shen; M. Jiang; J. Li; Q. Yuan; Y. Wei; L. Zhang","School of Resource and Environmental Science, Wuhan University, Wuhan, China; School of Resource and Environmental Science, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; MCFLY Technology, Beijing, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","8","6169","6181","In the field of spatial-spectral fusion, the variational model-based methods and the deep learning (DL)-based methods are state-of-the-art approaches. This paper presents a fusion method that combines the deep neural network with a variational model for the most common case of spatial-spectral fusion: panchromatic (PAN)/multispectral (MS) fusion. Specifically, a deep residual convolutional neural network (CNN) is first trained to learn the gradient features of the high spatial resolution multispectral image (HR-MS). The image observation variational models are then formulated to describe the relationships of the ideal fused image, the observed low spatial resolution multispectral image (LR-MS) image, and the gradient priors learned before. Then, fusion result can then be obtained by solving the fusion variational model. Both quantitative and visual assessments on high-quality images from various sources demonstrate that the proposed fusion method is superior to all the mainstream algorithms included in the comparison, in terms of overall fusion accuracy.","","","10.1109/TGRS.2019.2904659","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8685700","Deep learning (DL);gradient network;model-based optimization;spatial–spectral fusion","Spatial resolution;Deep learning;Optimization;Distortion;Mathematical model;Remote sensing","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);sensor fusion;variational techniques","high spatial resolution multispectral image;image observation variational models;fusion variational model;fusion method;spatial-spectral fusion;variational model-based methods;deep learning-based methods;deep neural network;deep residual convolutional neural network;low spatial resolution multispectral image;panchromatic-multispectral fusion","","","53","","","","","IEEE","IEEE Journals"
"Scalable Healthcare Assessment for Diabetic Patients Using Deep Learning on Multiple GPUs","D. Sierra-Sosa; B. Garcia-Zapirain; C. Castillo; I. Oleagordia; R. Nuño-Solinis; M. Urtaran-Laresgoiti; A. Elmaghraby","Computer Engineering and Computer Science Department, University of Louisville, Louisville, KY, USA; eVida Research Group, University of Deusto, Bilbao, Spain; eVida Research Group, University of Deusto, Bilbao, Spain; eVida Research Group, University of Deusto, Bilbao, Spain; Deusto Business School of Health, University of Deusto, Bilbao, Spain; Deusto Business School of Health, University of Deusto, Bilbao, Spain; Computer Engineering and Computer Science Department, University of Louisville, Louisville, KY, USA","IEEE Transactions on Industrial Informatics","","2019","15","10","5682","5689","The large-scale parallel computation that became available on the new generation of graphics processing units (GPUs) and on cloud-based services can be exploited for use in healthcare data analysis. Furthermore, computation workstations suited for deep learning are usually equipped with multiple GPUs allowing for workload distribution among multiple GPUs for larger datasets while exploiting parallelism in each GPU. In this paper, we utilize distributed and parallel computation techniques to efficiently analyze healthcare data using deep learning techniques. We demonstrate the scalability and computational benefits of this approach with a case study of longitudinal assessment of approximately 150 000 type 2 diabetic patients. Type 2 diabetes mellitus (T2DM) is the fourth case of mortality worldwide with rising prevalence. T2DM leads to adverse events such as acute myocardial infarction, major amputations, and avoidable hospitalizations. This paper aims to establish a relation between laboratory and medical assessment variables with the occurrence of the aforementioned adverse events and its prediction using machine learning techniques. We use a raw database provided by Basque Health Service, Spain, to conduct this study. This database contains 150 156 patients diagnosed with T2DM, from whom 321 laboratory and medical assessment variables recorded over four years are available. Predictions of adverse events on T2DM patients using both classical machine learning and deep learning techniques were performed and evaluated using accuracy, precision, recall and F1-score as metrics. The best performance for the prediction of acute myocardial infarction is obtained by linear discriminant analysis (LDA) and support vector machines (SVM) both balanced and weight models with an accuracy of 97%; hospital admission for avoidable causes best performance is obtained by LDA balanced and SVMs balanced both with an accuracy of 92%. For the prediction of the incidence of at least one adverse event, the model with the best performance is the recurrent neural network trained with a balanced dataset with an accuracy of 94.6%. The ability to perform and compare these experiments was possible through the use of a workstation with multi-GPUs. This setup allows for scalability to larger datasets. Such models are also cloud ready and can be deployed on similar architectures hosted on AWS for even larger datasets.","","","10.1109/TII.2019.2919168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8723173","Deep learning;graphics processing unit (GPU) computing;healthcare","Diabetes;Databases;Diseases;Deep learning;Medical diagnostic imaging","data analysis;diseases;health care;hospitals;learning (artificial intelligence);medical diagnostic computing;support vector machines","scalable healthcare assessment;multiGPUs;classical machine learning;T2DM patients;Basque Health Service;machine learning techniques;aforementioned adverse events;medical assessment variables;acute myocardial infarction;type 2 diabetes mellitus;approximately 150 000 type 2 diabetic patients;longitudinal assessment;computational benefits;deep learning techniques;parallel computation techniques;parallelism;workload distribution;computation workstations;healthcare data analysis;cloud-based services;large-scale parallel computation;multiple GPUs","","","30","Traditional","","","","IEEE","IEEE Journals"
"Multimodal Assessment of Parkinson's Disease: A Deep Learning Approach","J. C. Vásquez-Correa; T. Arias-Vergara; J. R. Orozco-Arroyave; B. Eskofier; J. Klucken; E. Nöth","Faculty of Engineering, University of Antioquia UdeA, Medellín, Colombia; Faculty of Engineering, University of Antioquia UdeA, Medellín, Colombia; Faculty of Engineering, University of Antioquia UdeA, Medellín, Colombia; Machine Learning and Data Analytics Laboratory, Department of Computer Science, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Department of Molecular Neurology, University Hospital Erlangen, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Pattern Recognition Laboratory, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany","IEEE Journal of Biomedical and Health Informatics","","2019","23","4","1618","1630","Parkinson's disease is a neurodegenerative disorder characterized by a variety of motor symptoms. Particularly, difficulties to start/stop movements have been observed in patients. From a technical/diagnostic point of view, these movement changes can be assessed by modeling the transitions between voiced and unvoiced segments in speech, the movement when the patient starts or stops a new stroke in handwriting, or the movement when the patient starts or stops the walking process. This study proposes a methodology to model such difficulties to start or to stop movements considering information from speech, handwriting, and gait. We used those transitions to train convolutional neural networks to classify patients and healthy subjects. The neurological state of the patients was also evaluated according to different stages of the disease (initial, intermediate, and advanced). In addition, we evaluated the robustness of the proposed approach when considering speech signals in three different languages: Spanish, German, and Czech. According to the results, the fusion of information from the three modalities is highly accurate to classify patients and healthy subjects, and it shows to be suitable to assess the neurological state of the patients in several stages of the disease. We also aimed to interpret the feature maps obtained from the deep learning architectures with respect to the presence or absence of the disease and the neurological state of the patients. As far as we know, this is one of the first works that considers multimodal information to assess Parkinson's disease following a deep learning approach.","","","10.1109/JBHI.2018.2866873","CODI from University of Antioquia; European Union's Horizon 2020 research and innovation programme; Marie Sklodowska-Curie; German Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444654","Parkinson's disease;deep learning;convolutional neural networks;speech;handwriting;gait","Diseases;Machine learning;Informatics;Feature extraction;Acceleration;Robustness;Perturbation methods","convolutional neural nets;diseases;feature extraction;gait analysis;learning (artificial intelligence);medical diagnostic computing;neurophysiology;speech processing","healthy subjects;neurological state;deep learning architectures;multimodal information;deep learning approach;multimodal assessment;neurodegenerative disorder;motor symptoms;movement changes;handwriting;convolutional neural networks;speech signals;Parkinsons disease","","1","45","","","","","IEEE","IEEE Journals"
"Efficient Training Management for Mobile Crowd-Machine Learning: A Deep Reinforcement Learning Approach","T. T. Anh; N. C. Luong; D. Niyato; D. I. Kim; L. Wang","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan","IEEE Wireless Communications Letters","","2019","8","5","1345","1348","In this letter, we consider the concept of mobile crowd-machine learning (MCML) for a federated learning model. The MCML enables mobile devices in a mobile network to collaboratively train neural network models required by a server while keeping data on the mobile devices. The MCML thus addresses data privacy issues of traditional machine learning. However, the mobile devices are constrained by energy, CPU, and wireless bandwidth. Thus, to minimize the energy consumption, training time, and communication cost, the server needs to determine proper amounts of data and energy that the mobile devices use for training. However, under the dynamics and uncertainty of the mobile environment, it is challenging for the server to determine the optimal decisions on mobile device resource management. In this letter, we propose to adopt a deep $Q$ -learning algorithm that allows the server to learn and find optimal decisions without any a priori knowledge of network dynamics. Simulation results show that the proposed algorithm outperforms the static algorithms in terms of energy consumption and training latency.","","","10.1109/LWC.2019.2917133","A*STAR-NTU-SUTD Joint Research Grant Call on Artificial Intelligence for the Future of Manufacturing; WASP/NTU; Singapore MOE Tier 1; MOE Tier 2; Singapore; Singapore EMA Energy Resilience; National Research Foundation of Korea; Korean Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8716527","Mobile crowd;federated learning;deep reinforcement learning","Mobile handsets;Servers;Training;Data models;Energy consumption;Heuristic algorithms;Machine learning","data privacy;learning (artificial intelligence);mobile computing;neural nets","mobile devices;MCML;energy consumption;mobile environment;mobile device resource management;learning algorithm;efficient training management;mobile crowd-machine learning;federated learning model;mobile network;neural network models","","1","7","","","","","IEEE","IEEE Journals"
"Improving Performance of Devanagari Script Input-Based P300 Speller Using Deep Learning","G. B. Kshirsagar; N. D. Londhe","National Institute of Technology Raipur; National Institute of Technology Raipur, Raipur, India","IEEE Transactions on Biomedical Engineering","","2019","66","11","2992","3005","The performance of an existing Devanagari script (DS) input-based P300 speller with conventional machine learning techniques suffers from low information transfer rate (ITR). This occurs due to its required large size of display, i.e., 8 × 8 row-column (RC) paradigm which exhibits issues like crowding effect, adjacency, fatigue, task difficulty, and required large number of trials for character recognition. For P300 detection, deep learning algorithms have shown the state of art performance compared to the conventional machine learning algorithms in the recent past. Therefore, authors have been motivated to develop a deep learning architecture for DS-based P300 speller which can detect the target characters more accurately and in less number of trials. For this, two proven deep learning algorithms, stacked autoencoder (SAE) and deep convolution neural network (DCNN) have been adopted. For further bettering their performances, batch normalization and innovative double batch training is included here to achieve accelerated training and alleviate the problem of overfitting. Additionally, a leaky ReLU activation function has also been used in DCNN to overcome dying ReLU problem. The experiments have been performed on self-generated dataset of 20 Devanagari words with 79 characters acquired from 10 subjects using 16 channel actiCAP Xpress EEG recorder. The experimental results illustrated that the proposed DCNN is able to detect 88.22% correct targets in just three trials. Moreover, it also provides ITR of 20.58 bits per minutes, which is significantly higher than existing techniques.","","","10.1109/TBME.2018.2875024","Department of Science and Technology; Cognitive Science Research Initiative (CSRI); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8486648","Brain-computer interface (BCI);Devanagari script (DS);stacked autoencoder (SAE);deep convolution neural network (DCNN);P300 speller","Machine learning;Electroencephalography;Support vector machines;Machine learning algorithms;Training;Brain modeling;Data acquisition","brain-computer interfaces;convolutional neural nets;electroencephalography;feature extraction;learning (artificial intelligence);medical signal processing","low information transfer rate;machine learning techniques;row-column paradigm;character recognition;deep learning algorithms;deep learning architecture;DS-based P300 speller;deep convolution neural network;DCNN;innovative double batch training;devanagari script input-based P300 speller;stacked autoencoder;batch normalization;ReLU activation function;16 channel actiCAP Xpress EEG recorder;devanagari words","","","44","Traditional","","","","IEEE","IEEE Journals"
"A Deep Learning Approach to Competing Risks Representation in Peer-to-Peer Lending","F. Tan; X. Hou; J. Zhang; Z. Wei; Z. Yan","Department of Computer Science, New Jersey Institute of Technology, Newark, NJ, USA; Department of Computer Science, New Jersey Institute of Technology, Newark, NJ, USA; Adobe Systems, San Jose, CA, USA; Department of Computer Science, New Jersey Institute of Technology, Newark, NJ, USA; Adobe Systems, San Jose, CA, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","5","1565","1574","Online peer-to-peer (P2P) lending is expected to benefit both investors and borrowers due to their low transaction cost and the elimination of expensive intermediaries. From the lenders' perspective, maximizing their return on investment is an ultimate goal during their decision-making procedure. In this paper, we explore and address a fundamental problem underlying such a goal: how to represent the two competing risks, charge-off and prepayment, in funded loans. We propose to model both potential risks simultaneously, which remains largely unexplored until now. We first develop a hierarchical grading framework to integrate two risks of loans both qualitatively and quantitatively. Afterward, we introduce an end-to-end deep learning approach to solve this problem by breaking it down into multiple binary classification subproblems that are amenable to both feature representation and risks learning. Particularly, we leverage deep neural networks to jointly solve these subtasks, which leads to the in-depth exploration of the interaction involved in these tasks. To the best of our knowledge, this is the first attempt to characterize competing risks for loans in P2P lending via deep neural networks. The comprehensive experiments on real-world loan data show that our methodology is able to achieve an appealing investment performance by modeling the competition within and between risks explicitly and properly. The feature analysis based on saliency maps provides useful insights into payment dynamics of loans for potential investors intuitively.","","","10.1109/TNNLS.2018.2870573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8488590","Competing risks;deep neural networks;peer-to-peer (P2P) lending;return on investment (ROI)","Investment;Neural networks;Peer-to-peer computing;Machine learning;Loans and mortgages;Data models;Learning systems","decision making;investment;learning (artificial intelligence);neural nets;pattern classification;peer-to-peer computing;risk analysis","borrowers;low transaction cost;decision-making procedure;hierarchical grading framework;end-to-end deep learning approach;multiple binary classification subproblems;feature representation;real-world loan data;potential investors;peer-to-peer lending;deep neural networks;risks representation;P2P lending;prepayment","","2","42","","","","","IEEE","IEEE Journals"
"Deep Learning for Monitoring of Human Gait: A Review","A. S. Alharthi; S. U. Yunas; K. B. Ozanyan","School of Electrical and Electronic Engineering, The University of Manchester, Manchester, U.K.; School of Electrical and Electronic Engineering, The University of Manchester, Manchester, U.K.; School of Electrical and Electronic Engineering, The University of Manchester, Manchester, U.K.","IEEE Sensors Journal","","2019","19","21","9575","9591","The essential human gait parameters are briefly reviewed, followed by a detailed review of the state of the art in deep learning for the human gait analysis. The modalities for capturing the gait data are grouped according to the sensing technology: video sequences, wearable sensors, and floor sensors, as well as the publicly available datasets. The established artificial neural network architectures for deep learning are reviewed for each group, and their performance are compared with particular emphasis on the spatiotemporal character of gait data and the motivation for multi-sensor, multi-modality fusion. It is shown that by most of the essential metrics, deep learning convolutional neural networks typically outperform shallow learning models. In the light of the discussed character of gait data, this is attributed to the possibility to extract the gait features automatically in deep learning as opposed to the shallow learning from the handcrafted gait features.","","","10.1109/JSEN.2019.2928777","Government of Saudi Arabia; Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762100","Deep learning;floor sensor;gait;neural network;sensor fusion;video sequence;wearable sensor","Deep learning;Foot;Wearable sensors;Knee;Legged locomotion;Force","convolutional neural nets;feature extraction;gait analysis;image fusion;image sensors;learning (artificial intelligence)","human gait analysis;wearable sensors;floor sensors;multimodality fusion;shallow learning models;handcrafted gait features;human gait parameters;artificial neural network;multisensor fusion;deep learning;human gait monitoring;convolutional neural networks;video sequences;spatiotemporal character","","","108","CCBY","","","","IEEE","IEEE Journals"
"Automatic 3D Bi-Ventricular Segmentation of Cardiac Images by a Shape-Refined Multi- Task Deep Learning Approach","J. Duan; G. Bello; J. Schlemper; W. Bai; T. J. W. Dawes; C. Biffi; A. de Marvao; G. Doumoud; D. P. O’Regan; D. Rueckert","Biomedical Image Analysis Group, Imperial College London, London, U.K.; MRC London Institute of Medical Sciences, Imperial College London, London, U.K.; Biomedical Image Analysis Group, Imperial College London, London, U.K.; Biomedical Image Analysis Group, Imperial College London, London, U.K.; MRC London Institute of Medical Sciences, Imperial College London, London, U.K.; Biomedical Image Analysis Group, Imperial College London, London, U.K.; MRC London Institute of Medical Sciences, Imperial College London, London, U.K.; MRC London Institute of Medical Sciences, Imperial College London, London, U.K.; MRC London Institute of Medical Sciences, Imperial College London, London, U.K.; Biomedical Image Analysis Group, Imperial College London, London, U.K.","IEEE Transactions on Medical Imaging","","2019","38","9","2151","2164","Deep learning approaches have achieved state-of-the-art performance in cardiac magnetic resonance (CMR) image segmentation. However, most approaches have focused on learning image intensity features for segmentation, whereas the incorporation of anatomical shape priors has received less attention. In this paper, we combine a multi-task deep learning approach with atlas propagation to develop a shape-refined bi-ventricular segmentation pipeline for short-axis CMR volumetric images. The pipeline first employs a fully convolutional network (FCN) that learns segmentation and landmark localization tasks simultaneously. The architecture of the proposed FCN uses a 2.5D representation, thus combining the computational advantage of 2D FCNs networks and the capability of addressing 3D spatial consistency without compromising segmentation accuracy. Moreover, a refinement step is designed to explicitly impose shape prior knowledge and improve segmentation quality. This step is effective for overcoming image artifacts (e.g., due to different breath-hold positions and large slice thickness), which preclude the creation of anatomically meaningful 3D cardiac shapes. The pipeline is fully automated, due to network's ability to infer landmarks, which are then used downstream in the pipeline to initialize atlas propagation. We validate the pipeline on 1831 healthy subjects and 649 subjects with pulmonary hypertension. Extensive numerical experiments on the two datasets demonstrate that our proposed method is robust and capable of producing accurate, high-resolution, and anatomically smooth bi-ventricular 3D models, despite the presence of artifacts in input CMR volumes.","","","10.1109/TMI.2019.2894322","British Heart Foundation; Engineering and Physical Sciences Research Council; National Institute for Health Research; Medical Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624549","Deep learning;bi-ventricular CMR segmentation;landmark localization;non-rigid registration;label fusion;multi-atlas segmentation;shape prior;cardiac artifacts","Image segmentation;Pipelines;Three-dimensional displays;Heart;Shape;Deep learning;Imaging","biomedical MRI;cardiology;convolutional neural nets;image segmentation;learning (artificial intelligence);medical image processing;pneumodynamics","3D cardiac shape;pulmonary hypertension;input CMR volumes;anatomically smooth bi-ventricular 3D models;image artifacts;segmentation quality;shape prior knowledge;refinement step;segmentation accuracy;3D spatial consistency;2D FCNs networks;landmark localization tasks;fully convolutional network;short-axis CMR volumetric images;shape-refined bi-ventricular segmentation pipeline;atlas propagation;multitask deep learning approach;learning image intensity;cardiac magnetic resonance image segmentation;learning approaches;shape-refined multi task","","","40","CCBY","","","","IEEE","IEEE Journals"
"Deep Learning-Based Pilot Design for Multi-User Distributed Massive MIMO Systems","J. Xu; P. Zhu; J. Li; X. You","National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China","IEEE Wireless Communications Letters","","2019","8","4","1016","1019","This letter proposes a deep learning-based pilot design scheme to minimize the sum mean square error (MSE) of channel estimation for multi-user distributed massive multiple-input multiple-output (MIMO) systems. The pilot signal of each user is expressed as a weighted superposition of orthonormal pilot sequence basis, where the power assigned to each pilot sequence is the corresponding weight. A multi-layer fully connected deep neural network (DNN) is designed to optimize the power allocated to each pilot sequence to minimize the sum MSE, which takes the channel large-scale fading coefficients as input and outputs the pilot power allocation vector. The loss function of the DNN is defined as the sum MSE, and we leverage the unsupervised learning strategy to train the DNN. Simulation results show that the proposed scheme achieves better sum MSE performance than other methods with low complexity.","","","10.1109/LWC.2019.2904229","Natural Science Foundation of Jiangsu Province; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664168","Pilot design;deep learning;deep neural network (DNN);multi-user distributed massive MIMO","Resource management;Fading channels;Complexity theory;Channel estimation;MIMO communication;Unsupervised learning;Training","channel estimation;fading channels;mean square error methods;MIMO communication;multiuser detection;neural nets;telecommunication computing;unsupervised learning","DNN;channel large-scale fading coefficients;pilot power allocation vector;massive MIMO systems;deep learning-based pilot design scheme;channel estimation;multiuser distributed massive multiple-input multiple-output systems;multilayer fully connected deep neural network;unsupervised learning;orthonormal pilot sequence;sum mean square error","","1","9","","","","","IEEE","IEEE Journals"
"From Pixels to Percepts: Highly Robust Edge Perception and Contour Following Using Deep Learning and an Optical Biomimetic Tactile Sensor","N. F. Lepora; A. Church; C. de Kerckhove; R. Hadsell; J. Lloyd","Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Google DeepMind, London, U.K.; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.","IEEE Robotics and Automation Letters","","2019","4","2","2101","2107","Deep learning has the potential to have same the impact on robot touch as it has had on robot vision. Optical tactile sensors act as a bridge between the subjects by allowing techniques from vision to be applied to touch. In this letter, we apply deep learning to an optical biomimetic tactile sensor, the TacTip, which images an array of papillae (pins) inside its sensing surface analogous to structures within human skin. Our main result is that the application of a deep convolutional neural network can give reliable edge perception and, thus a robust policy for planning contact points to move around object contours. Robustness is demonstrated over several irregular and compliant objects with both tapping and continuous sliding, using a model trained only by tapping onto a disk. These results relied on using techniques to encourage generalization to tasks beyond which the model was trained. We expect this is a generic problem in practical applications of tactile sensing that deep learning will solve.","","","10.1109/LRA.2019.2899192","A biomimetic forebrain for robot touch; NVIDIA Corporation with donation of a TITAN Xp GPU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641397","Force and tactile sensing;biomimetics;deep learning in robotics and automation","Tactile sensors;Deep learning;Pins;Biomedical optical imaging;Optical imaging","biomimetics;convolutional neural nets;learning (artificial intelligence);robot vision;skin;tactile sensors","deep learning;optical biomimetic tactile sensor;deep convolutional neural network;reliable edge perception;robot touch;robot vision;robust edge perception;TacTip;sensing surface analogous;human skin;contact points","","2","44","","","","","IEEE","IEEE Journals"
"Uniform and Variational Deep Learning for RGB-D Object Recognition and Person Re-Identification","L. Ren; J. Lu; J. Feng; J. Zhou","State Key Laboratory of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; State Key Laboratory of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; State Key Laboratory of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; State Key Laboratory of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China","IEEE Transactions on Image Processing","","2019","28","10","4970","4983","In this paper, we propose a uniform and variational deep learning (UVDL) method for RGB-D object recognition and person re-identification. Unlike most existing object recognition and person re-identification methods, which usually use only the visual appearance information from RGB images, our method recognizes visual objects and persons with RGB-D images to exploit more reliable information such as geometric and anthropometric information that are robust to different viewpoints. Specifically, we extract the depth feature and the appearance feature from the depth and RGB images with two deep convolutional neural networks, respectively. In order to combine the depth feature and the appearance feature to exploit their relationship, we design a uniform and variational multi-modal auto-encoder at the top layer of our deep network to seek a uniform latent variable by projecting them into a common space, which contains the whole information of RGB-D images and has small intra-class variation and large inter-class variation, simultaneously. Finally, we optimize the auto-encoder layer and two deep convolutional neural networks jointly to minimize the discriminative loss and the reconstruction error. The experimental results on both RGB-D object recognition and RGB-D person re-identification are presented to show the efficiency of our proposed approach.","","","10.1109/TIP.2019.2915655","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715446","Deep learning;object recognition;person re-identification;uniform;variational;multi-modal learning","Object recognition;Feature extraction;Deep learning;Image color analysis;Visualization;Convolutional neural networks;Image reconstruction","convolutional neural nets;feature extraction;image classification;image colour analysis;image recognition;learning (artificial intelligence);object detection;object recognition","RGB-D object recognition;person re-identification;visual appearance information;RGB images;visual objects;RGB-D images;geometric information;anthropometric information;depth feature;appearance feature;deep convolutional neural networks;deep network;uniform latent variable;object recognition;intraclass variation;interclass variation;variational multimodal autoencoder","","","80","","","","","IEEE","IEEE Journals"
"AgentGraph: Toward Universal Dialogue Management With Structured Deep Reinforcement Learning","L. Chen; Z. Chen; B. Tan; S. Long; M. Gašić; K. Yu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Heinrich Heine University Düsseldorf, Düsseldorf, Germany; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","9","1378","1391","Dialogue policy plays an important role in task-oriented spoken dialogue systems. It determines how to respond to users. The recently proposed deep reinforcement learning (DRL) approaches have been used for policy optimization. However, these deep models are still challenging for two reasons: first, many DRL-based policies are not sample efficient; and second, most models do not have the capability of policy transfer between different domains. In this paper, we propose a universal framework, AgentGraph, to tackle these two problems. The proposed AgentGraph is the combination of graph neural network (GNN) based architecture and DRL-based algorithm. It can be regarded as one of the multi-agent reinforcement learning approaches. Each agent corresponds to a node in a graph, which is defined according to the dialogue domain ontology. When making a decision, each agent can communicate with its neighbors on the graph. Under AgentGraph framework, we further propose dual GNN-based dialogue policy, which implicitly decomposes the decision in each turn into a high-level global decision and a low-level local decision. Experiments show that AgentGraph models significantly outperform traditional reinforcement learning approaches on most of the 18 tasks of the PyDial benchmark. Moreover, when transferred from the source task to a target task, these models not only have acceptable initial performance but also converge much faster on the target task.","","","10.1109/TASLP.2019.2919872","National Basic Research Program of China (973 Program); Shanghai International Science and Technology Cooperation Fund; Alexander von Humboldt Sofja Kovalevskaja Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8726165","Dialogue policy;deep reinforcement learning;graph neural networks;policy adaptation;transfer learning","Reinforcement learning;Task analysis;Neural networks;Optimization;Ontologies;Computational modeling;Training","graph theory;interactive systems;learning (artificial intelligence);multi-agent systems;neural nets;ontologies (artificial intelligence)","policy optimization;deep models;DRL-based policies;policy transfer;graph neural network based architecture;DRL-based algorithm;dialogue domain ontology;AgentGraph framework;dual GNN-based dialogue policy;high-level global decision;low-level local decision;AgentGraph models;structured deep reinforcement learning;task-oriented spoken dialogue systems;multiagent reinforcement learning;universal dialogue management;GNN based architecture;PyDial benchmark","","","54","","","","","IEEE","IEEE Journals"
"Deep Hashing Neural Networks for Hyperspectral Image Feature Extraction","L. Fang; Z. Liu; W. Song","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","9","1412","1416","Recently, deep learning has been recognized as a powerful tool to extract hierarchical features of hyperspectral images (HSIs). The existing deep learning-based methods exploit label information of land classes as the supervised information to train deep networks. However, considering that HSIs exhibit very complex spectral-spatial characteristic, e.g., the large intraclass variations and small interclass variations, these semantic information (i.e., label information)-based deep networks may not effectively cope with the above problem. In this letter, we propose a novel deep model, named deep hashing neural network (DHNN), to learn similarity-preserving deep features (SPDFs) for HSI classification. First, a well-pretrained network is introduced to simultaneously extract features of a pair of input samples. Second, a novel hashing layer is inserted after the last fully connected layer to transfer the real-value features into binary features, which can significantly speed up the computation for feature distance. Then, a loss function is elaborately designed to minimize the feature distance of similar pairs and maximize the feature distance of dissimilar pairs in Hamming space. Finally, the SPDF extracted by propagating the samples through the trained DHNN are fed into a support vector machine (SVM) classifier for HSI classification. Experimental results on two real HSIs demonstrate that the proposed feature extraction method in conjunction with a linear SVM classifier outperforms other feature extraction methods and competitive classifiers.","","","10.1109/LGRS.2019.2899823","National Natural Science Foundation of China; National Natural Science Foundation for Young Scientist of China; China Postdoctoral Science Foundation; Natural Science Foundation of Hunan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663589","Classification;deep learning;feature extraction;hashing;hyperspectral images (HSIs)","Feature extraction;Hyperspectral imaging;Support vector machines;Deep learning;Semantics;Task analysis","feature extraction;hyperspectral imaging;image classification;image representation;learning (artificial intelligence);neural nets;support vector machines","HSIs;label information;land classes;supervised information;deep networks;intraclass variations;interclass variations;deep model;named deep hashing neural network;similarity-preserving deep features;HSI classification;well-pretrained network;fully connected layer;real-value features;binary features;feature distance;trained DHNN;feature extraction method;neural networks;hyperspectral image feature extraction;hierarchical features;deep learning-based methods;hashing layer;linear SVM classifier","","1","28","","","","","IEEE","IEEE Journals"
"Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning","N. Dilokthanakul; C. Kaplanis; N. Pawlowski; M. Shanahan","Computing Department, Imperial College London, London, U.K.; Computing Department, Imperial College London, London, U.K.; Computing Department, Imperial College London, London, U.K.; Computing Department, Imperial College London, London, U.K.","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","11","3409","3418","One of the main concerns of deep reinforcement learning (DRL) is the data inefficiency problem, which stems both from an inability to fully utilize data acquired and from naive exploration strategies. In order to alleviate these problems, we propose a DRL algorithm that aims to improve data efficiency via both the utilization of unrewarded experiences and the exploration strategy by combining ideas from unsupervised auxiliary tasks, intrinsic motivation, and hierarchical reinforcement learning (HRL). Our method is based on a simple HRL architecture with a metacontroller and a subcontroller. The subcontroller is intrinsically motivated by the metacontroller to learn to control aspects of the environment, with the intention of giving the agent: 1) a neural representation that is generically useful for tasks that involve manipulation of the environment and 2) the ability to explore the environment in a temporally extended manner through the control of the metacontroller. In this way, we reinterpret the notion of pixel- and feature-control auxiliary tasks as reusable skills that can be learned via an intrinsic reward. We evaluate our method on a number of Atari 2600 games. We found that it outperforms the baseline in several environments and significantly improves performance in one of the hardest games-Montezuma's revenge-for which the ability to utilize sparse data is key. We found that the inclusion of intrinsic reward is crucial for the improvement in the performance and that most of the benefit seems to be derived from the representations learned during training.","","","10.1109/TNNLS.2019.2891792","Thai Government through DPST scholarship; Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629360","Auxiliary task;deep reinforcement learning (DRL);hierarchical reinforcement learning (HRL);intrinsic motivation","Task analysis;Reinforcement learning;Training;Neural networks;Visualization;Trajectory;Learning systems","computer games;learning (artificial intelligence);neural nets","subcontroller;metacontroller;intrinsic reward;sparse data;intrinsic motivation;hierarchical reinforcement learning;deep reinforcement learning;data inefficiency problem;naive exploration strategies;DRL algorithm;data efficiency;unsupervised auxiliary tasks;Montezuma's revenge;unrewarded experiences;HRL architecture;neural representation;Atari 2600 games;feature control auxiliary tasks;pixel control auxiliary tasks","","1","38","","","","","IEEE","IEEE Journals"
"Learning to See the Wood for the Trees: Deep Laser Localization in Urban and Natural Environments on a CPU","G. Tinchev; A. Penate-Sanchez; M. Fallon","Dynamic Systems Group, Oxford Robotics Institute, University of Oxford, Oxford, U.K.; Dynamic Systems Group, Oxford Robotics Institute, University of Oxford, Oxford, U.K.; Dynamic Systems Group, Oxford Robotics Institute, University of Oxford, Oxford, U.K.","IEEE Robotics and Automation Letters","","2019","4","2","1327","1334","Localization in challenging, natural environments, such as forests or woodlands, is an important capability for many applications from guiding a robot navigating along a forest trail to monitoring vegetation growth with handheld sensors. In this letter, we explore laser-based localization in both urban and natural environments, which is suitable for online applications. We propose a deep learning approach capable of learning meaningful descriptors directly from three-dimensional point clouds by comparing triplets (anchor, positive, and negative examples). The approach learns a feature space representation for a set of segmented point clouds that are matched between a current and previous observations. Our learning method is tailored toward loop closure detection resulting in a small model that can be deployed using only a CPU. The proposed learning method would allow the full pipeline to run on robots with limited computational payloads, such as drones, quadrupeds, or Unmanned Ground Vehicles (UGVs).","","","10.1109/LRA.2019.2895264","EPSRC RAIN and ORCA Robotics Hubs; Royal Society University Research Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8626476","Localization;deep learning in robotics and automation;visual learning;SLAM;field robots","Three-dimensional displays;Robots;Laser radar;Feature extraction;Sensors;Deep learning;Neural networks","forestry;learning (artificial intelligence);mobile robots;path planning;robot vision;sensors;SLAM (robots);vegetation","natural environments;CPU;woodlands;forest trail;laser-based localization;deep learning approach;three-dimensional point clouds;urban environment;robot navigation;handheld sensor;loop closure detection;computational payload;drone;quadruped;unmanned ground vehicle;UGV","","","31","","","","","IEEE","IEEE Journals"
"Speech Enhancement Based on Teacher–Student Deep Learning Using Improved Speech Presence Probability for Noise-Robust Speech Recognition","Y. Tu; J. Du; C. Lee","University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; Georgia Institute of Technology, Atlanta, GA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","12","2080","2091","In this paper, we propose a novel teacher-student learning framework for the preprocessing of a speech recognizer, leveraging the online noise tracking capabilities of improved minima controlled recursive averaging (IMCRA) and deep learning of nonlinear interactions between speech and noise. First, a teacher model with deep architectures is built to learn the target of ideal ratio masks (IRMs) using simulated training pairs of clean and noisy speech data. Next, a student model is trained to learn an improved speech presence probability by incorporating the estimated IRMs from the teacher model into the IMCRA approach. The student model can be compactly designed in a causal processing mode having no latency with the guidance of a complex and noncausal teacher model. Moreover, the clean speech requirement, which is difficult to meet in real-world adverse environments, can be relaxed for training the student model, implying that noisy speech data can be directly used to adapt the regression-based enhancement model to further improve speech recognition accuracies for noisy speech collected in such conditions. Experiments on the CHiME-4 challenge task show that our best student model with bidirectional gated recurrent units (BGRUs) can achieve a relative word error rate (WER) reduction of 18.85% for the real test set when compared to unprocessed system without acoustic model retraining. However, the traditional teacher model degrades the performance of the unprocessed system in this case. In addition, the student model with a deep neural network (DNN) in causal mode having no latency yields a relative WER reduction of 7.94% over the unprocessed system with 670 times less computing cycles when compared to the BGRU-equipped student model. Finally, the conventional speech enhancement and IRM-based deep learning method destroyed the ASR performance when the recognition system became more powerful. While our proposed approach could still improve the ASR performance even in the more powerful recognition system.","","","10.1109/TASLP.2019.2940662","National Key R&D Program of China; National Natural Science Foundation of China; Key Science and Technology Project of Anhui Province; Huawei Noah's Ark Lab; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8834827","Teacher-student learning;improved minima controlled recursive averaging;improved speech presence probability;deep learning based speech enhancement;noise-robust speech recognition","Speech enhancement;Speech recognition;Noise measurement;Adaptation models;Computational modeling;Training","error statistics;learning (artificial intelligence);neural nets;noise;speech enhancement;speech recognition","speech enhancement;teacher-student deep learning;improved speech presence probability;noise-robust speech recognition;online noise tracking;improved minima controlled recursive averaging;nonlinear interactions;ratio masks;causal processing mode;complex noncausal teacher model;regression-based enhancement model;CHiME-4 challenge task;bidirectional gated recurrent units;word error rate;deep neural network;IRM-based deep learning method;BGRU-equipped student model;relative WER reduction;IMCRA","","","52","Traditional","","","","IEEE","IEEE Journals"
"Cooperative Communications With Relay Selection Based on Deep Reinforcement Learning in Wireless Sensor Networks","Y. Su; X. Lu; Y. Zhao; L. Huang; X. Du","Xiamen University, Xiamen, China; Communication Engineering Department, Xiamen University, Xiamen, China; Communication Engineering Department, Xiamen University, Xiamen, China; Communication Engineering Department, Xiamen University, Xiamen, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA","IEEE Sensors Journal","","2019","19","20","9561","9569","Cooperative communication technology has become a research hotspot in wireless sensor networks (WSNs) in recent years, and will become one of the key technologies for improving spectrum utilization in wireless communication systems in the future. It leverages cooperation among multiple relay nodes in the wireless network to realize path transmission sharing, thereby improving the system throughput. In this paper, we model the process of cooperative communications with relay selection in WSNs as a Markov decision process and propose DQ-RSS, a deep-reinforcement-learning-based relay selection scheme, in WSNs. In DQ-RSS, a deep-Q-network (DQN) is trained according to the outage probability and mutual information, and the optimal relay is selected from a plurality of relay nodes without the need for a network model or prior data. More specifically, we use DQN to process high-dimensional state spaces and accelerate the learning rate. We compare DQ-RSS with the Q-learning-based relay selection scheme and evaluate the network performance on the basis of three aspects: outage probability, system capacity, and energy consumption. Simulation results indicate that DQ-RSS can achieve better performance on these elements and save the convergence time compared with existing schemes.","","","10.1109/JSEN.2019.2925719","National Natural Science Foundation of China; Key Laboratory of Digital Fujian on IoT Communication, Architecture and Safety Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8750861","Wireless sensor networks;cooperative communications;relay selection;outage probability;deep reinforcement learning","Relays;Wireless sensor networks;Cooperative communication;Reinforcement learning;Ad hoc networks;Sensors","cooperative communication;learning (artificial intelligence);Markov processes;probability;relay networks (telecommunication);wireless sensor networks","wireless sensor networks;communication technology;WSNs;wireless communication systems;multiple relay nodes;system throughput;Markov decision process;DQ-RSS;deep-reinforcement-learning-based relay selection scheme;deep-Q-network;outage probability;optimal relay;network model;learning rate;network performance;high-dimensional state spaces;cooperative communication;deep reinforcement learning;spectrum utilization;path transmission sharing","","","33","","","","","IEEE","IEEE Journals"
"A Novel Semisupervised Deep Learning Method for Human Activity Recognition","Q. Zhu; Z. Chen; Y. C. Soh","Department of Electrical and Electronic Engneering, Nanyang Technological University, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Department of Electrical and Electronic Engneering, Nanyang Technological University, Singapore","IEEE Transactions on Industrial Informatics","","2019","15","7","3821","3830","Human activity recognition (HAR) based on inertial sensors has been investigated for many industrial informatics applications, such as healthcare and ubiquitous computing. Existing methods mainly rely on supervised learning schemes, which require large labeled training data. However, labeled data are sometimes difficult to acquire, while unlabeled data are readily available. Thus, we intend to make use of both labeled and unlabeled data with semisupervised learning for accurate HAR. In this paper, we propose a semisupervised deep learning approach, using temporal ensembling of deep long short-term memory, to recognize human activities with smartphone inertial sensors. With the deep neural network processing, features are extracted for local dependencies in the recurrent framework. Besides, with an ensemble approach based on both labeled and unlabeled data, we can combine together the supervised and unsupervised losses, so as to make good use of unlabeled data that the supervised learning method cannot leverage. Experimental results indicate the effectiveness of our proposed semisupervised learning scheme, when compared to several state-of-the-art semisupervised learning approaches.","","","10.1109/TII.2018.2889315","A*STAR Industrial Internet of Things Research Program; RIE2020 IAF-PP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8586957","Deep long short-term memory (DLSTM);human activity recognition;semisupervised learning;temporal ensembling","Activity recognition;Feature extraction;Semisupervised learning;Informatics;Supervised learning","feature extraction;image recognition;neural nets;sensors;smart phones;supervised learning","smartphone inertial sensors;HAR;deep neural network;semisupervised deep learning method;healthcare;ubiquitous computing;temporal ensembling;long short-term memory;feature extraction;recurrent framework;industrial informatics applications;human activity recognition","","","45","","","","","IEEE","IEEE Journals"
"Semisupervised Hyperspectral Image Classification Using Deep Features","M. S. Aydemir; G. Bilgin","Scientific and Technological Research Council of Turkey, Kocaeli, Turkey; Department of Computer Engineering and Signal and Image Processing Laboratory, Yildiz Technical University, Istanbul, Turkey","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","9","3615","3622","As in other remote-sensing applications, collecting ground-truth information from the earth's surface is expensive and time-consuming process for hyperspectral imaging. In this study, a deep learning-based semisupervised learning framework is proposed to solve this small labeled sample size problem. The main contribution of this study is the construction of a deep learning model for each hyperspectral sensor type that can be used for data obtained from these sensors. In the proposed framework, the “trained base model” is obtained with any dataset from a hyperspectral sensor, and fine-tuned and evaluated with another dataset. In this way, a general deep model is developed for extracting deep features which can be linearly classified or clustered. The system is evaluated with three different clustering techniques, the modified k-means, subtractive, and mean-shift clustering, for selecting initial representative labeled training samples comparatively. Another contribution of this study is to exploit the labeled and unlabeled sample information with linear transductive support vector machines. The proposed semisupervised learning framework is proven by the experimental results using different number of small sample sizes.","","","10.1109/JSTARS.2019.2921033","Scientific Research Projects Coordination Department, Yildiz Technical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8745667","Deep features;deep learning (DL);fine-tuning;hyperspectral images;semisupervised learning (SSL);transductive support vector machines (TSVM)","Training;Feature extraction;Hyperspectral imaging;Support vector machines;Tuning;Data models","geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);pattern classification;pattern clustering;support vector machines","hyperspectral imaging;deep learning-based;labeled sample size problem;deep learning model;hyperspectral sensor type;trained base model;general deep model;deep features;mean-shift clustering;initial representative labeled training samples;labeled sample information;unlabeled sample information;linear transductive support vector machines;hyperspectral image classification;remote-sensing applications;ground-truth information;earth;time-consuming process","","","33","Traditional","","","","IEEE","IEEE Journals"
"Deep Learning in Downlink Coordinated Multipoint in New Radio Heterogeneous Networks","F. B. Mismar; B. L. Evans","Department of Electrical and Computer Engineering, Wireless Networking and Communications Group, University of Texas at Austin, Austin, TX, USA; Department of Electrical and Computer Engineering, Wireless Networking and Communications Group, University of Texas at Austin, Austin, TX, USA","IEEE Wireless Communications Letters","","2019","8","4","1040","1043","We propose a method to improve the performance of the downlink coordinated multipoint (DL CoMP) in heterogeneous fifth generation new radio (NR) networks. The standards-compliant method is based on the construction of a surrogate CoMP trigger function using deep learning. The cooperating set is a single-tier of sub-6 GHz heterogeneous base stations operating in the frequency division duplex mode (i.e., no channel reciprocity). This surrogate function enhances the downlink user throughput distribution through online learning of non-linear interactions of features and lower bias learning models. In simulation, the proposed method outperforms industry standards in a realistic and scalable heterogeneous cellular environment.","","","10.1109/LWC.2019.2904686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8665922","MIMO;DL CoMP;new radio;NR;5G;LTE-A;FDD;deep learning;heterogeneous networks;SON","Deep learning;Base stations;Interference;Signal to noise ratio;Downlink;Heterogeneous networks;OFDM","5G mobile communication;learning (artificial intelligence);neural nets;radio networks;telecommunication computing","frequency division duplex mode;channel reciprocity;surrogate function;online learning;scalable heterogeneous cellular environment;deep learning;radio heterogeneous networks;DL CoMP;heterogeneous fifth generation new radio networks;standards-compliant method;surrogate CoMP trigger function;sub-6 GHz heterogeneous base stations;bias learning models;downlink coordinated multipoint;frequency 6.0 GHz","","","13","","","","","IEEE","IEEE Journals"
"Learning QoE of Mobile Video Transmission With Deep Neural Network: A Data-Driven Approach","X. Tao; Y. Duan; M. Xu; Z. Meng; J. Lu","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Shenzhen Graduate School of Tsinghua University, Shenzhen, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Journal on Selected Areas in Communications","","2019","37","6","1337","1348","Quality of experience (QoE) serves as a direct evaluation of users' experiences in mobile video transmission and thus essential for network management, such as network optimization. In this paper, we propose a deep learning-based QoE prediction approach with a large-scale QoE dataset for mobile video transmission. Specifically, we develop a mobile phone application for collecting user QoE data when viewing videos transmitted over the mobile internet in a practical environment. Then, we construct a large-scale dataset by collecting over 80000 piece of data with four kinds of subjective scores and 89 network parameters. Each QoE metric is related to only some of the 89 network parameters. Therefore, we apply the feature selection method to find the feature parameters related to user scores. Additionally, the boxplot method is used to clean the raw data by removing outliers. Finally, a deep neural network (DNN) is developed to learn the relationships between the network parameters and the subjective QoE scores. The proposed DNN can also be seen as a data-driven objective QoE prediction approach for mobile video transmission, which can be used to predict the user QoE scores. The experimental results show that the proposed approach can effectively remove most features irrelevant to QoE prediction. Moreover, the performance of QoE prediction by the proposed model outperforms other state-of-the-art approaches.","","","10.1109/JSAC.2019.2904359","National Natural Science Foundation of China; Chinese Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664625","Quality of experience;data collection;feature selection;data cleaning;deep learning","Quality of experience;Streaming media;Quality of service;Quality assessment;Deep learning;Databases;Monitoring","feature selection;Internet;learning (artificial intelligence);mobile computing;neural nets;quality of experience;video streaming","mobile video transmission;deep neural network;data-driven approach;network management;network optimization;deep learning-based QoE prediction approach;large-scale QoE dataset;mobile phone application;user QoE data;mobile internet;QoE metric;subjective QoE scores;data-driven objective QoE prediction approach;user QoE scores;network parameters;feature selection;quality of experience","","1","50","","","","","IEEE","IEEE Journals"
"Universal Approximation Capability of Broad Learning System and Its Structural Variations","C. L. P. Chen; Z. Liu; S. Feng","Faculty of Science and Technology, University of Macau, Macau, China; Faculty of Science and Technology, University of Macau, Macau, China; Faculty of Science and Technology, University of Macau, Macau, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","4","1191","1204","After a very fast and efficient discriminative broad learning system (BLS) that takes advantage of flatted structure and incremental learning has been developed, here, a mathematical proof of the universal approximation property of BLS is provided. In addition, the framework of several BLS variants with their mathematical modeling is given. The variations include cascade, recurrent, and broad-deep combination structures. From the experimental results, the BLS and its variations outperform several exist learning algorithms on regression performance over function approximation, time series prediction, and face recognition databases. In addition, experiments on the extremely challenging data set, such as MS-Celeb-1M, are given. Compared with other convolutional networks, the effectiveness and efficiency of the variants of BLS are demonstrated.","","","10.1109/TNNLS.2018.2866622","National Natural Science Foundation of China; Fundo para o Desenvolvimento das Ciências e da Tecnologia; University of Macau through the Multiyear Research Grants; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457525","Broad learning system (BLS);deep learning;face recognition;functional link neural networks (FLNNs);nonlinear function approximation;time-variant big data modeling;universal approximation","Neural networks;Learning systems;Data models;Training;Face recognition;Machine learning;Mathematical model","approximation theory;learning (artificial intelligence);regression analysis;time series","incremental learning;universal approximation property;broad-deep combination structures;function approximation;structural variations;flatted structure;BLS system;discriminative broad learning system;MS-Celeb-1M dataset;time series prediction;face recognition databases;regression performance","","4","36","","","","","IEEE","IEEE Journals"
"Distributive Dynamic Spectrum Access Through Deep Reinforcement Learning: A Reservoir Computing-Based Approach","H. Chang; H. Song; Y. Yi; J. Zhang; H. He; L. Liu","Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Standards and Mobility Innovation Lab., Samsung Research America, Richardson, TX, USA; Department of Electrical, Computer, and Biomedical Engineering, University of Rhode Island, South Kingstown, RI, USA; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA","IEEE Internet of Things Journal","","2019","6","2","1938","1948","Dynamic spectrum access (DSA) is regarded as an effective and efficient technology to share radio spectrum among different networks. As a secondary user (SU), a DSA device will face two critical problems: 1) avoiding causing harmful interference to primary users (PUs) and 2) conducting effective interference coordination with other SUs. These two problems become even more challenging for a distributed DSA network where there is no centralized controllers for SUs. In this paper, we investigate communication strategies of a distributive DSA network under the presence of spectrum sensing errors. To be specific, we apply the powerful machine learning tool, deep reinforcement learning (DRL), for SUs to learn “appropriate” spectrum access strategies in a distributed fashion assuming NO knowledge of the underlying system statistics. Furthermore, a special type of recurrent neural network, called the reservoir computing (RC), is utilized to realize DRL by taking advantage of the underlying temporal correlation of the DSA network. Using the introduced machine learning-based strategy, SUs could make spectrum access decisions distributedly relying only on their own current and past spectrum sensing outcomes. Through extensive experiments, our results suggest that the RC-based spectrum access strategy can help the SU to significantly reduce the chances of collision with PUs and other SUs. We also show that our scheme outperforms the myopic method which assumes the knowledge of system statistics, and converges faster than the Q-learning method when the number of channels is large.","","","10.1109/JIOT.2018.2872441","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8474348","Deep Q-network (DQN);deep reinforcement learning (DRL);dynamic spectrum access (DSA);echo state network (ESN);reservoir computing (RC);resource allocation","Sensors;Machine learning;Recurrent neural networks;Dynamic spectrum access;Reservoirs;Interference;Training","learning (artificial intelligence);radio spectrum management;recurrent neural nets;signal detection;statistical analysis;telecommunication computing","primary users;SU;communication strategies;distributive DSA network;spectrum sensing errors;deep reinforcement learning;recurrent neural network;RC-based spectrum access strategy;Q-learning method;reservoir computing-based approach;radio spectrum;secondary user;interference coordination;machine learning-based strategy;distributive dynamic spectrum access strategies;PU;DRL;RC","","4","37","","","","","IEEE","IEEE Journals"
"EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification","P. Helber; B. Bischke; A. Dengel; D. Borth","Technische Universität Kaiserslautern, Kaiserslautern, Germany; Technische Universität Kaiserslautern, Kaiserslautern, Germany; Technische Universität Kaiserslautern, Kaiserslautern, Germany; Institute for Computer Science, University of St. Gallen, St. Gallen, Switzerland","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","7","2217","2226","In this paper, we present a patch-based land use and land cover classification approach using Sentinel-2 satellite images. The Sentinel-2 satellite images are openly and freely accessible, and are provided in the earth observation program Copernicus. We present a novel dataset, based on these images that covers 13 spectral bands and is comprised of ten classes with a total of 27 000 labeled and geo-referenced images. Benchmarks are provided for this novel dataset with its spectral bands using state-of-the-art deep convolutional neural networks. An overall classification accuracy of 98.57% was achieved with the proposed novel dataset. The resulting classification system opens a gate toward a number of earth observation applications. We demonstrate how this classification system can be used for detecting land use and land cover changes, and how it can assist in improving geographical maps. The geo-referenced dataset EuroSAT is made publicly available at https://github.com/phelber/eurosat.","","","10.1109/JSTARS.2019.2918242","BMBF Project DeFuseNN; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736785","Dataset;deep convolutional neural network;deep learning;earth observation;land cover classification;land use classification;machine learning;remote sensing;satellite image classification;satellite images","Satellites;Earth;Remote sensing;Machine learning;Spatial resolution;Feature extraction;Benchmark testing","convolutional neural nets;feature extraction;geophysical image processing;image classification;land cover;land use;learning (artificial intelligence);remote sensing","deep learning benchmark;patch-based land use;land cover classification approach;Sentinel-2 satellite images;land cover changes;geo-referenced dataset EuroSAT;geo-referenced images;deep convolutional neural networks;Earth observation program Copernicus;geographical maps","","","38","Traditional","","","","IEEE","IEEE Journals"
"Deep Quadruplet Appearance Learning for Vehicle Re-Identification","J. Hou; H. Zeng; J. Zhu; J. Hou; J. Chen; K. Ma","School of Information Science and Engineering, Huaqiao University, Xiamen, China; School of Information Science and Engineering, Huaqiao University, Xiamen, China; School of Engineering, Huaqiao University, Quanzhou, China; Department of Computer Science, The City University of Hong Kong, Hong Kong; School of Information Science and Engineering, Huaqiao University, Xiamen, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Vehicular Technology","","2019","68","9","8512","8522","Vehicle re-identification (Re-ID) plays an important role in intelligent transportation systems. It usually suffers from various challenges encountered on the real-life environments, such as viewpoint variations, illumination changes, object occlusions, and other complicated scenarios. To effectively improve the vehicle Re-ID performance, a new method, called the deep quadruplet appearance learning (DQAL), is proposed in this paper. The novelty of the proposed DQAL lies on the consideration of the special difficulty in vehicle Re-ID that the vehicles with the same model and color but different identities (IDs) are highly similar to each other. For that, the proposed DQAL designs the concept of quadruplet and forms the quadruplets as the input, where each quadruplet is composed of the anchor (or target), positive, negative, and the specially considered high-similar (i.e., the same model and color but different IDs with respect to the anchor) vehicle samples. Then, the quadruplet network with the incorporation of the proposed quadruplet loss and softmax loss is developed to learn a more discriminative feature for vehicle Re-ID, especially discerning those difficult high-similar cases. Extensive experiments conducted on two commonly used datasets VeRi-776 and VehicleID have demonstrated that the proposed DQAL approach outperforms multiple recently reported vehicle Re-ID methods.","","","10.1109/TVT.2019.2927353","National Natural Science Foundation of China; Natural Science Foundation for Outstanding Young Scholars of Fujian Province; Natural Science Foundation of Fujian Province; Fujian-100 Talented People Program; High-level Talent Innovation Program of Quanzhou City; Promotion Program for Young and Middle-aged Teacher in Science and Technology Research of Huaqiao University; High-Level Talent Project Foundation of Huaqiao University; Subsidized Project for Postgraduates Innovative Fund in Scientific Research of Huaqiao University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758238","Vehicle re-identification;quadruplet network;quadruplet loss;deep learning","Learning systems;Measurement;Image color analysis;Deep learning;Surveillance;Cameras;Convolutional neural networks","feature extraction;image colour analysis;intelligent transportation systems;learning (artificial intelligence);object detection;object recognition;video signal processing","deep quadruplet appearance learning;vehicle re-identification;intelligent transportation systems;quadruplet network;quadruplet loss;softmax loss;DQAL approach;vehicle Re-ID methods","","1","46","Traditional","","","","IEEE","IEEE Journals"
"Deep Learning Denoising Based Line Spectral Estimation","Y. Jiang; H. Li; M. Rangaswamy","Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; AFRL/RYAP, Wright-Patterson Air Force Base, OH, USA","IEEE Signal Processing Letters","","2019","26","11","1573","1577","Many well-known line spectral estimators may experience significant performance loss with noisy measurements. To address the problem, we propose a deep learning denoising based approach for line spectral estimation. The proposed approach utilizes a residual learning assisted denoising convolutional neural network (DnCNN) trained to recover the unstructured noise component, which is used to denoise the original measurements. Following the denoising step, we employ a popular model order selection method and a subspace line spectral estimator to the denoised measurements for line spectral estimation. Numerical results show that the proposed approach outperforms a recently introduced atomic norm minimization based denoising method and offers a substantial improvement compared with the line spectral estimation results obtained by directly applying the subspace estimator without denoising.","","","10.1109/LSP.2019.2939049","Air Force Office of Scientific Research; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8822737","line spectral estimation;signal denoising;deep learning","Noise reduction;Estimation;Training;Convolution;Noise measurement;Deep learning;Atomic measurements","convolutional neural nets;learning (artificial intelligence);minimisation;signal denoising","deep learning denoising based line spectral estimation;line spectral estimators;residual learning;denoising step;subspace line spectral estimator;denoised measurements;line spectral estimation results;subspace estimator;residual learning assisted denoising convolutional neural network","","","29","Traditional","","","","IEEE","IEEE Journals"
"Super-Resolution via Image-Adapted Denoising CNNs: Incorporating External and Internal Learning","T. Tirer; R. Giryes","School of Electrical Engineering, Tel Aviv University, Tel Aviv, Israel; School of Electrical Engineering, Tel Aviv University, Tel Aviv, Israel","IEEE Signal Processing Letters","","2019","26","7","1080","1084","While deep neural networks exhibit state-of-the-art results in the task of image super-resolution (SR) with a fixed known acquisition process (e.g., a bicubic downscaling kernel), they experience a huge performance loss when the real observation model mismatches the one used in training. Recently, two different techniques suggested to mitigate this deficiency, i.e., enjoy the advantages of deep learning without being restricted by the training phase. The first one follows the plug-and-play (P&P) approach that solves general inverse problems (e.g., SR) by using Gaussian denoisers for handling the prior term in model-based optimization schemes. The second builds on internal recurrence of information inside a single image, and trains a super-resolver network at test time on examples synthesized from the low-resolution image. Our letter incorporates these two independent strategies, enjoying the impressive generalization capabilities of deep learning, captured by the first, and further improving it through internal learning at test time. First, we apply a recent P&P strategy to SR. Then, we show how it may become image-adaptive in test time. This technique outperforms the above two strategies on popular datasets and gives better results than other state-of-the-art methods in practical cases where the observation model is inexact or unknown in advance.","","","10.1109/LSP.2019.2920250","H2020 European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8727404","Deep learning;image super-resolution;internal learning;denoising neural network;plug-and-play","Training;Image resolution;Deep learning;Signal resolution;Noise reduction;Optimization;Kernel","image denoising;image resolution;inverse problems;learning (artificial intelligence);neural nets;optimisation","state-of-the-art methods;image-adaptive;impressive generalization capabilities;independent strategies;low-resolution image;test time;super-resolver network;single image;model-based optimization schemes;prior term;Gaussian denoisers;general inverse problems;training phase;deep learning;observation model;huge performance loss;bicubic downscaling kernel;fixed known acquisition process;SR;image super-resolution;state-of-the-art results;deep neural networks;internal learning;incorporating external;image-adapted denoising CNNs;P&P strategy","","2","35","","","","","IEEE","IEEE Journals"
"Unsupervised Deep Learning of Compact Binary Descriptors","K. Lin; J. Lu; C. Chen; J. Zhou; M. Sun","Department of Electrical Engineering, University of Washington, Seattle, WA; Department of Automation, Tsinghua University, Beijing, China; Academia Sinica, Taipei, Taiwan; Department of Automation, Tsinghua University, Beijing, China; Department of Electrical Engineering, University of Washington, Seattle, WA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","6","1501","1514","Binary descriptors have been widely used for efficient image matching and retrieval. However, most existing binary descriptors are designed with hand-craft sampling patterns or learned with label annotation provided by datasets. In this paper, we propose a new unsupervised deep learning approach, called DeepBit, to learn compact binary descriptor for efficient visual object matching. We enforce three criteria on binary descriptors which are learned at the top layer of the deep neural network: 1) minimal quantization loss, 2) evenly distributed codes and 3) transformation invariant bit. Then, we estimate the parameters of the network through the optimization of the proposed objectives with a back-propagation technique. Extensive experimental results on various visual recognition tasks demonstrate the effectiveness of the proposed approach. We further demonstrate our proposed approach can be realized on the simplified deep neural network, and enables efficient image matching and retrieval speed with very competitive accuracies.","","","10.1109/TPAMI.2018.2833865","National Basic Research Program of China (973 Program); Ministry of Science and Technology, Taiwan; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356135","Binary descriptors;unsupervised learning;deep learning;convolutional neural networks","Binary codes;Neural networks;Machine learning;Optimization;Task analysis;Visualization;Quantization (signal)","backpropagation;image matching;image retrieval;image sampling;neural nets;unsupervised learning","hand-craft sampling patterns;unsupervised deep learning approach;compact binary descriptor;deep neural network;visual object matching;binary descriptors;image matching;image retrieval;backpropagation technique","","3","72","","","","","IEEE","IEEE Journals"
"Decentralized Scheduling for Cooperative Localization With Deep Reinforcement Learning","B. Peng; G. Seco-Granados; E. Steinmetz; M. Fröhle; H. Wymeersch","Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Department of Telecommunications and Systems Engineering, IEEC-CERES, Universitat Autonoma de Barcelona, Barcelona, Spain; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden","IEEE Transactions on Vehicular Technology","","2019","68","5","4295","4305","Cooperative localization is a promising solution to the vehicular high-accuracy localization problem. Despite its high potential, exhaustive measurement and information exchange between all adjacent vehicles are expensive and impractical for applications with limited resources. Greedy policies or hand-engineering heuristics may not be able to meet the requirement of complicated use cases. In this paper, we formulate a scheduling problem to improve the localization accuracy (measured through the Cramér-Rao lower bound) of every vehicle up to a given threshold using the minimum number of measurements. The problem is cast as a partially observable Markov decision process and solved using decentralized scheduling algorithms with deep reinforcement learning, which allow vehicles to optimize the scheduling (i.e., the instants to execute measurement and information exchange with each adjacent vehicle) in a distributed manner without a central controlling unit. Simulation results show that the proposed algorithms have a significant advantage over random and greedy policies in terms of both required numbers of measurements to localize all nodes and achievable localization precision with limited numbers of measurements.","","","10.1109/TVT.2019.2913695","Spanish Ministry of Science, Innovation and Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8701533","Machine-learning for vehicular localization;cooperative localization;deep reinforcement learning;deep Q-learning;policy gradient","Reinforcement learning;Information exchange;Markov processes;Sensors;5G mobile communication;Wireless communication","cooperative communication;greedy algorithms;learning (artificial intelligence);Markov processes;radio networks;telecommunication computing","greedy policies;hand-engineering heuristics;decentralized scheduling algorithms;deep reinforcement learning;information exchange;exhaustive measurement;Cramér-Rao lower bound;Markov decision process;cooperative localization;vehicular high-accuracy localization problem","","","44","","","","","IEEE","IEEE Journals"
"Energy-Efficient Distributed Mobile Crowd Sensing: A Deep Learning Approach","C. H. Liu; Z. Chen; Y. Zhan","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Journal on Selected Areas in Communications","","2019","37","6","1262","1276","High-quality data collection is crucial for mobile crowd sensing (MCS) with various applications like smart cities and emergency rescues, where various unmanned mobile terminals (MTs), e.g., driverless cars and unmanned aerial vehicles (UAVs), are equipped with different sensors that aid to collect data. However, they are limited with fixed carrying capacity, and thus, MT's energy resource and sensing range are constrained. It is quite challenging to navigate a group of MTs to move around a target area to maximize their total amount of collected data with the limited energy reserve, while geographical fairness among those point-of-interests (PoIs) should also be maximized. It is even more challenging if fully distributed execution is enforced, where no central control is allowed at the backend. To this end, we propose to leverage emerging deep reinforcement learning (DRL) techniques for directing MT's sensing and movement and to present a novel and highly efficient control algorithm, called energy-efficient distributed MCS (Edics). The proposed neural network integrates convolutional neural network (CNN) for feature extraction and then makes decision under the guidance of multi-agent deep deterministic policy gradient (DDPG) method in a fully distributed manner. We also propose two enhancements into Edics with N-step return and prioritized experienced replay buffer. Finally, we evaluate Edics through extensive simulations and found the appropriate set of hyperparameters in terms of number of CNN hidden layers and neural units for all the fully connected layers. Compared with three commonly used baselines, results have shown its benefits.","","","10.1109/JSAC.2019.2904353","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664596","Mobile crowd sensing;deep reinforcement learning;energy-efficiency;distributed data collection","Sensors;Task analysis;Data collection;Autonomous automobiles;Navigation;Reinforcement learning;Smart phones","convolutional neural nets;energy conservation;feature extraction;learning (artificial intelligence);mobile computing;multi-agent systems;sensor fusion","multiagent deep deterministic policy gradient method;Edics;deep learning approach;smart cities;emergency rescues;fixed carrying capacity;energy reserve;deep reinforcement learning techniques;convolutional neural network;distributed manner;data collection;sensors;distributed execution;efficient control algorithm;energy-efficient distributed MCS;MT energy resource;energy-efficient distributed mobile crowdsensing;CNN;geographical fairness;MT sensing;feature extraction;multi-agent deep deterministic policy gradient method","","2","36","","","","","IEEE","IEEE Journals"
"A Sequence-to-Sequence Deep Learning Architecture Based on Bidirectional GRU for Type Recognition and Time Location of Combined Power Quality Disturbance","Y. Deng; L. Wang; H. Jia; X. Tong; F. Li","Department of Electrical Engineering, Xi'an University of Technology, Xi'an, China; Department of Electrical Engineering, Xi'an University of Technology, Xi'an, China; Department of Electrical Engineering, Xi'an University of Technology, Xi'an, China; Department of Electrical Engineering, Xi'an University of Technology, Xi'an, China; Ningxia Electric Power Research Institute, Yinchuan, China","IEEE Transactions on Industrial Informatics","","2019","15","8","4481","4493","In this paper, a sequence-to-sequence deep learning architecture based on the bidirectional gated recurrent unit (Bi-GRU) for type recognition and time location of combined power quality disturbance is proposed. Especially, the proposed methodology can determine the type of each element in input sequence, which is different from existing sequence-to-sequence model employing encoder-decoder network. First, the input sequence is normalized and batched. Second, deep features are extracted from input sequence by constructing Bi-GRU recurrent neural network, where multiple Bi-GRU layers are stacked together in both forward direction and backward direction. Third, according to aforementioned extracted features, fully connected layer and Softmax are employed to calculate the corresponding probability indicating the category that each element in input sequence is classified to. Fourth, Argmax or Top_K operation is further integrated to determine the type of each element in input sequence by selecting the maximal probability. Finally, the type is recognized, and meanwhile, starting-ending times of disturbances are also located just at the moment when the type is changed. The proposed model is further validated and tested by synthetic signals and practical field signals, respectively. Experimental results demonstrate that the accuracy of type recognition is over 98% for 96 kinds of disturbances including single and combined disturbances with signal-to-noise ration being 20 dB. Besides, the starting-ending times are also located with the absolute error less than six sampling points when sampling frequency is 256 points per cycle with noisy environment.","","","10.1109/TII.2019.2895054","National Natural Science Foundation of China; Scientific Research Projects of Shaan'xi Education Department; Xi'an University of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625549","Bidirectional gated recurrent unit (Bi-GRU);deep learning;power quality disturbance (PQD);sequence-to-sequence model;time location;type recognition","Feature extraction;Logic gates;Deep learning;Transient analysis;Power systems;Harmonic analysis","feature extraction;learning (artificial intelligence);power engineering computing;power supply quality;power system faults;power system harmonics;probability;recurrent neural nets;signal classification;signal sampling","type recognition;time location;combined power quality disturbance;sequence-to-sequence deep learning architecture;sequence-to-sequence model;Bi-GRU recurrent neural network;Bi-GRU layers;bidirectional GRU;bidirectional gated recurrent unit;encoder-decoder network;input sequence normalization;deep feature extraction;Softmax;fully connected layer;input sequence classification;maximal probability;Top_K operation;Argmax;disturbance starting-ending times;sampling frequency\","","1","30","Traditional","","","","IEEE","IEEE Journals"
"Deep Learning-Based Channel Estimation for Massive MIMO Systems","C. Chun; J. Kang; I. Kim","Department of Electrical and Computer Engineering, Queen’s University, Kingston, ON, Canada; School of Intelligent Mechatronics Engineering, Sejong University, Seoul, South Korea; Department of Electrical and Computer Engineering, Queen’s University, Kingston, ON, Canada","IEEE Wireless Communications Letters","","2019","8","4","1228","1231","In this letter, we propose a deep learning (DL)-based channel estimation scheme for the massive multiple-input multiple-output (MIMO) system. Unlike existing studies, we develop the channel estimation scheme for the case that the pilot length is smaller than the number of transmit antennas. The proposed scheme takes a two-stage estimation process: 1) a DL-based pilot-aided channel estimation and 2) a DL-based data-aided channel estimation. In the first stage, the pilot itself and the channel estimator are jointly designed by using both a two-layer neural network (TNN) and a deep neural network (DNN). In the second stage, the accuracy of channel estimation is further enhanced by using another DNN in an iterative manner. The simulation results demonstrate that the proposed channel estimation scheme has much better performance than the conventional channel estimation scheme. We also derive a useful insight into the optimal pilot length given the number of transmit antennas.","","","10.1109/LWC.2019.2912378","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693948","Channel estimation;deep learning;massive MIMO system","Channel estimation;MIMO communication;Transmitting antennas;Neural networks;Receivers;Deep learning;Antenna arrays","channel estimation;iterative methods;learning (artificial intelligence);MIMO communication;neural nets;telecommunication computing","massive multiple-input multiple-output system;two-stage estimation process;pilot-aided channel estimation;data-aided channel estimation;deep neural network;massive MIMO systems;deep learning-based channel estimation scheme","","","13","","","","","IEEE","IEEE Journals"
"Deep Self-Organizing Maps for Unsupervised Image Classification","C. S. Wickramasinghe; K. Amarasinghe; M. Manic","Department of Computer Science, Virginia Commonwealth University, Richmond, VA, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, VA, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, VA, USA","IEEE Transactions on Industrial Informatics","","2019","15","11","5837","5845","The deep self-organizing map (DSOM) was introduced to embed hierarchical feature abstraction capability to self-organizing maps (SOMs). This paper presents an extended version of the original DSOM algorithm (E-DSOM). E-DSOM enhances the DSOM in two ways-learning algorithm is modified to be completely unsupervised, and architecture is modified to learn features of different resolution in hidden layers. E-DSOM has three main advantages over the original DSOM: 1) improved classification accuracy; 2) improved generalization capability; and 3) need of fewer sequential layers (reduced training time). E-DSOM was tested on benchmark and real-world datasets and was compared against DSOM, SOM, sStacked autoencoder (AE), and stacked convolutional autoencoder (CAE). Experimental results showed that the E-DSOM outperformed DSOM with improvements of classification accuracy up to 15% while saving training time up to 19% on all datasets. Moreover, E-DSOM evidenced better generalization capability compared to the DSOM by showing superior performance on all datasets with induced noise. Further, E-DSOM showed comparable performance to the AE and the CAE while outperforming them on two datasets.","","","10.1109/TII.2019.2906083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8669852","Deep learning;deep self-organizing maps (DSOMs);generalization;image classification;unsupervised learning","Self-organizing feature maps;Training;Computer architecture;Neurons;Unsupervised learning;Deep learning;Data mining","image classification;self-organising feature maps;unsupervised learning","deep self-organizing map;unsupervised image classification;hierarchical feature abstraction capability;DSOM algorithm;E-DSOM;stacked autoencoder;stacked convolutional autoencoder;CAE","","1","42","IEEE","","","","IEEE","IEEE Journals"
"Deep and Embedded Learning Approach for Traffic Flow Prediction in Urban Informatics","Z. Zheng; Y. Yang; J. Liu; H. Dai; Y. Zhang","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Faculty of Information Technology, Macau University of Science and Technology, Macau, China; Department of Informatics, University of Oslo, Oslo, Norway","IEEE Transactions on Intelligent Transportation Systems","","2019","20","10","3927","3939","Traffic flow prediction has received extensive attention recently, since it is a key step to prevent and mitigate traffic congestion in urban areas. However, most previous studies on traffic flow prediction fail to capture fine-grained traffic information (like link-level traffic) and ignore the impacts from other factors, such as route structure and weather conditions. In this paper, we propose a deep and embedding learning approach (DELA) that can help to explicitly learn from fine-grained traffic information, route structure, and weather conditions. In particular, our DELA consists of an embedding component, a convolutional neural network (CNN) component and a long short-term memory (LSTM) component. The embedding component can capture the categorical feature information and identify correlated features. Meanwhile, the CNN component can learn the 2-D traffic flow data while the LSTM component has the benefits of maintaining a long-term memory of historical data. The integration of the three models together can improve the prediction accuracy of traffic flow. We conduct extensive experiments on realistic traffic flow dataset to evaluate the performance of our DELA and make comparison with other existing models. The experimental results show that the proposed DELA outperforms the existing methods in terms of prediction accuracy.","","","10.1109/TITS.2019.2909904","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Macao Science and Technology Development Fund; Guangdong Province Universities and Colleges Pearl River Scholar Funded Scheme (2016); European Union’s Horizon 2020 Research and Innovation Programme; Marie Skłodowska-Curie Agreement 824019; Sichuan Science and Technology Program; Program for Guangdong Introducing Innovative and Entrepreneurial Teams; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8694956","Urban informatics;traffic flow prediction;embedding neural networks;deep learning","Meteorology;Predictive models;Urban areas;Deep learning;Sensors;Roads;Informatics","convolutional neural nets;learning (artificial intelligence);road traffic control;traffic engineering computing","traffic flow prediction;traffic congestion;fine-grained traffic information;link-level traffic;route structure;weather conditions;deep learning approach;embedding learning approach;DELA;embedding component;convolutional neural network component;short-term memory component;realistic traffic flow dataset","","","53","","","","","IEEE","IEEE Journals"
"SWiBluX: Multi-Sensor Deep Learning Fingerprint for Precise Real-Time Indoor Tracking","A. Belmonte-Hernández; G. Hernández-Peñaloza; D. Martín Gutiérrez; F. Álvarez","Visual Telecommunications Applications Group, Escuela Tecnica Superior de Ingenieros de Telecomunicacion, Universidad Politecnica de Madrid, Madrid, Spain; Visual Telecommunications Applications Group, Escuela Tecnica Superior de Ingenieros de Telecomunicacion, Universidad Politecnica de Madrid, Madrid, Spain; Visual Telecommunications Applications Group, Escuela Tecnica Superior de Ingenieros de Telecomunicacion, Universidad Politecnica de Madrid, Madrid, Spain; Visual Telecommunications Applications Group, Escuela Tecnica Superior de Ingenieros de Telecomunicacion, Universidad Politecnica de Madrid, Madrid, Spain","IEEE Sensors Journal","","2019","19","9","3473","3486","Indoor/outdoor localization topic has gained a significant research interest due to the wide range of potential applications. Commonly, the Fingerprinting methods for spatial characterization of the environments monitored are employed in deterministic/statistical estimation. However, there are Fingerprint parameters that are generally neglected and can seriously affect the performance yielding to low accurate location. Nowadays, machine and deep learning (DL) methods are employed in this topic due to its ability to approximate complex non-linear models being capable of mitigating the undesirable effects of wireless propagation. In this paper, a complete overview of most influential aspects in Fingerprinting and indoor tracking methods is presented. Furthermore, a novel multi-modal complete tracking system, called SWiBluX, based on statistic and DL techniques is presented. The system relies on relevant feature extraction from available data sources to estimate user's/target indoor position using a multi-phase statistical Fingerprint and DL disruptive approach. In addition, a Gaussian outlier filter is applied to the position estimation model output to further reduce the error in the estimation. The set of experiments performed shows that Fingerprint positioning accuracy estimation can be improved up to 45% resulting in a final estimation error that outperforms related literature.","","","10.1109/JSEN.2019.2892590","European Project: ICT4Life within the H2020 Research and Innovation Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8612930","Indoor positioning;tracking;orientation;fingerprinting;particle filter;wireless;RSSI;IMU;feature vector;machine learning;deep learning;neural network","Fingerprint recognition;Sensors;Estimation;Wireless sensor networks;Wireless communication;Monitoring;Deep learning","feature extraction;filtering theory;Gaussian processes;image colour analysis;indoor radio;learning (artificial intelligence);sensor fusion;statistical analysis;target tracking;wireless sensor networks","real-time indoor tracking;spatial characterization;approximate complex nonlinear models;wireless propagation;indoor tracking methods;statistic DL techniques;position estimation model output;Fingerprint positioning accuracy estimation;indoor-outdoor localization;feature extraction;SWiBluX;multimodal complete tracking system;deterministic-statistical estimation;multisensor deep learning fingerprint;Gaussian outlier filter;multiphase statistical fingerprint","","2","62","","","","","IEEE","IEEE Journals"
"Privacy-Preserving Deep Learning via Weight Transmission","L. T. Phong; T. T. Phuong","National Institute of Information and Communications Technology (NICT), Tokyo, Japan; National Institute of Information and Communications Technology (NICT), Tokyo, Japan","IEEE Transactions on Information Forensics and Security","","2019","14","11","3003","3015","This paper considers the scenario that multiple data owners wish to apply a machine learning method over the combined dataset of all owners to obtain the best possible learning output but do not want to share the local datasets owing to privacy concerns. We design systems for the scenario that the stochastic gradient descent (SGD) algorithm is used as the machine learning method, because SGD (or its variants) is at the heart of recent deep learning techniques over neural networks. Our systems differ from the existing systems in the following features: 1) any activation function can be used, meaning that no privacy-preserving-friendly approximation is required; 2) gradients computed by SGD are not shared but the weight parameters are shared instead; and 3) robustness against colluding parties even in the extreme case that only one honest party exists. One of our systems requires a shared symmetric key among the data owners (trainers) to ensure the secrecy of the weight parameters against a central server. We prove that our systems, while privacy preserving, achieve the same learning accuracy as SGD and, hence, retain the merit of deep learning with respect to accuracy. Finally, we conduct several experiments using benchmark datasets and show that our systems outperform the previous system in terms of learning accuracies.","","","10.1109/TIFS.2019.2911169","Japan Science and Technology Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691476","Privacy preservation;stochastic gradient descent;distributed trainers;neural networks","Servers;Cryptography;Deep learning;Data privacy;Neural networks;Privacy","data privacy;gradient methods;learning (artificial intelligence)","privacy-preserving deep;weight transmission;multiple data owners;machine learning method;combined dataset;possible learning output;local datasets;privacy concerns;design systems;stochastic gradient descent algorithm;SGD;privacy-preserving-friendly approximation;shared symmetric key;learning accuracy;benchmark datasets;deep learning techniques","","","46","","","","","IEEE","IEEE Journals"
"A Multi-View Deep Learning Framework for EEG Seizure Detection","Y. Yuan; G. Xun; K. Jia; A. Zhang","College of Information and Communication Engineering, Beijing University of Technology, Beijing, China; Department of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, USA; College of Information and Communication Engineering, Beijing University of Technology, Beijing, China; Department of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, USA","IEEE Journal of Biomedical and Health Informatics","","2019","23","1","83","94","The recent advances in pervasive sensing technologies have enabled us to monitor and analyze the multi-channel electroencephalogram (EEG) signals of epilepsy patients to prevent serious outcomes caused by epileptic seizures. To avoid manual visual inspection from long-term EEG readings, automatic EEG seizure detection has garnered increasing attention among researchers. In this paper, we present a unified multi-view deep learning framework to capture brain abnormalities associated with seizures based on multi-channel scalp EEG signals. The proposed approach is an end-to-end model that is able to jointly learn multi-view features from both unsupervised multi-channel EEG reconstruction and supervised seizure detection via spectrogram representation. We construct a new autoencoder-based multi-view learning model by incorporating both inter and intra correlations of EEG channels to unleash the power of multi-channel information. By adding a channel-wise competition mechanism in the training phase, we propose a channel-aware seizure detection module to guide our multi-view structure to focus on important and relevant EEG channels. To validate the effectiveness of the proposed framework, extensive experiments against nine baselines, including both traditional handcrafted feature extraction and conventional deep learning methods, are carried out on a benchmark scalp EEG dataset. Experimental results show that the proposed model is able to achieve higher average accuracy and f1-score at 94.37% and 85.34%, respectively, using 5-fold subject-independent cross validation, demonstrating a powerful and effective method in the task of EEG seizure detection.","","","10.1109/JBHI.2018.2871678","National Natural Science Foundation of China; Beijing Natural Science Foundation; Science and Technology Project of Beijing Municipal Education Commission; Beijing Laboratory of Advanced Information Networks; China Scholarship Council Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8470079","Deep learning;epileptic seizure;electroencephalogram;multi-view learning;feature extraction","Electroencephalography;Feature extraction;Machine learning;Brain modeling;Scalp;Informatics;Spectrogram","bioelectric potentials;diseases;electroencephalography;feature extraction;learning (artificial intelligence);medical disorders;medical signal detection;medical signal processing;neurophysiology;signal classification;signal reconstruction","supervised seizure detection;autoencoder-based multiview learning model;channel-wise competition mechanism;channel-aware seizure detection module;multichannel electroencephalogram signals;epileptic seizures;manual visual inspection;long-term EEG readings;automatic EEG seizure detection;unified multiview deep learning framework;multichannel scalp EEG signals;end-to-end model;multiview feature extraction;unsupervised multichannel EEG reconstruction;5-fold subject-independent cross validation","","6","88","","","","","IEEE","IEEE Journals"
"PolSAR Image Semantic Segmentation Based on Deep Transfer Learning—Realizing Smooth Classification With Small Training Sets","W. Wu; H. Li; X. Li; H. Guo; L. Zhang","Key Laboratory of Digital Earth Sciences, Institute of Remote Sensing and Digital Earth (Aerospace Information Research Institute), Chinese Academy of Sciences, Beijing, China; Information Engineering School, Nanchang University, Nanchang, China; Key Laboratory of Digital Earth Sciences, Institute of Remote Sensing and Digital Earth (Aerospace Information Research Institute), Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Sciences, Institute of Remote Sensing and Digital Earth (Aerospace Information Research Institute), Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Sciences, Institute of Remote Sensing and Digital Earth (Aerospace Information Research Institute), Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","6","977","981","Suffering from speckle noise and complex scattering phenomena, classification results of SAR images are usually noisy and shattered, which makes them difficult to use in practical applications. Deep-learning-based semantic segmentation realizes segmentation and categorization at the same time, and thus can obtain smooth and fine-grained classification maps. However, this kind of methods require large data sets with pixel-wise categorical annotations, which are time consuming and tedious to retrieve. Compared with photographs and optical remote sensing images, manually annotating SAR data is even harder, which results in a delay of using relevant techniques in this field. In this letter, a new data set is proposed to support semantic segmentation for high-resolution PolSAR images. Limited by the aforementioned problems, the data set is only a small one with 50 image patches. Therefore, two transfer learning strategies are proposed, which adopt the fully convolutional network (FCN) and U-net architecture, respectively, and use distinct pretraining data sets to adapt to different situations. The experiments demonstrate the good performance of both methods and a promising applicability of using small training sets. Moreover, although trained with small patches, both networks can perfectly apply on large images. The new data set and methods are hopeful to support various PolSAR applications as baselines.","","","10.1109/LGRS.2018.2886559","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8601351","Deep learning;image classification;image segmentation;polarimetry;SAR","Semantics;Image segmentation;Training;Synthetic aperture radar;Deep learning;Data models;Optical sensors","feature extraction;geophysical image processing;image classification;image segmentation;learning (artificial intelligence);radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","training sets;complex scattering phenomena;categorization;fine-grained classification maps;pixel-wise categorical annotations;optical remote sensing images;high-resolution PolSAR images;transfer learning strategies;distinct pretraining data sets;image semantic segmentation;image patches;deep transfer learning","","2","36","","","","","IEEE","IEEE Journals"
"Adaptive Transform Domain Image Super-Resolution via Orthogonally Regularized Deep Networks","T. Guo; H. Seyed Mousavi; V. Monga","Department of Electrical Engineering, Pennsylvania State University, University Park, PA, USA; Department of Electrical Engineering, Pennsylvania State University, University Park, PA, USA; Department of Electrical Engineering, Pennsylvania State University, University Park, PA, USA","IEEE Transactions on Image Processing","","2019","28","9","4685","4700","Deep learning methods, in particular, trained convolutional neural networks (CNNs) have recently been shown to produce compelling results for single image super-resolution (SR). Invariably, a CNN is learned to map the low resolution (LR) image to its corresponding high resolution (HR) version in the spatial domain. We propose a novel network structure for learning the SR mapping function in an image transform domain, specifically the discrete cosine transform (DCT). As the first contribution, we show that DCT can be integrated into the network structure as a convolutional DCT (CDCT) layer. With the CDCT layer, we construct the DCT deep SR (DCT-DSR) network. We further extend the DCT-DSR to allow the CDCT layer to become trainable (i.e., optimizable). Because this layer represents an image transform, we enforce pairwise orthogonality constraints and newly formulated complexity order constraints on the individual basis functions/filters. This orthogonally regularized deep SR network (ORDSR) simplifies the SR task by taking advantage of image transform domain while adapting the design of transform basis to the training image set. The experimental results show ORDSR achieves state-of-the-art SR image quality with fewer parameters than most of the deep CNN methods. A particular success of ORDSR is in overcoming the artifacts introduced by bicubic interpolation. A key burden of deep SR has been identified as the requirement of generous training LR and HR image pairs; ORSDR exhibits a much more graceful degradation as training size is reduced with significant benefits in the regime of limited training. Analysis of memory and computation requirements confirms that ORDSR can allow for a more efficient network with faster inference.","","","10.1109/TIP.2019.2913500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704993","Deep learning;super-resolution;image transform domain;orthogonality constraint;complexity constraint","Discrete cosine transforms;Training;Spatial resolution;Deep learning;Dictionaries","convolutional neural nets;discrete cosine transforms;image resolution;interpolation;learning (artificial intelligence)","convolutional neural networks;ORDSR;SR image quality;SR image quality;deep learning methods;high resolution image;image transform domain;discrete cosine transform;bicubic interpolation;deep CNN methods;transform basis;orthogonally regularized deep SR network;pairwise orthogonality constraints;DCT-DSR;DCT deep SR network;CDCT layer;convolutional DCT layer;SR mapping function;network structure;spatial domain;low resolution image;single image super-resolution;orthogonally regularized deep networks;adaptive transform domain image super-resolution","","1","74","","","","","IEEE","IEEE Journals"
"SSIM—A Deep Learning Approach for Recovering Missing Time Series Sensor Data","Y. Zhang; P. J. Thorburn; W. Xiang; P. Fitch","Department of Agriculture and Food, CSIRO, Brisbane, QLD, Australia; Department of Agriculture and Food, CSIRO, Brisbane, QLD, Australia; College of Science and Engineering, James Cook University, Cairns, QLD, Australia; Department of Land and Water, CSIRO, Canberra, ACT, Australia","IEEE Internet of Things Journal","","2019","6","4","6618","6628","Missing data are unavoidable in wireless sensor networks, due to issues such as network communication outage, sensor maintenance or failure, etc. Although a plethora of methods have been proposed for imputing sensor data, limitations still exist. First, most methods give poor estimates when a consecutive number of data are missing. Second, some methods reconstruct missing data based on other parameters monitored simultaneously. When all the data are missing, these methods are no longer effective. Third, the performance of deep learning methods relies highly on a massive number of training data. Moreover in many scenarios, it is difficult to obtain large volumes of data from wireless sensor networks. Hence, we propose a new sequence-to-sequence imputation model (SSIM) for recovering missing data in wireless sensor networks. The SSIM uses the state-of-the-art sequence-to-sequence deep learning architecture, and the long short-term memory network is chosen to utilize both past and future information for a given time. Moreover, a variable-length sliding window algorithm is developed to generate a large number of training samples so the SSIM can be trained with small data sets. We evaluate the SSIM by using real-world time series data from a water quality monitoring network. Compared to methods like ARIMA, seasonal ARIMA, matrix factorization, multivariate imputation by chained equations, and expectation-maximization, the proposed SSIM achieves up to 69.2%, 70.3%, 98.3%, and 76% improvements in terms of the root mean square error, mean absolute error, mean absolute percentage error (MAPE), and symmetric MAPE, respectively, when recovering missing data sequences of three different lengths. The SSIM is therefore a promising approach for data quality control in wireless sensor networks.","","","10.1109/JIOT.2019.2909038","CSIRO Digiscape Future Science Platform; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8681112","Deep learning;imputation;sequence-to-sequence;time series;wireless sensor networks","Time series analysis;Data models;Wireless sensor networks;Deep learning;Decoding;Indexes;Monitoring","data handling;mean square error methods;neural nets;quality control;time series;wireless sensor networks","real-world time series data;water quality monitoring network;data quality control;wireless sensor networks;network communication outage;sensor maintenance;deep learning methods;sequence-to-sequence imputation model;state-of-the-art sequence-to-sequence deep learning architecture;short-term memory network;data sets;SSIM;missing time series sensor data recovery;seasonal ARIMA;matrix factorization;multivariate imputation;chained equations;expectation-maximization;mean absolute percentage error;root mean square error;mean absolute error;symmetric MAPE","","","37","","","","","IEEE","IEEE Journals"
"Real-Time Deep Tracking via Corrective Domain Adaptation","H. Li; X. Wang; F. Shen; Y. Li; F. Porikli; M. Wang","School of Computer and Information Engineering, Jiangxi Normal University, Nanchang, China; Australian Institute for Machine Learning (AIML), The University of Adelaide, Adelaide, SA, Australia; Center for Future Media, University of Electronic Science and Technology of China, Chengdu, China; Google Brain and X, Moonshot Factory, Mountain View, CA, USA; Research School of Engineering, The Australian National University, Canberra, ACT, Australia; School of Computer and Information Engineering, Jiangxi Normal University, Nanchang, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","9","2600","2612","Visual tracking is one of the fundamental problems in computer vision. Recently, some deep-learning-based tracking algorithms have been illustrating record-breaking performances. However, due to the high complexity of neural networks, most deep trackers suffer from low tracking speed and are, thus, impractical in many real-world applications. Some recently proposed deep trackers with smaller network structure achieve high efficiency while at the cost of significant decrease in precision. In this paper, we propose to transfer the deep feature, which is learned originally for image classification to the visual tracking domain. The domain adaptation is achieved via some “grafted” auxiliary networks, which are trained by regressing the object location in tracking frames. This adaptation improves the tracking performance significantly both on accuracy and efficiency. The yielded deep tracker is real time and also illustrates the state-of-the-art accuracies in the experiment involving two well-adopted benchmarks with more than 100 test videos. Furthermore, the adaptation is also naturally used for introducing the objectness concept into visual tracking. This removes a long-standing target ambiguity in visual tracking tasks, and we illustrate the empirical superiority of the more well-defined task.","","","10.1109/TCSVT.2019.2923639","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8740907","Visual tracking;deep learning;real-time","Target tracking;Visualization;Feature extraction;Real-time systems;Detectors;Task analysis;Deep learning","computer vision;feature extraction;image classification;learning (artificial intelligence);neural nets;object tracking;video signal processing","deep feature;visual tracking domain;grafted auxiliary networks;tracking performance;visual tracking tasks;corrective domain adaptation;computer vision;tracking algorithms;record-breaking performances;neural networks;deep tracking","","","43","","","","","IEEE","IEEE Journals"
"Highly Accurate Machine Fault Diagnosis Using Deep Transfer Learning","S. Shao; S. McAleer; R. Yan; P. Baldi","School of Instrument Science and Engineering, Southeast University, Nanjing, China; Department of statistics, Department of Computer Science, School of Information and Computer Science, University of California, Irvine, USA; School of Instrument Science and Engineering, Southeast University, Nanjing, China; Department of statistics, Department of Computer Science, School of Information and Computer Science, University of California, Irvine, USA","IEEE Transactions on Industrial Informatics","","2019","15","4","2446","2455","We develop a novel deep learning framework to achieve highly accurate machine fault diagnosis using transfer learning to enable and accelerate the training of deep neural network. Compared with existing methods, the proposed method is faster to train and more accurate. First, original sensor data are converted to images by conducting a Wavelet transformation to obtain time-frequency distributions. Next, a pretrained network is used to extract lower level features. The labeled time-frequency images are then used to fine-tune the higher levels of the neural network architecture. This paper creates a machine fault diagnosis pipeline and experiments are carried out to verify the effectiveness and generalization of the pipeline on three main mechanical datasets including induction motors, gearboxes, and bearings with sizes of 6000, 9000, and 5000 time series samples, respectively. We achieve state-of-the-art results on each dataset, with most datasets showing test accuracy near 100%, and in the gearbox dataset, we achieve significant improvement from 94.8% to 99.64%. We created a repository including these datasets located at mlmechanics.ics.uci.edu.","","","10.1109/TII.2018.2864759","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities and Research Innovation Program for College Graduates of Jiangsu Province; Defense Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8432110","Convolutional neural network (CNN);deep learning (DL);fault diagnosis;machine health monitoring;pretrained model;transfer learning (TL)","Fault diagnosis;Wavelet transforms;Convolution;Time-frequency analysis;Kernel;Machine learning;Training","condition monitoring;fault diagnosis;feature extraction;image representation;learning (artificial intelligence);mechanical engineering computing;neural net architecture;time series;time-frequency analysis;wavelet transforms","pretrained network;labeled time-frequency images;neural network architecture;machine fault diagnosis pipeline;deep transfer learning;deep neural network;time-frequency distributions","","13","31","","","","","IEEE","IEEE Journals"
"Applying Deep Learning to Hail Detection: A Case Study","M. Pullman; I. Gurung; M. Maskey; R. Ramachandran; S. A. Christopher","U.S. Army Corps of Engineers Vicksburg District, Vicksburg, MS, USA; Inter Agency Implementation and Advanced Concepts (IMPACT), NASA Marshall Space Flight Center, Huntsville, AL, USA; NASA Marshall Space Flight Center, Huntsville, AL, USA; NASA Marshall Space Flight Center, Huntsville, AL, USA; Department of Atmospheric Science, University of Alabama in Huntsville, Huntsville, AL, USA","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","12","10218","10225","Deep learning is a subset of machine learning that uses deep neural networks (DNNs) capable of learning representations and extracting valuable information from vast data sets. Similarly, weather phenomena are often identified by patterns in data that serve as precursor signatures. Therefore, deep learning networks can be used to identify signatures of the weather phenomena, or possibly signatures not yet established by forecasters in addition to aiding forecasters in synthesizing the growing amount of meteorological observations. In this article, we demonstrate the value of deep learning for atmospheric science applications by providing a proof of concept, using deep learning for the detection of hail-bearing storms as a test case study. The deep learning network presented in this article obtains a higher precision when presented with multisource data and is able to identify a common feature associated with hail storms-decreased infrared brightness temperatures. This network and case study illustrate the capability of deep networks for the detection of weather phenomena and contribute to the growing awareness of deep learning among atmospheric scientists.","","","10.1109/TGRS.2019.2931944","National Aeronautics and Space Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818638","Artificial intelligence;event detection;neural networks","Deep learning;Storms;Weather forecasting;Satellites;Spaceborne radar","atmospheric precipitation;geophysics computing;ice;learning (artificial intelligence);neural nets;storms;weather forecasting","deep learning network;weather phenomena;machine learning;deep neural networks;hail detection;atmospheric science applications;hail-bearing storms;multisource data;infrared brightness temperatures","","","27","IEEE","","","","IEEE","IEEE Journals"
"Deep Learning From Multiple Crowds: A Case Study of Humanitarian Mapping","J. Chen; Y. Zhou; A. Zipf; H. Fan","GIScience, University of Heidelberg, Heidelberg, Germany; GIScience, University of Heidelberg, Heidelberg, Germany; GIScience, University of Heidelberg, Heidelberg, Germany; School of Remote Sensing and Information, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","3","1713","1722","Satellite images are widely applied in humanitarian mapping that labels buildings, roads, and so on for humanitarian aid and economic development. However, the labeling now is mostly done by volunteers. In this paper, we utilize deep learning to solve humanitarian mapping tasks of a mobile software named MapSwipe. The current deep learning techniques, e.g., convolutional neural network (CNN), can recognize ground objects from satellite images but rely on numerous labels for training for each specific task. We solve this problem by fusing multiple freely accessible crowdsourced geographic data and propose an active learning-based CNN training framework named MC-CNN to deal with the quality issues of the labels extracted from these data, including incompleteness (e.g., some kinds of object are not labeled) and heterogeneity (e.g., different spatial granularities). The method is evaluated with building mapping in South Malawi and road mapping in Guinea with level-18 satellite images provided by Bing Map and volunteered geographic information from OpenStreetMap, MapSwipe, and OsmAnd. The results based on multiple metrics, including Precision, Recall, F1 Score, and area under the receiver operating characteristic curve, show that MC-CNN can fuse the crowdsourced labels for higher prediction performance and be successfully applied in MapSwipe for humanitarian mapping with 85% labor saved and an overall accuracy of 0.86 achieved.","","","10.1109/TGRS.2018.2868748","Klaus Tschira Foundation, Heidelberg; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8480860","Active learning;deep learning;humanitarian mapping;satellite image;volunteered geographic information (VGI)","Satellites;Roads;Machine learning;Training;Buildings;Labeling;Task analysis","convolutional neural nets;crowdsourcing;geographic information systems;learning (artificial intelligence);mobile computing","multiple crowds;humanitarian aid;economic development;deep learning;humanitarian mapping tasks;mobile software;MapSwipe;convolutional neural network;ground objects;numerous labels;multiple freely accessible crowdsourced geographic data;active learning-based CNN training framework;MC-CNN;building mapping;road mapping;level-18 satellite images;Bing Map;multiple metrics;crowdsourced labels;OsmAnd;OpenStreetMap","","1","41","","","","","IEEE","IEEE Journals"
"Experience-Driven Congestion Control: When Multi-Path TCP Meets Deep Reinforcement Learning","Z. Xu; J. Tang; C. Yin; Y. Wang; G. Xue","Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, USA; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, USA; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Ira A. Fulton Schools of Engineering, Arizona State University, Tempe, AZ, USA","IEEE Journal on Selected Areas in Communications","","2019","37","6","1325","1336","In this paper, we aim to study networking problems from a whole new perspective by leveraging emerging deep learning, to develop an experience-driven approach, which enables a network or a protocol to learn the best way to control itself from its own experience (e.g., runtime statistics data), just as a human learns a skill. We present design, implementation and evaluation of a deep reinforcement learning (DRL)-based control framework, DRL-CC (DRL for Congestion Control), which realizes our experience-driven design philosophy on multi-path TCP (MPTCP) congestion control. DRL-CC utilizes a single (instead of multiple independent) agent to dynamically and jointly perform congestion control for all active MPTCP flows on an end host with the objective of maximizing the overall utility. The novelty of our design is to utilize a flexible recurrent neural network, LSTM, under a DRL framework for learning a representation for all active flows and dealing with their dynamics. Moreover, we, for the first time, integrate the above LSTM-based representation network into an actor-critic framework for continuous (congestion) control, which leverages the emerging deterministic policy gradient to train critic, actor, and LSTM networks in an end-to-end manner. We implemented DRL-CC based on the MPTCP implementation in the Linux kernel. The experimental results show that 1) DRL-CC consistently and significantly outperforms a few well-known MPTCP congestion control algorithms in terms of goodput without sacrificing fairness, 2) it is flexible and robust to highly-dynamic network environments with time-varying flows, and 3) it is friendly to regular TCP.","","","10.1109/JSAC.2019.2904358","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664598","AI;deep learning;experience-driven control;congestion control;TCP;multi-path TCP","Runtime;Mathematical model;Reinforcement learning;Protocols;Heuristic algorithms;Recurrent neural networks;Resource management","learning (artificial intelligence);Linux;recurrent neural nets;telecommunication congestion control;transport protocols","networking problems;deep learning;DRL-CC;experience-driven design philosophy;multipath TCP congestion control;active MPTCP;flexible recurrent neural network;DRL framework;LSTM-based representation network;actor-critic framework;LSTM networks;MPTCP congestion control algorithms;highly-dynamic network environments;multipath TCP;deep reinforcement learning","","3","46","","","","","IEEE","IEEE Journals"
"Ordinal Deep Learning for Facial Age Estimation","H. Liu; J. Lu; J. Feng; J. Zhou","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","2","486","501","In this paper, we propose an ordinal deep learning approach for facial age estimation. Unlike conventional hand-crafted feature-based methods that require prior and expert knowledge, we propose an ordinal deep feature learning (ODFL) method to learn feature descriptors for face representation directly from raw pixels. Motivated by the fact that age labels are chronologically correlated and age estimation is an ordinal learning problem, our proposed ODFL enforces two criteria on the descriptors, which are learned at the top of the deep networks: 1) the topology-preserving ordinal relation is employed to exploit the order information in the learned feature space and 2) the age-difference cost information is leveraged to dynamically measure face pairs with different age value gaps. However, both the procedures of feature extraction and age estimation are learned independently in ODFL, which may lead to a sub-optimal problem. To address this, we further propose an end-to-end ordinal deep learning (ODL) framework, where the complementary information of both the procedures is exploited to reinforce our model. Extensive experimental results on five face aging datasets show that both our ODFL and ODL achieve superior performance in comparisons with most state-of-the-art methods.","","","10.1109/TCSVT.2017.2782709","National Natural Science Foundation of China; National 1000 Young Talents Plan Program; National Basic Research Program of China; Ministry of Education of China; Shenzhen Fundamental Research Fund (Subject Arrangement); Tsinghua University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8187689","Facial age estimation;deep learning;feature learning;ordinal embedding","Face;Estimation;Machine learning;Training;Aging","face recognition;feature extraction;image representation;learning (artificial intelligence);topology","ordinal deep feature learning method;conventional hand-crafted feature-based methods;facial age estimation;face aging datasets;feature extraction;age-difference cost information;topology-preserving ordinal relation;ODFL","","1","84","","","","","IEEE","IEEE Journals"
"Comments on “Traffic Sign Recognition Using Kernel Extreme Learning Machines With Deep Perceptual Features”","S. Jain; M. Singhal; S. Shukla","Maulana Azad National Institute of Technology, Bhopal, India; Maulana Azad National Institute of Technology, Bhopal, India; Maulana Azad National Institute of Technology, Bhopal, India","IEEE Transactions on Intelligent Transportation Systems","","2019","20","10","3759","3761","This paper comments on the recently published article entitled “Traffic sign recognition using kernel extreme learning machines with deep perceptual features”. This paper presents a disagreement on the claim of the above-mentioned article related to the reduction in computational complexity. This paper also suggests not to use a single notation for two different terms.","","","10.1109/TITS.2018.2850057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8409297","Traffic sign recognition;extreme learning machine;kernel extreme learning machine;kernel matrix","Mathematical model;Kernel;Training;Computational modeling;Computational complexity;Feature extraction","computational complexity;feature extraction;feedforward neural nets;image classification;learning (artificial intelligence);traffic engineering computing","traffic sign recognition;kernel extreme learning machines;deep perceptual features;computational complexity","","","12","","","","","IEEE","IEEE Journals"
"EdgeSanitizer: Locally Differentially Private Deep Inference at the Edge for Mobile Data Analytics","C. Xu; J. Ren; L. She; Y. Zhang; Z. Qin; K. Ren","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; Mobile E-Business Collaborative Innovation Center of Hunan Province, Hunan University of Commerce, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; Institute of Cyberspace Research, Zhejiang University, Hangzhou, China; Institute of Cyberspace Research, Zhejiang University, Hangzhou, China","IEEE Internet of Things Journal","","2019","6","3","5140","5151","Deep neural networks have been widely applied in various machine learning applications for mobile data analytics in cloud. However, this approach introduces significant data challenges, because the cloud operator can perform deep inferences on the available data. Recent advances in edge computing have paved the way to more efficient and private data processing at the edge of the network for simple tasks and lightweight models, but challenges still remain in building efficient complex models (e.g., deep learning) for edge computing. To tackle these issues, we propose EdgeSanitizer, a deep inference framework-based edge computing with local differential privacy for mobile data analytics. EdgeSanitizer leverages deep learning model to conduct data minimization and obfuscates the learned features by adaptively injecting noise, thereby forming a new protection layer against sensitive inference. We evaluate its performance in terms of data privacy and utility through theoretical analysis and experimental evaluation. The theoretical analysis proves that EdgeSanitizer can provide provable privacy guarantees with a large improvement in utility. And the experimental results demonstrate the robustness of our approach against sensitive inference, as well as its applicability on resource-constrained edge devices.","","","10.1109/JIOT.2019.2897005","Young and Middle-Aged Scientific Research Project in the Department of Education of Fujian; National Natural Science Foundation of China; 111 Project; Scientific Research Fund of Hunan Education Department; Central South University; International Science and Technology Cooperation Program of China; China Hunan Provincial Science and Technology Program; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8632695","Deep inference;edge computing;local differential privacy (LDP);mobile data analytics","Feature extraction;Data analysis;Differential privacy;Deep learning;Edge computing;Data models","cloud computing;data analysis;data privacy;learning (artificial intelligence);mobile computing;neural nets","edge computing;local differential privacy;mobile data analytics;data minimization;sensitive inference;data privacy;resource-constrained edge devices;deep neural networks;machine learning applications;deep inferences;private data processing;deep inference framework-based edge computing;edgesanitizer leverages deep learning model;cloud operator","","","46","","","","","IEEE","IEEE Journals"
"Scheduling the Operation of a Connected Vehicular Network Using Deep Reinforcement Learning","R. F. Atallah; C. M. Assi; M. J. Khabbaz","CIISE, Concordia University, Montreal, QC, Canada; CIISE, Concordia University, Montreal, QC, Canada; ECCE Department, Notre Dame University of Louaize, Zouk Mosbeh, Lebanon","IEEE Transactions on Intelligent Transportation Systems","","2019","20","5","1669","1682","Driven by the expeditious evolution of the Internet of Things, the conventional vehicular ad hoc networks will progress toward the Internet of Vehicles (IoV). With the rapid development of computation and communication technologies, IoV promises huge commercial interest and research value, thereby attracting a large number of companies and researchers. In an effort to satisfy the driver's well-being and demand for continuous connectivity in the IoV era, this paper addresses both safety and quality-of-service (QoS) concerns in a green, balanced, connected, and efficient vehicular network. Using the recent advances in training deep neural networks, we exploit the deep reinforcement learning model, namely deep Q-network, which learns a scheduling policy from high-dimensional inputs corresponding to the current characteristics of the underlying model. The realized policy serves to extend the lifetime of the battery-powered vehicular network while promoting a safe environment that meets acceptable QoS levels. Our presented deep reinforcement learning model is found to outperform several scheduling benchmarks in terms of completed request percentage (10-25%), mean request delay (10-15%), and total network lifetime (5-65%).","","","10.1109/TITS.2018.2832219","Natural Sciences and Engineering Research Council of Canada; Concordia University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8365853","internet of Vehicles;Deep reinforcement learning;scheduling","Machine learning;Optimal scheduling;Internet;Quality of service;Roads;Vehicle dynamics;Safety","intelligent transportation systems;Internet of Things;learning (artificial intelligence);neural nets;quality of service;scheduling;traffic engineering computing;vehicular ad hoc networks","connected vehicular network;quality-of-service concerns;deep Q-network;scheduling policy;battery-powered vehicular network;vehicular ad hoc networks;deep neural network training;QoS levels;deep reinforcement learning;Internet of Things;Internet of Vehicles;intelligent transportation system","","5","48","","","","","IEEE","IEEE Journals"
"Deep Few-Shot Learning for Hyperspectral Image Classification","B. Liu; X. Yu; A. Yu; P. Zhang; G. Wan; R. Wang","Institute of Surveying and Mapping, Information Engineering University, Zhengzhou, China; Institute of Surveying and Mapping, Information Engineering University, Zhengzhou, China; Institute of Surveying and Mapping, Information Engineering University, Zhengzhou, China; Institute of Surveying and Mapping, Information Engineering University, Zhengzhou, China; Institute of Surveying and Mapping, Information Engineering University, Zhengzhou, China; Institute of Surveying and Mapping, Information Engineering University, Zhengzhou, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","4","2290","2304","Deep learning methods have recently been successfully explored for hyperspectral image (HSI) classification. However, training a deep-learning classifier notoriously requires hundreds or thousands of labeled samples. In this paper, a deep few-shot learning method is proposed to address the small sample size problem of HSI classification. There are three novel strategies in the proposed algorithm. First, spectral-spatial features are extracted to reduce the labeling uncertainty via a deep residual 3-D convolutional neural network. Second, the network is trained by episodes to learn a metric space where samples from the same class are close and those from different classes are far. Finally, the testing samples are classified by a nearest neighbor classifier in the learned metric space. The key idea is that the designed network learns a metric space from the training data set. Furthermore, such metric space could generalize to the classes of the testing data set. Note that the classes of the testing data set are not seen in the training data set. Four widely used HSI data sets were used to assess the performance of the proposed algorithm. The experimental results indicate that the proposed method can achieve better classification accuracy than the conventional semisupervised methods with only a few labeled samples.","","","10.1109/TGRS.2018.2872830","National Natural Science Foundation of China; Scientific and Technological Project in Henan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8506606","3-D convolutional neural networks (3-D CNNs);few-shot learning;hyperspectral image (HSI) classification;residual learning","Feature extraction;Training;Extraterrestrial measurements;Testing;Hyperspectral sensors","convolutional neural nets;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);remote sensing;stereo image processing","hyperspectral image classification;deep-learning classifier;few-shot learning method;HSI classification;deep residual 3-D convolutional neural network;nearest neighbor classifier;conventional semisupervised methods;spectral-spatial feature extraction;deep few-shot learning","","4","62","","","","","IEEE","IEEE Journals"
"3-D PersonVLAD: Learning Deep Global Representations for Video-Based Person Reidentification","L. Wu; Y. Wang; L. Shao; M. Wang","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","11","3347","3359","We present the global deep video representation learning to video-based person reidentification (re-ID) that aggregates local 3-D features across the entire video extent. Existing methods typically extract frame-wise deep features from 2-D convolutional networks (ConvNets) which are pooled temporally to produce the video-level representations. However, 2-D ConvNets lose temporal priors immediately after the convolutions, and a separate temporal pooling is limited in capturing human motion in short sequences. In this paper, we present global video representation learning, to be complementary to 3-D ConvNets as a novel layer to capture the appearance and motion dynamics in full-length videos. Nevertheless, encoding each video frame in its entirety and computing aggregate global representations across all frames is tremendously challenging due to the occlusions and misalignments. To resolve this, our proposed network is further augmented with the 3-D part alignment to learn local features through the soft-attention module. These attended features are statistically aggregated to yield identity-discriminative representations. Our global 3-D features are demonstrated to achieve the state-of-the-art results on three benchmark data sets: MARS, Imagery Library for Intelligent Detection Systems-Video Re-identification, and PRID2011.","","","10.1109/TNNLS.2019.2891244","Fusion of Digital Microscopy and Plain Text Reports through Improved Pathology; National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8632948","3-D convolution;global representations;person reidentification (re-ID);vector of local aggregated descriptors (VLAD)","Feature extraction;Spatiotemporal phenomena;Neural networks;Streaming media;Solid modeling;Learning systems;Computer science","convolutional neural nets;feature extraction;image matching;image motion analysis;image representation;image sequences;learning (artificial intelligence);object detection;video signal processing","full-length videos;video frame;aggregate global representations;3-D part alignment;local features;attended features;identity-discriminative representations;deep global representations;video-based person reidentification;global deep video representation learning;entire video extent;frame-wise deep features;2-D convolutional networks;video-level representations;temporal priors;separate temporal pooling;global video representation learning;intelligent detection systems-video re-identification;3-D ConvNets","","6","75","","","","","IEEE","IEEE Journals"
"Protein–Protein Interactions Prediction via Multimodal Deep Polynomial Network and Regularized Extreme Learning Machine","H. Lei; Y. Wen; Z. You; A. Elazab; E. Tan; Y. Zhao; B. Lei","School of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; School of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Xinjiang Technical Institutes of Physics and Chemistry, Chinese Academy of Science, Urumqi, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, China; School of Electronic and Electric Engineering, Nanyang Technological University, Singapore; School of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, China","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","1290","1303","Predicting the protein-protein interactions (PPIs) has played an important role in many applications. Hence, a novel computational method for PPIs prediction is highly desirable. PPIs endow with protein amino acid mutation rate and two physicochemical properties of protein (e.g., hydrophobicity and hydrophilicity). Deep polynomial network (DPN) is well-suited to integrate these modalities since it can represent any function on a finite sample dataset via the supervised deep learning algorithm. We propose a multimodal DPN (MDPN) algorithm to effectively integrate these modalities to enhance prediction performance. MDPN consists of a two-stage DPN, the first stage feeds multiple protein features into DPN encoding to obtain high-level feature representation while the second stage fuses and learns features by cascading three types of high-level features in the DPN encoding. We employ a regularized extreme learning machine to predict PPIs. The proposed method is tested on the public dataset of H. pylori, Human, and Yeast and achieves average accuracies of 97.87%, 99.90%, and 98.11%, respectively. The proposed method also achieves good accuracies on other datasets. Furthermore, we test our method on three kinds of PPI networks and obtain superior prediction results.","","","10.1109/JBHI.2018.2845866","National Natural Science Foundation of Guangdong Province; Shenzhen Key Basic Research Project; Innovation and Entrepreneurship Training Program for College Students; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8382154","Protein-protein interactions;prediction;multimodal deep polynomial network;regularization extreme learning machine","Amino acids;Feature extraction;Protein engineering;Machine learning;Encoding;Protein sequence","bioinformatics;feature extraction;learning (artificial intelligence);molecular biophysics;network theory (graphs);proteins","high-level feature representation;DPN encoding;PPI networks;protein-protein interactions prediction;multimodal deep polynomial network;PPIs prediction;finite sample dataset;supervised deep learning algorithm;multimodal DPN algorithm;MDPN;two-stage DPN;multiple protein features;extreme learning machine;protein amino acid mutation rate;physicochemical properties","","","74","","","","","IEEE","IEEE Journals"
"Auction-Based Charging Scheduling With Deep Learning Framework for Multi-Drone Networks","M. Shin; J. Kim; M. Levorato","School of Computer Science and Engineering, Chung-Ang University, Seoul, South Korea; School of Computer Science and Engineering, Chung-Ang University, Seoul, South Korea; Department of Computer Science, Donald Bren School of Information and Computer Sciences, University of California, Irvine, Irvine, CA, USA","IEEE Transactions on Vehicular Technology","","2019","68","5","4235","4248","State-of-the-art drone technologies have severe flight time limitations due to weight constraints, which inevitably lead to a relatively small amount of available energy. Therefore, frequent battery replacement or recharging is necessary in applications such as delivery, exploration, or support to the wireless infrastructure. Mobile charging stations (i.e., mobile stations with charging equipment) for outdoor ad-hoc battery charging is one of the feasible solutions to address this issue. However, the ability of these platforms to charge the drones is limited in terms of the number and charging time. This paper designs an auction-based mechanism to control the charging schedule in multi-drone setting. In this paper, charging time slots are auctioned, and their assignment is determined by a bidding process. The main challenge in developing this framework is the lack of prior knowledge on the distribution of the number of drones participating in the auction. Based on optimal second-price-auction, the proposed formulation, then, relies on deep learning algorithms to learn such distribution online. Numerical results from extensive simulations show that the proposed deep-learning-based approach provides effective battery charging control in multi-drone scenarios.","","","10.1109/TVT.2019.2903144","Institute for Information & Communications Technology Promotion; Korea government; Virtual Presence in Moving Objects through 5G; Chung-Ang University Graduate Research Scholarship in 2018; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660495","Auction;deep learning;charging;drone networks;unmanned aerial vehicle (UAV)","Drones;Charging stations;Deep learning;Batteries;Processor scheduling;Feature extraction;Resource management","battery management systems;battery powered vehicles;electric vehicle charging;learning (artificial intelligence);remotely operated vehicles","wireless infrastructure;mobile charging stations;outdoor ad-hoc battery charging;auction-based mechanism;deep learning algorithms;auction-based charging scheduling;multidrone networks;battery replacement;battery charging control;drone technologies","","","42","","","","","IEEE","IEEE Journals"
"A Combined Deep Learning Model for the Scene Classification of High-Resolution Remote Sensing Image","Y. Dong; Q. Zhang","Department of Computer Science and Technology, East China Normal University, Shanghai, China; Department of Computer Science and Technology, East China Normal University, Shanghai, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","10","1540","1544","Deep learning now plays an important role in solving complex problems in computer vision fields. The highly challenging high-resolution remote sensing image scene classification problem can also be solved using deep learning methods. The most commonly used method of deep learning is the convolutional neural network model. In this letter, based on deep learning, a combined model named Inception-long short-term memory (LSTM) is proposed. First, we combine the deep learning feature extracted from the pretrained Inception-V3 model with a hand-crafted feature: the GIST feature. The different features are then combined and input into the batch normalization (BN) layer. Second, the BN layer plays the role of the bridge to combine the InceptionV3 model with the LSTM model, which features a softmax classifier. The LSTM model is used to analyze the features and classify the different high-resolution remote sensing scene images. The proposed model, as a whole, can be uniformly trained. Three different datasets-the NWPU-RESISC45 dataset, the UC Merced dataset, and the SIRI-WHU dataset-were used to verify the effectiveness of the proposed model. The results show that the proposed Inception-LSTM model shows an outstanding performance in the scene classification task.","","","10.1109/LGRS.2019.2902675","National Natural Science Foundation of China; Engineering Research Center of Geospatial Information and Digital Technology, NASG; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673564","GIST;Inception-V3;long short-term memory (LSTM);pretraining;scene classification","Feature extraction;Deep learning;Remote sensing;Logic gates;Analytical models;Computer architecture;Semantics","computer vision;convolutional neural nets;feature extraction;geophysical image processing;image classification;image resolution;learning (artificial intelligence);remote sensing","high-resolution remote sensing image;deep learning methods;convolutional neural network model;Inception-long short-term memory;pretrained Inception-V3 model;hand-crafted feature;Inception-LSTM model;scene classification task;high-resolution remote sensing scene images;combined deep learning model;GIST feature;NWPU-RESISC45 dataset;UC Merced dataset;SIRI-WHU dataset;batch normalization layer;softmax classifier","","","27","","","","","IEEE","IEEE Journals"
"A Deep Learning Approach for Probabilistic Security in Multi-Robot Teams","R. Wehbe; R. K. Williams","Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA; Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA","IEEE Robotics and Automation Letters","","2019","4","4","4262","4269","In this letter, we train a convolutional neural network (CNN) to predict the probability of security of a multi-robot system (MRS) when robot interactions are probabilistic. In the context of MRSs, probabilistic security is defined using the control-theoretic notion of left invertibility, a necessary and sufficient condition to avoid perfect attacks. As the probabilistic security problem is NP-Complete, current solutions fail to generalize as the size of the MRS increases. Fortunately, deep neural networks have shown promising results in the efficient computation of solutions to hard problems, which motivates our CNN-based approach. In this context, formulating a method for data generation is non-trivial due to the large space of available interaction graph topologies and training biases introduced by random sampling. As such, we use a two-step approach for data generation where we first explore the space of available topologies, then populate the sampled topologies with probability distributions, all while preventing any biases from occurring in the data. We then train a CNN with convolution layers specifically tailored for graph adjacency matrices. Finally, the validity of our results is demonstrated through simulations.","","","10.1109/LRA.2019.2931238","Virginia Tech ICTAS Junior Faculty Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772111","Multi-Robot Systems;Deep Learning in Robotics and Automation;Distributed Robot Systems;Networked Robots","Probabilistic logic;Security;Observers;Deep learning;Robot kinematics;Topology","computational complexity;convolutional neural nets;graph theory;learning (artificial intelligence);matrix algebra;multi-robot systems;neurocontrollers;statistical distributions","deep learning approach;multirobot teams;convolutional neural network;multirobot system;robot interactions;control-theoretic notion;left invertibility;necessary condition;sufficient condition;probabilistic security problem;deep neural networks;CNN-based approach;data generation;training biases;two-step approach;sampled topologies;MRS;interaction graph topologies;probability distribution;random sampling;graph adjacency matrices","","","25","Traditional","","","","IEEE","IEEE Journals"
"Void Filling of Digital Elevation Models With Deep Generative Models","K. Gavriil; G. Muntingh; O. J. D. Barrowclough","Evolute GmbH, Vienna University of Technology, Vienna, Austria; SINTEF Digital, Oslo, Norway; SINTEF Digital, Oslo, Norway","IEEE Geoscience and Remote Sensing Letters","","2019","16","10","1645","1649","In recent years, advances in machine learning algorithms, cheap computational resources, and the availability of big data have spurred the deep learning revolution in various application domains. In particular, supervised learning techniques in image analysis have led to a superhuman performance in various tasks, such as classification, localization, and segmentation, whereas unsupervised learning techniques based on increasingly advanced generative models have been applied to generate high-resolution synthetic images indistinguishable from real images. In this letter, we consider a state-of-the-art machine learning model for image inpainting, namely, a Wasserstein Generative Adversarial Network based on a fully convolutional architecture with a contextual attention mechanism. We show that this model can be successfully transferred to the setting of digital elevation models for the purpose of generating semantically plausible data for filling voids. Training, testing, and experimentation are done on GeoTIFF data from various regions in Norway, made openly available by the Norwegian Mapping Authority.","","","10.1109/LGRS.2019.2902222","H2020 Marie Skłodowska-Curie Actions; Norges Forskningsråd; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8669876","Digital elevation models (DEMs);predictive models;remote sensing;unsupervised learning","Remote sensing;Data models;Adaptation models;Digital elevation models;Generative adversarial networks;Unsupervised learning;Generative adversarial networks;Image reconstruction;Learning systems","convolutional neural nets;digital elevation models;image resolution;image restoration;supervised learning;unsupervised learning","supervised learning techniques;image analysis;unsupervised learning techniques;high-resolution synthetic images;image inpainting;digital elevation models;GeoTIFF data;void filling;machine learning algorithms;Wasserstein generative adversarial network;Big Data;deep learning;deep generative models;contextual attention mechanism;fully convolutional architecture","","","21","","","","","IEEE","IEEE Journals"
"Reconstructing Perceived Images From Human Brain Activities With Bayesian Deep Multiview Learning","C. Du; C. Du; L. Huang; H. He","Research Center for Brain-Inspired Intelligence, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Laboratory of Parallel Software and Computational Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Research Center for Brain-Inspired Intelligence, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Research Center for Brain-Inspired Intelligence, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","8","2310","2323","Neural decoding, which aims to predict external visual stimuli information from evoked brain activities, plays an important role in understanding human visual system. Many existing methods are based on linear models, and most of them only focus on either the brain activity pattern classification or visual stimuli identification. Accurate reconstruction of the perceived images from the measured human brain activities still remains challenging. In this paper, we propose a novel deep generative multiview model for the accurate visual image reconstruction from the human brain activities measured by functional magnetic resonance imaging (fMRI). Specifically, we model the statistical relationships between the two views (i.e., the visual stimuli and the evoked fMRI) by using two view-specific generators with a shared latent space. On the one hand, we adopt a deep neural network architecture for visual image generation, which mimics the stages of human visual processing. On the other hand, we design a sparse Bayesian linear model for fMRI activity generation, which can effectively capture voxel correlations, suppress data noise, and avoid overfitting. Furthermore, we devise an efficient mean-field variational inference method to train the proposed model. The proposed method can accurately reconstruct visual images via Bayesian inference. In particular, we exploit a posterior regularization technique in the Bayesian inference to regularize the model posterior. The quantitative and qualitative evaluations conducted on multiple fMRI data sets demonstrate the proposed method can reconstruct visual images more accurately than the state of the art.","","","10.1109/TNNLS.2018.2882456","National Natural Science Foundation of China; CAS Scientific Equipment Development Project; Beijing Municipal Science and Technology Commission; Youth Innovation Promotion Association CAS; Strategic Priority Research Program of CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574054","Deep neural network (DNN);image reconstruction;multiview learning;neural decoding;variational Bayesian inference","Visualization;Brain modeling;Functional magnetic resonance imaging;Image reconstruction;Decoding;Bayes methods","Bayes methods;biomedical MRI;brain;image reconstruction;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;pattern classification","human brain activities;visual image reconstruction;Bayesian inference;visual images;mean-field variational inference method;fMRI activity generation;sparse Bayesian linear model;human visual processing;visual image generation;deep neural network architecture;view-specific generators;functional magnetic resonance imaging;deep generative multiview model;visual stimuli identification;brain activity pattern classification;linear models;human visual system;evoked brain activities;external visual stimuli information;Bayesian deep multiview learning","","","61","","","","","IEEE","IEEE Journals"
"Probabilistic Sequential Network for Deep Learning of Complex Process Data and Soft Sensor Application","Q. Sun; Z. Ge","Department of Control Science and Engineering, Zhejiang University, Hangzhou, China; Department of Control Science and Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Industrial Informatics","","2019","15","5","2700","2709","Soft sensing of quality/key variables is critical to the control and optimization of industrial processes. One of the main drawbacks of data-driven soft sensors is to deal with the dynamic and nonlinear characteristics of process data. This paper proposes a deep learning structure and corresponding training algorithm for the purpose of soft sensor, which is called probabilistic sequential network. The proposed model merges unsupervised feature extraction and supervised dynamic modeling approaches to improve the prediction performance. It is mainly based on the Gaussian-Bernoulli restricted Boltzmann machine and the recurrent neural network structure. To avoid the overfitting problem in the training procedure of deep learning algorithms, the L2 regularization and dropout technique are adopted. The new method can not only deeply extract the nonlinear feature but also widely capture dynamic characteristic of process data. Effectiveness and superiority of the new method are validated through an actual CO2 absorption column, compared to traditional methods.","","","10.1109/TII.2018.2869899","National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463614","Deep learning;dynamic modeling;Gaussian-Bernoulli restricted Boltzmann machine (GRBM);nonlinear feature extraction;recurrent neural network (RNN);soft sensor","Feature extraction;Machine learning;Hidden Markov models;Data mining;Heuristic algorithms;Recurrent neural networks;Training","Boltzmann machines;feature extraction;Gaussian processes;learning (artificial intelligence);recurrent neural nets","Gaussian-Bernoulli restricted Boltzmann machine;recurrent neural network structure;training procedure;deep learning algorithms;dropout technique;nonlinear feature;probabilistic sequential network;complex process data;soft sensor application;soft sensing;optimization;industrial processes;data-driven soft sensors;dynamic characteristics;nonlinear characteristics;deep learning structure;feature extraction;supervised dynamic modeling approaches;training algorithm","","5","36","","","","","IEEE","IEEE Journals"
"The Classification of Minor Gait Alterations Using Wearable Sensors and Deep Learning","A. Turner; S. Hayes","Department of Computer Science and Technology, University of Hull, Hull, U.K.; Department of Sport, Health and ExerciseUniversity of Hull","IEEE Transactions on Biomedical Engineering","","2019","66","11","3136","3145","Objective: This paper describes how non-invasive wearable sensors can be used in combination with deep learning to classify artificially induced gait alterations without the requirement for a medical professional or gait analyst to be present. This approach is motivated by the goal of diagnosing gait abnormalities on a symptom-by-symptom basis, irrespective of other neuromuscular movement disorders the patients may be affected by. This could lead to improvements in treatment and offer a greater insight into movement disorders. Methods: In-shoe pressure was measured for 12 able-bodied participants, each subject to eight artificially induced gait alterations, achieved by modifying the underside of the shoe. The data were recorded at 100 Hz over 2520 data channels and were analyzed using the deep learning architecture and the long term short term memory networks. Additionally, the rationale for the decision-making process of these networks was investigated. Conclusion: Long term short term memory networks are applicable to the classification of the gait function. The classifications can be made using only 2 s of sparse data (82.0% accuracy over 96 000 instances of test data) from participants who were not a part of the training set. Significance: This paper provides potential for the gait function to be accurately classified using non-invasive techniques, and at more regular intervals, outside of a clinical setting, without the need for healthcare professionals to be present.","","","10.1109/TBME.2019.2900863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648216","Gait abnormalities;gait diagnostics;gait alterations;deep learning;LSTM;high performance computing","Deep learning;Footwear;Sensors;Foot;Legged locomotion;Parkinson's disease;Feature extraction","biomechanics;biomedical measurement;biomedical optical imaging;diseases;gait analysis;geriatrics;health care;learning (artificial intelligence);medical disorders;medical image processing;neural nets;neurophysiology;patient diagnosis;patient monitoring;pattern classification","wearable sensors;minor gait alteration classification;noninvasive techniques;gait function;long term short term memory networks;deep learning architecture;data channels;neuromuscular movement disorders;symptom-by-symptom basis;gait abnormalities;artificially induced gait alterations;noninvasive wearable sensors;frequency 100.0 Hz;time 2.0 s","","","55","Traditional","","","","IEEE","IEEE Journals"
"Dominant-Current Deep Learning Scheme for Electrical Impedance Tomography","Z. Wei; D. Liu; X. Chen","Department of Electrical and Computer EngineeringNational University of Singapore; CAS Key Laboratory of Microscale Magnetic Resonance and Department of Modern PhysicsUniversity of Science and Technology of China (USTC); Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Biomedical Engineering","","2019","66","9","2546","2555","Objective: Deep learning has recently been applied to electrical impedance tomography (EIT) imaging. Nevertheless, there are still many challenges that this approach has to face, e.g., targets with sharp corners or edges cannot be well recovered when using circular inclusion training data. This paper proposes an iterative-based inversion method and a convolutional neural network (CNN) based inversion method to recover some challenging inclusions such as triangular, rectangular, or lung shapes, where the CNN-based method uses only random circle or ellipse training data. Methods: First, the iterative method, i.e., bases-expansion subspace optimization method (BE-SOM), is proposed based on a concept of induced contrast current (ICC) with total variation regularization. Second, the theoretical analysis of BE-SOM and the physical concepts introduced there motivate us to propose a dominant-current deep learning scheme for EIT imaging, in which dominant parts of ICC are utilized to generate multi-channel inputs of CNN. Results: The proposed methods are tested with both numerical and experimental data, where several realistic phantoms including simulated pneumothorax and pleural effusion pathologies are also considered. Conclusions and Significance: Significant performance improvements of the proposed methods are shown in reconstructing targets with sharp corners or edges. It is also demonstrated that the proposed methods are capable of fast, stable, and high-quality EIT imaging, which is promising in providing quantitative images for potential clinical applications.","","","10.1109/TBME.2019.2891676","National Research Foundation; Prime Minister's Office, Singapore; Competitive Research Program; National Natural Science Foundation of China; Anhui Provincial Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606211","Electrical impedance tomography;induced contrast current;subspace optimization method;deep learning","Tomography;Electrodes;Conductivity;Deep learning;Image reconstruction;Mathematical model","computerised tomography;convolutional neural nets;electric impedance imaging;image reconstruction;iterative methods;learning (artificial intelligence);medical image processing;optimisation;phantoms","circular inclusion training data;convolutional neural network based inversion method;CNN-based method;iterative method;bases-expansion subspace optimization method;induced contrast current;ICC;total variation regularization;electrical impedance tomography imaging;deep learning scheme;EIT imaging","","6","50","Traditional","","","","IEEE","IEEE Journals"
"Fog-Embedded Deep Learning for the Internet of Things","L. Lyu; J. C. Bezdek; X. He; J. Jin","Research School of Computer Science, Australian National University, Canberra, ACT, Australia; Department of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia","IEEE Transactions on Industrial Informatics","","2019","15","7","4206","4215","In current deep learning models, centralized architecture forces participants to pool their data to the central Cloud to train a global model, while distributed architecture requires a parameter server to mediate the training process. However, privacy issues, response delays, and computation and communication bottlenecks prevent these architectures from working well at the scale of Internet of Things devices. To counter these problems, in this paper we build a Fog-embedded privacy-preserving deep learning framework (FPPDL), which moves computation from the centralized Cloud to Fog nodes near the end devices. The experimental results on benchmark image datasets under different settings demonstrate that FPPDL achieves comparable accuracy to the centralized stochastic gradient descent (SGD) framework, and delivers better accuracy than the standalone SGD framework. Our evaluations also show that both computation and communication cost are greatly reduced by FPPDL, hence achieving the desired tradeoff between privacy and performance.","","","10.1109/TII.2019.2912465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695142","Deep learning;Fog computing;Internet of Things;privacy-preserving","Computational modeling;Deep learning;Servers;Cloud computing;Privacy;Data models;Training","cloud computing;data privacy;gradient methods;Internet of Things;learning (artificial intelligence);stochastic processes","distributed architecture;privacy issues;FPPDL;centralized stochastic gradient descent framework;centralized architecture;centralized cloud;Internet of Things;SGD framework;fog-embedded privacy-preserving deep learning framework","","","30","","","","","IEEE","IEEE Journals"
"Deep Learning-Based Decoding of Constrained Sequence Codes","C. Cao; D. Li; I. Fair","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada; Department of Civil and Environmental Engineering, University of Alberta, Edmonton, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada","IEEE Journal on Selected Areas in Communications","","2019","37","11","2532","2543","Constrained sequence (CS) codes, including fixed-length CS codes and variable-length CS codes, have been widely used in modern wireless communication and data storage systems. Sequences encoded with constrained sequence codes satisfy constraints imposed by the physical channel to enable efficient and reliable transmission of coded symbols. In this paper, we propose using deep learning approaches to decode fixed-length and variable-length CS codes. Traditional encoding and decoding of fixed-length CS codes rely on look-up tables (LUTs), which is prone to errors that occur during transmission. We introduce fixed-length constrained sequence decoding based on multiple layer perception (MLP) networks and convolutional neural networks (CNNs), and demonstrate that we are able to achieve low bit error rates that are close to maximum a posteriori probability (MAP) decoding as well as improve the system throughput. Further, implementation of capacity-achieving fixed-length codes, where the complexity is prohibitively high with LUT decoding, becomes practical with deep learning-based decoding. We then consider CNN-aided decoding of variable-length CS codes. Different from conventional decoding where the received sequence is processed bit-by-bit, we propose using CNNs to perform one-shot batch-processing of variable-length CS codes such that an entire batch is decoded at once, which improves the system throughput. Moreover, since the CNNs can exploit global information with batch-processing instead of only making use of local information as in conventional bit-by-bit processing, the error rates can be reduced. We present simulation results that show excellent performance with both fixed-length and variable-length CS codes that are used in the frontiers of wireless communication systems.","","","10.1109/JSAC.2019.2933954","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792188","Channel coding;constrained sequence codes;deep learning based decoding;multiple layer perception networks;convolutional neural networks;error rate performance;capacity-approaching codes;one-shot decoding","Training;Optimization;Wireless networks;Resource management;Power control;Deep learning","channel coding;convolutional neural nets;decoding;error statistics;learning (artificial intelligence);multilayer perceptrons;probability;radio networks;table lookup;telecommunication computing;variable length codes","deep learning-based decoding;constrained sequence codes;fixed-length CS codes;variable-length CS codes;capacity-achieving fixed-length codes;multiple layer perception networks;MLP networks;convolutional neural networks;CNN-aided decoding;system throughput","","","48","","","","","IEEE","IEEE Journals"
"An Efficient Passenger-Hunting Recommendation Framework With Multitask Deep Learning","Z. Huang; J. Tang; G. Shan; J. Ni; Y. Chen; C. Wang","School of Computer Science, South China Normal University, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China; Department of Computer Science and Engineering, Tongji University, Shanghai, China; School of Politics and Administration, South China Normal University, Guangzhou, China; Research and Development Department, DataGrand, Inc., Shanghai, China; Department of Computer Science and Engineering, Tongji University, Shanghai, China","IEEE Internet of Things Journal","","2019","6","5","7713","7721","Using large-scale GPS trajectory data to improve taxi services has recently attracted much attention in Internet of Things and smart city communities. In this paper, we use a large-scale GPS trajectory dataset generated by over 12 000 taxis in a period of three months in Shanghai, China, and present an efficient passenger-hunting recommendation framework with the multitask deep learning paradigm. This framework contains two modules: 1) offline training of passenger-hunting recommendation model (OT-PHRM) and 2) online application of passenger-hunting recommendation model (OA-PHRM). The module OT-PHRM mainly includes two deep convolutional neural networks (DCNNs) and uses the multitask learning strategy. The first DCNN realizes the region prediction for picking up passengers, while the second DCNN uses the weight-sharing structure to predict the levels of road congestion and earnings of carrying passengers. In particular, for the input of two DCNNs, we not only consider contextual features of taxi driving, region features and valuable statistical features, but also combine individual features into meaningful ones. In the module OA-PHRM, we propose DL-PHRec, which calculates three prediction values using two trained DCNNs in OT-PHRM in real time, and then recommends a personal ranking-list of regions to each taxi driver according to their scores. The experimental results show the feasibility and effectiveness of our recommendation framework.","","","10.1109/JIOT.2019.2901759","National Natural Science Foundation of China; Natural Science Foundation of Shanghai; Major Project of Ministry of Industry and Information Technology of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653341","Deep learning;Internet of Things;passenger-hunting;representation learning;smart city","Public transportation;Global Positioning System;Training;Vehicles;Task analysis;Trajectory;Roads","data mining;Global Positioning System;learning (artificial intelligence);neural nets;recommender systems;regression analysis;road traffic;traffic information systems","module OA-PHRM;large-scale GPS trajectory data;taxi services;large-scale GPS trajectory dataset;multitask deep learning paradigm;passenger-hunting recommendation model;module OT-PHRM;deep convolutional neural networks;multitask learning strategy;passenger-hunting recommendation framework;DCNN","","6","31","","","","","IEEE","IEEE Journals"
"Binocular Fusion Net: Deep Learning Visual Comfort Assessment for Stereoscopic 3D","H. G. Kim; H. Jeong; H. Lim; Y. M. Ro","Image and Video Systems Lab., School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Lab., School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Lab., School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Lab., School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","4","956","967","In this paper, we propose a novel deep learning-based visual comfort assessment (VCA) for stereoscopic images. To assess the overall degree of visual discomfort in stereoscopic viewing, we devise a binocular fusion deep network (BFN) learning binocular characteristics between stereoscopic images. The proposed BFN learns the latent binocular feature representations for the visual comfort score prediction. In the BFN, the binocular feature is encoded by fusing the spatial features extracted from left and right views. Finally, the visual comfort score is predicted by projecting the binocular feature onto the subjective score space. In addition, we devise a disparity regularization network (DRN) for improving the prediction results. The proposed DRN takes the binocular feature from the BFN and estimates disparity maps from the feature in order to embed disparity relations between left and right views into the deep network. The proposed deep network with BFN and DRN is end-to-end trained in a unified framework in which the DRN acts as disparity regularization. We evaluated the prediction performance of the proposed deep network for VCA by the comparison of existing objective VCA metrics. Further, we demonstrated that the proposed BFN showed various factors causing visual discomfort by using network visualization.","","","10.1109/TCSVT.2018.2817250","Institute for Information and communications Technology Promotion; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8319944","Binocular fusion;deep learning;stereoscopic images;visual comfort assessment;visual discomfort","Visualization;Stereo image processing;Three-dimensional displays;Convolution;Training;Feature extraction;Encoding","feature extraction;learning (artificial intelligence);neural nets;stereo image processing;visual perception","subjective score space;disparity regularization network;DRN;BFN;right views;visual discomfort;network visualization;binocular fusion net;deep learning visual comfort assessment;stereoscopic 3d;deep learning-based visual comfort assessment;stereoscopic images;stereoscopic viewing;binocular fusion deep network;binocular characteristics;latent binocular feature representations;visual comfort score prediction","","2","38","","","","","IEEE","IEEE Journals"
"An Adaptive Dropout Deep Computation Model for Industrial IoT Big Data Learning With Crowdsourcing to Cloud Computing","Q. Zhang; L. T. Yang; Z. Chen; P. Li; F. Bu","School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Software Technology, Dalian University of Technology, Dalian, China; School of Software Technology, Dalian University of Technology, Dalian, China; School of Software Technology, Dalian University of Technology, Dalian, China","IEEE Transactions on Industrial Informatics","","2019","15","4","2330","2337","Deep computation, as an advanced machine learning model, has achieved the state-of-the-art performance for feature learning on big data in industrial Internet of Things (IoT). However, the current deep computation model usually suffers from overfitting due to the lack of public available labeled training samples, limiting its performance for big data feature learning. Motivated by the idea of active learning, an adaptive dropout deep computation model (ADDCM) with crowdsourcing to cloud is proposed for industrial IoT big data feature learning in this paper. First, a distribution function is designed to set the dropout rate for each hidden layer to prevent overfitting for the deep computation model. Furthermore, the outsourcing selection algorithm based on the maximum entropy is employed to choose appropriate samples from the training set to crowdsource on the cloud platform. Finally, an improved supervised learning from multiple experts scheme is presented to aggregate answers given by human workers and to update the parameters of the ADDCM simultaneously. Extensive experiments are conducted to evaluate the performance of the presented model by comparing with the dropout deep computation model and other state-of-the-art crowdsourcing algorithms. The results demonstrate that the proposed model can prevent overfitting effectively and aggregate the labeled samples to train the parameters of the deep computation model with crowdsouring for industrial IoT big data feature learning.","","","10.1109/TII.2018.2791424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8252729","Big data;cloud computing;deep computation;dropout;industrial Internet of Things","Computational modeling;Adaptation models;Tensile stress;Data models;Big Data;Training;Cloud computing","Big Data;cloud computing;Internet of Things;learning (artificial intelligence);outsourcing","adaptive dropout deep computation model;industrial IoT big data learning;cloud computing;advanced machine learning model;state-of-the-art performance;current deep computation model;public available labeled training samples;active learning;improved supervised learning;state-of-the-art crowdsourcing algorithms;ADDCM;industrial IoT big data feature learning","","9","25","","","","","IEEE","IEEE Journals"
"Piecewise Classifier Mappings: Learning Fine-Grained Learners for Novel Categories With Few Examples","X. Wei; P. Wang; L. Liu; C. Shen; J. Wu","Megvii Research Nanjing, Megvii Technology, Nanjing, China; School of Computing and Information Technology, University of Wollongong, Sydney, NSW, Australia; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Image Processing","","2019","28","12","6116","6125","Humans are capable of learning a new fine-grained concept with very little supervision, e.g., few exemplary images for a species of bird, yet our best deep learning systems need hundreds or thousands of labeled examples. In this paper, we try to reduce this gap by studying the fine-grained image recognition problem in a challenging few-shot learning setting, termed few-shot fine-grained recognition (FSFG). The task of FSFG requires the learning systems to build classifiers for the novel fine-grained categories from few examples (only one or less than five). To solve this problem, we propose an end-to-end trainable deep network, which is inspired by the state-of-the-art fine-grained recognition model and is tailored for the FSFG task. Specifically, our network consists of a bilinear feature learning module and a classifier mapping module: while the former encodes the discriminative information of an exemplar image into a feature vector, the latter maps the intermediate feature into the decision boundary of the novel category. The key novelty of our model is a “piecewise mappings” function in the classifier mapping module, which generates the decision boundary via learning a set of more attainable sub-classifiers in a more parameter-economic way. We learn the exemplar-to-classifier mapping based on an auxiliary dataset in a meta-learning fashion, which is expected to be able to generalize to novel categories. By conducting comprehensive experiments on three fine-grained datasets, we demonstrate that the proposed method achieves superior performance over the competing baselines.","","","10.1109/TIP.2019.2924811","National Natural Science Foundation of China; Nanjing University; CRC GeoVision Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752297","Computer vision;fine-grained image recognition;few-shot learning;learning to learn","Image recognition;Task analysis;Training;Birds;Learning systems;Computer vision;Computational modeling","feature extraction;image classification;learning (artificial intelligence);learning systems;neural nets","classifier mappings;deep learning systems;end-to-end trainable deep network;feature vector;piecewise mappings function;exemplar-to-classifier mapping;image recognition;few-shot fine-grained recognition;few-shot learning","","","36","","","","","IEEE","IEEE Journals"
"Prior Information Guided Regularized Deep Learning for Cell Nucleus Detection","M. Tofighi; T. Guo; J. K. P. Vanamala; V. Monga","Department of Electrical Engineering, Pennsylvania State University, University Park, PA, USA; Department of Electrical Engineering, Pennsylvania State University, University Park, PA, USA; Center for Molecular Immunology and Infectious Disease, Pennsylvania State University, University Park, PA, USA; Department of Electrical Engineering, Pennsylvania State University, University Park, PA, USA","IEEE Transactions on Medical Imaging","","2019","38","9","2047","2058","Cell nuclei detection is a challenging research topic because of limitations in cellular image quality and diversity of nuclear morphology, i.e., varying nuclei shapes, sizes, and overlaps between multiple cell nuclei. This has been a topic of enduring interest with promising recent success shown by deep learning methods. These methods train convolutional neural networks (CNNs) with a training set of input images and known, labeled nuclei locations. Many such methods are supplemented by spatial or morphological processing. Using a set of canonical cell nuclei shapes, prepared with the help of a domain expert, we develop a new approach that we call shape priors (SPs) with CNNs (SPs-CNN). We further extend the network to introduce an SP layer and then allowing it to become trainable (i.e., optimizable). We call this network as tunable SP-CNN (TSP-CNN). In summary, we present new network structures that can incorporate “expected behavior” of nucleus shapes via two components: learnable layers that perform the nucleus detection and a fixed processing part that guides the learning with prior information. Analytically, we formulate two new regularization terms that are targeted at: 1) learning the shapes and 2) reducing false positives while simultaneously encouraging detection inside the cell nucleus boundary. Experimental results on two challenging datasets reveal that the proposed SP-CNN and TSP-CNN can outperform the state-of-the-art alternatives.","","","10.1109/TMI.2019.2895318","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8626549","Nucleus detection;deep learning;convolutional neural networks;shape priors;learnable shapes","Shape;Image edge detection;Computer architecture;Microprocessors;Deep learning;Biomedical imaging;Image segmentation","biology computing;cellular biophysics;convolutional neural nets;learning (artificial intelligence);medical image processing","nucleus shapes;learnable layers;fixed processing part;regularization terms;cell nucleus boundary;TSP-CNN;cell nucleus detection;cell nuclei detection;cellular image quality;nuclear morphology;multiple cell nuclei;deep learning methods;convolutional neural networks;training set;input images;labeled nuclei locations;spatial processing;morphological processing;canonical cell nuclei shapes;domain expert;shape priors;tunable SP-CNN;network structures","","","47","","","","","IEEE","IEEE Journals"
"Deep Learning-Based Beam Management and Interference Coordination in Dense mmWave Networks","P. Zhou; X. Fang; X. Wang; Y. Long; R. He; X. Han","Key Lab of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; Key Lab of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; Department of Electrical and Computer Engineering, University of Western Ontario, London, ON, Canada; Key Lab of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; Key Lab of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; Wireless Technology Lab, 2012 Laboratories, Huawei, China","IEEE Transactions on Vehicular Technology","","2019","68","1","592","603","Due to severe signal pathloss in millimeter wave (mmWave) band, beamforming enabled directional transmission is critical to overcome the attenuation challenge in future mmWave communication systems. Furthermore, in order to improve signal coverage of mmWave networks, network densification has to be used at the same time. However, the concurrent use of directional transmission and network densification will make the radio resource management (RRM) of dense mmWave network dramatically more complicated than that of microwave network. In order to maximize the sum-rate of the entire network, tedious and complex RRM algorithms are usually needed to obtain good results, which require high complexity of computation. To address this challenge, we proposed a deep learning-based beam management and interference coordination (BM-IC) method in dense mmWave network, through which the conventional complex BM-IC algorithm is transformed into a deep neural network (DNN)-based approximation. Because DNN only requires a series of simple calculations (e.g., some additions and multiplications), the complexity of computation is greatly reduced. Simulation results show that the proposed deep learning-based BM-IC approach can obtain comparable sum-rate to conventional BM-IC algorithm, but with much less computation time. Thus, deep learning could be a powerful tool to mitigate the complexity of RRM problems in dense mmWave networks.","","","10.1109/TVT.2018.2882635","National Natural Science Foundation of China; NSFC Guangdong Joint Foundation; EP7-PEOPLE-2013-IRSES; Huawei HIRP Flagship Project; Cultivation Program for the Excellent Doctoral Dissertation of Southwest Jiaotong University; NSERC Discovery; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8542687","Millimeter wave (mmWave);wireless local area network (WLAN);deep learning;beam management;interference coordination","Approximation algorithms;Wireless LAN;Interference;Resource management;Optimization;Network architecture","array signal processing;cellular radio;learning (artificial intelligence);neural nets;optimisation;radiofrequency interference","conventional complex BM-IC algorithm;deep neural network-based approximation;deep learning-based BM-IC approach;dense mmWave network;millimeter wave band;directional transmission;future mmWave communication systems;network densification;microwave network;deep learning-based beam management;BM-IC method;RRM problems;radio resource management","","5","41","","","","","IEEE","IEEE Journals"
"Deep-Reinforcement Learning Multiple Access for Heterogeneous Wireless Networks","Y. Yu; T. Wang; S. C. Liew","Department of Information Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong","IEEE Journal on Selected Areas in Communications","","2019","37","6","1277","1290","This paper investigates a deep reinforcement learning (DRL)-based MAC protocol for heterogeneous wireless networking, referred to as a Deep-reinforcement Learning Multiple Access (DLMA). Specifically, we consider the scenario of a number of networks operating different MAC protocols trying to access the time slots of a common wireless medium. A key challenge in our problem formulation is that we assume our DLMA network does not know the operating principles of the MACs of the other networks-i.e., DLMA does not know how the other MACs make decisions on when to transmit and when not to. The goal of DLMA is to be able to learn an optimal channel access strategy to achieve a certain pre-specified global objective. Possible objectives include maximizing the sum throughput and maximizing α-fairness among all networks. The underpinning learning process of DLMA is based on DRL. With proper definitions of the state space, action space, and rewards in DRL, we show that DLMA can easily maximize the sum throughput by judiciously selecting certain time slots to transmit. Maximizing general α-fairness, however, is beyond the means of the conventional reinforcement learning (RL) framework. We put forth a new multi-dimensional RL framework that enables DLMA to maximize general α-fairness. Our extensive simulation results show that DLMA can maximize sum throughput or achieve proportional fairness (two special classes of α-fairness) when coexisting with TDMA and ALOHA MAC protocols without knowing they are TDMA or ALOHA. Importantly, we show the merit of incorporating the use of neural networks into the RL framework (i.e., why DRL and not just traditional RL): specifically, the use of DRL allows DLMA (i) to learn the optimal strategy with much faster speed and (ii) to be more robust in that it can still learn a near-optimal strategy even when the parameters in the RL framework are not optimally set.","","","10.1109/JSAC.2019.2904329","University Research Committee, University of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8665952","Deep reinforcement learning;heterogeneous wireless networks;medium access control (MAC);α-fairness","Media Access Protocol;Throughput;Wireless networks;Neural networks;Time division multiple access","access protocols;learning (artificial intelligence);optimisation;radio networks;time division multiple access","heterogeneous wireless networks;heterogeneous wireless networking;common wireless medium;DLMA network;optimal channel access strategy;sum throughput;ALOHA MAC protocols;neural networks;RL framework;deep-reinforcement learning multiple access;reinforcement learning framework;general alpha-fairness;deep reinforcement learning","","4","27","","","","","IEEE","IEEE Journals"
"TextField: Learning a Deep Direction Field for Irregular Scene Text Detection","Y. Xu; Y. Wang; W. Zhou; Y. Wang; Z. Yang; X. Bai","School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Image Processing","","2019","28","11","5566","5579","Scene text detection is an important step in the scene text reading system. The main challenges lie in significantly varied sizes and aspect ratios, arbitrary orientations, and shapes. Driven by the recent progress in deep learning, impressive performances have been achieved for multi-oriented text detection. Yet, the performance drops dramatically in detecting the curved texts due to the limited text representation (e.g., horizontal bounding boxes, rotated rectangles, or quadrilaterals). It is of great interest to detect the curved texts, which are actually very common in natural scenes. In this paper, we present a novel text detector named TextField for detecting irregular scene texts. Specifically, we learn a direction field pointing away from the nearest text boundary to each text point. This direction field is represented by an image of 2D vectors and learned via a fully convolutional neural network. It encodes both binary text mask and direction information used to separate adjacent text instances, which is challenging for the classical segmentation-based approaches. Based on the learned direction field, we apply a simple yet effective morphological-based post-processing to achieve the final detection. The experimental results show that the proposed TextField outperforms the state-of-the-art methods by a large margin (28% and 8%) on two curved text datasets: Total-Text and SCUT-CTW1500, respectively; TextField also achieves very competitive performance on multi-oriented datasets: ICDAR 2015 and MSRA-TD500. Furthermore, TextField is robust in generalizing unseen datasets.","","","10.1109/TIP.2019.2900589","National Natural Science Foundation of China; Natural Science Foundation of Hubei Province; Alibaba Group through the Alibaba Innovative Research Program; Young Elite Scientists Sponsorship Program by CAST; National Program for Support of Top-Notch Young Professionals; Huazhong University of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648420","Scene text detection;multi-oriented text;curved text;deep neural networks","Pipelines;Feature extraction;Shape;Image segmentation;Detectors;Object detection;Deep learning","convolutional neural nets;document image processing;image representation;image segmentation;learning (artificial intelligence);natural scenes;text detection","curved text datasets;Total-Text;TextField;multioriented datasets;deep direction field;irregular scene text detection;scene text reading system;arbitrary orientations;deep learning;multioriented text detection;text representation;horizontal bounding boxes;natural scenes;text detector;irregular scene texts;nearest text boundary;text point;binary text mask;direction information;adjacent text instances","","1","70","","","","","IEEE","IEEE Journals"
"Breast Cancer Diagnosis in Digital Breast Tomosynthesis: Effects of Training Sample Size on Multi-Stage Transfer Learning Using Deep Neural Nets","R. K. Samala; H. Chan; L. Hadjiiski; M. A. Helvie; C. D. Richter; K. H. Cha","Department of Radiology, University of Michigan, Ann Arbor, MI, USA; Department of Radiology, University of Michigan, Ann Arbor, MI, USA; Department of Radiology, University of Michigan, Ann Arbor, MI, USA; Department of Radiology, University of Michigan, Ann Arbor, MI, USA; Department of Radiology, University of Michigan, Ann Arbor, MI, USA; Department of Radiology, University of Michigan, Ann Arbor, MI, USA","IEEE Transactions on Medical Imaging","","2019","38","3","686","696","In this paper, we developed a deep convolutional neural network (CNN) for the classification of malignant and benign masses in digital breast tomosynthesis (DBT) using a multi-stage transfer learning approach that utilized data from similar auxiliary domains for intermediate-stage fine-tuning. Breast imaging data from DBT, digitized screen-film mammography, and digital mammography totaling 4039 unique regions of interest (1797 malignant and 2242 benign) were collected. Using cross validation, we selected the best transfer network from six transfer networks by varying the level up to which the convolutional layers were frozen. In a single-stage transfer learning approach, knowledge from CNN trained on the ImageNet data was fine-tuned directly with the DBT data. In a multi-stage transfer learning approach, knowledge learned from ImageNet was first fine-tuned with the mammography data and then fine-tuned with the DBT data. Two transfer networks were compared for the second-stage transfer learning by freezing most of the CNN structures versus freezing only the first convolutional layer. We studied the dependence of the classification performance on training sample size for various transfer learning and fine-tuning schemes by varying the training data from 1% to 100% of the available sets. The area under the receiver operating characteristic curve (AUC) was used as a performance measure. The view-based AUC on the test set for single-stage transfer learning was 0.85 ± 0.05 and improved significantly (p <; 0.05$ ) to 0.91 ± 0.03 for multi-stage learning. This paper demonstrated that, when the training sample size from the target domain is limited, an additional stage of transfer learning using data from a similar auxiliary domain is advantageous.","","","10.1109/TMI.2018.2870343","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466816","Breast cancer;computer-aided diagnosis;convolutional neural network;deep-learning;digital breast tomosynthesis;transfer learning","Breast;Task analysis;Training;Biomedical imaging;Cancer;Training data","biological organs;cancer;diagnostic radiography;image classification;learning (artificial intelligence);mammography;medical image processing;neural nets","ImageNet data;DBT data;multistage transfer;mammography data;transfer network;second-stage transfer learning;convolutional layer;fine-tuning schemes;training data;single-stage transfer learning;multistage learning;breast cancer diagnosis;digital breast tomosynthesis;deep convolutional neural network;malignant masses;benign masses;intermediate-stage fine-tuning;digitized screen-film mammography;digital mammography;auxiliary domain;classification performance","","","38","","","","","IEEE","IEEE Journals"
" $L1$ -Norm Batch Normalization for Efficient Training of Deep Neural Networks","S. Wu; G. Li; L. Deng; L. Liu; D. Wu; Y. Xie; L. Shi","Center for Brain-Inspired Computing Research, Tsinghua University, Beijing, China; Center for Brain-Inspired Computing Research, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Institute of Microelectronics, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Center for Brain-Inspired Computing Research, Tsinghua University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","7","2043","2051","Batch normalization (BN) has recently become a standard component for accelerating and improving the training of deep neural networks (DNNs). However, BN brings in additional calculations, consumes more memory, and significantly slows down the training iteration. Furthermore, the nonlinear square and sqrt operations in the normalization process impede low bit-width quantization techniques, which draw much attention to the deep learning hardware community. In this paper, we propose an L1-norm BN (L1BN) with only linear operations in both forward and backward propagations during training. L1BN is approximately equivalent to the conventional L2-norm BN (L2BN) by multiplying a scaling factor that equals (π/2)1/2. Experiments on various convolutional neural networks and generative adversarial networks reveal that L1BN can maintain the same performance and convergence rate as L2BN but with higher computational efficiency. In real application-specified integrated circuit synthesis with reduced resources, L1BN achieves 25% speedup and 37% energy saving compared to the original L2BN. Our hardware-friendly normalization method not only surpasses L2BN in speed but also simplifies the design of deep learning accelerators. Last but not least, L1BN promises a fully quantized training of DNNs, which empowers future artificial intelligence applications on mobile devices with transfer and continual learning capability.","","","10.1109/TNNLS.2018.2876179","National Natural Science Foundation of China; Suzhou-Tsinghua Innovation Leading Program; Brain-Science Special Program of Beijing; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8528524","Batch normalization (BN);deep neural network (DNN);discrete online learning;  $L1$    -norm;mobile intelligence","Training;Convergence;Convolution;Standards;Acceleration;Gaussian distribution;Neural networks","convolutional neural nets;iterative methods;learning (artificial intelligence)","deep neural networks;L1-norm BN;L1BN;L1 -norm batch normalization;nonlinear square and sqrt operations;low bit-width quantization techniques;deep learning hardware community;forwardpropagations;backward propagation;L2-norm BN;convolutional neural networks;generative adversarial networks;application-specified integrated circuit synthesis;hardware-friendly normalization method;deep learning accelerators;artificial intelligence applications;L2BN","","2","46","","","","","IEEE","IEEE Journals"
"Deep Learning for Segmentation Using an Open Large-Scale Dataset in 2D Echocardiography","S. Leclerc; E. Smistad; J. Pedrosa; A. Østvik; F. Cervenansky; F. Espinosa; T. Espeland; E. A. R. Berg; P. Jodoin; T. Grenier; C. Lartizien; J. D’hooge; L. Lovstakken; O. Bernard","University of Lyon, CREATIS, CNRS UMR5220, Inserm U1044, INSA-Lyon, University of Lyon 1, Villeurbanne, France; Department of Circulation and Medical Imaging, Center of Innovative Ultrasound Solutions, Norwegian University of Science and Technology, Trondheim, Norway; Department of Cardiovascular Sciences, KU Leuven, Leuven, Belgium; Department of Circulation and Medical Imaging, Center of Innovative Ultrasound Solutions, Norwegian University of Science and Technology, Trondheim, Norway; University of Lyon, CREATIS, CNRS UMR5220, Inserm U1044, INSA-Lyon, University of Lyon 1, Villeurbanne, France; Cardiovascular Department, Centre Hospitalier Universitaire de Saint-Etienne, Saint-Etienne, France; Center of Innovative Ultrasound Solutions and the Clinic of Cardiology, St. Olavs Hospital, Trondheim, Norway; Center of Innovative Ultrasound Solutions and the Clinic of Cardiology, St. Olavs Hospital, Trondheim, Norway; Computer Science Department, University of Sherbrooke, Sherbrooke, QC, Canada; University of Lyon, CREATIS, CNRS UMR5220, Inserm U1044, INSA-Lyon, University of Lyon 1, Villeurbanne, France; University of Lyon, CREATIS, CNRS UMR5220, Inserm U1044, INSA-Lyon, University of Lyon 1, Villeurbanne, France; Department of Cardiovascular Sciences, KU Leuven, Leuven, Belgium; Department of Circulation and Medical Imaging, Center of Innovative Ultrasound Solutions, Norwegian University of Science and Technology, Trondheim, Norway; University of Lyon, CREATIS, CNRS UMR5220, Inserm U1044, INSA-Lyon, University of Lyon 1, Villeurbanne, France","IEEE Transactions on Medical Imaging","","2019","38","9","2198","2210","Delineation of the cardiac structures from 2D echocardiographic images is a common clinical task to establish a diagnosis. Over the past decades, the automation of this task has been the subject of intense research. In this paper, we evaluate how far the state-of-the-art encoder-decoder deep convolutional neural network methods can go at assessing 2D echocardiographic images, i.e., segmenting cardiac structures and estimating clinical indices, on a dataset, especially, designed to answer this objective. We, therefore, introduce the cardiac acquisitions for multi-structure ultrasound segmentation dataset, the largest publicly-available and fully-annotated dataset for the purpose of echocardiographic assessment. The dataset contains two and four-chamber acquisitions from 500 patients with reference measurements from one cardiologist on the full dataset and from three cardiologists on a fold of 50 patients. Results show that encoder-decoder-based architectures outperform state-of-the-art non-deep learning methods and faithfully reproduce the expert analysis for the end-diastolic and end-systolic left ventricular volumes, with a mean correlation of 0.95 and an absolute mean error of 9.5 ml. Concerning the ejection fraction of the left ventricle, results are more contrasted with a mean correlation coefficient of 0.80 and an absolute mean error of 5.6%. Although these results are below the inter-observer scores, they remain slightly worse than the intra-observer's ones. Based on this observation, areas for improvement are defined, which open the door for accurate and fully-automatic analysis of 2D echocardiographic images.","","","10.1109/TMI.2019.2900516","framework of the LABEX PRIMES (ANR-11-LABX-0063) of the Université de Lyon, within the program “Investissements d’Avenir” (ANR-11-IDEX-0007) operated by the French National Research Agency (ANR); Centre for Innovative Ultrasound Solutions through the Norwegian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8649738","Cardiacsegmentation and diagnosis;deep learning;ultrasound;left ventricle,myocardium;left atrium","Image segmentation;Two dimensional displays;Deep learning;Ultrasonic imaging;Three-dimensional displays;Myocardium;Training","convolutional neural nets;diseases;echocardiography;image segmentation;learning (artificial intelligence);medical image processing","open large-scale dataset;2D echocardiography;cardiac structures;2D echocardiographic images;common clinical task;cardiac acquisitions;multistructure ultrasound segmentation dataset;echocardiographic assessment;absolute mean error;clinical indices;encoder-decoder-based architectures;nondeep learning methods;encoder-decoder deep convolutional neural network methods;end-systolic left ventricular volumes","","2","30","","","","","IEEE","IEEE Journals"
"Active Object Detection With Multistep Action Prediction Using Deep Q-Network","X. Han; H. Liu; F. Sun; X. Zhang","State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, and University of Chinese Academy of Sciences, and Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing, China","IEEE Transactions on Industrial Informatics","","2019","15","6","3723","3731","In recent years, great success has been achieved in visual object detection, which is one of the fundamental tasks in the field of industrial intelligence. Most of existing methods have been proposed to deal with single well-captured still images, while in practical robotic applications, due to nuisances, such as tiny scale, partial view, or occlusion, one still image may not contain enough information for object detection. However, an intelligent robot has the capability to adjust its viewpoint to get better images for detection. Therefore, active object detection becomes a very important perception strategy for intelligent robots. In this paper, by formulating active object detection as a sequential action decision process, a deep reinforcement learning framework is established to resolve it. Furthermore, a novel deep Q-learning network (DQN) with a dueling architecture is proposed, the network has two separate output channels, one predicts action type and the other predicts action range. By combining the two output channels, the action space is explored more efficiently. Several methods are extensively validated and the results show that the proposed one obtains the best results and predicts action in real time.","","","10.1109/TII.2019.2890849","National Natural Science Foundation of China; German Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8601345","Active object detection;active vision;deep Q-learning network (DQN);dueling architecture;reinforcement learning","Object detection;Robots;Feature extraction;Proposals;Informatics;Visualization;Reinforcement learning","intelligent robots;learning (artificial intelligence);object detection;robot vision","active object detection;multistep action prediction;deep Q-network;visual object detection;intelligent robot;sequential action decision process;deep reinforcement learning framework;deep Q-learning network;robotic applications;industrial intelligence","","","32","","","","","IEEE","IEEE Journals"
"Optimal and Fast Real-Time Resource Slicing With Deep Dueling Neural Networks","N. Van Huynh; D. Thai Hoang; D. N. Nguyen; E. Dutkiewicz","School of Electrical and Data Engineering, University of Technology Sydney, Broadway, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Broadway, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Broadway, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Broadway, NSW, Australia","IEEE Journal on Selected Areas in Communications","","2019","37","6","1455","1470","Effective network slicing requires an infrastructure/network provider to deal with the uncertain demands and real-time dynamics of the network resource requests. Another challenge is the combinatorial optimization of numerous resources, e.g., radio, computing, and storage. This paper develops an optimal and fast real-time resource slicing framework that maximizes the long-term return of the network provider while taking into account the uncertainty of resource demands from tenants. Specifically, we first propose a novel system model that enables the network provider to effectively slice various types of resources to different classes of users under separate virtual slices. We then capture the real-time arrival of slice requests by a semi-Markov decision process. To obtain the optimal resource allocation policy under the dynamics of slicing requests, e.g., uncertain service time and resource demands, a Q-learning algorithm is often adopted in the literature. However, such an algorithm is notorious for its slow convergence, especially for problems with large state/action spaces. This makes Q-learning practically inapplicable to our case, in which multiple resources are simultaneously optimized. To tackle it, we propose a novel network slicing approach with an advanced deep learning architecture, called deep dueling, that attains the optimal average reward much faster than the conventional Q-learning algorithm. This property is especially desirable to cope with the real-time resource requests and the dynamic demands of the users. Extensive simulations show that the proposed framework yields up to 40% higher long-term average return while being few thousand times faster, compared with the state-of-the-art network slicing approaches.","","","10.1109/JSAC.2019.2904371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666109","Network slicing;MDP;Q-learning;deep reinforcement learning;deep dueling;resource allocation","Network slicing;Resource management;Dynamic scheduling;Real-time systems;Uncertainty;Heuristic algorithms;Neural networks","learning (artificial intelligence);Markov processes;neural nets;optimisation;resource allocation;virtualisation","deep dueling neural networks;network resource requests;combinatorial optimization;resource demands;semiMarkov decision process;optimal resource allocation policy;uncertain service time;optimal average reward;real-time resource requests;network provider;virtual slices;Q-learning algorithm;optimal resource slicing framework;network slicing approach;deep learning architecture","","2","49","","","","","IEEE","IEEE Journals"
"“Jam Me If You Can:” Defeating Jammer With Deep Dueling Neural Network Architecture and Ambient Backscattering Augmented Communications","N. Van Huynh; D. N. Nguyen; D. T. Hoang; E. Dutkiewicz","School of Electrical and Data Engineering, University of Technology Sydney, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, NSW, Australia","IEEE Journal on Selected Areas in Communications","","2019","37","11","2603","2620","With conventional anti-jamming solutions like frequency hopping or spread spectrum, legitimate transceivers often tend to “escape” or “hide” themselves from jammers. These reactive anti-jamming approaches are constrained by the lack of timely knowledge of jamming attacks (especially from smart jammers). Bringing together the latest advances in neural network architectures and ambient backscattering communications, this work allows wireless nodes to effectively “face” the jammer (instead of escaping) by first learning its jamming strategy, then adapting the rate or transmitting information right on the jamming signals (i.e., backscattering modulated information on the jamming signals). Specifically, to deal with unknown jamming attacks (e.g., jamming strategies, jamming power levels, and jamming capability), existing work often relies on reinforcement learning algorithms, e.g., Q -learning. However, the Q -learning algorithm is notorious for its slow convergence to the optimal policy, especially when the system state and action spaces are large. This makes the Q -learning algorithm pragmatically inapplicable. To overcome this problem, we design a novel deep reinforcement learning algorithm using the recent dueling neural network architecture. Our proposed algorithm allows the transmitter to effectively learn about the jammer and attain the optimal countermeasures (e.g., adapt the transmission rate or backscatter or harvest energy or stay idle) thousand times faster than that of the conventional Q -learning algorithm. Through extensive simulation results, we show that our design (using ambient backscattering and the deep dueling neural network architecture) can improve the average throughput (under smart and reactive jamming attacks) by up to 426% and reduce the packet loss by 24%. By augmenting the ambient backscattering capability on devices and using our algorithm, it is interesting to observe that the (successful) transmission rate increases with the jamming power. Our proposed solution can find its applications in both civil (e.g., ultra-reliable and low-latency communications or URLLC) and military scenarios (to combat both inadvertent and deliberate jamming).","","","10.1109/JSAC.2019.2933889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792369","Anti-jamming;ambient backscatter;AI-powered rate adaptation;RF energy harvesting;deep dueling;deep neural networks;deep reinforcement learning;Q-learning","Jamming;Backscatter;Radio transmitters;Radio frequency;Wireless communication;Neural networks","electromagnetic wave scattering;frequency hop communication;jamming;learning (artificial intelligence);neural nets;radio transceivers;spread spectrum communication;telecommunication computing;telecommunication network reliability","deep reinforcement learning algorithm;ambient backscattering augmented communications;anti-jamming solutions;neural network architectures;ambient backscattering communications;dueling neural network architecture;Q -learning algorithm","","","45","","","","","IEEE","IEEE Journals"
"Rehab-Net: Deep Learning Framework for Arm Movement Classification Using Wearable Sensors for Stroke Rehabilitation","M. Panwar; D. Biswas; H. Bajaj; M. Jöbges; R. Turk; K. Maharatna; A. Acharyya","Department of Electrical EngineeringIndian Institute of Technology Hyderabad; Biomedical Circuits and Systems Group, IMEC; Department of Electrical and ElectronicsNational Institute of Technology Trichy; Brandenburg Klinik; Faculty of Heath SciencesUniversity of Southampton; School of Electronics and Computer ScienceUniversity of Southampton; Department of Electrical Engineering, Indian Institute of Technology Hyderabad, Telangana, India","IEEE Transactions on Biomedical Engineering","","2019","66","11","3026","3037","In this paper, we present a deep learning framework “Rehab-Net” for effectively classifying three upper limb movements of the human arm, involving extension, flexion, and rotation of the forearm, which, over the time, could provide a measure of rehabilitation progress. The proposed framework, Rehab-Net is formulated with a personalized, light weight and low-complex, customized convolutional neural network (CNN) model, using two-layers of CNN, interleaved with pooling layers, followed by a fully connected layer that classifies the three movements from tri-axial acceleration input data collected from the wrist. The proposed Rehab-Net framework was validated on sensor data collected in two situations: 1) semi-naturalistic environment involving an archetypal activity of “making-tea” with four stroke survivors and 2) natural environment, where ten stroke survivors were free to perform any desired arm movement for the duration of 120 min. We achieved an overall accuracy of 97.89% on semi-naturalistic data and 88.87% on naturalistic data which exceeded state-of-the-art learning algorithms namely, linear discriminant analysis, support vector machines, and k-means clustering with an average accuracy of 48.89%, 44.14%, and 27.64%. Subsequently, a computational complexity analysis of the proposed model has been discussed with an eye toward hardware implementation. The clinical significance of this study is to accurately monitor the clinical progress of the rehabilitated subjects under the ambulatory settings.","","","10.1109/TBME.2019.2899927","Science and Engineering Research Board; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643399","Convolutional neural network;deep learning;human activity recognition;rehabilitation;times-series classification","Task analysis;Stroke (medical condition);Monitoring;Accelerometers;Feature extraction;Wrist;Deep learning","biomechanics;computational complexity;convolutional neural nets;learning (artificial intelligence);linear discriminant analysis;medical signal processing;patient rehabilitation;pattern clustering;signal classification;support vector machines","fully connected layer;tri-axial acceleration input data;Rehab-Net framework;sensor data;seminaturalistic environment;stroke survivors;naturalistic data;deep learning framework;arm movement classification;wearable sensors;stroke rehabilitation;upper limb movements;human arm;pooling layers;customized convolutional neural network model;CNN;linear discriminant analysis;support vector machines;K-means clustering;computational complexity analysis","","","61","Traditional","","","","IEEE","IEEE Journals"
"Spatial-spectral classification of hyperspectral images: a deep learning framework with Markov Random fields based modelling","C. Qing; J. Ruan; X. Xu; J. Ren; J. Zabalza","School of Electronic and Information Engineering, South China University of Technology, People's Republic of China; School of Electronic and Information Engineering, South China University of Technology, People's Republic of China; School of Electronic and Information Engineering, South China University of Technology, People's Republic of China; Department of Electronic and Electrical Engineering, University of Strathclyde, UK; Department of Electronic and Electrical Engineering, University of Strathclyde, UK","IET Image Processing","","2019","13","2","235","245","For the spatial-spectral classification of hyperspectral images (HSIs), a deep learning framework is proposed in this study, which consists of convolutional neural networks (CNNs) and Markov random fields (MRFs). Firstly, a CNN model to learn the deep spectral feature from the HSI is built and the class posterior probability distribution is estimated. The CNN with a dropout layer can relieve the overfitting in classification. The CNN is utilised as a pixel-classifier, so it only works in the spectral domain. Then, the spatial information will be encoded by MRF-based multilevel logistic prior for regularising the classification. To derive the correlation of both spectral and spatial features for improving algorithm performance, the marginal probability distribution in HSI is learned using MRF-based loopy belief propagation. In comparison with several state-of-the-art approaches for data classification on three publicly available HSI datasets, experimental results have demonstrated the superior performance of the proposed methodology.","","","10.1049/iet-ipr.2018.5727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8649874","","","belief networks;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);Markov processes;neural nets;probability;regression analysis;remote sensing;statistical distributions","data classification;MRF-based loopy belief propagation;marginal probability distribution;spatial features;spectral features;spatial information;class posterior probability distribution;HSI;deep spectral feature;CNN model;Markov random fields;convolutional neural networks;deep learning framework;hyperspectral images;spatial-spectral classification","","","47","","","","","IET","IET Journals"
"Multi-Agent Deep Reinforcement Learning for Dynamic Power Allocation in Wireless Networks","Y. S. Nasir; D. Guo","Department of Electrical and Computer Engineering, Northwestern University, Evanston, IL, USA; Department of Electrical and Computer Engineering, Northwestern University, Evanston, IL, USA","IEEE Journal on Selected Areas in Communications","","2019","37","10","2239","2250","This work demonstrates the potential of deep reinforcement learning techniques for transmit power control in wireless networks. Existing techniques typically find near-optimal power allocations by solving a challenging optimization problem. Most of these algorithms are not scalable to large networks in real-world scenarios because of their computational complexity and instantaneous cross-cell channel state information (CSI) requirement. In this paper, a distributively executed dynamic power allocation scheme is developed based on model-free deep reinforcement learning. Each transmitter collects CSI and quality of service (QoS) information from several neighbors and adapts its own transmit power accordingly. The objective is to maximize a weighted sum-rate utility function, which can be particularized to achieve maximum sum-rate or proportionally fair scheduling. Both random variations and delays in the CSI are inherently addressed using deep Q-learning. For a typical network architecture, the proposed algorithm is shown to achieve near-optimal power allocation in real time based on delayed CSI measurements available to the agents. The proposed scheme is especially suitable for practical scenarios where the system model is inaccurate and CSI delay is non-negligible.","","","10.1109/JSAC.2019.2933973","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792117","Deep Q-learning;radio resource management;interference mitigation;power control;Jakes fading model","Resource management;Transmitters;Receivers;Fading channels;Dynamic scheduling;Power control;Heuristic algorithms","control engineering computing;learning (artificial intelligence);MIMO communication;multi-agent systems;optimisation;power control;quality of service;radio networks;radiofrequency interference;resource allocation;telecommunication computing;telecommunication control;wireless channels","wireless networks;transmit power control;near-optimal power allocation;challenging optimization problem;instantaneous cross-cell channel state information requirement;distributively executed dynamic power allocation scheme;model-free deep reinforcement learning;weighted sum-rate utility function;maximum sum-rate;deep Q-learning;typical network architecture;delayed CSI measurements;CSI delay;multiagent deep reinforcement;quality of service information;proportionally fair scheduling;real time","","1","42","","","","","IEEE","IEEE Journals"
"Unsupervised Feature Extraction via Deep Learning for Histopathological Classification of Colon Tissue Images","C. T. Sari; C. Gunduz-Demir","Department of Computer Engineering, Bilkent University, Ankara, Turkey; Department of Computer Engineering, Bilkent University, Ankara, Turkey","IEEE Transactions on Medical Imaging","","2019","38","5","1139","1149","Histopathological examination is today's gold standard for cancer diagnosis. However, this task is time consuming and prone to errors as it requires a detailed visual inspection and interpretation of a pathologist. Digital pathology aims at alleviating these problems by providing computerized methods that quantitatively analyze digitized histopathological tissue images. The performance of these methods mainly relies on the features that they use, and thus, their success strictly depends on the ability of these features by successfully quantifying the histopathology domain. With this motivation, this paper presents a new unsupervised feature extractor for effective representation and classification of histopathological tissue images. This feature extractor has three main contributions: First, it proposes to identify salient subregions in an image, based on domain-specific prior knowledge, and to quantify the image by employing only the characteristics of these subregions instead of considering the characteristics of all image locations. Second, it introduces a new deep learning-based technique that quantizes the salient subregions by extracting a set of features directly learned on image data and uses the distribution of these quantizations for image representation and classification. To this end, the proposed deep learning-based technique constructs a deep belief network of the restricted Boltzmann machines (RBMs), defines the activation values of the hidden unit nodes in the final RBM as the features, and learns the quantizations by clustering these features in an unsupervised way. Third, this extractor is the first example for successfully using the restricted Boltzmann machines in the domain of histopathological image analysis. Our experiments on microscopic colon tissue images reveal that the proposed feature extractor is effective to obtain more accurate classification results compared to its counterparts.","","","10.1109/TMI.2018.2879369","Türkiye Bilimsel ve Teknolojik Araştirma Kurumu; Türkiye Bilimler Akademisi; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520760","Deep learning;feature learning;histopathological image representation;digital pathology;automated cancer diagnosis;saliency;colon cancer;hematoxylin-eosin staining","Feature extraction;Image segmentation;Cancer;Task analysis;Pathology;Quantization (signal)","belief networks;biological organs;biological tissues;biomedical optical imaging;Boltzmann machines;cancer;feature extraction;image classification;image representation;image segmentation;learning (artificial intelligence);medical image processing","microscopic colon tissue images;histopathological image analysis;restricted Boltzmann machines;deep belief network;image representation;image data;deep learning-based technique;image locations;domain-specific prior knowledge;unsupervised feature extractor;histopathology domain;digitized histopathological tissue images;computerized methods;digital pathology;cancer diagnosis;histopathological examination;histopathological classification;unsupervised feature extraction","","1","47","","","","","IEEE","IEEE Journals"
"Deep PDS-Learning for Privacy-Aware Offloading in MEC-Enabled IoT","X. He; R. Jin; H. Dai","Department of Electrical Engineering, Lamar University, Beaumont, TX, USA; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, USA; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, USA","IEEE Internet of Things Journal","","2019","6","3","4547","4555","The rapid uptake of Internet-of-Things (IoT) devices imposes an unprecedented pressure for data communication and processing on the backbone network and the central cloud infrastructure. To overcome this issue, the recently advocated mobile-edge computing (MEC)-enabled IoT is promising. Meanwhile, driven by the growing social awareness of privacy, significant research efforts have been devoted to relevant issues in IoT; however, most of them mainly focus on the conventional cloud-based IoT. In this paper, a new privacy vulnerability caused by the wireless offloading feature of MEC-enabled IoT is identified. To address this vulnerability, an effective privacy-aware offloading scheme is developed based on a newly proposed deep post-decision state (PDS)-learning algorithm. By exploiting extra prior information, the proposed deep PDS-learning algorithm allows the IoT devices to learn a good privacy-aware offloading strategy much faster than the conventional deep Q-network. Theoretic analysis and numerical results are provided to corroborate the correctness and the effectiveness of the proposed algorithm.","","","10.1109/JIOT.2018.2878718","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8515032","Deep learning;Internet-of-Things (IoT);mobile-edge computing (MEC);post-decision state (PDS);privacy","Privacy;Servers;Task analysis;Internet of Things;Wireless communication;Cloud computing;Numerical models","cloud computing;computer network security;data privacy;Internet of Things;learning (artificial intelligence);mobile computing","MEC-enabled IoT;Internet-of-Things devices;data communication;backbone network;central cloud infrastructure;mobile-edge computing-enabled IoT;conventional cloud-based IoT;privacy vulnerability;wireless offloading feature;deep PDS-learning algorithm;IoT devices;good privacy-aware offloading strategy;conventional deep Q-network;deep post-decision state-learning algorithm;privacy-aware offloading scheme","","1","40","","","","","IEEE","IEEE Journals"
"Modulation Classification Based on Signal Constellation Diagrams and Deep Learning","S. Peng; H. Jiang; H. Wang; H. Alwageed; Y. Zhou; M. M. Sebdani; Y. Yao","Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; FutureWei Technologies, Inc., Bridgewater, NJ, USA; Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; Publicis Media, New York, NY, USA; Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","3","718","727","Deep learning (DL) is a new machine learning (ML) methodology that has found successful implementations in many application domains. However, its usage in communications systems has not been well explored. This paper investigates the use of the DL in modulation classification, which is a major task in many communications systems. The DL relies on a massive amount of data and, for research and applications, this can be easily available in communications systems. Furthermore, unlike the ML, the DL has the advantage of not requiring manual feature selections, which significantly reduces the task complexity in modulation classification. In this paper, we use two convolutional neural network (CNN)-based DL models, AlexNet and GoogLeNet. Specifically, we develop several methods to represent modulated signals in data formats with gridlike topologies for the CNN. The impacts of representation on classification performance are also analyzed. In addition, comparisons with traditional cumulant and ML-based algorithms are presented. Experimental results demonstrate the significant performance advantage and application feasibility of the DL-based approach for modulation classification.","","","10.1109/TNNLS.2018.2850703","National Natural Science Foundation of China; National Natural Science Foundation of China; China Scholarship Council; Huaqiao University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418751","Convolutional neural network (CNN);data conversion;deep learning (DL);modulation classification","Modulation;Communication systems;Feature extraction;Convolution;Neurons;Training;Biological neural networks","convolutional neural nets;learning (artificial intelligence);modulation;signal classification;topology","modulation classification;signal constellation diagrams;deep learning;ML;machine learning;convolutional neural network;signal modulation;DL models;AlexNet;GoogLeNet;gridlike topologies;data formats","","18","25","","","","","IEEE","IEEE Journals"
"A Multimodal Deep Learning Method for Android Malware Detection Using Various Features","T. Kim; B. Kang; M. Rho; S. Sezer; E. G. Im","Department of Computer and Software, Hanyang University, Seoul, South Korea; Centre for Secure Information Technologies, Queen’s University of Belfast, Belfast, U.K.; Department of Computer Science and Engineering, Hanyang University, Seoul, South Korea; Centre for Secure Information Technologies, Queen’s University of Belfast, Belfast, U.K.; Department of Computer Science and Engineering, Hanyang University, Seoul, South Korea","IEEE Transactions on Information Forensics and Security","","2019","14","3","773","788","With the widespread use of smartphones, the number of malware has been increasing exponentially. Among smart devices, android devices are the most targeted devices by malware because of their high popularity. This paper proposes a novel framework for android malware detection. Our framework uses various kinds of features to reflect the properties of android applications from various aspects, and the features are refined using our existence-based or similarity-based feature extraction method for effective feature representation on malware detection. Besides, a multimodal deep learning method is proposed to be used as a malware detection model. This paper is the first study of the multimodal deep learning to be used in the android malware detection. With our detection model, it was possible to maximize the benefits of encompassing multiple feature types. To evaluate the performance, we carried out various experiments with a total of 41 260 samples. We compared the accuracy of our model with that of other deep neural network models. Furthermore, we evaluated our framework in various aspects including the efficiency in model updates, the usefulness of diverse features, and our feature representation method. In addition, we compared the performance of our framework with those of other existing methods including deep learning-based methods.","","","10.1109/TIFS.2018.2866319","MSIT (Ministry of Science, ICT), South Korea, under the ITRC (Information Technology Research Center) Support Program; Institute for Information & Communication Technology Promotion (IITP); IITP Grant; Korea Government (MSIT), Development of Defense Technologies against Ransomware; National Research Foundation of Korea; Korea Government (MSIP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8443370","Android malware;intrusion detection;machine learning;malware detection;neural network","Feature extraction;Malware;Androids;Humanoid robots;Machine learning;Neural networks;Libraries","feature extraction;invasive software;learning (artificial intelligence);neural nets;smart phones","feature representation;deep learning-based methods;feature representation method;deep neural network models;multiple feature types;malware detection model;feature extraction method;android applications;android malware detection;multimodal deep learning method","","6","62","","","","","IEEE","IEEE Journals"
"Simultaneously Learning Neighborship and Projection Matrix for Supervised Dimensionality Reduction","Y. Pang; B. Zhou; F. Nie","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Center for Optical Imagery Analysis and Learning, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","9","2779","2793","Explicitly or implicitly, most dimensionality reduction methods need to determine which samples are neighbors and the similarities between the neighbors in the original high-dimensional space. The projection matrix is then learnt on the assumption that the neighborhood information, e.g., the similarities, are known and fixed prior to learning. However, it is difficult to precisely measure the intrinsic similarities of samples in high-dimensional space because of the curse of dimensionality. Consequently, the neighbors selected according to such similarities and the projection matrix obtained according to such similarities and the corresponding neighbors might not be optimal in the sense of classification and generalization. To overcome this drawback, in this paper, we propose to let the similarities and neighbors be variables and model these in a low-dimensional space. Both the optimal similarity and projection matrix are obtained by minimizing a unified objective function. Nonnegative and sum-to-one constraints on the similarity are adopted. Instead of empirically setting the regularization parameter, we treat it as a variable to be optimized. It is interesting that the optimal regularization parameter is adaptive to the neighbors in a low-dimensional space and has an intuitive meaning. Experimental results on the YALE B, COIL-100, and MNIST data sets demonstrate the effectiveness of the proposed method.","","","10.1109/TNNLS.2018.2886317","National Natural Science Foundation of China; Nokia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8608009","Dimensionality reduction;feature extraction;projection matrix;subspace learning","Dimensionality reduction;Training;Feature extraction;Toy manufacturing industry;Learning systems;Deep learning;Sparse matrices","feature extraction;learning (artificial intelligence);matrix algebra","high-dimensional space;projection matrix;low-dimensional space;learning neighborship;supervised dimensionality reduction;unified objective function;sum-to-one constraints;nonnegative constraints;curse of dimensionality","","","56","","","","","IEEE","IEEE Journals"
"CellinDeep: Robust and Accurate Cellular-Based Indoor Localization via Deep Learning","H. Rizk; M. Torki; M. Youssef","Department of Computer Science and Engineering, Egypt-Japan University of Science and Technology, Alexandria, Egypt; Faculty of Engineering, Alexandria University, Alexandria, Egypt; Faculty of Engineering, Alexandria University, Alexandria, Egypt","IEEE Sensors Journal","","2019","19","6","2305","2312","The demand for a ubiquitous and accurate indoor localization service is continuously growing. Current solutions for indoor localization usually depend on using the embedded sensors on high-end phones or provide coarse-grained accuracy. We present CellinDeep: a deep learning-based localization system that achieves fine-grained accuracy using the ubiquitous cellular technology. Specifically, CellinDeep captures the non-linear relation between the cellular signal heard by a mobile phone and its location. To do that, it leverages a deep network to model the inherent dependency between the signals of the different cell towers in the area of interest, allowing it achieve high localization accuracy. As part of the design of CellinDeep, we introduce modules to address a number of practical challenges such as handling the noise in the input wireless signal, reducing the amount of data required for the deep learning model, as avoiding over-training. Implementation of CellinDeep on different Android phones shows that it can achieve a median localization accuracy of 0.78m. This accuracy is better than the state-of-the-art indoor cellular-based systems by at least 350%. In addition, CellinDeep provides at least 93.45% savings in power compared to the WiFi-based techniques.","","","10.1109/JSEN.2018.2885958","National Telecommunication Regulatory Authority; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8570849","Cellular;indoor;localization;deep learning;fingerprinting","Poles and towers;Wireless fidelity;Sensors;Wireless communication;Neural networks;Wireless sensor networks;Data models","cellular radio;indoor radio;intelligent sensors;learning (artificial intelligence);smart phones;telecommunication computing","CellinDeep;ubiquitous localization service;high-end phones;coarse-grained accuracy;deep learning-based localization system;fine-grained accuracy;ubiquitous cellular technology;nonlinear relation;cellular signal;mobile phone;deep network;high localization accuracy;input wireless signal;deep learning model;median localization accuracy;indoor localization service;Android phones;cellular-based indoor localization system;embedded sensors;cell towers;size 0.78 m","","2","40","","","","","IEEE","IEEE Journals"
"Robot-Assisted Training in Laparoscopy Using Deep Reinforcement Learning","X. Tan; C. Chng; Y. Su; K. Lim; C. Chui","Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore","IEEE Robotics and Automation Letters","","2019","4","2","485","492","Minimally invasive surgery (MIS) is increasingly becoming a vital method of reducing surgical trauma and significantly improving postoperative recovery. However, skillful handling of surgical instruments used in MIS, especially for laparoscopy, requires a long period of training and depends highly on the experience of surgeons. This letter presents a new robot-assisted surgical training system which is designed to improve the practical skills of surgeons through intrapractice feedback and demonstration from both human experts and reinforcement learning (RL) agents. This system utilizes proximal policy optimization to learn the control policy in simulation. Subsequently, a generative adversarial imitation learning agent is trained based on both expert demonstrations and learned policies in simulation. This agent then generates demonstration policies on the robot-assisted device for trainees and produces feedback scores during practice. To further acquire surgical tools coordinates and encourage self-oriented practice, a mask region-based convolution neural network is trained to perform the semantic segmentation of surgical tools and targets. To the best of our knowledge, this system is the first robot-assisted laparoscopy training system which utilizes actual surgical tools and leverages deep reinforcement learning to provide demonstration training from both human expert perspectives and RL criterion.","","","10.1109/LRA.2019.2891311","Ministry of Education Academic Research Fund Tier 1; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8604070","Surgical robotics;laparoscopy;learning from demonstration;deep learning in robotics and automation;AI-based methods","Training;Laparoscopes;Tools;Trajectory;Robot kinematics;Task analysis","biomedical education;computer based training;feedforward neural nets;image segmentation;learning (artificial intelligence);medical robotics;surgery","human experts;proximal policy optimization;control policy;generative adversarial imitation learning agent;expert demonstrations;learned policies;demonstration policies;robot-assisted device;feedback scores;surgical tools coordinates;self-oriented practice;mask region-based convolution neural network;robot-assisted laparoscopy training system;actual surgical tools;leverages deep reinforcement learning;demonstration training;human expert perspectives;robot-assisted training;minimally invasive surgery;MIS;vital method;surgical trauma;postoperative recovery;skillful handling;surgical instruments;robot-assisted surgical training system;intrapractice feedback","","1","29","","","","","IEEE","IEEE Journals"
"Hierarchy-Dependent Cross-Platform Multi-View Feature Learning for Venue Category Prediction","S. Jiang; W. Min; S. Mei","Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Shandong University of Science and Technology, Shandong, China","IEEE Transactions on Multimedia","","2019","21","6","1609","1619","In this paper, we focus on visual venue category prediction, which can facilitate various applications for location-based service and personalization. Considering the complementarity of different media platforms, it is reasonable to leverage venue-relevant media data from different platforms to boost the prediction performance. Intuitively, recognizing one venue category involves multiple semantic cues, especially objects and scenes and, thus, they should contribute together to venue category prediction. In addition, these venues can be organized in a natural hierarchical structure, which provides prior knowledge to guide venue category estimation. Taking these aspects into account, we propose a Hierarchy-dependent Cross-platform Multi-view Feature Learning (HCM-FL) framework for venue category prediction from videos by leveraging images from other platforms. HCM-FL includes two major components, namely Cross-Platform Transfer Deep Learning (CPTDL) and Multi-View Feature Learning with the Hierarchical Venue Structure (MVFL-HVS). CPTDL is capable of reinforcing the learned deep network from videos using images from other platforms. Specifically, CPTDL first trained a deep network using videos. These images from other platforms are filtered by the learnt network and these selected images are then fed into this learnt network to enhance it. Two kinds of pre-trained networks on the ImageNet and Places dataset are employed. Therefore, we can harness both object-oriented and scene-oriented deep features through these enhanced deep networks. MVFL-HVS is then developed to enable multi-view feature fusion. It is capable of embedding the hierarchical structure ontology to support more discriminative joint feature learning. We conduct the experiment on videos from Vine and images from Foursquare. These experimental results demonstrate the advantage of our proposed framework in jointly utilizing multi-platform data, multi-view deep features, and hierarchical venue structure knowledge.","","","10.1109/TMM.2018.2876830","Natural Science Foundation of Beijing Municipality; National Natural Science Foundation of China; Lenovo Outstanding Young Scientists Program; National Program for Special Support of Eminent Professionals and National Program for Support of Top-notch Young Professionals; China Postdoctoral Science Foundation; State Key Laboratory of Robotics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8496844","Feature extraction;knowledge transfer;super-vised learning;video signal processing;Web 2.0","Videos;Visualization;Training;Machine learning;Feature extraction;Animals;Noise measurement","learning (artificial intelligence);object detection;ontologies (artificial intelligence);video signal processing","object-oriented scene-oriented deep features;discriminative joint feature learning;multiplatform data;multiview deep features;hierarchical venue structure knowledge;visual venue category prediction;prediction performance;venue category estimation;learned deep network;learnt network;cross-platform transfer deep learning;venue-relevant media data;hierarchy-dependent cross-platform multiview feature learning","","1","46","","","","","IEEE","IEEE Journals"
"Fully Automated Annotation With Noise-Masked Visual Markers for Deep-Learning-Based Object Detection","T. Kiyokawa; K. Tomochika; J. Takamatsu; T. Ogasawara","Division of Information Science, Nara Institute of Science and Technology, Nara, Japan; Division of Information Science, Nara Institute of Science and Technology, Nara, Japan; Division of Information Science, Nara Institute of Science and Technology, Nara, Japan; Division of Information Science, Nara Institute of Science and Technology, Nara, Japan","IEEE Robotics and Automation Letters","","2019","4","2","1972","1977","Automated factories use deep-learning-based vision systems to accurately detect various products. However, training such vision systems requires manual annotation of a significant amount of data to optimize the large number of parameters of the deep convolutional neural networks. Such manual annotation is very time-consuming and laborious. To reduce this burden, we propose a fully automated annotation approach without any manual intervention. To do this, we associate one visual marker with one object and capture them in the same image. However, if an image showing the marker is used for training, normally, the neural network learns the marker as a feature of the object. By hiding the marker with a noise mask, we succeeded in reducing this erroneous learning. Experiments verified the effectiveness of the proposed method in comparison with manual annotation, in terms of both the time needed to collect training data and the resulting detection accuracy of the vision system. The time required for data collection was reduced from 16.1 to 1.87 h. The accuracy of the vision system trained with the proposed method was 87.3%, which is higher than the accuracy of a vision system trained with the manual method.","","","10.1109/LRA.2019.2899153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641376","Computer vision for automation;deep learning in robotics and automation;object detection;segmentation and categorization","Training;Visualization;Machine vision;Training data;Manuals;Object detection;Three-dimensional displays","computer vision;factory automation;learning (artificial intelligence);neural nets;object detection","noise-masked visual markers;deep-learning-based object detection;automated factories;deep-learning-based vision systems;manual annotation;deep convolutional neural networks;fully automated annotation approach;manual intervention;visual marker;neural network;noise mask","","","17","","","","","IEEE","IEEE Journals"
"Road Detection and Centerline Extraction Via Deep Recurrent Convolutional Neural Network U-Net","X. Yang; X. Li; Y. Ye; R. Y. K. Lau; X. Zhang; X. Huang","Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; City University of Hong Kong, Hong Kong; Harbin Institute of Technology, Shenzhen, China; School of Information Engineering, East China Jiaotong University, Nanchang, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","9","7209","7220","Road information extraction based on aerial images is a critical task for many applications, and it has attracted considerable attention from researchers in the field of remote sensing. The problem is mainly composed of two subtasks, namely, road detection and centerline extraction. Most of the previous studies rely on multistage-based learning methods to solve the problem. However, these approaches may suffer from the well-known problem of propagation errors. In this paper, we propose a novel deep learning model, recurrent convolution neural network U-Net (RCNN-UNet), to tackle the aforementioned problem. Our proposed RCNN-UNet has three distinct advantages. First, the end-to-end deep learning scheme eliminates the propagation errors. Second, a carefully designed RCNN unit is leveraged to build our deep learning architecture, which can better exploit the spatial context and the rich low-level visual features. Thereby, it alleviates the detection problems caused by noises, occlusions, and complex backgrounds of roads. Third, as the tasks of road detection and centerline extraction are strongly correlated, a multitask learning scheme is designed so that two predictors can be simultaneously trained to improve both effectiveness and efficiency. Extensive experiments were carried out based on two publicly available benchmark data sets, and nine state-of-the-art baselines were used in a comparative evaluation. Our experimental results demonstrate the superiority of the proposed RCNN-UNet model for both the road detection and the centerline extraction tasks.","","","10.1109/TGRS.2019.2912301","National Basic Research Program of China (973 Program); Shenzhen Science and Technology Program; Research Grants Council of the Hong Kong SAR; National Natural Science Foundation of China; CityU Shenzhen Research Institute; Natural Science Foundation of Jiangxi Province; Education Department of Jiangxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8714072","Recurrent convolutional neural network (RCNN);road centerline extraction;road detection;U-Net","Roads;Task analysis;Feature extraction;Image segmentation;Deep learning;Remote sensing;Automobiles","computer vision;convolutional neural nets;feature extraction;learning (artificial intelligence);recurrent neural nets;road traffic;traffic engineering computing","multitask learning scheme;RCNN-UNet model;road detection;deep recurrent convolutional neural network U-Net;road information extraction;deep learning model;end-to-end deep learning scheme;deep learning architecture;centerline extraction","","1","42","","","","","IEEE","IEEE Journals"
"Iris: Deep Reinforcement Learning Driven Shared Spectrum Access Architecture for Indoor Neutral-Host Small Cells","X. Foukas; M. K. Marina; K. Kontovasilis","School of Informatics, The University of Edinburgh, Edinburgh, U.K.; School of Informatics, The University of Edinburgh, Edinburgh, U.K.; Institute of Informatics and Telecommunications, NCSR “Demokritos,”, Agia Paraskevi, Greece","IEEE Journal on Selected Areas in Communications","","2019","37","8","1820","1837","We consider indoor mobile access, a vital use case for current and future mobile networks. For this key use case, we outline a vision that combines a neutral-host-based shared small-cell infrastructure with a common pool of spectrum for dynamic sharing as a way forward to proliferate indoor small-cell deployments and open up the mobile operator ecosystem. Toward this vision, we focus on the challenges pertaining to managing access to shared spectrum [e.g., 3.5-GHz U.S. Citizen Broadband Radio Service (CBRS) spectrum]. We propose Iris, a practical shared spectrum access architecture for indoor neutral-host small-cells. At the core of Iris is a deep reinforcement learning-based dynamic pricing mechanism that efficiently mediates access to shared spectrum for diverse operators in a way that provides incentives for operators and the neutral-host alike. We then present the Iris system architecture that embeds this dynamic pricing mechanism alongside cloud-RAN and RAN slicing design principles in a practical neutral-host design tailored for the indoor small-cell environment. Using a prototype implementation of the Iris system, we present the extensive experimental evaluation results that not only offer insight into the Iris dynamic pricing process and its superiority over alternative approaches but also demonstrate its deployment feasibility.","","","10.1109/JSAC.2019.2927067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8756106","Indoor mobile access;small cells;neutral host;RAN slicing;C-RAN;shared spectrum;dynamic pricing;deep reinforcement learning","Pricing;Iris recognition;Reinforcement learning;Prototypes;Informatics;Quality of experience;Broadband communication","indoor radio;learning (artificial intelligence);mobile radio;radio spectrum management","indoor small-cell environment;deep reinforcement learning driven shared spectrum access architecture;indoor neutral-host small cells;indoor mobile access;small-cell infrastructure;dynamic sharing;indoor small-cell deployments;mobile operator ecosystem;deep reinforcement learning-based dynamic pricing mechanism;Iris system architecture","","","84","","","","","IEEE","IEEE Journals"
"ResInNet: A Novel Deep Neural Network With Feature Reuse for Internet of Things","X. Sun; G. Gui; Y. Li; R. P. Liu; Y. An","Department of Communication Engineering, College of Information Engineering, North China University of Science and Technology, Tangshan, China; Key Laboratory of Broadband Wireless Communication and Sensor Network Technology, Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Communication Engineering, College of Information Engineering, North China University of Science and Technology, Tangshan, China; Digital and Productivity Flagship (DP&S) Department, Commonwealth Scientific and Industrial Research Organization, Sydney, NSW, Australia; Department of Communication Engineering, College of Information Engineering, North China University of Science and Technology, Tangshan, China","IEEE Internet of Things Journal","","2019","6","1","679","691","Deep neural networks (DNNs) have widely used in various Internet-of-Things (IoT) applications. Pursuing superior performance is always a hot spot in the field of DNN modeling. Recently, feature reuse provides an effective means of achieving favorable nonlinear approximation performance in deep learning. Existing implementations utilizes a multilayer perception (MLP) to act as a functional unit for feature reuse. However, determining connection weight and bias of MLP is a rather intractable problem, since the conventional back-propagation learning approach encounters the limitations of slow convergence and local optimum. To address this issue, this paper develops a novel DNN considering a well-behaved alternative called reservoir computing, i.e., reservoir in network (ResInNet). In this structure, the built-in reservoir has two notable functions. First, it behaves as a bridge between any two restricted Boltzmann machines in the feature learning part of ResInNet, performing a feature abstraction once again. Such reservoir-based feature translation provides excellent starting points for the following nonlinear regression. Second, it serves as a nonlinear approximation, trained by a simple linear regression using the most representative (learned) features. Experimental results over various benchmark datasets show that ResInNet can achieve the superior nonlinear approximation performance in comparison to the baseline models, and produce the excellent dynamic characteristics and memory capacity. Meanwhile, the merits of our approach is further demonstrated in the network traffic prediction related to real-world IoT application.","","","10.1109/JIOT.2018.2853663","National Natural Science Foundation of China; Jiangsu Specially Appointed Professor; Innovation and Entrepreneurship of Jiangsu High-Level Talent; NUPTSF; Nanjing University of Posts and Telecommunications; Natural Science Foundation of Hebei Province; Hebei Colleges and Universities Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8405574","Deep belief network;deep learning;feature reuse;nonlinear approximation;reservoir computing","Reservoirs;Internet of Things;Feature extraction;Machine learning;Training;Nonhomogeneous media;Computational modeling","approximation theory;Boltzmann machines;Internet of Things;learning (artificial intelligence);multilayer perceptrons;regression analysis","superior nonlinear approximation performance;network traffic prediction;ResInNet;novel deep neural network;deep neural networks;Internet-of-Things applications;DNN modeling;favorable nonlinear approximation performance;deep learning;MLP;connection weight;alternative called reservoir computing;feature abstraction;reservoir-based feature translation;nonlinear regression","","23","50","","","","","IEEE","IEEE Journals"
"An Active Deep Learning Approach for Minimally Supervised PolSAR Image Classification","H. Bi; F. Xu; Z. Wei; Y. Xue; Z. Xu","School of Electronics, Computing and Mathematics, University of Derby, Derby, U.K.; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China; Xi’an Electronics and Engineering Institute, Xi’an, China; School of Electronics, Computing and Mathematics, University of Derby, Derby, U.K.; Institute for Information and System Sciences, Xi’an Jiaotong University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","9378","9395","Recently, deep neural networks have received intense interests in polarimetric synthetic aperture radar (PolSAR) image classification. However, its success is subject to the availability of large amounts of annotated data which require great efforts of experienced human annotators. Aiming at improving the classification performance with greatly reduced annotation cost, this paper presents an active deep learning approach for minimally supervised PolSAR image classification, which integrates active learning and fine-tuned convolutional neural network (CNN) into a principled framework. Starting from a CNN trained using a very limited number of labeled pixels, we iteratively and actively select the most informative candidates for annotation, and incrementally fine-tune the CNN by incorporating the newly annotated pixels. Moreover, to boost the performance and robustness of the proposed method, we employ Markov random field (MRF) to enforce class label smoothness, and data augmentation technique to enlarge the training set. We conducted extensive experiments on four real benchmark PolSAR images, and experiments demonstrated that our approach achieved state-of-the-art classification results with significantly reduced annotation cost.","","","10.1109/TGRS.2019.2926434","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784406","Active learning;convolutional neural network (CNN);data augmentation;fine-tuning;Markov random field (MRF);polarimetric synthetic aperture radar (PolSAR) image classification","Deep learning;Neural networks;Training;Synthetic aperture radar;Remote sensing;Learning systems;Task analysis","convolutional neural nets;image classification;iterative methods;Markov processes;radar imaging;synthetic aperture radar","active deep learning approach;minimally supervised PolSAR image classification;deep neural networks;polarimetric synthetic aperture radar image classification;annotated data;CNN;benchmark PolSAR images;convolutional neural network;Markov random field;class label smoothness;data augmentation technique","","","54","","","","","IEEE","IEEE Journals"
"Traffic Light Recognition With High Dynamic Range Imaging and Deep Learning","J. Wang; L. Zhou","Robotics Department, Institute for Infocomm Research, Singapore; Autonomous Vehicle Department, Institute for Infocomm Research, Singapore","IEEE Transactions on Intelligent Transportation Systems","","2019","20","4","1341","1352","Traffic light recognition (TLR) detects the traffic light from an image and then estimates the state of the light signal. TLR is important for autonomous vehicles because running against a red light could cause a deadly car accident. For a practical TLR system, computation time, varying illumination conditions, and false positives are three key challenges. In this paper, a novel real-time method is proposed to recognize a traffic light with high dynamic imaging and deep learning. In our approach, traffic light candidates are robustly detected from low exposure/dark frames and accurately classified using a deep neural network in consecutive high exposure/bright frames. This dual-channel mechanism can make full use of undistorted color and shape information in dark frames as well as the rich context in bright frames. In the dark channel, a non-parametric multi-color saliency model is proposed to simultaneously extract lights with different colors. A multiclass classifier with convolutional neural network (CNN) model is then adopted to reduce the number of false positives in the bright channel. The performance is further boosted by incorporating temporal trajectory tracking. In order to speed up the algorithm, a prior detection mask is generated to limit the potential search regions. Intensive experiments on a large dual-channel dataset show that the proposed approach outperforms the state-of-the-art real-time deep learning object detector, which could cause more false positives because it uses bright images only. The algorithm has been integrated into our autonomous vehicle and can work robustly on real roads.","","","10.1109/TITS.2018.2849505","A*STAR Grant for Autonomous Systems Project, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419782","Traffic light recognition;autonomous vehicle;high dynamic range imaging;deep learning","Image color analysis;Machine learning;Cameras;Lighting;Image recognition;Robustness;Histograms","convolutional neural nets;feature extraction;image classification;image colour analysis;image recognition;image segmentation;learning (artificial intelligence);object detection;traffic engineering computing","TLR system;bright images;real-time deep learning object detector;bright channel;convolutional neural network model;nonparametric multicolor saliency model;dark channel;shape information;undistorted color;dual-channel mechanism;deep neural network;false positives;red light;autonomous vehicle;light signal;high dynamic range imaging;traffic light recognition","","2","33","","","","","IEEE","IEEE Journals"
"Joint Learning of Degradation Assessment and RUL Prediction for Aeroengines via Dual-Task Deep LSTM Networks","H. Miao; B. Li; C. Sun; J. Liu","State Key Laboratory for Manufacturing Systems Engineering, Xi'an Jiaotong University, Xi'an, China; State Key Laboratory for Manufacturing Systems Engineering, Xi'an Jiaotong University, Xi'an, China; State Key Laboratory for Manufacturing Systems Engineering, Xi'an Jiaotong University, Xi'an, China; State Key Laboratory for Manufacturing Systems Engineering, Xi'an Jiaotong University, Xi'an, China","IEEE Transactions on Industrial Informatics","","2019","15","9","5023","5032","Health assessment and prognostics are two key tasks within the prognostics and health management frame of equipment. However, existing works are performing these two tasks separately and hierarchically. In this paper, we design and establish dual-task deep long short-term memory networks for joint learning of degradation assessment and remaining useful life prediction of aeroengines. This enables a more robust and accurate assessment and prediction results making for the increment of operational reliability and safety as well as maintenance cost reduction. Meanwhile, the target label functions that match the network training are constructed in an adaptive way according to the health state of an individual aeroengine. Experiments on the popular C-MAPSS lifetime dataset of aeroengines are employed to verify the accuracy and effectiveness. The performance of our proposed work exhibits superiority over other state-of-the-art approaches and demonstrate its application potential.","","","10.1109/TII.2019.2900295","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Shaanxi Province Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643968","Deep learning (DL);degradation assessment;long short term memory networks;prognostics and health management;remaining useful life (RUL)","Logic gates;Prognostics and health management;Task analysis;Deep learning;Degradation;Maintenance engineering;Recurrent neural networks","aerospace computing;aerospace engines;condition monitoring;cost reduction;design engineering;learning (artificial intelligence);maintenance engineering;mechanical engineering computing;recurrent neural nets;reliability;remaining life assessment;safety","health state;remaining useful life prediction;safety;maintenance cost reduction;reliability;degradation assessment joint learning;prognostic model;equipment frame;design method;dual-task deep long short-term memory networks;aeroengine C-MAPSS lifetime dataset","","","38","Traditional","","","","IEEE","IEEE Journals"
"Fringe Pattern Improvement and Super-Resolution Using Deep Learning in Digital Holography","Z. Ren; H. K. -. So; E. Y. Lam","School of Natural and Applied Sciences, Northwestern Polytechnical University, Xi'an, China; Department of Electrical and Electronic Engineering, University of Hong Kong, Pokfulam, Hong Kong; Department of Electrical and Electronic Engineering, University of Hong Kong, Pokfulam, Hong Kong","IEEE Transactions on Industrial Informatics","","2019","15","11","6179","6186","Digital holographic imaging is a powerful technique that can provide wavefront information of a three-dimensional object for biological and industrial applications. However, due to the constraint and cost of imaging sensors, the acquired digital hologram is limited in terms of pixel count, thus affecting the resolution in holographic reconstruction. To overcome this constraint, in this paper we propose a deep learning-based method to super-resolve holograms and to improve the quality of low-resolution holograms by training a convolutional neural network with large-scale data for resolution enhancement. Moreover, this algorithm can be broadly adapted to enhance the space-bandwidth product of a holographic imaging system without the need of any advanced hardware. We experimentally validate its capability using a lens-free off-axis holographic system, and compare the performance of various loss functions and interpolation methods in training such a network.","","","10.1109/TII.2019.2913853","Research Grants Council and University Grants Committee (RGC, UGC) of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8701653","Computational imaging;deep learning;digital holography (DH);super-resolution","Spatial resolution;Laser beams;Image reconstruction;Deep learning;Holography","convolutional neural nets;holography;image reconstruction;image resolution;interpolation;learning (artificial intelligence)","three-dimensional object;biological applications;industrial applications;imaging sensors;pixel count;holographic reconstruction;deep learning-based method;super-resolve holograms;low-resolution holograms;convolutional neural network;large-scale data;resolution enhancement;space-bandwidth product;lens-free off-axis holographic system;fringe pattern improvement;super-resolution;digital holographic imaging;wavefront information","","","50","IEEE","","","","IEEE","IEEE Journals"
"Deep Learning in Edge of Vehicles: Exploring Trirelationship for Data Transmission","Z. Ning; Y. Feng; M. Collotta; X. Kong; X. Wang; L. Guo; X. Hu; B. Hu","School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Software, Dalian University of Technology, Dalian, China; Kore University of Enna, Enna, Italy; School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China; Chongqing Key Laboratory of Mobile Communications Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China","IEEE Transactions on Industrial Informatics","","2019","15","10","5737","5746","Currently, vehicles have the abilities to communicate with each other autonomously. For Internet of Vehicles (IoV), it is urgent to reduce the latency and improve the throughput for data transmission among vehicles. This article proposes a deep learning based transmission strategy by exploring trirelationships among vehicles. Specifically, we consider both the social and physical attributes of vehicles at the edge of IoV, i.e., edge of vehicles. The social features of vehicles are extracted to establish the network model by constructing triangle motif structures to obtain primary neighbors with close relationships. Additionally, the connection probabilities of nodes based on the characteristics of vehicles and devices can be estimated, by which a content sharing partner discovery algorithm is proposed based on convolutional neural network. Finally, the experiment results demonstrate the efficiency of our method with respect to various aspects, such as message delivery ratio, average latency, and percentage of connected devices.","","","10.1109/TII.2019.2929740","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Fundamental Research Funds for the Central Universities; State Key Laboratory of Integrated Services Networks, Xidian University; Science and Technology Innovation Program of National Defense; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765802","Data transmission;deep learning;device to device;edge of vehicles;triangle motif","Device-to-device communication;Mobile handsets;Data communication;Deep learning;Physical layer;Real-time systems;Trajectory","Internet of Things;learning (artificial intelligence);neural nets;traffic engineering computing","IoV;data transmission;deep learning based transmission strategy;social attributes;physical attributes;social features;network model;triangle motif structures;content sharing partner discovery algorithm;convolutional neural network","","12","19","Traditional","","","","IEEE","IEEE Journals"
"Nonlinear Unmixing of Hyperspectral Data via Deep Autoencoder Networks","M. Wang; M. Zhao; J. Chen; S. Rahardja","School of Marine Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Marine Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Marine Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Marine Science and Technology, Northwestern Polytechnical University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","9","1467","1471","Nonlinear spectral unmixing is an important and challenging problem in hyperspectral image processing. Classical nonlinear algorithms are usually derived based on specific assumptions on the nonlinearity. In recent years, deep learning shows its advantage in addressing general nonlinear problems. However, existing ways of using deep neural networks for unmixing are limited and restrictive. In this letter, we develop a novel blind hyperspectral unmixing scheme based on a deep autoencoder network. Both encoder and decoder of the network are carefully designed so that we can conveniently extract estimated endmembers and abundances simultaneously from the nonlinearly mixed data. Because an autoencoder is essentially an unsupervised algorithm, this scheme only relies on the current data and, therefore, does not require additional training. Experimental results validate the proposed scheme and show its superior performance over several existing algorithms.","","","10.1109/LGRS.2019.2900733","National Natural Science Foundation of China; NSF of Shenzhen; 111 Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667664","Autoencoder network;deep learning;hyperspectral imaging;nonlinear spectral unmixing","Decoding;Hyperspectral imaging;Signal processing algorithms;Neural networks;Deep learning;Training","hyperspectral imaging;image processing;neural nets;unsupervised learning","deep neural networks;deep autoencoder network;nonlinearly mixed data;hyperspectral data;nonlinear spectral unmixing;hyperspectral image processing;deep learning;nonlinear algorithms;blind hyperspectral unmixing scheme;unsupervised algorithm","","1","27","","","","","IEEE","IEEE Journals"
"Semantic Cluster Unary Loss for Efficient Deep Hashing","S. Zhang; J. Li; B. Zhang","Department of Computer Science and Technology, State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Institute for Artificial Intelligence, Tsinghua University, Beijing, China; Department of Computer Science and Technology, State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Institute for Artificial Intelligence, Tsinghua University, Beijing, China; Department of Computer Science and Technology, State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Institute for Artificial Intelligence, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","","2019","28","6","2908","2920","With the rapid development of deep learning, deep hashing methods have achieved promising results in efficient information retrieval. The hashing method maps similar data to binary hashcodes with smaller hamming distance, which has received broad attention due to its low storage cost and fast retrieval speed. Most of the existing deep hashing methods adopt pairwise or triplet losses to deal with similarities underlying the data, but their training is difficult and less efficient because O(n2) data pairs and O(n3) triplets are involved. To address these issues, we propose a novel deep hashing algorithm with the unary loss which can be trained very efficiently. First, we introduce a Unary Upper Bound of the traditional triplet loss, thus reducing the complexity to O(n) and bridging the classification-based unary loss and the triplet loss. Second, we propose a novel Semantic Cluster Deep Hashing (SCDH) algorithm by introducing a modified Unary Upper Bound loss, called Semantic Cluster Unary Loss. The resultant hashcodes form several compact clusters, which means the hashcodes in the same cluster have similar semantic information. We also demonstrate that the proposed SCDH is easy to extend to semi-supervised settings by incorporating the state-of-the-art semi-supervised learning algorithms. The experiments on large-scale datasets show that the proposed method is superior to the state-of-the-art hashing algorithms.","","","10.1109/TIP.2019.2891967","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8607035","Deep hashing;unary loss;semi-supervised learning;information retrieval","Semantics;Clustering algorithms;Upper bound;Measurement;Complexity theory;Semisupervised learning;Hamming distance","file organisation;information retrieval;learning (artificial intelligence);pattern classification;pattern clustering","state-of-the-art semisupervised learning algorithms;state-of-the-art hashing algorithms;deep learning;efficient information retrieval;smaller hamming distance;low storage cost;fast retrieval speed;traditional triplet loss;classification-based unary loss;resultant hashcodes;compact clusters;similar semantic information;deep hashing methods;efficient deep hashing;semantic cluster deep hashing algorithm;semantic cluster unary loss;modified unary upper bound loss","","","64","","","","","IEEE","IEEE Journals"
"When Edge Computing Meets Microgrid: A Deep Reinforcement Learning Approach","M. S. Munir; S. F. Abedin; N. H. Tran; C. S. Hong","Department of Computer Science and Engineering, Kyung Hee University, Seoul, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Seoul, South Korea; School of Computer Science, University of Sydney, Sydney, NSW, Australia; Department of Computer Science and Engineering, Kyung Hee University, Seoul, South Korea","IEEE Internet of Things Journal","","2019","6","5","7360","7374","The computational tasks at multiaccess edge computing (MEC) are unpredictable in nature, which raises uneven energy demand for MEC networks. Thus, to handle this problem, microgrid has the potentiality to provides seamless energy supply from its energy sources (i.e., renewable, nonrenewable, and storage). However, supplying energy from the microgrid faces challenges due to the high uncertainty and irregularity of the renewable energy generation over the time horizon. Therefore, in this paper, we study about the microgrid-enabled MEC networks' energy supply plan, where we first formulate an optimization problem and the objective is to minimize the energy consumption of microgrid-enabled MEC networks. The problem is a mixed integer nonlinear optimization with computational and latency constraints for tasks fulfillment, and also coupled with the dependencies of uncertainty for both energy consumption and generation. Therefore, we show that the problem is an NP-hard problem. As a result, second, we decompose our formulated problem into two subproblems: 1) energy-efficient tasks assignment problem for MEC into community discovery problem and 2) energy supply plan problem into Markov decision process. Third, we apply a low complexity density-based spatial clustering of applications with noise to solve the first subproblem for each base station distributedly. Sequentially, we use the output of the first subproblem as a input for solving the second subproblem, where we apply a model-based deep reinforcement learning. Finally, the simulation results demonstrate the significant performance gain of the proposed model with a high accuracy energy supply plan.","","","10.1109/JIOT.2019.2899673","National Research Foundation of Korea; Korea Government (MSIT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642832","Computational tasks;deep reinforcement learning (RL);demand response (DR);energy management;Internet of Things (IoT);microgrid;multiaccess edge computing (MEC);unsupervised learning","Task analysis;Energy consumption;Microgrids;Internet of Things;Servers;Renewable energy sources;Uncertainty","computational complexity;demand side management;distributed power generation;distributed processing;energy conservation;energy consumption;integer programming;learning (artificial intelligence);Markov processes;nonlinear programming;pattern clustering;power engineering computing;power generation economics;power generation planning;renewable energy sources","deep reinforcement learning approach;multiaccess edge computing;uneven energy demand;energy sources;renewable energy generation;optimization problem;mixed integer nonlinear optimization;computational constraints;latency constraints;NP-hard problem;community discovery problem;model-based deep reinforcement learning;energy consumption minimization;energy-efficient tasks assignment problem;microgrid-enabled MEC networks energy supply plan problem;Markov decision process;complexity density-based spatial clustering","","1","58","","","","","IEEE","IEEE Journals"
"Deep CNN-Based Blind Image Quality Predictor","J. Kim; A. Nguyen; S. Lee","Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","1","11","24","Image recognition based on convolutional neural networks (CNNs) has recently been shown to deliver the state-of-the-art performance in various areas of computer vision and image processing. Nevertheless, applying a deep CNN to no-reference image quality assessment (NR-IQA) remains a challenging task due to critical obstacles, i.e., the lack of a training database. In this paper, we propose a CNN-based NR-IQA framework that can effectively solve this problem. The proposed method-deep image quality assessor (DIQA)-separates the training of NR-IQA into two stages: (1) an objective distortion part and (2) a human visual system-related part. In the first stage, the CNN learns to predict the objective error map, and then the model learns to predict subjective score in the second stage. To complement the inaccuracy of the objective error map prediction on the homogeneous region, we also propose a reliability map. Two simple handcrafted features were additionally employed to further enhance the accuracy. In addition, we propose a way to visualize perceptual error maps to analyze what was learned by the deep CNN model. In the experiments, the DIQA yielded the state-of-the-art accuracy on the various databases.","","","10.1109/TNNLS.2018.2829819","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8383698","Convolutional neural network (CNN);deep learning;image quality assessment (IQA);no-reference IQA (NR-IQA)","Training;Distortion;Visualization;Machine learning;Image quality;Databases;Reliability","computer vision;feature extraction;feedforward neural nets;image classification;image recognition;image representation;learning (artificial intelligence)","deep CNN-based blind image quality predictor;image recognition;convolutional neural networks;state-of-the-art performance;computer vision;image processing;no-reference image quality assessment;training database;CNN-based NR-IQA framework;objective distortion part;human visual system-related part;objective error map prediction;perceptual error maps;deep CNN model;state-of-the-art accuracy;deep image quality assessor;DIQA","","7","43","","","","","IEEE","IEEE Journals"
"Multi-Modal Multi-Scale Deep Learning for Large-Scale Image Annotation","Y. Niu; Z. Lu; J. Wen; T. Xiang; S. Chang","Beijing Key Laboratory of Big Data Management and Analysis Methods, School of Information, Renmin University of China, Beijing, China; Beijing Key Laboratory of Big Data Management and Analysis Methods, School of Information, Renmin University of China, Beijing, China; Beijing Key Laboratory of Big Data Management and Analysis Methods, School of Information, Renmin University of China, Beijing, China; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.; Department of Electrical Engineering, Columbia University, New York, NY, USA","IEEE Transactions on Image Processing","","2019","28","4","1720","1731","Image annotation aims to annotate a given image with a variable number of class labels corresponding to diverse visual concepts. In this paper, we address two main issues in large-scale image annotation: 1) how to learn a rich feature representation suitable for predicting a diverse set of visual concepts ranging from object, scene to abstract concept and 2) how to annotate an image with the optimal number of class labels. To address the first issue, we propose a novel multi-scale deep model for extracting rich and discriminative features capable of representing a wide range of visual concepts. Specifically, a novel two-branch deep neural network architecture is proposed, which comprises a very deep main network branch and a companion feature fusion network branch designed for fusing the multi-scale features computed from the main branch. The deep model is also made multi-modal by taking noisy user-provided tags as model input to complement the image input. For tackling the second issue, we introduce a label quantity prediction auxiliary task to the main label prediction task to explicitly estimate the optimal label number for a given image. Extensive experiments are carried out on two large-scale image annotation benchmark datasets, and the results show that our method significantly outperforms the state of the art.","","","10.1109/TIP.2018.2881928","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities and the Research Funds of Renmin University of China; Renmin University of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8538003","Large-scale image annotation;multi-scale deep model;multi-modal deep model;label quantity prediction","Image annotation;Feature extraction;Visualization;Task analysis;Noise measurement;Image recognition;Predictive models","feature extraction;image classification;image fusion;image representation;learning (artificial intelligence);neural nets","companion feature fusion network branch;image input;label quantity prediction auxiliary task;main label prediction task;large-scale image annotation benchmark datasets;class labels;rich feature representation;abstract concept;novel multiscale deep model;two-branch deep neural network architecture;multiscale feature fusion;discriminative feature extraction;multimodal multiscale deep learning;visual concepts;novel two-branch deep neural network architecture","","2","48","","","","","IEEE","IEEE Journals"
"A Deep Learning Method for Change Detection in Synthetic Aperture Radar Images","Y. Li; C. Peng; Y. Chen; L. Jiao; L. Zhou; R. Shang","Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","8","5751","5763","With the rapid development of various technologies of satellite sensor, synthetic aperture radar (SAR) image has been an import source of data in the application of change detection. In this paper, a novel method based on a convolutional neural network (CNN) for SAR image change detection is proposed. The main idea of our method is to generate the classification results directly from the original two SAR images through a CNN without any preprocessing operations, which also eliminate the process of generating the difference image (DI), thus reducing the influence of the DI on the final classification result. In CNN, the spatial characteristics of the raw image can be extracted and captured by automatic learning and the results with stronger robustness can be obtained. The basic idea of the proposed method includes three steps: it first produces false labels through unsupervised spatial fuzzy clustering. Then we train the CNN through proper samples that are selected from the samples with false labels. Finally, the final detection results are obtained by the trained convolutional network. Although training the convolutional network is a supervised learning fashion, the whole process of the algorithm is an unsupervised process without priori knowledge. The theoretical analysis and experimental results demonstrate the validity, robustness, and potential of our algorithm in simulated and real data sets. In addition, we try to apply our algorithm to the change detection of heterogeneous images, which also achieves satisfactory results.","","","10.1109/TGRS.2019.2901945","National Natural Science Foundation of China; Program for Cheung Kong Scholars and Innovative Research Team in University; Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project); Major Research Plan of the National Natural Science Foundation of China; Technology Foundation for Selected Overseas Chinese Scholar in Shaanxi; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673805","Change detection;convolutional neural network (CNN);spatial fuzzy clustering;synthetic aperture radar (SAR) images","Radar polarimetry;Training;Clustering algorithms;Speckle;Synthetic aperture radar;Change detection algorithms;Deep learning","convolutional neural nets;fuzzy set theory;image classification;pattern clustering;radar computing;radar detection;radar imaging;supervised learning;unsupervised learning","convolutional neural network;CNN;SAR image change detection;automatic learning;unsupervised spatial fuzzy clustering;trained convolutional network;supervised learning fashion;heterogeneous image detection;DI generation;satellite sensor technologies;deep learning method;difference image generation;synthetic aperture radar imaging","","","41","","","","","IEEE","IEEE Journals"
"A Cost-Sensitive Deep Belief Network for Imbalanced Classification","C. Zhang; K. C. Tan; H. Li; G. S. Hong","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","1","109","122","Imbalanced data with a skewed class distribution are common in many real-world applications. Deep Belief Network (DBN) is a machine learning technique that is effective in classification tasks. However, conventional DBN does not work well for imbalanced data classification because it assumes equal costs for each class. To deal with this problem, cost-sensitive approaches assign different misclassification costs for different classes without disrupting the true data sample distributions. However, due to lack of prior knowledge, the misclassification costs are usually unknown and hard to choose in practice. Moreover, it has not been well studied as to how cost-sensitive learning could improve DBN performance on imbalanced data problems. This paper proposes an evolutionary cost-sensitive deep belief network (ECS-DBN) for imbalanced classification. ECS-DBN uses adaptive differential evolution to optimize the misclassification costs based on the training data that presents an effective approach to incorporating the evaluation measure (i.e., G-mean) into the objective function. We first optimize the misclassification costs, and then apply them to DBN. Adaptive differential evolution optimization is implemented as the optimization algorithm that automatically updates its corresponding parameters without the need of prior domain knowledge. The experiments have shown that the proposed approach consistently outperforms the state of the art on both benchmark data sets and real-world data set for fault diagnosis in tool condition monitoring.","","","10.1109/TNNLS.2018.2832648","Neuromorphic Computing Program AME Programmatic Grant; A*STAR, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8368071","Cost sensitive;deep belief network;evolutionary algorithm (EA);imbalanced classification","Learning systems;Neural networks;Task analysis;Optimization;Tools;Machine learning;Fault diagnosis","belief networks;condition monitoring;evolutionary computation;fault diagnosis;learning (artificial intelligence);mechanical engineering computing;optimisation;pattern classification","data sample distributions;cost-sensitive learning;DBN performance;imbalanced data problems;imbalanced classification;ECS-DBN;training data;adaptive differential evolution optimization;benchmark data sets;real-world data;skewed class distribution;machine learning technique;classification tasks;conventional DBN;imbalanced data classification;equal costs;evolutionary cost-sensitive deep belief network;misclassification costs","","5","69","","","","","IEEE","IEEE Journals"
"Predicting Head Movement in Panoramic Video: A Deep Reinforcement Learning Approach","M. Xu; Y. Song; J. Wang; M. Qiao; L. Huo; Z. Wang","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","11","2693","2708","Panoramic video provides immersive and interactive experience by enabling humans to control the field of view (FoV) through head movement (HM). Thus, HM plays a key role in modeling human attention on panoramic video. This paper establishes a database collecting subjects' HM in panoramic video sequences. From this database, we find that the HM data are highly consistent across subjects. Furthermore, we find that deep reinforcement learning (DRL) can be applied to predict HM positions, via maximizing the reward of imitating human HM scanpaths through the agent's actions. Based on our findings, we propose a DRL-based HM prediction (DHP) approach with offline and online versions, called offline-DHP and online-DHP. In offline-DHP, multiple DRL workflows are run to determine potential HM positions at each panoramic frame. Then, a heat map of the potential HM positions, named the HM map, is generated as the output of offline-DHP. In online-DHP, the next HM position of one subject is estimated given the currently observed HM position, which is achieved by developing a DRL algorithm upon the learned offline-DHP model. Finally, the experiments validate that our approach is effective in both offline and online prediction of HM positions for panoramic video, and that the learned offline-DHP model can improve the performance of online-DHP.","","","10.1109/TPAMI.2018.2858783","National Natural Science Foundation of China; Fok Ying Tung Education Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418756","Panoramic video;head movement;reinforcement learning;deep learning","Saliency detection;Two dimensional displays;Predictive models;Hidden Markov models;Databases;Machine learning;Visualization","image sequences;learning (artificial intelligence);video signal processing","online prediction;online-DHP;deep reinforcement learning approach;human attention;panoramic video sequences;HM data;human HM scanpaths;DRL-based HM prediction approach;panoramic frame;HM map;head movement prediction;field of view;offline-DHP model;DRL algorithm","","8","57","","","","","IEEE","IEEE Journals"
"CrossCount: A Deep Learning System for Device-Free Human Counting Using WiFi","O. T. Ibrahim; W. Gomaa; M. Youssef","Wireless Research Center, Egypt-Japan University of Science and Technology, New Borg El Arab, Egypt; Computer Science and Engineering Department, Egypt-Japan University of Science and Technology, New Borg El Arab, Egypt; Computer and Systems Engineering Department, Alexandria University, Alexandria, Egypt","IEEE Sensors Journal","","2019","19","21","9921","9928","Counting humans is an essential part of many people-centric applications. In this paper, we propose CrossCount: an accurate deep-learning-based human count estimator that uses a single WiFi link to estimate the human count in an area of interest. The main idea is to depend on the temporal link-blockage pattern as a discriminant feature that is more robust to wireless channel noise than the signal strength, hence delivering a ubiquitous and accurate human counting system. As part of its design, CrossCount addresses a number of deep learning challenges such as class imbalance and training data augmentation for enhancing the model generalizability. Implementation and evaluation of CrossCount in multiple testbeds show that it can achieve a human counting accuracy to within a maximum of 2 persons 100% of the time. This highlights the promise of CrossCount as a ubiquitous crowd estimator with nonlabor-intensive data collection from off-the-shelf devices.","","","10.1109/JSEN.2019.2928502","Ph.D. Scholarship from the Missions Department, Ministry of Higher Education, Government of Egypt (MoHE); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8760508","Crowd counting;human counting;recurrent neural networks;device-free identification;sequence classification","Wireless fidelity;Training;Training data;Wireless communication;Deep learning;Mathematical model;Wireless sensor networks","learning (artificial intelligence);ubiquitous computing;wireless channels;wireless LAN","deep learning system;device-free human counting;people-centric applications;CrossCount;single WiFi link;temporal link-blockage pattern;wireless channel noise;ubiquitous counting system;accurate human counting system;class imbalance;training data augmentation;human counting accuracy;ubiquitous crowd estimator;deep-learning-based human count estimator;discriminant feature;signal strength;multiple testbeds;nonlabor-intensive data collection;off-the-shelf devices","","","26","","","","","IEEE","IEEE Journals"
"Neural-Response-Based Extreme Learning Machine for Image Classification","H. Li; H. Zhao; H. Li","School of Mathematics and Statistics, Huazhong University of Science and Technology, Wuhan, China; Department of the Mathematics, University of California at Irvine, Irvine, CA, USA; School of Mathematics and Statistics, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","2","539","552","This paper proposes a novel and simple multilayer feature learning method for image classification by employing the extreme learning machine (ELM). The proposed algorithm is composed of two stages: the multilayer ELM (ML-ELM) feature mapping stage and the ELM learning stage. The ML-ELM feature mapping stage is recursively built by alternating between feature map construction and maximum pooling operation. In particular, the input weights for constructing feature maps are randomly generated and hence need not be trained or tuned, which makes the algorithm highly efficient. Moreover, the maximum pooling operation enables the algorithm to be invariant to certain transformations. During the ELM learning stage, elastic-net regularization is proposed to learn the output weight. Elastic-net regularization helps to learn more compact and meaningful output weight. In addition, we preprocess the input data with the dense scale-invariant feature transform operation to improve both the robustness and invariance of the algorithm. To evaluate the effectiveness of the proposed method, several experiments are conducted on three challenging databases. Compared with the conventional deep learning methods and other related ones, the proposed method achieves the best classification results with high computational efficiency.","","","10.1109/TNNLS.2018.2845857","National Natural Science Foundation of China; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8402232","Elastic-net regularization;extreme learning machine (ELM);image classification;multilayer;random weights","Feature extraction;Nonhomogeneous media;Learning systems;Machine learning;Training data;Robustness;Task analysis","feature extraction;image classification;learning (artificial intelligence)","feature maps;ML-ELM feature mapping stage;multilayer ELM feature mapping stage;image classification;neural-response-based extreme learning machine;conventional deep learning methods;dense scale-invariant feature;elastic-net regularization;ELM learning stage;maximum pooling operation","","3","47","","","","","IEEE","IEEE Journals"
"Automatic Design of Convolutional Neural Network for Hyperspectral Image Classification","Y. Chen; K. Zhu; L. Zhu; X. He; P. Ghamisi; J. A. Benediktsson","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","9","7048","7066","Hyperspectral image (HSI) classification is a core task in the remote sensing community, and recently, deep learning-based methods have shown their capability of accurate classification of HSIs. Among the deep learning-based methods, deep convolutional neural networks (CNNs) have been widely used for the HSI classification. In order to obtain a good classification performance, substantial efforts are required to design a proper deep learning architecture. Furthermore, the manually designed architecture may not fit a specific data set very well. In this paper, the idea of automatic CNN for the HSI classification is proposed for the first time. First, a number of operations, including convolution, pooling, identity, and batch normalization, are selected. Then, a gradient descent-based search algorithm is used to effectively find the optimal deep architecture that is evaluated on the validation data set. After that, the best CNN architecture is selected as the model for the HSI classification. Specifically, the automatic 1-D Auto-CNN and 3-D Auto-CNN are used as spectral and spectral-spatial HSI classifiers, respectively. Furthermore, the cutout is introduced as a regularization technique for the HSI spectral-spatial classification to further improve the classification accuracy. The experiments on four widely used hyperspectral data sets (i.e., Salinas, Pavia University, Kennedy Space Center, and Indiana Pines) show that the automatically designed data-dependent CNNs obtain competitive classification accuracy compared with the state-of-the-art methods. In addition, the automatic design of the deep learning architecture opens a new window for future research, showing the huge potential of using neural architectures' optimization capabilities for the accurate HSI classification.","","","10.1109/TGRS.2019.2910603","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703410","Convolutional neural network (CNN);deep learning;hyperspectral image (HSI) classification;neural architecture search (NAS)","Feature extraction;Deep learning;Hyperspectral imaging;Convolution;Training","convolutional neural nets;geophysical image processing;gradient methods;hyperspectral imaging;image classification;learning (artificial intelligence);remote sensing","spectral-spatial HSI classifiers;HSI spectral-spatial classification;widely used hyperspectral data sets;competitive classification accuracy;automatic design;neural architectures;accurate HSI classification;convolutional neural network;hyperspectral image classification;deep learning-based methods;deep convolutional neural networks;proper deep learning architecture;manually designed architecture;automatic CNN;gradient descent-based search algorithm;optimal deep architecture;CNN architecture;Auto-CNN;automatically designed data-dependent CNN","","1","74","","","","","IEEE","IEEE Journals"
"Smart and Fast Data Processing for Deep Learning in Internet of Things: Less is More","S. D. Liang","Department of Computer Science, University of Texas at Austin, Austin, TX, USA","IEEE Internet of Things Journal","","2019","6","4","5981","5989","Recent advances of deep learning have produced encouraging results comparable to and in some cases superior to human experts. However, the large amount of data input has been a daunting task for deep learning to be widely applied in Internet of Things (IoT) with real-time processing. The goal of this paper is to develop smart and fast data processing scheme for more computational efficient deep learning to support adaptive and real-time applications, which will be able to increase the spectrum and energy efficiency in IoT. We propose to apply singular-value decomposition (SVD)-QR algorithm to preprocessing of deep learning for large scale data input. For the mass data input, we apply limited memory subspace optimization for SVD (LMSVD)-QR algorithm to increase the data processing speed. Simulation results in automated handwritten digit recognition show that SVD-QR and LMSVD-QR can tremendously reduce the number of input to deep learning neural network without losing its performance, and both can tremendously increase the data processing speed for deep learning in IoT.","","","10.1109/JIOT.2018.2864579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8430521","Deep learning;Internet of Things (IoT);limited memory subspace optimization for SVD (LMSVD)-QR;neural networks (NNs);singular-value decomposition (SVD)-QR;smart and fast data processing","Machine learning;Internet of Things;Principal component analysis;Error analysis;Data preprocessing;Optimization","handwritten character recognition;Internet of Things;learning (artificial intelligence);neural nets;optimisation;singular value decomposition","singular-value decomposition-QR algorithm;deep learning neural network;real-time processing;smart data processing scheme;fast data processing scheme;Internet of things;limited memory subspace optimization;handwritten digit recognition;LMSVD-QR","","","32","","","","","IEEE","IEEE Journals"
"Deep Learning-Based Detector for OFDM-IM","T. V. Luong; Y. Ko; N. A. Vien; D. H. N. Nguyen; M. Matthaiou","ECIT Institute, Queen’s University Belfast, Belfast, U.K.; ECIT Institute, Queen’s University Belfast, Belfast, U.K.; ECIT Institute, Queen’s University Belfast, Belfast, U.K.; Department of Electrical and Computer Engineering, San Diego State University, San Diego, CA, USA; ECIT Institute, Queen’s University Belfast, Belfast, U.K.","IEEE Wireless Communications Letters","","2019","8","4","1159","1162","This letter presents the first attempt of exploiting deep learning (DL) in the signal detection of orthogonal frequency division multiplexing with index modulation (OFDM-IM) systems. Particularly, we propose a novel DL-based detector termed as DeepIM, which employs a deep neural network with fully connected layers to recover data bits in an OFDM-IM system. To enhance the performance of DeepIM, the received signal and channel vectors are pre-processed based on the domain knowledge before entering the network. Using datasets collected by simulations, DeepIM is first trained offline to minimize the bit error rate (BER) and then the trained model is deployed for the online signal detection of OFDM-IM. Simulation results show that DeepIM can achieve a near-optimal BER with a lower runtime than existing hand-crafted detectors.","","","10.1109/LWC.2019.2909893","Engineering and Physical Sciences Research Council; Leverhulme Trust; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8684894","DeepIM;deep learning;deep neural network;index modulation;low-complexity detector;OFDM-IM","Detectors;Complexity theory;Training;Bit error rate;OFDM;Indexes;Simulation","error statistics;learning (artificial intelligence);neural nets;OFDM modulation;radio networks;signal detection","index modulation;DeepIM;deep neural network;OFDM-IM system;received signal;channel vectors;online signal detection;deep learning based detector;bit error rate;near-optimal BER;orthogonal frequency division multiplexing","","1","13","","","","","IEEE","IEEE Journals"
"Parking Space Status Inference Upon a Deep CNN and Multi-Task Contrastive Network With Spatial Transform","H. T. Vu; C. Huang","Department of Electrical Engineering, National Chung Cheng University, Minxiong, Taiwan; Department of Electrical Engineering, National Chung Cheng University, Minxiong, Taiwan","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","4","1194","1208","Deep learning methods, especially CNNs, have achieved many promising results in a wide range of computer vision applications. However, few studies focused on designing suitable deep learning methods for parking space status inference. As we have known, it is challenging to detect parking spaces in an outdoor environment due to dynamic lighting variations, weather changes, and perspective distortion. By off-the-shelf CNNs, lighting variations might be handled well. However, to realize a practical and robust inference system, we also need to address troublesome problems, such as parking displacements, non-unified car sizes, inter-object occlusion, and perspective distortion. These problems may become even challenging if also considering the difference of space sizes. To overcome the problems, we proposed a custom-tailored deep convolutional and contrastive network with three contributions. First, we introduced a Siamese architecture to learn the contrastive and robust feature descriptor. This helps to reduce the effects owing to the variety of inter-object occlusion. Second, we integrated a convolutional Spatial Transformer Network (STN) to adaptively transform a 3-space input patch according to vehicle sizes and parking displacement. STN also helps to overcome the perspective distortion problem. Third, a multi-task loss function was designed to train the network by simultaneously considering the accuracy of inferring the status of the target space and the semantic smoothness of high-level features. Thereby, the errors caused by inter-object occlusion could be alleviated. To verify the proposed network, we visualized the learned features and analyzed their functionality. Experiments and evaluations have shown the robustness of our system in parking status inference. The real-time system currently running in public parking lots also demonstrates the effectiveness of the proposed deep network.","","","10.1109/TCSVT.2018.2826053","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8337011","Parking status inference;spatial transformer network;siamese network;deep learning;CNNs;multi-task optimization","Feature extraction;Space vehicles;Lighting;Robustness;Distortion;Machine learning;Meteorology","computer vision;convolutional neural nets;feature extraction;learning (artificial intelligence);object detection;traffic engineering computing","parking space status inference;deep CNN;dynamic lighting variations;parking displacement;nonunified car sizes;inter-object occlusion;custom-tailored deep convolutional;perspective distortion problem;multitask loss function;learned features;parking status inference;feature descriptor;convolutional spatial transformer network;deep learning methods;STN;multitask contrastive network;Siamese architecture;outdoor environment;weather changes","","2","38","","","","","IEEE","IEEE Journals"
"Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition","S. Li; W. Deng","Pattern Recognition and Intelligent System Laboratory, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition and Intelligent System Laboratory, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Image Processing","","2019","28","1","356","370","Facial expression is central to human experience, but most previous databases and studies are limited to posed facial behavior under controlled conditions. In this paper, we present a novel facial expression database, Real-world Affective Face Database (RAF-DB), which contains approximately 30 000 facial images with uncontrolled poses and illumination from thousands of individuals of diverse ages and races. During the crowdsourcing annotation, each image is independently labeled by approximately 40 annotators. An expectation-maximization algorithm is developed to reliably estimate the emotion labels, which reveals that real-world faces often express compound or even mixture emotions. A cross-database study between RAF-DB and CK+ database further indicates that the action units of real-world emotions are much more diverse than, or even deviate from, those of laboratory-controlled emotions. To address the recognition of multi-modal expressions in the wild, we propose a new deep locality-preserving convolutional neural network (DLP-CNN) method that aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatter. Benchmark experiments on 7-class basic expressions and 11-class compound expressions, as well as additional experiments on CK+, MMI, and SFEW 2.0 databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning-based methods for expression recognition in the wild. To promote further study, we have made the RAF database, benchmarks, and descriptor encodings publicly available to the research community.","","","10.1109/TIP.2018.2868382","National Natural Science Foundation of China; Beijing Nova Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453893","Expression recognition;basic emotion;compound emotion;deep learning","Databases;Face;Face recognition;Compounds;Reactive power;Machine learning;Reliability","convolution;emotion recognition;expectation-maximisation algorithm;face recognition;feedforward neural nets;learning (artificial intelligence);visual databases","mixture emotions;cross-database study;RAF-DB;CK+ database;real-world emotions;laboratory-controlled emotions;deep locality-preserving convolutional neural network method;DLP-CNN;locality closeness;inter-class scatter;deep learning-based methods;RAF database;unconstrained facial expression recognition;crowdsourcing annotation;expectation-maximization algorithm;emotion labels;express compound;facial images;facial behavior posed;facial expression database;real-world affective face database;multimodal expressions recognition;descriptor encodings","","7","81","","","","","IEEE","IEEE Journals"
"Transferred Deep Learning for Sea Ice Change Detection From Synthetic-Aperture Radar Images","Y. Gao; F. Gao; J. Dong; S. Wang","Qingdao Key Laboratory of Mixed Reality and Virtual Ocean, School of Information Science and Engineering, Ocean University of China, Qingdao, China; Qingdao Key Laboratory of Mixed Reality and Virtual Ocean, School of Information Science and Engineering, Ocean University of China, Qingdao, China; Qingdao Key Laboratory of Mixed Reality and Virtual Ocean, School of Information Science and Engineering, Ocean University of China, Qingdao, China; Qingdao Key Laboratory of Mixed Reality and Virtual Ocean, School of Information Science and Engineering, Ocean University of China, Qingdao, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","10","1655","1659","High-quality sea ice monitoring is crucial to navigation safety and climate research in the polar regions. In this letter, a transferred multilevel fusion network (MLFN) is proposed for sea ice change detection from synthetic-aperture radar (SAR) images. Considering the fact that training data are limited in the task of sea ice change detection, a large data set was used to train the MLFN, and the deep knowledge can be transferred to sea ice analysis. In addition, cascade dense blocks are employed to optimize the convolutional layers. Multilayer feature fusion is introduced to exploit the complementary information among low-, mid-, and high-level feature representations. Therefore, more discriminative feature extraction can be achieved by the MLFN. Furthermore, the fine-tune strategy is utilized to optimize the network parameters. The experimental results on two real sea ice data sets demonstrated that the proposed method achieved better performance than other competitive methods.","","","10.1109/LGRS.2019.2906279","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8684298","Change detection;deep learning;fine-tune;neural network;synthetic-aperture radar (SAR)","Feature extraction;Sea ice;Task analysis;Radar polarimetry;Training;Deep learning","feature extraction;image representation;learning (artificial intelligence);oceanographic techniques;radar imaging;sea ice;synthetic aperture radar","transferred deep learning;sea ice change detection;synthetic-aperture radar images;high-quality sea ice monitoring;transferred multilevel fusion network;MLFN;sea ice analysis;sea ice data sets","","","17","","","","","IEEE","IEEE Journals"
"3-D Deep Feature Construction for Mobile Laser Scanning Point Cloud Registration","Z. Zhang; L. Sun; R. Zhong; D. Chen; Z. Xu; C. Wang; C. Qin; H. Sun; R. Li","National Engineering Laboratory for Surface Transportation Weather Impacts Prevention, Broadvision Engineering Consultants, Kunming, China; National Engineering Laboratory for Surface Transportation Weather Impacts Prevention, Broadvision Engineering Consultants, Kunming, China; National Engineering Laboratory for Surface Transportation Weather Impacts Prevention, Broadvision Engineering Consultants, Kunming, China; College of Civil Engineering, Nanjing Forestry University, Nanjing, China; College of Geoscience and Surveying Engineering, China University of Mining and Technology (Beijing), Beijing, China; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China; State Key Laboratory of Resources and Environmental Information System, Institute of Geographic Sciences and Natural Resources Research, Beijing, China; National Engineering Laboratory for Surface Transportation Weather Impacts Prevention, Broadvision Engineering Consultants, Kunming, China; Beijing Advanced Innovation Center for Imaging Theory and Technology, Capital Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","12","1904","1908","Due to errors in sensors and positioning, there exist mismatches between different phases of mobile laser scanning point clouds, which impede the application of point cloud, such as changing detection and deformation monitoring. To rectify such mismatches, we designed a 3-D deep feature construction method for point cloud registration. The proposed method combines two 3-D convolutional neural networks into a uniform deep learning model to extract 3-D deep features. First, the corresponding points and noncorresponding points are set to train the deep learning model to minimize the distance between corresponding points' features and maximize the distance between features of noncorresponding points. Second, in the test phase, the 3-D deep feature for each keypoint was extracted by the trained deep learning model. This could be used to determine the corresponding points by the k-dimensional tree and random sample consensus (RANSAC) algorithm. Finally, a transformation matrix was calculated based on the corresponding points and was then applied to point cloud registration. The experimental results illustrated that the proposed method of using 3-D deep features is more efficient at a corresponding point search than representatives of three existing methods. It also improved registration accuracy.","","","10.1109/LGRS.2019.2910546","Chinese Academy of Sciences; National Natural Science Foundation of China; State Key Laboratory of Resources and Environmental Information System; State Key Laboratory of Remote Sensing Science; Opening Fund of Key Laboratory of Geohazard Prevention of Hilly Mountains, Ministry of Land and Resources (Fujian Key Laboratory of Geohazard Prevention); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704944","3-D deep features;convolutional neural networks (CNNs);mobile laser scanning (MLS) point clouds;registration","Three-dimensional displays;Feature extraction;Deep learning;Solid modeling;Laboratories;Training;Lasers","convolutional neural nets;feature extraction;image registration;iterative methods;learning (artificial intelligence);matrix algebra;stereo image processing;trees (mathematics)","mobile laser scanning point cloud registration;3D convolutional neural networks;deep learning model;3D deep feature construction method;3D deep features extraction;k-dimensional tree;random sample consensus algorithm;transformation matrix","","1","31","IEEE","","","","IEEE","IEEE Journals"
"Unsupervised Deep Hashing With Adaptive Feature Learning for Image Retrieval","Y. Zhu; Y. Li; S. Wang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Signal Processing Letters","","2019","26","3","395","399","The hashing method is widely used for large-scale image retrieval due to its low time and space complexity. However, the existing deep hashing methods are mainly designed for labeled datasets. Without supervised information, retrieval performance on unlabeled datasets is not guaranteed. In this letter, we propose a novel deep hashing approach for unsupervised image retrieval applications. The contributions are two-fold. First, the pseudolabels are generated using their global features aggregated from the pretrained network and employed as self-supervised information to optimize the objective function of training. Second, adaptive feature learning is used in this deep hashing framework to perform simultaneous hash function learning and global features learning in an unsupervised manner. The experimental results validated the effectiveness of the proposed method, obtaining state-of-the-art performances on several public datasets such as CIFAR-10, Holidays, and Oxford5k.","","","10.1109/LSP.2019.2892233","State Key Development Program in 13th Five-Year; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8607047","Large-scale image retrieval;unsupervised deep hashing;pseudo-labels;adaptive feature learning","Signal processing algorithms;Hash functions;Adaptation models;Image retrieval;Training;Task analysis;Mathematical model","computational complexity;feature extraction;file organisation;image retrieval;learning (artificial intelligence);optimisation","objective function optimization;time complexity;image retrieval;hash function learning;space complexity;unsupervised deep hashing;deep hashing framework;adaptive feature learning;self-supervised information","","1","36","","","","","IEEE","IEEE Journals"
"Deep Q-Learning Aided Networking, Caching, and Computing Resources Allocation in Software-Defined Satellite-Terrestrial Networks","C. Qiu; H. Yao; F. R. Yu; F. Xu; C. Zhao","Key Laboratory of University Wireless Communications, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Carleton University, Ottawa, ON, Canada; Key Laboratory of University Wireless Communications, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of University Wireless Communications, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Vehicular Technology","","2019","68","6","5871","5883","With the development of satellite networks, there is an emerging trend to integrate satellite networks with terrestrial networks, called satellite-terrestrial networks (STNs). The improvements of STNs need innovative information and communication technologies, such as networking, caching, and computing. In this paper, we propose a software-defined STN to manage and orchestrate networking, caching, and computing resources jointly. We formulate the joint resources allocation problem as a joint optimization problem, and use a deep Q-learning approach to solve it. Simulation results show the effectiveness of our proposed scheme.","","","10.1109/TVT.2019.2907682","National Natural Science Foundation of China; BUPT Excellent Ph.D. Students Foundation; Multi-level Intelligent Governance Platform for Urban Public Safety Driven by Big Data; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675467","Satellite-terrestrial networks;software-defined networking (SDN);network virtualization;caching;edge computing;resources allocation;deep Q-learning","Satellites;Satellite broadcasting;Resource management;Virtualization;Computational modeling;5G mobile communication;Cloud computing","learning (artificial intelligence);optimisation;resource allocation;satellite communication;software defined networking;telecommunication computing","caching;software-defined STN;joint resources allocation problem;computing resources allocation;software-defined satellite-terrestrial networks;information-and-communication technologies;joint optimization problem;deep Q-learning aided networking resources allocation;deep Q-learning aided caching resources allocation;deep Q-learning aided computing resources allocation","","3","48","","","","","IEEE","IEEE Journals"
"Scaling Geo-Distributed Network Function Chains: A Prediction and Learning Framework","Z. Luo; C. Wu; Z. Li; W. Zhou","Department of Computer Science, The University of Hong Kong, Hong Kong; Department of Computer Science, The University of Hong Kong, Hong Kong; School of Computer Science, Wuhan University, Wuhan, China; Wireless Network Research Department, Huawei Technologies, Shanghai, China","IEEE Journal on Selected Areas in Communications","","2019","37","8","1838","1850","Geo-distributed virtual network function (VNF) chaining has been useful, such as in network slicing in 5G networks and for network traffic processing in the WAN. Agile scaling of the VNF chains according to real-time traffic rates is the key in network function virtualization. Designing efficient scaling algorithms is challenging, especially for geo-distributed chains, where bandwidth costs and latencies incurred by the WAN traffic are important but difficult to handle in making scaling decisions. Existing studies have largely resorted to optimization algorithms in scaling design. Aiming at better decisions empowered by in-depth learning from experiences, this paper proposes a deep learning-based framework for scaling of the geo-distributed VNF chains, exploring inherent pattern of traffic variation and good deployment strategies over time. We novelly combine a recurrent neural network as the traffic model for predicting upcoming flow rates and a deep reinforcement learning (DRL) agent for making chain placement decisions. We adopt the experience replay technique based on the actor-critic DRL algorithm to optimize the learning results. Trace-driven simulation shows that with limited offline training, our learning framework adapts quickly to traffic dynamics online and achieves lower system costs, compared to the existing representative algorithms.","","","10.1109/JSAC.2019.2927068","Huawei Innovation Research Program (HIRP); Hong Kong Research Grants Council (RGC); National Natural Science Foundation of China; Natural Science Foundation of Hubei Province; Technological Innovation Major Projects of Hubei Province; Science and Technology Program of Wuhan City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8756207","Network function virtualization;service function chain;reinforcement learning;deep learning","Reinforcement learning;Heuristic algorithms;Artificial neural networks;Data centers;Wide area networks;Adaptation models;Predictive models","decision making;learning (artificial intelligence);optimisation;recurrent neural nets;telecommunication traffic;virtualisation;wide area networks","actor-critic DRL algorithm;network function virtualization;WAN traffic;deep learning-based framework;traffic variation;recurrent neural network;deep reinforcement learning agent;scaling algorithms;geodistributed virtual network function chaining;geodistributed VNF chains;chain placement decisions making","","","37","","","","","IEEE","IEEE Journals"
"A Novel Deep Learning Network via Multiscale Inner Product With Locally Connected Feature Extraction for Intelligent Fault Detection","T. Pan; J. Chen; Z. Zhou; C. Wang; S. He","State Key Laboratory for Manufacturing Systems Engineering, Xi'an Jiaotong University, Xi'an, China; State Key Laboratory for Manufacturing Systems Engineering, Xi'an Jiaotong University, Xi'an, China; State Key Laboratory for Manufacturing Systems Engineering, Xi'an Jiaotong University, Xi'an, China; State Key Laboratory for Manufacturing Systems Engineering, Xi'an Jiaotong University, Xi'an, China; Guilin University of Electronic Technology, Guilin, China","IEEE Transactions on Industrial Informatics","","2019","15","9","5119","5128","Intelligent fault detection is an important application of artificial intelligence and has been widely used in many mechanical systems. The shipborne antenna that is a typical and an important mechanical system plays an irreplaceable role in ships. Considering the tough working environment and heavy background noise, fault detection is difficult for the shipborne antenna. Therefore, the paper presents an intelligent fault detection method via multiscale inner product with locally connected feature extraction for shipborne antenna fault detection. Inspired by inner product principle, this paper takes advantage of inner product to capture fault information in the vibration signals and detect the faults in rolling bearing of the shipborne antenna. Meanwhile, multiscale analysis is employed in two layers of the network to improve the feature extraction ability. The local features under different scales are collected and used for fault classification. Finally, the proposed method is verified by three datasets and comparison methods are also developed to show its superiority. Results show that the proposed method can learn sensitive features directly from raw vibration signals and detect the faults in rolling bearing of shipborne antenna effectively.","","","10.1109/TII.2019.2896665","National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630634","Deep learning;inner product;intelligent fault detection;multiscale analysis","Feature extraction;Fault detection;Deep learning;Vibrations;Rolling bearings;Informatics;Antennas","fault diagnosis;feature extraction;learning (artificial intelligence);mechanical engineering computing;rolling bearings;vibrational signal processing","deep learning network;raw vibration signals;rolling bearing;multiscale analysis;fault information;inner product principle;shipborne antenna fault detection;intelligent fault detection method;mechanical systems;artificial intelligence;locally connected feature extraction;multiscale inner product;fault classification;local features","","","30","Traditional","","","","IEEE","IEEE Journals"
"Reconstructible Nonlinear Dimensionality Reduction via Joint Dictionary Learning","X. Wei; H. Shen; Y. Li; X. Tang; F. Wang; M. Kleinsteuber; Y. L. Murphey","Chinese Academy of Sciences, Fujian Institute of Research on the Structure of Matter, Fuzhou, China; Department of Electrical and Computer Engineering, Technical University of Munich, Munich, Germany; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China; Chinese Academy of Sciences, Fujian Institute of Research on the Structure of Matter, Fuzhou, China; Chinese Academy of Sciences, Fujian Institute of Research on the Structure of Matter, Fuzhou, China; Department of Electrical and Computer Engineering, Technical University of Munich, Munich, Germany; Department of Electrical and Computer Engineering, University of Michigan–Dearborn, Dearborn, MI, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","1","175","189","This paper presents a parametric low-dimensional (LD) representation learning method that allows to reconstruct high-dimensional (HD) input vectors in an unsupervised manner. Under the assumption that the HD data and its LD representation share the same or similar local sparse structure, the proposed method achieves reconstructible dimensionality reduction via jointly learning dictionaries in both the original HD data space and its LD representation space. By regarding the sparse representation as a smooth function with respect to a specific dictionary, we construct an encoding-decoding block for learning LD representations from sparse coefficients of HD data. It is expected that this learning process preserves the desirable structure of HD data in the LD representation space, and simultaneously allows a reliable reconstruction from the LD space back to the original HD space. In addition, the proposed single layer encoding-decoding block can be easily extended to deep learning structures. Numerical experiments on both synthetic data sets and real images show that the proposed method achieves strongly competitive and robust performance in data DR, reconstruction, and synthesis, even on heavily corrupted data. The proposed method can be used as an alternative approach to compressive sensing (CS); however, it can outperform the traditional CS methods in: 1) task-driven learning problems, such as 2-D/3-D data visualization, and 2) data reconstruction at a lower dimensional space.","","","10.1109/TNNLS.2018.2836802","Chinese Academy of Sciences; National Natural Science Foundation of China; National Science Found for Young Scholars; Fujian Provincial Department of Science and Technology; External Cooperation Program of CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8372926","Compressive sensing (CS);coupled dictionary learning (DL);reconstructible nonlinear dimensionality reduction (DR);sparse representation","Dictionaries;High definition video;Image reconstruction;Task analysis;Machine learning;Manifolds;Learning systems","compressed sensing;decoding;encoding;image reconstruction;image representation;learning (artificial intelligence)","reconstructible nonlinear dimensionality reduction;high-dimensional input vectors;sparse coefficients;deep learning structures;dictionary learning;corrupted data;HD data space;low-dimensional representation learning method;encoding-decoding block;LD representations;compressive sensing;CS methods;data reconstruction;2-D-3-D data visualization","","1","56","","","","","IEEE","IEEE Journals"
"Naturalness-Aware Deep No-Reference Image Quality Assessment","B. Yan; B. Bare; W. Tan","School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China","IEEE Transactions on Multimedia","","2019","21","10","2603","2615","No-reference image quality assessment (NR-IQA) is a non-trivial task, because it is hard to find a pristine counterpart for an image in real applications, such as image selection, high quality image recommendation, etc. In recent years, deep learning-based NR-IQA methods emerged and achieved better performance than previous methods. In this paper, we present a novel deep neural networks-based multi-task learning approach for NR-IQA. Our proposed network is designed by a multi-task learning manner that consists of two tasks, namely, natural scene statistics (NSS) features prediction task and the quality score prediction task. NSS features prediction is an auxiliary task, which helps the quality score prediction task to learn better mapping between the input image and its quality score. The main contribution of this work is to integrate the NSS features prediction task to the deep learning-based image quality prediction task to improve the representation ability and generalization ability. To the best of our knowledge, it is the first attempt. We conduct the same database validation and cross database validation experiments on LIVE1, TID20132, CSIQ3, LIVE multiply distorted image quality database (LIVE MD)4, CID20135, and LIVE in the wild image quality challenge (LIVE challenge)6 databases to verify the superiority and generalization ability of the proposed method. Experimental results confirm the superior performance of our method on the same database validation; our method especially achieves 0.984 and 0.986 on the LIVE image quality assessment database in terms of the Pearson linear correlation coefficient (PLCC) and Spearman rank-order correlation coefficient (SROCC), respectively. Also, experimental results from cross database validation verify the strong generalization ability of our method. Specifically, our method gains significant improvement up to 21.8% on unseen distortion types.","","","10.1109/TMM.2019.2904879","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666733","No-reference image quality assessment;natural scene statistics;multi-task learning;naturalness-aware deep image quality assessment","Image quality;Task analysis;Databases;Feature extraction;Distortion;Neural networks","distortion;feature extraction;image representation;learning (artificial intelligence);natural scenes;neural nets;visual databases","nontrivial task;image selection;high quality image recommendation;deep learning-based NR-IQA methods;deep neural networks-based multitask learning approach;multitask learning manner;quality score prediction task;auxiliary task;input image;NSS features prediction task;deep learning-based image quality prediction task;representation ability;generalization ability;cross database validation experiments;distorted image quality database;wild image quality challenge;LIVE image quality assessment database;naturalness-aware deep no-reference image quality assessment;natural scene statistics feature prediction task;Spearman rank-order correlation coefficient;Pearson linear correlation coefficient","","","53","Traditional","","","","IEEE","IEEE Journals"
"Real-time comprehensive glass container inspection system based on deep learning framework","Q. Liang; S. Xiang; J. Long; W. Sun; Y. Wang; D. Zhang","Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, National Engineering Laboratory for Robot Vision Perception and Control, Hunan University, People's Republic of China; Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, National Engineering Laboratory for Robot Vision Perception and Control, Hunan University, People's Republic of China; Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, National Engineering Laboratory for Robot Vision Perception and Control, Hunan University, People's Republic of China; Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, National Engineering Laboratory for Robot Vision Perception and Control, Hunan University, People's Republic of China; Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, National Engineering Laboratory for Robot Vision Perception and Control, Hunan University, People's Republic of China; York University, Canada","Electronics Letters","","2019","55","3","131","132","Reusable glass containers have become extremely popular in recent years due to their cost effectiveness. Quality control such as inspecting and identifying container defects is an essential part of the reusable containers production systems. Many aspects of modern society already benefit from developments in machine learning (ML) technology. However, to the authors' knowledge, the ML technology approaches have not been extensively applied in the practical inspection instrumentations for glass containers. In this Letter, a comprehensive inspection system for reusable containers based on a deep learning framework is proposed. The experimental results demonstrated that the developed system is capable of inspecting defects of glass containers with superior accuracy and speed. After the success of other practical applications with deep learning approaches, they wish that this Letter could inspire more and more research results in deep learning methods to be widely applied to comprehensive inspection tasks.","","","10.1049/el.2018.6934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8634628","","","containers;glass products;inspection;learning (artificial intelligence);production engineering computing;quality control","reusable glass containers;quality control;machine learning technology;comprehensive inspection system;deep learning methods;comprehensive glass container inspection system","","","","","","","","IET","IET Journals"
"HyperFace: A Deep Multi-Task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition","R. Ranjan; V. M. Patel; R. Chellappa","Department of Electrical and Computer Engineering, University of Maryland, College Park, MD; Rutgers University, New Brunswick, NJ; Department of Electrical and Computer Engineering, University of Maryland, College Park, MD","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","1","121","135","We present an algorithm for simultaneous face detection, landmarks localization, pose estimation and gender recognition using deep convolutional neural networks (CNN). The proposed method called, HyperFace, fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features. It exploits the synergy among the tasks which boosts up their individual performances. Additionally, we propose two variants of HyperFace: (1) HyperFace-ResNet that builds on the ResNet-101 model and achieves significant improvement in performance, and (2) Fast-HyperFace that uses a high recall fast face detector for generating region proposals to improve the speed of the algorithm. Extensive experiments show that the proposed models are able to capture both global and local information in faces and performs significantly better than many competitive algorithms for each of these four tasks.","","","10.1109/TPAMI.2017.2781233","Office of the Director of National Intelligence (ODNI); Intelligence Advanced Research Projects Activity (IARPA); IARPA R&D Contract; ODNI; IARPA; U.S. Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170321","Face detection;landmarks localization;head pose estimation;gender recognition;deep convolutional neural networks;multi-task learning","Face;Face detection;Pose estimation;Face recognition;Feature extraction","convolution;face recognition;feedforward neural nets;learning (artificial intelligence);pose estimation","deep multitask learning framework;landmark localization;pose estimation;gender recognition;simultaneous face detection;landmarks localization;deep convolutional neural networks;deep CNN;multitask learning algorithm;fused features;global information;local information;HyperFace-ResNet;Fast-HyperFace","","43","71","","","","","IEEE","IEEE Journals"
"Deep Semantic-Preserving Ordinal Hashing for Cross-Modal Similarity Search","L. Jin; K. Li; Z. Li; F. Xiao; G. Qi; J. Tang","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Facebook, Menlo Park, CA, USA; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Computer Science, University of Central Florida, Orlando, FL, USA; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","5","1429","1440","Cross-modal hashing has attracted increasing research attention due to its efficiency for large-scale multimedia retrieval. With simultaneous feature representation and hash function learning, deep cross-modal hashing (DCMH) methods have shown superior performance. However, most existing methods on DCMH adopt binary quantization functions (e.g., sign(·)) to generate hash codes, which limit the retrieval performance since binary quantization functions are sensitive to the variations of numeric values. Toward this end, we propose a novel end-to-end ranking-based hashing framework, in this paper, termed as deep semantic-preserving ordinal hashing (DSPOH), to learn hash functions with deep neural networks by exploring the ranking structure of feature dimensions. In DSPOH, the ordinal representation, which encodes the relative rank ordering of feature dimensions, is explored to generate hash codes. Such ordinal embedding benefits from the numeric stability of rank correlation measures. To make the hash codes discriminative, the ordinal representation is expected to well predict the class labels so that the ranking-based hash function learning is optimally compatible with the label predicting. Meanwhile, the intermodality similarity is preserved to guarantee that the hash codes of different modalities are consistent. Importantly, DSPOH can be effectively integrated with different types of network architectures, which demonstrates the flexibility and scalability of our proposed hashing framework. Extensive experiments on three widely used multimodal data sets show that DSPOH outperforms state of the art for cross-modal retrieval tasks.","","","10.1109/TNNLS.2018.2869601","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478207","Class labels;cross-modal hashing;deep neural networks;intermodality similarity;ranking-based hash function","Semantics;Feature extraction;Correlation;Neural networks;Quantization (signal);Scalability;Learning systems","binary codes;file organisation;image representation;image retrieval;information retrieval;learning (artificial intelligence);neural nets;optimisation","cross-modal similarity search;binary quantization functions;DSPOH;deep neural networks;feature dimensions;ordinal representation;cross-modal retrieval tasks;feature representation;hash function learning;deep cross-modal hashing methods;deep semantic-preserving ordinal hashing;large-scale multimedia retrieval;ranking-based hashing framework;rank correlation measures","","1","61","","","","","IEEE","IEEE Journals"
"Unsupervised Deep Learning for MU-SIMO Joint Transmitter and Noncoherent Receiver Design","S. Xue; Y. Ma; N. Yi; R. Tafazolli","Institute for Communication Systems, University of Surrey, Guildford, U.K.; Institute for Communication Systems, University of Surrey, Guildford, U.K.; Institute for Communication Systems, University of Surrey, Guildford, U.K.; Institute for Communication Systems, University of Surrey, Guildford, U.K.","IEEE Wireless Communications Letters","","2019","8","1","177","180","This letter aims to handle the joint transmitter and noncoherent receiver optimization for multiuser single-input multiple-output (MU-SIMO) communications through unsupervised deep learning. It is shown that MU-SIMO can be modeled as a deep neural network with three essential layers, which include a partially-connected linear layer for joint multiuser waveform design at the transmitter side, and two nonlinear layers for the noncoherent signal detection. The proposed approach demonstrates remarkable MU-SIMO noncoherent communication performance in Rayleigh fading channels.","","","10.1109/LWC.2018.2865563","U.K. 5G Innovation Centre; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8437142","Unsupervised deep learning;joint transmitter and receiver design;noncoherent detection;multiuser single-input and multiple-output (MU-SIMO)","Transmitters;Receivers;Machine learning;Optimization;Neural networks;Training;Wireless communication","multiuser channels;neural nets;radio receivers;radio transmitters;Rayleigh channels;signal detection;SIMO communication;unsupervised learning","unsupervised deep learning;MU-SIMO joint transmitter;noncoherent receiver design;noncoherent receiver optimization;multiuser single-input multiple-output communications;deep neural network;joint multiuser waveform design;transmitter side;nonlinear layers;noncoherent signal detection;remarkable MU-SIMO noncoherent communication performance;partially-connected linear layer;Rayleigh fading channels","","4","12","","","","","IEEE","IEEE Journals"
"A Framework for Remote Sensing Images Processing Using Deep Learning Techniques","R. Cresson","IRSTEA (French Research Institute for Environment and Agriculture)/UMR TETIS, Montpellier, France","IEEE Geoscience and Remote Sensing Letters","","2019","16","1","25","29","Deep learning (DL) techniques are becoming increasingly important to solve a number of image processing tasks. Among common algorithms, convolutional neural network- and recurrent neural network-based systems achieve state-of-the-art results on satellite and aerial imagery in many applications. While these approaches are subject to scientific interest, there is currently no operational and generic implementation available at the user level for the remote sensing (RS) community. In this letter, we present a framework enabling the use of DL techniques with RS images and geospatial data. Our solution takes roots in two extensively used open-source libraries, the RS image processing library Orfeo ToolBox and the high-performance numerical computation library TensorFlow. It can apply deep nets without restriction on image size and is computationally efficient, regardless of hardware configuration.","","","10.1109/LGRS.2018.2867949","public funds received through GEOSUD, a project of the Investissements d’Avenir program managed by the French National Research Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8465982","Aerial images;deep learning (DL);neural networks;Orfeo Toolbox (OTB);remote sensing (RS);TensorFlow (TF)","Pipelines;Machine learning;Image processing;Libraries;Tensile stress;Radio frequency;Remote sensing","geophysical image processing;learning (artificial intelligence);recurrent neural nets;remote sensing;software libraries","remote sensing images;deep learning techniques;image processing tasks;convolutional neural network;recurrent neural network-based systems;state-of-the-art results;aerial imagery;scientific interest;operational implementation;generic implementation;user level;remote sensing community;DL techniques;RS images;geospatial data;open-source libraries;RS image processing library Orfeo ToolBox;deep nets;image size;high-performance numerical computation library;TensorFlow","","","28","","","","","IEEE","IEEE Journals"
"An Incremental Deep Convolutional Computation Model for Feature Learning on Industrial Big Data","P. Li; Z. Chen; L. T. Yang; J. Gao; Q. Zhang; M. J. Deen","School of Software Technology, Dalian University of Technology, Dalian, China; School of Software Technology, Dalian University of Technology, Dalian, China; Department of Computer Science, St. Francis Xavier University, Antigonish, NS, Canada; School of Software Technology, Dalian University of Technology, Dalian, China; Department of Computer Science, St. Francis Xavier University, Antigonish, NS, Canada; Department of Electrical and Computer Engineering, McMaster University, Hamilton, ON, Canada","IEEE Transactions on Industrial Informatics","","2019","15","3","1341","1349","The deep convolutional computation model (DCCM) enabled remarkable progress in feature learning of industrial big data in Internet of Things. However, as a typical static deep learning model, it is difficult to learn features for incremental industrial big data. To solve this problem, we propose an incremental DCCM by developing two incremental algorithms, i.e., parameter-incremental algorithm and structure-incremental algorithm. The parameter-incremental algorithm aims to incrementally train the fully connected layers together with fine tuning for incorporating the new knowledge into the prior one. Then, the structure-incremental algorithm is used to transfer the previous knowledge by introducing an updating rule of the tensor convolutional, pooling, and fully connected layers. Furthermore, the dropout strategy is extended into the tensor fully connected layer to improve the robustness of the proposed model. Finally, extensive experiments are carried out on the representative datasets including CIFRA and CUAVE to justify the proposed model in terms of adaption, preservation, and convergence efficiency.","","","10.1109/TII.2018.2871084","National Natural Science Foundation of China; Major Science and Technology Planning Project of Guangdong Province; Doctoral Scientific Research Foundation of Liaoning Province; Dalian University of Technology Fundamental Research Fund; Canada Research Chair program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8468087","Deep convolutional computation model (DCCM);incremental learning;industrial big data;tensor computation","Tensile stress;Computational modeling;Data models;Big Data;Adaptation models;Training;Heuristic algorithms","Big Data;convolutional neural nets;Internet of Things;learning (artificial intelligence);tensors","incremental deep convolutional computation model;feature learning;incremental industrial big data;incremental DCCM;parameter-incremental algorithm;structure-incremental algorithm;static deep learning model;Internet of Things","","","27","","","","","IEEE","IEEE Journals"
"A Multiscale Deep Framework for Ocean Fronts Detection and Fine-Grained Location","X. Sun; C. Wang; J. Dong; E. Lima; Y. Yang","Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","2","178","182","Ocean front plays an important role in marine fishery production and biogeochemical cycling. This letter proposes a multiscale deep framework to meet the need for automatic ocean front detection and fine-grained location. The framework mainly focuses on bringing a well-trained deep learning model into front detection and location on the global satellite sea surface temperature image. First, a multiscale scanner is designed to divide the ocean into small areas of different scales. Then, we introduce the deep model to determine that a front has occurred, and translate the global image into binary ones of various grained. Here, an overlapping scanning way is suggested to locate the front in a small region. Finally, all the binary images are scale-weighted fused into one image, which presents the center and periphery with different brightness levels. Experimental illustrations on three typical areas of the ocean are featured with six scanning scales to show the effectiveness and practical use of the proposed framework. Moreover, the comparison experiments with the traditional method also show its advantages.","","","10.1109/LGRS.2018.2869647","National Natural Science Foundation of China; Key Research and Development Program of Shandong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8469062","Deep learning;front detection;front location;ocean front","Machine learning;Ocean temperature;Silicon;Image edge detection;Detectors","aquaculture;feature extraction;geophysical image processing;image classification;learning (artificial intelligence);ocean temperature;oceanographic regions;oceanographic techniques;remote sensing","ocean fronts detection;fine-grained location;marine fishery production;biogeochemical cycling;multiscale deep framework;automatic ocean;deep learning model;global satellite sea surface temperature image;multiscale scanner;deep model;binary images;brightness levels;well-trained deep learning model","","","16","","","","","IEEE","IEEE Journals"
"On-Policy Dataset Synthesis for Learning Robot Grasping Policies Using Fully Convolutional Deep Networks","V. Satish; J. Mahler; K. Goldberg","Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, USA; Department of Electrical Engineering and Computer Science and Department of Industrial Operations and Engineering Research, University of California, Berkeley, Berkeley, USA; Department of Electrical Engineering and Computer Science and Department of Industrial Operations and Engineering Research, University of California, Berkeley, Berkeley, USA","IEEE Robotics and Automation Letters","","2019","4","2","1357","1364","Rapid and reliable robot grasping for a diverse set of objects has applications from warehouse automation to home decluttering. One promising approach is to learn deep policies from synthetic training datasets of point clouds, grasps, and rewards sampled using analytic models with stochastic noise models for domain randomization. In this letter, we explore how the distribution of synthetic training examples affects the rate and reliability of the learned robot policy. We propose a synthetic data sampling distribution that combines grasps sampled from the policy action set with guiding samples from a robust grasping supervisor that has full state knowledge. We use this to train a robot policy based on a fully convolutional network architecture that evaluates millions of grasp candidates in 4-DOF (3-D position and planar orientation). Physical robot experiments suggest that a policy based on fully convolutional grasp quality CNNs (FC-GQ-CNNs) can plan grasps in 0.625 s, considering 5000x more grasps than our prior policy based on iterative grasp sampling and evaluation. This computational efficiency improves rate and reliability, achieving 296 mean picks per hour (MPPH) compared to 250 MPPH for iterative policies. Sensitivity experiments explore the effect of supervisor guidance level and granularity of the policy action space. Code, datasets, videos, and supplementary material can be found at http://berkeleyautomation.github.io/fcgqcnn.","","","10.1109/LRA.2019.2895878","AUTOLAB at UC Berkeley in affiliation with the Berkeley AI Research (BAIR) Lab; Berkeley Deep Drive (BDD); Real-Time Intelligent Secure Execution (RISE) Lab; CITRIS “People and Robots” (CPAR) Initiative; Siemens, Google, Amazon Robotics, Toyota Research Institute, Autodesk, ABB, Samsung, Knapp, Loccioni, Honda, Intel, Comcast, Cisco, and Hewlett-Packard; PhotoNeo, Nvidia, and Intuitive Surgical; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629272","Grasping;Deep Learning in Robotics and Automation","Grasping;Training;Three-dimensional displays;Robot sensing systems;Reliability;Planning","convolutional neural nets;dexterous manipulators;grippers;iterative methods;learning (artificial intelligence);neurocontrollers;warehouse automation","policy dataset synthesis;robot grasping policies;convolutional deep networks;reliable robot grasping;deep policies;synthetic training datasets;analytic models;stochastic noise models;synthetic training examples;reliability;learned robot policy;synthetic data sampling distribution;guiding samples;robust grasping supervisor;fully convolutional network architecture;grasp candidates;physical robot experiments;iterative grasp sampling;iterative policies;policy action space;fully convolutional grasp quality CNN","","1","38","","","","","IEEE","IEEE Journals"
"Conditional Random Field and Deep Feature Learning for Hyperspectral Image Classification","F. I. Alam; J. Zhou; A. W. Liew; X. Jia; J. Chanussot; Y. Gao","Institute of Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia; Institute of Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia; Institute of Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; GIPSA-Lab, CNRS, Grenoble INP, University of Grenoble Alpes, Grenoble, France; Institute of Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","3","1612","1628","Image classification is considered to be one of the critical tasks in hyperspectral remote sensing image processing. Recently, a convolutional neural network (CNN) has established itself as a powerful model in classification by demonstrating excellent performances. The use of a graphical model such as a conditional random field (CRF) contributes further in capturing contextual information and thus improving the classification performance. In this paper, we propose a method to classify hyperspectral images by considering both spectral and spatial information via a combined framework consisting of CNN and CRF. We use multiple spectral band groups to learn deep features using CNN, and then formulate deep CRF with CNN-based unary and pairwise potential functions to effectively extract the semantic correlations between patches consisting of 3-D data cubes. Furthermore, we introduce a deep deconvolution network that improves the final classification performance. We also introduced a new data set and experimented our proposed method on it along with several widely adopted benchmark data sets to evaluate the effectiveness of our method. By comparing our results with those from several state-of-the-art models, we show the promising potential of our method.","","","10.1109/TGRS.2018.2867679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8469063","Conditional random field (CRF);convolutional neural network (CNN);deep learning;image classification","Hyperspectral imaging;Training;Feature extraction;Kernel;Convolution;Data models","convolutional neural nets;feature extraction;geophysical image processing;image classification;learning (artificial intelligence);remote sensing","deep deconvolution network;conditional random field;deep feature learning;hyperspectral image classification;hyperspectral remote sensing image processing;convolutional neural network;graphical model;contextual information;hyperspectral images;spectral information;spatial information;multiple spectral band groups;deep features;deep CRF;3-D data cubes;classification performance;CNN-based unary potential functions;CNN-based pairwise potential functions","","3","41","","","","","IEEE","IEEE Journals"
"Adversarial Deep Learning in EEG Biometrics","O. Özdenizci; Y. Wang; T. Koike-Akino; D. Erdoğmuş","Department of Electrical and Computer Engineering, Cognitive Systems Laboratory, Northeastern University, Boston, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Department of Electrical and Computer Engineering, Cognitive Systems Laboratory, Northeastern University, Boston, MA, USA","IEEE Signal Processing Letters","","2019","26","5","710","714","Deep learning methods for person identification based on electroencephalographic (EEG) brain activity encounters the problem of exploiting the temporally correlated structures or recording session specific variability within EEG. Furthermore, recent methods have mostly trained and evaluated based on single session EEG data. We address this problem from an invariant representation-learning perspective. We propose an adversarial inference approach to extend such deep learning models to learn session-invariant person-discriminative representations that can provide robustness in terms of longitudinal usability. Using adversarial learning within a deep convolutional network, we empirically assess and show improvements with our approach based on longitudinally collected EEG data for person identification from half-second EEG epochs.","","","10.1109/LSP.2019.2906826","National Science Foundation; NIDLRR; National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675355","Person identification;biometrics;EEG;adversarial learning;invariant representation;convolutional networks","Electroencephalography;Brain modeling;Biometrics (access control);Biological system modeling;Convolution;Training;Feature extraction","brain;convolutional neural nets;electroencephalography;learning (artificial intelligence);medical signal processing","deep learning models;session-invariant person-discriminative representations;deep convolutional network;longitudinally collected EEG data;person identification;adversarial deep learning;EEG biometrics;electroencephalographic brain activity;temporally correlated structures;recording session specific variability;single session EEG data;invariant representation-learning perspective;adversarial inference approach","","1","45","","","","","IEEE","IEEE Journals"
"Buffer-Aware Streaming in Small-Scale Wireless Networks: A Deep Reinforcement Learning Approach","Y. Guo; F. R. Yu; J. An; K. Yang; Y. He; V. C. M. Leung","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Communications Engineering, Dalian University of Technology, Dalian, China; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada","IEEE Transactions on Vehicular Technology","","2019","68","7","6891","6902","Buffer-aware video streaming, which exploits the available storage space in user device to store the prefetched video data in good channels for video data use in poor channels, has been proved to have the potential to reduce the impact of fluctuating wireless channels on user-perceived video performance. However, in practical wireless networks, due to the unknown channel state and video rate, providing buffer-aware video streaming service to wireless user is a challenging problem. In this paper, with the aim to design an autonomous wireless video streaming system, we apply the deep reinforcement learning approach to dynamic resource optimization for wireless buffer-aware video streaming under unknown channel state and video rate. Specifically, we define a reward function for buffer-aware video streaming as the effective video streaming time when neither video-playback overflow nor video-playback underflow occurs. We propose a Markov decision process based problem formulation of the joint bandwidth allocation and buffer management for maximizing the effective video streaming time of all users. The optimal bandwidth allocation and buffer management policy is learned from training a deep neural network based on a deep reinforcement learning algorithm. We simulate the proposed algorithm in Tensorflow. Simulation results verify that the proposed deep reinforcement learning approach is effective for buffer-aware video streaming in wireless networks.","","","10.1109/TVT.2019.2909055","China Postdoctoral Science Foundation; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8681145","Buffer-aware video streaming;wireless networks;deep reinforcement learning;Markov decision process","Streaming media;Optimization;Wireless networks;Heuristic algorithms;Reinforcement learning;Resource management","bandwidth allocation;learning (artificial intelligence);Markov processes;neural nets;optimisation;telecommunication computing;telecommunication traffic;video streaming;wireless channels","video-playback overflow;video-playback underflow;deep reinforcement learning approach;small-scale wireless networks;prefetched video data;wireless channels;user-perceived video performance;unknown channel state;buffer-aware video streaming service;autonomous wireless video streaming system;wireless buffer-aware video;Markov decision process;bandwidth allocation;Tensorflow","","1","41","","","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Offloading and Resource Allocation in Vehicle Edge Computing and Networks","Y. Liu; H. Yu; S. Xie; Y. Zhang","School of Automation, Guangdong University of Technology, Key Lab. of Ministry of Education and Guangdong Province Key Lab. of IoT Information Technology, Guangzhou, China; College of Information Science and Engineering, Hunan Normal University, Changsha, China; School of Automation, Guangdong University of Technology, and State Key Lab. of Precision Electronic Manufacturing Technology and Equipment, Guangzhou, China; University of Oslo, Oslo, Norway","IEEE Transactions on Vehicular Technology","","2019","68","11","11158","11168","Mobile Edge Computing (MEC) is a promising technology to extend the diverse services to the edge of Internet of Things (IoT) system. However, the static edge server deployment may cause “service hole” in IoT networks in which the location and service requests of the User Equipments (UEs) may be dynamically changing. In this paper, we firstly explore a vehicle edge computing network architecture in which the vehicles can act as the mobile edge servers to provide computation services for nearby UEs. Then, we propose as vehicle-assisted offloading scheme for UEs while considering the delay of the computation task. Accordingly, an optimization problem is formulated to maximize the long-term utility of the vehicle edge computing network. Considering the stochastic vehicle traffic, dynamic computation requests and time-varying communication conditions, the problem is further formulated as a semi-Markov process and two reinforcement learning methods: Q-learning based method and deep reinforcement learning (DRL) method, are proposed to obtain the optimal policies of computation offloading and resource allocation. Finally, we analyze the effectiveness of the proposed scheme in the vehicular edge computing network by giving numerical results.","","","10.1109/TVT.2019.2935450","National Natural Science Foundation of China; Pearl River S and T Nova Program of Guangzhou; The European Unions Horizon 2020 research and innovation programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798668","Vehicle edge computing;resource allocation;IoT;deep reinforcement learning","Task analysis;Edge computing;Servers;Internet of Things;Computational modeling;Iron;Reinforcement learning","cellular radio;cloud computing;Internet of Things;learning (artificial intelligence);Markov processes;mobile computing;resource allocation;telecommunication traffic","vehicle-assisted offloading scheme;computation task;vehicle edge computing network;stochastic vehicle traffic;mobile edge computing;computation services;mobile edge servers;service requests;IoT networks;service hole;diverse services;vehicular edge computing network;resource allocation;computation offloading;deep reinforcement learning method;reinforcement learning methods;time-varying communication conditions;dynamic computation requests","","1","37","IEEE","","","","IEEE","IEEE Journals"
"Transferred Deep Learning-Based Change Detection in Remote Sensing Images","M. Yang; L. Jiao; F. Liu; B. Hou; S. Yang","Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education and the Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","9","6960","6973","Supervised deep neural networks (DNNs) have been extensively used in diverse tasks. Generally, training such DNNs with superior performance requires a large amount of labeled data. However, it is time-consuming and expensive to manually label the data, especially for tasks in remote sensing, e.g., change detection. The situation motivates us to resort to the existing related images with labels, from which the concept of change can be adapted to new images. However, the distributions of the related labeled images (source domain) and unlabeled new images (target domain) are similar but not identical. It impedes a change detection model learned from source domains being well applied to the target domain. In this paper, we propose a transferred deep learning-based change detection framework to solve this problem. It consists of pretraining and fine-tuning stages. In the pretraining process, we propose two tasks to be learned simultaneously, namely, change detection for the source domain with labels and reconstruction of the unlabeled target data. The auxiliary task aims to reconstruct the difference image (DI) for the target domain. DI is an effective feature, such that the auxiliary task is of much relevance to change detection. The lower layers are shared between these two tasks in the training process. It mitigates the distribution discrepancy between the source and target domains and makes the concept of change from the source domain adapt to the target domain. In addition, we evaluate three modes of the U-net architecture to merge the information for a pair of patches. To fine-tune the change detection network (CDN) for the target domain, two strategies are exploited to select the pixels that have a high possibility of being correctly classified by an unsupervised approach. The proposed method demonstrates an excellent capacity for adapting the concept of change from the source domain to the target domain. It outperforms the state-of-the-art change detection methods via experimental results on real remote sensing data sets.","","","10.1109/TGRS.2019.2909781","National Natural Science Foundation of China; State Key Program of National Natural Science of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703425","Adaptation;change detection;deep neural networks (DNNs);reconstruction;remote sensing","Remote sensing;Task analysis;Image reconstruction;Feature extraction;Training;Adaptation models;Neural networks","feature extraction;geophysical image processing;image classification;image reconstruction;neural net architecture;remote sensing;supervised learning;unsupervised learning","remote sensing images;deep neural networks;geographical area;U-net architecture;difference image reconstruction;supervised deep neural networks;transferred deep learning-based change detection","","","47","","","","","IEEE","IEEE Journals"
"Online Vehicle Routing With Neural Combinatorial Optimization and Deep Reinforcement Learning","J. J. Q. Yu; W. Yu; J. Gu","Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Automatic Control, National Polytechnic Institute (CINVESTAV-IPN), Mexico City, Mexico; Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong","IEEE Transactions on Intelligent Transportation Systems","","2019","20","10","3806","3817","Online vehicle routing is an important task of the modern transportation service provider. Contributed by the ever-increasing real-time demand on the transportation system, especially small-parcel last-mile delivery requests, vehicle route generation is becoming more computationally complex than before. The existing routing algorithms are mostly based on mathematical programming, which requires huge computation time in city-size transportation networks. To develop routes with minimal time, in this paper, we propose a novel deep reinforcement learning-based neural combinatorial optimization strategy. Specifically, we transform the online routing problem to a vehicle tour generation problem, and propose a structural graph embedded pointer network to develop these tours iteratively. Furthermore, since constructing supervised training data for the neural network is impractical due to the high computation complexity, we propose a deep reinforcement learning mechanism with an unsupervised auxiliary network to train the model parameters. A multisampling scheme is also devised to further improve the system performance. Since the parameter training process is offline, the proposed strategy can achieve a superior online route generation speed. To assess the proposed strategy, we conduct comprehensive case studies with a real-world transportation network. The simulation results show that the proposed strategy can significantly outperform conventional strategies with limited computation time in both static and dynamic logistic systems. In addition, the influence of control parameters on the system performance is investigated.","","","10.1109/TITS.2019.2909109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693516","Online vehicle routing;logistic system;neural combinatorial optimization;deep reinforcement learning;intelligent transportation","Logistics;Transportation;Green products;Routing;Optimization;Vehicle routing;Computational modeling","combinatorial mathematics;computational complexity;graph theory;intelligent transportation systems;learning (artificial intelligence);neural nets;optimisation;vehicle routing","vehicle tour generation problem;neural network;high computation complexity;deep reinforcement learning mechanism;unsupervised auxiliary network;online vehicle routing;transportation service provider;last-mile delivery requests;vehicle route generation;city-size transportation networks;deep reinforcement learning-based neural combinatorial optimization strategy;online routing problem;neural combinatorial optimization;tructural graph embedded pointer networ;multisampling scheme;dynamic logistic systems","","1","40","","","","","IEEE","IEEE Journals"
"A Deep Learning-Based Approach to Forecast Ionospheric Delays for GPS Signals","I. Srivani; G. Siva Vara Prasad; D. Venkata Ratnam","Department of Electronics & Communication Engineering, Koneru Lakshmaiah Education Foundation, K L University, Vaddeswaram, Guntur, India; Department of Electronics & Communication Engineering, Koneru Lakshmaiah Education Foundation, K L University, Vaddeswaram, Guntur, India; Department of Electronics & Communication Engineering, Koneru Lakshmaiah Education Foundation, K L University, Vaddeswaram, Guntur, India","IEEE Geoscience and Remote Sensing Letters","","2019","16","8","1180","1184","This letter proposes the implementation of ionospheric forecasting model based on the long short-term memory (LSTM) networks. Ionospheric region produces time delay for radio wave propagation of global positioning system (GPS) satellites. The ionospheric delays for GPS signals degrade the position accuracy in the measurements for precise navigation and positioning services. Utilizing the emerging artificial intelligence mathematical tools to forecast ionospheric disturbances using GPS-estimated total electron content (TEC) observations is decisive. In this letter, multi-input LSTM forecasting technique is investigated and tested for evaluating its capability in forecasting the ionospheric delays over Bengaluru station (16.26° N, 80.44° E) using eight years (2009-2016) of GPS measured vertical TEC (VTEC) time-series data. The assessment of the LSTM model performance during geomagnetic quiet and disturbed conditions is carried out in comparison with artificial neural networks model and International Reference Ionosphere (IRI-2016) model based on statistical parameters like root-mean-square error and coefficient of determination (R2). The experimental analysis delineates that the proposed LSTM model has provided the correlation of 0.99 with the GPS-measured VTEC and with a forecasting error of 1-2 TEC units.","","","10.1109/LGRS.2019.2895112","Department of Science and Technology, New Delhi, India; FIST Program and Science and Engineering Research Board, Government of India, ECR Scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8638517","Deep learning;forecast;Global Navigation Satellite System;ionospheric delays;long short-term memory (LSTM) model","Predictive models;Global Positioning System;Forecasting;Logic gates;Data models;Deep learning;Artificial neural networks","Global Positioning System;ionospheric disturbances;ionospheric electromagnetic wave propagation;ionospheric techniques;learning (artificial intelligence);neural nets;radiowave propagation;total electron content (atmosphere)","deep learning-based approach;ionospheric delays;GPS signals;ionospheric forecasting model;short-term memory networks;ionospheric region;time delay;radio wave propagation;global positioning system satellites;position accuracy;precise navigation;positioning services;ionospheric disturbances;GPS-estimated total electron content observations;multiinput LSTM forecasting technique;vertical TEC time-series data;LSTM model performance;artificial neural networks model;International Reference Ionosphere model;GPS-measured VTEC;forecasting error;artificial intelligence mathematical tools","","2","28","","","","","IEEE","IEEE Journals"
"Lifelong Learning for Scene Recognition in Remote Sensing Images","M. Zhai; H. Liu; F. Sun","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","9","1472","1476","The development of visual sensing technologies has made it possible to obtain some high resolution and to gather many high-resolution satellite images. To make the best use of these images, it is essential to be able to recognize and retrieve their intrinsic scene information. The problem of scene recognition in remote sensing images has recently aroused considerable interest, mainly due to the great success achieved by deep learning methods in generic image classification. Nevertheless, such methods usually require large amounts of labeled data. By contrast, remote sensing images are relatively scarce and expensive to obtain. Moreover, data sets from different aerospace research institutions exhibit large disparities. In order to address these problems, we propose a model based on a meta-learning method with the ability of learning a classifier from just few-shot samples. With the proposed model, the knowledge learned from one data set can be easily adapted to a new data set, which, in turn, would serve in the lifelong few-shot learning. Scene-level image recognition experiments, on public high-resolution remote sensing image data sets, validate our proposed lifelong few-shot learning model.","","","10.1109/LGRS.2019.2897652","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8662768","Gradient-based method;lifelong few-shot learning;meta-learning;remote sensing image;scene-classification","Task analysis;Remote sensing;Adaptation models;Data models;Image recognition;Deep learning;Satellites","geophysical image processing;image classification;image resolution;learning (artificial intelligence);remote sensing","generic image classification;meta-learning method;scene-level image recognition experiments;image data sets;lifelong few-shot learning model;scene recognition;visual sensing technologies;high-resolution satellite images;intrinsic scene information;deep learning methods;public high-resolution remote sensing image data sets;aerospace research institutions","","","18","","","","","IEEE","IEEE Journals"
"Knowledge-based Collaborative Deep Learning for Benign-Malignant Lung Nodule Classification on Chest CT","Y. Xie; Y. Xia; J. Zhang; Y. Song; D. Feng; M. Fulham; W. Cai","Shaanxi Key Lab of Speech and Image Information Processing, Centre for Multidisciplinary Convergence Computing, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; Shaanxi Key Lab of Speech and Image Information Processing, Centre for Multidisciplinary Convergence Computing, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; Shaanxi Key Lab of Speech and Image Information Processing, Centre for Multidisciplinary Convergence Computing, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, NSW, Australia; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, NSW, Australia; Department of Molecular Imaging, Royal Prince Alfred Hospital, NSW, Australia; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Medical Imaging","","2019","38","4","991","1004","The accurate identification of malignant lung nodules on chest CT is critical for the early detection of lung cancer, which also offers patients the best chance of cure. Deep learning methods have recently been successfully introduced to computer vision problems, although substantial challenges remain in the detection of malignant nodules due to the lack of large training data sets. In this paper, we propose a multi-view knowledge-based collaborative (MV-KBC) deep model to separate malignant from benign nodules using limited chest CT data. Our model learns 3-D lung nodule characteristics by decomposing a 3-D nodule into nine fixed views. For each view, we construct a knowledge-based collaborative (KBC) submodel, where three types of image patches are designed to fine-tune three pre-trained ResNet-50 networks that characterize the nodules' overall appearance, voxel, and shape heterogeneity, respectively. We jointly use the nine KBC submodels to classify lung nodules with an adaptive weighting scheme learned during the error back propagation, which enables the MV-KBC model to be trained in an end-to-end manner. The penalty loss function is used for better reduction of the false negative rate with a minimal effect on the overall performance of the MV-KBC model. We tested our method on the benchmark LIDC-IDRI data set and compared it to the five state-of-the-art classification approaches. Our results show that the MV-KBC model achieved an accuracy of 91.60% for lung nodule classification with an AUC of 95.70%. These results are markedly superior to the state-of-the-art approaches.","","","10.1109/TMI.2018.2876510","National Natural Science Foundation of China; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8494708","Lung nodule classification;deep learning;collaborative learning;computed tomography (CT)","Lung;Cancer;Feature extraction;Computed tomography;Three-dimensional displays;Shape;Machine learning","cancer;computerised tomography;feature extraction;image classification;image segmentation;learning (artificial intelligence);lung;medical image processing;tumours","LIDC-IDRI data;3D lung nodule characteristics;training data sets;computer vision problems;lung cancer;malignant lung nodules;benign-malignant lung nodule classification;knowledge-based collaborative deep;MV-KBC model;KBC submodels;chest CT data;benign nodules","","2","62","","","","","IEEE","IEEE Journals"
"Pixel Objectness: Learning to Segment Generic Objects Automatically in Images and Videos","B. Xiong; S. D. Jain; K. Grauman","Department of Computer Science, University of Texas at Austin, Austin, TX, USA; Department of Computer Science, University of Texas at Austin, Austin, TX, USA; Department of Computer Science, University of Texas at Austin, Austin, TX, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","11","2677","2692","We propose an end-to-end learning framework for segmenting generic objects in both images and videos. Given a novel image or video, our approach produces a pixel-level mask for all “object-like” regions-even for object categories never seen during training. We formulate the task as a structured prediction problem of assigning an object/background label to each pixel, implemented using a deep fully convolutional network. When applied to a video, our model further incorporates a motion stream, and the network learns to combine both appearance and motion and attempts to extract all prominent objects whether they are moving or not. Beyond the core model, a second contribution of our approach is how it leverages varying strengths of training annotations. Pixel-level annotations are quite difficult to obtain, yet crucial for training a deep network approach for segmentation. Thus we propose ways to exploit weakly labeled data for learning dense foreground segmentation. For images, we show the value in mixing object category examples with image-level labels together with relatively few images with boundary-level annotations. For video, we show how to bootstrap weakly annotated videos together with the network trained for image segmentation. Through experiments on multiple challenging image and video segmentation benchmarks, our method offers consistently strong results and improves the state-of-the-art for fully automatic segmentation of generic (unseen) objects. In addition, we demonstrate how our approach benefits image retrieval and image retargeting, both of which flourish when given our high-quality foreground maps. Code, models, and videos are at: http://vision.cs.utexas.edu/projects/pixelobjectness/.","","","10.1109/TPAMI.2018.2865794","ONR YIP; AWS Machine Learning Research Award; DARPA Lifelong Learning Machines; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8438912","Image segmentation;video segmentation;deep learning;foreground segmentation","Image segmentation;Videos;Motion segmentation;Training;Proposals;Object segmentation;Task analysis","feature extraction;image retrieval;image segmentation;learning (artificial intelligence);video signal processing","image retrieval;image retargeting;pixel objectness;end-to-end learning framework;pixel-level mask;object categories;structured prediction problem;deep fully convolutional network;motion stream;pixel-level annotations;deep network approach;learning dense foreground segmentation;image-level labels;boundary-level annotations;image segmentation;fully automatic segmentation;generic objects segment","","","95","","","","","IEEE","IEEE Journals"
"Enhanced Motion-Compensated Video Coding With Deep Virtual Reference Frame Generation","L. Zhao; S. Wang; X. Zhang; S. Wang; S. Ma; W. Gao","School of Electronic Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; School of Electronic Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing, China; School of Electronic Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing, China; School of Electronic Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing, China","IEEE Transactions on Image Processing","","2019","28","10","4832","4844","In this paper, we propose an efficient inter prediction scheme by introducing the deep virtual reference frame (VRF), which serves better reference in the temporal redundancy removal process of video coding. In particular, the high quality VRF is generated with the deep learning-based frame rate up conversion (FRUC) algorithm from two reconstructed bi-directional frames, which is subsequently incorporated into the reference list serving as the high quality reference. Moreover, to alleviate the compression artifacts of VRF, we develop a convolutional neural network (CNN)-based enhancement model to further improve its quality. To facilitate better utilization of the VRF, a CTU level coding mode termed as direct virtual reference frame (DVRF) is devised, which achieves better trade-off between compression performance and complexity. The proposed scheme is integrated into HM-16.6 and JEM-7.1 software platforms, and the simulation results under random access (RA) configuration demonstrate significant superiority of the proposed method. When adding VRF to RPS, more than 6% average BD-rate gain is achieved for HEVC test sequences on HM-16.6, and 0.8% BD-rate gain is observed based on JEM-7.1 software. Regarding the DVRF mode, 3.6% bitrate saving is achieved on HM-16.6 with the computational complexity effectively reduced.","","","10.1109/TIP.2019.2913545","National Natural Science Foundation of China; National Basic Research Program of China (973 Program); Peking University; Hong Kong RGC Early Career Scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704997","Inter prediction;virtual reference frame;deep learning;video coding","Image coding;Image reconstruction;Deep learning;Convolution;Video coding;Indexes;Encoding","computational complexity;convolutional neural nets;image reconstruction;image sequences;learning (artificial intelligence);motion compensation;video coding","enhanced motion-compensated video coding;deep virtual reference frame generation;efficient inter prediction scheme;temporal redundancy removal process;high quality VRF;deep learning-based frame rate;conversion algorithm;reconstructed bi-directional frames;reference list serving;high quality reference;compression artifacts;convolutional neural network-based enhancement model;CTU level coding mode;direct virtual reference frame;HM-16 software platform;JEM-7.1 software platform;HEVC test sequences","","","46","","","","","IEEE","IEEE Journals"
"Trimmed categorical cross-entropy for deep learning with label noise","A. Rusiecki","Wroclaw University of Science and Technology, Poland","Electronics Letters","","2019","55","6","319","320","Deep learning methods are nowadays considered as state-of-the-art approach in many sophisticated problems, such as computer vision, speech understanding or natural language processing. However, their performance relies on the quality of large annotated datasets. If the data are not well-annotated and label noise occur, such data-driven models become less reliable. In this Letter, the authors present very simple way to make the training process robust to noisy labels. Without changing network architecture and learning algorithm, the authors apply modified error measure that improves network generalisation when trained with label noise. Preliminary results obtained for deep convolutional neural networks, trained with novel trimmed categorical cross-entropy loss function, revealed its improved robustness for several levels of label noise.","","","10.1049/el.2018.7980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666219","","","convolutional neural nets;entropy;learning (artificial intelligence)","modified error measure;natural language processing;trimmed categorical cross-entropy loss function;large annotated datasets;deep convolutional neural networks;data-driven models;label noise;speech understanding;computer vision;deep learning methods","","","24","","","","","IET","IET Journals"
"Deep Learning for Single Image Super-Resolution: A Brief Review","W. Yang; X. Zhang; Y. Tian; W. Wang; J. Xue; Q. Liao","Department of Electronic Engineering, Graduate School at Shenzhen, Tsinghua University, Beijing, China; Department of Electronic Engineering, Graduate School at Shenzhen, Tsinghua University, Beijing, China; University of Rochester, Rochester, NY, USA; Department of Electronic Engineering, Graduate School at Shenzhen, Tsinghua University, Beijing, China; Department of Statistical Science, University College London, London, U.K.; Department of Electronic Engineering, Graduate School at Shenzhen, Tsinghua University, Beijing, China","IEEE Transactions on Multimedia","","2019","21","12","3106","3121","Single image super-resolution (SISR) is a notoriously challenging ill-posed problem that aims to obtain a high-resolution output from one of its low-resolution versions. Recently, powerful deep learning algorithms have been applied to SISR and have achieved state-of-the-art performance. In this survey, we review representative deep learning-based SISR methods and group them into two categories according to their contributions to two essential aspects of SISR: The exploration of efficient neural network architectures for SISR and the development of effective optimization objectives for deep SISR learning. For each category, a baseline is first established, and several critical limitations of the baseline are summarized. Then, representative works on overcoming these limitations are presented based on their original content, as well as our critical exposition and analyses, and relevant comparisons are conducted from a variety of perspectives. Finally, we conclude this review with some current challenges and future trends in SISR that leverage deep learning algorithms.","","","10.1109/TMM.2019.2919431","National Natural Science Foundation of China; National Basic Research Program of China (973 Program); Special Foundation for the Development of Strategic Emerging Industries of Shenzhen; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8723565","Single image super-resolution;deep learning;neural networks;objective function","Image resolution;Deep learning;Machine learning algorithms;Image reconstruction;Neural networks","","","","3","127","IEEE","","","","IEEE","IEEE Journals"
"Automatic Plaque Detection in IVOCT Pullbacks Using Convolutional Neural Networks","N. Gessert; M. Lutz; M. Heyder; S. Latus; D. M. Leistner; Y. S. Abdelwahed; A. Schlaefer","Institute of Medical Technology, Hamburg University of Technology, Hamburg, Germany; Universitätsklinikum Schleswig-Holstein, Kiel, Germany; Institute of Medical Technology, Hamburg University of Technology, Hamburg, Germany; Institute of Medical Technology, Hamburg University of Technology, Hamburg, Germany; Charité Universitätsmedizin Berlin, Berlin, Germany; Charité Universitätsmedizin Berlin, Berlin, Germany; Institute of Medical Technology, Hamburg University of Technology, Hamburg, Germany","IEEE Transactions on Medical Imaging","","2019","38","2","426","434","Coronary heart disease is a common cause of death despite being preventable. To treat the underlying plaque deposits in the arterial walls, intravascular optical coherence tomography can be used by experts to detect and characterize the lesions. In clinical routine, hundreds of images are acquired for each patient, which require automatic plaque detection for fast and accurate decision support. So far, automatic approaches rely on classic machine learning methods and deep learning solutions have rarely been studied. Given the success of deep learning methods with other imaging modalities, a thorough understanding of deep learning-based plaque detection for future clinical decision support systems is required. We address this issue with a new data set consisting of in vivo patient images labeled by three trained experts. Using this data set, we employ the state-of-the-art deep learning models that directly learn plaque classification from the images. For improved performance, we study different transfer learning approaches. Furthermore, we investigate the use of Cartesian and polar image representations and employ data augmentation techniques tailored to each representation. We fuse both representations in a multi-path architecture for more effective feature exploitation. Last, we address the challenge of plaque differentiation in addition to detection. Overall, we find that our combined model performs best with an accuracy of 91.7%, a sensitivity of 90.9%, and a specificity of 92.4%. Our results indicate that building a deep learning-based clinical decision support system for plaque detection is feasible.","","","10.1109/TMI.2018.2865659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8438495","IVOCT;deep learning;plaque detection;transfer learning","Machine learning;Image segmentation;Arteries;Image resolution;Biomedical imaging;Diseases","biomedical optical imaging;blood vessels;cardiology;cardiovascular system;convolutional neural nets;decision support systems;diseases;image classification;image representation;learning (artificial intelligence);medical image processing;optical tomography","plaque classification;polar image representations;plaque differentiation;automatic plaque detection;IVOCT pullbacks;convolutional neural networks;coronary heart disease;intravascular optical coherence tomography;deep learning-based plaque detection;future clinical decision support systems;vivo patient images;data augmentation;machine learning;deep learning;transfer learning","","3","44","","","","","IEEE","IEEE Journals"
"A Joint Learning Algorithm for Complex-Valued T-F Masks in Deep Learning-Based Single-Channel Speech Enhancement Systems","J. Lee; H. Kang","Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","6","1098","1108","This paper presents a joint learning algorithm for complex-valued time-frequency (T-F) masks in single-channel speech enhancement systems. Most speech enhancement algorithms operating in a single-channel microphone environment aim to enhance the magnitude component in a T-F domain, while the input noisy phase component is used directly without any processing. Consequently, the mismatch between the processed magnitude and the unprocessed phase degrades the sound quality. To address this issue, a learning method of targeting a T-F mask that is defined in a complex domain has recently been proposed. However, due to a wide dynamic range and an irregular spectrogram pattern of the complex-valued T-F mask, the learning process is difficult even with a large-scale deep learning network. Moreover, the learning process targeting the T-F mask itself does not directly minimize the distortion in spectra or time domains. In order to address these concerns, we focus on three issues: 1) an effective estimation of complex numbers with a wide dynamic range; 2) a learning method that is directly related to speech enhancement performance; and 3) a way to resolve the mismatch between the estimated magnitude and phase spectra. In this study, we propose objective functions that can solve each of these issues and train the network by minimizing them with a joint learning framework. The evaluation results demonstrate that the proposed learning algorithm achieves significant performance improvement in various objective measures and subjective preference listening test.","","","10.1109/TASLP.2019.2910638","National Research Foundation of Korea; Korean Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691424","Single-channel speech enhancement;complex-valued time-frequency mask;exact time-domain reconstruction;spectrogram consistency","Speech enhancement;Time-domain analysis;Linear programming;Noise measurement;Learning systems;Dynamic range","learning (artificial intelligence);microphones;speech enhancement;time-frequency analysis","sound quality;irregular spectrogram pattern;complex-valued time-frequency masks;complex-valued T-F masks;single-channel microphone environment;single-channel speech enhancement systems;speech enhancement algorithms;joint learning algorithm;phase spectra;large-scale deep learning network;input noisy phase component","","","45","","","","","IEEE","IEEE Journals"
"Learning Discriminative Compact Representation for Hyperspectral Imagery Classification","L. Zhang; J. Zhang; W. Wei; Y. Zhang","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","10","8276","8289","Abundant spectral information of hyperspectral images (HSIs) has shown an obvious advantage in improving the performance of classification in the remote sensing domain. However, this is paid by the expensive consumption on the computation, transmission, as well as storage of HSIs. To address this problem, we propose to learn the discriminative compact representation for HSIs classification, which not only greatly reduces the data redundancy in the image but also preserves the discriminative information required for pixelwise classification in HSIs. To this end, we present a multi-task deep learning framework, which integrates HSIs autoencoding and classification into a two-branch deep neural network for jointly learning. In the network, we employ an encoder block to learn the compact representation of the input HSI via compression in the spectral domain. Being fed with the compact representation, the autoencoding branch then employs a decoder block to reconstruct the input HSI, while the classification branch utilizes a classifier block to predict the label for each pixel. Through end-to-end joint learning, the compact representation is not only informative enough to accurately reconstruct the original HSI but also discriminative enough to appropriately label each pixel with the trained classier. Sufficient experimental results on four HSIs classification data sets demonstrate the effectiveness of the proposed framework.","","","10.1109/TGRS.2019.2919938","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8741172","Autoencoding;band selection;compact representation learning;hyperspectral images (HSIs) classification","Image coding;Hyperspectral imaging;Deep learning;Decoding;Neural networks;Redundancy","data compression;geophysical image processing;hyperspectral imaging;image classification;image reconstruction;learning (artificial intelligence);neural nets;remote sensing","autoencoding branch;classification branch;end-to-end joint learning;hyperspectral imagery classification;hyperspectral images;remote sensing domain;pixelwise classification;multitask deep learning framework;two-branch deep neural network;discriminative compact representation learning","","","49","","","","","IEEE","IEEE Journals"
"Visual Permutation Learning","R. Santa Cruz; B. Fernando; A. Cherian; S. Gould","Australian Centre for Robotic Vision, Australian National University, Canberra, ACT, Australia; A*STAR, Singapore; Mitsubishi Electric Research Labs (MERL), Cambridge, MA, USA; Australian Centre for Robotic Vision, Australian National University, Canberra, ACT, Australia","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","12","3100","3114","We present a principled approach to uncover the structure of visual data by solving a deep learning task coined visual permutation learning. The goal of this task is to find the permutation that recovers the structure of data from shuffled versions of it. In the case of natural images, this task boils down to recovering the original image from patches shuffled by an unknown permutation matrix. Permutation matrices are discrete, thereby posing difficulties for gradient-based optimization methods. To this end, we resort to a continuous approximation using doubly-stochastic matrices and formulate a novel bi-level optimization problem on such matrices that learns to recover the permutation. Unfortunately, such a scheme leads to expensive gradient computations. We circumvent this issue by further proposing a computationally cheap scheme for generating doubly stochastic matrices based on Sinkhorn iterations. To implement our approach we propose DeepPermNet, an end-to-end CNN model for this task. The utility of DeepPermNet is demonstrated on three challenging computer vision problems, namely, relative attributes learning, supervised learning-to-rank, and self-supervised representation learning. Our results show state-of-the-art performance on the Public Figures and OSR benchmarks for relative attributes learning, chronological and interestingness image ranking for supervised learning-to-rank, and competitive results in the classification and segmentation tasks of the PASCAL VOC dataset for self-supervised representation learning.","","","10.1109/TPAMI.2018.2873701","Australian Research Council; National Computational Infrastructure; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8481554","Permutation learning;self-supervised learning;relative attributes;representation learning;learning-to-rank","Visualization;Task analysis;Machine learning;Image sequences;Computational modeling;Computer vision;Predictive models","computer vision;convolutional neural nets;data structures;gradient methods;image classification;image representation;image segmentation;matrix algebra;optimisation;stochastic processes;supervised learning","classification tasks;segmentation tasks;self-supervised representation learning;visual permutation learning;visual data structure;deep learning;natural images;permutation matrix;gradient-based optimization;bilevel optimization problem;doubly stochastic matrices;end-to-end CNN model;computer vision;supervised learning-to-rank;Sinkhorn iterations;DeepPermNet;image representations","","","82","","","","","IEEE","IEEE Journals"
"Automatic Fruit Classification Using Deep Learning for Industrial Applications","M. S. Hossain; M. Al-Hammadi; G. Muhammad","Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia","IEEE Transactions on Industrial Informatics","","2019","15","2","1027","1034","Fruit classification is an important task in many industrial applications. A fruit classification system may be used to help a supermarket cashier identify the fruit species and prices. It may also be used to help people decide whether specific fruit species meet their dietary requirements. In this paper, we propose an efficient framework for fruit classification using deep learning. More specifically, the framework is based on two different deep learning architectures. The first is a proposed light model of six convolutional neural network layers, whereas the second is a fine-tuned visual geometry group-16 pretrained deep learning model. Two color image datasets, one of which is publicly available, are used to evaluate the proposed framework. The first dataset (dataset 1) consists of clear fruit images, whereas the second dataset (dataset 2) contains fruit images that are challenging to classify. Classification accuracies of 99.49% and 99.75% were achieved on dataset 1 for the first and second models, respectively. On dataset 2, the first and second models obtained accuracies of 85.43% and 96.75%, respectively.","","","10.1109/TII.2018.2875149","Deanship of Scientific Research, King Saud University; King Saud University, Riyadh, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8488544","Computer vision;convolutional neural networks (CNNs);deep learning;fruit classification;VGG-16","Machine learning;Shape;Image color analysis;Support vector machines;Informatics;Feature extraction;Neural networks","food products;food safety;geometry;image classification;image colour analysis;learning (artificial intelligence);neural nets;product quality","deep learning model;color image datasets;automatic fruit classification;industrial applications;convolutional neural network layers;supermarket cashier identify;fruit price;dietary requirements;fine-tuned visual geometry","","2","29","","","","","IEEE","IEEE Journals"
"Deep Representation Learning With Part Loss for Person Re-Identification","H. Yao; S. Zhang; R. Hong; Y. Zhang; C. Xu; Q. Tian","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Electronic Engineering and Computer Science, Peking University, Beijing, China; Department of Computer Science and Technology, University of Technology, Hefei, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, The University of Texas at San Antonio, San Antonio, TX, USA","IEEE Transactions on Image Processing","","2019","28","6","2860","2871","Learning discriminative representations for unseen person images is critical for person re-identification (ReID). Most of the current approaches learn deep representations in classification tasks, which essentially minimize the empirical classification risk on the training set. As shown in our experiments, such representations easily get over-fitted on a discriminative human body part on the training set. To gain the discriminative power on unseen person images, we propose a deep representation learning procedure named part loss network, to minimize both the empirical classification risk on training person images and the representation learning risk on unseen person images. The representation learning risk is evaluated by the proposed part loss, which automatically detects human body parts and computes the person classification loss on each part separately. Compared with traditional global classification loss, simultaneously considering part loss enforces the deep network to learn representations for different body parts and gain the discriminative power on unseen persons. Experimental results on three person ReID datasets, i.e., Market1501, CUHK03 and VIPeR, show that our representation outperforms existing deep representations.","","","10.1109/TIP.2019.2891888","National Postdoctoral Programme for Innovative Talents; National Natural Science Foundation of China; Key Research Program of Frontier Sciences, CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8607050","Person re-identification;representation learning;part loss networks;convolutional neural networks","Training;Feature extraction;Measurement;Probes;Convolutional neural networks;Cameras;Reliability","image classification;image representation;learning (artificial intelligence);neural nets;object detection","person re-identification;unseen person images;empirical classification risk;training set;discriminative human body part;discriminative power;deep representation learning procedure;part loss network;representation learning risk;person classification loss;deep network;person ReID datasets;discriminative representation learning;automatic human body part detection;Market1501;CUHK03;VIPeR","","12","65","","","","","IEEE","IEEE Journals"
"Deep Group-Wise Fully Convolutional Network for Co-Saliency Detection With Graph Propagation","L. Wei; S. Zhao; O. E. F. Bourahla; X. Li; F. Wu; Y. Zhuang","College of Computer Science, Zhejiang University, Hangzhou, China; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China","IEEE Transactions on Image Processing","","2019","28","10","5052","5063","A key problem in co-saliency detection is how to effectively model the interactive relationship of a whole image group and the individual perspective of each image in a united data-driven manner. In this paper, we propose a group-wise deep co-saliency detection approach to address the co-saliency object discovery problem based on the fully convolutional network (FCN). The proposed approach captures the group-wise interaction information for group images by learning a semantics-aware image representation based on a convolutional neural network, which adaptively learns the group-wise features for co-saliency detection. Furthermore, the proposed approach discovers the collaborative and interactive relationships between group-wise feature representation and single-image individual feature representation and models this in a collaborative learning framework. Then, we set up a unified deep learning scheme to jointly optimize the process of group-wise feature representation learning and collaborative learning, leading to more reliable and robust co-saliency detection results. Finally, we present a graph Laplacian regularized nonlinear regression model for saliency refinement. The experimental results demonstrate the effectiveness of our approach in comparison with the state-of-the-art approaches.","","","10.1109/TIP.2019.2909649","Natural Science Foundation of Zhejiang Province; National Basic Research Program of China; National Natural Science Foundation of China; Zhejiang University; Tencent AI Lab Rhino-Bird Joint Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691877","Co-saliency object detection;group-wise interaction;jointly optimize","Feature extraction;Object detection;Task analysis;Saliency detection;Semantics;Collaborative work;Deep learning","convolutional neural nets;feature extraction;graph theory;image classification;image representation;learning (artificial intelligence);object detection;regression analysis","group-wise deep co-saliency detection approach;co-saliency object discovery problem;group-wise interaction information;semantics-aware image representation;group-wise features;single-image individual feature representation;collaborative learning framework;unified deep learning scheme;reliable co-saliency detection results;robust co-saliency detection results;deep group-wise fully convolutional neural network;graph propagation;group-wise feature representation learning;graph Laplacian regularized nonlinear regression model","","","59","","","","","IEEE","IEEE Journals"
"Rotation-Based Deep Forest for Hyperspectral Imagery Classification","X. Cao; L. Wen; Y. Ge; J. Zhao; L. Jiao","School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; Laboratories and Equipments Department, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","7","1105","1109","In recent years, deep learning methods have been widely used for the classification of hyperspectral images (HSIs). However, the training of deep models is very time-consuming. In addition, the rare labeled samples of remote sensing images also limit the classification performance of deep models. In this letter, a simple deep learning model, a rotation-based deep forest (RBDF), is proposed for the classification of HSIs. Specifically, the output probability of each layer is used as the supplement feature of the next layer. The rotation forest is used to increase the discriminative power of spectral features and neighboring pixels are used to introduce spatial information. The RBDF consumes much less training time than traditional deep models. Experimental results based on three HSIs demonstrate that the proposed method achieves the state-of-the-art classification performance. In addition, the RBDF obtains satisfied classification results with very few training samples.","","","10.1109/LGRS.2019.2892117","Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625718","Classification;deep forest;hyperspectral imagery;rotation forest (ROF)","Training;Hyperspectral imaging;Forestry;Feature extraction;Spatial resolution;Deep learning","geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);probability;remote sensing","HSIs;rare labeled samples;remote sensing images;rotation-based deep forest;RBDF;hyperspectral imagery classification;deep learning methods;probability;spectral features","","","28","","","","","IEEE","IEEE Journals"
"Improved Search in Hamming Space Using Deep Multi-Index Hashing","H. Lai; Y. Pan; S. Liu; Z. Weng; J. Yin","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","9","2844","2855","Similarity-preserving hashing is a widely used method for nearest neighbor search in large-scale image retrieval tasks. Considerable research has been conducted on deep-network-based hashing approaches to improve the performance. However, the binary codes generated from deep networks may be not uniformly distributed over the Hamming space, which will greatly increase the retrieval time. To this end, we propose a deep-network-based multi-index hashing (MIH) for retrieval efficiency. We first introduce the MIH mechanism into the proposed deep architecture, which divides the binary codes into multiple substrings. Each substring corresponds to one hash table. Then, we add the two balanced constraints to obtain more uniformly distributed binary codes: 1) balanced substrings, where the Hamming distances of each substring are equal for any two binary codes and 2) balanced hash buckets, where the sizes of each bucket are equal. Extensive evaluations on several benchmark image retrieval data sets show that the learned balanced binary codes bring dramatic speedups and achieve comparable performance over the existing baselines.","","","10.1109/TCSVT.2018.2869921","National Natural Science Foundation of China; Research Foundation of Science and Technology Plan Project in Guangdong Province; CCF-Tencent Open Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463534","Image retrieval;deep multi-index hashing;deep-learning;nearest neighbor search","","binary codes;file organisation;image retrieval;learning (artificial intelligence);search problems","multiple substrings;hash table;uniformly distributed binary codes;Hamming distances;benchmark image retrieval data sets;learned balanced binary codes;hamming space;deep multiindex hashing;similarity-preserving hashing;nearest neighbor search;large-scale image retrieval tasks;deep networks;retrieval time;deep-network-based multiindex hashing;retrieval efficiency;MIH mechanism;deep architecture","","","42","","","","","IEEE","IEEE Journals"
"Spatial Deep Learning for Wireless Scheduling","W. Cui; K. Shen; W. Yu","The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada","IEEE Journal on Selected Areas in Communications","","2019","37","6","1248","1261","The optimal scheduling of interfering links in a dense wireless network with full frequency reuse is a challenging task. The traditional method involves first estimating all the interfering channel strengths and then optimizing the scheduling based on the model. This model-based method is, however, resource intensive and computationally hard because channel estimation is expensive in dense networks; furthermore, finding even a locally optimal solution of the resulting optimization problem may be computationally complex. This paper shows that by using a deep learning approach, it is possible to bypass the channel estimation and to schedule links efficiently based solely on the geographic locations of the transmitters and the receivers due to the fact that in many propagation environments, the wireless channel strength is largely a function of the distance-dependent path-loss. This is accomplished by unsupervised training over randomly deployed networks and by using a novel neural network architecture that computes the geographic spatial convolutions of the interfering or interfered neighboring nodes along with subsequent multiple feedback stages to learn the optimum solution. The resulting neural network gives a near-optimal performance for sum-rate maximization and is capable of generalizing to larger deployment areas and to deployments of different link densities. Moreover, to provide fairness, this paper proposes a novel scheduling approach that utilizes the sum-rate optimal scheduling algorithm over judiciously chosen subsets of links for maximizing a proportional fairness objective over the network. The proposed approach shows highly competitive and generalizable network utility maximization results.","","","10.1109/JSAC.2019.2904352","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664604","Deep learning;discrete optimization;geographic location;proportional fairness;scheduling;spatial convolution","Optimal scheduling;Wireless communication;Neural networks;Channel estimation;Transmitters;Receivers","computational complexity;learning (artificial intelligence);mobile radio;neural nets;optimisation;radio networks;resource allocation;telecommunication scheduling;wireless channels","geographic spatial convolutions;interfered neighboring nodes;subsequent multiple feedback stages;near-optimal performance;sum-rate maximization;sum-rate optimal scheduling algorithm;generalizable network utility maximization results;wireless scheduling;interfering links;dense wireless network;interfering channel strengths;channel estimation;dense networks;locally optimal solution;propagation environments;wireless channel strength;distance-dependent path-loss;randomly deployed networks;neural network;link densities;full frequency reuse;optimization problem;deep learning;neural network architecture;Spatial Deep Learning;deployment areas","","3","26","","","","","IEEE","IEEE Journals"
"Late fusion of deep learning and handcrafted visual features for biomedical image modality classification","S. L. Lee; M. R. Zare; H. Muller","Monash University Malaysia, Malaysia; University of Leicester, UK; University of Applied Sciences Western Switzerland (HES-SO) Valais, Switzerland","IET Image Processing","","2019","13","2","382","391","Much of medical knowledge is stored in the biomedical literature, collected in archives like PubMed Central that continue to grow rapidly. A significant part of this knowledge is contained in images with limited metadata available which makes it difficult to explore the visual knowledge in the biomedical literature. Thus, extraction of metadata from visual content is important. One important piece of metadata is the type of the image, which could be one of the various medical imaging modalities such as X-ray, computed tomography or magnetic resonance images and also of general graphs that are frequent in the literature. This study explores a late, score-based fusion of several deep convolutional neural networks with a traditional hand-crafted bag of visual words classifier to classify images from the biomedical literature into image types or modalities. It achieved a classification accuracy of 85.51% on the ImageCLEF 2013 modality classification task, which is better than the best visual methods in the challenge that the data were produced for, and similar compared to mixed methods that make use of both visual and textual information. It achieved similarly good results of 84.23 and 87.04% classification accuracy before and after augmentation, respectively, on the related ImageCLEF 2016 subfigure classification task.","","","10.1049/iet-ipr.2018.5054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8650039","","","biomedical MRI;computerised tomography;feature extraction;feedforward neural nets;image classification;image fusion;image representation;image retrieval;learning (artificial intelligence);medical image processing;meta data","classification accuracy;textual information;visual information;visual methods;ImageCLEF 2013 modality classification task;image types;visual words classifier;traditional hand-crafted bag;deep convolutional neural networks;late score-based fusion;magnetic resonance images;medical imaging modalities;visual content;visual knowledge;metadata;PubMed Central;biomedical literature;medical knowledge;biomedical image modality classification;visual features;deep learning;late fusion;related ImageCLEF 2016 subfigure classification task","","","56","","","","","IET","IET Journals"
"Incremental Wishart Broad Learning System for Fast PolSAR Image Classification","J. Fan; X. Wang; X. Wang; J. Zhao; X. Liu","Department of Ocean Remote Sensing, Key Laboratory of Sea-Area Management Technology, National Marine Environmental Monitoring Center, Dalian, China; Department of Ocean Remote Sensing, Key Laboratory of Sea-Area Management Technology, National Marine Environmental Monitoring Center, Dalian, China; Department of Ocean Remote Sensing, Key Laboratory of Sea-Area Management Technology, National Marine Environmental Monitoring Center, Dalian, China; Department of Ocean Remote Sensing, Key Laboratory of Sea-Area Management Technology, National Marine Environmental Monitoring Center, Dalian, China; Department of Computer Science and Engineering, Washington University in St. Louis, St. Louis, MO, USA","IEEE Geoscience and Remote Sensing Letters","","2019","16","12","1854","1858","In recent years, deep learning neural networks have seen wide adoption in synthetic aperture radar (SAR) image applications. Comparatively, convenient and fast neural network models have attracted less attention. In this letter, a novel incremental Wishart broad learning system (IWBLS) is specifically designed to achieve polarimetric SAR (PolSAR) image classification for the first time. IWBLS can effectively transfer essential Wishart distribution and other types of polarimetric decomposition and spatial features to establish mapped feature and enhancement nodes in one layer without deep learning structures, which means that massive layer-by-layer training consumption can be decreased significantly. Incremental learning concept is incorporated to deal with new PolSAR images or additional features, thereby avoiding retraining entire neural networks, whose properties are very appropriate for long-term monitoring or stepwise feature integration. The experiments substantiate advantages of PolSAR image classification based on our proposed IWBLS algorithm.","","","10.1109/LGRS.2019.2913999","National Natural Science Foundation of China; National Basic Research Program of China (973 Program); National High Resolution Special Research; Key Laboratory of Sea-Area Management Technology Foundation; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8713848","Broad learning system (BLS);classification;incremental learning;multisource feature combination;polarimetric SAR (PolSAR);Wishart classifier","Neural networks;Scattering;Noise measurement;Synthetic aperture radar;Learning systems;Adaptation models;Data models","image classification;image enhancement;learning (artificial intelligence);neural nets;radar imaging;radar polarimetry;statistical distributions;synthetic aperture radar","deep learning neural networks;layer-by-layer training consumption;incremental Wishart broad learning system;polarimetric SAR image classification;neural network models;synthetic aperture radar image applications;fast PolSAR image classification;IWBLS algorithm;stepwise feature integration;PolSAR images;incremental learning;deep learning structures;enhancement nodes;mapped feature;spatial features;polarimetric decomposition;Wishart distribution","","","17","IEEE","","","","IEEE","IEEE Journals"
"Deep-learning-based depth estimation from light field images","I. Schiopu; A. Munteanu","Vrije Universiteit Brussel, Belgium; Vrije Universiteit Brussel, Belgium","Electronics Letters","","2019","55","20","1086","1088","A novel deep-learning-based depth estimation method for light field images is introduced. The proposed method employs a novel neural network design to estimate the disparity of each pixel based on block patches extracted from epipolar plane images. The network output is further refined based on filtering and denoising algorithms. Experimental results demonstrate an average improvement of 34.35% in root mean squared error (RMSE) and 49.44% in mean squared error over machine learning-based state-of-the-art methods.","","","10.1049/el.2019.2073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8855175","","","image denoising;learning (artificial intelligence);neural nets","depth estimation method;deep-learning-based depth estimation;machine learning-based state-of-the-art methods;network output;epipolar plane images;neural network design;light field images","","","7","","","","","IET","IET Journals"
"Supervised Deep Feature Embedding With Handcrafted Feature","S. Kan; Y. Cen; Z. He; Z. Zhang; L. Zhang; Y. Wang","School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Department of Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA; Department of Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA; College of Mechanical Engineering, Guizhou University, Guiyang, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Image Processing","","2019","28","12","5809","5823","Image representation methods based on deep convolutional neural networks (CNNs) have achieved the state-of-the-art performance in various computer vision tasks, such as image retrieval and person re-identification. We recognize that more discriminative feature embeddings can be learned with supervised deep metric learning and handcrafted features for image retrieval and similar applications. In this paper, we propose a new supervised deep feature embedding with a handcrafted feature model. To fuse handcrafted feature information into CNNs and realize feature embeddings, a general fusion unit is proposed (called Fusion-Net). We also define a network loss function with image label information to realize supervised deep metric learning. Our extensive experimental results on the Stanford online products' data set and the in-shop clothes retrieval data set demonstrate that our proposed methods outperform the existing state-of-the-art methods of image retrieval by a large margin. Moreover, we also explore the applications of the proposed methods in person re-identification and vehicle re-identification; the experimental results demonstrate both the effectiveness and efficiency of the proposed methods.","","","10.1109/TIP.2019.2901407","National Natural Science Foundation of China; Natural Science Foundation of Guizhou Province; Science and Technology Program of Guangzhou; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8651322","Deep feature embedding;handcrafted feature;image representation;deep metric learning;image retrieval;person re-identification;vehicle re-identification","Measurement;Image retrieval;Feature extraction;Fuses;Task analysis;Training;Neural networks","computer vision;convolutional neural nets;feature extraction;image representation;image retrieval;learning (artificial intelligence)","supervised deep metric learning;image retrieval;supervised deep feature embedding;handcrafted feature model;handcrafted feature information;image label information;person re-identification;image representation methods;deep convolutional neural networks;discriminative feature embeddings;computer vision tasks;CNNs;general fusion unit;network loss function;Stanford online product data set;in-shop clothes retrieval data set;vehicle re-identification","","1","63","","","","","IEEE","IEEE Journals"
"Joint Design of Measurement Matrix and Sparse Support Recovery Method via Deep Auto-Encoder","S. Li; W. Zhang; Y. Cui; H. V. Cheng; W. Yu","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; University of Toronto, Toronto, ON, Canada; University of Toronto, Toronto, ON, Canada","IEEE Signal Processing Letters","","2019","26","12","1778","1782","Sparse support recovery arises in many applications in communications and signal processing. Existing methods tackle sparse support recovery problems for a given measurement matrix, and cannot flexibly exploit the properties of sparsity patterns for improving performance. In this letter, we propose a data-driven approach to jointly design the measurement matrix and support recovery method for complex sparse signals, using auto-encoder in deep learning. The proposed architecture includes two components, an auto-encoder and a hard thresholding module. The proposed auto-encoder successfully handles complex signals using standard auto-encoder for real numbers. The proposed approach can effectively exploit properties of sparsity patterns, and is especially useful when these underlying properties do not have analytic models. In addition, the proposed approach can achieve sparse support recovery with low computational complexity. Experiments are conducted on an application example, device activity detection in grant-free massive access for massive machine type communications (mMTC). Numerical results show that the proposed approach achieves significantly better performance with much less computation time than classic methods, in the presence of extra structures in sparsity patterns.","","","10.1109/LSP.2019.2945683","National Key R&D Program of China; NSERC of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8861085","Sparse support recovery;auto-encoder;deep learning;device activity detection;grant-free massive access","Sparse matrices;Neurons;Noise measurement;Training;Deep learning;Computer architecture;Computational complexity","compressed sensing;computational complexity;learning (artificial intelligence);signal reconstruction;sparse matrices","machine type communications;device activity detection;low computational complexity;deep learning;data-driven approach;measurement matrix;signal processing;deep auto-encoder;sparse support recovery method;complex sparse signals;sparsity patterns","","","23","IEEE","","","","IEEE","IEEE Journals"
"Deep Learning Based on Striation Images for Underwater and Surface Target Classification","X. Zhou; K. Yang; R. Duan","School of Marine Science and Technology, Northwestern Polytechnical University, Xi'an, China; School of Marine Science and Technology, Northwestern Polytechnical University, Xi'an, China; School of Marine Science and Technology, Northwestern Polytechnical University, Xi'an, China","IEEE Signal Processing Letters","","2019","26","9","1378","1382","Currently, synthetic aperture radar (SAR) images are generally used for ship identification. However, the SAR can only be used to classify surface ships without any underwater target. In contrast, since sonar can receive radiated noises from both vessels and underwater targets, its images can be used to effectively identify targets at different depths. However, due to the shortage of underwater target data and the difficulty in modeling, sonar images are hardly used for deep learning (DL). To solve such problems, this letter proposes a compound convolutional neural network (CSDN) based on a shared latent sparse feature (SLS) and a deep belief network (DBN) to learn striation-based sonar images. This letter has three contributions. First, this letter uses striation images to overcome the lack of training data for CNNs. Second, the DBN is applied to optimize fuzzy or discontinuous fringes, and an SLS feature is proposed to represent the interference fringe. Finally, the two features are exploited to separately train CNNs, combined with weight, to enhance the accuracy of classifying water targets. The experimental results indicate that, compared with the other DL-based models-VGG, SSD, RFCN, and SCDAE-a CSDN is more stable for different datasets and has the highest accuracy of up to 93.34%.","","","10.1109/LSP.2019.2919102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8723113","Striation;neural network;sonar images","Interference;Sonar;Training;Synthetic aperture radar;Optimization;Deep learning;Sea surface","belief networks;convolutional neural nets;image classification;interference (signal);learning (artificial intelligence);object detection;ships;sonar imaging;sonar target recognition;synthetic aperture radar","sonar images;deep learning;deep belief network;DBN;striation images;SLS feature;water targets;surface target classification;synthetic aperture radar images;SAR;ship identification;surface ships;underwater target data;convolutional neural network","","","26","Traditional","","","","IEEE","IEEE Journals"
"Semisupervised Discriminant Multimanifold Analysis for Action Recognition","Z. Xu; R. Hu; J. Chen; C. Chen; J. Jiang; J. Li; H. Li","National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; Department of Electrical and Computer Engineering, University of North Carolina at Charlotte, Charlotte, NC, USA; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Mathematics and Computing Science, Guangxi Colleges and Universities Key Laboratory of Data Analysis and Computation, Guilin University of Electronic Technology, Guilin, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","10","2951","2962","Although recent semisupervised approaches have proven their effectiveness when there are limited training data, they assume that the samples from different actions lie on a single data manifold in the feature space and try to uncover a common subspace for all samples. However, this assumption ignores the intraclass compactness and the interclass separability simultaneously. We believe that human actions should occupy multimanifold subspace and, therefore, model the samples of the same action as the same manifold and those of different actions as different manifolds. In order to obtain the optimum subspace projection matrix, the current approaches may be mathematically imprecise owe to the badly scaled matrix and improper convergence. To address these issues in unconstrained convex optimization, we introduce a nontrivial spectral projected gradient method and Karush-Kuhn-Tucker conditions without matrix inversion. Through maximizing the separability between different classes by using labeled data points and estimating the intrinsic geometric structure of the data distributions by exploring unlabeled data points, the proposed algorithm can learn global and local consistency and boost the recognition performance. Extensive experiments conducted on the realistic video data sets, including JHMDB, HMDB51, UCF50, and UCF101, have demonstrated that our algorithm outperforms the compared algorithms, including deep learning approach when there are only a few labeled samples.","","","10.1109/TNNLS.2018.2886008","National Natural Science Foundation of China; Technology Research Program of Ministry of Public Security; Hubei Province Technological Innovation Major Project; National Key Research and Development Program of China; Guangxi Key Research and Development Program; Natural Science Foundation of Guangxi Province; Guangxi Young Teachers’ Basic Ability Improvement Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641437","Discriminant analysis;Karush–Kuhn–Tucker (KKT) conditions;manifold learning;semisupervised learning;spectral projected gradient (SPG)","Manifolds;Semisupervised learning;Correlation;Training data;Deep learning;Convergence;Multimedia communication","convex programming;feature extraction;gradient methods;image motion analysis;learning (artificial intelligence);matrix algebra;video signal processing","action recognition;training data;single data manifold;feature space;intraclass compactness;interclass separability;human actions;multimanifold subspace;optimum subspace projection matrix;badly scaled matrix;unconstrained convex optimization;nontrivial spectral projected gradient method;Karush-Kuhn-Tucker conditions;labeled data points;data distributions;unlabeled data points;recognition performance;realistic video data sets;deep learning approach;labeled samples;semisupervised discriminant multimanifold analysis;intrinsic geometric structure estimation","","3","76","","","","","IEEE","IEEE Journals"
"Fault Diagnosis for Electromechanical Drivetrains Using a Joint Distribution Optimal Deep Domain Adaptation Approach","Z. Liu; B. Lu; H. Wei; X. Li; L. Chen","School of Information and Electrical Engineering, Hunan University of Science and Technology, Xiangtan, China; School of Information and Electrical Engineering, Hunan University of Science and Technology, Xiangtan, China; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.; School of Information and Electrical Engineering, Hunan University of Science and Technology, Xiangtan, China; School of Information and Electrical Engineering, Hunan University of Science and Technology, Xiangtan, China","IEEE Sensors Journal","","2019","19","24","12261","12270","Robust and reliable drivetrain is important for preventing electromechanical (e.g., wind turbine) downtime. In recent years, advanced machine learning (ML) techniques including deep learning have been introduced to improve fault diagnosis performance for electromechanical systems. However, electromechanical systems (e.g., wind turbine) operate in varying working conditions, meaning that the distribution of the test data (in the target domain) is different from the training data used for model training, and the diagnosis performance of an ML method may become downgraded for practical applications. This paper proposes a joint distribution optimal deep domain adaptation approach (called JDDA) based auto-encoder deep classifier for fault diagnosis of electromechanical drivetrains under the varying working conditions. First, the representative features are extracted by the deep auto-encoder. Then, the joint distribution adaptation is used to implement the domain adaptation, so the classifier trained with the source domain features can be used to classify the target domain data. Lastly, the classification performance of the proposed JDDA is tested using two test-rig datasets, compared with three traditional machine learning methods and two domain adaptation approaches. Experimental results show that the JDDA can achieve better performance compared with the reference machine learning, deep learning and domain adaptation approaches.","","","10.1109/JSEN.2019.2939360","National Natural Science Foundation of China; Hunan Provincial Hu-Xiang Young Talents Project of China; Natural Science Foundation of Hunan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824122","Fault diagnosis;electromechanical drivetrain;deep neural network;deep learning;domain adaptation (DA);joint distribution optimal;auto-encoder (AE);machine learning;artificial intelligence;bearing;gearboxes;wind turbine;varying working conditions","Fault diagnosis;Deep learning;Feature extraction;Adaptation models;Training data;Data models","","","","","41","IEEE","","","","IEEE","IEEE Journals"
"Deep Learning for Fast and Spatially Constrained Tissue Quantification From Highly Accelerated Data in Magnetic Resonance Fingerprinting","Z. Fang; Y. Chen; M. Liu; L. Xiang; Q. Zhang; Q. Wang; W. Lin; D. Shen","Department of Radiology, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; School of Biomedical Engineering, Institute for Medical Imaging Technology, Shanghai Jiao Tong University, Shanghai, China; Department of Radiology, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; School of Biomedical Engineering, Institute for Medical Imaging Technology, Shanghai Jiao Tong University, Shanghai, China; Department of Radiology, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Medical Imaging","","2019","38","10","2364","2374","Magnetic resonance fingerprinting (MRF) is a quantitative imaging technique that can simultaneously measure multiple important tissue properties of human body. Although MRF has demonstrated improved scan efficiency as compared to conventional techniques, further acceleration is still desired for translation into routine clinical practice. The purpose of this paper is to accelerate MRF acquisition by developing a new tissue quantification method for MRF that allows accurate quantification with fewer sampling data. Most of the existing approaches use the MRF signal evolution at each individual pixel to estimate tissue properties, without considering the spatial association among neighboring pixels. In this paper, we propose a spatially constrained quantification method that uses the signals at multiple neighboring pixels to better estimate tissue properties at the central pixel. Specifically, we design a unique two-step deep learning model that learns the mapping from the observed signals to the desired properties for tissue quantification, i.e.: 1) with a feature extraction module for reducing the dimension of signals by extracting a low-dimensional feature vector from the high-dimensional signal evolution and 2) a spatially constrained quantification module for exploiting the spatial information from the extracted feature maps to generate the final tissue property map. A corresponding two-step training strategy is developed for network training. The proposed method is tested on highly undersampled MRF data acquired from human brains. Experimental results demonstrate that our method can achieve accurate quantification for T1 and T2 relaxation times by using only 1/4 time points of the original sequence (i.e., four times of acceleration for MRF acquisition).","","","10.1109/TMI.2019.2899328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641364","Magnetic resonance fingerprinting;neural network;quantitative magnetic resonance imaging;tissue quantification","Imaging;Feature extraction;Dictionaries;Deep learning;Mathematical model;Acceleration;Learning systems","biological tissues;biomedical MRI;brain;feature extraction;image sampling;learning (artificial intelligence);medical image processing;phantoms","undersampled MRF data;final tissue property map;extracted feature maps;spatially constrained quantification module;high-dimensional signal evolution;low-dimensional feature vector;feature extraction module;deep learning model;central pixel;multiple neighboring pixels;spatially constrained quantification method;estimate tissue properties;MRF signal evolution;sampling data;tissue quantification method;MRF acquisition;human body;multiple important tissue properties;quantitative imaging technique;magnetic resonance fingerprinting;spatially constrained tissue quantification","","1","54","","","","","IEEE","IEEE Journals"
"LIDAR Data for Deep Learning-Based mmWave Beam-Selection","A. Klautau; N. González-Prelcic; R. W. Heath","Computer and Telecommunications Department, Universidade Federal do Pará, Belém, Brazil; Wireless Networking and Communications Group, University of Texas at Austin, Austin, TX, USA; Wireless Networking and Communications Group, University of Texas at Austin, Austin, TX, USA","IEEE Wireless Communications Letters","","2019","8","3","909","912","Millimeter wave (mmWave) communication systems can leverage information from sensors to reduce the overhead associated with link configuration. Light detection and ranging (LIDAR) is one sensor widely used in autonomous driving for high resolution mapping and positioning. This letter shows how LIDAR data can be used for line-of-sight detection and to reduce the overhead in mmWave beam-selection. In the proposed distributed architecture, the base station broadcasts its position. The connected vehicle leverages its LIDAR data to suggest a set of beams selected via a deep convolutional neural network. Co-simulation of communications and LIDAR in a vehicle-to-infrastructure (V2I) scenario confirm that LIDAR can help configuring mmWave V2I links.","","","10.1109/LWC.2019.2899571","National Science Foundation; Nokia; Toyota USA; Conselho Nacional de Desenvolvimento Científico e Tecnológico; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642397","LIDAR;mmWave;machine learning;deep learning;convolutional networks","Laser radar;Three-dimensional displays;Ray tracing;Sensors;Connected vehicles;Receivers;Antenna arrays","convolution;directive antennas;learning (artificial intelligence);millimetre wave devices;neural nets;optical radar;radio networks;road vehicles","LIDAR data;deep learning-based mmWave beam-selection;millimeter wave communication systems;leverage information;link configuration;light detection;high resolution mapping;positioning;line-of-sight detection;base station;deep convolutional neural network;mmWave V2I links","","1","20","","","","","IEEE","IEEE Journals"
"Coarse-to-Fine Image DeHashing Using Deep Pyramidal Residual Learning","Y. Wang; R. Ward; Z. J. Wang","Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada","IEEE Signal Processing Letters","","2019","26","9","1295","1299","Image dehashing refers to the process of inferring images by inverting image hashes. Recently, image dehashing from real-valued image retrieval hashes is shown feasible using deep convolutional neural networks. However, the perceptual quality of dehashed images is challenged when real-valued hashes are quantized to less bits. Besides, the scalability to larger or color image dehashing is limited in the previous dehashing network. To this end, we propose a pyramidal long-range residual-learning network (PyLRR-Net). PyLRR-Net is a pyramidal image reconstruction network to dehash images in a progressive manner. At each image scale, we design and insert a long-range residual block to refine the coarse image reconstruction leveraging deep residual learning. Experiments on both grayscale and color image datasets show that the proposed PyLRR-Net outperforms previous work in terms of image dehashing quality, scalability, and flexibility for large and color image dehashing problems.","","","10.1109/LSP.2019.2917073","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715652","Image de-hashing;pyramidal residual learning;long-range residuals","Image reconstruction;Color;Convolution;Image retrieval;Training;Loss measurement;Task analysis","convolutional neural nets;image coding;image colour analysis;image reconstruction;image representation;image retrieval;learning (artificial intelligence)","long-range residual block;coarse image reconstruction;deep residual learning;grayscale;color image datasets;PyLRR-Net outperforms previous work;image dehashing quality;color image dehashing problems;coarse-to-fine image DeHashing;deep pyramidal residual learning;real-valued image retrieval;feasible using deep convolutional neural networks;dehashed images;real-valued hashes;larger color image dehashing;previous dehashing network;long-range residual-learning network;pyramidal image reconstruction network;image scale","","","32","Traditional","","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Robot for Search and Rescue Applications: Exploration in Unknown Cluttered Environments","F. Niroui; K. Zhang; Z. Kashino; G. Nejat","Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada; Autonomous Systems and Biomechatronics Laboratory, Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada","IEEE Robotics and Automation Letters","","2019","4","2","610","617","Rescue robots can be used in urban search and rescue (USAR) applications to perform the important task of exploring unknown cluttered environments. Due to the unpredictable nature of these environments, deep learning techniques can be used to perform these tasks. In this letter, we present the first use of deep learning to address the robot exploration task in USAR applications. In particular, we uniquely combine the traditional approach of frontier-based exploration with deep reinforcement learning to allow a robot to autonomously explore unknown cluttered environments. Experiments conducted with a mobile robot in unknown cluttered environments of varying sizes and layouts showed that the proposed exploration approach can effectively determine appropriate frontier locations to navigate to, while being robust to different environment layouts and sizes. Furthermore, a comparison study with other frontier exploration approaches showed that our learning-based frontier exploration technique was able to explore more of an environment earlier on, allowing for potential identification of a larger number of victims at the beginning of the time-critical exploration task.","","","10.1109/LRA.2019.2891991","Natural Sciences and Engineering Council of Canada; Canada Research Chairs Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606991","Autonomous agents;deep learning in robotics and automation;search and rescue robots","Computer architecture;Robot sensing systems;Microprocessors;Task analysis;Navigation;Layout","learning (artificial intelligence);path planning;rescue robots","urban search and rescue applications;deep reinforcement learning robot;mobile robot;USAR applications;robot exploration task;rescue robots;unknown cluttered environments;time-critical exploration task;learning-based frontier exploration technique","","4","32","","","","","IEEE","IEEE Journals"
"Relative CNN-RNN: Learning Relative Atmospheric Visibility From Images","Y. You; C. Lu; W. Wang; C. Tang","Department of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, Shanghai Jiao Tong University, Shanghai, China; Department of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong","IEEE Transactions on Image Processing","","2019","28","1","45","55","We propose a deep learning approach for directly estimating relative atmospheric visibility from outdoor photos without relying on weather images or data that require expensive sensing or custom capture. Our data-driven approach capitalizes on a large collection of Internet images to learn rich scene and visibility varieties. The relative CNN-RNN coarse-to-fine model, where CNN stands for convolutional neural network and RNN stands for recurrent neural network, exploits the joint power of relative support vector machine, which has a good ranking representation, and the data-driven deep learning features derived from our novel CNN-RNN model. The CNN-RNN model makes use of shortcut connections to bridge a CNN module and an RNN coarse-to-fine module. The CNN captures the global view while the RNN simulates human's attention shift, namely, from the whole image (global) to the farthest discerned region (local). The learned relative model can be adapted to predict absolute visibility in limited scenarios. Extensive experiments and comparisons are performed to verify our method. We have built an annotated dataset consisting of about 40000 images with 0.2 million human annotations. The large-scale, annotated visibility data set will be made available to accompany this paper.","","","10.1109/TIP.2018.2857219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8412582","Convolutional neural network;recurrent neural network;deep learning;atmospheric visibility;relative attributes learning;large-scale image collection","Atmospheric modeling;Atmospheric measurements;Estimation;Meteorology;Observatories;Support vector machines;Recurrent neural networks","image representation;learning (artificial intelligence);recurrent neural nets;support vector machines","CNN-RNN coarse-to-fine model;convolutional neural network;recurrent neural network;deep learning approach;support vector machine;weather imaging;Internet imaging;relative atmospheric visibility learning;data-driven deep learning feature approach","","2","30","","","","","IEEE","IEEE Journals"
"Thermal Comfort Modeling for Smart Buildings: A Fine-Grained Deep Learning Approach","W. Zhang; W. Hu; Y. Wen","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Internet of Things Journal","","2019","6","2","2540","2549","The emerging Internet of Things (IoT) technology enables smart building management and operation to improve building energy efficiency and occupant thermal comfort. In this paper, we perform data analysis using the IoT generated building data to derive accurate thermal comfort model for smart building control. Deep neural network (DNN) is used to model the relationship between the controllable building operations and thermal comfort. As thermal comfort is determined by multiple comfort factors, a fine-grained architecture is proposed, where an exclusive model is trained for each factor and accordingly the corresponding thermal comfort can be evaluated. The experimental results show that the proposed fine-grained DNN outperforms its coarse-grained counterpart by 3.5× and is 1.7×, 2.5×, 2.4×, and 1.9× more accurate compared to four popular machine learning algorithms. Besides, DNN's performance promotes with deeper network topology and more neurons, and a simple topology with the same number of neurons per network hidden layer is sufficient to achieve high modeling accuracy. Finally, the derived thermal comfort model reveals a linear relationship between comfort and air conditioning setpoint. The linear property helps quickly and accurately search for the optimal controllable setpoint with the desired comfort.","","","10.1109/JIOT.2018.2871461","Singapore National Research Foundation (NRF) via the Green Buildings Innovation Cluster (GBIC) administered by the Building and Construction Authority; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8469108","Deep learning;smart building;smart city;thermal comfort","Internet of Things;Smart buildings;Computational modeling;Temperature;Network topology","air conditioning;building management systems;data analysis;energy conservation;home automation;Internet of Things;learning systems;neurocontrollers","IoT;smart building management;building energy efficiency;occupant thermal comfort;data analysis;smart building control;deep neural network;controllable building operations;multiple comfort factors;fine-grained architecture;fine-grained DNN;deeper network topology;high modeling accuracy;air conditioning setpoint;Internet of Things technology;machine learning algorithms;DNN performance;thermal comfort model;fine-grained deep learning approach;IoT generated building data;linear property","","3","25","","","","","IEEE","IEEE Journals"
"On Spatial Diversity in WiFi-Based Human Activity Recognition: A Deep Learning-Based Approach","F. Wang; W. Gong; J. Liu","School of Computing Science, Simon Fraser University, Burnaby, Canada; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computing Science, Simon Fraser University, Burnaby, Canada","IEEE Internet of Things Journal","","2019","6","2","2035","2047","The deeply penetrated WiFi signals not only provide fundamental communications for the massive Internet of Things devices but also enable cognitive sensing ability in many other applications, such as human activity recognition. State-of-the-art WiFi-based device-free systems leverage the correlations between signal changes and body movements for human activity recognition. They have demonstrated reasonably good recognition results with a properly placed transceiver pair, or, in other words, when the human body is within a certain sweet zone. Unfortunately, the sweet zone is not ubiquitous. When the person moves out of the area and enters a dead zone, or even just the orientation changes, the recognition accuracy can quickly decay. In this paper, we closely examine such spatial diversity in WiFi-based human activity recognition. We identify the dead zones and their key influential factors, and accordingly present WiSDAR, a WiFi-based spatial diversity-aware device-free activity recognition system. WiSDAR overshadows the dead zones yet with only one physical WiFi sender and receiver. The key innovation is extending the multiple antennas of modern WiFi devices to construct multiple separated antenna pairs for activity observing. Profiling activity features from multiple spatial dimensions can be more complicated and offer much richer information for further recognition. To this end, we propose a deep learning-based framework that integrates the hidden features from both temporal and spatial dimensions, achieving highly accurate and reliable recognition results. WiSDAR is fully compatible with commercial off-the-shelf WiFi devices, and we have implemented it on the commonly available Intel WiFi 5300 cards. Our real-world experiments demonstrate that it recognizes human activities with a stable accuracy of around 96%.","","","10.1109/JIOT.2018.2871445","Canada Technology Demonstration Program; Canada NSERC Discovery Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8469082","Deep learning;human activity recognition;spatial diversity","Activity recognition;Transceivers;Wireless fidelity;Spatial diversity;Hidden Markov models;Internet of Things;Time-frequency analysis","learning (artificial intelligence);wireless LAN","deep learning-based approach;deeply penetrated WiFi signals;signal changes;body movements;human body;sweet zone;dead zone;recognition accuracy;receiver;modern WiFi devices;profiling activity features;deep learning-based framework;human activity recognition;Intel WiFi 5300 cards;spatial diversity-aware device-free activity recognition system","","3","35","","","","","IEEE","IEEE Journals"
"Extending 2-D Convolutional Neural Networks to 3-D for Advancing Deep Learning Cancer Classification With Application to MRI Liver Tumor Differentiation","E. Trivizakis; G. C. Manikis; K. Nikiforaki; K. Drevelegas; M. Constantinides; A. Drevelegas; K. Marias","Department of Informatics Engineering, Technological Educational Institute of Crete, Heraklion, Greece; Computational Biomedicine Lab, Institute of Computer Science, Foundation for Research and Technology – Hellas, Heraklion, Greece; Computational Biomedicine Lab, Institute of Computer Science, Foundation for Research and Technology – Hellas, Heraklion, Greece; Department of Radiology, Interbalkan Medical Center, Thessaloniki, Greece; Department of Radiology, Interbalkan Medical Center, Thessaloniki, Greece; Department of Radiology, Interbalkan Medical Center, Thessaloniki, Greece; Department of Informatics Engineering, Technological Educational Institute of Crete, Heraklion, Greece","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","923","930","Deep learning (DL) architectures have opened new horizons in medical image analysis attaining unprecedented performance in tasks such as tissue classification and segmentation as well as prediction of several clinical outcomes. In this paper, we propose and evaluate a novel three-dimensional (3-D) convolutional neural network (CNN) designed for tissue classification in medical imaging and applied for discriminating between primary and metastatic liver tumors from diffusion weighted MRI (DW-MRI) data. The proposed network consists of four consecutive strided 3-D convolutional layers with 3 × 3 × 3 kernel size and rectified linear unit (ReLU) as activation function, followed by a fully connected layer with 2048 neurons and a Softmax layer for binary classification. A dataset comprising 130 DWMRI scans was used for the training and validation of the network. To the best of our knowledge this is the first DL solution for the specific clinical problem and the first 3-D CNN for cancer classification operating directly on whole 3-D tomographic data without the need of any preprocessing step such as region cropping, annotating, or detecting regions of interest. The classification performance results, 83% (3-D) versus 69.6% and 65.2% (2-D), demonstrated significant tissue classification accuracy improvement compared to two 2-D CNNs of different architectures also designed for the specific clinical problem with the same dataset. These results suggest that the proposed 3-D CNN architecture can bring significant benefit in DW-MRI liver discrimination and potentially, in numerous other tissue classification problems based on tomographic data, especially in size-limited, disease-specific clinical datasets.","","","10.1109/JBHI.2018.2886276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8572767","Image classification;DW-MRI;deep learning;3D-CNN;liver cancer","Liver;Cancer;Three-dimensional displays;Biomedical imaging;Two dimensional displays;Tumors;Image segmentation","biodiffusion;biomedical MRI;cancer;convolutional neural nets;image classification;image segmentation;learning (artificial intelligence);liver;medical image processing;tumours","2-D convolutional neural networks;deep learning cancer classification;MRI liver tumor differentiation;deep learning architectures;medical imaging;primary liver tumors;metastatic liver tumors;diffusion weighted MRI data;fully connected layer;Softmax layer;binary classification;specific clinical problem;DW-MRI liver discrimination;disease-specific clinical datasets;medical image analysis;three-dimensional convolutional neural network;kernel size;whole 3-D tomographic data;tissue segmentation;consecutive strided 3-D convolutional layers;rectified linear unit;activation function;DWMRI scans;region cropping;annotating region;detecting region;tissue classification accuracy improvement;3-D CNN architecture","","","37","","","","","IEEE","IEEE Journals"
"Lifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systems","B. Liu; L. Wang; M. Liu","Cloud Computing Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Cloud Computing Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Electronics and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong","IEEE Robotics and Automation Letters","","2019","4","4","4555","4562","This letter was motivated by the problem of how to make robots fuse and transfer their experience so that they can effectively use prior knowledge and quickly adapt to new environments. To address the problem, we present a learning architecture for navigation in cloud robotic systems: Lifelong Federated Reinforcement Learning (LFRL). In the letter, we propose a knowledge fusion algorithm for upgrading a shared model deployed on the cloud. Then, effective transfer learning methods in LFRL are introduced. LFRL is consistent with human cognitive science and fits well in cloud robotic systems. Experiments show that LFRL greatly improves the efficiency of reinforcement learning for robot navigation. The cloud robotic system deployment also shows that LFRL is capable of fusing prior knowledge. In addition, we release a cloud robotic navigation-learning website to provide the service based on LFRL: www.shared-robotics.com.","","","10.1109/LRA.2019.2931179","Shenzhen Science and Technology Innovation Commission; Guangdong Science and Technology Plan Guangdong-Hong Kong Cooperation Innovation Platform; National Natural Science Foundation of China; National Natural Science Foundation of China; Shenzhen Science, Technology and Innovation Commission; Basic Research Project of Shanghai Science and Technology Commission; Chengzhong Xu from University of Macau; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772088","Deep learning in robotics and automation;autonomous vehicle navigation;AI-based methods","Navigation;Reinforcement learning;Task analysis;Cloud computing;Training;Robot kinematics","cloud computing;learning (artificial intelligence);mobile robots;path planning;Web sites","transfer learning methods;LFRL;robot navigation;cloud robotic system deployment;learning architecture;knowledge fusion algorithm;lifelong federated reinforcement learning;human cognitive science;cloud robotic navigation-learning Website","","1","30","Traditional","","","","IEEE","IEEE Journals"
"Transfer Neural Trees: Semi-Supervised Heterogeneous Domain Adaptation and Beyond","W. Chen; T. H. Hsu; Y. H. Tsai; M. Chen; Y. F. Wang","Electrical and Computer Engineering Department, Carnegie Mellon University, Pittsburgh, PA, USA; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Machine Learning, Carnegie Mellon University, Pittsburgh, PA, USA; Graduate Institute of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan","IEEE Transactions on Image Processing","","2019","28","9","4620","4633","Heterogeneous domain adaptation (HDA) addresses the task of associating data not only across dissimilar domains but also described by different types of features. Inspired by the recent advances of neural networks and deep learning, we propose a deep leaning model of transfer neural trees (TNT), which jointly solves cross-domain feature mapping, adaptation, and classification in a unified architecture. As the prediction layer in TNT, we introduce transfer neural decision forest (transfer-NDF), which is able to learn the neurons in TNT for adaptation by stochastic pruning. In order to handle semi-supervised HDA, a unique embedding loss term is introduced to TNT for preserving prediction and structural consistency between labeled and unlabeled target-domain data. Furthermore, we show that our TNT can be extended to zero shot learning for associating image and attribute data with promising performance. Finally, experiments on different classification tasks across features, datasets, and modalities would verify the effectiveness of our TNT.","","","10.1109/TIP.2019.2912126","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703744","Transfer learning;domain adaptation;neural decision forest;neural network;zero-shot learning","Task analysis;Artificial neural networks;Deep learning;Forestry;Training;Biological neural networks","image classification;learning (artificial intelligence);pattern classification;trees (mathematics)","classification tasks;unlabeled target-domain data;labeled target-domain data;unique embedding loss term;semisupervised HDA;transfer-NDF;transfer neural decision forest;cross-domain feature mapping;TNT;deep leaning model;deep learning;neural networks;dissimilar domains;semisupervised heterogeneous domain adaptation;transfer neural trees","","","49","","","","","IEEE","IEEE Journals"
"Detecting Traffic Information From Social Media Texts With Deep Learning Approaches","Y. Chen; Y. Lv; X. Wang; L. Li; F. Wang","State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Electrical and Computer Engineering, Indiana University–Purdue University, Indianapolis, IN, USA; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Intelligent Transportation Systems","","2019","20","8","3049","3058","Mining traffic-relevant information from social media data has become an emerging topic due to the real-time and ubiquitous features of social media. In this paper, we focus on a specific problem in social media mining which is to extract traffic relevant microblogs from Sina Weibo, a Chinese microblogging platform. It is transformed into a machine learning problem of short text classification. First, we apply the continuous bag-of-word model to learn word embedding representations based on a data set of three billion microblogs. Compared to the traditional one-hot vector representation of words, word embedding can capture semantic similarity between words and has been proved effective in natural language processing tasks. Next, we propose using convolutional neural networks (CNNs), long short-term memory (LSTM) models and their combination LSTM-CNN to extract traffic relevant microblogs with the learned word embeddings as inputs. We compare the proposed methods with competitive approaches, including the support vector machine (SVM) model based on a bag of n-gram features, the SVM model based on word vector features, and the multi-layer perceptron model based on word vector features. Experiments show the effectiveness of the proposed deep learning approaches.","","","10.1109/TITS.2018.2871269","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8527675","Deep learning;social transportation;traffic information detection;social media;text mining","Data mining;Twitter;Feature extraction;Support vector machines;Predictive models;Real-time systems","data mining;learning (artificial intelligence);multilayer perceptrons;natural language processing;pattern classification;social networking (online);support vector machines;text analysis","traffic information;social media texts;deep learning approaches;social media data;emerging topic;ubiquitous features;social media mining;traffic relevant microblogs;Sina Weibo;Chinese microblogging platform;machine learning problem;short text classification;bag-of-word model;word embedding representations;data set;billion microblogs;one-hot vector representation;natural language processing tasks;learned word embeddings;support vector machine model;SVM model;word vector features;multilayer perceptron model;long short-term memory models;traffic-relevant information","","2","51","","","","","IEEE","IEEE Journals"
"SkeletonNet: A Hybrid Network With a Skeleton-Embedding Process for Multi-View Image Representation Learning","S. Yang; L. Li; S. Wang; W. Zhang; Q. Huang; Q. Tian","School of Computer Science and Technology and the Key Lab of Big Data Mining and Knowledge Management, University of Chinese Academy of Sciences, Beijing, China; Key Lab of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Lab of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, Harbin Institute of Technology, Weihai, China; School of Computer Science and Technology and the Key Lab of Big Data Mining and Knowledge Management, University of Chinese Academy of Sciences, Beijing, China; Department of Computer Sciences, The University of Texas at San Antonio, San Antonio, TX, USA","IEEE Transactions on Multimedia","","2019","21","11","2916","2929","Multi-view representation learning plays a fundamental role in multimedia data analysis. Some specific inter-view alignment principles are adopted in conventional models, where there is an assumption that different views share a common latent subspace. However, when dealing views on diverse semantic levels, the view-specific characteristics are neglected, and the divergent inconsistency of similarity measurements hinders sufficient information sharing. This paper proposes a hybrid deep network by introducing tensor factorization into the multi-view deep auto-encoder. The network adopts skeleton-embedding process for unsupervised multi-view subspace learning. It takes full consideration of view-specific characteristics, and leverages the strength of both shallow and deep architectures for modeling low- and high-level views, respectively. We first formulate the high-level-view semantic distribution as the underlying skeleton structure of the learned subspace, and then infer the local tangent structures according to the affinity propagation of low-level-view geometric correlations. As a consequence, more discriminative subspace representation can be learned from global semantic pivots to local geometric details. Experimental comparisons on three benchmark image datasets show the promising performance and flexibility of our model.","","","10.1109/TMM.2019.2912735","National Natural Science Foundation of China; National Basic Research Program of China (973 Program); Key Research Program of Frontier Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695120","Unsupervised multi-view subspace learning;semantic inconsistency;tensor factorization;deep auto-encoders","Semantics;Correlation;Visualization;Skeleton;Matrix decomposition;Kernel;Laplace equations","data analysis;feature extraction;image classification;image representation;learning (artificial intelligence);pattern clustering;tensors","discriminative subspace representation;low-level-view geometric correlations;learned subspace;underlying skeleton structure;high-level-view semantic distribution;high-level views;deep architectures;shallow architectures;unsupervised multiview subspace learning;multiview deep auto-encoder;hybrid deep network;similarity measurements hinders sufficient information sharing;view-specific characteristics;diverse semantic levels;common latent subspace;specific inter-view alignment principles;multimedia data analysis;multiview representation learning;multiview image representation learning;skeleton-embedding process;hybrid network","","","60","Traditional","","","","IEEE","IEEE Journals"
"Hierarchical Scene Parsing by Weakly Supervised Learning with Image Descriptions","R. Zhang; L. Lin; G. Wang; M. Wang; W. Zuo","Sun Yat-sen University, Guangzhou, P. R. China; Sun Yat-sen University, Guangzhou, P. R. China; Sun Yat-sen University, Guangzhou, P. R. China; Hefei University of Technology, Hefei, P. R. China; Harbin Institute of Technology, Harbin, P. R. China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","3","596","610","This paper investigates a fundamental problem of scene understanding: how to parse a scene image into a structured configuration (i.e., a semantic object hierarchy with object interaction relations). We propose a deep architecture consisting of two networks: i) a convolutional neural network (CNN) extracting the image representation for pixel-wise object labeling and ii) a recursive neural network (RsNN) discovering the hierarchical object structure and the inter-object relations. Rather than relying on elaborative annotations (e.g., manually labeled semantic maps and relations), we train our deep model in a weakly-supervised learning manner by leveraging the descriptive sentences of the training images. Specifically, we decompose each sentence into a semantic tree consisting of nouns and verb phrases, and apply these tree structures to discover the configurations of the training images. Once these scene configurations are determined, then the parameters of both the CNN and RsNN are updated accordingly by back propagation. The entire model training is accomplished through an Expectation-Maximization method. Extensive experiments show that our model is capable of producing meaningful scene configurations and achieving more favorable scene labeling results on two benchmarks (i.e., PASCAL VOC 2012 and SYSU-Scenes) compared with other state-of-the-art weakly-supervised deep learning methods. In particular, SYSU-Scenes contains more than 5,000 scene images with their semantic sentence descriptions, which is created by us for advancing research on scene parsing.","","","10.1109/TPAMI.2018.2799846","State Key Development Program; National Natural Science Foundation of China; Guangdong Natural Science Foundation Project for Research Teams; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8274992","Scene parsing;deep learning;cross-modal learning;high-level understanding;recursive structured prediction","Semantics;Labeling;Training;Neural networks;Task analysis;Predictive models;Image segmentation","convolutional neural nets;image representation;learning (artificial intelligence)","SYSU-Scenes;state-of-the-art weakly-supervised deep learning methods;semantic sentence descriptions;hierarchical scene parsing;weakly supervised learning;image descriptions;scene image;semantic object hierarchy;object interaction relations;deep architecture;convolutional neural network;CNN;image representation;pixel-wise object;recursive neural network;RsNN;hierarchical object structure;inter-object relations;semantic maps;weakly-supervised learning manner;training images;semantic tree;tree structures;scene labeling;descriptive sentence leveraging","","1","53","","","","","IEEE","IEEE Journals"
"Deep Convolution Network for Direction of Arrival Estimation With Sparse Prior","L. Wu; Z. Liu; Z. Huang","State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China","IEEE Signal Processing Letters","","2019","26","11","1688","1692","In this letter, a deep learning framework for direction of arrival (DOA) estimation is developed. We first show that the columns of the array covariance matrix can be formulated as under-sampled noisy linear measurements of the spatial spectrum. Then, a deep convolution network (DCN) that learns the inverse transformation from large training dataset is introduced. In contrast to traditional sparsity-inducing methods with computationally complex iterations, the proposed DCN-based framework could efficiently obtain DOA estimates in near real time. Moreover, the utilization of the sparsity prior improves DOA estimation performance compared to existing deep learning based methods. Simulation results have demonstrated the superiority of the proposed method in both DOA estimation precision and computation efficiency especially when SNR is low.","","","10.1109/LSP.2019.2945115","Program for Innovative Research Groups; Hunan Provincial Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854868","Direction of arrival estimation;deep convolution network;sparse representation","Direction-of-arrival estimation;Estimation;Training;Convolution;Signal to noise ratio;Deep learning","compressed sensing;convolutional neural nets;covariance matrices;direction-of-arrival estimation;inverse transforms;learning (artificial intelligence)","computation efficiency;real time estimates;training dataset;inverse transformation;spatial spectrum;undersampled noisy linear measurements;array covariance matrix;deep learning framework;sparse prior;direction of arrival estimation;deep convolution network","","","19","Traditional","","","","IEEE","IEEE Journals"
"Novel IoT-Based Privacy-Preserving Yoga Posture Recognition System Using Low-Resolution Infrared Sensors and Deep Learning","M. Gochoo; T. Tan; S. Huang; T. Batjargal; J. Hsieh; F. S. Alnajjar; Y. Chen","Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Computer Science and Engineering, National Taiwan Ocean University, Keelung City, Taiwan; College of Information Technology, United Arab Emirates University, Al Ain, UAE; Department of Dental Technology and Materials Science, Central Taiwan University of Science and Technology, Taichung, Taiwan","IEEE Internet of Things Journal","","2019","6","4","7192","7200","In recent years, the number of yoga practitioners has been drastically increased and there are more men and older people practice yoga than ever before. Internet of Things (IoT)-based yoga training system is needed for those who want to practice yoga at home. Some studies have proposed RGB/Kinect camera-based or wearable device-based yoga posture recognition methods with a high accuracy; however, the former has a privacy issue and the latter is impractical in the long-term application. Thus, this paper proposes an IoT-based privacy-preserving yoga posture recognition system employing a deep convolutional neural network (DCNN) and a low-resolution infrared sensor-based wireless sensor network (WSN). The WSN has three nodes (x, y, and z-axes) where each integrates 8 × 8 pixels' thermal sensor module and a Wi-Fi module for connecting the deep learning server. We invited 18 volunteers to perform 26 yoga postures for two sessions each lasted for 20 s. First, recorded sessions are saved as .csv files, then preprocessed and converted to grayscale posture images. Totally, 93200 posture images are employed for the validation of the proposed DCNN models. The tenfold cross-validation results revealed that F1-scores of the models trained with xyz (all 3-axes) and y (only y-axis) posture images were 0.9989 and 0.9854, respectively. An average latency for a single posture image classification on the server was 107 ms. Thus, we conclude that the proposed IoT-based yoga posture recognition system has a great potential in the privacy-preserving yoga training system.","","","10.1109/JIOT.2019.2915095","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8707064","CNN;device-free;infrared;privacy-preserving;yoga posture recognition","Wireless sensor networks;Cameras;Infrared sensors;Deep learning;Privacy;Servers;Temperature measurement","convolutional neural nets;data privacy;feature extraction;image classification;image colour analysis;image resolution;image sensors;Internet of Things;learning (artificial intelligence);pose estimation;sport;wireless sensor networks","privacy-preserving yoga training system;deep convolutional neural network;deep learning server;grayscale posture images;wearable device;posture image classification;IoT-based privacy-preserving yoga posture recognition system;Internet of Things-based yoga training system;infrared sensor-based wireless sensor network;RGB/Kinect camera","","","33","","","","","IEEE","IEEE Journals"
"EOVNet: Earth-Observation Image-Based Vehicle Detection Network","Z. Gao; H. Ji; T. Mei; B. Ramesh; X. Liu","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Temasek Laboratories, National University of Singapore, Singapore; Temasek Laboratories, National University of Singapore, Singapore","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","9","3552","3561","Vehicle detection from earth-observation (EO) image has been attracting remarkable attention for its critical value in a variety of applications. Encouraged by the stunning success of deep learning techniques based on convolutional neural networks (CNNs), which have revolutionized the visual data processing community and obtained the state-of-the-art performance in a variety of classification and recognition tasks on benchmark datasets, we propose a network, called EOVNet (EO image-based vehicle detection network), to bridge the gap between the advanced deep learning research of object detection and the specific task of vehicle detection in EO images. Our network has integrated nearly all advanced techniques including very deep residual networks for feature extraction, feature pyramid to fuse multiscale features, network for proposal generation with feature sharing, and hard example mining. Moreover, our novel designs of probability-based localization and homography-based data augmentation have been investigated, resulting in further improvement of the detection performance. For performance evaluation, we have collected nearly all the representative EO datasets associated with vehicle detection. Extensive experiments on the representative datasets demonstrate that our method outperforms the state-of-the-art object detection approach Faster R-CNN++ (which is based on the Faster R-CNN framework, but with significant improvement) with 5% average precision improvement. The source code will be made available after the review process.","","","10.1109/JSTARS.2019.2933501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809375","Convolutional neural network (CNN);deep learning;earth-observation (EO) images;vehicle detection","Feature extraction;Vehicle detection;Proposals;Object detection;Neural networks;Task analysis;Deep learning","data mining;feature extraction;image classification;learning (artificial intelligence);neural nets;object detection;probability;traffic engineering computing","EO image-based vehicle detection network;advanced deep learning research;deep residual networks;feature extraction;probability-based localization;homography-based data augmentation;detection performance;representative EO datasets;earth-observation image-based vehicle detection network;deep learning techniques;convolutional neural networks;visual data processing community;classification;state-of-the-art object detection approach;EOVNet;R-CNN","","","42","Traditional","","","","IEEE","IEEE Journals"
"Multimodal Face-Pose Estimation With Multitask Manifold Deep Learning","C. Hong; J. Yu; J. Zhang; X. Jin; K. Lee","College of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; School of Science and Technology, Zhejiang International Studies University, Hangzhou, China; Department of Computer Science, Yonsei University, Seoul, South Korea; Department of Computer Science, Yonsei University, Seoul, South Korea","IEEE Transactions on Industrial Informatics","","2019","15","7","3952","3961","Face-pose estimation aims at estimating the gazing direction with two-dimensional face images. It gives important communicative information and visual saliency. However, it is challenging because of lights, background, face orientations, and appearance visibility. Therefore, a descriptive representation of face images and mapping it to poses are critical. In this paper, we use multimodal data and propose a novel face-pose estimation framework named multitask manifold deep learning (M2DL). It is based on feature extraction with improved convolutional neural networks (CNNs) and multimodal mapping relationship with multitask learning. In the proposed CNNs, manifold regularized convolutional layers learn the relationship between outputs of neurons in a low-rank space. Besides, in the proposed mapping relationship learning method, different modals of face representations are naturally combined by applying multitask learning with incoherent sparse and low-rank learning with a least-squares loss. Experimental results on three challenging benchmark datasets demonstrate the performance of M2DL.","","","10.1109/TII.2018.2884211","National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; Natural Science Foundation of Fujian Province; Fujian Provincial High School Natural Science Foundation of China; Foundation of Fujian Educational Committee; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554134","Convolutional neural networks (CNNs);face-pose estimation;low-rank learning;multitask learning","Face;Manifolds;Pose estimation;Task analysis;Neurons;Informatics","convolutional neural nets;face recognition;feature extraction;image representation;learning (artificial intelligence);least squares approximations;pose estimation","mapping relationship learning method;face representations;low-rank learning;multimodal face-pose estimation;multitask manifold deep learning;gazing direction;two-dimensional face images;face orientations;multimodal mapping relationship;CNN;face-pose estimation;feature extraction;convolutional neural networks;least-squares loss","","","51","","","","","IEEE","IEEE Journals"
"Dynamic traffic aware active queue management using deep reinforcement learning","W. Jin; R. Gu; Y. Ji; T. Dong; J. Yin; Z. Liu","Beijing University of Posts and Telecommunications, People's Republic of China; Beijing University of Posts and Telecommunications, People's Republic of China; Beijing University of Posts and Telecommunications, People's Republic of China; Beijing Institute of Satellite Information Engineering, People's Republic of China; Beijing Institute of Satellite Information Engineering, People's Republic of China; Beijing Institute of Satellite Information Engineering, People's Republic of China","Electronics Letters","","2019","55","20","1084","1086","Traditional design of active queue management (AQM) assumes a fixed model at an operating point and lacks planning. AQMs like controlled delay provide some planning but are insensitive to dynamic traffic. In this Letter, the authors propose dynamic traffic aware AQM with deep reinforcement learning to expand the model on a grid of operating points and to add more planning. By learning the network parameter on the grid and the transitions between operating points with collected data, the fluid model of transmission control protocol dynamics is obtained. Planning on operating points is enhanced by value iterations. Evaluations show that the algorithm decreases queuing delay for dynamic traffic with more planning.","","","10.1049/el.2019.1146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8855202","","","learning (artificial intelligence);queueing theory;telecommunication congestion control;telecommunication network management;telecommunication traffic;transport protocols","dynamic traffic aware AQM;controlled delay;fixed model;traditional design;dynamic traffic aware active queue management;transmission control protocol dynamics;fluid model;operating point;deep reinforcement learning","","","5","","","","","IET","IET Journals"
"Collision Detection for Industrial Collaborative Robots: A Deep Learning Approach","Y. J. Heo; D. Kim; W. Lee; H. Kim; J. Park; W. K. Chung","Neuromeka, Seoul, South Korea; Department of Mechanical Engineering, POSTECH, Pohang, South Korea; Neuromeka, Seoul, South Korea; Department of Mechanical Engineering, POSTECH, Pohang, South Korea; Neuromeka, Seoul, South Korea; Department of Mechanical Engineering, POSTECH, Pohang, South Korea","IEEE Robotics and Automation Letters","","2019","4","2","740","746","With increased human-robot interactions in industrial settings, a safe and reliable collision detection framework has become an indispensable element of collaborative robots. The conventional framework detects collisions by estimating collision monitoring signals with a particular type of observer, which is followed by collision decision processes. This results in unavoidable tradeoff between sensitivity to collisions and robustness to false alarms. In this study, we propose a collision detection framework (CollisionNet) based on a deep learning approach. We designed a deep neural network model to learn robot collision signals and recognize any occurrence of a collision. This data-driven approach unifies feature extraction from high-dimensional signals and the decision processes. CollisionNet eliminates heuristic and cumbersome nature of the traditional decision processes, showing high detection performance and generalization capability in real time. We verified the performance of the proposed framework through various experiments.","","","10.1109/LRA.2019.2893400","Technology Innovation Program (or Industrial Strategic Technology Development Program; Development of Robot System Control S/W for Smart Robot Factory with IoT; Technology development Program; Ministry of SMEs and Startups (MSS, Korea); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613933","Deep learning in robotics and automation;physical human-robot interaction","Collision avoidance;Service robots;Robot sensing systems;Monitoring;Convolution;Safety","collision avoidance;feature extraction;human-robot interaction;industrial robots;learning (artificial intelligence);neurocontrollers","high-dimensional signals;CollisionNet;high detection performance;industrial collaborative robots;deep learning approach;industrial settings;safe collision detection framework;reliable collision detection framework;collision monitoring signals;collision decision processes;deep neural network model;robot collision signals;human-robot interactions;data-driven approach","","1","20","","","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Optimal Transmission Policy for Communication Systems With Energy Harvesting and Adaptive MQAM","M. Li; X. Zhao; H. Liang; F. Hu","College of Communication Engineering, Jilin University, Changchun, China; College of Communication Engineering, Jilin University, Changchun, China; College of Communication Engineering, Jilin University, Changchun, China; College of Communication Engineering, Jilin University, Changchun, China","IEEE Transactions on Vehicular Technology","","2019","68","6","5782","5793","In this paper, we study an optimal transmission problem in a point-to-point wireless communication system with energy harvesting and limited battery at its transmitter. Considering the non-availability of prior information about distribution on energy arrival process and channel coefficient, we propose a deep reinforcement learning (DRL) based optimal policy to allocate transmission power and adaptively adjust multi-ary modulation level according to the obtained causal information on harvested energy, battery state, and channel gain to achieve maximum throughput of the system. This optimization problem is formulated as a Markov decision process with unknown state transition probability. Applying the principle of the DRL, we use a deep Q-network to find the optimal solution in continuous state space, which provides rapid convergence since there is no additional memory required. Simulation results show that the proposed policy is effective and valid and it can improve the throughput of the system compared with Q-learning, greedy, random, and constant modulation level transmission policies.","","","10.1109/TVT.2019.2911544","National Natural Science Foundation of China; basic scientific research business expenses of central universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8692577","Transmission policy;energy harvesting;multi-ary communications;Markov decision process;reinforcement learning;deep Q-network","Wireless communication;Modulation;Throughput;Energy harvesting;Data models;Markov processes","energy harvesting;learning (artificial intelligence);Markov processes;optimisation;probability;quadrature amplitude modulation;radio networks;telecommunication computing;telecommunication power management;wireless channels","channel gain;optimization problem;Markov decision process;unknown state transition probability;DRL;deep Q-network;continuous state space;energy harvesting;adaptive MQAM;optimal transmission problem;point-to-point wireless communication system;limited battery;energy arrival process;channel coefficient;transmission power;multiary modulation level;battery state;deep reinforcement learning optimal transmission policy","","","41","","","","","IEEE","IEEE Journals"
"Deep Learning Aided Fingerprint-Based Beam Alignment for mmWave Vehicular Communication","K. Satyanarayana; M. El-Hajjar; A. A. M. Mourad; L. Hanzo","Department of Electronics and Computer Science, University of Southampton, Southampton, U.K.; Department of Electronics and Computer Science, University of Southampton, Southampton, U.K.; InterDigital, London, U.K.; Department of Electronics and Computer Science, University of Southampton, Southampton, U.K.","IEEE Transactions on Vehicular Technology","","2019","68","11","10858","10871","Harnessing the substantial bandwidth available at millimeter wave (mmWave) carrier frequencies has proved to be beneficial to accommodate a large number of users with increased data rates. However, owing to the high propagation losses observed at mmWave frequencies, directional transmission has to be employed. This necessitates efficient beam-alignment for a successful transmission. Achieving perfect beam-alignment is however challenging, especially in the scenarios when there is a rapid movement of vehicles associated with ever-changing traffic density, which is governed by the topology of roads as well as the time of the day. Therefore, in this paper, we take the approach of fingerprint based beam-alignment, where a set of beam pairs constitute the fingerprint of a given location. Furthermore, given the time-varying traffic density, we propose a multi-fingerprint based database for a given location, where the base station (BS) intelligently adapts the fingerprints with the aid of learning. Additionally, we propose multi-functional beam transmission as an application of our proposed design, where the beam-pairs that satisfy the required received signal strength (RSS) participate in increasing the spectral efficiency or improving the end-to-end performance in some other way. Explicitly, the BS leverages the plurality of beam-pairs to attain both multiplexing and diversity gains. Furthermore, if the plurality of beam-pairs is higher than the number of RF chains, the BS may also employ beam-index modulation to further improve the spectral efficiency. We demonstrate that having multiple fingerprint-based beam-alignment provides superior performance than that of the single fingerprint based beam-alignment. Furthermore, we show that our learning-aided multiple fingerprint design provides a better fidelity compared to that of the benchmark scheme also employing multiple fingerprint but dispensing with learning. Additionally, our reduced-search based learning-aided beam-alignment design performs similarly to beam-sweeping based beam-alignment, even though an exhaustive beam-search is carried out by the latter. More explicitly, our design is capable of maintaining the target performance in dense vehicular environments, while both single fingerprint and line-of-sight (LOS) based beam-alignment suffer from blockages.","","","10.1109/TVT.2019.2939400","InterDigital; Engineering and Physical Sciences Research Council; COALESCE; Royal Society's Global Challenges Research Fund Grant; H2020 European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823977","Millimeter Wave;MIMO;Beamforming;Machine Learning","Training;Databases;Complexity theory;Support vector machines;Deep learning;Fingerprint recognition;Wireless communication","learning (artificial intelligence);millimetre wave communication;millimetre wave propagation;multiplexing;radio spectrum management;road traffic;RSSI;search problems;vehicular ad hoc networks","fingerprint-based beam alignment;mmWave vehicular communication;millimeter wave carrier frequencies;mmWave frequencies;beam pairs;time-varying traffic density;multifingerprint based database;multifunctional beam transmission;spectral efficiency;beam-index modulation;multiple fingerprint-based beam-alignment;single fingerprint based beam-alignment;learning-aided multiple fingerprint design;beam-sweeping based beam-alignment;exhaustive beam-search;line-of-sight based beam-alignment;reduced-search based learning-aided beam-alignment design;received signal strength;RSS;propagation losses;line-of-sight;LOS based beam-alignment","","","40","IEEE","","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning in Soft Viscoelastic Actuator of Dielectric Elastomer","L. Li; J. Li; L. Qin; J. Cao; M. S. Kankanhalli; J. Zhu","Department of Mechanical Engineering, National University of Singapore, Singapore; Graduate School of Integrative Science and Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore","IEEE Robotics and Automation Letters","","2019","4","2","2094","2100","Dielectric elastomer actuators (DEAs) have been widely employed as artificial muscles in soft robots. Due to material viscoelasticity and nonlinear electromechanical coupling, it is challenging to accurately model a viscoelastic DEA, especially when the actuator is of a complex or irregular configuration. Control of DEAs is thus challenging but significant. In this letter, we propose a model-free method for control of DEAs, based on deep reinforcement learning. We perform dynamic feedback control by considering the time-dependent behavior of DEAs. Our method is generic in that it does not require task-specific knowledge about the structure or material parameters of the DEA. The experiments show that our method is robust to achieve accurate control for the DEAs of different configurations, different prestretches, and at different times (the material property usually changes due to viscoelasticity effects). To the best of our knowledge, this letter is the first effort to explore deep reinforcement learning for control of DEAs.","","","10.1109/LRA.2019.2898710","MOE Tier 1, Singapore; ASTAR, Singapore; National Research Foundation; Prime Ministers Office, Singapore; Strategic Capability Research Centres Funding Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8638989","Modeling, control, and learning for soft robots;model learning for control","Actuators;Reinforcement learning;Voltage control;Electrodes;Robots;Task analysis;Muscles","dielectric materials;elastomers;electric actuators;electroactive polymer actuators;feedback;learning (artificial intelligence);muscle;viscoelasticity","dielectric elastomer actuators;material viscoelasticity;viscoelastic DEA;model-free method;deep reinforcement learning;dynamic feedback control;viscoelasticity effects;soft viscoelastic actuator","","1","36","","","","","IEEE","IEEE Journals"
"IoT Wearable Sensor and Deep Learning: An Integrated Approach for Personalized Human Activity Recognition in a Smart Home Environment","V. Bianchi; M. Bassoli; G. Lombardo; P. Fornacciari; M. Mordonini; I. De Munari","Department of Engineering and Architecture, University of Parma, Parma, Italy; Department of Engineering and Architecture, University of Parma, Parma, Italy; Department of Engineering and Architecture, University of Parma, Parma, Italy; Department of Engineering and Architecture, University of Parma, Parma, Italy; Department of Engineering and Architecture, University of Parma, Parma, Italy; Department of Engineering and Architecture, University of Parma, Parma, Italy","IEEE Internet of Things Journal","","2019","6","5","8553","8562","Human activity recognition (HAR) is currently recognized as a key element of a more general framework designed to perform continuous monitoring of human behaviors in the area of ambient assisted living (AAL), well-being management, medical diagnosis, elderly care, rehabilitation, entertainment, and surveillance in smart home environments. In this paper, an innovative HAR system, exploiting the potential of wearable devices integrated with the skills of deep learning techniques, is presented with the aim of recognizing the most common daily activities of a person at home. The designed wearable sensor embeds an inertial measurement unit (IMU) and a Wi-Fi section to send data on a cloud service and to allow direct connection to the Internet through a common home router so that the user themselves could manage the installation procedure. The sensor is coupled to a convolutional neural network (CNN) network designed to make inferences with the minimum possible resources to keep open the way of its implementation on low-cost or embedded devices. The system is conceived for daily activity monitor and nine different activities can be highlighted with an accuracy of 97%.","","","10.1109/JIOT.2019.2920283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8727452","Activity recognition;Internet of Things (IoT);machine learning;wearable sensor","Wearable sensors;Activity recognition;Deep learning;Monitoring;Feature extraction;Internet of Things;Cloud computing","assisted living;biomedical equipment;cloud computing;convolutional neural nets;geriatrics;home computing;image recognition;Internet of Things;learning (artificial intelligence);patient diagnosis;patient monitoring;sensors;wearable computers;wireless LAN","common home router;convolutional neural network network;daily activity monitor;IoT wearable sensor;personalized human activity recognition;smart home environment;continuous monitoring;human behaviors;ambient assisted living;medical diagnosis;elderly care;innovative HAR system;wearable devices;deep learning techniques;inertial measurement unit;Wi-Fi section;AAL;well-being management;rehabilitation;entertainment;surveillance;IMU;cloud service;Internet;CNN;embedded devices;low-cost devices","","","57","","","","","IEEE","IEEE Journals"
"Joint Discriminative Learning of Deep Dynamic Textures for 3D Mask Face Anti-Spoofing","R. Shao; X. Lan; P. C. Yuen","Department of Computer Science, Hong Kong Baptist University, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Hong Kong","IEEE Transactions on Information Forensics and Security","","2019","14","4","923","938","Three-dimensional mask spoofing attacks have been one of the main challenges in face recognition. Compared with a 3D mask, a real face displays different facial motion patterns that are reflected by different facial dynamic textures. However, a large portion of these facial motion differences is subtle. We find that the subtle facial motion can be fully captured by multiple deep dynamic textures from a convolutional layer of a convolutional neural network, but not all deep dynamic textures from different spatial regions and different channels of a convolutional layer are useful for differentiation of subtle motions between real faces and 3D masks. In this paper, we propose a novel feature learning model to learn discriminative deep dynamic textures for 3D mask face anti-spoofing. A novel joint discriminative learning strategy is further incorporated in the learning model to jointly learn the spatial- and channel-discriminability of the deep dynamic textures. The proposed joint discriminative learning strategy can be used to adaptively weight the discriminability of the learned feature from different spatial regions or channels, which ensures that more discriminative deep dynamic textures play more important roles in face/mask classification. Experiments on several publicly available data sets validate that the proposed method achieves promising results in intra- and cross-data set scenarios.","","","10.1109/TIFS.2018.2868230","Hong Kong Research Grant Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453011","3D mask face anti-spoofing;deep dynamic textures;spatial- and channel-discriminability","Face;Three-dimensional displays;Dynamics;Videos;Feature extraction;Visualization;Face recognition","convolution;face recognition;feature extraction;feedforward neural nets;image classification;image motion analysis;image texture;learning (artificial intelligence);stereo image processing","facial motion differences;subtle facial motion;convolutional layer;discriminative deep dynamic textures;3D mask face anti-spoofing;learning model;spatial- channel-discriminability;learned feature;face/mask classification;three-dimensional mask spoofing attacks;face recognition;facial dynamic textures;spatial regions;joint discriminative learning strategy;convolutional neural network;facial motion patterns","","4","59","","","","","IEEE","IEEE Journals"
"Deep Sparse Representation-Based Classification","M. Abavisani; V. M. Patel","Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ, USA; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA","IEEE Signal Processing Letters","","2019","26","6","948","952","We present a transductive deep learning-based formulation for the sparse representation-based classification (SRC) method. The proposed network consists of a convolutional autoencoder along with a fully connected layer. The role of the autoencoder network is to learn robust deep features for classification. On the other hand, the fully connected layer, which is placed in between the encoder and the decoder networks, is responsible for finding the sparse representation. The estimated sparse codes are then used for classification. Various experiments on three different datasets show that the proposed network leads to sparse representations that give better classification results than state-of-the-art SRC methods. The source code is available at: github.com/mahdiabavisani/DSRC.","","","10.1109/LSP.2019.2913022","National Science Foundation; US Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698287","Deep learning;sparse representation-based classification;deep sparse representation-based classification","Decoding;Training;Feeds;Encoding;Kernel;Optimization;Testing","image classification;image representation;learning (artificial intelligence);neural nets;sparse matrices","decoder networks;deep sparse representation-based classification;transductive deep learning-based formulation;sparse representation-based classification method;convolutional autoencoder;fully connected layer;autoencoder network;robust deep features;sparse codes estimation","","1","37","","","","","IEEE","IEEE Journals"
"Deep Dictionary Learning: A PARametric NETwork Approach","S. Mahdizadehaghdam; A. Panahi; H. Krim; L. Dai","Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, USA; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, USA; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, USA; Raytheon Integrated Defense Systems, Tewksbury, MA, USA","IEEE Transactions on Image Processing","","2019","28","10","4790","4802","Deep dictionary learning seeks multiple dictionaries at different image scales to capture complementary coherent characteristics. We propose a method for learning a hierarchy of synthesis dictionaries with an image classification goal. The dictionaries and classification parameters are trained by a classification objective, and the sparse features are extracted by reducing a reconstruction loss in each layer. The reconstruction objectives in some sense regularize the classification problem and inject source signal information in the extracted features. The performance of the proposed hierarchical method increases by adding more layers, which consequently makes this model easier to tune and adapt. The proposed algorithm furthermore shows a remarkably lower fooling rate in the presence of adversarial perturbation. The validation of the proposed approach is based on its classification performance using four benchmark datasets and is compared to a Convolutional Neural Network (CNN) of similar size.","","","10.1109/TIP.2019.2914376","Army Research Office; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8708973","Image classification;deep learning;sparse representation","Dictionaries;Machine learning;Feature extraction;Task analysis;Neural networks;Training;Image reconstruction","feature extraction;image classification;image reconstruction;learning (artificial intelligence)","deep dictionary learning;PARametric NETwork approach;multiple dictionaries;complementary coherent characteristics;synthesis dictionaries;image classification goal;classification parameters;classification objective;reconstruction objectives;classification problem;inject source signal information;classification performance;image scales","","","52","","","","","IEEE","IEEE Journals"
"DeepIG: Multi-Robot Information Gathering With Deep Reinforcement Learning","A. Viseras; R. Garcia","Institute of Communications and Navigation of the German Aerospace Center (DLR), Oberpfaffenhofen, Germany; E.T.S.I. Telecomunicación, Universidad Politécnica de Madrid, Madrid, Spain","IEEE Robotics and Automation Letters","","2019","4","3","3059","3066","State-of-the-art multi-robot information gathering (MR-IG) algorithms often rely on a model that describes the structure of the information of interest to drive the robots motion. This causes MR-IG algorithms to fail when they are applied to new IG tasks, as existing models cannot describe the information of interest. Therefore, we propose in this letter a MR-IG algorithm that can be applied to new IG tasks with little algorithmic changes. To this end, we introduce DeepIG: a MR-IG algorithm that uses deep reinforcement learning to allow robots to learn how to gather information. Nevertheless, there are IG tasks for which accurate models have been derived. Therefore, we extend DeepIG to exploit existing models for such IG tasks. This algorithm we term it model-based DeepIG (MB-DeepIG). First, we evaluate DeepIG in simulations, and in an indoor experiment with three quadcopters that autonomously map an unknown terrain profile built in our lab. Results demonstrate that DeepIG can be applied to different IG tasks without algorithmic changes, and that it is robust to measurement noise. Then, we benchmark MB-DeepIG against state-of-theart information-driven Gaussian-processes-based IG algorithms. Results demonstrate that MB-DeepIG outperforms the considered benchmarks.","","","10.1109/LRA.2019.2924839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744599","Multi-robot systems;deep learning in robotics and automation","Robot sensing systems;Task analysis;Robot kinematics;Collision avoidance;Reinforcement learning;Monitoring","Gaussian processes;helicopters;indoor environment;learning (artificial intelligence);mobile robots;motion control;multi-robot systems","deep reinforcement learning;robots motion;MR-IG algorithm;multi-robot information gathering algorithms;quadcopters;IG tasks;MB-DeepIG;information-driven Gaussian-processes-based IG algorithms","","","14","","","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Based Resource Allocation for V2V Communications","H. Ye; G. Y. Li; B. F. Juang","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Vehicular Technology","","2019","68","4","3163","3173","In this paper, we develop a novel decentralized resource allocation mechanism for vehicle-to-vehicle (V2V) communications based on deep reinforcement learning, which can be applied to both unicast and broadcast scenarios. According to the decentralized resource allocation mechanism, an autonomous “agent,” a V2V link or a vehicle, makes its decisions to find the optimal sub-band and power level for transmission without requiring or having to wait for global information. Since the proposed method is decentralized, it incurs only limited transmission overhead. From the simulation results, each agent can effectively learn to satisfy the stringent latency constraints on V2V links while minimizing the interference to vehicle-to-infrastructure communications.","","","10.1109/TVT.2019.2897134","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633948","Deep Reinforcement Learning;V2V Communication;Resource Allocation","Resource management;Interference;Reinforcement learning;Unicast;Optimization;Vehicular ad hoc networks;Reliability","interference suppression;learning (artificial intelligence);resource allocation;vehicular ad hoc networks","V2V Communications;vehicle-to-vehicle communications;deep reinforcement learning;broadcast scenarios;autonomous agent;vehicle-to-infrastructure communications;decentralized resource allocation mechanism;V2V links;unicast scenario","","15","28","","","","","IEEE","IEEE Journals"
"Learning to Navigate Endoscopic Capsule Robots","M. Turan; Y. Almalioglu; H. B. Gilbert; F. Mahmood; N. J. Durr; H. Araujo; A. E. Sarı; A. Ajay; M. Sitti","Physical Intelligence Department, Max Planck Institute for Intelligent Systems, Stuttgart, Germany; Computer Science Department, University of Oxford, Oxford, U.K.; Department of Mechanical and Industrial Engineering, Louisiana State University, Baton Rouge, LA, USA; Department of Biomedical Engineering, Johns Hopkins University, Baltimore, MD, USA; Department of Biomedical Engineering, Johns Hopkins University, Baltimore, MD, USA; Institute for Systems and Robotics, University of Coimbra, Coimbra, Portugal; Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Physical Intelligence Department, Max Planck Institute for Intelligent Systems, Stuttgart, Germany","IEEE Robotics and Automation Letters","","2019","4","3","3075","3082","Deep reinforcement learning (DRL) techniques have been successful in several domains, such as physical simulations, computer games, and simulated robotic tasks, yet the transfer of these successful learning concepts from simulations into the real world scenarios remains still a challenge. In this letter, a DRL approach is proposed to learn the continuous control of a magnetically actuated soft capsule endoscope (MASCE). Proposed controller approach can alleviate the need for tedious modeling of complex and highly nonlinear physical phenomena, such as magnetic interactions, robot body dynamics and tissue-robot interactions. Experiments performed in real ex-vivo porcine stomachs prove the successful control of the MASCE with trajectory tracking errors on the order of millimeter.","","","10.1109/LRA.2019.2924846","Max Planck ETH Center for Learning Systems; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744622","Deep reinforcement learning;model-free control learning;endoscopic capsule robot;actor-critic","Robots;Reinforcement learning;Magnetic resonance imaging;Stomach;Medical services;Navigation;Task analysis","endoscopes;learning (artificial intelligence);magnetic actuators;medical robotics;mobile robots;robot dynamics","trajectory tracking errors;ex-vivo porcine stomachs;endoscopic capsule robot navigation;highly nonlinear physical phenomena;magnetic interactions;magnetically actuated soft capsule endoscope;continuous control;DRL approach;physical simulations;deep reinforcement learning techniques;tissue-robot interactions;robot body dynamics","","","25","","","","","IEEE","IEEE Journals"
"Deep learning based EVM correction for RF receiver of vector signal analyser","Z. Jiang; J. Liu; K. Ye; W. Hong","School of Information Science and Engineering, Southeast University, People's Republic of China; Transcom Instruments Co., Ltd, People's Republic of China; Stanford University, USA; School of Information Science and Engineering, Southeast University, People's Republic of China","Electronics Letters","","2019","55","7","391","393","This Letter presents a novel deep learning approach for optimising the receiver performance with respect to the error vector magnitude (EVM) metric, which was verified and evaluated by applying it to a self-developed proprietary vector signal analyser (VSA). A four-layer neural network was built and trained to estimate and correct the systematic error of the VSA receiver by using a calibrated commercially available vector signal generator as the training source. Experimental results show that the EVM performance of the self-developed VSA is improved and approaches that of a state-of-the-art VSA.","","","10.1049/el.2018.7691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8684984","","","millimetre wave receivers;learning (artificial intelligence);neural nets;calibration;signal generators","EVM correction;RF receiver;Letter;deep learning approach;receiver performance;error vector magnitude metric;proprietary vector signal analyser;four-layer neural network;systematic error;VSA receiver;calibrated commercially available vector signal generator;training source;EVM performance;state-of-the-art VSA","","","5","","","","","IET","IET Journals"
"3D palmprint recognition using unsupervised convolutional deep learning network and SVM classifier","M. Chaa; Z. Akhtar; A. Attia","University of Ouargla, Algeria; University of Quebec, Canada; Mohamed El Bachir El Ibrahimi University, Algeria","IET Image Processing","","2019","13","5","736","745","Since past decade, efforts are afoot to design better hand-based automatic person recognition systems. Among the various hand-based biometric traits, palmprint as a biometric characteristic is now gaining increased attention from both the academic and industrial communities owing to its highly distinctive texture patterns, features richness, and stability. Here, the authors propose a new 3D palmprint recognition framework based on an unsupervised convolutional deep learning network named PCANet. Specifically, the proposed framework first reconstructs illumination-invariant 3D palmprint images using Single Scale Retinex (SSR) algorithm. Then, PCANet topology is employed to extract discriminative features from SSR images. Finally, a multi-class support vector machine (SVM) classification scheme is utilised to determine the identity of the person. Extensive experimental analysis on publicly available 3D palmprint PolyU dataset, which is composed of 8000 range images from 200 individuals, shows that proposed method outperforms existing approaches and is also able to attain 99.98% rank-1 accuracy.","","","10.1049/iet-ipr.2018.5642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8689157","","","convolutional neural nets;feature extraction;image classification;image texture;learning (artificial intelligence);palmprint recognition;support vector machines","3D palmprint PolyU dataset;PCANet topology;illumination-invariant 3D palmprint images;SSR images;multiclass support vector machine classification scheme;SVM classifier;hand-based automatic person recognition systems;hand-based biometric traits;biometric characteristic;academic communities;industrial communities;features richness;3D palmprint recognition;distinctive texture patterns;single scale retinex algorithm;PCANet unsupervised convolutional deep learning network","","","30","","","","","IET","IET Journals"
"Blind symbol packing ratio estimation for faster-than-Nyquist signalling based on deep learning","P. Song; F. Gong; Q. Li","Xidian University, People's Republic of China; Xidian University, People's Republic of China; Xidian University, People's Republic of China","Electronics Letters","","2019","55","21","1155","1157","This Letter proposes a blind symbol packing ratio estimation for faster-than-Nyquist (FTN) signalling based on state-of-the-art deep learning technology. The symbol packing ratio (also named speeding parameter, time packing parameter etc.) is a vital parameter to obtain the real symbol rate and recover the origin symbols from the received symbols by calculating the intersymbol interference. To the best of the authors' knowledge, this is the first effective estimation approach for symbol packing ratio in FTN signalling and has shown its fast convergence and robustness to signal-to-noise ratio by numerical simulations. Benefiting from the proposed blind estimation, the packing-ratio-based adaptive FTN transmission without dedicate channel or control frame becomes available. Also, the secure FTN communications based on the secret symbol packing ratio can be easily cracked.","","","10.1049/el.2019.2379","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866862","","","interference suppression;intersymbol interference;learning (artificial intelligence);signal detection","received symbols;FTN signalling;signal-to-noise ratio;blind estimation;packing-ratio-based adaptive FTN transmission;secret symbol packing ratio;blind symbol packing ratio estimation;faster-than-Nyquist signalling;state-of-the-art deep learning technology;time packing parameter etc;symbol rate;origin symbols","","","7","","","","","IET","IET Journals"
"Deep Learning for Multilabel Land Cover Scene Categorization Using Data Augmentation","R. Stivaktakis; G. Tsagkatakis; P. Tsakalides","Institute of Computer Science, FORTH, Heraklion, Greece; Institute of Computer Science, FORTH, Heraklion, Greece; Institute of Computer Science, FORTH, Heraklion, Greece","IEEE Geoscience and Remote Sensing Letters","","2019","16","7","1031","1035","Land cover classification is a flourishing research topic in the field of remote sensing. Conventional methodologies mainly focus either on the simplified single-label case or on the pixel-based approaches that cannot efficiently handle high-resolution images. On the other hand, the problem of multilabel land cover scene categorization remains, to this day, fairly unexplored. While deep learning and convolutional neural networks have demonstrated an astounding capacity at handling challenging machine learning tasks, such as image classification, they exhibit an underwhelming performance when trained with a limited amount of annotated examples. To overcome this issue, this paper proposes a data augmentation technique that can drastically increase the size of a smaller data set to copious amounts. Our experiments on a multilabel variation of the UC Merced Land Use data set demonstrate the potential of the proposed methodology, which outperforms the current state of the art by more than 6% in terms of the F-score metric.","","","10.1109/LGRS.2019.2893306","European Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633359","Convolutional neural networks (CNNs);data augmentation;land cover;multilabel classification;remote sensing;scene categorization","Training;Feature extraction;Deep learning;Remote sensing;Data models;Convolutional neural networks;Sensors","convolutional neural nets;geophysical image processing;image classification;image resolution;land cover;learning (artificial intelligence)","image classification;data augmentation technique;UC Merced Land Use data set;multilabel land cover scene categorization;simplified single-label case;pixel-based approaches;high-resolution images;deep learning;machine learning tasks;land cover classification;convolutional neural networks","","","23","","","","","IEEE","IEEE Journals"
"A Nonlinear Regression Application via Machine Learning Techniques for Geomagnetic Data Reconstruction Processing","H. Liu; Z. Liu; S. Liu; Y. Liu; J. Bin; F. Shi; H. Dong","School of Automation, Institute of Geophysics and Geomatics, China University of Geosciences, Wuhan, China; School of Engineering, Faculty of Applied Science, The University of British Columbia Okanagan Campus, Kelowna, Canada; School of Engineering, Faculty of Applied Science, The University of British Columbia Okanagan Campus, Kelowna, Canada; School of Engineering, Faculty of Applied Science, The University of British Columbia Okanagan Campus, Kelowna, Canada; School of Engineering, Faculty of Applied Science, The University of British Columbia Okanagan Campus, Kelowna, Canada; School of Engineering, Faculty of Applied Science, The University of British Columbia Okanagan Campus, Kelowna, Canada; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","1","128","140","The integrity of geomagnetic data is a critical factor in understanding the evolutionary process of Earth's magnetic field, as it provides useful information for near-surface exploration, unexploded explosive ordnance detection, and so on. Aimed to reconstruct undersampled geomagnetic data, this paper presents a geomagnetic data reconstruction approach based on machine learning techniques. The traditional linear interpolation approaches are prone to time inefficiency and high labor cost, while the proposed approach has a significant improvement. In this paper, three classic machine learning models, support vector machine, random forests, and gradient boosting were built. Besides, a deep learning algorithm, recurrent neural network, was explored to further improve the training performance. The proposed learning models were used to specify a continuous regression hyperplane from a training data. The specified regression hyperplane is a mapping of the relation between the mock-up missing data and the surrounding intact data. Afterward, the trained models, essentially the hyperplanes, were used to reconstruct the missing geomagnetic traces for validation, and they can be used for reconstructing further collected new field data. Finally, numerical experiments were derived. The results showed that the performance of our methods was more competitive in comparison with the traditional linear method, as the reconstruction accuracy was increased by approximately 10%~20%.","","","10.1109/TGRS.2018.2852632","National Natural Science Foundation of China; National Key Scientific Instrument and Equipment Development Project of China; Foundation of Science and Technology on Near-Surface Detection Laboratory; Fundamental Research Funds for the Central Universities, China University of Geosciences (Wuhan); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8423081","Deep neural network;geomagnetic;machine learning;modeling;reconstruction","Machine learning;Data models;Training;Support vector machines;Vegetation;Machine learning algorithms;Predictive models","data handling;geophysics computing;interpolation;learning (artificial intelligence);recurrent neural nets;regression analysis;support vector machines","Earth magnetic field;linear interpolation approaches;explosive ordnance detection;geomagnetic traces;recurrent neural network;continuous regression hyperplane;deep learning algorithm;support vector machine;geomagnetic data reconstruction approach;undersampled geomagnetic data;near-surface exploration;evolutionary process;geomagnetic data reconstruction processing;machine learning techniques;nonlinear regression application","","4","39","","","","","IEEE","IEEE Journals"
"Enhanced Machine Learning Techniques for Early HARQ Feedback Prediction in 5G","N. Strodthoff; B. Göktepe; T. Schierl; C. Hellge; W. Samek","Fraunhofer Heinrich Hertz Institute, Berlin, Germany; Fraunhofer Heinrich Hertz Institute, Berlin, Germany; Fraunhofer Heinrich Hertz Institute, Berlin, Germany; Fraunhofer Heinrich Hertz Institute, Berlin, Germany; Fraunhofer Heinrich Hertz Institute, Berlin, Germany","IEEE Journal on Selected Areas in Communications","","2019","37","11","2573","2587","We investigate Early Hybrid Automatic Repeat reQuest (E-HARQ) feedback schemes enhanced by machine learning techniques as a path towards ultra-reliable and low-latency communication (URLLC). To this end, we propose machine learning methods to predict the outcome of the decoding process ahead of the end of the transmission. We discuss different input features and classification algorithms ranging from traditional methods to newly developed supervised autoencoders. These methods are evaluated based on their prospects of complying with the URLLC requirements of effective block error rates below 10-5 at small latency overheads. We provide realistic performance estimates in a system model incorporating scheduling effects to demonstrate the feasibility of E-HARQ across different signal-to-noise ratios, subcode lengths, channel conditions and system loads, and show the benefit over regular HARQ and existing E-HARQ schemes without machine learning.","","","10.1109/JSAC.2019.2934001","Bundesministerium für Bildung und Forschung; Berlin Center for Machine Learning; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792202","5G;mobile communication;low latency communication;physical layer;machine learning;anomaly detection;deep learning","Machine learning;Decoding;Probabilistic logic;Machine learning algorithms;Error analysis;Delays;Receivers","5G mobile communication;automatic repeat request;error statistics;learning (artificial intelligence);telecommunication computing","E-HARQ;decoding process;URLLC requirements;block error rates;latency overheads;regular HARQ;machine learning;HARQ feedback prediction;hybrid automatic repeat request feedback;5G mobile communication;ultrareliable-low latency communication;signal-to-noise ratios;subcode lengths","","","25","CCBY","","","","IEEE","IEEE Journals"
"Deep Learning for Database Mapping and Asking Clarification Questions in Dialogue Systems","M. Korpusik; J. Glass","MIT Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; MIT Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","8","1321","1334","A dialogue system will often ask followup clarification questions when interacting with a user if the agent is unsure how to respond. In this new study, we explore deep reinforcement learning (RL) for asking followup questions when a user records a meal description, and the system needs to narrow down the options for which foods the person has eaten. We build off of prior work in which we use novel convolutional neural network models to bypass the standard feature engineering used in dialogue systems to handle the text mismatch between natural language user queries and structured database entries, demonstrating that our model learns semantically meaningful embedding representations of natural language. In this new nutrition domain, the followup clarification questions consist of possible attributes for each food that was consumed; for example, if the user drinks a cup of milk, the system should ask about the percent milkfat. We investigate an RL agent to dynamically follow up with the user, which we compare to rule-based and entropy-based methods. On a held-out test set, assuming the followup questions are answered correctly, deep RL significantly boosts top five food recall from 54.9% without followup to 89.0%. We also demonstrate that a hybrid RL model achieves the best perceived naturalness ratings in a human evaluation.","","","10.1109/TASLP.2019.2918618","National Institutes of Health; U.S. Department of Agriculture; Tufts University [Roberts]; Quanta Computing, Inc.; U.S. Department of Defense; National Defense Science Engineering Graduate Fellowship Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721137","Reinforcement learning;convolutional neural network;semantic embedding;crowdsourcing;dialogue system","Databases;Task analysis;Natural languages;Semantics;Reinforcement learning;Games;Speech processing","convolutional neural nets;entropy;interactive systems;learning (artificial intelligence);natural language interfaces;natural language processing;query languages;query processing;text analysis","convolutional neural network models;standard feature engineering;dialogue system;natural language user queries;structured database entries;RL agent;deep RL;hybrid RL model;database mapping;deep reinforcement learning;text mismatch;natural language representations;nutrition domain;entropy-based methods;rule-based method","","","45","","","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Mode Selection and Resource Management for Green Fog Radio Access Networks","Y. Sun; M. Peng; S. Mao","Key Laboratory of Universal Wireless Communications (Ministry of Education), Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communications (Ministry of Education), Beijing University of Posts and Telecommunications, Beijing, China; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA","IEEE Internet of Things Journal","","2019","6","2","1960","1971","Fog radio access networks (F-RANs) are seen as potential architectures to support services of Internet of Things by leveraging edge caching and edge computing. However, current works studying resource management in F-RANs mainly consider a static system with only one communication mode. Given network dynamics, resource diversity, and the coupling of resource management with mode selection, resource management in F-RANs becomes very challenging. Motivated by the recent development of artificial intelligence, a deep reinforcement learning (DRL)-based joint mode selection and resource management approach is proposed. Each user equipment (UE) can operate either in cloud RAN (C-RAN) mode or in device-to-device mode, and the resource managed includes both radio resource and computing resource. The core idea is that the network controller makes intelligent decisions on UE communication modes and processors' on-off states with precoding for UEs in C-RAN mode optimized subsequently, aiming at minimizing long-term system power consumption under the dynamics of edge cache states. By simulations, the impacts of several parameters, such as learning rate and edge caching service capability, on system performance are demonstrated, and meanwhile the proposal is compared with other different schemes to show its effectiveness. Moreover, transfer learning is integrated with DRL to accelerate learning process.","","","10.1109/JIOT.2018.2871020","State Major Science and Technology Special Project; National Natural Science Foundation of China; National Program for Special Support of Eminent Professionals; Division of Computer and Network Systems; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8468000","Artificial intelligence;communication mode selection;deep reinforcement learning (DRL);fog radio access networks (F-RANs);resource management","Resource management;Power demand;Program processors;Device-to-device communication;Heuristic algorithms;Cloud computing;Internet of Things","cloud computing;learning (artificial intelligence);quality of service;radio access networks;resource allocation","resource management approach;cloud RAN mode;device-to-device mode;radio resource;computing resource;UE communication modes;C-RAN mode;edge caching service capability;green fog radio access networks;F-RANs;edge computing;communication mode;resource diversity;deep reinforcement learning-based joint mode selection;network dynamics;on-off states;transfer learning","","15","39","","","","","IEEE","IEEE Journals"
"Deep Learning Aided Grant-Free NOMA Toward Reliable Low-Latency Access in Tactile Internet of Things","N. Ye; X. Li; H. Yu; A. Wang; W. Liu; X. Hou","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; DOCOMO Beijing Communications Laboratories Co., Ltd., Beijing, China; DOCOMO Beijing Communications Laboratories Co., Ltd., Beijing, China","IEEE Transactions on Industrial Informatics","","2019","15","5","2995","3005","Tactile Internet of Things (IoT) requires ultraresponsive and ultrareliable connections for massive IoT devices. As a promising enabler of tactile IoT, grant-free nonorthogonal multiple access (NOMA) exploits the joint benefit of grant-free access and nonorthogonal transmissions to achieve low latency massive access. However, it suffers from the reduced reliability caused by random interference. Hence, we formulate a variational optimization problem to improve the reliability of grant-free NOMA. Due to the intractability of this problem, we resort to deep learning by parameterizing the intractable variational function with a specially designed deep neural network, which incorporates random user activation and symbol spreading. The network is trained according to a novel multiloss function where a confidence penalty based on the user activation probability is considered. The spreading signatures are automatically generated while training, which matches the highly automatic applications in tactile IoT. The significant reliability gain of our scheme is validated by simulations.","","","10.1109/TII.2019.2895086","National Natural Science Foundation of China; MOE-CMCC Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625480","Deep learning (DL);fifth generation (5G);grant-free;nonorthogonal multiple access (NOMA);tactile Internet of Things (IoT);variational autoencoding","NOMA;Internet of Things;Reliability;Optimization;Informatics;Neural networks;Training","Internet of Things;learning (artificial intelligence);multi-access systems;neural nets;optimisation;probability;radiofrequency interference;telecommunication network reliability","tactile IoT;grant-free nonorthogonal multiple access;random interference;variational optimization problem;intractable variational function;random user activation;deep learning aided grant-free NOMA;tactile Internet of Things;deep neural network;symbol spreading;multiloss function","","2","40","","","","","IEEE","IEEE Journals"
"Deep Geodesic Learning for Segmentation and Anatomical Landmarking","N. Torosdagli; D. K. Liberton; P. Verma; M. Sincan; J. S. Lee; U. Bagci","Center for Research in Computer Vision, University of Central Florida, Orlando, FL, USA; Craniofacial Anomalies and Regeneration Section, National Institute of Dental and Craniofacial Research, National Institutes of Health, Bethesda, MD, USA; Craniofacial Anomalies and Regeneration Section, National Institute of Dental and Craniofacial Research, National Institutes of Health, Bethesda, MD, USA; Sanford School of Medicine, University of South Dakota, Sioux Falls, SD, USA; Craniofacial Anomalies and Regeneration Section, National Institute of Dental and Craniofacial Research, National Institutes of Health, Bethesda, MD, USA; Center for Research in Computer Vision, University of Central Florida, Orlando, FL, USA","IEEE Transactions on Medical Imaging","","2019","38","4","919","931","In this paper, we propose a novel deep learning framework for anatomy segmentation and automatic landmarking. Specifically, we focus on the challenging problem of mandible segmentation from cone-beam computed tomography (CBCT) scans and identification of 9 anatomical landmarks of the mandible on the geodesic space. The overall approach employs three inter-related steps. In the first step, we propose a deep neural network architecture with carefully designed regularization, and network hyper-parameters to perform image segmentation without the need for data augmentation and complex post-processing refinement. In the second step, we formulate the landmark localization problem directly on the geodesic space for sparsely-spaced anatomical landmarks. In the third step, we utilize a long short-term memory network to identify the closely-spaced landmarks, which is rather difficult to obtain using other standard networks. The proposed fully automated method showed superior efficacy compared to the state-of-the-art mandible segmentation and landmarking approaches in craniofacial anomalies and diseased states. We used a very challenging CBCT data set of 50 patients with a high-degree of craniomaxillofacial variability that is realistic in clinical practice. The qualitative visual inspection was conducted for distinct CBCT scans from 250 patients with high anatomical variability. We have also shown the state-of-the-art performance in an independent data set from the MICCAI Head-Neck Challenge (2015).","","","10.1109/TMI.2018.2875814","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8490669","Mandible segmentation;craniomaxillofacial deformities;deep learning;convolutional neural network;geodesic mapping;cone beam computed tomography (CBCT)","Image segmentation;Bones;Three-dimensional displays;Computed tomography;Computer architecture;Image analysis","computerised tomography;diseases;image reconstruction;image registration;image segmentation;learning (artificial intelligence);medical image processing;neural nets","anatomical landmarking;deep learning framework;anatomy segmentation;automatic landmarking;geodesic space;deep neural network architecture;network hyper-parameters;image segmentation;data augmentation;complex post-processing refinement;landmark localization problem;sparsely-spaced anatomical landmarks;short-term memory network;distinct CBCT scans;MICCAI Head-Neck Challenge;CBCT data set","","","30","","","","","IEEE","IEEE Journals"
"Diagnosis of Calibration State for Massive Antenna Array via Deep Learning","C. Shan; X. Chen; H. Yin; W. Wang; G. Wei; Y. Zhang","Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China; Wireless Positioning Capability Center, Shanghai Huawei Technologies Company, Ltd., Shanghai, China","IEEE Wireless Communications Letters","","2019","8","5","1431","1434","Time division duplex mode provides an attracting advantage called channel reciprocity to avoid excessive downlink (DL) pilot overhead, where the DL channel state information can be obtained from the uplink pilot. Unfortunately, such an ideal reciprocity would be broken easily because of hardware glitches from the transmit or receive radio frequency branches. In this letter, we develop a novel learning-based calibration state diagnosis network, where the linear pilot matrix and the non-linear reconstruction mapping decoder are learned jointly through training measure samples. With the assist of proposed network, a learning-based diagnosis procedure is designed to pick out the antennas that need to be calibrated. Experimental results demonstrate that both the normalized mean square error and accuracy are improved compared with the existing compressive sensing-based method.","","","10.1109/LWC.2019.2920818","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8728139","Massive antenna array;calibration state diagnosis;reciprocity calibration;deep learning;TDD","Antenna arrays;Calibration;Decoding;Radio frequency;Receiving antennas;Transmitting antennas;Antenna feeds","antenna arrays;compressed sensing;decoding;learning (artificial intelligence);matrix algebra;mean square error methods;neural nets;radio receivers;telecommunication computing;time division multiplexing;wireless channels","massive antenna array;deep learning;time division duplex mode;channel reciprocity;DL channel state information;radio frequency branches;linear pilot matrix;nonlinear reconstruction mapping decoder;learning-based diagnosis;learning-based calibration state diagnosis network;downlink pilot overhead;normalized mean square error","","","17","","","","","IEEE","IEEE Journals"
"DECODE: Deep Confidence Network for Robust Image Classification","G. Ding; Y. Guo; K. Chen; C. Chu; J. Han; Q. Dai","School of Software, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Computing and Communications, Lancaster University, Lancaster, U.K.; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","","2019","28","8","3752","3765","Recent years have witnessed the success of deep convolutional neural networks for image classification and many related tasks. It should be pointed out that the existing training strategies assume that there is a clean dataset for model learning. In elaborately constructed benchmark datasets, deep network has yielded promising performance under the assumption. However, in real-world applications, it is burdensome and expensive to collect sufficient clean training samples. On the other hand, collecting noisy labeled samples is very economical and practical, especially with the rapidly increasing amount of visual data in the web. Unfortunately, the accuracy of current deep models may drop dramatically even with 5%-10% label noise. Therefore, enabling label noise resistant classification has become a crucial issue in the data driven deep learning approaches. In this paper, we propose a DEep COnfiDEnce network (DECODE) to address this issue. In particular, based on the distribution of mislabeled data, we adopt a confidence evaluation module that is able to determine the confidence that a sample is mislabeled. With the confidence, we further use a weighting strategy to assign different weights to different samples so that the model pays less attention to low confidence data, which is more likely to be noise. In this way, the deep model is more robust to label noise. DECODE is designed to be general, such that it can be easily combined with existing studies. We conduct extensive experiments on several datasets, and the results validate that DECODE can improve the accuracy of deep models trained with noisy data.","","","10.1109/TIP.2019.2902115","National Key R&D Program of China; National Natural Science Foundation of China; National Postdoctoral Program for Innovative Talents; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653989","Deep learning;robustness;confidence model","Training;Noise measurement;Data models;Error analysis;Benchmark testing;Robustness;Training data","convolutional neural nets;data visualisation;image classification;image denoising;learning (artificial intelligence)","DECODE;deep confidence network;robust image classification;deep convolutional neural networks;model learning;deep network;label noise resistant classification;confidence evaluation module;deep model;noisy data;data visualization","","3","60","","","","","IEEE","IEEE Journals"
"Blockchain-Enabled Data Collection and Sharing for Industrial IoT With Deep Reinforcement Learning","C. H. Liu; Q. Lin; S. Wen","Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Computer Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Industrial Informatics","","2019","15","6","3516","3526","With the rapid development of smart mobile terminals (MTs), various industrial Internet of things (IIoT) applications can fully leverage them to collect and share data for providing certain services. However, two key challenges still remain. One is how to achieve high-quality data collection with limited MT energy resource and sensing range. Another is how to ensure security when sharing and exchanging data among MTs, to prevent possible device failure, network communication failure, malicious users or attackers, etc. To this end, we propose a blockchain-enabled efficient data collection and secure sharing scheme combining Ethereum blockchain and deep reinforcement learning (DRL) to create a reliable and safe environment. In this scheme, DRL is used to achieve the maximum amount of collected data, and the blockchain technology is used to ensure security and reliability of data sharing. Extensive simulation results demonstrate that the proposed scheme can provide higher security level and stronger resistance to attack than a traditional database based data sharing scheme for different levels/types of attacks.","","","10.1109/TII.2018.2890203","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8594641","Blockchain;crowdsourcing;deep reinforcement learning (DRL);energy efficiency;industrial Internet of thing;secure data sharing","Blockchain;Data collection;Security;Sensors;Task analysis;Distributed databases;Reinforcement learning","computer network security;data privacy;database management systems;Internet;Internet of Things;learning (artificial intelligence);mobile computing","deep reinforcement learning;DRL;safe environment;collected data;blockchain technology;data sharing;higher security level;blockchain-enabled data collection;industrial IoT;smart mobile terminals;high-quality data collection;sensing range;network communication failure;malicious users;blockchain-enabled efficient data collection;secure sharing scheme;Ethereum blockchain;device failure;industrial Internet of things applications","","4","43","","","","","IEEE","IEEE Journals"
"Joint Power Allocation and Channel Assignment for NOMA With Deep Reinforcement Learning","C. He; Y. Hu; Y. Chen; B. Zeng","Center for Intelligent Networking and Communications (CINC), School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Center for Intelligent Networking and Communications (CINC), School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal on Selected Areas in Communications","","2019","37","10","2200","2210","Non-orthogonal multiple access (NOMA) has been considered as a significant candidate technique for the next generation wireless communication to support high throughput and massive connectivity. It allows different users to be multiplexed on one channel through applying superposition coding at the transmitter and successive interference cancellation (SIC) at the receiver. To fully utilize the benefit of the NOMA technique, the key problem is how to optimally allocate resources, such as power and channels, to users to maximize the system performance. There have been some existing works on the power allocation for the single-carrier NOMA system. However, how to optimally assign channels in the multi-carrier NOMA system is still unclear. In this paper, we propose a deep reinforcement learning framework to allocate resources to users in a near optimal way. Specifically, we exploit an attention-based neural network (ANN) to perform the channel assignment. Simulation results show that the proposed framework can achieve better system performance, compared with the state-of-the-art approaches.","","","10.1109/JSAC.2019.2933762","National Natural Science Foundation of China; 111 Project; Thousand Youth Talents Program of China (to Yan Chen); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8790780","Non-orthogonal multiple access (NOMA);channel assignment;power allocation;deep reinforcement learning;attention-based neural network","NOMA;Resource management;Channel allocation;Reinforcement learning;Wireless communication;Multiplexing;System performance","channel allocation;channel coding;interference suppression;learning (artificial intelligence);multi-access systems;neural nets;radio receivers;radio transmitters;resource allocation;telecommunication computing;wireless channels","massive connectivity;superposition coding;transmitter;successive interference cancellation;NOMA technique;system performance;single-carrier NOMA system;multicarrier NOMA system;deep reinforcement learning framework;channel assignment;joint power allocation;nonorthogonal multiple access;next generation wireless communication;attention-based neural network","","","31","","","","","IEEE","IEEE Journals"
"Deep Learning for Fall Detection: Three-Dimensional CNN Combined With LSTM on Video Kinematic Data","N. Lu; Y. Wu; L. Feng; J. Song","Systems Engineering Institute, Xi'an Jiaotong University, Xi'an, China; Systems Engineering Institute, Xi'an Jiaotong University, Xi'an, China; Second Medical Imaging Department, Affiliated Hospital of Xi'an Jiaotong University (Ninth Hospital of Xi'an), Xi'an, China; Systems Engineering Institute, Xi'an Jiaotong University, Xi'an, China","IEEE Journal of Biomedical and Health Informatics","","2019","23","1","314","323","Fall detection is an important public healthcare problem. Timely detection could enable instant delivery of medical service to the injured. A popular nonintrusive solution for fall detection is based on videos obtained through ambient camera, and the corresponding methods usually require a large dataset to train a classifier and are inclined to be influenced by the image quality. However, it is hard to collect fall data and instead simulated falls are recorded to construct the training dataset, which is restricted to limited quantity. To address these problems, a three-dimensional convolutional neural network (3-D CNN) based method for fall detection is developed, which only uses video kinematic data to train an automatic feature extractor and could circumvent the requirement for large fall dataset of deep learning solution. 2-D CNN could only encode spatial information, and the employed 3-D convolution could extract motion feature from temporal sequence, which is important for fall detection. To further locate the region of interest in each frame, a long short-term memory (LSTM) based spatial visual attention scheme is incorporated. Sports dataset Sports-1 M with no fall examples is employed to train the 3-D CNN, which is then combined with LSTM to train a classifier with fall dataset. Experiments have verified the proposed scheme on fall detection benchmark with high accuracy as 100%. Superior performance has also been obtained on other activity databases.","","","10.1109/JBHI.2018.2808281","National Natural Science Foundation of China; Beijing Advanced Innovation Center for Intelligent Robots and Systems; Fundamental Research Funds for the Central Universities, in part by the Research Fund for the Doctoral Program of Higher Education of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8295206","Activity recognition;convolutional neural network;deep learning;fall detection;visual attention","Three-dimensional displays;Feature extraction;Machine learning;Convolution;Data mining;Two dimensional displays;Convolutional neural networks","feature extraction;health care;image classification;image motion analysis;image representation;image sequences;learning (artificial intelligence);medical image processing;object detection;recurrent neural nets;video cameras;video signal processing","Sports-1 M;sport dataset;long short-term memory based spatial visual attention scheme;temporal sequence;motion feature extraction;3D convolution;2D CNN;automatic feature extractor;3D CNN based method;three-dimensional convolutional neural network;fall data collection;image quality;ambient camera;medical service;public healthcare problem;LSTM;three-dimensional CNN;deep learning;video kinematic data;fall detection benchmark;fall dataset","","9","59","","","","","IEEE","IEEE Journals"
"Multi-Tenant Cross-Slice Resource Orchestration: A Deep Reinforcement Learning Approach","X. Chen; Z. Zhao; C. Wu; M. Bennis; H. Liu; Y. Ji; H. Zhang","VTT Technical Research Centre of Finland, Oulu, Finland; Zhejiang Lab, Hangzhou, China; Graduate School of Informatics and Engineering, The University of Electro-Communications, Tokyo, Japan; Centre for Wireless Communications, University of Oulu, Oulu, Finland; Department of Electrical Engineering and Computer Science, The Catholic University of America, Washington, DC, USA; Information Systems Architecture Research Division, National Institute of Informatics, Tokyo, Japan; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Journal on Selected Areas in Communications","","2019","37","10","2377","2392","With the cellular networks becoming increasingly agile, a major challenge lies in how to support diverse services for mobile users (MUs) over a common physical network infrastructure. Network slicing is a promising solution to tailor the network to match such service requests. This paper considers a system with radio access network (RAN)-only slicing, where the physical infrastructure is split into slices providing computation and communication functionalities. A limited number of channels are auctioned across scheduling slots to MUs of multiple service providers (SPs) (i.e., the tenants). Each SP behaves selfishly to maximize the expected long-term payoff from the competition with other SPs for the orchestration of channels, which provides its MUs with the opportunities to access the computation and communication slices. This problem is modelled as a stochastic game, in which the decision makings of a SP depend on the global network dynamics as well as the joint control policy of all SPs. To approximate the Nash equilibrium solutions, we first construct an abstract stochastic game with the local conjectures of channel auction among the SPs. We then linearly decompose the per-SP Markov decision process to simplify the decision makings at a SP and derive an online scheme based on deep reinforcement learning to approach the optimal abstract control policies. Numerical experiments show significant performance gains from our scheme.","","","10.1109/JSAC.2019.2933893","Academy of Finland; National Key R&D Program of China; National Natural Science Foundation of China; Zhejiang Key Research and Development Plan; Japan Society for the Promotion of Science; Telecommunications Advanced Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792072","Network slicing;radio access networks;mobile-edge computing;packet scheduling;Markov decision process;deep reinforcement learning","Resource management;Vehicle-to-everything;Reliability;Safety;Fading channels;Reinforcement learning;3GPP","cellular radio;decision making;learning (artificial intelligence);Markov processes;mobile computing;radio access networks;stochastic games;telecommunication computing;telecommunication scheduling;wireless channels","multitenant cross-slice resource orchestration;cellular networks;diverse services;mobile users;MUs;common physical network infrastructure;network slicing;service requests;radio access network-only slicing;physical infrastructure;communication functionalities;multiple service providers;SPs;long-term payoff;communication slices;decision makings;global network dynamics;abstract stochastic game;channel auction;deep reinforcement learning;optimal abstract control policies;Nash equilibrium solutions","","1","68","","","","","IEEE","IEEE Journals"
"Autonomous Navigation of UAVs in Large-Scale Complex Environments: A Deep Reinforcement Learning Approach","C. Wang; J. Wang; Y. Shen; X. Zhang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Transactions on Vehicular Technology","","2019","68","3","2124","2136","In this paper, we propose a deep reinforcement learning (DRL)-based method that allows unmanned aerial vehicles (UAVs) to execute navigation tasks in large-scale complex environments. This technique is important for many applications such as goods delivery and remote surveillance. The problem is formulated as a partially observable Markov decision process (POMDP) and solved by a novel online DRL algorithm designed based on two strictly proved policy gradient theorems within the actor-critic framework. In contrast to conventional simultaneous localization and mapping-based or sensing and avoidance-based approaches, our method directly maps UAVs' raw sensory measurements into control signals for navigation. Experiment results demonstrate that our method can enable UAVs to autonomously perform navigation in a virtual large-scale complex environment and can be generalized to more complex, larger-scale, and three-dimensional environments. Besides, the proposed online DRL algorithm addressing POMDPs outperforms the state-of-the-art.","","","10.1109/TVT.2018.2890773","National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600371","Autonomous navigation;deep reinforcement learning;partially observable Markov decision process","Navigation;Aerospace electronics;Sensors;Markov processes;Reinforcement learning;Path planning","autonomous aerial vehicles;gradient methods;learning (artificial intelligence);Markov processes;mobile robots;path planning;SLAM (robots)","control signals;UAV raw sensory measurements;online DRL algorithm;POMDP;UAV;simultaneous localization and mapping;three-dimensional environments;avoidance-based approaches;partially observable Markov decision process;remote surveillance;goods delivery;navigation tasks;unmanned aerial vehicles;deep reinforcement learning approach;large-scale complex environment;autonomous navigation","","2","42","","","","","IEEE","IEEE Journals"
"Comprehensive Analysis of Deep Learning-Based Vehicle Detection in Aerial Images","L. Sommer; T. Schuchert; J. Beyerer","Vision and Fusion Laboratory, Karlsruhe Institute of Technology, Karlsruhe, Germany; IOSB, Fraunhofer Institute of Optronics, System Technologies and Image Exploitation, Karlsruhe, Germany; IOSB, Fraunhofer Institute of Optronics, System Technologies and Image Exploitation, Karlsruhe, Germany","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","9","2733","2747","Vehicle detection in aerial images is a crucial image processing step for many applications such as screening of large areas as used for surveillance, reconnaissance, or rescue tasks. In recent years, several deep learning-based frameworks have been proposed for object detection. However, these detectors were developed for data sets that considerably differ from aerial images. In this paper, we systematically investigate the potential of fast R-CNN and faster R-CNN for aerial images, which achieve top performing results on common detection benchmark data sets. Therefore, the applicability of eight state-of-the-art object proposal methods used to generate a set of candidate regions and of both detectors is examined. Relevant adaptations to account for the characteristics of the aerial images are provided. To overcome the shortcomings of the original approach in the case of handling small instances, we further propose our own networks that clearly outperform state-of-the-art methods for vehicle detection in aerial images. Furthermore, we analyze the impact of the different adaptations with respect to various ground sampling distances to provide a guideline for detecting small objects in aerial images. All experiments are performed on two publicly available data sets to account for differing characteristics such as varying object sizes, number of objects per image, and varying backgrounds.","","","10.1109/TCSVT.2018.2874396","Technical Center for Information Technology and Electronics (WTD 81) of the German Federal Office for Armament and Procurement; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8485630","Vehicle detection;aerial imagery;deep learning","Proposals;Microsoft Windows;Vehicle detection;Machine learning;Image edge detection;Detectors;Image segmentation","geophysical image processing;learning (artificial intelligence);object detection;road vehicles;traffic engineering computing","deep learning-based vehicle detection;aerial images;crucial image processing step;common detection benchmark data sets;object detection;data sets;candidate regions","","2","37","","","","","IEEE","IEEE Journals"
"End-to-End Automatic Image Annotation Based on Deep CNN and Multi-Label Data Augmentation","X. Ke; J. Zou; Y. Niu","Fujian Key Laboratory of Network Computing and Intelligent Information Processing, College of Mathematics and Computer Science and the Key Laboratory of Spatial Data Mining and Information Sharing, Ministry of Education, Fuzhou University, Fuzhou, China; Fujian Key Laboratory of Network Computing and Intelligent Information Processing, College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; Fujian Key Laboratory of Network Computing and Intelligent Information Processing, College of Mathematics and Computer Science and the Key Laboratory of Spatial Data Mining and Information Sharing, Ministry of Education, Fuzhou University, Fuzhou, China","IEEE Transactions on Multimedia","","2019","21","8","2093","2106","Automatic image annotation is a key step in image retrieval and image understanding. In this paper, we present an end-to-end automatic image annotation method based on a deep convolutional neural network (CNN) and multi-label data augmentation. Different from traditional annotation models that usually perform feature extraction and annotation as two independent tasks, we propose an end-to-end automatic image annotation model based on deep CNN (E2E-DCNN). E2E-DCNN transforms the image annotation problem into a multi-label learning problem. It uses a deep CNN structure to carry out the adaptive feature learning before constructing the end-to-end annotation structure using multiple cross-entropy loss functions for training. It is difficult to train a deep CNN model using small-scale datasets or scale up multi-label datasets using traditional data augmentation methods; hence, we propose a multi-label data augmentation method based on Wasserstein generative adversarial networks (ML-WGAN). The ML-WGAN generator can approximate the data distribution of a single multi-label image. The images generated by ML-WGAN can assist in the reduction of the over-fitting problem of training a deep CNN model and enhance the generalization ability of the trained CNN model. We optimize the network structure by using deformable convolution and spatial pyramid pooling. We experiment the proposed E2E-DCNN model with data augmentation by the proposed ML-WGAN on several public datasets. The experimental results demonstrate that the proposed model outperforms the state-of-the-art automatic image annotation models.","","","10.1109/TMM.2019.2895511","National Natural Science Foundation of China; Technology Guidance Project of Fujian Province; Natural Science Foundation of Fujian Province; University Production Project of Fujian Province; Fujian Collaborative Innovation Center for Big Data Application in Governments; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8626536","Image annotation;convolutional neural network;deep learning;generative adversarial networks;data augmentation","Image annotation;Training;Task analysis;Visualization;Semantics;Adaptation models;Data models","convolutional neural nets;entropy;feature extraction;image annotation;image retrieval;learning (artificial intelligence);optimisation","feature extraction;image annotation problem;multilabel learning problem;deep CNN structure;end-to-end annotation structure;deep CNN model;multilabel datasets;multilabel data augmentation method;Wasserstein generative adversarial networks;ML-WGAN generator;data distribution;trained CNN model;E2E-DCNN model;image retrieval;image understanding;end-to-end automatic image annotation method;deep convolutional neural network;adaptive feature learning;cross-entropy loss functions;small-scale datasets;multilabel image;over-fitting problem;network structure optimization;deformable convolution;spatial pyramid pooling","","2","50","Traditional","","","","IEEE","IEEE Journals"
"A Deep Learning Approach for Multi-Frame In-Loop Filter of HEVC","T. Li; M. Xu; C. Zhu; R. Yang; Z. Wang; Z. Guan","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Cyber Science and Technology, Beihang University, Beijing, China","IEEE Transactions on Image Processing","","2019","28","11","5663","5678","An extensive study on the in-loop filter has been proposed for a high efficiency video coding (HEVC) standard to reduce compression artifacts, thus improving coding efficiency. However, in the existing approaches, the in-loop filter is always applied to each single frame, without exploiting the content correlation among multiple frames. In this paper, we propose a multi-frame in-loop filter (MIF) for HEVC, which enhances the visual quality of each encoded frame by leveraging its adjacent frames. Specifically, we first construct a large-scale database containing encoded frames and their corresponding raw frames of a variety of content, which can be used to learn the in-loop filter in HEVC. Furthermore, we find that there usually exist a number of reference frames of higher quality and of similar content for an encoded frame. Accordingly, a reference frame selector (RFS) is designed to identify these frames. Then, a deep neural network for MIF (known as MIF-Net) is developed to enhance the quality of each encoded frame by utilizing the spatial information of this frame and the temporal information of its neighboring higher-quality frames. The MIF-Net is built on the recently developed DenseNet, benefiting from its improved generalization capacity and computational efficiency. In addition, a novel block-adaptive convolutional layer is designed and applied in the MIF-Net, for handling the artifacts influenced by coding tree unit (CTU) structure in HEVC. Extensive experiments show that our MIF approach achieves on average 11.621% saving of the Bjøntegaard delta bit-rate (BD-BR) on the standard test set, significantly outperforming the standard in-loop filter in HEVC and other state-of-the-art approaches.","","","10.1109/TIP.2019.2921877","National Natural Science Foundation of China; Fok Ying Tung Education Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736997","High efficiency video coding;in-loop filter;deep learning;multiple frames","Image coding;Video coding;Standards;Encoding;Radio frequency;Learning systems;Databases","data compression;learning (artificial intelligence);neural nets;video coding","encoded frame;HEVC;reference frame selector;MIF-Net;neighboring higher-quality frames;deep learning approach;high efficiency video coding standard;multi-frame in-loop filter;compression artifacts;RFS;Bjøntegaard delta bit-rate;computational efficiency;block-adaptive convolutional layer;densenet;visual quality;coding tree unit","","1","54","","","","","IEEE","IEEE Journals"
"Efficient B-Mode Ultrasound Image Reconstruction From Sub-Sampled RF Data Using Deep Learning","Y. H. Yoon; S. Khan; J. Huh; J. C. Ye","Department of Bio and Brain Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Bio and Brain Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Bio and Brain Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Bio and Brain Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Medical Imaging","","2019","38","2","325","336","In portable, 3-D, and ultra-fast ultrasound imaging systems, there is an increasing demand for the reconstruction of high-quality images from a limited number of radio-frequency (RF) measurements due to receiver (Rx) or transmit (Xmit) event sub-sampling. However, due to the presence of side lobe artifacts from RF sub-sampling, the standard beamformer often produces blurry images with less contrast, which are unsuitable for diagnostic purposes. Existing compressed sensing approaches often require either hardware changes or computationally expensive algorithms, but their quality improvements are limited. To address this problem, in this paper, we propose a novel deep learning approach that directly interpolates the missing RF data by utilizing redundancy in the Rx-Xmit plane. Our extensive experimental results using sub-sampled RF data from a multi-line acquisition B-mode system confirm that the proposed method can effectively reduce the data rate without sacrificing the image quality.","","","10.1109/TMI.2018.2864821","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8432500","Ultrasound imaging;B-mode;multi-line acquisition;deep learning;Hankel matrix","Radio frequency;Imaging;Neural networks;Ultrasonic imaging;Machine learning;Image reconstruction;Redundancy","biomedical ultrasonics;image reconstruction;learning (artificial intelligence);medical image processing","ultra-fast ultrasound imaging systems;high-quality images;radio-frequency measurements;blurry images;compressed sensing approaches;deep learning approach;multiline acquisition B-mode system;image quality;B-mode ultrasound image reconstruction","","7","54","","","","","IEEE","IEEE Journals"
"Change Detection From Synthetic Aperture Radar Images Based on Channel Weighting-Based Deep Cascade Network","Y. Gao; F. Gao; J. Dong; S. Wang","Qingdao Key Laboratory of Mixed Reality and Virtual Ocean, School of Information Science and Engineering, Ocean University of China, Qingdao, China; Qingdao Key Laboratory of Mixed Reality and Virtual Ocean, School of Information Science and Engineering, Ocean University of China, Qingdao, China; Qingdao Key Laboratory of Mixed Reality and Virtual Ocean, School of Information Science and Engineering, Ocean University of China, Qingdao, China; Qingdao Key Laboratory of Mixed Reality and Virtual Ocean, School of Information Science and Engineering, Ocean University of China, Qingdao, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","11","4517","4529","Deep learning methods have recently demonstrated their significant capability for synthetic aperture radar (SAR) image change detection. However, with the increase of network depth, convolutional neural networks often encounter some negative effects, such as overfitting and exploding gradients. In addition, the existing deep networks employed in SAR change detection tend to produce a lot of redundant features that affect the performance of the network. To solve the aforementioned problems, this article proposed a deep cascade network (DCNet) for SAR image change detection. On the one hand, a very DCNet is established to exploit discriminative features, and residual learning is introduced to solve the exploding gradients problem. In addition, a fusion mechanism is employed to combine the outputs of different hierarchical layers to further alleviate the exploding gradient problem. Moreover, a simple yet effective channel weighting-based module is designed for SAR change detection. Average pooling and max pooling are used to aggregate channel-wise information. Meaningful channel-wise features are emphasized and unnecessary ones are suppressed. Therefore, the similarity in feature maps can be reduced, and then, the classification performance of the DCNet is improved. Experimental results on four real SAR datasets demonstrated that the proposed DCNet can obtain better change detection performance than several competitive methods. Our codes are available at https://github.com/summitgao/SAR_CD_DCNet.","","","10.1109/JSTARS.2019.2953128","National Key R&D Program of China; National Natural Science Foundation of China; Key R&D Program of Shandong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911214","Change detection;deep cascade network (DCNet);deep learning;residual learning;synthetic aperture radar (SAR)","Feature extraction;Synthetic aperture radar;Radar polarimetry;Speckle;Training;Deep learning;Reliability","","","","","48","IEEE","","","","IEEE","IEEE Journals"
"Fault Detection and Isolation in Industrial Processes Using Deep Learning Approaches","R. Iqbal; T. Maniak; F. Doctor; C. Karyotis","Institute of Future Transport and Cities, Coventry University, Coventry, U.K.; Nippon Seiki, Redditch, U.K.; School of Computer Science and Electronic Engineering, University of Essex, Colchester, U.K.; Interactive Coventry, Ltd., Coventry, U.K.","IEEE Transactions on Industrial Informatics","","2019","15","5","3077","3084","Automated fault detection is an important part of a quality control system. It has the potential to increase the overall quality of monitored products and processes. The fault detection of automotive instrument cluster systems in computer-based manufacturing assembly lines is currently limited to simple boundary checking. The analysis of more complex nonlinear signals is performed manually by trained operators, whose knowledge is used to supervise quality checking and manual detection of faults. We present a novel approach for automated Fault Detection and Isolation (FDI) based on deep learning. The approach was tested on data generated by computer-based manufacturing systems equipped with local and remote sensing devices. The results show that the approach models the different spatial/temporal patterns found in the data. The approach can successfully diagnose and locate multiple classes of faults under real-time working conditions. The proposed method is shown to outperform other established FDI methods.","","","10.1109/TII.2019.2902274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8654684","Artificial Neural Networks (ANNs);computer-aided manufacturing;deep learning;fault detection;machine learning;manufacturing automation","Fault detection;Monitoring;Manufacturing;Mathematical model;Expert systems;Sensors;Correlation","assembling;fault diagnosis;learning (artificial intelligence);manufacturing systems;process monitoring;quality control","deep learning approaches;automated fault detection;quality control system;automotive instrument cluster systems;complex nonlinear signals;computer-based manufacturing systems;isolation method;industrial process;process monitoring;computer-based manufacturing assembly lines;manual fault detection;remote sensing devices;real-time working condition","","2","34","","","","","IEEE","IEEE Journals"
"A Two-Stage Transfer Learning-Based Deep Learning Approach for Production Progress Prediction in IoT-Enabled Manufacturing","S. Huang; Y. Guo; D. Liu; S. Zha; W. Fang","College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Mechanical Engineering, Purdue University, West Lafayette, IN, USA","IEEE Internet of Things Journal","","2019","6","6","10627","10638","In make-to-order manufacturing enterprises, accurate production progress (PP) prediction is an important basis for dynamic production process optimization and on-time delivery of orders. The implementation of Internet of Things (IoT) makes it possible to take real-time production state as an important factor affecting PP. In the IoT-enabled workshop, a two-stage transfer learning-based prediction method using both historical production data and real-time state data is proposed to solve the problem of low-prediction accuracy and poor generalization performance caused by insufficient data of target order. The deep autoencoder (DAE) model with transfer learning is designed to extract the generalized features of target order in the first stage, which uses bootstrap sampling to avoid over fitting. The deep belief network (DBN) model with transfer learning is constructed to fit the nonlinear relation for PP prediction in the second stage. A real case from an IoT enabled machining workshop is taken to validate the performance of the proposed method over the other methods such as DBN, deep neural network.","","","10.1109/JIOT.2019.2940131","National Natural Science Foundation of China; National Defense Basic Scientific Research of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827506","Deep autoencoder (DAE);deep belief network (DBN);Internet of Things (IoT);production progress (PP);transfer learning","Predictive models;Real-time systems;Data models;Internet of Things;Conferences;Task analysis","","","","","36","IEEE","","","","IEEE","IEEE Journals"
"Divide and Conquer: A Deep CASA Approach to Talker-Independent Monaural Speaker Separation","Y. Liu; D. Wang","Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering and the Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","12","2092","2102","We address talker-independent monaural speaker separation from the perspectives of deep learning and computational auditory scene analysis (CASA). Specifically, we decompose the multi-speaker separation task into the stages of simultaneous grouping and sequential grouping. Simultaneous grouping is first performed in each time frame by separating the spectra of different speakers with a permutation-invariantly trained neural network. In the second stage, the frame-level separated spectra are sequentially grouped to different speakers by a clustering network. The proposed deep CASA approach optimizes frame-level separation and speaker tracking in turn, and produces excellent results for both objectives. Experimental results on the benchmark WSJ0-2mix database show that the new approach achieves the state-of-the-art results with a modest model size.","","","10.1109/TASLP.2019.2941148","National Institute on Deafness and Other Communication Disorders; Ohio Supercomputer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8834838","Monaural speech separation;speaker separation;computational auditory scene analysis;deep CASA","Training;Neural networks;Deep learning;Image analysis;Speech processing;Noise measurement;Indexes","acoustic signal processing;audio databases;divide and conquer methods;learning (artificial intelligence);neural nets;optimisation;pattern clustering;speaker recognition;speech processing","WSJ0-2mix database;divide and conquer;speaker tracking;frame-level separation;frame-level separated spectra;permutation-invariantly trained neural network;sequential grouping;simultaneous grouping;multispeaker separation task;computational auditory scene analysis;deep learning;talker-independent monaural speaker separation;deep CASA approach","","","44","Traditional","","","","IEEE","IEEE Journals"
"Deep Spatio-Temporal Representation for Detection of Road Accidents Using Stacked Autoencoder","D. Singh; C. K. Mohan","Department of Computer Science and Engineering, Visual Learning and Intelligence Group, IIT Hyderabad, Hyderabad, India; Department of Computer Science and Engineering, Visual Learning and Intelligence Group, IIT Hyderabad, Hyderabad, India","IEEE Transactions on Intelligent Transportation Systems","","2019","20","3","879","887","Vision-based detection of road accidents using traffic surveillance video is a highly desirable but challenging task. In this paper, we propose a novel framework for automatic detection of road accidents in surveillance videos. The proposed framework automatically learns feature representation from the spatiotemporal volumes of raw pixel intensity instead of traditional hand-crafted features. We consider the accident of the vehicles as an unusual incident. The proposed framework extracts deep representation using denoising autoencoders trained over the normal traffic videos. The possibility of an accident is determined based on the reconstruction error and the likelihood of the deep representation. For the likelihood of the deep representation, an unsupervised model is trained using one class support vector machine. Also, the intersection points of the vehicle's trajectories are used to reduce the false alarm rate and increase the reliability of the overall system. We evaluated out proposed approach on real accident videos collected from the CCTV surveillance network of Hyderabad City in India. The experiments on these real accident videos demonstrate the efficacy of the proposed approach.","","","10.1109/TITS.2018.2835308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367975","Accident detection;anomaly detection;deep learning;stacked autoencoder","Accidents;Videos;Trajectory;Feature extraction;Target tracking;Urban areas","feature extraction;image classification;image representation;neural nets;object detection;road accidents;support vector machines;traffic engineering computing;unsupervised learning;video coding;video signal processing;video surveillance","denoising autoencoders;normal traffic videos;accident videos;CCTV surveillance network;deep spatio-temporal representation;stacked autoencoder;vision-based detection;traffic surveillance video;feature representation;raw pixel intensity;road accident detection;one class support vector machine;false alarm rate reduction;Hyderabad city;India","","5","41","","","","","IEEE","IEEE Journals"
"Sparse2Dense: From Direct Sparse Odometry to Dense 3-D Reconstruction","J. Tang; J. Folkesson; P. Jensfelt","Centre for Autonomous Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Centre for Autonomous Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Centre for Autonomous Systems, KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Robotics and Automation Letters","","2019","4","2","530","537","In this letter, we proposed a new deep learning based dense monocular simultaneous localization and mapping (SLAM) method. Compared to existing methods, the proposed framework constructs a dense three-dimensional (3-D) model via a sparse to dense mapping using learned surface normals. With single view learned depth estimation as prior for monocular visual odometry, we obtain both accurate positioning and high-quality depth reconstruction. The depth and normal are predicted by a single network trained in a tightly coupled manner. Experimental results show that our method significantly improves the performance of visual tracking and depth prediction in comparison to the state-of-the-art in deep monocular dense SLAM.","","","10.1109/LRA.2019.2891433","Wallenberg AI, Autonomous Systems and Software Program; SSF Project FACT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8605349","Visual-based navigation;SLAM;deep learning in robotics and automation","Image reconstruction;Three-dimensional displays;Estimation;Deep learning;Simultaneous localization and mapping;Training;Visualization","distance measurement;image reconstruction;learning (artificial intelligence);mobile robots;robot vision;SLAM (robots)","sparse2dense;direct sparse odometry;dense 3-D reconstruction;deep learning;dense mapping;learned surface normals;depth estimation;monocular visual odometry;accurate positioning;high-quality depth reconstruction;visual tracking;depth prediction;deep monocular dense SLAM;dense monocular simultaneous localization and mapping","","1","47","","","","","IEEE","IEEE Journals"
"Deep Convolutional Hashing for Low-Dimensional Binary Embedding of Histopathological Images","M. Sapkota; X. Shi; F. Xing; L. Yang","Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Biomedical Engineering, University of Florida, Gainesville, FL, USA; Department of Biostatistics and Informatics, Colorado School of Public Health, University of Colorado Denver, Denver, CO, USA; Department of Biomedical Engineering, University of Florida, Gainesville, FL, USA","IEEE Journal of Biomedical and Health Informatics","","2019","23","2","805","816","Compact binary representations of histopathology images using hashing methods provide efficient approximate nearest neighbor search for direct visual query in large-scale databases. They can be utilized to measure the probability of the abnormality of the query image based on the retrieved similar cases, thereby providing support for medical diagnosis. They also allow for efficient managing of large-scale image databases because of a low storage requirement. However, the effectiveness of binary representations heavily relies on the visual descriptors that represent the semantic information in the histopathological images. Traditional approaches with hand-crafted visual descriptors might fail due to significant variations in image appearance. Recently, deep learning architectures provide promising solutions to address this problem using effective semantic representations. In this paper, we propose a deep convolutional hashing method that can be trained “pointwise” to simultaneously learn both semantic and binary representations of histopathological images. Specifically, we propose a convolutional neural network that introduces a latent binary encoding ('BE) layer for low-dimensional feature embedding to learn binary codes. We design a joint optimization objective function that encourages the network to learn discriminative representations from the label information, and reduce the gap between the real-valued low-dimensional embedded features and desired binary values. The binary encoding for new images can be obtained by forward propagating through the network and quantizing the output of the 'BE layer. Experimental results on a large-scale histopathological image dataset demonstrate the effectiveness of the proposed method.","","","10.1109/JBHI.2018.2827703","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8338362","Binary hashing;convolutional neural network;deep learning;feature embedding;microscopic images;search and retrieval","Binary codes;Image representation;Semantics;Machine learning;Training;Quantization (signal);Biomedical imaging","binary codes;convolutional neural nets;feature extraction;image coding;image representation;image retrieval;learning (artificial intelligence);medical image processing;visual databases","low-dimensional binary embedding;histopathological images;compact binary representations;hashing methods;efficient approximate nearest neighbor search;direct visual query;query image;hand-crafted visual descriptors;image appearance;deep learning architectures;effective semantic representations;deep convolutional hashing method;latent binary encoding layer;low-dimensional feature;binary codes;binary values;image dataset;real-valued low-dimensional embedded features;joint optimization objective","","6","69","","","","","IEEE","IEEE Journals"
"Exploiting Bi-Directional Channel Reciprocity in Deep Learning for Low Rate Massive MIMO CSI Feedback","Z. Liu; L. Zhang; Z. Ding","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Department of Electrical and Computer Engineering, University of California at Davis, Davis, CA, USA","IEEE Wireless Communications Letters","","2019","8","3","889","892","Channel state information (CSI) feedback is important for multiple-input multiple-output (MIMO) wireless systems to achieve their capacity gain in frequency division duplex mode. For massive MIMO systems, CSI feedback may consume too much bandwidth and degrade spectrum efficiency. This letter proposes a learning-based CSI feedback framework based on limited feedback and bi-directional reciprocal channel characteristics. The massive MIMO base station exploits the available uplink CSI to help recovering the unknown downlink CSI from low rate user feedback. We propose two deep learning architectures, DualNet-MAG and DualNet-ABS, to significantly reduce the CSI feedback payload based on the multipath reciprocity. DualNet-MAG and DualNet-ABS can exploit the bi-directional correlation of the magnitude and the absolute value of real/imaginary parts of the CSI coefficients, respectively. The experimental results demonstrate that our architectures bring an obvious improvement compared with the downlink-based architecture.","","","10.1109/LWC.2019.2898662","National Basic Research Program of China (973 Program); China Scholarship Council; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8638509","Massive MIMO;CSI feedback;multipath reciprocity;deep learning","Downlink;Uplink;Correlation;MIMO communication;Quantization (signal);Delays;Bidirectional control","channel capacity;frequency division multiplexing;learning (artificial intelligence);MIMO communication;telecommunication computing;wireless channels","available uplink CSI;unknown downlink CSI;low rate user feedback;deep learning architectures;DualNet-MAG;DualNet-ABS;CSI feedback payload;multipath reciprocity;CSI coefficients;low rate massive MIMO CSI feedback;channel state information feedback;multiple-input multiple-output wireless systems;frequency division duplex mode;massive MIMO systems;learning-based CSI feedback framework;massive MIMO base station;bidirectional correlation;spectrum efficiency;bidirectional channel reciprocity;capacity gain;bidirectional reciprocal channel characteristics","","1","15","","","","","IEEE","IEEE Journals"
"Internet of Things Meets Brain–Computer Interface: A Unified Deep Learning Framework for Enabling Human-Thing Cognitive Interactivity","X. Zhang; L. Yao; S. Zhang; S. Kanhere; M. Sheng; Y. Liu","School of Computer Science and Engineering, University of New South Wales, Kensington, NSW, Australia; School of Computer Science and Engineering, University of New South Wales, Kensington, NSW, Australia; School of Computer Science and Engineering, University of New South Wales, Kensington, NSW, Australia; School of Computer Science and Engineering, University of New South Wales, Kensington, NSW, Australia; Department of Computing, Macquarie University, Sydney, NSW, Australia; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA","IEEE Internet of Things Journal","","2019","6","2","2084","2092","A brain-computer interface (BCI) acquires brain signals, analyzes, and translates them into commands that are relayed to actuation devices for carrying out desired actions. With the widespread connectivity of everyday devices realized by the advent of the Internet of Things (IoT), BCI can empower individuals to directly control objects such as smart home appliances or assistive robots, directly via their thoughts. However, realization of this vision is faced with a number of challenges, most importantly being the issue of accurately interpreting the intent of the individual from the raw brain signals that are often of low fidelity and subject to noise. Moreover, preprocessing brain signals and the subsequent feature engineering are both time-consuming and highly reliant on human domain expertise. To address the aforementioned issues, in this paper, we propose a unified deep learning-based framework that enables effective human-thing cognitive interactivity in order to bridge individuals and IoT objects. We design a reinforcement learning-based selective attention mechanism (SAM) to discover the distinctive features from the input brain signals. In addition, we propose a modified long short-term memory to distinguish the interdimensional information forwarded from the SAM. To evaluate the efficiency of the proposed framework, we conduct extensive real-world experiments and demonstrate that our model outperforms a number of competitive state-of-the-art baselines. Two practical real-time human-thing cognitive interaction applications are presented to validate the feasibility of our approach.","","","10.1109/JIOT.2018.2877786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8506382","Brain–computer interface (BCI);cognitive;deep learning (DL);Internet of Things (IoT)","Electroencephalography;Internet of Things;Brain-computer interfaces;Brain modeling","brain-computer interfaces;cognition;Internet of Things;learning (artificial intelligence)","BCI;smart home appliances;assistive robots;subsequent feature engineering;input brain signals;actuation devices;deep learning-based framework;IoT;reinforcement learning;brain-computer interface;human-thing cognitive interactivity;selective attention mechanism;SAM;Internet of things;long short-term memory","","2","30","","","","","IEEE","IEEE Journals"
"Deep Learning-Based CSI Feedback Approach for Time-Varying Massive MIMO Channels","T. Wang; C. Wen; S. Jin; G. Y. Li","National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Institute of Communications Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA","IEEE Wireless Communications Letters","","2019","8","2","416","419","Massive multiple-input multiple-output (MIMO) systems rely on channel state information (CSI) feedback to perform precoding and achieve performance gain in frequency division duplex networks. However, the huge number of antennas poses a challenge to the conventional CSI feedback reduction methods and leads to excessive feedback overhead. In this letter, we develop a real-time CSI feedback architecture, called CsiNet-long short-term memory (LSTM), by extending a novel deep learning (DL)-based CSI sensing and recovery network. CsiNet-LSTM considerably enhances recovery quality and improves tradeoff between compression ratio (CR) and complexity by directly learning spatial structures combined with time correlation from training samples of time-varying massive MIMO channels. Simulation results demonstrate that CsiNet-LSTM outperforms existing compressive sensing-based and DL-based methods and is remarkably robust to CR reduction.","","","10.1109/LWC.2018.2874264","National Science Foundation (NSFC) for Distinguished Young Scholars of China; National Natural Science Foundation of China; Ministry of Science and Technology, Taiwan; ITRI in Hsinchu, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8482358","Massive MIMO;FDD;CSI feedback;compressive sensing;deep learning","Correlation;Decoding;Feature extraction;MIMO communication;Sensors;Image reconstruction;Training","compressed sensing;computational complexity;data compression;feedback;frequency division multiplexing;learning (artificial intelligence);MIMO communication;precoding;recurrent neural nets;telecommunication computing;time-varying channels;wireless channels","time-varying massive MIMO channels;multiple-input multiple-output systems;channel state information;performance gain;frequency division duplex networks;conventional CSI feedback reduction methods;excessive feedback overhead;real-time CSI feedback architecture;CsiNet-LSTM;time correlation;deep learning-based CSI feedback approach;CsiNet-long short-term memory;recovery network;recovery quality;compression ratio;complexity by directly learning spatial structures;DL-based methods;compressive sensing-based method;CR reduction","","21","14","","","","","IEEE","IEEE Journals"
"Adversarial Deep Tracking","F. Zhao; J. Wang; Y. Wu; M. Tang","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; Department of Medicine, Indiana University School of Medicine, Indianapolis, IN, USA; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","7","1998","2011","A number of visual tracking methods achieve the state-of-the-art performance based on deep learning recently. However, most of these trackers utilize the deep neural network in regression task or classification task separately. In this paper, we propose an adversarial deep tracking framework. The framework is composed of a fully convolutional Siamese neural network (regression network) and a discriminative classification network. Then, we jointly optimize the regression network and the classification network by adversarial learning. In the uniform framework, the regression network and classification network can be trained end-to-end as a whole using large amounts of video training data sets. During the testing phase, the regression network generates a response map which reflects the location and the size of the target within each candidate search patch, and the classification network discriminates which response map is the best in terms of the corresponding template patch and candidate search patch. In addition, we propose an attention visualization algorithm for our tracker, and it reflects the area that attracts the attention of our tracker during tracking. The experimental results on three large-scale visual tracking benchmarks (OTB-100, TC-128, and VOT2016) demonstrate the effectiveness of the proposed tracking algorithm and show that our tracker performs comparably against the state-of-the-art trackers.","","","10.1109/TCSVT.2018.2856540","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8412248","Visual tracking;deep learning;adversarial training;attention","Target tracking;Visualization;Task analysis;Training;Computational modeling;Neural networks;Correlation","image classification;learning (artificial intelligence);neural nets;object tracking;regression analysis","adversarial deep tracking framework;fully convolutional Siamese neural network;regression network;discriminative classification network;adversarial learning;classification network discriminates;large-scale visual tracking benchmarks;deep learning;deep neural network;regression task","","3","77","","","","","IEEE","IEEE Journals"
"Deep Cognitive Perspective: Resource Allocation for NOMA-Based Heterogeneous IoT With Imperfect SIC","M. Liu; T. Song; G. Gui","National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; College of Telecommunication and Information Engineering, Nanjing University of Posts and Telecommunication, Nanjing, China","IEEE Internet of Things Journal","","2019","6","2","2885","2894","The Internet of Things (IoT) has attracted significant attentions in the fifth generation mobile networks and the smart cities. However, considering the large numbers of connectivity demands, it is vital to improve the spectrum efficiency (SE) of the IoT with an affordable power consumption. To improve the SE, the nonorthogonal multiple access (NOMA) technology is newly proposed through accommodating multiple users in the same spectrums. As a result, in this paper, an energy efficient resource allocation (RA) problem is introduced for the NOMA-based heterogeneous IoT. At first, we assume the successive interference cancellation (SIC) is imperfect for practical implementations. Then, based on the analyzing method for cognitive radio networks, we present a stepwise RA scheme for the mobile users and the IoT users with the mutual interference management. Third, we propose a deep recurrent neural network-based algorithm to solve the problem optimally and rapidly. Moreover, a priorities and rate demands-based user scheduling method is supplemented, to coordinate the access of the heterogeneous users with the limited radio resource. At last, the simulation results verify that the deep learning-based scheme is able to provide optimal RA results for the NOMA heterogeneous IoT with fast convergence and low computational complexity. Compared with the conventional orthogonal frequency division multiple access system, the NOMA system with imperfect SIC yields better performance on the SE and the scale of connectivity, at the cost of high power consumption and low energy efficiency.","","","10.1109/JIOT.2018.2876152","Priority Academic Program Development of Jiangsu Higher Education Institutions; National Natural Science Foundation of China; Jiangsu Specially Appointed Professor Program; Program for Jiangsu Six Top Talent; Program for High-Level Entrepreneurial and Innovative Talents Introduction; Natural Science Foundation of Jiangsu Province; Natural Science Foundation of Jiangsu Higher Education Institutions; NUPTSF; Nanjing University of Posts and Telecommunications; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8500158","Cognitive radio (CR);deep learning;heterogeneous Internet of Things (IoT);imperfect successive interference cancelation (SIC);nonorthogonal multiple access (NOMA);resource allocation (RA)","NOMA;Interference;Silicon carbide;Internet of Things;Machine learning;Resource management;Signal to noise ratio","cognitive radio;communication complexity;computer network management;energy conservation;interference suppression;Internet of Things;learning (artificial intelligence);multi-access systems;radio networks;radiofrequency interference;recurrent neural nets;resource allocation;telecommunication power management;telecommunication scheduling","deep recurrent neural network-based algorithm;radio resource;deep learning-based scheme;SE;deep cognitive perspective;fifth generation mobile networks;spectrum efficiency;nonorthogonal multiple access;energy efficient resource allocation problem;cognitive radio networks;stepwise RA scheme;orthogonal frequency division multiple access system;imperfect SIC;power consumption;NOMA-based heterogeneous IoT system;Internet of Things;successive interference cancellation;smart cities;mutual interference management;rate demand-based user scheduling method;computational complexity","","48","39","","","","","IEEE","IEEE Journals"
"A Comparative Land-Cover Classification Feature Study of Learning Algorithms: DBM, PCA, and RF Using Multispectral LiDAR Data","S. Pan; H. Guan; Y. Yu; J. Li; D. Peng","School of Geographical Sciences, Nanjing University of Information Science and Technology, Nanjing, China; School of Remote Sensing and Geomatics Engineering, Nanjing University of Information Science and Technology, Nanjing, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China; Department of Geography and Environmental Management, University of Waterloo, Waterloo, ON, Canada; School of Remote Sensing and Geomatics Engineering, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","4","1314","1326","Multispectral LiDAR, characterization of completeness, and consistency of spectrum and spatial geometric data provide a new data source for land cover classification. However, how to choose the optimal features for a given set of land covers is an open problem for effective land cover classification. To address this problem, we propose a comparative scheme, which investigates a popular deep learning (deep Boltzmann machine, DBM) model for high-level feature representation and widely used machine learning methods for low-level feature extraction and selection [principal component analysis (PCA) and random forest (RF)] in land cover classification. The comparative study was conducted on the multispectral LiDAR point clouds, acquired by a Teledyne Optech's Titan airborne system. The deep learning-based high-level feature representation experimental results showed that, on an ordinary personal computer or workstation, this method required larger training samples and more computational complexity than the machine learning-based low-level feature extraction and selection methods. However, our comparative experiments demonstrated that the classification accuracies of the DBM-based method were higher than those of the RF-based and PCA-based methods using multispectral LiDAR data.","","","10.1109/JSTARS.2019.2899033","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661505","Feature extraction;feature selection;land cover classification;learning algorithms;multispectral LiDAR data","Feature extraction;Laser radar;Three-dimensional displays;Principal component analysis;Radio frequency;Deep learning;Remote sensing","Boltzmann machines;feature extraction;feature selection;geophysical image processing;image classification;land cover;optical radar;principal component analysis;random forests","classification accuracies;DBM-based method;RF-based;PCA-based methods;multispectral LiDAR data;comparative land-cover classification feature study;learning algorithms;spatial geometric data;data source;optimal features;open problem;comparative scheme;random forest;multispectral LiDAR point clouds;deep learning-based high-level feature representation;selection methods;comparative experiments;deep Boltzmann machine;machine learning methods;principal component analysis;Teledyne Optech Titan airborne system;low-level feature extraction method;low-level feature selection method","","","40","","","","","IEEE","IEEE Journals"
"3D2SeqViews: Aggregating Sequential Views for 3D Global Feature Learning by CNN With Hierarchical Attention Aggregation","Z. Han; H. Lu; Z. Liu; C. Vong; Y. Liu; M. Zwicker; J. Han; C. L. P. Chen","School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Aeronautics, Northwestern Polytechnical University, Xi’an, China; Department of Computer and Information Science, University of Macau, Macau, China; School of Software, Tsinghua University, Beijing, China; University of Maryland, College Park, College Park, MD, USA; School of Automation, Northwestern Polytechnical University, Xi’an, China; Faculty of Science and Technology, University of Macau, Macau, China","IEEE Transactions on Image Processing","","2019","28","8","3986","3999","Learning 3D global features by aggregating multiple views is important. Pooling is widely used to aggregate views in deep learning models. However, pooling disregards a lot of content information within views and the spatial relationship among the views, which limits the discriminability of learned features. To resolve this issue, 3D to Sequential Views (3D2SeqViews) is proposed to more effectively aggregate the sequential views using convolutional neural networks with a novel hierarchical attention aggregation. Specifically, the content information within each view is first encoded. Then, the encoded view content information and the sequential spatiality among the views are simultaneously aggregated by the hierarchical attention aggregation, where view-level attention and class-level attention are proposed to hierarchically weight sequential views and shape classes. View-level attention is learned to indicate how much attention is paid to each view by each shape class, which subsequently weights sequential views through a novel recursive view integration. Recursive view integration learns the semantic meaning of view sequence, which is robust to the first view position. Furthermore, class-level attention is introduced to describe how much attention is paid to each shape class, which innovatively employs the discriminative ability of the fine-tuned network. 3D2SeqViews learns more discriminative features than the state-of-the-art, which leads to the outperforming results in shape classification and retrieval under three large-scale benchmarks.","","","10.1109/TIP.2019.2904460","National Key R&D Program of China; National Natural Science Foundation of China; National Science Foundation; Shaanxi Key Research and Development Program; Aeronautical Science Fund; NWPU Basic Research Fund; University of Macau Research; Science and Technology Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666059","3D global feature learning;view aggregation;sequential views;hierarchical attention aggregation;CNN","Three-dimensional displays;Shape;Deep learning;Solid modeling;Feature extraction;Aggregates;Convolutional neural networks","convolutional neural nets;feature extraction;image classification;image representation;image retrieval;learning (artificial intelligence)","encoded view content information;view-level attention;class-level attention;shape class;weights sequential views;view sequence;view position;3D2SeqViews;3D global feature learning;multiple views;deep learning models;learned features;hierarchical attention aggregation;recursive view integration;shape retrieval;shape classification;CNN","","","43","","","","","IEEE","IEEE Journals"
"Personalized Recommendation of Social Images by Constructing a User Interest Tree With Deep Features and Tag Trees","J. Zhang; Y. Yang; L. Zhuo; Q. Tian; X. Liang","Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing University of Technology, Beijing, China; Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing University of Technology, Beijing, China; Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing University of Technology, Beijing, China; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA; Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing University of Technology, Beijing, China","IEEE Transactions on Multimedia","","2019","21","11","2762","2775","In view of the great diversity and complexity of social images, it is of great significance to improve the performance of personalized recommendation by learning a user interest from large-scale social images. Deep learning, as the latest research in the field of artificial intelligence, provides a new personalized recommendation solution of social images for learning a user's interest. Moreover, social image sharing websites (such as Flickr) allow users to tag uploaded images with tags. As an important image semantic cue, effective tags not only represent the latent image information but also show personalized user interest. Therefore, a personalized recommendation method of social image is proposed by constructing a user-interest tree with deep features and tag trees in this paper. The main contributions of our paper are as follows: first, to efficiently make use of tags, a tag tree of social images is created by the re-ranked tags; second, for compactly representing the image content, deep features are learned by training the AlexNet network; third, a user-interest tree is constructed with deep features and tag trees that include the user-interest tree of social images and the user-interest tree of tags, respectively, and finally, a personalized recommendation system of social images is built based on a user-interest tree. Experiments on the NUS-WIDE dataset have shown that our method outperforms state-of-the-art methods in terms of both precision and recall of personalized recommendations.","","","10.1109/TMM.2019.2912124","Natural Science Foundation of Beijing Municipality; National Natural Science Foundation of China; Beijing Municipal Natural Science Foundation Cooperation Beijing Education Committee; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693783","Social image;personalized recommendation;user-interest tree;deep features;tag trees","Deep learning;Semantics;Feature extraction;Predictive models;Cultural differences;Flickr;Training","information retrieval;Internet;learning (artificial intelligence);recommender systems;social networking (online);trees (mathematics);user experience;Web sites","tag tree;personalized recommendation;large-scale social images;personalized user interest;user-interest tree;deep features;user interest tree;social image sharing Websites;image semantic cue;AlexNet network;user interest learning;NUS-WIDE dataset","","1","36","Traditional","","","","IEEE","IEEE Journals"
"Driver Activity Recognition for Intelligent Vehicles: A Deep Learning Approach","Y. Xing; C. Lv; H. Wang; D. Cao; E. Velenis; F. Wang","School of Mechanical and Aerospace Engineering, Nanyang Technology University, Singapore; School of Mechanical and Aerospace Engineering, Nanyang Technology University, Singapore; AVL Powertrain UK Ltd, Coventry, U.K.; Department of Mechanical and Mechatronics Engineering, University of Waterloo, Waterloo, ON, Canada; Advanced Vehicle Engineering Centre, Cranfield University, Bedford, U.K.; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Vehicular Technology","","2019","68","6","5379","5390","Driver decisions and behaviors are essential factors that can affect the driving safety. To understand the driver behaviors, a driver activities recognition system is designed based on the deep convolutional neural networks (CNN) in this paper. Specifically, seven common driving activities are identified, which are the normal driving, right mirror checking, rear mirror checking, left mirror checking, using in-vehicle radio device, texting, and answering the mobile phone, respectively. Among these activities, the first four are regarded as normal driving tasks, while the rest three are classified into the distraction group. The experimental images are collected using a low-cost camera, and ten drivers are involved in the naturalistic data collection. The raw images are segmented using the Gaussian mixture model to extract the driver body from the background before training the behavior recognition CNN model. To reduce the training cost, transfer learning method is applied to fine tune the pre-trained CNN models. Three different pre-trained CNN models, namely, AlexNet, GoogLeNet, and ResNet50 are adopted and evaluated. The detection results for the seven tasks achieved an average of 81.6% accuracy using the AlexNet, 78.6% and 74.9% accuracy using the GoogLeNet and ResNet50, respectively. Then, the CNN models are trained for the binary classification task and identify whether the driver is being distracted or not. The binary detection rate achieved 91.4% accuracy, which shows the advantages of using the proposed deep learning approach. Finally, the real-world application are analyzed and discussed.","","","10.1109/TVT.2019.2908425","Young Elite Scientist Sponsorship Program by CAST; SUG-NAP of Nanyang Technological University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8678436","Driver behavior;driver distraction;convolutional neural network;transfer learning","Vehicles;Task analysis;Image segmentation;Activity recognition;Mirrors;Monitoring;Feature extraction","convolutional neural nets;driver information systems;Gaussian processes;image classification;image segmentation;learning (artificial intelligence);mixture models;object detection;road safety","deep learning approach;driver activity recognition;intelligent vehicles;driving safety;deep convolutional neural networks;in-vehicle radio device;naturalistic data collection;Gaussian mixture model;behavior recognition CNN model;transfer learning method;binary classification task;mirror checking;mobile phone;image segmentation","","4","52","","","","","IEEE","IEEE Journals"
"Implicit and Explicit Concept Relations in Deep Neural Networks for Multi-Label Video/Image Annotation","F. Markatopoulou; V. Mezaris; I. Patras","Centre for Research and Technology Hellas, Information Technologies Institute, Thermi, Greece; Centre for Research and Technology Hellas, Information Technologies Institute, Thermi, Greece; Mile End Campus, Queen Mary University of London, London, U.K.","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","6","1631","1644","In this paper, we propose a deep convolutional neural network (DCNN) architecture that addresses the problem of video/image concept annotation by exploiting concept relations at two different levels. At the first level, we build on ideas from multi-task learning, and propose an approach to learn concept-specific representations that are sparse, linear combinations of representations of latent concepts. By enforcing the sharing of the latent concept representations, we exploit the implicit relations between the target concepts. At a second level, we build on ideas from structured output learning and propose the introduction, at training time, of a new cost term that explicitly models the correlations between the concepts. By doing so, we explicitly model the structure in the output space (i.e., the concept labels). Both of the above are implemented using standard convolutional layers and are incorporated in a single DCNN architecture that can then be trained end-to-end with standard back-propagation. Experiments on four large-scale video and image data sets show that the proposed DCNN improves concept annotation accuracy and outperforms the related state-of-the-art methods.","","","10.1109/TCSVT.2018.2848458","H2020 LEIT Information and Communication Technologies; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8387768","Video/image concept annotation;deep learning;multi-task learning;structured outputs;multi-label learning;concept correlations;video analysis","Task analysis;Correlation;Standards;Training;Electronic mail;Neural networks;Semantics","convolutional neural nets;image annotation;image classification;learning (artificial intelligence);neural net architecture;video signal processing","concept labels;standard convolutional layers;large-scale video;image data sets;deep convolutional neural network architecture;concept relations;multitask learning;concept-specific representations;latent concept representations;structured output learning;DCNN architecture;implicit concept relation;explicit concept relation;image concept annotation;video concept annotation","","5","62","","","","","IEEE","IEEE Journals"
"Polar-Spatial Feature Fusion Learning With Variational Generative-Discriminative Network for PolSAR Classification","Z. Wen; Q. Wu; Z. Liu; Q. Pan","Key Laboratory of Information Fusion Technology of Ministry of Education, Northwestern Polytechnical University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Information Fusion Technology of Ministry of Education, Northwestern Polytechnical University, Xi’an, China; Key Laboratory of Information Fusion Technology of Ministry of Education, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","8914","8927","Feature learning-based polarimetric synthetic aperture radar (PolSAR) classification model will generally suffer from the challenge of deficient labeled pixels. In this paper, we propose a novel generative-discriminative network for PolSAR polar-spatial feature fusion learning and classification, which comprises of a deep generative network and a discriminative network with their bottom layers shared. With this architecture, it enables to make use of both labeled and unlabeled pixels in a PolSAR image for model learning in a semisupervised way. Moreover, the proposed network imposes a Gaussian random field prior and a conditional random field posterior on the learned fusion features and the output label configuration, respectively. Without the need of the complicated recurrent iterations, our network can still efficiently produce the structured fusion feature as well as a smoothed classification map by involving some auxiliary variables, and it is specifically optimized via variational inference within an alternating direction method of multipliers iteration scheme. Extensive experiments on different benchmark PolSAR imageries demonstrate the effectiveness and superiority of the proposed network. Compared with other state-of-the-art algorithms of PolSAR feature learning and classification, our model can achieve a much better performance in terms of the visual quality of the label map and overall classification accuracy, facilitating the much less labeling pixels.","","","10.1109/TGRS.2019.2923738","National Natural Science Foundation of China; Northwestern Polytechnical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765386","Deep learning;discriminative model;feature fusion;generative model;polarimetric synthetic aperture radar (PolSAR) classification;variational inference","Feature extraction;Data models;Scattering;Task analysis;Covariance matrices;Adaptation models;Deep learning","image classification;image fusion;iterative methods;neural nets;radar computing;radar imaging;radar polarimetry;synthetic aperture radar","variational generative-discriminative network;PolSAR classification;deficient labeled pixels;PolSAR polar-spatial feature fusion learning;deep generative network;unlabeled pixels;PolSAR image;Gaussian random field;conditional random field;learned fusion features;output label configuration;structured fusion feature;smoothed classification map;benchmark PolSAR imagery;feature learning;polarimetric synthetic aperture radar;complicated recurrent iterations;alternating direction method of multipliers iteration","","","47","","","","","IEEE","IEEE Journals"
"Plume Tracing via Model-Free Reinforcement Learning Method","H. Hu; S. Song; C. L. P. Chen","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","8","2515","2527","This paper studies the plume-tracing strategy for an autonomous underwater vehicle (AUV) in the deep-sea turbulent environment. The tracing problem is modeled as a partially observable Markov decision process with continuous state space and action space due to the spatio-temporal changes of environment. An long short-term memory-based reinforcement learning framework with full use of history information is proposed to generate a smooth strategy while the AUV interacting with the environment. Continuous temporal difference and deterministic policy gradient methods are employed to improve the strategy. To promote the performance of the algorithm, a supervised strategy generated by dynamic programming methods is utilized as transcendental knowledge of the agent. Historical searching trajectory's form and the exploration technology are specially designed to fit the algorithm. Simulation environments are established based on Reynolds-averaged Navier-Stokes equations and the effectiveness of the learned plume-tracing strategy is validated with simulation experiments.","","","10.1109/TNNLS.2018.2885374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598800","Deterministic policy gradient (DPG);long short-term memory (LSTM);partially observable Markov decision process (POMDP);plume-tracing;reinforcement learning (RL);supervised learning","Chemicals;Heuristic algorithms;Dynamic programming;Markov processes;Mathematical model;Reinforcement learning;Trajectory","autonomous underwater vehicles;decision theory;dynamic programming;gradient methods;Markov processes;mobile robots;Navier-Stokes equations;search problems;supervised learning;trajectory control","autonomous underwater vehicle;deep-sea turbulent environment;partially observable Markov decision process;smooth strategy;continuous temporal difference;deterministic policy gradient methods;supervised strategy;dynamic programming methods;historical searching trajectory;model-free reinforcement learning method;plume-tracing strategy;long short-term memory-based reinforcement;AUV interaction;Reynolds-averaged Navier-Stokes equations","","","35","","","","","IEEE","IEEE Journals"
"A Three-Stage Deep Learning Model for Accurate Retinal Vessel Segmentation","Z. Yan; X. Yang; K. Cheng","Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong","IEEE Journal of Biomedical and Health Informatics","","2019","23","4","1427","1436","Automatic retinal vessel segmentation is a fundamental step in the diagnosis of eye-related diseases, in which both thick vessels and thin vessels are important features for symptom detection. All existing deep learning models attempt to segment both types of vessels simultaneously by using a unified pixel-wise loss that treats all vessel pixels with equal importance. Due to the highly imbalanced ratio between thick vessels and thin vessels (namely the majority of vessel pixels belong to thick vessels), the pixelwise loss would be dominantly guided by thick vessels and relatively little influence comes from thin vessels, often leading to low segmentation accuracy for thin vessels. To address the imbalance problem, in this paper, we explore to segment thick vessels and thin vessels separately by proposing a three-stage deep learning model. The vessel segmentation task is divided into three stages, namely thick vessel segmentation, thin vessel segmentation, and vessel fusion. As better discriminative features could be learned for separate segmentation of thick vessels and thin vessels, this process minimizes the negative influence caused by their highly imbalanced ratio. The final vessel fusion stage refines the results by further identifying nonvessel pixels and improving the overall vessel thickness consistency. The experiments on public datasets DRIVE, STARE, and CHASE_DB1 clearly demonstrate that the proposed three-stage deep learning model outperforms the current state-of-the-art vessel segmentation methods.","","","10.1109/JBHI.2018.2872813","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Wuhan Science and Technology Bureau; Program for Huazhong University of Science and Technology Academic Frontier Youth Team; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8476171","Deep learning;vessel segmentation;imbalance problem;retinal image analysis","Machine learning;Training;Image segmentation;Feature extraction;Retinal vessels;Manuals;Propagation losses","blood vessels;diseases;eye;image segmentation;medical image processing;neural nets","automatic retinal vessel segmentation;vessel pixels;three-stage deep learning model;thick vessel segmentation;eye-related disease diagnosis;vessel fusion;nonvessel pixel identification","","3","42","","","","","IEEE","IEEE Journals"
"Hyperspectral Image Classification Based on Two-Phase Relation Learning Network","X. Ma; S. Ji; J. Wang; J. Geng; H. Wang","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","12","10398","10409","Deep learning-based classification methods are competent to achieve an excellent performance under one necessary condition, i.e., there are sufficient labeled samples in each class, which is extremely impractical in most of the remote sensing tasks. To improve the performance with small training sets, we resort to other hyperspectral images and design a two-phase relation learning network that can be transferred between different images for general information sharing and fine-trained on a specific hyperspectral image for individual information learning. Specifically, we use a relation learning method to compare samples and deal with the task inconsistency between different data sets, and we adopt an episode-based training strategy to mimic the testing setup and learn the transferable comparison ability. Benefited from these two strategies, the proposed network takes the advantage of extra knowledge for information supplement and learns to compare rather than to classify for information exploration, which guarantees a reasonable performance even with small training sets. Extensive experiments and analysis on three benchmarks demonstrate that the proposed method can provide an effective solution for hyperspectral image classification with small training sets, which makes it possible to work on large-scale applications of earth observation with less effort on field investigation.","","","10.1109/TGRS.2019.2934218","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Northwestern Polytechnical University; Natural Science Foundation of Liaoning Province; Dalian High-level Talent Innovation Support Program Project; Dalian Science and Technology Innovation Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818626","Classification;deep network;hyperspectral image","Hyperspectral imaging;Training;Measurement;Deep learning;Task analysis","hyperspectral imaging;image classification;learning (artificial intelligence);neural nets","two-phase relation learning network;information exploration;information supplement;episode-based training strategy;deep learning-based classification methods;hyperspectral image classification","","","40","IEEE","","","","IEEE","IEEE Journals"
"FissureNet: A Deep Learning Approach For Pulmonary Fissure Detection in CT Images","S. E. Gerard; T. J. Patton; G. E. Christensen; J. E. Bayouth; J. M. Reinhardt","Department of Biomedical Engineering, The University of Iowa, Iowa, IA, USA; Department of Medical Physics, University of Wisconsin-Madison, Madison, WI, USA; Departments of Electrical and Computer Engineering and Radiation Oncology, The University of Iowa, Iowa, IA, USA; Department of Radiation Oncology, University of Wisconsin-Madison, Madison, WI, USA; Department of Biomedical Engineering, The University of Iowa, Iowa, IA, USA","IEEE Transactions on Medical Imaging","","2019","38","1","156","166","Pulmonary fissure detection in computed tomography (CT) is a critical component for automatic lobar segmentation. The majority of fissure detection methods use feature descriptors that are hand-crafted, low-level, and have local spatial extent. The design of such feature detectors is typically targeted toward normal fissure anatomy, yielding low sensitivity to weak, and abnormal fissures that are common in clinical data sets. Furthermore, local features commonly suffer from low specificity, as the complex textures in the lung can be indistinguishable from the fissure when the global context is not considered. We propose a supervised discriminative learning framework for simultaneous feature extraction and classification. The proposed framework, called FissureNet, is a coarse-to-fine cascade of two convolutional neural networks. The coarse-to-fine strategy alleviates the challenges associated with training a network to segment a thin structure that represents a small fraction of the image voxels. FissureNet was evaluated on a cohort of 3706 subjects with inspiration and expiration 3DCT scans from the COPDGene clinical trial and a cohort of 20 subjects with 4DCT scans from a lung cancer clinical trial. On both data sets, FissureNet showed superior performance compared with a deep learning approach using the U-Net architecture and a Hessian-based fissure detection method in terms of area under the precision-recall curve (PR-AUC). The overall PR-AUC for FissureNet, U-Net, and Hessian on the COPDGene (lung cancer) data set was 0.980 (0.966), 0.963 (0.937), and 0.158 (0.182), respectively. On a subset of 30 COPDGene scans, FissureNet was compared with a recently proposed advanced fissure detection method called derivative of sticks (DoS) and showed superior performance with a PR-AUC of 0.991 compared with 0.668 for DoS.","","","10.1109/TMI.2018.2858202","National Cancer Institute; Graduate College, University of Iowa; Iowa Space Grant Consortium; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8432388","Lung;segmentation;X-ray imaging and computed tomography;machine learning;ConvNet;CNN","Lung;Computed tomography;Image segmentation;Feature extraction;Detectors;Machine learning;Training","cancer;computerised tomography;feature extraction;feedforward neural nets;image classification;image segmentation;learning (artificial intelligence);lung;medical image processing","coarse-to-fine cascade;coarse-to-fine strategy;COPDGene clinical trial;lung cancer clinical trial;deep learning approach;Hessian-based fissure detection method;PR-AUC;COPDGene data set;recently proposed advanced fissure detection method;pulmonary fissure detection;automatic lobar segmentation;feature descriptors;local spatial extent;feature detectors;normal fissure anatomy;abnormal fissures;clinical data sets;local features;supervised discriminative learning framework;simultaneous feature extraction;classification;CT images;FissureNet;computed tomography;convolutional neural networks;image voxels;3DCT scans;4DCT scans","","","47","","","","","IEEE","IEEE Journals"
"Unifying Visual Attribute Learning with Object Recognition in a Multiplicative Framework","K. Liang; H. Chang; B. Ma; S. Shan; X. Chen","Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","7","1747","1760","Attributes are mid-level semantic properties of objects. Recent research has shown that visual attributes can benefit many typical learning problems in computer vision community. However, attribute learning is still a challenging problem as the attributes may not always be predictable directly from input images and the variation of visual attributes is sometimes large across categories. In this paper, we propose a unified multiplicative framework for attribute learning, which tackles the key problems. Specifically, images and category information are jointly projected into a shared feature space, where the latent factors are disentangled and multiplied to fulfil attribute prediction. The resulting attribute classifier is category-specific instead of being shared by all categories. Moreover, our model can leverage auxiliary data to enhance the predictive ability of attribute classifiers, which can reduce the effort of instance-level attribute annotation to some extent. By integrated into an existing deep learning framework, our model can both accurately predict attributes and learn efficient image representations. Experimental results show that our method achieves superior performance on both instance-level and category-level attribute prediction. For zero-shot learning based on visual attributes and human-object interaction recognition, our method can improve the state-of-the-art performance on several widely used datasets.","","","10.1109/TPAMI.2018.2836461","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8371732","Attribute learning;zero-shot learning;image understanding","Visualization;Semantics;Predictive models;Task analysis;Machine learning;Training;Correlation","computer vision;image representation;learning (artificial intelligence);object recognition","zero-shot learning;visual attributes;typical learning problems;unified multiplicative framework;category-specific;attribute classifiers;instance-level attribute annotation;category-level attribute prediction;deep learning framework;visual attribute learning","","1","57","","","","","IEEE","IEEE Journals"
"Unsupervised Deep Slow Feature Analysis for Change Detection in Multi-Temporal Remote Sensing Images","B. Du; L. Ru; C. Wu; L. Zhang","School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","12","9976","9992","Change detection has been a hotspot in the remote sensing technology for a long time. With the increasing availability of multi-temporal remote sensing images, numerous change detection algorithms have been proposed. Among these methods, image transformation methods with feature extraction and mapping could effectively highlight the changed information and thus has a better change detection performance. However, the changes of multi-temporal images are usually complex, and the existing methods are not effective enough. In recent years, the deep network has shown its brilliant performance in many fields, including feature extraction and projection. Therefore, in this paper, based on the deep network and slow feature analysis (SFA) theory, we proposed a new change detection algorithm for multi-temporal remotes sensing images called deep SFA (DSFA). In the DSFA model, two symmetric deep networks are utilized for projecting the input data of bi-temporal imagery. Then, the SFA module is deployed to suppress the unchanged components and highlight the changed components of the transformed features. The change vector analysis pre-detection is employed to find unchanged pixels with high confidence as training samples. Finally, the change intensity is calculated with chi-square distance and the changes are determined by threshold algorithms. The experiments are performed on two real-world data sets and a public hyperspectral data set. The visual comparison and the quantitative evaluation have shown that DSFA could outperform the other state-of-the-art algorithms, including other SFA-based and deep learning methods.","","","10.1109/TGRS.2019.2930682","National Key R&D Program of China; National Natural Science Foundation of China; Natural Science Foundation of Hubei Province; National Basic Research Program of China (973 Program); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824216","Change detection;deep network;remote sensing images;slow feature analysis (SFA)","Feature extraction;Remote sensing;Change detection algorithms;Detection algorithms;Eigenvalues and eigenfunctions;Artificial neural networks;Training","feature extraction;geophysical image processing;geophysical signal processing;image classification;learning (artificial intelligence);remote sensing;unsupervised learning","unsupervised deep slow feature analysis;multitemporal remote sensing images;remote sensing technology;image transformation methods;multitemporal images;slow feature analysis theory;change detection algorithm;multitemporal remotes;deep SFA;symmetric deep networks;bi-temporal imagery;changed components;change vector analysis pre-detection;change intensity;deep learning methods","","","49","IEEE","","","","IEEE","IEEE Journals"
"Microwave SAIR Imaging Approach Based on Deep Convolutional Neural Network","Y. Zhang; Y. Ren; W. Miao; Z. Lin; H. Gao; S. Shi","Key Laboratory of Radio Astronomy, Purple Mountain Observatory, Chinese Academy of Sciences (CAS), Nanjing, China; Key Laboratory of Radio Astronomy, Purple Mountain Observatory, Chinese Academy of Sciences (CAS), Nanjing, China; Key Laboratory of Radio Astronomy, Purple Mountain Observatory, Chinese Academy of Sciences (CAS), Nanjing, China; Key Laboratory of Radio Astronomy, Purple Mountain Observatory, Chinese Academy of Sciences (CAS), Nanjing, China; Key Laboratory of Radio Astronomy, Purple Mountain Observatory, Chinese Academy of Sciences (CAS), Nanjing, China; Key Laboratory of Radio Astronomy, Purple Mountain Observatory, Chinese Academy of Sciences (CAS), Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","12","10376","10389","Microwave synthetic aperture interferometric radiometers (SAIRs) are very powerful instruments for high-resolution remote sensing of the atmosphere and the earth surfaces at microwave frequencies. Microwave SAIR imaging reconstruction from interferometric measurements suffers from hardware non-identities, limited prior information, and noise interference, and consequently often requires expert calibration strategies to reduce imaging error and improve the accuracy of the reconstruction. In this article, we propose a new SAIR imaging approach with a deep convolutional neural network (CNN) learning framework to optimize the reconstruction performance. We interpret interferometric measurements of SAIR as a signal encoding representation and SAIR imaging as the corresponding decoding representation. A deep CNN framework with additional fully connected layers is utilized to autonomously learn the decoding representation from interferometric measurement samples and perform SAIR imaging. The supervised learning forward model with hyperparameters makes that the proposed approach could accurately obtain the SAIR imaging representation involving multiple systematic features for real applications. We demonstrate the performance of the proposed imaging approach through extensive numerical experiments. Compared with conventional handcrafted Fourier transform and sparse regularization reconstruction imaging approaches, the proposed imaging approach based on deep learning is superior in terms of image quality, computing efficiency, and noise suppression.","","","10.1109/TGRS.2019.2934154","National Key Basic Research and Development Program of China; Chinese Academy of Sciences; National Natural Science Foundation of China; National Postdoctoral Program for Innovative Talents; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818625","Convolutional neural network (CNN);deep learning (DL);imaging approach;microwave synthetic aperture interferometric radiometers (SAIRs)","Microwave imaging;Microwave measurement;Microwave antenna arrays;Microwave radiometry;Microwave theory and techniques","calibration;convolutional neural nets;decoding;Fourier transforms;geophysical image processing;image reconstruction;image representation;image resolution;microwave imaging;radar imaging;radiometers;remote sensing;supervised learning;synthetic aperture radar","interferometric measurement samples;SAIR imaging representation;microwave SAIR imaging approach;microwave synthetic aperture interferometric radiometers;high-resolution remote sensing;expert calibration strategies;deep convolutional neural network learning framework;deep CNN framework;sparse regularization reconstruction imaging approaches;decoding representation;handcrafted Fourier transform;signal encoding representation;supervised learning forward model","","","41","IEEE","","","","IEEE","IEEE Journals"
"Label-Efficient Breast Cancer Histopathological Image Classification","Q. Qi; Y. Li; J. Wang; H. Zheng; Y. Huang; X. Ding; G. K. Rohde","Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Department of Biomedical Engineering and Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA, USA","IEEE Journal of Biomedical and Health Informatics","","2019","23","5","2108","2116","The automatic classification of breast cancer histopathological images has great significance in computer-aided diagnosis. Recently, deep learning via neural networks has enabled pattern detection and prediction using large, labeled datasets; whereas, collecting and annotating sufficient histological data using professional pathologists is time consuming, tedious, and extremely expensive. In the proposed paper, a deep active learning framework is designed and implemented for classification of breast cancer histopathological images, with the goal of maximizing the learning accuracy from very limited labeling. This method involves manual annotation of the most valuable unlabeled samples, which are then integrated into the training set. The model is then iteratively updated with an increasing training set. Here, two selection strategies are discussed for the proposed deep active learning framework: An entropy-based strategy and a confidence-boosting strategy. The proposed method has been validated using a publicly available breast cancer histopathological image dataset, wherein each image patch is binarily classified as benign or malignant. The experimental results demonstrate that, compared with a random selection, our proposed framework can reduce annotation costs up to 66.67%, with higher accuracy and less expensive annotation than standard query strategy.","","","10.1109/JBHI.2018.2885134","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; National Natural Science Foundation of Fujian Province, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8561292","Breast cancer;histopathological image classification;deep learning;active learning;query strategy","Training;Breast cancer;Entropy;Labeling;Manuals;Deep learning","biological tissues;cancer;entropy;feature extraction;feature selection;image classification;learning (artificial intelligence);medical image processing;neural nets","computer-aided diagnosis;pattern detection;deep active learning framework;breast cancer histopathological image classification;image patch classification;neural networks;selection strategies;entropy-based strategy;confidence-boosting strategy","","","26","Traditional","","","","IEEE","IEEE Journals"
"Deep Learning-Based Video System for Accurate and Real-Time Parking Measurement","B. Y. Cai; R. Alvarez; M. Sit; F. Duarte; C. Ratti","Senseable City Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Senseable City Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Computer Science and Engineering, University of California at San Diego, La Jolla, CA, USA; Senseable City Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Senseable City Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE Internet of Things Journal","","2019","6","5","7693","7701","Parking spaces are costly to build, parking payments are difficult to enforce, and drivers waste an excessive amount of time searching for empty lots. Accurate quantification would inform developers and municipalities in space allocation and design, while real-time measurements would provide drivers and parking enforcement with information that saves time and resources. In this paper, we propose an accurate and real-time video system for future Internet of Things (IoT) and smart cities applications. Using recent developments in deep convolutional neural networks (DCNNs) and a novel vehicle tracking filter, we combine information across multiple image frames in a video sequence to remove noise introduced by occlusions and detection failures. We demonstrate that our system achieves higher accuracy than pure image-based instance segmentation, and is comparable in performance to industry benchmark systems that utilize more expensive sensors, such as radar. Furthermore, our system shows significant potential in its scalability to a city-wide scale and also in the richness of its output that goes beyond traditional binary occupancy statistics.","","","10.1109/JIOT.2019.2902887","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660429","Artificial intelligence;computer vision;deep learning;Internet of Things (IoT);parking;smart city","Urban areas;Internet of Things;Object detection;Vehicles;Training;Real-time systems;Image segmentation","convolutional neural nets;image denoising;image filtering;image sequences;Internet of Things;learning (artificial intelligence);object tracking;traffic engineering computing;video signal processing","multiple image frames;video sequence;industry benchmark systems;deep learning-based video system;real-time parking measurement;parking spaces;parking payments;space allocation;parking enforcement;smart cities applications;deep convolutional neural networks;vehicle tracking filter;Internet of Things","","","35","","","","","IEEE","IEEE Journals"
"DRFN: Deep Recurrent Fusion Network for Single-Image Super-Resolution With Large Factors","X. Yang; H. Mei; J. Zhang; K. Xu; B. Yin; Q. Zhang; X. Wei","Department of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Department of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Department of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Department of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Department of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Department of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Department of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Multimedia","","2019","21","2","328","337","Recently, single-image super-resolution has made great progress due to the development of deep convolutional neural networks (CNNs). The vast majority of CNN-based models use a predefined upsampling operator, such as bicubic interpolation, to upscale input low-resolution images to the desired size and learn nonlinear mapping between the interpolated image and ground truth high-resolution (HR) image. However, interpolation processing can lead to visual artifacts as details are over smoothed, particularly when the super-resolution factor is high. In this paper, we propose a deep recurrent fusion network (DRFN), which utilizes transposed convolution instead of bicubic interpolation for upsampling and integrates different-level features extracted from recurrent residual blocks to reconstruct the final HR images. We adopt a deep recurrence learning strategy and, thus, have a larger receptive field, which is conducive to reconstructing an image more accurately. Furthermore, we show that the multilevel fusion structure is suitable for dealing with image super-resolution problems. Extensive benchmark evaluations demonstrate that the proposed DRFN performs better than most current deep learning methods in terms of accuracy and visual effects, especially for large-scale images, while using fewer parameters.","","","10.1109/TMM.2018.2863602","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8425771","Image super-resolution;transposed convolution;deep recurrent network;multi-level fusion structure;large factors","Feature extraction;Image reconstruction;Convolution;Image resolution;Interpolation;Databases;Visualization","convolutional neural nets;feature extraction;image resolution;image sampling;interpolation;learning (artificial intelligence)","DRFN;deep recurrent fusion network;single-image super-resolution;deep convolutional neural networks;predefined upsampling operator;bicubic interpolation;upscale input low-resolution images;interpolated image;ground truth high-resolution image;interpolation processing;super-resolution factor;recurrent residual blocks;final HR images;deep recurrence learning strategy;image super-resolution problems;large-scale images;CNNs;deep learning methods;feature extraction","","","45","","","","","IEEE","IEEE Journals"
"SSDH: Semi-Supervised Deep Hashing for Large Scale Image Retrieval","J. Zhang; Y. Peng","Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","1","212","225","Hashing methods have been widely used for efficient similarity retrieval on large scale image database. Traditional hashing methods learn hash functions to generate binary codes from hand-crafted features, which achieve limited accuracy since the hand-crafted features cannot optimally represent the image content and preserve the semantic similarity. Recently, several deep hashing methods have shown better performance because the deep architectures generate more discriminative feature representations. However, these deep hashing methods are mainly designed for supervised scenarios, which only exploit the semantic similarity information, but ignore the underlying data structures. In this paper, we propose the semi-supervised deep hashing approach, to perform more effective hash function learning by simultaneously preserving semantic similarity and underlying data structures. The main contributions are as follows: 1) We propose a semi-supervised loss to jointly minimize the empirical error on labeled data, as well as the embedding error on both labeled and unlabeled data, which can preserve the semantic similarity and capture the meaningful neighbors on the underlying data structures for effective hashing. 2) A semi-supervised deep hashing network is designed to extensively exploit both labeled and unlabeled data, in which we propose an online graph construction method to benefit from the evolving deep features during training to better capture semantic neighbors. To the best of our knowledge, the proposed deep network is the first deep hashing method that can perform hash code learning and feature learning simultaneously in a semi-supervised fashion. Experimental results on five widely-used data sets show that our proposed approach outperforms the state-of-the-art hashing methods.","","","10.1109/TCSVT.2017.2771332","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8101524","Large scale image retrieval;online graph construction;semi-supervised deep hashing;underlying data structures","Semantics;Data structures;Training;Feature extraction;Binary codes;Convolutional codes;Image retrieval","binary codes;data structures;file organisation;graph theory;image representation;image retrieval;learning (artificial intelligence);visual databases","scale image retrieval;scale image database;hash functions;hand-crafted features;semantic similarity information;semisupervised loss;unlabeled data;semisupervised deep hashing network;evolving deep features;deep network;code learning;feature learning;data structures;SSDH;large scale image retrieval;similarity retrieval;online graph construction","","8","45","","","","","IEEE","IEEE Journals"
"Melanoma Recognition in Dermoscopy Images via Aggregated Deep Convolutional Features","Z. Yu; X. Jiang; F. Zhou; J. Qin; D. Ni; S. Chen; B. Lei; T. Wang","National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science CenterShenzhen University; School of Electrical and Electronic EngineeringNanyang Technological University; Department of Industrial and Manufacturing, Systems EngineeringThe University of Michigan; Centre for Smart HealthSchool of Nursing, The Hong Kong Polytechnic University; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science CenterShenzhen University; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science CenterShenzhen University; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, China","IEEE Transactions on Biomedical Engineering","","2019","66","4","1006","1016","In this paper, we present a novel framework for dermoscopy image recognition via both a deep learning method and a local descriptor encoding strategy. Specifically, deep representations of a rescaled dermoscopy image are first extracted via a very deep residual neural network pretrained on a large natural image dataset. Then these local deep descriptors are aggregated by orderless visual statistic features based on Fisher vector (FV) encoding to build a global image representation. Finally, the FV encoded representations are used to classify melanoma images using a support vector machine with a Chi-squared kernel. Our proposed method is capable of generating more discriminative features to deal with large variations within melanoma classes, as well as small variations between melanoma and nonmelanoma classes with limited training data. Extensive experiments are performed to demonstrate the effectiveness of our proposed method. Comparisons with state-of-the-art methods show the superiority of our method using the publicly available ISBI 2016 Skin lesion challenge dataset.","","","10.1109/TBME.2018.2866166","National Natural Science Foundation of China; National Key Research and Develop Program; National Natural Science Foundation of Guangdong Province; Shenzhen Peacock Plan; Shenzhen Key Basic Research Project; Hong Kong RGC General Research Fund; National Taipei University of Technology-Shenzhen University Joint Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8440053","Dermoscopy image;melanoma recognition;residual network;fisher vector;deep learning","Melanoma;Feature extraction;Lesions;Image recognition;Skin;Image color analysis;Neural networks","cancer;convolutional neural nets;feature extraction;image classification;image coding;image representation;image segmentation;image texture;learning (artificial intelligence);medical image processing;skin;support vector machines","global image representation;melanoma images;support vector machine;Chi-squared kernel;discriminative features;melanoma classes;nonmelanoma classes;melanoma recognition;aggregated deep convolutional features;dermoscopy image recognition;deep learning method;local descriptor encoding strategy;deep representations;rescaled dermoscopy image;deep residual neural network;natural image dataset;local deep descriptors;orderless visual statistic features;Fisher vector;large natural image dataset","","5","71","","","","","IEEE","IEEE Journals"
"Change detection in SAR images using deep belief network: a new training approach based on morphological images","F. Samadi; G. Akbarizadeh; H. Kaabi","Shahid Chamran University of Ahvaz, Iran; Shahid Chamran University of Ahvaz, Iran; Shahid Chamran University of Ahvaz, Iran","IET Image Processing","","2019","13","12","2255","2264","In solving change detection problem, unsupervised methods are usually preferred to their supervised counterparts due to the difficulty of producing labelled data. Nevertheless, in this paper, a supervised deep learning-based method is presented for change detection in synthetic aperture radar (SAR) images. A Deep Belief Network (DBN) was employed as the deep architecture in the proposed method, and the training process of this network included unsupervised feature learning followed by supervised network fine-tuning. From a general perspective, the trained DBN produces a change detection map as the output. Studies on DBNs demonstrate that they do not produce ideal output without a proper dataset for training. Therefore, the proposed method in this study provided a dataset with an appropriate data volume and diversity for training the DBN using the input images and those obtained from applying the morphological operators on them. The great computational volume and the time-consuming nature of simulation are the drawbacks of deep learning-based algorithms. To overcome such disadvantages, a method was introduced to greatly reduce computations without compromising the performance of the trained DBN. Experimental results indicated that the proposed method had an acceptable implementation time in addition to its desirable performance and high accuracy.","","","10.1049/iet-ipr.2018.6248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870592","","","belief networks;image classification;learning (artificial intelligence);radar imaging;synthetic aperture radar;unsupervised learning","input images;diversity;appropriate data volume;input SAR images;change detection map;supervised network fine-tuning;training process;introduced method;deep architecture;synthetic aperture radar image changes;deep learning-based supervised method;labelled data;supervised counterparts;unsupervised methods;morphological images;training approach;deep belief network;trained DBN;dataset;detection performance;change detection problems;deep learning-based algorithms","","","26","","","","","IET","IET Journals"
"Time-Contrastive Learning Based Deep Bottleneck Features for Text-Dependent Speaker Verification","A. K. Sarkar; Z. Tan; H. Tang; S. Shon; J. Glass","Department of Electronic Systems, Aalborg University, Aalborg, Denmark; Department of Electronic Systems, Aalborg University, Aalborg, Denmark; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","8","1267","1279","There are a number of studies about extraction of bottleneck (BN) features from deep neural networks (DNNs) trained to discriminate speakers, pass-phrases, and triphone states for improving the performance of text-dependent speaker verification (TD-SV). However, a moderate success has been achieved. A recent study presented a time contrastive learning (TCL) concept to explore the non-stationarity of brain signals for classification of brain states. Speech signals have similar non-stationarity property, and TCL further has the advantage of having no need for labeled data. We therefore present a TCL based BN feature extraction method. The method uniformly partitions each speech utterance in a training dataset into a predefined number of multi-frame segments. Each segment in an utterance corresponds to one class, and class labels are shared across utterances. DNNs are then trained to discriminate all speech frames among the classes to exploit the temporal structure of speech. In addition, we propose a segment-based unsupervised clustering algorithm to re-assign class labels to the segments. TD-SV experiments were conducted on the RedDots challenge database. The TCL-DNNs were trained using speech data of fixed pass-phrases that were excluded from the TD-SV evaluation set, so the learned features can be considered phrase-independent. We compare the performance of the proposed TCL BN feature with those of short-time cepstral features and BN features extracted from DNNs discriminating speakers, pass-phrases, speaker+pass-phrase, as well as monophones whose labels and boundaries are generated by three different automatic speech recognition (ASR) systems. Experimental results show that the proposed TCL-BN outperforms cepstral features and speaker+pass-phrase discriminant BN features, and its performance is on par with those of ASR derived BN features. Moreover, the clustering method improves the TD-SV performance of TCL-BN and ASR derived BN features with respect to their standalone counterparts. We further study the TD-SV performance of fusing cepstral and BN features.","","","10.1109/TASLP.2019.2915322","iSocioBot project; Danish Council for Independent Research-Technology and Production Sciences; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge MA, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8708955","DNNs;time-contrastive learning;bottleneck feature;GMM-UBM;speaker verification","Feature extraction;Training;Speech recognition;Phonetics;Clustering methods;Mel frequency cepstral coefficient","brain;cepstral analysis;feature extraction;Gaussian processes;image segmentation;learning (artificial intelligence);neural nets;pattern clustering;speaker recognition","BN feature extraction method;speech utterance;training dataset;multiframe segments;speech frames;segment-based unsupervised clustering algorithm;TCL-DNNs;speech data;fixed pass-phrases;learned features;TCL BN feature;short-time cepstral features;BN features;DNNs discriminating speakers;TCL-BN;deep bottleneck features;text-dependent speaker verification;deep neural networks;time contrastive learning concept;brain signals;brain states;speech signals;nonstationarity property;time-contrastive features;automatic speech recognition systems","","","56","","","","","IEEE","IEEE Journals"
"A Knowledge-Based Recommendation System That Includes Sentiment Analysis and Deep Learning","R. L. Rosa; G. M. Schwartz; W. V. Ruggiero; D. Z. Rodríguez","Federal University of Lavras, Lavras, Brazil; Biosciences Institute of Rio Claro, Universidade Estadual Paulista Júlio de Mesquita Filho, São Paulo, Brazil; Polytechnic School of the University of São Paulo, São Paulo, Brazil; Federal University of Lavras, Lavras, Brazil","IEEE Transactions on Industrial Informatics","","2019","15","4","2124","2135","Online social networks provide relevant information on users' opinion about different themes. Thus, applications, such as monitoring and recommendation systems (RS) can collect and analyze this data. This paper presents a knowledge-based recommendation system (KBRS), which includes an emotional health monitoring system to detect users with potential psychological disturbances, specifically, depression and stress. Depending on the monitoring results, the KBRS, based on ontologies and sentiment analysis, is activated to send happy, calm, relaxing, or motivational messages to users with psychological disturbances. Also, the solution includes a mechanism to send warning messages to authorized persons, in case a depression disturbance is detected by the monitoring system. The detection of sentences with depressive and stressful content is performed through a convolutional neural network and a bidirectional long short-term memory - recurrent neural networks (RNN); the proposed method reached an accuracy of 0.89 and 0.90 to detect depressed and stressed users, respectively. Experimental results show that the proposed KBRS reached a rating of 94% of very satisfied users, as opposed to 69% reached by a RS without the use of neither a sentiment metric nor ontologies. Additionally, subjective test results demonstrated that the proposed solution consumes low memory, processing, and energy from current mobile electronic devices.","","","10.1109/TII.2018.2867174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445585","Deep learning;knowledge personalization and customization;recommendation system;sentiment analysis;social networks","Measurement;Machine learning;Monitoring;Stress;Sentiment analysis;Informatics;Ontologies","knowledge based systems;learning (artificial intelligence);ontologies (artificial intelligence);psychology;recommender systems;recurrent neural nets;sentiment analysis;social networking (online)","knowledge-based recommendation system;KBRS;emotional health monitoring system;sentiment analysis;depression disturbance;depressive content;stressful content;convolutional neural network;recurrent neural networks;online social networks;psychological disturbances;deep learning;users opinion;ontologies analysis;send warning messages;bidirectional long short-term memory;mobile electronic devices","","1","69","","","","","IEEE","IEEE Journals"
"Hierarchical Tracking by Reinforcement Learning-Based Searching and Coarse-to-Fine Verifying","B. Zhong; B. Bai; J. Li; Y. Zhang; Y. Fu","Department of Computer Science and Engineering, Huaqiao University, Xiamen, China; Department of Computer Science and Engineering, Huaqiao University, Xiamen, China; Department of Electrical and Computer Engineering, College of Computer and Information Science, Northeastern University, Boston, MA, USA; Department of Electrical and Computer Engineering, College of Computer and Information Science, Northeastern University, Boston, MA, USA; Department of Electrical and Computer Engineering, College of Computer and Information Science, Northeastern University, Boston, MA, USA","IEEE Transactions on Image Processing","","2019","28","5","2331","2341","A class-agnostic tracker typically consists of three key components, i.e., its motion model, its target appearance model, and its updating strategy. However, most recent top-performing trackers mainly focus on constructing complicated appearance models and updating strategies, while using comparatively simple and heuristic motion models that may result in an inefficient search and degrade the tracking performance. To address this issue, we propose a hierarchical tracker that learns to move and track based on the combination of data-driven search at the coarse level and coarse-to-fine verification at the fine level. At the coarse level, a data-driven motion model learned from deep recurrent reinforcement learning provides our tracker with coarse localization of an object. By formulating motion search as an action-decision problem in reinforcement learning, our tracker utilizes a recurrent convolutional neural network-based deep Q-network to effectively learn data-driven searching policies. The learned motion model can not only significantly reduce the search space but also provide more reliable interested regions for further verifying. At the fine level, a kernelized correlation filter (KCF)-based appearance model is adopted to densely yet efficiently verify a local region centered on the predicted location from the motion model. Through use of circulant matrices and fast Fourier transformation, a large number of candidate samples in the local region can be efficiently and effectively evaluated by the KCF-based appearance model. Finally, a simple yet robust estimator is designed to analyze possible tracking failure. The experiments on OTB50 and OTB100 illustrate that our tracker achieves better performance than the state-of-the-art trackers.","","","10.1109/TIP.2018.2885238","National Natural Science Foundation of China; Natural Science Foundation of Fujian Province; Division of Information and Intelligent Systems; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8561254","Visual tracking;motion model;reinforcement learning;coarse-to-fine verification;correlation filter","Target tracking;Reinforcement learning;Correlation;Adaptation models;Visualization;Search problems","convolutional neural nets;correlation methods;fast Fourier transforms;image motion analysis;learning (artificial intelligence);object tracking;recurrent neural nets","coarse-to-fine verification;data-driven motion model;deep recurrent reinforcement learning;recurrent convolutional neural network-based deep Q-network;learned motion model;kernelized correlation filter-based appearance model;KCF-based appearance model;hierarchical tracking;reinforcement learning-based searching;object localization;action-decision problem;circulant matrices;fast Fourier transformation;visual tracking","","4","58","","","","","IEEE","IEEE Journals"
"Deep-Learning-Based Multi-Modal Fusion for Fast MR Reconstruction","L. Xiang; Y. Chen; W. Chang; Y. Zhan; W. Lin; Q. Wang; D. Shen","Institute for Medical Imaging Technology, School of Biomedical EngineeringShanghai Jiao Tong University; Department of Radiology and BRICUniversity of North Carolina at Chapel Hill; Department of Radiology and BRICUniversity of North Carolina at Chapel Hill; Institute for Medical Imaging Technology, School of Biomedical EngineeringShanghai Jiao Tong University; Department of Radiology and BRICUniversity of North Carolina at Chapel Hill; Institute for Medical Imaging Technology, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Radiology and BRICUniversity of North Carolina at Chapel Hill","IEEE Transactions on Biomedical Engineering","","2019","66","7","2105","2114","T1-weighted image (T1WI) and T2-weighted image (T2WI) are the two routinely acquired magnetic resonance (MR) modalities that can provide complementary information for clinical and research usages. However, the relatively long acquisition time makes the acquired image vulnerable to motion artifacts. To speed up the imaging process, various algorithms have been proposed to reconstruct high-quality images from under-sampled k-space data. However, most of the existing algorithms only rely on mono-modality acquisition for the image reconstruction. In this paper, we propose to combine complementary MR acquisitions (i.e., T1WI and under-sampled T2WI particularly) to reconstruct the high-quality image (i.e., corresponding to the fully sampled T2WI). To the best of our knowledge, this is the first work to fuse multi-modal MR acquisitions through deep learning to speed up the reconstruction of a certain target image. Specifically, we present a novel deep learning approach, namely Dense-Unet, to accomplish the reconstruction task. The proposed Dense-Unet requires fewer parameters and less computation, while achieving promising performance. Our results have shown that Dense-Unet can reconstruct a three-dimensional T2WI volume in less than 10 s with an under-sampling rate of 8 for the k-space and negligible aliasing artifacts or signal-noise-ratio loss. Experiments also demonstrate excellent transferring capability of Dense-Unet when applied to the datasets acquired by different MR scanners. The above-mentioned results imply great potential of our method in many clinical scenarios.","","","10.1109/TBME.2018.2883958","National Natural Science Foundation of China; China Scholarship Council; National Key R&D Program of China; Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8552399","Deep learning;dense block;fast MR reconstruction;multi-model fusion","Image reconstruction;Acceleration;Biomedical imaging;Lesions;Signal to noise ratio","biomedical MRI;image reconstruction;image sequences;learning (artificial intelligence);medical image processing","mono-modality acquisition;image reconstruction;complementary MR acquisitions;high-quality image;multimodal MR acquisitions;deep learning approach;reconstruction task;multimodal fusion;T1-weighted image;T2-weighted image;magnetic resonance modalities;imaging process;under-sampled k-space data","","","45","","","","","IEEE","IEEE Journals"
"Fast H.264 to HEVC Transcoding: A Deep Learning Method","J. Xu; M. Xu; Y. Wei; Z. Wang; Z. Guan","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China","IEEE Transactions on Multimedia","","2019","21","7","1633","1645","With the development of video coding technology, high-efficiency video coding (HEVC) has become a promising alternative, compared with the previous coding standards, for example, H.264. In general, H.264 to HEVC transcoding can be accomplished by fully H.264 decoding and fully HEVC encoding, which suffers from considerable time consumption on the brute-force search of the HEVC coding tree unit (CTU) partition for rate-distortion optimization (RDO). In this paper, we propose a deep learning method to predict the HEVC CTU partition, instead of the brute-force RDO search, for H.264 to HEVC transcoding. First, we build a large-scale H.264 to HEVC transcoding database. Second, we investigate the correlation between the HEVC CTU partition and H.264 features, and analyze both temporal and spatial-temporal similarities of the CTU partition across video frames. Third, we propose a deep learning architecture of a hierarchical long short-term memory (H-LSTM) network to predict the CTU partition of HEVC. Then, the brute-force RDO search of the CTU partition is replaced by the H-LSTM prediction such that the computational time can be significantly reduced for fast H.264 to HEVC transcoding. Finally, the experimental results verify that the proposed H-LSTM method can achieve a better tradeoff between coding efficiency and complexity, compared to the state-of-the-art H.264 to HEVC transcoding methods.","","","10.1109/TMM.2018.2885921","National Natural Science Foundation of China; Fok Ying Tung Education Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8570845","H.264;HEVC;transcoding;deep learning;LSTM","Transcoding;Databases;Feature extraction;Streaming media;Video coding","decoding;learning (artificial intelligence);optimisation;transcoding;video coding","brute-force RDO search;HEVC transcoding database;deep learning architecture;high-efficiency video coding;HEVC encoding;brute-force search;coding tree unit partition;CTU partition;spatial-temporal similarities;H-LSTM prediction","","","41","","","","","IEEE","IEEE Journals"
"Deep Learning From Noisy Image Labels With Quality Embedding","J. Yao; J. Wang; I. W. Tsang; Y. Zhang; J. Sun; C. Zhang; R. Zhang","Cooperative Medianet Innovation CenterShanghai Jiao Tong University; Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Center for Artificial Intelligence, FEIT, University of Technology Sydney, Ultimo, NSW, Australia; Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Center for Artificial Intelligence, FEIT, University of Technology Sydney, Ultimo, NSW, Australia; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Image Processing","","2019","28","4","1909","1922","There is an emerging trend to leverage noisy image datasets in many visual recognition tasks. However, the label noise among datasets severely degenerates the performance of deep learning approaches. Recently, one mainstream is to introduce the latent label to handle label noise, which has shown promising improvement in the network designs. Nevertheless, the mismatch between latent labels and noisy labels still affects the predictions in such methods. To address this issue, we propose a probabilistic model, which explicitly introduces an extra variable to represent the trustworthiness of noisy labels, termed as the quality variable. Our key idea is to identify the mismatch between the latent and noisy labels by embedding the quality variables into different subspaces, which effectively minimizes the influence of label noise. At the same time, reliable labels are still able to be applied for training. To instantiate the model, we further propose a contrastive-additive noise network (CAN), which consists of two important layers: 1) the contrastive layer that estimates the quality variable in the embedding space to reduce the influence of noisy labels and 2) the additive layer that aggregates the prior prediction and noisy labels as the posterior to train the classifier. Moreover, to tackle the challenges in optimization, we deduce an SGD algorithm with the reparameterization tricks, which makes our method scalable to big data. We validate the proposed method on a range of noisy image datasets. Comprehensive results have demonstrated that CAN outperforms the state-of-the-art deep learning approaches.","","","10.1109/TIP.2018.2877939","The High Technology Research and Development Program of China; National Natural Science Foundation of China; STCSM; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8506425","Deep learning;noisy image labels;quality embedding","Noise measurement;Training;Estimation;Image recognition;Predictive models;Optimization","Big Data;gradient methods;image denoising;image recognition;learning (artificial intelligence);optimisation;probability","contrastive-additive noise network;noisy image labels;Big Data;SGD algorithm;optimization;CAN;probabilistic model;network designs;deep learning approaches;visual recognition tasks;quality embedding","","1","51","","","","","IEEE","IEEE Journals"
"Graph-Regularized Locality-Constrained Joint Dictionary and Residual Learning for Face Sketch Synthesis","J. Jiang; Y. Yu; Z. Wang; X. Liu; J. Ma","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Digital Content and Media Sciences Research Division, National Institute of Informatics, Tokyo, Japan; Digital Content and Media Sciences Research Division, National Institute of Informatics, Tokyo, Japan; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Digital Content and Media Sciences Research Division, National Institute of Informatics, Tokyo, Japan","IEEE Transactions on Image Processing","","2019","28","2","628","641","Face sketch synthesis is a crucial issue in digital entertainment and law enforcement. It can bridge the considerable texture discrepancy between face photos and sketches. Most of the current face sketch synthesis approaches directly to learn the relationship between the photos and sketches, and it is very difficult for them to generate the individual specific features, which we call rare characteristics. In this paper, we propose a novel face sketch synthesis approach through residual learning. In contrast to traditional approaches, which aim to reconstruct a sketch image directly (i.e., learn the mapping relationship between the photo and sketch), we aim to predict the residual image by learning the mapping relationship between the photo and residual, i.e., the difference between the photo and sketch, given an observed photo. This technique will render optimizing the residual mapping easier than optimizing the original mapping and deriving rare characteristic information. We also introduce a joint dictionary learning algorithm by preserving the local geometry structure of a data space. Through the learned joint dictionary, we transform the face sketch synthesis from an image space to a new and compact space; the new and compact space is spanned by learned dictionary atoms, where the manifold assumption can be further guaranteed. Results show that the proposed method demonstrates an impressive performance in the face sketch synthesis task on three public face sketch datasets and various real-world photos. These results are derived by comparing the proposed method with several state-of-the-art techniques, including certain recently proposed deep learning-based approaches.","","","10.1109/TIP.2018.2870936","National Natural Science Foundation of China; JSPS KAKENHI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8467342","Face sketch synthesis;residual learning;joint dictionary learning;local geometry structure;rare characteristics","Face;Training;Dictionaries;Manifolds;Machine learning;Geometry;Transforms","face recognition;graph theory;image representation;learning (artificial intelligence)","residual learning;sketch image;photo;public face sketch datasets;face photos;face sketch synthesis;dictionary learning algorithm;deep learning-based approaches","","3","60","","","","","IEEE","IEEE Journals"
"Unsupervised Clustering of Seismic Signals Using Deep Convolutional Autoencoders","S. M. Mousavi; W. Zhu; W. Ellsworth; G. Beroza","Department of Geophysics, Stanford University, Stanford, CA, USA; Department of Geophysics, Stanford University, Stanford, CA, USA; Department of Geophysics, Stanford University, Stanford, CA, USA; Department of Geophysics, Stanford University, Stanford, CA, USA","IEEE Geoscience and Remote Sensing Letters","","2019","16","11","1693","1697","In this letter, we use deep neural networks for unsupervised clustering of seismic data. We perform the clustering in a feature space that is simultaneously optimized with the clustering assignment, resulting in learned feature representations that are effective for a specific clustering task. To demonstrate the application of this method in seismic signal processing, we design two different neural networks consisting primarily of full convolutional and pooling layers and apply them to: 1) discriminate waveforms recorded at different hypocentral distances and 2) discriminate waveforms with different first-motion polarities. Our method results in precisions that are comparable to those recently achieved by supervised methods, but without the need for labeled data, manual feature engineering, and large training sets. The applications we present here can be used in standard single-site earthquake early warning systems to reduce the false alerts on an individual station level. However, the presented technique is general and suitable for a variety of applications including quality control of the labeling and classification results of other supervised methods.","","","10.1109/LGRS.2019.2909218","Stanford Center for Induced and Triggered Seismicity; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704258","Clustering;deep learning;neural networks;seismic signal;unsupervised learning;waveform discrimination","Convolution;Earthquakes;Neural networks;Training;Decoding;Unsupervised learning;Signal processing algorithms","convolutional neural nets;earthquakes;feature extraction;geophysical signal processing;pattern clustering;quality control;seismology;signal classification;unsupervised learning","feature representations;clustering task;seismic signal processing;convolutional layers;pooling layers;hypocentral distances;supervised methods;feature engineering;unsupervised clustering;seismic signals;deep convolutional autoencoders;deep neural networks;seismic data;feature space;clustering assignment;single-site earthquake early warning systems;quality control","","1","22","","","","","IEEE","IEEE Journals"
"Joint Classification and Regression via Deep Multi-Task Multi-Channel Learning for Alzheimer's Disease Diagnosis","M. Liu; J. Zhang; E. Adeli; D. Shen","University of North Carolina at Chapel Hill, Chapel Hill; University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Biomedical Engineering","","2019","66","5","1195","1206","In the field of computer-aided Alzheimer's disease (AD) diagnosis, jointly identifying brain diseases and predicting clinical scores using magnetic resonance imaging (MRI) have attracted increasing attention since these two tasks are highly correlated. Most of existing joint learning approaches require hand-crafted feature representations for MR images. Since hand-crafted features of MRI and classification/regression models may not coordinate well with each other, conventional methods may lead to sub-optimal learning performance. Also, demographic information (e.g., age, gender, and education) of subjects may also be related to brain status, and thus can help improve the diagnostic performance. However, conventional joint learning methods seldom incorporate such demographic information into the learning models. To this end, we propose a deep multi-task multi-channel learning (DM2L) framework for simultaneous brain disease classification and clinical score regression, using MRI data and demographic information of subjects. Specifically, we first identify the discriminative anatomical landmarks from MR images in a data-driven manner, and then extract multiple image patches around these detected landmarks. We then propose a deep multi-task multi-channel convolutional neural network for joint classification and regression. Our DM2L framework can not only automatically learn discriminative features for MR images, but also explicitly incorporate the demographic information of subjects into the learning process. We evaluate the proposed method on four large multi-center cohorts with 1984 subjects, and the experimental results demonstrate that DM2 L is superior to several state-of-the-art joint learning methods in both the tasks of disease classification and clinical score regression.","","","10.1109/TBME.2018.2869989","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463559","Anatomical landmark;brain disease diagnosis;classification;convolutional neural network (CNN);regression","Task analysis;Feature extraction;Magnetic resonance imaging;Dementia;Convolutional neural networks;Education","biomedical MRI;brain;diseases;feature extraction;image classification;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;regression analysis","joint classification;deep multitask multichannel learning;brain diseases;magnetic resonance imaging;joint learning approaches;hand-crafted feature representations;MR images;demographic information;learning models;simultaneous brain disease classification;clinical score regression;learning process;joint learning methods;computer-aided Alzheimer disease;suboptimal learning performance","","","57","","","","","IEEE","IEEE Journals"
"Learning Navigation Behaviors End-to-End With AutoRL","H. L. Chiang; A. Faust; M. Fiser; A. Francis","Robotics at Google, Google AI, Mountain View, CA, USA; Robotics at Google, Google AI, Mountain View, CA, USA; Robotics at Google, Google AI, Mountain View, CA, USA; Robotics at Google, Google AI, Mountain View, CA, USA","IEEE Robotics and Automation Letters","","2019","4","2","2007","2014","We learn end-to-end point-to-point and pathfollowing navigation behaviors that avoid moving obstacles. These policies receive noisy lidar observations and output robot linear and angular velocities. The policies are trained in small, static environments with AutoRL, an evolutionary automation layer around reinforcement learning (RL) that searches for a deep RL reward and neural network architecture with large-scale hyper-parameter optimization. AutoRL first finds a reward that maximizes task completion and then finds a neural network architecture that maximizes the cumulative of the found reward. Empirical evaluations, both in simulation and on-robot, show that AutoRL policies do not suffer from the catastrophic forgetfulness that plagues many other deep reinforcement learning algorithms, generalize to new environments and moving obstacles, are robust to sensor, actuator, and localization noise, and can serve as robust building blocks for larger navigation tasks. Our path-following and point-to-point policies are, respectively, 23% and 26% more successful than comparison methods across new environments.","","","10.1109/LRA.2019.2899918","Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643443","Autonomous agents;collision avoidance;deep learning in robotics and automation;motion and path planning","Navigation;Robot sensing systems;Task analysis;Collision avoidance;Training;Reinforcement learning","collision avoidance;evolutionary computation;learning (artificial intelligence);mobile robots;neural nets;optical radar;optimisation","deep RL reward;neural network architecture;large-scale hyper-parameter optimization;task completion;AutoRL policies;moving obstacles;larger navigation tasks;point-to-point policies;navigation behaviors end-to-end;end-to-end point-to-point;pathfollowing navigation behaviors;noisy lidar observations;output robot linear;angular velocities;static environments;evolutionary automation layer;deep reinforcement learning algorithms","","1","38","","","","","IEEE","IEEE Journals"
"Real-Time Deep Pose Estimation With Geodesic Loss for Image-to-Template Rigid Registration","S. S. Mohseni Salehi; S. Khan; D. Erdogmus; A. Gholipour","Radiology Department, Boston Children’s Hospital, Boston, MA, USA; Radiology Department, Boston Children’s Hospital, Boston, MA, USA; Electrical and Computer Engineering Department, Northeastern University, Boston, MA, USA; Radiology Department, Boston Children’s Hospital, Boston, MA, USA","IEEE Transactions on Medical Imaging","","2019","38","2","470","481","With an aim to increase the capture range and accelerate the performance of state-of-the-art inter-subject and subject-to-template 3-D rigid registration, we propose deep learning-based methods that are trained to find the 3-D position of arbitrarily-oriented subjects or anatomy in a canonical space based on slices or volumes of medical images. For this, we propose regression convolutional neural networks (CNNs) that learn to predict the angle-axis representation of 3-D rotations and translations using image features. We use and compare mean square error and geodesic loss to train regression CNNs for 3-D pose estimation used in two different scenarios: slice-to-volume registration and volume-to-volume registration. As an exemplary application, we applied the proposed methods to register arbitrarily oriented reconstructed images of fetuses scanned in-utero at a wide gestational age range to a standard atlas space. Our results show that in such registration applications that are amendable to learning, the proposed deep learning methods with geodesic loss minimization achieved 3-D pose estimation with a wide capture range in real-time (<;100ms). We also tested the generalization capability of the trained CNNs on an expanded age range and on images of newborn subjects with similar and different MR image contrasts. We trained our models on T2-weighted fetal brain MRI scans and used them to predict the 3-D pose of newborn brains based on T1-weighted MRI scans. We showed that the trained models generalized well for the new domain when we performed image contrast transfer through a conditional generative adversarial network. This indicates that the domain of application of the trained deep regression CNNs can be further expanded to image modalities and contrasts other than those used in training. A combination of our proposed methods with accelerated optimization-based registration algorithms can dramatically enhance the performance of automatic imaging devices and image processing methods of the future.","","","10.1109/TMI.2018.2866442","McKnight Foundation; National Institute of Biomedical Imaging and Bioengineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8443391","Image registration;pose estimation;deep learning;convolutional neural network;CNN;MRI;fetal MRI","Three-dimensional displays;Pose estimation;Magnetic resonance imaging;Solid modeling;Real-time systems;Biomedical imaging;Predictive models","biomedical MRI;convolutional neural nets;image classification;image reconstruction;image registration;image segmentation;learning (artificial intelligence);mean square error methods;medical image processing;optimisation;pose estimation;regression analysis","image features;mean square error;3-D pose estimation;slice-to-volume registration;volume-to-volume registration;standard atlas space;geodesic loss minimization;newborn subjects;T2-weighted fetal brain MRI scans;T1-weighted MRI scans;image contrast transfer;image modalities;automatic imaging devices;image processing methods;image-to-template rigid registration;deep learning-based methods;3-D position;arbitrarily-oriented subjects;canonical space;medical images;regression convolutional neural networks;angle-axis representation;real-time deep pose estimation;3-D rotations;3-D translations;images reconstruction;conditional generative adversarial network;optimization-based registration algorithms;subject-to-template 3-D rigid registration;MR image;deep regression CNNs training","","","44","","","","","IEEE","IEEE Journals"
"DeepLab-Based Spatial Feature Extraction for Hyperspectral Image Classification","Z. Niu; W. Liu; J. Zhao; G. Jiang","School of Science, Yanshan University, Qinhuangdao, China; School of Science, Yanshan University, Qinhuangdao, China; School of Mechanical Engineering, Yanshan University, Qinhuangdao, China; School of Electrical Engineering, Yanshan University, Qinhuangdao, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","2","251","255","Recently, deep learning has been used for hyperspectral image classification (HSIC) due to its powerful feature learning and classification ability. In this letter, a novel deep learning-based framework based on DeepLab is proposed for HSIC. Inspired by the excellent performance of DeepLab in semantic segmentation, the proposed framework applies DeepLab to excavate spatial features of the hyperspectral image (HSI) pixel to pixel. It breaks through the limitation of patch-wise feature learning in the most of existing deep learning methods used in HSIC. More importantly, it can extract features at multiple scales and effectively avoid the reduction of spatial resolution. Furthermore, to improve the HSIC performance, the spatial features extracted by DeepLab and the spectral features are fused by a weighted fusion method, then the fused features are input into support vector machine for final classification. Experimental results on two public HSI data sets demonstrate that the proposed framework outperformed the traditional methods and the existing deep learning-based methods, especially for small-scale classes.","","","10.1109/LGRS.2018.2871507","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8482250","Convolutional neural network (CNN);deep learning;DeepLab;features fusion;hyperspectral image classification (HSIC)","Feature extraction;Convolution;Support vector machines;Training;Semantics;Hyperspectral imaging;Machine learning","feature extraction;image classification;image segmentation;learning (artificial intelligence);support vector machines","feature learning;fused features;spectral features;HSIC performance;spatial resolution;deep learning methods;patch-wise feature;hyperspectral image pixel;spatial features;deep learning-based framework;classification ability;hyperspectral image classification;DeepLab-based spatial feature extraction","","","20","","","","","IEEE","IEEE Journals"
"Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning","T. Miyato; S. Maeda; M. Koyama; S. Ishii","Preferred Networks, Inc., Tokyo, Japan; Preferred Networks, Inc., Tokyo, Japan; Department of Mathematical Science, Ritsumeikan University, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Kyoto, Japan","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","8","1979","1993","We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is defined as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method defines the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only “virtually” adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward-and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.","","","10.1109/TPAMI.2018.2858821","New Energy and Industrial Technology Development Organization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417973","Semi-supervised learning;supervised learning;robustness;adversarial training;adversarial examples;deep learning","Training;Perturbation methods;Artificial neural networks;Semisupervised learning;Data models;Computational modeling;Robustness","approximation theory;backpropagation;gradient methods;image classification;minimum entropy methods;neural nets;supervised learning","virtual adversarial loss;VAT;semisupervised learning tasks;image classification;entropy minimization principle;CIFAR-10;SVHN;forward-and back-propagations;approximated gradient;neural networks;supervised learning;virtual adversarial training;conditional label distribution;regularization method","","8","49","","","","","IEEE","IEEE Journals"
"A New Adaptive Type-II Fuzzy-Based Deep Reinforcement Learning Control: Fuel Cell Air-Feed Sensors Control","M. Gheisarnejad; J. Boudjadar; M. Khooban","Department of Electrical Engineering, Najafabad Branch, Islamic Azad University, Isfahan, Iran; Department of Engineering, Aarhus University, Aarhus, Denmark; Department of Engineering, Aarhus University, Aarhus, Denmark","IEEE Sensors Journal","","2019","19","20","9081","9089","This paper proposes a new adaptive controller for air-feed on proton exchange membrane fuel cell (PEMFC) plants. The oxygen excess ratio is first regulated by a single input interval type-2 fuzzy PI (SIT2-FPI) controller during the current variation of the PEMFC system. Then, a deep deterministic policy gradient (DDPG) algorithm is adopted to adaptively adjust the baseline PI coefficients of the SIT2 controller by taking the benefits of the online learning and model-free features of reinforcement learning. In the DDPG structure, an actor-network determines the policy signals, while a critic-network evaluates the quality of the policy provided by the actor. The suggested DDPG algorithm naturally takes into account the baseline PI coefficients into the design objective and offers the SIT2-FPI structure with online coefficient adjusting ability through learning. Based on reward feedback of the oxygen excess ratio error, the weights of the actor and critic networks are updated by the gradient descent technique. Detailed real-time model-in-the loop (MIL) simulation outcomes and comparative analysis are presented to confirm the adaptation capability of the DDPG-based online SIT2-FPI coefficient tuning strategy.","","","10.1109/JSEN.2019.2924726","Energy Technology Development and Demonstration Program (EUDP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744606","Proton exchange membrane fuel cell (PEMFC);oxygen excess ratio;a single input interval type-2 fuzzy PI (SIT2-FPI);deep deterministic policy gradient (DDPG)","Adaptation models;Fuel cells;Atmospheric modeling;Cathodes;Reinforcement learning;Tuning;Hydrogen","adaptive control;control nonlinearities;control system analysis;control system synthesis;fuzzy control;fuzzy set theory;gradient methods;learning (artificial intelligence);PD control;PI control;proton exchange membrane fuel cells","fuel cell air-feed sensors control;proton exchange membrane fuel cell plants;PEMFC system;deep deterministic policy gradient algorithm;SIT2 controller;model-free features;DDPG structure;actor-network;critic-network;SIT2-FPI structure;oxygen excess ratio error;reinforcement learning control;single input interval type-2 fuzzy PI controller;real-time model-in-the loop simulation;DDPG-based online SIT2-FPI coefficient tuning strategy;adaptive type-II fuzzy controller","","1","27","","","","","IEEE","IEEE Journals"
"An Optimized Deep Network Representation of Multimutation Differential Evolution and its Application in Seismic Inversion","Z. Gao; Z. Pan; C. Zuo; J. Gao; Z. Xu","National Engineering Laboratory for Offshore Oil Exploration, Xi’an Jiaotong University, Xi’an, China; School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China; School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China; National Engineering Laboratory for Offshore Oil Exploration, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","7","4720","4734","Seismic inversion problems are well-known to be nonlinear and their misfit functions often involve many local minima. Global optimization methods are capable of converging to the global minimum of a misfit function, thus, they are promising in seismic inversion. As a global optimization method, multimutation differential evolution (MMDE) has been proven to be effective in solving high-dimensional seismic inversion problems. However, it is challenging to choose the optimal parameters for MMDE to achieve the best performance in seismic inversion. In this paper, we propose a new deep network based on MMDE and name it as MMDE-Net, which enables us to learn the optimal parameters by using a network training procedure rather than empirically choosing them. Benefiting from the learned parameters, MMDE-Net has advantages over MMDE in applications. Numerical examples based on synthetic and field data set clearly indicate that MMDE-Net can provide faster convergence speed and better inversion result than conventional methods in seismic inversion.","","","10.1109/TGRS.2019.2892567","National Natural Science Foundation of China; National Postdoctoral Program for Innovative Talents; Major Projects of the National Natural Science Foundation of China; National Science and Technology Major Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8637964","Deep learning;differential evolution (DE);global optimization method;seismic inversion","Optimization methods;Data models;Mathematical model;Training;Convergence;Deep learning","evolutionary computation;geophysics computing;inverse problems;learning (artificial intelligence);optimisation;seismology","global optimization method;misfit function;multimutation differential evolution;high-dimensional seismic inversion problems;MMDE-Net;optimized deep network representation","","","40","","","","","IEEE","IEEE Journals"
"Deep Visual Saliency on Stereoscopic Images","A. Nguyen; J. Kim; H. Oh; H. Kim; W. Lin; S. Lee","Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; Microsoft Research Asia, Beijing, China; Electronics and Telecommunications Research Institute, Daejeon, South Korea; Korea Institute of Science and Technology, Seoul, South Korea; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Image Processing","","2019","28","4","1939","1953","Visual saliency on stereoscopic 3D (S3D) images has been shown to be heavily influenced by image quality. Hence, this dependency is an important factor in image quality prediction, image restoration and discomfort reduction, but it is still very difficult to predict such a nonlinear relation in images. In addition, most algorithms specialized in detecting visual saliency on pristine images may unsurprisingly fail when facing distorted images. In this paper, we investigate a deep learning scheme named Deep Visual Saliency (DeepVS) to achieve a more accurate and reliable saliency predictor even in the presence of distortions. Since visual saliency is influenced by low-level features (contrast, luminance, and depth information) from a psychophysical point of view, we propose seven low-level features derived from S3D image pairs and utilize them in the context of deep learning to detect visual attention adaptively to human perception. During analysis, it turns out that the low-level features play a role to extract distortion and saliency information. To construct saliency predictors, we weight and model the human visual saliency through two different network architectures, a regression and a fully convolutional neural networks. Our results from thorough experiments confirm that the predicted saliency maps are up to 70% correlated with human gaze patterns, which emphasize the need for the hand-crafted features as input to deep neural networks in S3D saliency detection.","","","10.1109/TIP.2018.2879408","Samsung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520871","Saliency prediction;stereoscopic image;distorted image;convolutional neural network;deep learning","Feature extraction;Visualization;Distortion;Saliency detection;Two dimensional displays;Image color analysis","convolution;feature extraction;feedforward neural nets;image restoration;learning (artificial intelligence);neural net architecture;regression analysis;stereo image processing;visual perception","Deep Visual Saliency;S3D image pairs;saliency information;deep neural networks;S3D saliency detection;stereoscopic 3D images;image quality prediction;image restoration;distorted images;deep learning;distortion information;visual saliency prediction;regression network architecture;convolutional neural network architecture","","1","55","","","","","IEEE","IEEE Journals"
"Deep Learning for Spectrum Sensing","J. Gao; X. Yi; C. Zhong; X. Chen; Z. Zhang","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Wireless Communications Letters","","2019","8","6","1727","1730","In cognitive radio systems, the ability to accurately detect primary user’s signal is essential to secondary user in order to utilize idle licensed spectrum. Conventional energy detector is a good choice for blind signal detection, while it suffers from the well-known SNR-wall due to noise uncertainty. In this letter, we firstly propose a deep learning based signal detector which exploits the underlying structural information of the modulated signals, and is shown to achieve the state of the art detection performance, requiring no prior knowledge about channel state information or background noise. In addition, the impacts of modulation scheme and sample length on performance are investigated. Finally, a deep learning based cooperative detection system is proposed, which is shown to provide substantial performance gain over conventional cooperative sensing methods.","","","10.1109/LWC.2019.2939314","National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824091","Spectrum sensing;SNR-wall;deep learning;cooperative detection","Detectors;Modulation;Training;Signal to noise ratio;Deep learning;Uncertainty","","","","","17","IEEE","","","","IEEE","IEEE Journals"
"Deep Network Ensembles for Aerial Scene Classification","M. A. Dede; E. Aptoula; Y. Genc","Gebze Technical University, Kocaeli, Turkey; Gebze Technical University, Kocaeli, Turkey; Gebze Technical University, Kocaeli, Turkey","IEEE Geoscience and Remote Sensing Letters","","2019","16","5","732","735","It is well known in the machine learning community that the ensembles of neural networks outperform the respective individual networks; hence recent work on aerial scene classification has been trending toward various network fusion strategies. However, training multiple deep networks can be computationally expensive. “Snapshot ensembling” has recently appeared as an alternative and claims to provide the performance of network ensembles at the cost of a single network's training. In this letter, we present the results of a comparative study on deep network ensembles in the context of aerial scene classification, where homogeneous, heterogeneous, and snapshot ensembling strategies are explored with both DenseNet and Inception networks; contrary to the existing work, we employ model fusion at the last convolutional layer's level. The explored approaches are validated with the two largest and most challenging data sets available (AID and NWPU-RESISC45), and state-of-the-art results are achieved on both.","","","10.1109/LGRS.2018.2880136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550682","Aerial scene classification;convolutional neural network (CNN);DenseNet;ensemble learning;remote sensing","Training;Computer architecture;Neural networks;Computational modeling;Network architecture;Annealing;Data models","convolutional neural nets;image classification;learning (artificial intelligence)","aerial scene classification;network fusion strategies;deep network ensembles;snapshot ensembling strategies;neural networks;machine learning;DenseNet;inception networks;convolutional layer","","","17","","","","","IEEE","IEEE Journals"
"Vehicle Driving Behavior Recognition Based on Multi-View Convolutional Neural Network With Joint Data Augmentation","Y. Zhang; J. Li; Y. Guo; C. Xu; J. Bao; Y. Song","School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Intel China Ltd., Beijing, China","IEEE Transactions on Vehicular Technology","","2019","68","5","4223","4234","This paper proposes a method for vehicle driving behavior recognition based on a six-axis motion processor. This method uses deep-learning technology to learn the sample data collected by the on-board sensor. To solve the problem of small sample size and easy overfitting, we propose a joint data augmentation (JDA) scheme, and design a new multi-view convolutional neural network model (MV-CNN). The JDA includes the multi-axis weighted fusion algorithm, background noise fusion algorithm, and random cropping algorithm to construct a sample dataset that is more in line with a complex real driving environment. With the CNN model, the direction of information propagation improved, and a new MV-CNN model was developed for the training, learning, and recognition of driving behavior. The performance of MV-CNN is experimentally compared with CNN, recurrent neural networks (RNN), LSTM, CNN+LSTM, and three-dimensional CNN. The results show that MV-CNN can obtain the best recall, precision, and F1-score. At the same time, MV-CNN and JDA have better generalization ability, reduce the training variance and deviation, and increase the stability of the model training process.","","","10.1109/TVT.2019.2903110","National Natural Science Foundation of China; Fab. X Artificial Intelligence Research Center, Beijing, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660472","driving behavior recognition;data augmentation;deep neural network;deep learning;Internet of Things","Neural networks;Feature extraction;Hidden Markov models;Deep learning;Training;Vehicles;Roads","behavioural sciences computing;convolutional neural nets;driver information systems;learning (artificial intelligence);recurrent neural nets;sensor fusion","six-axis motion processor;joint data augmentation scheme;JDA;multiview convolutional neural network model;multiaxis weighted fusion algorithm;random cropping algorithm;MV-CNN model;vehicle driving behavior;deep-learning;on-board sensor;information propagation;recurrent neural networks;RNN;CNN+LSTM;sample data;background noise fusion algorithm","","","25","","","","","IEEE","IEEE Journals"
"Enhance Visual Recognition Under Adverse Conditions via Deep Networks","D. Liu; B. Cheng; Z. Wang; H. Zhang; T. S. Huang","Department of Electrical and Computer Engineering and Beckman Institute, University of Illinois at Urbana–Champaign, Urbana, IL, USA; Department of Electrical and Computer Engineering and Beckman Institute, University of Illinois at Urbana–Champaign, Urbana, IL, USA; Department of Computer Science and Engineering, Texas A&M University, TX, USA; Baidu Research, Sunnyvale, CA, USA; Department of Electrical and Computer Engineering and Beckman Institute, University of Illinois at Urbana–Champaign, Urbana, IL, USA","IEEE Transactions on Image Processing","","2019","28","9","4401","4412","Visual recognition under adverse conditions is a very important and challenging problem of high practical value, due to the ubiquitous existence of quality distortions during image acquisition, transmission, or storage. While deep neural networks have been extensively exploited in the techniques of low-quality image restoration and high-quality image recognition tasks, respectively, few studies have been done on the important problem of recognition from very low-quality images. This paper proposes a deep learning-based framework for improving the performance of image and video recognition models under adverse conditions, using robust adverse pre-training or its aggressive variant. The robust adverse pre-training algorithms leverage the power of pre-training and generalize the conventional unsupervised pre-training and data augmentation methods. We further develop a transfer learning approach to cope with real-world datasets of unknown adverse conditions. The proposed framework is comprehensively evaluated on a number of image and video recognition benchmarks, and obtains significant performance improvements under various single or mixed adverse conditions. Our visualization and analysis further add to the explainability of the results.","","","10.1109/TIP.2019.2908802","U.S. Army; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8678738","Deep learning;neural network;image recognition","Image recognition;Face recognition;Visualization;Task analysis;Degradation;Image restoration;Image reconstruction","image recognition;image restoration;learning (artificial intelligence);neural nets;performance evaluation;video signal processing","quality distortions;deep neural networks;low-quality image restoration;high-quality image recognition tasks;deep learning-based framework;video recognition models;video recognition benchmarks;unsupervised pre-training;very low-quality image recognition;performance improvement;robust adverse pre-training algorithms;data augmentation;transfer learning;visual recognition enhancement;mixed adverse conditions","","","48","","","","","IEEE","IEEE Journals"
"Sensor Transfer: Learning Optimal Sensor Effect Image Augmentation for Sim-to-Real Domain Adaptation","A. Carlson; K. A. Skinner; R. Vasudevan; M. Johnson-Roberson","Robotics Institute, University of Michigan, Ann Arbor, USA; Robotics Institute, University of Michigan, Ann Arbor, USA; Department of Mechanical Engineering, University of Michigan, Ann Arbor, USA; Department of Naval Architecture and Marine Engineering, University of Michigan, Ann Arbor, USA","IEEE Robotics and Automation Letters","","2019","4","3","2431","2438","Performance on benchmark datasets has drastically improved with advances in deep learning. Still, cross-dataset generalization performance remains relatively low due to the domain shift that can occur between two different datasets. This domain shift is especially exaggerated between synthetic and real datasets. Significant research has been done to reduce this gap, specifically via modeling variation in the spatial layout of a scene, such as occlusions, and scene environmental factors, such as time of day and weather effects. However, few works have addressed modeling the variation in the sensor domain as a means of reducing the synthetic to real domain gap. The camera or sensor used to capture a dataset introduces artifacts into the image data, which are unique to the sensor model, suggesting that sensor effects may also contribute to domain shift. To address this, we propose a learned augmentation network composed of physically-based augmentation functions. Our proposed augmentation pipeline transfers specific effects of the sensor model-chromatic aberration, blur, exposure, noise, and color temperature-from a real dataset to a synthetic dataset. We provide experiments which demonstrate that augmenting synthetic training datasets with the proposed learned augmentation framework reduces the domain gap between synthetic and real domains for object detection in urban driving scenes.","","","10.1109/LRA.2019.2896470","Ford Motor Company via the Ford-UM Alliance; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630027","Deep Learning in Robotics and Automation;Visual Learning;Computer Vision for Other Robotic Applications;Simulation and Animation","Robot sensing systems;Visualization;Task analysis;Pipelines;Training;Image color analysis;Rendering (computer graphics)","cameras;computer vision;image colour analysis;image restoration;image sensors;learning (artificial intelligence);object detection","cross-dataset generalization performance;domain shift;scene environmental factors;weather effects;sensor domain;image data;learned augmentation network;synthetic training datasets;learned augmentation framework;sensor transfer;optimal sensor effect image augmentation;sim-to-real domain adaptation;benchmark datasets;deep learning;chromatic aberration;spatial layout;sensor model;physically-based augmentation functions;color temperature;object detection;urban driving scenes;camera","","","43","","","","","IEEE","IEEE Journals"
"Weakly Supervised Biomedical Image Segmentation by Reiterative Learning","Q. Liang; Y. Nan; G. Coppola; K. Zou; W. Sun; D. Zhang; Y. Wang; G. Yu","College of Electrical and Information Engineering, Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, National Engineering Laboratory for Robot Vision Perception and Control, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, National Engineering Laboratory for Robot Vision Perception and Control, Hunan University, Changsha, China; Faculty of Engineering and Applied Science, University of Ontario Institute of Technology, Oshawa, ON, Canada; College of Electrical and Information Engineering, Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, National Engineering Laboratory for Robot Vision Perception and Control, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, National Engineering Laboratory for Robot Vision Perception and Control, Hunan University, Changsha, China; Department of Mechanical Engineering, York University, Toronto, ON, Canada; College of Electrical and Information Engineering, Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, National Engineering Laboratory for Robot Vision Perception and Control, Hunan University, Changsha, China; Department of Oncology, Longhua Hospital affiliated to Shanghai University of Traditional Chinese Medicine, Shanghai, China","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","1205","1214","Recent advances in deep learning have produced encouraging results for biomedical image segmentation; however, outcomes rely heavily on comprehensive annotation. In this paper, we propose a neural network architecture and a new algorithm, known as overlapped region forecast, for the automatic segmentation of gastric cancer images. To the best of our knowledge, this report for the first time describes that deep learning has been applied to the segmentation of gastric cancer images. Moreover, a reiterative learning framework that achieves superior performance without pretraining or further manual annotation is presented to train a simple network on weakly annotated biomedical images. We customize the loss function to make the model converge faster while avoiding becoming trapped in local minima. Patch boundary errors were eliminated by our overlapped region forecast algorithm. By studying the characteristics of the model trained using two different patch extraction methods, we train iteratively and integrate predictions and weak annotations to improve the quality of the training data. Using these methods, a mean Intersection over Union coefficient of 0.883 and a mean accuracy of 91.09% were achieved on the partially labeled dataset, thereby securing a win in the 2017 China Big Data and Artificial Intelligence Innovation and Entrepreneurship Competition.","","","10.1109/JBHI.2018.2850040","National Natural Science Foundation of China; Hunan Provincial Natural Science Foundation of China; Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8394987","Reiterative learning;gastric histopathology;deep neural networks;biomedical image segmentation","Image segmentation;Biomedical imaging;Cancer;Training;Biological system modeling;Machine learning;Object segmentation","cancer;image segmentation;iterative methods;medical image processing;neural nets;supervised learning","weakly supervised biomedical image segmentation;deep learning;neural network architecture;gastric cancer images;reiterative learning framework;patch boundary errors;overlapped region forecast algorithm;weak annotations;patch extraction methods;local minima","","","43","","","","","IEEE","IEEE Journals"
"Deep Learning Based on 1-D Ensemble Networks Using ECG for Real-Time User Recognition","M. Kim; S. B. Pan","Department of Control and Instrumentation Engineering Chosun University, Gwangju, Korea; Department of Electronics Engineering Chosun University, Gwangju, Korea","IEEE Transactions on Industrial Informatics","","2019","15","10","5656","5663","The postmobile era will go beyond using individual smart devices and allow for user interaction by connecting various devices with sensing capabilities, such as smartphones, wearable devices, automobiles, and the Internet of Things. Wearable devices can continuously collect a variety of information on the users and their environment as the devices are worn in daily life. Because of this, real-time big data analysis technology is needed. This paper proposes a deep learning-based ensemble network model for improving the performance and overcoming the problems, which can occur on a single network. This model is designed so that the features produced by n number of single networks are combined and relearned. In addition, different parameter values are used on each single network, and the data used in the experiments are generated by the fiducial point method, which uses feature point detection, and the nonfiducial point method for periods of 1 sec and n sec. In the experiment results, in the case of fiducial point-based ECG signals, the ensemble network recognition performance shows a maximum of 0.8% higher accuracy than that of the single network. In the case of a 1 sec period nonfiducial point-based ECG signal, the ensemble network recognition performance is a minimum of 0.4% and a maximum of 1% higher than that of the single network. In the case of an n sec period, there is a maximum difference of 1.3%, and the proposed ensemble network shows better performance than the single network.","","","10.1109/TII.2019.2909730","Basic Science Research Program; National Research Foundation of Korea; National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8684221","Biometrics;convolutional neural networks (CNNs);electrocardiogram (ECG);ensemble networks;user recognition","Electrocardiography;Feature extraction;Deep learning;Real-time systems;Neural networks;Face recognition","Big Data;convolutional neural nets;data analysis;electrocardiography;feature extraction;learning (artificial intelligence);medical signal processing;wearable computers","1-D ensemble networks;real-time user recognition;wearable devices;deep learning-based ensemble network model;single network;fiducial point-based ECG signals;1 sec period nonfiducial point-based ECG signal;Big Data analysis technology;feature point detection;convolutional neural networks","","1","30","Traditional","","","","IEEE","IEEE Journals"
"Corrections to “Recognizing Global Reservoirs From Landsat 8 Images: A Deep Learning Approach” [Sep 19 3168-3177]","W. Fang; C. Wang; X. Chen; W. Wan; H. Li; S. Zhu; Y. Fang; B. Liu; Y. Hong","Institute of Remote Sensing and GIS, School of Earth and Space Sciences, Peking University, Beijing, China; Department of Hydraulic Engineering, Tsinghua University, Beijing, China; Institute of Remote Sensing and GIS, School of Earth and Space Sciences, Peking University, Beijing, China; Institute of Remote Sensing and GIS, School of Earth and Space Sciences, Peking University, Beijing, China; Institute of Remote Sensing and GIS, School of Earth and Space Sciences, Peking University, Beijing, China; Department of Hydraulic Engineering, Tsinghua University, Beijing, China; Department of Hydraulic Engineering, Tsinghua University, Beijing, China; Institute of Remote Sensing and GIS, School of Earth and Space Sciences, Peking University, Beijing, China; School of Civil Engineering and Environmental Science, University of Oklahoma, Norman, OK, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","9","3701","3701","Presents corrections to above named paper.","","","10.1109/JSTARS.2019.2939941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844097","","Remote sensing;Object recognition;Reservoirs;Artificial satellites;Deep learning;Convolutional neural networks","","","","","1","Traditional","","","","IEEE","IEEE Journals"
"Feature Analysis of Marginalized Stacked Denoising Autoenconder for Unsupervised Domain Adaptation","P. Wei; Y. Ke; C. K. Goh","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Rolls-Royce Advanced Technology Centre, Singapore","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","5","1321","1334","Marginalized stacked denoising autoencoder (mSDA), has recently emerged with demonstrated effectiveness in domain adaptation. In this paper, we investigate the rationale for why mSDA benefits domain adaptation tasks from the perspective of adaptive regularization. Our investigations focus on two types of feature corruption noise: Gaussian noise (mSDAg) and Bernoulli dropout noise (mSDAbd). Both theoretical and empirical results demonstrate that mSDAbd successfully boosts the adaptation performance but mSDAg fails to do so. We then propose a new mSDA with data-dependent multinomial dropout noise (mSDAmd) that overcomes the limitations of mSDAbd and further improves the adaptation performance. Our mSDAmd is based on a more realistic assumption: different features are correlated and, thus, should be corrupted with different probabilities. Experimental results demonstrate the superiority of mSDAmd to mSDAbd on the adaptation performance and the convergence speed. Finally, we propose a deep transferable feature coding (DTFC) framework for unsupervised domain adaptation. The motivation of DTFC is that mSDA fails to consider the distribution discrepancy across different domains in the feature learning process. We introduce a new element to mSDA: domain divergence minimization by maximum mean discrepancy. This element is essential for domain adaptation as it ensures the extracted deep features to have a small distribution discrepancy. The effectiveness of DTFC is verified by extensive experiments on three benchmark data sets for both Bernoulli dropout noise and multinomial dropout noise.","","","10.1109/TNNLS.2018.2868709","National Research Foundation Singapore; Ministry of Education of Singapore through AcRF Tier-1; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8475034","Deep feature learning and feature analysis;marginalized denoising autoencoder (mDA);unsupervised domain adaptation","Noise reduction;Feature extraction;Adaptation models;Learning systems;Task analysis;Gaussian noise;Minimization","feature extraction;Gaussian noise;image denoising;minimisation;neural nets;probability;unsupervised learning","Bernoulli dropout noise;marginalized stacked denoising autoenconder;unsupervised domain adaptation;adaptive regularization;feature corruption noise;adaptation performance;data-dependent multinomial dropout noise;domain adaptation tasks;feature analysis;mSDA;Gaussian noise;probabilities;deep transferable feature coding framework;DTFC framework;feature learning process;distribution discrepancy;deep feature extraction;domain divergence minimization","","1","52","","","","","IEEE","IEEE Journals"
"Joint Optimization of Caching, Computing, and Radio Resources for Fog-Enabled IoT Using Natural Actor–Critic Deep Reinforcement Learning","Y. Wei; F. R. Yu; M. Song; Z. Han","Beijing Key Laboratory of Work Safety Intelligent Monitoring, School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada; Beijing Key Laboratory of Work Safety Intelligent Monitoring, School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Electrical and Computer Engineering Department and the Computer Science Department, University of Houston, Houston, TX, USA","IEEE Internet of Things Journal","","2019","6","2","2061","2073","The cloud-based Internet of Things (IoT) develops rapidly but suffer from large latency and backhaul bandwidth requirement, the technology of fog computing and caching has emerged as a promising paradigm for IoT to provide proximity services, and thus reduce service latency and save backhaul bandwidth. However, the performance of the fog-enabled IoT depends on the intelligent and efficient management of various network resources, and consequently the synergy of caching, computing, and communications becomes the big challenge. This paper simultaneously tackles the issues of content caching strategy, computation offloading policy, and radio resource allocation, and propose a joint optimization solution for the fog-enabled IoT. Since wireless signals and service requests have stochastic properties, we use the actor-critic reinforcement learning framework to solve the joint decision-making problem with the objective of minimizing the average end-to-end delay. The deep neural network (DNN) is employed as the function approximator to estimate the value functions in the critic part due to the extremely large state and action space in our problem. The actor part uses another DNN to represent a parameterized stochastic policy and improves the policy with the help of the critic. Furthermore, the Natural policy gradient method is used to avoid converging to the local maximum. Using the numerical simulations, we demonstrate the learning capacity of the proposed algorithm and analyze the end-to-end service latency.","","","10.1109/JIOT.2018.2878435","National Natural Science Foundation of China; U.S. MURI AFOSR; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8513863","Actor–critic;deep neural network (DNN);edge caching;fog computing;Internet of Things (IoT);reinforcement learning (RL)","Computational modeling;Cloud computing;Internet of Things;Optimization;Resource management;Edge computing;Games","cloud computing;decision making;function approximation;gradient methods;Internet of Things;learning (artificial intelligence);neural nets;optimisation;resource allocation","fog computing;proximity services;backhaul bandwidth;fog-enabled IoT;network resources;content caching strategy;computation offloading policy;radio resource allocation;joint optimization solution;actor-critic reinforcement;joint decision-making problem;end-to-end service;radio resources;cloud-based Internet;natural actor-critic deep reinforcement learning","","9","38","","","","","IEEE","IEEE Journals"
"Fully Polarized SAR imagery Classification Based on Deep Reinforcement Learning Method Using Multiple Polarimetric Features","K. Huang; W. Nie; N. Luo","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","10","3719","3730","Most traditional supervised classification methods for polarimetric synthetic aperture radar (PolSAR) imagery require abundant manually selected samples, and the classification results are affected by the size and quality of the samples. In this paper, we propose an improved deep Q-network (DQN) method for PolSAR image classification, which can generate amounts of valid data by interacting with the agent using the ε-greedy strategy. The PolSAR data are first preprocessed to reduce the influence of speckle noise and extract the multi-dimensional features. The multi-dimensional feature image and corresponding training image are then fed into a deep reinforcement learning model tailored for PolSAR image classification. After many epochs of training, the method was applied to identify different land cover types in two PolSAR images acquired by different sensors. The experimental results demonstrate that the proposed method has a better classification performance compared with traditional supervised classification methods, such as convolutional neural network (CNN), random forest (RF), and L2-loss linear support vector machine (L2-SVM), and also has a good performance compared with the deep learning method CNN-SVM, which integrates the synergy of the SVM and CNN methods, especially in small sample sizes. This study also provides a toolset for the DQN (kiwi.server) on the GitHub development platform for training and visualization.","","","10.1109/JSTARS.2019.2913445","National Key R&D Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8713855","Deep Q-network (DQN);deep reinforcement learning;polarimetric synthetic aperture radar (PolSAR) imagery classification","Feature extraction;Training;Task analysis;Reinforcement learning;Remote sensing;Support vector machines;Games","","","","1","47","IEEE","","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning-Based Energy Management for a Series Hybrid Electric Vehicle Enabled by History Cumulative Trip Information","Y. Li; H. He; J. Peng; H. Wang","National Engineering Laboratory for Electric Vehicles, School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; National Engineering Laboratory for Electric Vehicles, School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; National Engineering Laboratory for Electric Vehicles, School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; Department of Mechanical and Mechatronics Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Vehicular Technology","","2019","68","8","7416","7430","It is essential to develop proper energy management strategies (EMSs) with broad adaptability for hybrid electric vehicles (HEVs). This paper utilizes deep reinforcement learning (DRL) to develop EMSs for a series HEV due to DRL's advantages of requiring no future driving information in derivation and good generalization in solving energy management problem formulated as a Markov decision process. History cumulative trip information is also integrated for effective state of charge guidance in DRL-based EMSs. The proposed method is systematically introduced from offline training to online applications; its learning ability, optimality, and generalization are validated by comparisons with fuel economy benchmark optimized by dynamic programming, and real-time EMSs based on model predictive control (MPC). Simulation results indicate that without a priori knowledge of future trip, original DRL-based EMS achieves an average 3.5% gap from benchmark, superior to MPC-based EMS with accurate prediction; after further applying output frequency adjustment, a mean gap of 8.7%, which is comparable with MPC-based EMS with mean prediction error of 1 m/s, is maintained with concurrently noteworthy improvement in reducing engine start times. Besides, its impressive computation speed of about 0.001 s per simulation step proves its practical application potential, and this method is independent of powertrain topology such that it is applicative for any type of HEVs even when future driving information is unavailable.","","","10.1109/TVT.2019.2926472","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; China Postdoctoral Science Foundation; Project funded by Chief Expert for Modern Industry of Dezhou; International Graduate Exchange Program of the Beijing Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754786","Deep reinforcement learning;dynamic programming;energy management;generalization;model predictive control;optimality","Energy management;Engines;Hybrid electric vehicles;History;Training;Batteries;Computational modeling","dynamic programming;energy management systems;fuel economy;hybrid electric vehicles;learning (artificial intelligence);Markov processes;power engineering computing;traffic engineering computing","deep reinforcement learning;series hybrid electric vehicle;history cumulative trip information;hybrid electric vehicles;series HEV;future driving information;energy management problem;Markov decision process;DRL-based EMSs;fuel economy benchmark;model predictive control;MPC-based EMS;mean prediction error;energy management strategies;generalization;DRL-based EMS;velocity 1.0 m/s;time 0.001 s","","","41","Traditional","","","","IEEE","IEEE Journals"
"A Deep Network Solution for Attention and Aesthetics Aware Photo Cropping","W. Wang; J. Shen; H. Ling","Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","7","1531","1544","We study the problem of photo cropping, which aims to find a cropping window of an input image to preserve as much as possible its important parts while being aesthetically pleasant. Seeking a deep learning-based solution, we design a neural network that has two branches for attention box prediction (ABP) and aesthetics assessment (AA), respectively. Given the input image, the ABP network predicts an attention bounding box as an initial minimum cropping window, around which a set of cropping candidates are generated with little loss of important information. Then, the AA network is employed to select the final cropping window with the best aesthetic quality among the candidates. The two sub-networks are designed to share the same full-image convolutional feature map, and thus are computationally efficient. By leveraging attention prediction and aesthetics assessment, the cropping model produces high-quality cropping results, even with the limited availability of training data for photo cropping. The experimental results on benchmark datasets clearly validate the effectiveness of the proposed approach. In addition, our approach runs at 5 fps, outperforming most previous solutions. The code and results are available at: https://github.com/shenjianbing/DeepCropping.","","","10.1109/TPAMI.2018.2840724","Natural Science Foundation of Beijing Municipality; National Basic Research Program of China; Fok Ying-Tong Education Foundation; Beijing Municipal Education Commission; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8365844","Photo cropping;attention box prediction;aesthetics assessment;deep learning","Visualization;Microsoft Windows;Machine learning;Task analysis;Prediction algorithms;Feature extraction;Computational modeling","convolution;learning (artificial intelligence);neural nets","deep network solution;aesthetics aware photo cropping;deep learning-based solution;neural network;aesthetics assessment;ABP network;attention bounding box;initial minimum cropping window;cropping candidates;AA network;final cropping window;aesthetic quality;full-image convolutional feature map;leveraging attention prediction;cropping model;high-quality cropping results","","52","73","","","","","IEEE","IEEE Journals"
"SGANVO: Unsupervised Deep Visual Odometry and Depth Estimation With Stacked Generative Adversarial Networks","T. Feng; D. Gu","School of Computer Science and Electronic Engineering, University of Essex, Colchester, U.K.; School of Computer Science and Electronic Engineering, University of Essex, Colchester, U.K.","IEEE Robotics and Automation Letters","","2019","4","4","4431","4437","Recently end-to-end unsupervised deep learning methods have demonstrated an impressive performance for visual depth and ego-motion estimation tasks. These data-based learning methods do not rely on the same limiting assumptions that geometry-based methods do. The encoder-decoder network has been widely used in the depth estimation and the RCNN has brought significant improvements in the ego-motion estimation. Furthermore, the latest use of generative adversarial nets (GANs) in depth and ego-motion estimation has demonstrated that the estimation could be further improved by generating pictures in the game learning process. This paper proposes a novel unsupervised network system for visual depth and ego-motion estimation- stacked generative adversarial network. It consists of a stack of GAN layers, of which the lowest layer estimates the depth and egomotion while the higher layers estimate the spatial features. It can also capture the temporal dynamic due to the use of a recurrent representation across the layers. We select the most commonly used KITTI data set for evaluation. The evaluation results show that our proposed method can produce better or comparable results in depth and ego-motion estimation.","","","10.1109/LRA.2019.2925555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747446","Deep Learning in Robotics and Automation;SLAM;Localization;Mapping","Estimation;Generators;Image reconstruction;Generative adversarial networks;Visual odometry;Training;Gallium nitride","convolutional neural nets;mobile robots;motion estimation;unsupervised learning","SGANVO;generative adversarial network;novel unsupervised network system;generative adversarial nets;geometry-based methods;data-based learning methods;ego-motion estimation tasks;visual depth;deep learning methods;stacked generative adversarial networks;depth estimation;unsupervised deep visual odometry","","1","28","Traditional","","","","IEEE","IEEE Journals"
"Fast Inference Predictive Coding: A Novel Model for Constructing Deep Neural Networks","Z. Song; J. Zhang; G. Shi; J. Liu","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","4","1150","1165","As a biomimetic model of visual information processing, predictive coding (PC) has become increasingly popular for explaining a range of neural responses and many aspects of brain organization. While the development of PC model is encouraging in the neurobiology community, its practical applications in machine learning (e.g., image classification) have not been fully explored yet. In this paper, a novel image processing model called fast inference PC (FIPC) is presented for image representation and classification. Compared with the basic PC model, a regression procedure and a classification layer have been added to the proposed FIPC model. The regression procedure is used to learn regression mappings that achieve fast inference at test time, while the classification layer can instruct the model to extract more discriminative features. In addition, effective learning and fine-tuning algorithms are developed for the proposed model. Experimental results obtained on four image benchmark data sets show that our model is able to directly and fast infer representations and, simultaneously, produce lower error rates on image classification tasks.","","","10.1109/TNNLS.2018.2862866","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444980","Deep learning (DL);feature extraction;image classification;inference mechanisms;predictive coding (PC)","Brain modeling;Feature extraction;Mathematical model;Neurons;Task analysis;Predictive models;Visualization","image classification;image representation;learning (artificial intelligence);neural nets;regression analysis","fast inference predictive coding;deep neural networks;biomimetic model;visual information processing;neural responses;brain organization;neurobiology community;machine learning;image processing model;fast inference PC;image representation;regression procedure;classification layer;FIPC model;image benchmark data sets;image classification tasks","","1","55","","","","","IEEE","IEEE Journals"
"An AMP-Based Network With Deep Residual Learning for mmWave Beamspace Channel Estimation","Y. Wei; M. Zhao; M. Zhao; M. Lei; Q. Yu","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Institute of China Electronics Equipment System Engineering Corporation, Beijing, China","IEEE Wireless Communications Letters","","2019","8","4","1289","1292","Beamspace channel estimation in millimeter-wave (mmWave) massive MIMO system is a very challenging task, especially when the number of radio-frequency chains is limited. To address this problem, we present a novel approximate message passing (AMP)-based network with deep residual learning, referred to as LampResNet. It mainly consists of two components: 1) a learned AMP (LAMP) network and 2) a deep residual learning network (ResNet). The former utilizes the sparsity property of beamspace channel matrix and is employed to obtain a preliminary estimation result, while the latter is designed to reduce the impact of channel noise and further refine the coarse estimation obtained by the LAMP network. Simulation results validate the efficiency of the proposed network.","","","10.1109/LWC.2019.2916786","Fundamental Research Funds for the Central Universities; Ministry of Science and Technology of the People's Republic of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715473","AMP;beamspace channel estimation;deep residual learning;massive MIMO;mmWave communication","Channel estimation;Antenna arrays;Radio frequency;Training;Estimation;Lenses;MIMO communication","channel estimation;message passing;millimetre wave communication;MIMO communication","radio-frequency chains;learned AMP network;deep residual learning network;beamspace channel matrix;channel noise;coarse estimation;LAMP network;AMP-based network;mmWave beamspace channel estimation;millimeter-wave massive MIMO system;approximate message passing-based network","","","17","","","","","IEEE","IEEE Journals"
"Simultaneous Cell Detection and Classification in Bone Marrow Histology Images","T. Song; V. Sanchez; H. EI Daly; N. M. Rajpoot","Department of Computer Science, University of Warwick, Coventry, U.K.; Department of Computer Science, University of Warwick, Coventry, U.K.; Haematology Oncology Diagnostic Service, Addenbrooke's Hospital, Cambridge, U.K.; Department of Computer Science, University of Warwick, Coventry, U.K.","IEEE Journal of Biomedical and Health Informatics","","2019","23","4","1469","1476","Recently, deep learning frameworks have been shown to be successful and efficient in processing digital histology images for various detection and classification tasks. Among these tasks, cell detection and classification are key steps in many computer-assisted diagnosis systems. Traditionally, cell detection and classification is performed as a sequence of two consecutive steps by using two separate deep learning networks: one for detection and the other for classification. This strategy inevitably increases the computational complexity of the training stage. In this paper, we propose a synchronized deep autoencoder network for simultaneous detection and classification of cells in bone marrow histology images. The proposed network uses a single architecture to detect the positions of cells and classify the detected cells, in parallel. It uses a curvesupport Gaussian model to compute probability maps that allow detecting irregularly shape cells precisely. Moreover, the network includes a novel neighborhood selection mechanism to boost the classification accuracy. We show that the performance of the proposed network is superior than traditional deep learning detection methods and very competitive compared to traditional deep learning classification networks. Runtime comparison also shows that our network requires less time to be trained.","","","10.1109/JBHI.2018.2878945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516282","Digital pathology;bone marrow trephine biopsy;deep learning;cell detection;cell classification","Computer architecture;Microprocessors;Bones;Feature extraction;Synchronization;Stem cells","bone;computational complexity;Gaussian processes;image classification;learning (artificial intelligence);medical image processing;neural nets;object detection;probability","simultaneous cell detection;bone marrow histology images;deep learning frameworks;digital histology images;classification tasks;computer-assisted diagnosis systems;consecutive steps;computational complexity;synchronized deep autoencoder network;detected cells;irregularly shape cells;classification accuracy;traditional deep learning detection methods;traditional deep learning classification networks;curvesupport Gaussian model;novel neighborhood selection mechanism","","","29","","","","","IEEE","IEEE Journals"
"Reinforcement Learning for Real-Time Optimization in NB-IoT Networks","N. Jiang; Y. Deng; A. Nallanathan; J. A. Chambers","School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.; Department of Informatics, King’s College London, London, U.K.; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.; Department of Engineering, University of Leicester, Leicester, U.K.","IEEE Journal on Selected Areas in Communications","","2019","37","6","1424","1440","NarrowBand Internet of Things (NB-IoT) is an emerging cellular-based technology that offers a range of flexible configurations for massive IoT radio access from groups of devices with heterogeneous requirements. A configuration specifies the amount of radio resource allocated to each group of devices for random access and for data transmission. Assuming no knowledge of the traffic statistics, there exists an important challenge in “how to determine the configuration that maximizes the long-term average number of served IoT devices at each transmission time interval (TTI) in an online fashion.” Given the complexity of searching for optimal configuration, we first develop real-time configuration selection based on the tabular Q-learning (tabular-Q), the linear approximation-based Q-learning (LA-Q), and the deep neural network-based Q-learning (DQN) in the single-parameter single-group scenario. Our results show that the proposed reinforcement learning-based approaches considerably outperform the conventional heuristic approaches based on load estimation (LE-URC) in terms of the number of served IoT devices. This result also indicates that LA-Q and DQN can be good alternatives for tabular-Q to achieve almost the same performance with much less training time. We further advance LA-Q and DQN via actions aggregation (AA-LA-Q and AA-DQN) and via cooperative multi-agent learning (CMA-DQN) for the multi-parameter multi-group scenario, thereby solve the problem that Q-learning agents do not converge in high-dimensional configurations. In this scenario, the superiority of the proposed Q-learning approaches over the conventional LE-URC approach significantly improves with the increase of configuration dimensions, and the CMA-DQN approach outperforms the other approaches in both throughput and training efficiency.","","","10.1109/JSAC.2019.2904366","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664581","Narrowband Internet of Things;resource configuration;real-time optimization;reinforcement learning;cooperative learning","Interference;Signal to noise ratio;Long Term Evolution;Data communication;Numerical models;Analytical models;Uplink","approximation theory;data communication;Internet of Things;learning (artificial intelligence);multi-agent systems;neural nets;optimisation;radio access networks;resource allocation;telecommunication computing","real-time optimization;NB-IoT networks;flexible configurations;heterogeneous requirements;radio resource;random access;data transmission;long-term average number;served IoT devices;transmission time interval;optimal configuration;real-time configuration selection;tabular Q-learning;linear approximation-based Q-learning;deep neural network-based Q-learning;reinforcement learning-based approaches;AA-DQN;multiparameter multigroup scenario;high-dimensional configurations;Q-learning approaches;LE-URC approach;configuration dimensions;CMA-DQN approach;IoT radio access devices;cooperative multiagent learning;heuristic approaches;cellular-based technology","","2","44","","","","","IEEE","IEEE Journals"
"DispSegNet: Leveraging Semantics for End-to-End Learning of Disparity Estimation From Stereo Imagery","J. Zhang; K. A. Skinner; R. Vasudevan; M. Johnson-Roberson","Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA; Robotics Program, University of Michigan, Ann Arbor, MI, USA; Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Naval Architecture and Marine Engineering, University of Michigan, Ann Arbor, MI, USA","IEEE Robotics and Automation Letters","","2019","4","2","1162","1169","Recent work has shown that convolutional neural networks (CNNs) can be applied successfully in disparity estimation, but these methods still suffer from errors in regions of low texture, occlusions, and reflections. Concurrently, deep learning for semantic segmentation has shown great progress in recent years. In this letter, we design a CNN architecture that combines these two tasks to improve the quality and accuracy of disparity estimation with the help of semantic segmentation. Specifically, we propose a network structure in which these two tasks are highly coupled. One key novelty of this approach is the two-stage refinement process. Initial disparity estimates are refined with an embedding learned from the semantic segmentation branch of the network. The proposed model is trained using an unsupervised approach, in which images from one half of the stereo pair are warped and compared against images from the other camera. Another key advantage of the proposed approach is that a single network is capable of outputting disparity estimates and semantic labels. These outputs are of great use in autonomous vehicle operation; with real-time constraints being key, such performance improvements increase the viability of driving applications. Experiments on KITTI and Cityscapes datasets show that our model can achieve state-of-the-art results and that leveraging embedding learned from semantic segmentation improves the performance of disparity estimation.","","","10.1109/LRA.2019.2894913","Ford Motor Company; Ford-UM Alliance; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624344","Computer vision for transportation;deep learning in robotics and automation","Estimation;Semantics;Task analysis;Image segmentation;Feature extraction;Three-dimensional displays;Convolution","convolutional neural nets;estimation theory;image segmentation;image texture;learning (artificial intelligence);neural net architecture;object detection;stereo image processing;unsupervised learning","end-to-end learning;disparity estimation;convolutional neural networks;initial disparity estimates;semantic labels;DispSegNet;stereo imagery;CNNs;deep learning;semantic segmentation;two-stage refinement process;unsupervised approach;single network;autonomous vehicle operation;real-time constraints;Cityscapes datasets;KITTI datasets","","1","30","","","","","IEEE","IEEE Journals"
"Online Monitoring and Model-Free Adaptive Control of Weld Penetration in VPPAW Based on Extreme Learning Machine","D. Wu; H. Chen; Y. Huang; S. Chen","Shanghai Key Laboratory of Materials Laser Processing and Modification School of Materials Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Materials Laser Processing and Modification School of Materials Science and Engineering and Collaborative Innovation Center for Advanced Ship and Deep-sea Exploration, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Materials Laser Processing and Modification School of Materials Science and Engineering and Collaborative Innovation Center for Advanced Ship and Deep-sea Exploration, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Materials Laser Processing and Modification School of Materials Science and Engineering and Collaborative Innovation Center for Advanced Ship and Deep-sea Exploration, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Industrial Informatics","","2019","15","5","2732","2740","Monitoring and controlling of weld joint penetration are essential issues in variable polarity plasma arc welding (VPPAW). In this paper, we develop a flexible visual sensor system to measure the backside keyhole characteristic parameters such as keyhole length, width, and area. Further, data analysis from dynamic welding experiments reveals a nonlinear correlation of the keyhole features and the backside beam width. To provide accurate feedback information, an extreme learning machine method is applied to predict the weld width with sufficient accuracy and less computing time. Then a novel model-free adaptive control (MFAC) is designed to control the weld penetration evaluated by the weld width. Closed-loop experimental results confirm that the MFAC system can simultaneously adjust the welding current and plasma gas flow rate to control the VPPAW process for obtaining a full-penetrated weld under various initial welding conditions and disturbances.","","","10.1109/TII.2018.2870933","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466905","Extreme learning machine (ELM);keyhole characteristic parameters;model-free adaptive control (MFAC);penetration estimation;variable polarity plasma arc welding (VPPAW)","Welding;Plasma welding;Process control;Real-time systems;Monitoring;Adaptation models","adaptive control;arc welding;data analysis;learning (artificial intelligence);plasma materials processing;production engineering computing;welds","online monitoring;weld joint penetration;variable polarity plasma arc welding;flexible visual sensor system;backside keyhole characteristic parameters;dynamic welding experiments;plasma gas flow rate;model-free adaptive control;extreme learning machine;data analysis;nonlinear correlation;closed-loop experimental results","","4","33","","","","","IEEE","IEEE Journals"
"Combining Spectral and Spatial Features for Deep Learning Based Blind Speaker Separation","Z. Wang; D. Wang","Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","2","457","468","This study tightly integrates complementary spectral and spatial features for deep learning based multi-channel speaker separation in reverberant environments. The key idea is to localize individual speakers so that an enhancement network can be trained on spatial as well as spectral features to extract the speaker from an estimated direction and with specific spectral structures. The spatial and spectral features are designed in a way such that the trained models are blind to the number of microphones and microphone geometry. To determine the direction of the speaker of interest, we identify time-frequency (T-F) units dominated by that speaker and only use them for direction estimation. The T-F unit level speaker dominance is determined by a two-channel chimera++ network, which combines deep clustering and permutation invariant training at the objective function level, and integrates spectral and interchannel phase patterns at the input feature level. In addition, T-F masking based beamforming is tightly integrated in the system by leveraging the magnitudes and phases produced by beamforming. Strong separation performance has been observed on reverberant talker-independent speaker separation, which separates reverberant speaker mixtures based on a random number of microphones arranged in arbitrary linear-array geometry.","","","10.1109/TASLP.2018.2881912","Air Force Research Laboratory; National Science Foundation; Ohio Supercomputer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540037","Spatial features;beamforming;deep clustering;permutation invariant training;chimera++ networks;blind source separation","Array signal processing;Training;Speech processing;Microphone arrays;Geometry","acoustic signal processing;array signal processing;blind source separation;learning (artificial intelligence);microphones;reverberation;source separation;speaker recognition","microphone geometry;direction estimation;T-F unit level speaker dominance;two-channel chimera;deep clustering;permutation invariant training;objective function level;input feature level;strong separation performance;reverberant talker-independent speaker separation;reverberant speaker mixtures;spatial features;deep learning;blind speaker separation;complementary spectral features;multichannel speaker separation;reverberant environments;individual speakers;enhancement network;estimated direction;specific spectral structures;trained models","","5","50","","","","","IEEE","IEEE Journals"
"Robust Speaker Localization Guided by Deep Learning-Based Time-Frequency Masking","Z. Wang; X. Zhang; D. Wang","Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science, Inner Mongolia University, Hohhot, China; Department of Computer Science and Engineering and the Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","1","178","188","Deep learning-based time-frequency (T-F) masking has dramatically advanced monaural (single-channel) speech separation and enhancement. This study investigates its potential for direction of arrival (DOA) estimation in noisy and reverberant environments. We explore ways of combining T-F masking and conventional localization algorithms, such as generalized cross correlation with phase transform, as well as newly proposed algorithms based on steered-response SNR and steering vectors. The key idea is to utilize deep neural networks (DNNs) to identify speech dominant T-F units containing relatively clean phase for DOA estimation. Our DNN is trained using only monaural spectral information, and this makes the trained model directly applicable to arrays with various numbers of microphones arranged in diverse geometries. Although only monaural information is used for training, experimental results show strong robustness of the proposed approach in new environments with intense noise and room reverberation, outperforming traditional DOA estimation methods by large margins. Our study also suggests that the ideal ratio mask and its variants remain effective training targets for robust speaker localization.","","","10.1109/TASLP.2018.2876169","Air Force Research Laboratory; National Science Foundation; Ohio Supercomputer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8492455","GCC-PHAT;steered-response power;time-frequency masking;robust speaker localization;deep neural networks","Microphones;Estimation;Reverberation;Delay effects;Direction-of-arrival estimation;Signal to noise ratio;Signal processing algorithms","acoustic noise;acoustic signal processing;audio signal processing;direction-of-arrival estimation;learning (artificial intelligence);microphones;neural nets;speech processing;time-frequency analysis","robust speaker localization;monaural speech separation;direction of arrival estimation;monaural speech enhancement;reverberant environments;noisy environments;single-channel;deep learning-based time-frequency masking;ideal ratio mask;traditional DOA estimation methods;room reverberation;intense noise;trained model;monaural spectral information;speech dominant T-F units;deep neural networks;steering vectors;generalized cross correlation;conventional localization algorithms","","3","52","","","","","IEEE","IEEE Journals"
"Deep Proximal Unrolling: Algorithmic Framework, Convergence Analysis and Applications","R. Liu; S. Cheng; L. Ma; X. Fan; Z. Luo","Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, DUT-RU International School of Information Science and Engineering, Dalian University of Technology, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, DUT-RU International School of Information Science and Engineering, Dalian University of Technology, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, DUT-RU International School of Information Science and Engineering, Dalian University of Technology, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, DUT-RU International School of Information Science and Engineering, Dalian University of Technology, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, DUT-RU International School of Information Science and Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Image Processing","","2019","28","10","5013","5026","Deep learning models have gained great success in many real-world applications. However, most existing networks are typically designed in heuristic manners, thus these approaches lack rigorous mathematical derivations and clear interpretations. Several recent studies try to build deep models by unrolling a particular optimization model that involves task information. Unfortunately, due to the dynamic nature of network parameters, their resultant deep propagations do not possess the nice convergence property as the original optimization scheme does. In this work, we develop a generic paradigm to unroll nonconvex optimization for deep model design. Different from most existing frameworks, which just replace the iterations by network architectures, we prove in theory that the propagation generated by our proximally unrolled deep model can globally converge to the critical-point of the original optimization model. Moreover, even if the task information is only partially available (e.g., no prior regularization), we can still train convergent deep propagations. We also extend these theoretical investigations on the more general multi-block models and thus a lot of real-world applications can be successfully handled by the proposed framework. Finally, we conduct experiments on various low-level vision tasks (i.e., non-blind deconvolution, dehazing, and low-light image enhancement) and demonstrate the superiority of our proposed framework, compared with existing state-of-the-art approaches.","","","10.1109/TIP.2019.2913536","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704990","Deep propagation;proximal algorithm;global convergence;low-level computer vision","Task analysis;Optimization;Convergence;Mathematical model;Network architecture;Computer architecture;Data models","computer vision;concave programming;convergence;deconvolution;image enhancement;image restoration;learning (artificial intelligence);optimisation","deep proximal unrolling;algorithmic framework;convergence analysis;deep learning models;real-world applications;heuristic manners;rigorous mathematical derivations;clear interpretations;deep models;particular optimization model;task information;network parameters;nice convergence property;original optimization scheme;nonconvex optimization;deep model design;network architectures;proximally unrolled deep model;original optimization model;convergent deep propagations;general multiblock models;low-level vision tasks;state-of-the-art approaches","","","54","","","","","IEEE","IEEE Journals"
"RSDNet: Learning to Predict Remaining Surgery Duration from Laparoscopic Videos Without Manual Annotations","A. P. Twinanda; G. Yengera; D. Mutter; J. Marescaux; N. Padoy","ICube, University of Strasbourg, CNRS, IHU Strasbourg, Strasbourg, France; ICube, University of Strasbourg, CNRS, IHU Strasbourg, Strasbourg, France; University Hospital of Strasbourg, IRCAD, IHU Strasbourg, Strasbourg, France; University Hospital of Strasbourg, IRCAD, IHU Strasbourg, Strasbourg, France; ICube, University of Strasbourg, CNRS, IHU Strasbourg, Strasbourg, France","IEEE Transactions on Medical Imaging","","2019","38","4","1069","1078","Accurate surgery duration estimation is necessary for optimal OR planning, which plays an important role in patient comfort and safety as well as resource optimization. It is, however, challenging to preoperatively predict surgery duration since it varies significantly depending on the patient condition, surgeon skills, and intraoperative situation. In this paper, we propose a deep learning pipeline, referred to as RSDNet, which automatically estimates the remaining surgery duration (RSD) intraoperatively by using only visual information from laparoscopic videos. The previous state-of-the-art approaches for RSD prediction are dependent on manual annotation, whose generation requires expensive expert knowledge and is time-consuming, especially considering the numerous types of surgeries performed in a hospital and the large number of laparoscopic videos available. A crucial feature of RSDNet is that it does not depend on any manual annotation during training, making it easily scalable to many kinds of surgeries. The generalizability of our approach is demonstrated by testing the pipeline on two large datasets containing different types of surgeries: 120 cholecystectomy and 170 gastric bypass videos. The experimental results also show that the proposed network significantly outperforms a traditional method of estimating RSD without utilizing manual annotation. Further, this paper provides a deeper insight into the deep learning network through visualization and interpretation of the features that are automatically learned.","","","10.1109/TMI.2018.2878055","Agence Nationale de la Recherche; BPI France; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8509608","Bypass;cholecystectomy;deep learning;laparoscopic video;OR planning;remaining surgery duration","Surgery;Estimation;Pipelines;Videos;Manuals;Task analysis;Visualization","biomedical optical imaging;learning (artificial intelligence);medical image processing;medical robotics;surgery","RSDNet;laparoscopic videos;deep learning pipeline;RSD prediction;deep learning network;gastric bypass videos","","1","21","","","","","IEEE","IEEE Journals"
"SiftingGAN: Generating and Sifting Labeled Samples to Improve the Remote Sensing Image Scene Classification Baseline In Vitro","D. Ma; P. Tang; L. Zhao","Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","7","1046","1050","Lack of annotated samples greatly restrains the direct application of deep learning in remote sensing image scene classification. Although research studies have been done to tackle this issue by data augmentation with various image transformation operations, they are still limited in quantity and diversity. Recently, the advent of the unsupervised learning-based generative adversarial networks (GANs) brings us a new way to generate augmented samples. However, such GAN-generated samples are currently only served for training GANs model itself and for improving the performance of the discriminator in GANs internally (in vivo). It becomes a question of serious doubt whether the GAN-generated samples can help better improve the scene classification performance of other deep learning networks (in vitro), compared with the widely used transformed samples. To answer this question, this letter proposes a SiftingGAN approach to generate more numerous, more diverse, and more authentic labeled samples for data augmentation. SiftingGAN extends traditional GAN framework with an Online-Output method for sample generation, a Generative-Model-Sifting method for model sifting, and a Labeled-Sample-Discriminating method for sample sifting. Experiments on the well-known aerial image data set demonstrate that the proposed SiftingGAN method can not only effectively improve the performance of the scene classification baseline that is achieved without data augmentation but also significantly excels the comparison methods based on traditional geometric/radiometric transformation operations.","","","10.1109/LGRS.2018.2890413","Chinese Academy of Sciences; National Natural Science Foundation of China; Major Project of High Resolution Earth Observation System of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611213","Data augmentation;deep learning;generative adversarial networks (GANs);scene classification","Gallium nitride;Generators;Remote sensing;Training;In vitro;Generative adversarial networks;Deep learning","geophysical image processing;image classification;neural nets;remote sensing;unsupervised learning","remote sensing image scene classification baseline;data augmentation;image transformation operations;unsupervised learning-based generative adversarial networks;augmented samples;GAN-generated samples;GANs model;scene classification performance;deep learning networks;traditional GAN framework;sample generation;aerial image data;SiftingGAN method;labeled-sample-discriminating method;generative-model-sifting method;authentic labeled samples;online-output method;aerial image data set;geometric-radiometric transformation operations;labeled sample sifting","","1","15","","","","","IEEE","IEEE Journals"
"Learning Effective RGB-D Representations for Scene Recognition","X. Song; S. Jiang; L. Herranz; C. Chen","Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Computer Vision Center, Universitat Autònoma de Barcelona, Barcelona, Spain; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2019","28","2","980","993","Deep convolutional networks can achieve impressive results on RGB scene recognition thanks to large data sets such as places. In contrast, RGB-D scene recognition is still underdeveloped in comparison, due to two limitations of RGB-D data we address in this paper. The first limitation is the lack of depth data for training deep learning models. Rather than fine tuning or transferring RGB-specific features, we address this limitation by proposing an architecture and a two-step training approach that directly learns effective depth-specific features using weak supervision via patches. The resulting RGB-D model also benefits from more complementary multimodal features. Another limitation is the short range of depth sensors (typically 0.5 m to 5.5 m), resulting in depth images not capturing distant objects in the scenes that RGB images can. We show that this limitation can be addressed by using RGB-D videos, where more comprehensive depth information is accumulated as the camera travels across the scenes. Focusing on this scenario, we introduce the ISIA RGB-D video data set to evaluate RGB-D scene recognition with videos. Our video recognition architecture combines convolutional and recurrent neural networks that are trained in three steps with increasingly complex data to learn effective features (i.e., patches, frames, and sequences). Our approach obtains the state-of-the-art performances on RGB-D image (NYUD2 and SUN RGB-D) and video (ISIA RGB-D) scene recognition.","","","10.1109/TIP.2018.2872629","National Natural Science Foundation of China; Lenovo Outstanding Young Scientists Program; National Program for Special Support of Eminent Professionals; National Program for Support of Top-notch Young Professionals; National Postdoctoral Program for Innovative Talents; China Postdoctoral Science Foundation; European Union Research and Innovation Program under the Marie Skłodowska-Curie; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8476560","Scene recognition;deep learning;multimodal;RGB-D;video;CNN;RNN","Videos;Image recognition;Training;Tuning;Feature extraction;Databases;Data models","cameras;feature extraction;image colour analysis;image representation;image sensors;learning (artificial intelligence);object recognition;recurrent neural nets;video signal processing","deep convolutional networks;RGB-D data;depth data;training deep learning models;two-step training approach;effective depth-specific features;complementary multimodal features;depth sensors;depth images;comprehensive depth information;ISIA RGB-D video data;video recognition architecture;RGB-D image;SUN RGB-D;RGB scene recognition;effective RGB-D representation learning;camera;recurrent neural networks;convolutional neural networks","","1","43","","","","","IEEE","IEEE Journals"
"Toward More Accurate Iris Recognition Using Dilated Residual Features","K. Wang; A. Kumar","The Hong Kong Polytechnic University, Hong Kong; The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Information Forensics and Security","","2019","14","12","3233","3245","Iris recognition has emerged as the more accurate, convenient, and low-cost biometric approach to authenticate human subjects. However, the accuracy offered by current popular iris recognition algorithms is below the expectations from the community, and therefore, researchers have recently focused their attention on deep learning-based methods. This paper investigates a new deep learning-based approach for iris recognition and attempts to improve the accuracy using a more simplified framework to more accurately recover the representative features. We consider residual network learning with dilated convolutional kernels to optimize the training process and aggregate contextual information from the iris images. Such an approach also alleviates the need for the down-sampling and up-sampling layers, which not only results in a simplified network but also results in outperforming matching accuracy over several classical and state-of-the-art algorithms for iris recognition, i.e., further improvement in equal error rates by 7.14%, 10.7%, and 27.4% on three test databases. In this paper, our reproducible experimental results are presented on three publicly available datasets that illustrate outperforming results and validate the usefulness of our approach.","","","10.1109/TIFS.2019.2913234","General Research Fund from Research Grant Council of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698817","Iris recognition;personal identification;biometrics;deep learning","Iris recognition;Kernel;Feature extraction;Deep learning;Convolution;Convolutional neural networks;Image databases","convolutional neural nets;feature extraction;image matching;iris recognition;learning (artificial intelligence)","dilated residual features;dilated convolutional kernels;iris images;personal identification;residual network learning;deep learning;iris recognition","","1","39","","","","","IEEE","IEEE Journals"
"Deep Convolutional Identifier for Dynamic Modeling and Adaptive Control of Unmanned Helicopter","Y. Kang; S. Chen; X. Wang; Y. Cao","State Key Laboratory of Fire Science, Institute of Advanced Technology, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","2","524","538","Helicopters are complex high-order and time-varying nonlinear systems, strongly coupling with aerodynamic forces, engine dynamics, and other phenomena. Therefore, it is a great challenge to investigate system identification for dynamic modeling and adaptive control for helicopters. In this paper, we address the system identification problem as dynamic regression and propose to represent the uncertainties and the hidden states in the system dynamic model with a deep convolutional neural network. Particularly, the parameters of the network are directly learned from the real flight data of aerobatic helicopter. Since the deep convolutional model has a good performance for describing the dynamic behavior of the hidden states and uncertainties in the flight process, the proposed identifier manifests strong robustness and high accuracy, even for untrained aerobatic maneuvers. The effectiveness of the proposed method is verified by various experiments with the real-world flight data from the Stanford Autonomous Helicopter Project. Consequently, an adaptive flight control scheme including a deep convolutional identifier and a backstepping-based controller is presented. The stability of the flight control scheme is rigorously proved by the Lyapunov theory. It reveals that the tracking errors for both the position and attitude of unmanned helicopter asymptotic converge to a small neighborhood of the origin.","","","10.1109/TNNLS.2018.2844173","National Natural Science Foundation of China; Youth Topnotch Talent Support Program; Youth Yangtze River Scholar; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8401710","Adaptive control;deep convolutional neural network (CNN);Lyapunov theory;system identification;unmanned helicopter","Aerodynamics;Helicopters;Adaptation models;Nonlinear dynamical systems;Uncertainty;Atmospheric modeling","adaptive control;autonomous aerial vehicles;control system synthesis;convolutional neural nets;helicopters;learning systems;Lyapunov methods;neurocontrollers;nonlinear control systems;position control;robust control;stability;time-varying systems","deep convolutional identifier;dynamic modeling control;adaptive control;time-varying nonlinear systems;engine dynamics;system identification problem;dynamic regression;deep convolutional neural network;aerobatic helicopter;deep convolutional model;adaptive flight control scheme;backstepping-based controller;unmanned helicopter;stanford autonomous helicopter project","","","47","","","","","IEEE","IEEE Journals"
"RL-RRT: Kinodynamic Motion Planning via Learning Reachability Estimators From RL Policies","H. L. Chiang; J. Hsu; M. Fiser; L. Tapia; A. Faust","Robotics at Google, Google AI, Mountain View, CA, USA; Robotics at Google, Google AI, Mountain View, CA, USA; Robotics at Google, Google AI, Mountain View, CA, USA; Department of Computer Science, University of New Mexico, Albuquerque, NM, USA; Robotics at Google, Google AI, Mountain View, CA, USA","IEEE Robotics and Automation Letters","","2019","4","4","4298","4305","This letter addresses two challenges facing samplingbased kinodynamic motion planning: a way to identify good candidate states for local transitions and the subsequent computationally intractable steering between these candidate states. Through the combination of sampling-based planning, a Rapidly Exploring Randomized Tree (RRT) and an efficient kinodynamic motion planner through machine learning, we propose an efficient solution to long-range planning for kinodynamic motion planning. First, we use deep reinforcement learning to learn an obstacle-avoiding policy that maps a robot's sensor observations to actions, which is used as a local planner during planning and as a controller during execution. Second, we train a reachability estimator in a supervised manner, which predicts the RL policy's time to reach a state in the presence of obstacles. Lastly, we introduce RL-RRT that uses the RL policy as a local planner, and the reachability estimator as the distance function to bias tree-growth towards promising regions. We evaluate our method on three kinodynamic systems, including physical robot experiments. Results across all three robots tested indicate that RL-RRT outperforms state of the art kinodynamic planners in efficiency, and also provides a shorter path finish time than a steering function free method. The learned local planner policy and accompanying reachability estimator demonstrate transferability to the previously unseen experimental environments, making RL-RRT fast because the expensive computations are replaced with simple neural network inference.","","","10.1109/LRA.2019.2931199","Google; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772207","Motion and path planning;learning and adaptive systems;deep learning in robotics and automation","Planning;Robot sensing systems;Training;Noise measurement;Laser radar;Collision avoidance","collision avoidance;learning (artificial intelligence);mobile robots;robot dynamics;trees (mathematics)","kinodynamic motion planning;RL policy;subsequent computationally intractable steering;sampling-based planning;efficient kinodynamic motion planner;machine learning;long-range planning;deep reinforcement learning;obstacle-avoiding policy;kinodynamic systems;local planner policy;reachability estimator;RL-RRT","","","28","Traditional","","","","IEEE","IEEE Journals"
"Vision-Based High-Speed Driving With a Deep Dynamic Observer","P. Drews; G. Williams; B. Goldfain; E. A. Theodorou; J. M. Rehg","Institute for Robotics and Intelligent Machines (IRIM), Georgia Institute of Technology, Atlanta, GA, USA; Institute for Robotics and Intelligent Machines (IRIM), Georgia Institute of Technology, Atlanta, GA, USA; Institute for Robotics and Intelligent Machines (IRIM), Georgia Institute of Technology, Atlanta, GA, USA; Institute for Robotics and Intelligent Machines (IRIM), Georgia Institute of Technology, Atlanta, GA, USA; Institute for Robotics and Intelligent Machines (IRIM), Georgia Institute of Technology, Atlanta, GA, USA","IEEE Robotics and Automation Letters","","2019","4","2","1564","1571","In this letter, we present a framework for combining deep learning-based road detection, particle filters, and model predictive control (MPC) to drive aggressively using only a monocular camera, IMU, and wheel speed sensors. This framework uses deep convolutional neural networks combined with LSTMs to learn a local cost map representation of the track in front of the vehicle. A particle filter uses this dynamic observation model to localize in a schematic map, and MPC is used to drive aggressively using this particle filter based state estimate. We show extensive real world testing results and demonstrate reliable operation of the vehicle at the friction limits on a complex dirt track. We reach speeds above 27 m/h (12 m/s) on a dirt track with a 105 ft (32 m) long straight using our 1:5 scale test vehicle.","","","10.1109/LRA.2019.2896449","Qualcomm Innovation Fellowship; Army Research Office; DURIP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630018","Deep learning in robotics and automation;autonomous vehicle navigation;localization;computer vision for transportation","Cameras;Vehicle dynamics;Wheels;Neural networks;Computer architecture;Predictive control;Sensors","control engineering computing;convolutional neural nets;learning (artificial intelligence);mobile robots;object detection;observers;optimal control;particle filtering (numerical methods);predictive control;recurrent neural nets;road traffic control;robot vision;velocity control","deep dynamic observer;deep learning-based road detection;model predictive control;MPC;monocular camera;wheel speed sensors;deep convolutional neural networks;dynamic observation model;schematic map;particle filter based state estimate;complex dirt track;LSTMs;IMU;vision-based high-speed driving;map representation;velocity 12.0 m/s;size 32.0 m","","","22","","","","","IEEE","IEEE Journals"
"Augmentation of CBCT Reconstructed From Under-Sampled Projections Using Deep Learning","Z. Jiang; Y. Chen; Y. Zhang; Y. Ge; F. Yin; L. Ren","Department of Radiation Oncology, Duke University Medical Center (DUMC), Durham, NC, USA; Medical Physics Graduate Program, Duke University, Durham, NC, USA; Department of RadiationOncology, DukeUniversity Medical Center (DUMC), Durham, NC, USA; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; Department of Radiation Oncology, Duke University Medical Center (DUMC), Durham, NC, USA; Department of Radiation Oncology, Duke University Medical Center (DUMC), Durham, NC, USA","IEEE Transactions on Medical Imaging","","2019","38","11","2705","2715","Edges tend to be over-smoothed in total variation (TV) regularized under-sampled images. In this paper, symmetric residual convolutional neural network (SR-CNN), a deep learning based model, was proposed to enhance the sharpness of edges and detailed anatomical structures in under-sampled cone-beam computed tomography (CBCT). For training, CBCT images were reconstructed using TV-based method from limited projections simulated from the ground truth CT, and were fed into SR-CNN, which was trained to learn a restoring pattern from under-sampled images to the ground truth. For testing, under-sampled CBCT was reconstructed using TV regularization and was then augmented by SR-CNN. Performance of SR-CNN was evaluated using phantom and patient images of various disease sites acquired at different institutions both qualitatively and quantitatively using structure similarity (SSIM) and peak signal-to-noise ratio (PSNR). SR-CNN substantially enhanced image details in the TV-based CBCT across all experiments. In the patient study using real projections, SR-CNN augmented CBCT images reconstructed from as low as 120 half-fan projections to image quality comparable to the reference fully-sampled FDK reconstruction using 900 projections. In the tumor localization study, improvements in the tumor localization accuracy were made by the SR-CNN augmented images compared with the conventional FDK and TV-based images. SR-CNN demonstrated robustness against noise levels and projection number reductions and generalization for various disease sites and datasets from different institutions. Overall, the SR-CNN-based image augmentation technique was efficient and effective in considerably enhancing edges and anatomical structures in under-sampled 3D/4D-CBCT, which can be very valuable for image-guided radiotherapy.","","","10.1109/TMI.2019.2912791","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695858","Convolutional neural network;deep learning;under-sampled projections;4D-CBCT;imaging dose reduction","Image reconstruction;Image edge detection;TV;Computed tomography;Biomedical imaging;Deep learning;Image restoration","","","","","53","","","","","IEEE","IEEE Journals"
"Robust Methods for Real-Time Diabetic Foot Ulcer Detection and Localization on Mobile Devices","M. Goyal; N. D. Reeves; S. Rajbhandari; M. H. Yap","School of Computing, Mathematics and Digital Technology, Manchester Metropolitan University, Manchester, U.K.; Professor of musculoskeletal biomechanics with the Research Centre for Musculoskeletal Science & Sports Medicine, School of Healthcare Science, Faculty of Science and Engineering, Manchester Metropolitan University, Manchester, U.K.; Lancashire Teaching Hospital, Preston, U.K.; School of Computing, Mathematics and Digital Technology, Manchester Metropolitan University, Manchester, U.K.","IEEE Journal of Biomedical and Health Informatics","","2019","23","4","1730","1741","Current practice for diabetic foot ulcers (DFU) screening involves detection and localization by podiatrists. Existing automated solutions either focus on segmentation or classification. In this work, we design deep learning methods for real-time DFU localization. To produce a robust deep learning model, we collected an extensive database of 1775 images of DFU. Two medical experts produced the ground truths of this data set by outlining the region of interest of DFU with an annotator software. Using five-fold cross-validation, overall, faster R-CNN with InceptionV2 model using two-tier transfer learning achieved a mean average precision of 91.8%, the speed of 48 ms for inferencing a single image and with a model size of 57.2 MB. To demonstrate the robustness and practicality of our solution to realtime prediction, we evaluated the performance of the models on a NVIDIA Jetson TX2 and a smartphone app. This work demonstrates the capability of deep learning in real-time localization of DFU, which can be further improved with a more extensive data set.","","","10.1109/JBHI.2018.2868656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456504","Diabetic foot ulcers;deep learning;convolutional neural networks;DFU localization;real-time localization","Machine learning;Image segmentation;Skin;Foot;Diabetes;Feature extraction;Task analysis","convolutional neural nets;diseases;learning (artificial intelligence);medical image processing;mobile computing;real-time systems;smart phones","robust methods;real-time diabetic foot ulcer detection;mobile devices;diabetic foot ulcers screening;automated solutions;segmentation;design deep learning methods;real-time DFU localization;robust deep learning model;extensive database;medical experts;five-fold cross-validation;InceptionV2 model;two-tier transfer learning;single image;robustness;NVIDIA Jetson TX2;real-time localization;smartphone app;time 48.0 ms;memory size 57.2 MByte","","3","52","CCBY","","","","IEEE","IEEE Journals"
"Feature Affinity-Based Pseudo Labeling for Semi-Supervised Person Re-Identification","G. Ding; S. Zhang; S. Khan; Z. Tang; J. Zhang; F. Porikli","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; College of Engineering and Computer Science, Australian National University, Canberra, ACT, Australia; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electrical and Data Engineering, University of Technology Sydney, Ultimo, NSW, Australia; College of Engineering and Computer Science, Australian National University, Canberra, ACT, Australia","IEEE Transactions on Multimedia","","2019","21","11","2891","2902","Vision-based person re-identification aims to match a person's identity across multiple images, which is a fundamental task in multimedia content analysis and retrieval. Deep neural networks have recently manifested great potential in this task. However, a major bottleneck of existing supervised deep networks is their reliance on a large amount of annotated training data. Manual labeling for person identities in large-scale surveillance camera systems is quite challenging and incurs significant costs. Some recent studies adopt generative model outputs as training data augmentation. To more effectively use these synthetic data for an improved feature learning and re-identification performance, this paper proposes a novel feature affinity-based pseudo labeling method with two possible label encodings. To the best of our knowledge, this is the first study that employs pseudo-labeling by measuring the affinity of unlabeled samples with the underlying clusters of labeled data samples using the intermediate feature representations from deep networks. We propose training the network with the joint supervision of cross-entropy loss together with a center regularization term, which not only ensures discriminative feature representation learning but also simultaneously predicts pseudo-labels for unlabeled data. We show that both label encodings can be learned in a unified manner and help improve the overall performance. Our extensive experiments on three person re-identification datasets: Market-1501, DukeMTMC-reID, and CUHK03, demonstrate significant performance boost over the state-of-the-art person re-identification approaches.","","","10.1109/TMM.2019.2916456","National Natural Science Foundation of China; National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; CCF-Tencent Open Fund; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8713423","Pseudo-labeling;semi-supervised learning;person re-identification;deep networks;generative modeling","Labeling;Training;Gallium nitride;Generative adversarial networks;Encoding;Semisupervised learning;Task analysis","feature extraction;image representation;image retrieval;neural nets;pattern clustering;supervised learning","feature affinity-based pseudolabeling;semisupervised person re-identification;multimedia content analysis;deep neural networks;supervised deep networks;surveillance camera systems;data augmentation;discriminative feature representation learning;vision-based person re-identification;feature learning;deep network training","","1","59","Traditional","","","","IEEE","IEEE Journals"
"Accurate Facial Image Parsing at Real-Time Speed","Z. Wei; S. Liu; Y. Sun; H. Ling","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Image Processing","","2019","28","9","4659","4670","In this paper, we propose a design scheme for deep learning networks in the face parsing task with promising accuracy and real-time inference speed. By analyzing the differences between the general image parsing task and face parsing task, we first revisit the structure of traditional FCN and make improvements to adapt to the unique properties of the face parsing task. Especially, the concept of Normalized Receptive Field is proposed to give more insights on designing the network. Then, a novel loss function called Statistical Contextual Loss is introduced, which integrates richer contextual information and regularizes features during training. For further model acceleration, we propose a semi-supervised distillation scheme that effectively transfers the learned knowledge to a lighter network. Extensive experiments on LFW and Helen dataset demonstrate the significant superiority of the new design scheme on both efficacy and efficiency.","","","10.1109/TIP.2019.2909652","National Natural Science Foundation of China; National Key Research and Development Program of China; Major Scientific and Technological Project of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682072","Face parsing;receptive field;metrics learning;distillation;deep learning","Face;Task analysis;Deep learning;Real-time systems;Knowledge engineering;Training;Hair","face recognition;image classification;learning (artificial intelligence);statistical analysis","deep learning networks;facial image parsing;normalized receptive field;statistical contextual loss;semisupervised distillation scheme;LFW dataset;Helen dataset","","","41","","","","","IEEE","IEEE Journals"
"Aggregated Deep Fisher Feature for VHR Remote Sensing Scene Classification","B. Li; W. Su; H. Wu; R. Li; W. Zhang; W. Qin; S. Zhang","National Innovation Institute of Defense Technology, Academy of Military Sciences, Beijing, China; National Innovation Institute of Defense Technology, Academy of Military Sciences, Beijing, China; Institute of Medical Support Technology, Academy of Military Science, Tianjin, China; National Innovation Institute of Defense Technology, Academy of Military Sciences, Beijing, China; Institute of Medical Support Technology, Academy of Military Science, Tianjin, China; National Innovation Institute of Defense Technology, Academy of Military Sciences, Beijing, China; National Innovation Institute of Defense Technology, Academy of Military Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","9","3508","3523","With the development of very high resolution satellite image acquisition technology, remote sensing scene classification has become an important and challenging task. In this article, aiming at tackling this task, we propose a hybrid architecture, i.e., aggregated deep Fisher feature (ADFF), which can make full use of deep convolutional features' rich semantic information and unsupervised encoding's high robustness. Unlike the previous methods, we first explore the optimal encoding layer in the pretraining CNN model, which naturally fuses the local and global image information in a novel way, making the ability of semantic acquisition further enhanced. ADFF can learn more suitable internal features from the remote sensing data, boosting the final performance. We evaluate our algorithm based on several public datasets, and the results show that our approach achieves superior performance compared with the state-of-the-art methods. The proposed ADFF obtains average classification accuracy of 98.81%, 95.21%, 86.01%, and 88.79%, respectively, on the UC Merced Land-Use, RSSCN7, NWPU-RESISC45 (10% for training), and NWPU-RESISC45 (20% for training) datasets.","","","10.1109/JSTARS.2019.2934165","Tianjin Natural Science Foundation of China; Tianjin Science and Technology Support Project Key R&D Program Funding; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8821369","Aggregated deep Fisher feature (ADFF);deep convolutional features;unsupervised encoding;VHR remote sensing scene classification","Feature extraction;Remote sensing;Encoding;Semantics;Image coding;Learning systems;Task analysis","convolutional neural nets;geophysical image processing;image classification;image coding;image resolution;image sensors;learning (artificial intelligence);remote sensing","semantic acquisition;ADFF;remote sensing data;VHR remote sensing scene classification;high resolution satellite image acquisition technology;deep convolutional features;unsupervised encoding;optimal encoding layer;image information;deep Fisher feature aggregation;pretraining CNN model;UC Merced Land-Use;RSSCN7;NWPU-RESISC45","","","67","Traditional","","","","IEEE","IEEE Journals"
"The Value of SMAP for Long-Term Soil Moisture Estimation With the Help of Deep Learning","K. Fang; M. Pan; C. Shen","Department of Civil and Environmental Engineering, Pennsylvania State University, University Park, PA, USA; Department of Civil and Environmental Engineering, Princeton University, Princeton, NJ, USA; Department of Civil and Environmental Engineering, Pennsylvania State University, University Park, PA, USA","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","4","2221","2233","The Soil Moisture Active Passive (SMAP) mission measures important soil moisture data globally. SMAP's products might not always perform better than land surface models (LSM) when evaluated against in situ measurements. However, we hypothesize that SMAP presents added value for long-term soil moisture estimation in a data fusion setting as evaluated by in situ data. Here, with the help of a time series deep learning (DL) method, we created a seamlessly extended SMAP data set to test this hypothesis and, importantly, gauge whether such benefits extend to years beyond SMAP's limited lifespan. We first show that the DL model, called long short-term memory (LSTM), can extrapolate SMAP for several years and the results are similar to the training period. We obtained prolongation results with low-performance degradation where SMAP itself matches well with in situ data. Interannual trends of root-zone soil moisture are surprisingly well captured by LSTM. In some cases, LSTM's performance is limited by SMAP, whose main issue appears to be its shallow sensing depth. Despite this limitation, a simple average between LSTM and an LSM Noah frequently outperforms Noah alone. Moreover, Noah combined with LSTM is more skillful than when it is combined with another LSM. Over sparsely instrumented sites, the Noah-LSTM combination shows a stronger edge. Our results verified the value of LSTM-extended SMAP data. Moreover, DL is completely data driven and does not require structural assumptions. As such, it has its unique potential for long-term projections and may be applied synergistically with other model-data integration techniques.","","","10.1109/TGRS.2018.2872131","Penn State Institute of CyberScience; National Science Foundation; National Aeronautics and Space Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8497052","Deep learning (DL);hindcasting;Soil Moisture Active Passive (SMAP);soil moisture","Soil moisture;Moisture;Data models;Atmospheric modeling;Satellite broadcasting;Land surface","hydrological techniques;learning (artificial intelligence);moisture;remote sensing;sensor fusion;soil;time series","long-term soil moisture estimation;Soil Moisture Active Passive mission;data fusion setting;time series deep learning method;root-zone soil moisture;Noah-LSTM combination;LSTM-extended SMAP data;soil moisture data;SMAP products;SMAP mission;land surface models;model-data integration techniques","","","50","","","","","IEEE","IEEE Journals"
"Hybrid Deep-Learning-Based Anomaly Detection Scheme for Suspicious Flow Detection in SDN: A Social Multimedia Perspective","S. Garg; K. Kaur; N. Kumar; J. J. P. C. Rodrigues","Department of Electrical Engineering, École de technologie supérieure, Université du Québec, Montréal, QC, Canada; Department of Electrical Engineering, École de technologie supérieure, Université du Québec, Montréal, QC, Canada; Department of Computer Science and Engineering, Thapar Institute of Engineering and Technology (deemed to be university), Patiala, India; National Institute of Telecommunications (Inatel), Santa Rita do Sapuca, Brazil","IEEE Transactions on Multimedia","","2019","21","3","566","578","The continuous development and usage of multi-media-based applications and services have contributed to the exponential growth of social multimedia traffic. In this context, secure transmission of data plays a critical role in realizing all of the key requirements of social multimedia networks such as reliability, scalability, quality of information, and quality of service (QoS). Thus, a trust-based paradigm for multimedia analytics is highly desired to meet the increasing user requirements and deliver more timely and actionable insights. In this regard, software-defined networks (SDNs) play a vital role; however, several factors such as as-runtime security, and energy-aware networking limit its capabilities to facilitate efficient network control and management. Thus, with the view to enhance the reliability of the SDN, a hybrid deep-learning-based anomaly detection scheme for suspicious flow detection in the context of social multimedia is proposed. It consists of the following two modules: (1) an anomaly detection module that leverages improved restricted Boltzmann machine and gradient descent-based support vector machine to detect the abnormal activities, and (2) an end-to-end data delivery module to satisfy strict QoS requirements of the SDN, that is, high bandwidth and low latency. Finally, the proposed scheme has been experimentally evaluated on both real-time and benchmark datasets to prove its effectiveness and efficiency in terms of anomaly detection and data delivery essential for social multimedia. Further, a large-scale analysis over a Carnegie Mellon University (CMU)-based insider threat dataset has been conducted to identify its performance in terms of detecting malicious events such as-Identity theft, profile cloning, confidential data collection, etc.","","","10.1109/TMM.2019.2893549","National Funding from the FCT—Fundação para a Ciência e a Tecnologia; Finep, with resources from Funttel; Centro de Referência em Radiocomunicações—CRR project of the Instituto Nacional de Telecomunicações (Inatel), Brazil; Brazilian National Council for Research and Development (CNPq); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613868","Anomaly detection;deep learning;flow routing;software defined networks;social multimedia","Anomaly detection;Security;Social network services;Iron;Routing;Support vector machines;Quality of service","Boltzmann machines;computer network security;gradient methods;learning (artificial intelligence);multimedia computing;multimedia systems;social networking (online);software defined networking;support vector machines;telecommunication computing","Carnegie Mellon University;deep-learning-ased anomaly detection;secure transmission;Boltzmann machine;trust-based paradigm;social multimedia networks;gradient descent-based support vector machine;anomaly detection module;SDN;energy-aware networking;software-defined networks","","12","46","","","","","IEEE","IEEE Journals"
"A Remote Free-Head Pupillometry Based on Deep Learning and Binocular System","Y. Ni; B. Sun","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China","IEEE Sensors Journal","","2019","19","6","2362","2369","Objective: Pupillometer plays a key role in a variety of research areas, including disease diagnosis, human-machine interaction, and education. Here, we set out to leverage the deep learning theory to develop a remote binocular vision system for pupil diameter estimation. Approach: the system consists of three parts: eye detection, eye tracking, and pupil diameter estimation. We first train a convolutional neural network based on YOLO V2 to perform eye detection, leading to high accuracy and robustness under ambient light interference. By exploring the similarity of binocular camera images, we then propose a master-slave structure for eye tracking, surpassing the traditional parallel structure in tracking speed while keeping considerable accuracy. Furthermore, we develop a pupil diameter estimation algorithm based on binocular vision, avoiding the personal calibration procedure and reducing the measurement distortion error. Main results: Experimental results on real datasets reveal that our system exhibits the state-of-the-art performance with high eye detection accuracy (90.6%), fast eye tracking speed (<;11 ms per frame), low pupil diameter estimation error [(0.022±0.017) mm mean absolute error, and (0.6 ± 0.7)% percentage of the mean absolute error] and excellent flexibility. Significance: in contrast with previous pupillometers, which lead to pupil diameter measurement distortion error through a 2-D projection image on a single camera, our system measures pupil diameter in 3-D space without distortion influence, thus improving its robustness to head angle variation and making it more practical for real applications.","","","10.1109/JSEN.2018.2885355","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8565941","Pupil diameter (PD);deep learning;eye detection;master-slave structure;eye tracking;pupil detection;binocular vision","Cameras;Gaze tracking;Feature extraction;Estimation;Clustering algorithms;Calibration","biomedical equipment;biomedical measurement;calibration;cameras;diameter measurement;diseases;eye;learning (artificial intelligence);neural nets;user interfaces","high eye detection accuracy;low pupil diameter estimation error;mean absolute error;pupil diameter measurement distortion error;2-D projection image;system measures pupil diameter;remote free-head pupillometry;binocular system;disease diagnosis;human-machine interaction;deep learning theory;remote binocular vision system;eye tracking;convolutional neural network;YOLO V2;ambient light interference;binocular camera images;master-slave structure;traditional parallel structure;pupil diameter estimation algorithm","","1","29","","","","","IEEE","IEEE Journals"
"Caching Transient Data for Internet of Things: A Deep Reinforcement Learning Approach","H. Zhu; Y. Cao; X. Wei; W. Wang; T. Jiang; S. Jin","Wuhan National Laboratory for Optoelectronics, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China","IEEE Internet of Things Journal","","2019","6","2","2074","2083","Connected devices in Internet-of-Things (IoT) continuously generate enormous amount of data, which is transient and would be requested by IoT application users, such as autonomous vehicles. Transmitting IoT data through wireless networks would lead to congestions and long delays, which can be tackled by caching IoT data at the network edge. However, it is challenging to jointly consider IoT data-transiency and dynamic context characteristics. In this paper, we advocate the use of deep reinforcement learning (DRL) to solve the problem of caching IoT data at the edge without knowing future IoT data popularity, user request pattern, and other context characteristics. By defining data freshness metrics, the aim of determining IoT data caching policy is to strike a balance between the communication cost and the loss of data freshness. Extensive simulation results corroborate that the proposed DRL-based IoT data caching policy outperforms other baseline policies.","","","10.1109/JIOT.2018.2882583","National Natural Science Foundation of China; Major Program of National Natural Science Foundation of Hubei in China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8542696","Caching;data-transiency;deep reinforcement learning (DRL);Internet-of-Things (IoT)","Transient analysis;Data integrity;Monitoring;Internet of Things;Quality of service;Sensors","cache storage;Internet of Things;learning (artificial intelligence)","IoT data popularity;DRL-based IoT data caching policy;data freshness metrics;user request pattern;deep reinforcement learning;IoT data-transiency;IoT application users;Internet-of-Things","","2","27","","","","","IEEE","IEEE Journals"
"Blockchain-Based Software-Defined Industrial Internet of Things: A Dueling Deep  ${Q}$ -Learning Approach","C. Qiu; F. R. Yu; H. Yao; C. Jiang; F. Xu; C. Zhao","Key Laboratory of University Wireless Communication, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Department of Aerospace Engineering, Tsinghua University, Beijing, China; Key Laboratory of Universal Wireless Communication, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communication, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Internet of Things Journal","","2019","6","3","4627","4639","With the developments of communication technologies and smart manufacturing, Industrial Internet of Things (IIoT) has emerged. Software-defined networking (SDN), a promising paradigm shift, has provided a viable way to manage IIoT dynamically, called software-defined IIoT (SDIIoT). In SDIIoT, lots of data and flows are generated by industrial devices, where a physically distributed but logically centralized control plane is necessary. However, one of the most intractable problems is how to reach consensus among multiple controllers under complex industrial environments. In this paper, we propose a blockchain (BC)-based consensus protocol in SDIIoT, along with detailed consensus steps and theoretical analysis, where BC works as a trusted third party to collect and synchronize network-wide views between different SDN controllers. Specially, it is a permissioned BC. In order to improve the throughput of this BC-based SDIIoT, we jointly consider the trust features of BC nodes and controllers, as well as the computational capability of the BC system. Accordingly, we formulate view change, access selection, and computational resources allocation as a joint optimization problem. We describe this problem as a Markov decision process by defining state space, action space, and reward function. Due to the fact that it is difficult to solve this joint problem by traditional methods, we propose a novel dueling deep Q-learning approach. Simulation results are presented to show the effectiveness of our proposed scheme.","","","10.1109/JIOT.2018.2871394","National Natural Science Foundation of China; BUPT Excellent Ph.D. Students Foundation; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8469030","Blockchain (BC);dueling deep Q-learning (DQL);Industrial Internet of Things (IIoT);multiple controllers;software-defined networking (SDN)","Protocols;Internet of Things;Computational modeling;Synchronization;Computer architecture;Aerospace electronics","centralised control;cryptocurrencies;cryptographic protocols;distributed control;industrial control;learning (artificial intelligence);Markov processes;optimisation;resource allocation;software defined networking;synchronisation;telecommunication control","dueling deep Q-learning approach;smart manufacturing;software-defined networking;industrial devices;complex industrial environments;blockchain-based consensus protocol;permissioned BC;BC-based SDIIoT;BC system;SDN controllers;physically distributed control plane;logically centralized control plane;software-defined IIoT;software-defined industrial Internet of Things;computational resource allocation;Markov decision process","","8","40","","","","","IEEE","IEEE Journals"
"Rover-IRL: Inverse Reinforcement Learning With Soft Value Iteration Networks for Planetary Rover Path Planning","M. Pflueger; A. Agha; G. S. Sukhatme","Department of Computer Science, University of Southern California, Los Angeles, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Department of Computer Science, University of Southern California, Los Angeles, CA, USA","IEEE Robotics and Automation Letters","","2019","4","2","1387","1394","Planetary rovers, such as those currently on Mars, face difficult path planning problems, both before landing during the mission planning stages as well as once on the ground. In this work, we present a new approach to these planning problems based on inverse reinforcement learning using deep convolutional networks and value iteration networks (VIN) as important internal structures. VIN are an approximation of the value iteration (VI) algorithm implemented with convolutional neural networks to make VI fully differentiable. We propose a modification to the value iteration recurrence, referred to as the soft value iteration network (SVIN). SVIN is designed to produce more effective training gradients through the VIN. It relies on an internal soft policy model, where the policy is represented with a probability distribution over all possible actions, rather than a deterministic policy that returns only the best action. We demonstrate the effectiveness of our proposed architecture in both a grid world dataset as well as a highly realistic synthetic dataset generated from currently deployed rover mission planning tools and real Mars imagery.","","","10.1109/LRA.2019.2895892","Achievement Rewards for College Scientists Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629318","Space robotics and automation;deep learning in robotics and automation;learning from demonstration","Planning;Reinforcement learning;Space vehicles;Orbits;Navigation;Path planning;Mars","control engineering computing;convolutional neural nets;learning (artificial intelligence);mobile robots;path planning;planetary rovers;probability","inverse reinforcement learning;deep convolutional networks;VIN;convolutional neural networks;soft value iteration network;internal soft policy model;rover-IRL;planetary rover path planning;rover mission;internal structures;probability distribution;deterministic policy;grid world dataset;realistic synthetic dataset;rover mission planning tools;real Mars imagery","","3","20","","","","","IEEE","IEEE Journals"
"Adversarial Attacks on Deep-Learning Based Radio Signal Classification","M. Sadeghi; E. G. Larsson","Department of Electrical Engineering (ISY), Linköping University, Linköping, Sweden; Department of Electrical Engineering (ISY), Linköping University, Linköping, Sweden","IEEE Wireless Communications Letters","","2019","8","1","213","216","Deep learning (DL), despite its enormous success in many computer vision and language processing applications, is exceedingly vulnerable to adversarial attacks. We consider the use of DL for radio signal (modulation) classification tasks, and present practical methods for the crafting of white-box and universal black-box adversarial attacks in that application. We show that these attacks can considerably reduce the classification performance, with extremely small perturbations of the input. In particular, these attacks are significantly more powerful than classical jamming attacks, which raises significant security and robustness concerns in the use of DL-based algorithms for the wireless physical layer.","","","10.1109/LWC.2018.2867459","ELLIIT; Security-Link; Swedish Foundation for Strategic Research (SSF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449065","Adversarial attacks;deep learning;wireless security;modulation classification;neural networks","Perturbation methods;Modulation;Wireless communication;Signal to noise ratio;Computational modeling;Computer vision;Robustness","jamming;learning (artificial intelligence);radio networks;signal classification;telecommunication computing;telecommunication security","computer vision;language processing applications;radio signal classification tasks;classification performance;classical jamming attacks;DL-based algorithms;deep learning based radio signal classification;white-box crafting;universal black-box adversarial attack;wireless physical layer","","6","11","","","","","IEEE","IEEE Journals"
"Reliable Label-Efficient Learning for Biomedical Image Recognition","Y. Gu; M. Shen; J. Yang; G. Yang","Institute of Image Processing and Pattern Recognition and School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; Hamlyn Centre for Robotic SurgeryImperial College London; Institute of Image Processing and Pattern Recognition and School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; Hamlyn Centre for Robotic SurgeryImperial College London","IEEE Transactions on Biomedical Engineering","","2019","66","9","2423","2432","The use of deep neural networks for biomedical image analysis requires a sufficient number of labeled datasets. To acquire accurate labels as the gold standard, multiple observers with specific expertise are required for both annotation and proofreading. This process can be time-consuming and labor-intensive, making high-quality, and large-annotated biomedical datasets difficult. To address this problem, we propose a deep active learning framework that enables the active selection of both informative queries and reliable experts. To measure the uncertainty of the unlabeled data, a dropout-based strategy is integrated with a similarity criterion for both data selection and random error elimination. To select the reliable labelers, we adopt an expertise estimator to learn the expertise levels of labelers via offline-testing and online consistency evaluation. The proposed method is applied to classification tasks on two types of medical images including confocal endomicroscopy images and gastrointestinal endoscopic images. The annotations are acquired from multiple labelers with diverse levels of expertise. The experiments demonstrate the efficiency and promising performance of the proposed method compared to a set of baseline methods.","","","10.1109/TBME.2018.2889915","NSFC, China; Committee of Science and Technology, Shanghai, China; 973 Plan, China; Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8590767","Deep neural networks;active learning;diverse-level experts","Task analysis;Reliability;Neural networks;Medical diagnostic imaging;Biomedical measurement;Feature extraction","biomedical optical imaging;endoscopes;image classification;image recognition;image segmentation;learning (artificial intelligence);medical image processing;neural nets","large-annotated biomedical datasets;deep active learning framework;informative queries;dropout-based strategy;data selection;random error elimination;confocal endomicroscopy images;gastrointestinal endoscopic images;biomedical image recognition;deep neural networks;biomedical image analysis;reliable label-efficient learning","","","36","Traditional","","","","IEEE","IEEE Journals"
"Performance Optimization for Blockchain-Enabled Industrial Internet of Things (IIoT) Systems: A Deep Reinforcement Learning Approach","M. Liu; F. R. Yu; Y. Teng; V. C. M. Leung; M. Song","Beijing Key Laboratory of Space-ground Interconnection and Convergence, Beijing University of Posts and Telecommunications, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Beijing Key Laboratory of Space-ground Interconnection and Convergence, Beijing University of Posts and Telecommunications, Beijing, China; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Beijing Key Laboratory of Space-ground Interconnection and Convergence, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Industrial Informatics","","2019","15","6","3559","3570","Recent advances in the industrial Internet of things (IIoT) provide plenty of opportunities for various industries. To address the security and efficiency issues of the massive IIoT data, blockchain is widely considered as a promising solution to enable data storing/processing/sharing in a secure and efficient way. To meet the high throughput requirement, this paper proposes a novel deep reinforcement learning (DRL)-based performance optimization framework for blockchain-enabled IIoT systems, the goals of which are threefold: 1) providing a methodology for evaluating the system from the aspects of scalability, decentralization, latency, and security; 2) improving the scalability of the underlying blockchain without affecting the system's decentralization, latency, and security; and 3) designing a modulable blockchain for IIoT systems, where the block producers, consensus algorithm, block size, and block interval can be selected/adjusted using the DRL technique. Simulations results show that our proposed framework can effectively improve the performance of blockchain-enabled IIoT systems and well adapt to the dynamics of the IIoT.","","","10.1109/TII.2019.2897805","National Key R&D Program of China; National Natural Science Foundation of China; Beijing Natural Science Foundation; Beijing Major Science and Technology Special Projects; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636255","Blockchain;deep reinforcement learning (DRL);industrial Internet of Things (IIoT);performance optimization","Blockchain;Security;Scalability;Optimization;Internet of Things;Throughput;Performance analysis","cryptography;distributed databases;Internet of Things;learning (artificial intelligence);neural nets;optimisation;production engineering computing","blockchain-enabled IIoT systems;data storing;blockchain-enabled industrial Internet of Things systems;deep reinforcement learning;performance optimization;data processing;data sharing;DRL","","5","52","","","","","IEEE","IEEE Journals"
"Low–High-Power Consumption Architectures for Deep-Learning Models Applied to Hyperspectral Image Classification","J. M. Haut; S. Bernabé; M. E. Paoletti; R. Fernandez-Beltran; A. Plaza; J. Plaza","Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Computer Architecture and Automation, Complutense University, Madrid, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain","IEEE Geoscience and Remote Sensing Letters","","2019","16","5","776","780","Convolutional neural networks have emerged as an excellent tool for remotely sensed hyperspectral image (HSI) classification. Nonetheless, the high computational complexity and energy requirements of these models typically limit their application in on-board remote sensing scenarios. In this context, low-power consumption architectures are promising platforms that may provide acceptable on-board computing capabilities to achieve satisfactory classification results with reduced energy demand. For instance, the new NVIDIA Jetson Tegra TX2 device is an efficient solution for on-board processing applications using deep-learning (DL) approaches. So far, very few efforts have been devoted to exploiting this or other similar computing platforms in on-board remote sensing procedures. This letter explores the use of low-power consumption architectures and DL algorithms for HSI classification. The conducted experimental study reveals that the NVIDIA Jetson Tegra TX2 device offers a good choice in terms of performance, cost, and energy consumption for on-board HSI classification tasks.","","","10.1109/LGRS.2018.2881045","Ministerio de Educación, Cultura y Deporte; Consejería de Educación y Empleo, Junta de Extremadura; EU (FEDER); Secretaría de Estado de Investigación, Desarrollo e Innovación; Generalitat Valenciana; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554064","Deep learning (DL);embedded computing;hyperspectral image (HSI) classification;low-power consumption architectures","Graphics processing units;Computer architecture;Hardware;Performance evaluation;Energy consumption;Hyperspectral sensors","computational complexity;convolutional neural nets;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);power consumption;remote sensing","low-high-power consumption architectures;convolutional neural networks;remotely sensed hyperspectral image classification;energy requirements;on-board computing capabilities;reduced energy demand;NVIDIA Jetson Tegra TX2 device;on-board processing applications;on-board remote sensing procedures;energy consumption;on-board HSI classification;computational complexity;computing platforms;deep-learning models","","1","16","","","","","IEEE","IEEE Journals"
"Deep Constrained Low-Rank Subspace Learning for Multi-View Semi-Supervised Classification","Z. Xue; J. Du; D. Du; G. Li; Q. Huang; S. Lyu","Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Department of Computer Science, University at Albany, State University of New York, Albany, NY, USA; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science, University at Albany, State University of New York, Albany, NY, USA","IEEE Signal Processing Letters","","2019","26","8","1177","1181","Semi-supervised classification receives increasing interests because it can predict class labels based on both limited labeled and sufficient unlabeled data. In this letter, we propose a deep constrained low-rank subspace learning (DCLSL) method for multi-view semi-supervised classification. Specifically, we integrate deep constrained matrix factorization, low-rank subspace learning, and class label learning into a unified objective function to jointly learn data similarity matrices and class label matrix. DCLSL is able to obtain the discriminative subspace representation of each view and effectively aggregate similarity matrices of multiple views, resulting in better classification performance. Experimental results on various datasets demonstrate the effectiveness of our method.","","","10.1109/LSP.2019.2923857","National Natural Science Foundation of China; Fundamental Research Funds for the Central University; Director Foundation of Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia; U.S. National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8740894","Multi-view data;deep matrix factorization;low-rank subspace;semi-supervised classification","Symmetric matrices;Matrix decomposition;Linear programming;Aggregates;Computer science;Data models;Semisupervised learning","learning (artificial intelligence);matrix decomposition;pattern classification","multiview semisupervised classification;labeled data;low-rank subspace learning method;data similarity matrices;class label matrix;discriminative subspace representation;classification performance;unlabeled data;DCLSL method","","","34","","","","","IEEE","IEEE Journals"
"Physics-Based Convolutional Neural Network for Fault Diagnosis of Rolling Element Bearings","M. Sadoughi; C. Hu","Department of Mechanical Engineering, Iowa State University, Ames, IA, USA; Departments of Mechanical Engineering and Electrical and Computer Engineering, Iowa State University, Ames, IA, USA","IEEE Sensors Journal","","2019","19","11","4181","4192","During the past few years, deep learning has been recognized as a useful tool in condition monitoring and fault detection of rolling element bearings. Although existing deep learning approaches are able to intelligently detect and classify the faults in bearings, they still face one or both of the following challenges: 1) most of these approaches rely exclusively on data and do not incorporate physical knowledge into the learning and prediction processes and 2) the approaches often focus on the fault diagnosis of a single bearing in a rotating machine, while in reality, a rotating machine may contain multiple bearings. To address these challenges, this paper proposes a novel approach, namely physics-based convolutional neural network (PCNN), for fault diagnosis of rolling element bearings. In PCNN, an exclusively data-driven deep learning approach, called CNN, is carefully modified to incorporate useful information from physical knowledge about bearings and their fault characteristics. To this end, the proposed approach 1) utilizes spectral kurtosis and envelope analysis to extract sidebands from raw sensor signals and minimize non-transient components of the signals and 2) feeds the information about the fault characteristics into the CNN model. With the capability to process signals from multiple sensors, the proposed PCNN approach is capable of concurrently monitoring multiple bearings and detecting faults in these bearings. The performance of PCNN in machinery fault diagnosis is compared with that of traditional machine learning- and deep learning-based approaches reported in the literature.","","","10.1109/JSEN.2019.2898634","National Science Foundation; Iowa State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8638772","Deep learning;rotating machinery;rolling element bearing;condition-based maintenance;fault detection","Fault diagnosis;Vibrations;Deep learning;Rolling bearings;Machinery;Sensor phenomena and characterization","condition monitoring;convolutional neural nets;data analysis;fault diagnosis;learning (artificial intelligence);machinery;rolling bearings;vibrational signal processing","physics-based convolutional neural network;rolling element bearings;fault detection;rotating machine;fault characteristics;PCNN approach;machinery fault diagnosis;data-driven deep learning approach;raw sensor signals","","2","35","","","","","IEEE","IEEE Journals"
"Deep Learning Movement Intent Decoders Trained With Dataset Aggregation for Prosthetic Limb Control","H. Dantas; D. J. Warren; S. M. Wendelken; T. S. Davis; G. A. Clark; V. J. Mathews","School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, USA; Department of BioengineeringUniversity of Utah; Department of BioengineeringUniversity of Utah; Department of NeurosurgeryUniversity of Utah; Department of BioengineeringUniversity of Utah; School of Electrical Engineering and Computer ScienceOregon State University","IEEE Transactions on Biomedical Engineering","","2019","66","11","3192","3203","Significance: The performance of traditional approaches to decoding movement intent from electromyograms (EMGs) and other biological signals commonly degrade over time. Furthermore, conventional algorithms for training neural network based decoders may not perform well outside the domain of the state transitions observed during training. The work presented in this paper mitigates both these problems, resulting in an approach that has the potential to substantially improve the quality of life of the people with limb loss. Objective: This paper presents and evaluates the performance of four decoding methods for volitional movement intent from intramuscular EMG signals. Methods: The decoders are trained using the dataset aggregation (DAgger) algorithm, in which the training dataset is augmented during each training iteration based on the decoded estimates from previous iterations. Four competing decoding methods, namely polynomial Kalman filters (KFs), multilayer perceptron (MLP) networks, convolutional neural networks (CNN), and long short-term memory (LSTM) networks, were developed. The performances of the four decoding methods were evaluated using EMG datasets recorded from two human volunteers with transradial amputation. Short-term analyses, in which the training and cross-validation data came from the same dataset, and long-term analyses, in which the training and testing were done in different datasets, were performed. Results: Short-term analyses of the decoders demonstrated that CNN and MLP decoders performed significantly better than KF and LSTM decoders, showing an improvement of up to 60% in the normalized mean-square decoding error in cross-validation tests. Long-term analyses indicated that the CNN, MLP, and LSTM decoders performed significantly better than a KF-based decoder at most analyzed cases of temporal separations (0-150 days) between the acquisition of the training and testing datasets. Conclusion: The short-term and long-term performances of MLP- and CNN-based decoders trained with DAgger demonstrated their potential to provide more accurate and naturalistic control of prosthetic hands than alternate approaches.","","","10.1109/TBME.2019.2901882","National Science Foundation; Hand Proprioception and Touch Interfaces; Space and Naval Warfare Systems Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653382","Biomedical signal processing;deep learning;Kalman filter;learning systems;machine learning;motor intent decoder;neural engineering;neural network;reinforcement learning","Decoding;Training;Electromyography;Kinematics;Testing;Task analysis;Biology","biomechanics;decoding;electromyography;Kalman filters;learning (artificial intelligence);medical control systems;medical signal processing;multilayer perceptrons;neural nets;neurophysiology;prosthetics","convolutional neural networks;short-term memory networks;EMG datasets;Short-term analyses;long-term analyses;MLP decoders;KF-based decoder;testing datasets;long-term performances;CNN-based decoders;movement intent decoders;prosthetic limb control;decoding movement intent;biological signals;conventional algorithms;training neural network based decoders;limb loss;volitional movement intent;intramuscular EMG signals;dataset aggregation algorithm;training dataset;training iteration;decoded estimates;competing decoding methods;time 0.0 d to 150.0 d","","2","56","Traditional","","","","IEEE","IEEE Journals"
"Label-Consistent Transform Learning for Hyperspectral Image Classification","J. Maggu; H. K. Aggarwal; A. Majumdar","Department of Computer Science, Indraprastha Institute of Information Technology Delhi, New Delhi, India; Department of Computer Science, Indraprastha Institute of Information Technology Delhi, New Delhi, India; Department of Electronics and Communication Engineering, Indraprastha Institute of Information Technology Delhi, New Delhi, India","IEEE Geoscience and Remote Sensing Letters","","2019","16","9","1502","1506","This letter proposes a new image analysis tool called label-consistent transform learning. Transform learning is a recent unsupervised representation learning approach; we add supervision by incorporating a label consistency constraint. The proposed technique is especially suited for hyperspectral image classification problems owing to its ability to learn from fewer samples. We have compared our proposed method with the state-of-the-art techniques such as label-consistent K-singular value decomposition, stacked autoencoder, deep belief network, convolutional neural network, and generative adversarial network. Our method yields considerably better results than all the aforesaid techniques.","","","10.1109/LGRS.2019.2899121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667454","Classification;dictionary learning;transform learning","Transforms;Machine learning;Dictionaries;Hyperspectral imaging;Training;Testing","belief networks;convolutional neural nets;image classification;singular value decomposition;unsupervised learning","deep belief network;convolutional neural network;generative adversarial network;image analysis tool;transform learning;K-singular value decomposition;unsupervised representation learning approach;hyperspectral image classification;autoencoder","","1","33","","","","","IEEE","IEEE Journals"
"Machine Learning-Assisted Analysis of Polarimetric Scattering From Cylindrical Components of Vegetation","H. Chen; C. Yang; Y. Du","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","1","155","165","Reliable and efficient analysis of electromagnetic scattering by cylindrical components of vegetation is important for microwave remote sensing of vegetated terrain. In this paper, we proposed a machine learning (ML) scheme for the analysis of polarimetric bistatic scattering from a finite dielectric cylinder. A deep neural network architecture is adopted in the hope that with increased depth of the neural network, hence increased abstraction capability, it may be able to handle the highly oscillatory scattering patterns to an adequately acceptable degree. The scheme has demonstrated the capability of modifying and adapting itself to capture the complicated polarimetric bistatic scattering patterns of a finite dielectric cylinder. The physical consideration of reciprocity relation is largely fulfilled except for the scattered directions close to the cylinder axis. Moreover and more importantly, for cases where interpolation is expected, the scheme has unambiguously demonstrated the capability of learning the bistatic scattering cross section and phase patterns. The performance is also robust against the number of parameters to be interpolated, be it single or multiple. In summary, the proposed ML scheme bodes well for the design of the future physically based algorithms where the conventional datacube was used as the base for interpolation.","","","10.1109/TGRS.2018.2852644","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8423668","Bistatic scattering;cylindric component;deep neural network (DNN);machine learning (ML);polarimetric scattering","Scattering;Dielectric constant;Training;Neural networks;Machine learning;Interpolation","geophysics computing;learning (artificial intelligence);neural nets;vegetation mapping","machine learning-assisted analysis;cylindrical components;electromagnetic scattering;microwave remote sensing;vegetated terrain;machine learning scheme;finite dielectric cylinder;deep neural network architecture;increased depth;increased abstraction capability;highly oscillatory scattering patterns;complicated polarimetric bistatic scattering patterns;scattered directions;cylinder axis;bistatic scattering cross section;phase patterns;ML scheme","","1","26","","","","","IEEE","IEEE Journals"
"Multi-Branch Spatial-Temporal Network for Action Recognition","Y. Wang; W. Li; R. Tao","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China","IEEE Signal Processing Letters","","2019","26","10","1556","1560","Human action recognition based on deep-learning methods have received increasing attention and developed rapidly. However, current methods suffer from the confusion caused by convolving over time and space independently, processing shorter sequences, restricted to single temporal scale modeling and so on. The key objective of precisely classifying actions is to capture the appearance and motion throughout entire videos. Based on this purpose, a multi-branch spatial-temporal network (MSTN) is proposed. It consists of a multi-branch deep network and a long-term feature (LTF) layer. Benefits of the proposed MSTN include: (a) the multi-branch spatial-temporal network aims at encoding spatial and temporal information simultaneously, and (b) the LTF layer is used to aggregate the video-level representation with multiple temporal scales. Evaluations on two action datasets and comparison with several state-of-the-art approaches demonstrate the effectiveness of the proposed network.","","","10.1109/LSP.2019.2940111","Natural Science Foundation of Beijing Municipality; Beijing Nova Program; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832232","Action recognition;deep learning;spatial-temporal network;long-term feature layer","Videos;Three-dimensional displays;Two dimensional displays;Feature extraction;Biological system modeling;Deep learning;Training","feature extraction;image classification;image motion analysis;image representation;image sequences;learning (artificial intelligence);video signal processing","multibranch spatial-temporal network;spatial information;temporal information;multiple temporal scales;human action recognition;deep-learning methods;temporal scale modeling;multibranch deep network;precise action classification;MSTN;long-term feature layer;LTF layer;video-level representation","","","20","Traditional","","","","IEEE","IEEE Journals"
"A Concise Temporal Data Representation Model for Prediction in Biomedical Wearable Devices","A. Manashty; J. Light Thompson; H. Soleimani","Department of Computer Science, University of New Brunswick, Saint John, NB, Canada; Department of Computer Science, University of New Brunswick, Saint John, NB, Canada; Department of Bioengineering, Imperial College London, London, U.K.","IEEE Internet of Things Journal","","2019","6","2","1438","1445","Predictive analytics and event forecasting using deep learning techniques require processing long-term historical data which is infeasible in low-power wearable devices. Such devices are constrained in memory and computational power, and are pushed to their limits by resource hungry deep neural networks. Current techniques either ignore historical data, or convert temporal sequences to pattern sequences, eliminating valuable properties such as time and/or recency. The proposed model maps arbitrary-length multivariate discrete time series to a concise sequence, called mapped interval sequence (MIS). MIS retains original data properties such as time, recency, and scale, without being susceptible to missing values. Life model for time series (LMts) mapping, is capable of mapping billions of data elements with sampling rate of several kHz or higher into a sequence of 32 elements or fewer. Furthermore, a new loss function called as tolerance error is introduced to improve long-term forecasting events using LMts. In a smart health Internet of Things environment, LMts enables real-time health predictions depending on both recent and historical data. In addition, the LMts model can predict the approximate time of events, with granularity of seconds and up to years. Experimental results show that, compared to previous studies in fall prediction, LMts achieves the same 100% accuracy with a single long short-term memory layer, while covering 16× longer time period and using 80× less weight parameters. LMts is also used to forecast human fall up to 14 s in advance even with 50% missing values.","","","10.1109/JIOT.2018.2863039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424843","Biomedical wearable devices;deep learning;health analytics;Internet of Things (IoT);long short-term memory (LSTM);temporal sequences","Data models;Predictive models;Time series analysis;History;Real-time systems;Biomedical monitoring;Deep learning","Internet of Things;learning (artificial intelligence);medical information systems;neural nets;time series;wearable computers","concise temporal data representation model;biomedical wearable devices;deep learning techniques;resource hungry deep neural networks;temporal sequences;pattern sequences;MIS;fall prediction;short-term memory layer;arbitrary-length multivariate discrete time series;life model time series;smart health Internet of things environment;LMts;mapped interval sequence;tolerance error","","","23","","","","","IEEE","IEEE Journals"
"Vision-Based Estimation of Driving Energy for Planetary Rovers Using Deep Learning and Terramechanics","S. Higa; Y. Iwashita; K. Otsu; M. Ono; O. Lamarre; A. Didier; M. Hoffmann","Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; STARS Laboratory, University of Toronto Institute for Aerospace Studies, Toronto, ON, Canada; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA","IEEE Robotics and Automation Letters","","2019","4","4","3876","3883","This letter presents a prediction algorithm of driving energy for future Mars rover missions. The majority of future Mars rovers would be solar-powered, which would require energy-optimal driving to maximize the range with limited energy. The essential and arguably the most challenging technology for realizing energy-optimal driving is the capability to predict the driving energy, which is needed to construct an energy-aware cost function for path planning. In this letter, we propose vision-based algorithms to remotely predict the driving energy consumption using machine learning. Specifically, we develop and compare two machine-learning models in this letter, namely VeeGer-EnergyNet and Veeger-TerramechanicsNet, respectively. The former is trained directly using recorded power, while the latter estimates terrain parameters from the images using a simplified-terramechanics model, and calculate the power based on the model. The two approaches are fully automated self-supervised learning algorithms. To combine RGB and depth images efficiently with high accuracy, we propose a new network architecture called Two-PNASNet-5, which is based on PNASNet-5. We collected a new dataset to verify the effectiveness of the proposed approaches. Comparison of the two approaches showed that Veeger-TerramechanicsNet had better performance than VeeGer-EnergyNet.","","","10.1109/LRA.2019.2928765","National Aeronautics and Space Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8764007","Space robotics and automation;wheeled robotics;deep learning in robotics and automation","Space exploration;Energy consumption;Space vehicles;Mobile robots;Deep learning","","","","","26","Traditional","","","","IEEE","IEEE Journals"
"Deep Generative Adversarial Neural Networks for Compressive Sensing MRI","M. Mardani; E. Gong; J. Y. Cheng; S. S. Vasanawala; G. Zaharchuk; L. Xing; J. M. Pauly","Electrical Engineering Department, Stanford University, Stanford, CA, USA; Electrical Engineering Department, Stanford University, Stanford, CA, USA; Electrical Engineering Department, Stanford University, Stanford, CA, USA; Radiology Department, Stanford University, Stanford, CA, USA; Radiology Department, Stanford University, Stanford, CA, USA; Electrical Engineering Department, Stanford University, Stanford, CA, USA; Electrical Engineering Department, Stanford University, Stanford, CA, USA","IEEE Transactions on Medical Imaging","","2019","38","1","167","179","Undersampled magnetic resonance image (MRI) reconstruction is typically an ill-posed linear inverse task. The time and resource intensive computations require tradeoffs between accuracy and speed. In addition, state-of-the-art compressed sensing (CS) analytics are not cognizant of the image diagnostic quality. To address these challenges, we propose a novel CS framework that uses generative adversarial networks (GAN) to model the (low-dimensional) manifold of high-quality MR images. Leveraging a mixture of least-squares (LS) GANs and pixel-wise ℓ1/ℓ2 cost, a deep residual network with skip connections is trained as the generator that learns to remove the aliasing artifacts by projecting onto the image manifold. The LSGAN learns the texture details, while the ℓ1/ℓ2 cost suppresses high-frequency noise. A discriminator network, which is a multilayer convolutional neural network (CNN), plays the role of a perceptual cost that is then jointly trained based on high-quality MR images to score the quality of retrieved images. In the operational phase, an initial aliased estimate (e.g., simply obtained by zero-filling) is propagated into the trained generator to output the desired reconstruction. This demands a very low computational overhead. Extensive evaluations are performed on a large contrast-enhanced MR dataset of pediatric patients. Images rated by expert radiologists corroborate that GANCS retrieves higher quality images with improved fine texture details compared with conventional Wavelet-based and dictionary-learning-based CS schemes as well as with deep-learning-based schemes using pixel-wise training. In addition, it offers reconstruction times of under a few milliseconds, which are two orders of magnitude faster than the current state-of-the-art CS-MRI schemes.","","","10.1109/TMI.2018.2858752","National Institutes of Health; Subtle Medical Inc; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417964","Deep learning;generative adversarial networks (GAN);convolutional neural networks (CNN);rapid reconstruction;diagnostic quality;compressed sensing (CS)","Image reconstruction;Manifolds;Gallium nitride;Magnetic resonance imaging;Training;Generators;Neural networks","biomedical MRI;compressed sensing;data compression;feedforward neural nets;image enhancement;image reconstruction;image representation;image resolution;image sampling;image texture;learning (artificial intelligence);medical image processing;wavelet transforms","generative adversarial neural networks;compressive sensing;linear inverse task;resource intensive computations;state-of-the-art compressed sensing analytics;image diagnostic quality;novel CS framework;generative adversarial networks;GAN;deep residual network;skip connections;aliasing artifacts;image manifold;discriminator network;multilayer convolutional neural network;perceptual cost;retrieved images;initial aliased estimate;trained generator;desired reconstruction;low computational overhead;higher quality images;improved fine texture details;CS schemes;deep-learning-based schemes;pixel-wise training;reconstruction times;current state-of-the-art CS-MRI schemes;high-frequency noise;wavelet-based schemes;undersampled magnetic resonance image reconstruction","","4","36","","","","","IEEE","IEEE Journals"
"High-Resolution Aerial Images Semantic Segmentation Using Deep Fully Convolutional Network With Channel Attention Mechanism","H. Luo; C. Chen; L. Fang; X. Zhu; L. Lu","College of Mathematics and Computer Sciences, Fuzhou University, Fuzhou, China; College of Mathematics and Computer Sciences, Fuzhou University, Fuzhou, China; Spatial Information Research Center of Fujian, Fuzhou University, Fujian, China; Spatial Information Research Center of Fujian, Fuzhou University, Fujian, China; Spatial Information Research Center of Fujian, Fuzhou University, Fujian, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","9","3492","3507","Semantic segmentation is one of the fundamental tasks in understanding high-resolution aerial images. Recently, convolutional neural network (CNN) and fully convolutional network (FCN) have achieved excellent performance in general images' semantic segmentation tasks and have been introduced to the field of aerial images. In this paper, we propose a novel deep FCN with channel attention mechanism (CAM-DFCN) for high-resolution aerial images' semantic segmentation. The CAM-DFCN architecture follows the mode of encoder-decoder. In the encoder, two identical deep residual networks are both divided into multiple levels and acted on spectral images and auxiliary data, respectively. Then, the feature map concatenation is carried out at each level. In the decoder, the channel attention mechanism (CAM) is introduced to automatically weigh the channels of feature maps to perform feature selection. On the one hand, the CAM follows the concatenated feature maps at each level to select more discriminative features for classification. On the other hand, the CAM is used to further weigh the semantic information and spatial location information in the adjacent-level concatenated feature maps for more accurate predictions. We evaluate the proposed CAM-DFCN by using two benchmarks (the Potsdam set and the Vaihingen set) provided by the International Society for Photogrammetry and Remote Sensing. Experimental results show that the proposed method has considerable improvement.","","","10.1109/JSTARS.2019.2930724","National Basic Research Program of China (973 Program); Young Scientists Fund of the National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792383","Channel attention mechanism (CAM);convolutional neural networks (CNNs);deep learning;fully convolutional networks (FCNs);high-resolution aerial images;semantic segmentation","Semantics;Image segmentation;Feature extraction;Decoding;Deep learning;Training;Spatial resolution","convolutional neural nets;feature selection;geophysical image processing;image classification;image resolution;image segmentation","deep fully convolutional network;channel attention mechanism;convolutional neural network;CAM-DFCN architecture;spectral images;feature map concatenation;semantic information;adjacent-level concatenated feature maps;high-resolution aerial image semantic segmentation;deep residual networks;feature selection","","","41","Traditional","","","","IEEE","IEEE Journals"
"Machine Learning in the Air","D. Gündüz; P. de Kerret; N. D. Sidiropoulos; D. Gesbert; C. R. Murthy; M. van der Schaar","Department of Electrical and Electronics Engineering, Information Processing and Communications Laboratory, Imperial College London, London, U.K; Communication Systems Department, EURECOM, Sophia Antipolis, France; Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA, USA; Communication Systems Department, EURECOM, Sophia Antipolis, France; Department of Electrical and Computer Engineering, Indian Institute of Science, Bengaluru, India; Department of Electrical and Computer Engineering, University of California at Los Angeles, Los Angeles, CA, USA","IEEE Journal on Selected Areas in Communications","","2019","37","10","2184","2199","Thanks to the recent advances in processing speed, data acquisition and storage, machine learning (ML) is penetrating every facet of our lives, and transforming research in many areas in a fundamental manner. Wireless communications is another success story - ubiquitous in our lives, from handheld devices to wearables, smart homes, and automobiles. While recent years have seen a flurry of research activity in exploiting ML tools for various wireless communication problems, the impact of these techniques in practical communication systems and standards is yet to be seen. In this paper, we review some of the major promises and challenges of ML in wireless communication systems, focusing mainly on the physical layer. We present some of the most striking recent accomplishments that ML techniques have achieved with respect to classical approaches, and point to promising research directions where ML is likely to make the biggest impact in the near future. We also highlight the complementary problem of designing physical layer techniques to enable distributed ML at the wireless network edge, which further emphasizes the need to understand and connect ML with fundamental concepts in wireless communications.","","","10.1109/JSAC.2019.2933969","European Research Council (ERC) under the European Union’s Horizon 2020 Research and Innovation Program Starting Grant BEACON; ERC under the European Union’s Horizon 2020 Research and Innovation Program; National Science Foundation; Ministry of Electronics and Information technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839651","Autoencoders;channel coding;channel estimation;data-driven methods;distributed learning;distributed resource allocation;deep learning;federated edge learning;joint source-channel coding;machine learning;stochastic approximation;wireless communications","Wireless communication;Data models;Machine learning;Tools;Computers;Physical layer","data acquisition;learning (artificial intelligence);radio networks;telecommunication computing","machine learning;handheld devices;smart homes;ML tools;wireless communication problems;wireless communication systems;ML techniques;physical layer techniques;wireless network edge","","","102","","","","","IEEE","IEEE Journals"
"Learning Visual Similarity for Inspecting Defective Railway Fasteners","J. Liu; Y. Huang; Q. Zou; M. Tian; S. Wang; X. Zhao; P. Dai; S. Ren","Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Beijing Jiaotong University, Beijing, China; Infrastructure Inspection Research Institute, China Academy of Railway Sciences, Beijing, China; Infrastructure Inspection Research Institute, China Academy of Railway Sciences, Beijing, China; Infrastructure Inspection Research Institute, China Academy of Railway Sciences, Beijing, China; Infrastructure Inspection Research Institute, China Academy of Railway Sciences, Beijing, China","IEEE Sensors Journal","","2019","19","16","6844","6857","Vision-based automatic railway fastener inspection, instead of manual operation, remains a great challenge. Even though many supervised learning-based methods have been developed, expensive training labels and imbalanced data are the main obstacles to leverage the performance of the fastener inspection task. To tackle the problems, we present a novel vision-based fastener inspection system (VFIS) which is inspired by few-shot learning. VFIS can automatically collect and annotate a large number of fastener samples by using the proposed online template matching-based classification method, and it only requires a very small number of annotated fastener templates. Moreover, we employ a similarity-based deep network to solve the problem of the imbalanced dataset. The comprehensive experiments are conducted on a large scale fastener dataset. VFIS yields competitive performance on both fastener localization and fastener classification. Specifically, an average detection rate of 99.36% is achieved for fastener localization, and an average accuracy of 92.69% is achieved for fastener classification. Moreover, for identifying defective fasteners, our proposed method achieves an average precision of 92.63% and an average recall of 92.88%.","","","10.1109/JSEN.2019.2911015","Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8689059","Vision inspection;railway fasteners;few-shot learning;online learning;similarity-based deep network","Fasteners;Rail transportation;Inspection;Task analysis;Training;Rails;Visualization","automatic optical inspection;computer vision;fasteners;image classification;image matching;learning (artificial intelligence);mechanical engineering computing;neural nets;object detection;railway engineering","defective fastener identification;VFIS;large scale fastener dataset;defective railway fastener inspection;supervised learning-based methods;vision-based automatic railway fastener inspection;visual similarity;fastener classification;fastener localization;imbalanced dataset;similarity-based deep network;annotated fastener templates;online template matching-based classification method;few-shot learning;vision-based fastener inspection system","","","46","","","","","IEEE","IEEE Journals"
"Hyperspectral Classification Based on Lightweight 3-D-CNN With Transfer Learning","H. Zhang; Y. Li; Y. Jiang; P. Wang; Q. Shen; C. Shen","Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Faculty of Business and Physical Sciences, Aberystwyth University, Aberystwyth, U.K.; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","8","5813","5828","Recently, hyperspectral image (HSI) classification approaches based on deep learning (DL) models have been proposed and shown promising performance. However, because of very limited available training samples and massive model parameters, DL methods may suffer from overfitting. In this paper, we propose an end-to-end 3-D lightweight convolutional neural network (CNN) (abbreviated as 3-D-LWNet) for limited samples-based HSI classification. Compared with conventional 3-D-CNN models, the proposed 3-D-LWNet has a deeper network structure, less parameters, and lower computation cost, resulting in better classification performance. To further alleviate the small sample problem, we also propose two transfer learning strategies: 1) cross-sensor strategy, in which we pretrain a 3-D model in the source HSI data sets containing a greater number of labeled samples and then transfer it to the target HSI data sets and 2) cross-modal strategy, in which we pretrain a 3-D model in the 2-D RGB image data sets containing a large number of samples and then transfer it to the target HSI data sets. In contrast to previous approaches, we do not impose restrictions over the source data sets, in which they do not have to be collected by the same sensors as the target data sets. Experiments on three public HSI data sets captured by different sensors demonstrate that our model achieves competitive performance for HSI classification compared to several state-of-the-art methods.","","","10.1109/TGRS.2019.2902568","Foundation Project for Advanced Research Field of China; National Natural Science Foundation of China; National Key Research and Development Program of China; Northwestern Polytechnical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8685710","3-D lightweight convolutional network (3-D-LWNet);deep learning (DL);hyperspectral classification;transfer learning","Feature extraction;Training;Computational modeling;Solid modeling;Data models;Hyperspectral sensors;Convolution","convolutional neural nets;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence)","deeper network structure;transfer learning strategies;cross-sensor strategy;labeled samples;cross-modal strategy;source data sets;target data sets;public HSI data sets;hyperspectral image classification approaches;deep learning models;3-D-LWNet;3D-CNN model;2D RGB image data sets;limited sample-based HSI classification;3D-CNN model;end-to-end 3D lightweight convolutional neural network","","1","52","","","","","IEEE","IEEE Journals"
"Hyperspectral Image Classification Based on 3-D Separable ResNet and Transfer Learning","Y. Jiang; Y. Li; H. Zhang","Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Shaanxi Provincial Key Lab of Speech and Image Information Processing, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","12","1949","1953","Deep learning (DL) has proven to be a promising technique for hyperspectral image (HSI) classification. However, due to complex network structure and massive parameters, it is challenging to achieve satisfying classification accuracy with only a small number of training samples. In this letter, we propose a novel HSI classification method by collaborating the 3-D separable ResNet (3-D-SRNet) with cross-sensor transfer learning. The 3-D-SRNet replaces 3-D convolutions with spatial and spectral separable 3-D convolutions, thus showing much less parameters than models that use standard 3-D convolutions. First, we pretrain a classification model with the proposed 3-D-SRNet on the source HSI data set with sufficient training samples compared with the target HSI data set. Then, the pretrained model is transferred to the target HSI data set for fine-tuning to finish the classification task. It is worth noting that the source data for pretraining can be captured by the different sensor with the target data. Compared with the conventional 3-D-ResNet, the proposed 3-D-SRNet has less parameters involving lower computation cost while achieving better classification performance. Experimental results on three benchmark data sets show that our method outperforms several state-of-the-art methods in HSI classification with small training samples.","","","10.1109/LGRS.2019.2913011","National Natural Science Foundation of China; Foundation Project for Advanced Research Field of China; Northwestern Polytechnical University; Northwestern Polytechnical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721232","3-D separable ResNet (3-D-SRNet);deep learning (DL);hyperspectral image (HSI) classification;transfer learning","Feature extraction;Training;Hyperspectral imaging;Data models;Solid modeling;Task analysis","geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);neural nets","3-D-ResNet;3-D-SRNet;hyperspectral image classification;3-D separable ResNet;deep learning;HSI classification;transfer learning","","","25","IEEE","","","","IEEE","IEEE Journals"
"Machine Health Monitoring Using Adaptive Kernel Spectral Clustering and Deep Long Short-Term Memory Recurrent Neural Networks","Y. Cheng; H. Zhu; J. Wu; X. Shao","National Engineering Research Center of Digital Manufacturing Equipment, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center of Digital Manufacturing Equipment, Huazhong University of Science and Technology, Wuhan, China; School of Naval Architecture and Ocean Engineering, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center of Digital Manufacturing Equipment, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Industrial Informatics","","2019","15","2","987","997","Machine health monitoring is of great importance in industrial informatics field. Recently, deep learning methods applied to machine health monitoring have been proven effective. However, the existing methods face enormous difficulties in extracting heterogeneous features indicating the variation until failure and revealing the inherent high-dimensional features of massive signals, which affect the accuracy and efficiency of machine health monitoring. In this paper, a novel data-driven machine health monitoring method is proposed using adaptive kernel spectral clustering (AKSC) and deep long short-term memory recurrent neural networks (LSTM-RNN). This method include three steps: First, features in the time domain, frequency domain, and time-frequency domain are, respectively, extracted from massive measured signals. And, an Euclidean distance based algorithm is designed to select degradation features. Second, the AKSC algorithm is introduced to adaptively identify machine anomaly behaviors from multiple degradation features. Third, a new deep learning model (LSTM-RNN) is constructed to update and predict the failure time of the machine. The effectiveness of the proposed method is validated using a set of test-to-failure experimental data. The results show that the performance of the proposed method is competitive with other existing methods.","","","10.1109/TII.2018.2866549","National Natural Science Foundation of China; Foundation of the National Key Intergovernmental Special Project Development Plan of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444085","Adaptive kernel spectral clustering (AKSC);anomaly detection;deep long short-term memory recurrent neural networks (LSTM-RNN);failure prognostics;machine health monitoring","Feature extraction;Monitoring;Frequency-domain analysis;Machine learning;Time-domain analysis;Degradation","condition monitoring;learning (artificial intelligence);mechanical engineering computing;pattern clustering;recurrent neural nets","adaptive kernel spectral clustering;deep long short-term memory recurrent neural networks;deep learning methods;novel data-driven machine health monitoring method","","7","40","","","","","IEEE","IEEE Journals"
"Remote Sensing Single-Image Superresolution Based on a Deep Compendium Model","J. M. Haut; M. E. Paoletti; R. Fernandez-Beltran; J. Plaza; A. Plaza; J. Li","Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Guangdong Provincial Key Laboratory of Urbanization and Geosimulation, Center of Integrated Geographic Information Analysis, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","9","1432","1436","This letter introduces a novel remote sensing singleimage superresolution (SR) architecture based on a deep efficient compendium model. The current deep learning-based SR trend stands for using deeper networks to improve the performance. However, this practice often results in the degradation of visual results. To address this issue, the proposed approach harmonizes several different improvements on the network design to achieve state-of-the-art performance when superresolving remote sensing imagery. On the one hand, the proposal combines residual units and skip connections to extract more informative features on both local and global image areas. On the other hand, it makes use of parallelized 1×1 convolutional filters (network in network) to reconstruct the superresolved result while reducing the information loss through the network. Our experiments, conducted using seven different SR methods over the well-known UC Merced remote sensing data set, and two additional GaoFen-2 test images, show that the proposed model is able to provide competitive advantages.","","","10.1109/LGRS.2019.2899576","Ministerio de Educación, Cultura y Deporte; Generalitat Valenciana; Consejería de Educación y Empleo, Junta de Extremadura; Ministerio de Economía y Competitividad; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660433","Deep learning (DL);remote sensing;superresolution (SR)","Remote sensing;Spatial resolution;Kernel;Visualization;Training;Signal resolution","geophysical image processing;image denoising;image resolution;image texture;learning (artificial intelligence);pattern classification;remote sensing","single-image superresolution;deep compendium model;deep efficient compendium model;network design;remote sensing imagery;local image areas;global image areas;UC Merced remote sensing data;SR methods;GaoFen-2 test images;parallelized convolutional filters;deep learning-based SR trend;novel remote sensing single image superresolution architecture","","2","22","","","","","IEEE","IEEE Journals"
"Deep Intermediate Representation and In-Set Voting Scheme for Multiple-Beat Electrocardiogram Classification","W. Li","School of Instrument Science and Engineering, Southeast University, Nanjing, China","IEEE Sensors Journal","","2019","19","16","6895","6904","Electrocardiogram (ECG) analysis based on computer-aided diagnostic systems plays an important role in reducing the high mortality rate among cardiac patients, but this issue undergoes the challenges from complicated variations of the data. This paper conducts a pioneering research on a new branch of ECG analysis to recognize cardiovascular diseases: multiple-beat ECG classification. In the research, this paper proposes a novel approach that comprises two components, Deep Intermediate Representation (DIR) and In-Set Voting Scheme (ISVS), for representation and classification, respectively. By combining feature learning and selection, DIR can overcome the subjectivity of feature design that fully relies on human empirical knowledge, so that the discrimination and the generalization abilities are well balanced. In the feature space, ISVS can avoid the biased judgment caused by the noisy and anomalous samples of each set by means of committee voting and majority decision, which overturns the current belief in minority-based strategies for set-based discriminative measure. The effectiveness, efficiency, and adaptability of the proposed method have been fully demonstrated via theoretical analysis and experimental validation.","","","10.1109/JSEN.2019.2910853","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8689071","Multiple-beat electrocardiogram classification;deep intermediate representation;in-set voting scheme;cardiovascular diseases","Electrocardiography;Heart beat;Deep learning;Databases;Feature extraction;Discrete wavelet transforms","cardiovascular system;diseases;electrocardiography;feature selection;learning (artificial intelligence);medical signal processing;signal classification;signal representation","multiple-beat electrocardiogram classification;electrocardiogram analysis;computer-aided diagnostic systems;cardiac patients;ECG analysis;cardiovascular diseases;multiple-beat ECG classification;DIR;ISVS;feature learning;feature design;committee voting;majority decision;set-based discriminative measure;feature selection;deep intermediate representation;in-set voting scheme","","","53","","","","","IEEE","IEEE Journals"
"Capsule Network Assisted IoT Traffic Classification Mechanism for Smart Cities","H. Yao; P. Gao; J. Wang; P. Zhang; C. Jiang; Z. Han","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; College of Computer and Communication Engineering, China University of Petroleum (East China), Qingdao; Tsinghua Space Center, Tsinghua University, Beijing, China; Computer Science Department, University of Houston, Houston, TX, USA","IEEE Internet of Things Journal","","2019","6","5","7515","7525","With rapid development of compelling application scenarios of the Internet of Things (IoT), such as smart cities, it becomes substantially important to strengthen the management of data traffic in IoT networks. Traffic classification is beneficial in terms of both ensuring network security and improving quality of service. Traditional IoT traffic classification methods separate the classification algorithm and the design of feature engineering, which includes feature extraction and feature selection. Then, traffic identification or classification is performed by combining both. This paper proposes an end-to-end IoT traffic classification method relying on a deep learning aided capsule network for the sake of forming an efficient classification mechanism that integrates feature extraction, feature selection, and classification model. Our proposed traffic classification method beneficially eliminates the process of manually selecting traffic features, and is particularly applicable to smart city scenarios. To the best of our knowledge, this is the first time that capsule networks have been used in the context of traffic classification. Experimental results show the feasibility and effectiveness of our proposed traffic classification mechanism, which yields high classification accuracy.","","","10.1109/JIOT.2019.2901348","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities of China University of Petroleum (East China); National Basic Research Program (973) of China; Research on Coordinated Management and Control Technology of Network and Satellite Multidomain Network Resources; China Research Project on Key Technology Strategy of Infrastructure Security for Information Network Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8651277","Capsule network;deep learning;end-to-end;Internet of Things (IoT);smart city;traffic classification","Smart cities;Internet of Things;Feature extraction;Malware;Deep learning;Machine learning algorithms;Data models","computer network management;feature extraction;feature selection;Internet of Things;learning (artificial intelligence);pattern classification;security of data;smart cities","capsule network assisted IoT traffic classification mechanism;smart cities;data traffic;IoT networks;feature engineering;feature extraction;traffic identification;deep learning aided capsule network;network security;Internet of Things;quality of service;traffic feature selection","","1","35","","","","","IEEE","IEEE Journals"
"A Novel Deep Learning Framework for Industrial Multiphase Flow Characterization","W. Dang; Z. Gao; L. Hou; D. Lv; S. Qiu; G. Chen","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Tianjin Research Institute of Electric Science Company, Ltd, Tianjin, China; Department of Electronic Engineering, City University of Hong Kong, Hong Kong","IEEE Transactions on Industrial Informatics","","2019","15","11","5954","5962","Due to the inherent disturbances associated with flow structures, measurement of the complicated flow parameters in multiphase flows remains a challenging problem of significant importance. The flow dynamical behaviors are still elusive. In this paper, a multichannel complex impedance measurement system is designed to cope with this difficult issue. First, the geometry of the distributed multielectrode impedance sensor is optimized and a matched hardware measurement system is developed. After performance evaluation, a convolutional neural network and long short-term memory based measurement model is formulated for measuring flow parameters with high accuracy. The mean absolute error is only 0.36% for water cut and 0.77% for total flow velocity. Further, from the perspective of Lempel–Ziv complexity and mutual information, the relationship between the diverse flow structures and spatial flow behaviors is explored, leading to a deeper understanding of oil-water flows. All the experimental and analytical results demonstrate that the combination of deep learning and the designed impedance sensor measurement system allows measuring the complicated flow parameters, thereby characterizing the flow structures and behaviors. This opens up a new venue for exploring industrial multiphase flows and serving for an efficient oilfield exploitation as well.","","","10.1109/TII.2019.2908211","National Natural Science Foundation of China; Natural Science Foundation of Tianjin China; Hong Kong Research Grants Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676279","Deep learning;flow behavior;impedance sensor measurement system;oil-water flow","Impedance measurement;Electrodes;Impedance;Deep learning;Oils;Sensitivity;Solid modeling","","","","","45","IEEE","","","","IEEE","IEEE Journals"
"Semi-Supervised Multichannel Speech Enhancement With a Deep Speech Prior","K. Sekiguchi; Y. Bando; A. A. Nugraha; K. Yoshii; T. Kawahara","Graduate School of Informatics, Kyoto University, Kyoto, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Tokyo, Japan; Center for Advanced Intelligence Project (AIP), RIKEN, Tokyo, Japan; Graduate School of Informatics, Kyoto University, Kyoto, Japan; Graduate School of Informatics, Kyoto University, KyotoJapan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","12","2197","2212","This paper describes a semi-supervised multichannel speech enhancement method that uses clean speech data for prior training. Although multichannel nonnegative matrix factorization (MNMF) and its constrained variant called independent low-rank matrix analysis (ILRMA) have successfully been used for unsupervised speech enhancement, the low-rank assumption on the power spectral densities (PSDs) of all sources (speech and noise) does not hold in reality. To solve this problem, we replace a low-rank speech model with a deep generative speech model, i.e., formulate a probabilistic model of noisy speech by integrating a deep speech model, a low-rank noise model, and a full-rank or rank-1 model of spatial characteristics of speech and noise. The deep speech model is trained from clean speech data in an unsupervised auto-encoding variational Bayesian manner. Given multichannel noisy speech spectra, the full-rank or rank-1 spatial covariance matrices and PSDs of speech and noise are estimated in an unsupervised maximum-likelihood manner. Experimental results showed that the full-rank version of the proposed method was significantly better than MNMF, ILRMA, and the rank-1 version. We confirmed that the initialization-sensitivity and local-optimum problems of MNMF with many spatial parameters can be solved by incorporating the precise speech model.","","","10.1109/TASLP.2019.2944348","JST; JSPS KAKENHI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8861142","Multichannel speech enhancement;deep learning;variational autoencoder;nonnegative matrix factorization","Speech enhancement;Noise measurement;Data models;Probabilistic logic;Maximum likelihood estimation;Time-frequency analysis","blind source separation;covariance matrices;matrix algebra;matrix decomposition;maximum likelihood estimation;neural nets;signal denoising;speech enhancement;supervised learning;unsupervised learning","semisupervised multichannel speech enhancement method;clean speech data;multichannel nonnegative matrix factorization;low-rank matrix analysis;unsupervised speech enhancement;low-rank speech model;deep generative speech model;probabilistic model;deep speech model;low-rank noise model;rank-1 model;unsupervised auto-encoding variational Bayesian manner;multichannel noisy speech spectra","","","46","CCBY","","","","IEEE","IEEE Journals"
"A Hand-Based Multi-Biometrics via Deep Hashing Network and Biometric Graph Matching","D. Zhong; H. Shao; X. Du","School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China; School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China; School of Electrical Engineering, Xi’an Jiaotong University, Xi’an, China","IEEE Transactions on Information Forensics and Security","","2019","14","12","3140","3150","At present, the fusion of different unimodal biometrics has attracted increasing attention from researchers, who are dedicated to the practical application of biometrics. In this paper, we explored a multi-biometric algorithm that integrates palmprints and dorsal hand veins (DHV). Palmprint recognition has a rather high accuracy and reliability, and the most significant advantage of DHV recognition is the biopsy (Liveness detection). In order to combine the advantages of both and implement the fusion method, deep learning and graph matching were, respectively, introduced to identify palmprint and DHV. Upon using the deep hashing network (DHN), biometric images can be encoded as 128-bit codes. Then, the Hamming distances were used to represent the similarity of two codes. Biometric graph matching (BGM) can obtain three discriminative features for classification. In order to improve the accuracy of open-set recognition, in multi-modal fusion, the score-level fusion of DHN and BGM was performed and authentication was provided by support vector machine (SVM). Furthermore, based on DHN, all four levels of fusion strategies were used for multi-modal recognition of palmprint and DHV. Evaluation experiments and comprehensive comparisons were conducted on various commonly used datasets, and the promising results were obtained in this case where the equal error rates (EERs) of both palmprint recognition and multi-biometrics equal 0, demonstrating the great superiority of DHN in biometric verification.","","","10.1109/TIFS.2019.2912552","National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695776","Palmprint recognition;dorsal hand vein;fusion;deep hashing network;biometric graph matching","Veins;Palmprint recognition;Databases;Image recognition;Deep learning;Biomedical imaging;Skeleton","cryptography;Hamming codes;image classification;palmprint recognition;support vector machines","biometric graph matching;open-set recognition;multimodal fusion;score-level fusion;fusion strategies;multimodal recognition;palmprint recognition;biometric verification;hand-based multibiometrics;deep hashing network;multibiometric algorithm;dorsal hand veins;DHV recognition;fusion method;deep learning;biometric images","","1","62","","","","","IEEE","IEEE Journals"
"VIDOSAT: High-Dimensional Sparsifying Transform Learning for Online Video Denoising","B. Wen; S. Ravishankar; Y. Bresler","Department of Electrical and Computer Engineering, Coordinated Science Laboratory, University of Illinois at Urbana–Champaign, Champaign, IL, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA; Department of Electrical and Computer Engineering, Coordinated Science Laboratory, University of Illinois at Urbana–Champaign, Champaign, IL, USA","IEEE Transactions on Image Processing","","2019","28","4","1691","1704","Techniques exploiting the sparsity of images in a transform domain are effective for various applications in image and video processing. In particular, transform learning methods involve cheap computations and have been demonstrated to perform well in applications, such as image denoising and medical image reconstruction. Recently, we proposed methods for online learning of sparsifying transforms from streaming signals, which enjoy good convergence guarantees and involve lower computational costs than online synthesis dictionary learning. In this paper, we apply online transform learning to video denoising. We present a novel framework for online video denoising based on high-dimensional sparsifying transform learning for spatio-temporal patches. The patches are constructed either from corresponding 2D patches in successive frames or using an online block matching technique. The proposed online video denoising requires little memory and offers efficient processing. Numerical experiments evaluate the performance of the proposed video denoising algorithms on multiple video data sets. The proposed methods outperform several related and recent techniques, including denoising with 3D DCT, prior schemes based on dictionary learning, non-local means, background separation, and deep learning, as well as the popular VBM3D and VBM4D.","","","10.1109/TIP.2018.2865684","National Science Foundation; Office of Naval Research; Defense Advanced Research Projects Agency; Army Research Office; UM-SJTU Seed Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8438535","Sparse representations;sparsifying transforms;machine learning;data-driven techniques;online learning;big data;video denoising","Transforms;Streaming media;Noise reduction;Three-dimensional displays;Heuristic algorithms;Adaptation models;Tensile stress","image denoising;image reconstruction;learning (artificial intelligence);transforms;video signal processing","high-dimensional sparsifying;online video denoising;transform domain;learning methods;image denoising;medical image reconstruction;online learning;online synthesis dictionary learning;online block matching technique;multiple video data sets;deep learning","","1","47","","","","","IEEE","IEEE Journals"
"Building Extraction From LiDAR Data Applying Deep Convolutional Neural Networks","E. Maltezos; A. Doulamis; N. Doulamis; C. Ioannidis","School of Rural and Surveying Engineering, National Technical University of Athens, Zografou, Greece; School of Rural and Surveying Engineering, National Technical University of Athens, Zografou, Greece; School of Rural and Surveying Engineering, National Technical University of Athens, Zografou, Greece; School of Rural and Surveying Engineering, National Technical University of Athens, Zografou, Greece","IEEE Geoscience and Remote Sensing Letters","","2019","16","1","155","159","Deep learning paradigm has been shown to be a very efficient classification framework for many application scenarios, including the analysis of Light Detection and Ranging (LiDAR) data for building detection. In fact, deep learning acts as a set of mathematical transformations, encoding the raw input data into appropriate forms of representations that maximize the classification performance. However, it is clear that mathematical computations alone, even highly nonlinear, are not adequate to model the physical properties of a problem, distinguishing, for example, the building structures from vegetation. In this letter, we address this difficulty by augmenting the raw LiDAR data with features coming from a physical interpretation of the information. Then, we exploit a deep learning paradigm based on a convolutional neural network model to find out the best input representations suitable for the classification. As test sites, three complex urban study areas with various kinds of building structures through the LiDAR data set of Vaihingen, Germany were selected. Our method has been evaluated in the context of “ISPRS Test Project on Urban Classification and 3-D Building Reconstruction.” Comparisons with traditional methods, such as artificial neural networks and support vector machine-based classifiers, indicate the outperformance of the proposed approach in terms of robustness and efficiency.","","","10.1109/LGRS.2018.2867736","EU H2020 Project TERPSICHORE “Transforming Intangible Folkloric Performing Arts into Tangible Choreographic Digital Objects”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8465968","Building classification;convolutional neural networks (CNNs);Light Detection and Ranging (LiDAR);machine learning;point cloud","Laser radar;Buildings;Machine learning;Vegetation mapping;Vegetation;Feature extraction;Data models","feature extraction;feedforward neural nets;geophysical image processing;image classification;learning (artificial intelligence);optical radar;pattern classification;support vector machines","building structures;LiDAR data set;3-D Building Reconstruction;artificial neural networks;support vector machine-based classifiers;LiDAR data applying deep convolutional neural networks;deep learning paradigm;efficient classification framework;building detection;mathematical transformations;raw input data;classification performance;mathematical computations;physical properties;raw LiDAR data;physical interpretation;convolutional neural network model;input representations;urban classification","","","16","","","","","IEEE","IEEE Journals"
"Beyond Sharing Weights for Deep Domain Adaptation","A. Rozantsev; M. Salzmann; P. Fua","École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","4","801","814","The performance of a classifier trained on data coming from a specific domain typically degrades when applied to a related but different one. While annotating many samples from the new domain would address this issue, it is often too expensive or impractical. Domain Adaptation has therefore emerged as a solution to this problem; It leverages annotated data from a source domain, in which it is abundant, to train a classifier to operate in a target domain, in which it is either sparse or even lacking altogether. In this context, the recent trend consists of learning deep architectures whose weights are shared for both domains, which essentially amounts to learning domain invariant features. Here, we show that it is more effective to explicitly model the shift from one domain to the other. To this end, we introduce a two-stream architecture, where one operates in the source domain and the other in the target domain. In contrast to other approaches, the weights in corresponding layers are related but not shared. We demonstrate that this both yields higher accuracy than state-of-the-art methods on several object recognition and detection tasks and consistently outperforms networks with shared weights in both supervised and unsupervised settings.","","","10.1109/TPAMI.2018.2814042","Swiss Commission for Technology and Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8310033","Domain adaptation;deep learning","Training;Machine learning;Task analysis;Computer architecture;Computer vision;Training data;Detectors","learning (artificial intelligence);pattern classification","specific domain;source domain;target domain;deep architectures;domain invariant features;shared weights;deep domain adaptation","","11","75","","","","","IEEE","IEEE Journals"
"Multi-Stream Deep Neural Networks for RGB-D Egocentric Action Recognition","Y. Tang; Z. Wang; J. Lu; J. Feng; J. Zhou","Department of Automation, the State Key Laboratory of Intelligent Technologies and Systems, and the Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Department of Automation, the State Key Laboratory of Intelligent Technologies and Systems, and the Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Department of Automation, the State Key Laboratory of Intelligent Technologies and Systems, and the Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Department of Automation, the State Key Laboratory of Intelligent Technologies and Systems, and the Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Department of Automation, the State Key Laboratory of Intelligent Technologies and Systems, and the Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","10","3001","3015","In this paper, we investigate the problem of RGB-D egocentric action recognition. Unlike conventional human action videos that are passively recorded by static cameras, egocentric videos are self-generated from wearable sensors that are more flexible and provide the close-ups with the visual attention of the wearers when they act. Moreover, RGB-D videos contain the spatial appearance and temporal information in the RGB modality and reflect the 3D structure of the scenes in the depth modality. To adequately learn the nonlinear structure of heterogeneous representations from different modalities and exploit their complementary characteristics, we develop a multi-stream deep neural networks (MDNN) method, which aims to preserve the distinctive property for each modality and simultaneously explore their sharable information in a unified deep architecture. Specifically, we deploy a Cauchy estimator to maximize the correlations of the sharable components and enforce the orthogonality constraints on the distinctive components to guarantee their high independencies. Since the egocentric action recognition is usually sensitive to hand poses, we extend our MDNN by integrating with the hand cues to enhance the recognition accuracy. Extensive experimental results on a newly collected data set and two additional benchmarks are presented to demonstrate the effectiveness of our proposed method for RGB-D egocentric action recognition.","","","10.1109/TCSVT.2018.2875441","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; National 1000 Young Talents Plan Program; Shenzhen Fundamental Research Fund (Subject Arrangement); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489917","Egocentric action recognition;RGB-D videos;multi-view learning;deep learning","Videos;Head;Cameras;Visualization;Three-dimensional displays;Correlation;Magnetic heads","cameras;gesture recognition;image motion analysis;image recognition;image sequences;learning (artificial intelligence);neural nets;pose estimation;video signal processing","conventional human action videos;egocentric videos;RGB-D videos;RGB modality;deep neural networks method;multistream deep neural networks;RGB-D egocentric action recognition;3D structure;depth modality;MDNN;Cauchy estimator","","","104","","","","","IEEE","IEEE Journals"
"Deep  ${Q}$ -Network-Based Route Scheduling for TNC Vehicles With Passengers’ Location Differential Privacy","D. Shi; J. Ding; S. M. Errapotu; H. Yue; W. Xu; X. Zhou; M. Pan","Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; Computer Science Department, San Francisco State University, San Francisco, CA, USA; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Division of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, LA, USA; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA","IEEE Internet of Things Journal","","2019","6","5","7681","7692","The transportation network company (TNC) services efficiently pair the passengers with the vehicles/drivers through mobile applications, such as Uber, Lyft, Didi, etc. TNC services definitely facilitate the traveling of passengers, while it is equally important to effectively and intelligently schedule the routes of cruising TNC vehicles to improve TNC drivers' revenues. From the TNC drivers' side, the most critical question to address is how to reduce the cruising time, and improve the efficiency/earnings by using their own vehicles to provide TNC services. In this paper, we propose a deep reinforcement learning-based TNC route scheduling approach, which allows the TNC service center to learn about the dynamic TNC service environment and schedule the routes for the vacant TNC vehicles. In particular, we jointly consider multiple factors in the complex TNC environment, such as locations of the TNC vehicles, different time periods during the day, the competition among TNC vehicles, etc., and develop a deep Q-network-based route scheduling algorithm for vacant TNC vehicles based on distributed framework, which makes the server closer to the terminal users and accelerates the training speed. Furthermore, we apply the geo-indistinguishability scheme based on differential privacy to preserve the sensitive location information uploaded by the passengers. We evaluate the proposed algorithm's performance via simulations using open data sets from Didi Chuxing. Through extensive simulations, we show that the proposed scheme is effective in reducing the cruising time of vacant TNC vehicles and improving the earnings of TNC drivers.","","","10.1109/JIOT.2019.2902815","National Science Foundation; National Natural Science Foundation of China; U.S. National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8657772","Deep Q-network;differential privacy;TNC services;TNC vehicle route scheduling","Vehicles;Differential privacy;Urban areas;Scheduling;Processor scheduling;Computational modeling;Reinforcement learning","data analysis;data privacy;learning (artificial intelligence);mobile computing;telecommunication network routing;traffic information systems","transportation network company services;TNC drivers;cruising time;deep reinforcement learning-based TNC route scheduling approach;dynamic TNC service environment;vacant TNC vehicles;complex TNC environment;deep Q-network-based route scheduling algorithm","","2","31","","","","","IEEE","IEEE Journals"
"Low-Level Control of a Quadrotor With Deep Model-Based Reinforcement Learning","N. O. Lambert; D. S. Drew; J. Yaconelli; S. Levine; R. Calandra; K. S. J. Pister","Department of Electrical Engineering and Computer Sciences, University of California–Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California–Berkeley, Berkeley, CA, USA; University of Oregon, Eugene, OR, USA; Department of Electrical Engineering and Computer Sciences, University of California–Berkeley, Berkeley, CA, USA; Facebook AI Research, Menlo Park, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California–Berkeley, Berkeley, CA, USA","IEEE Robotics and Automation Letters","","2019","4","4","4224","4230","Designing effective low-level robot controllers often entail platform-specific implementations that require manual heuristic parameter tuning, significant system knowledge, or long design times. With the rising number of robotic and mechatronic systems deployed across areas ranging from industrial automation to intelligent toys, the need for a general approach to generating low-level controllers is increasing. To address the challenge of rapidly generating low-level controllers, we argue for using model-based reinforcement learning (MBRL) trained on relatively small amounts of automatically generated (i.e., without system simulation) data. In this letter, we explore the capabilities of MBRL on a Crazyflie centimeter-scale quadrotor with rapid dynamics to predict and control at ≤50 Hz. To our knowledge, this is the first use of MBRL for controlled hover of a quadrotor using only on-board sensors, direct motor input signals, and no initial dynamics knowledge. Our controller leverages rapid simulation of a neural network forward dynamics model on a graphic processing unit enabled base station, which then transmits the best current action to the quadrotor firmware via radio. In our experiments, the quadrotor achieved hovering capability of up to 6 s with 3 min of experimental training data.","","","10.1109/LRA.2019.2930489","Berkeley Sensors & Actuator Center SUPERB REU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8769882","Deep learning in robotics and automation;aerial systems: mechanics and control","Data models;Vehicle dynamics;Robots;Pulse width modulation;Attitude control;Trajectory;Predictive models","aircraft control;control engineering computing;helicopters;learning (artificial intelligence);mechatronics;mobile robots;multi-robot systems;path planning;trajectory control","rapid simulation;platform-specific implementations;on-board sensors;direct motor input signals;neural network forward dynamics model;quadrotor firmware;robotic systems;low-level robot controllers;deep model-based reinforcement learning;low-level control;controlled hover;Crazyflie centimeter-scale quadrotor;MBRL;low-level controllers;mechatronic systems;frequency 50.0 Hz;time 6.0 s;time 3.0 min","","","30","Traditional","","","","IEEE","IEEE Journals"
"Listening and Grouping: An Online Autoregressive Approach for Monaural Speech Separation","Z. Li; Y. Song; L. Dai; I. McLoughlin","National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; School of Computing, University of Kent, Medway, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","4","692","703","This paper proposes an autoregressive approach to harness the power of deep learning for multi-speaker monaural speech separation. It exploits a causal temporal context in both mixture and past estimated separated signals and performs online separation that is compatible with real-time applications. The approach adopts a learned listening and grouping architecture motivated by computational auditory scene analysis, with a grouping stage that effectively addresses the label permutation problem at both frame and segment levels. Experimental results on the WSJ0-2mix benchmark show that the new approach can achieve better signal-to-distortion ratio and perceptual evaluation of speech quality scores than most of the state-of-the-art methods for both closed-set and open-set evaluations, even methods that exploit whole-utterance statistics for separation. It achieves this while requiring fewer model parameters.","","","10.1109/TASLP.2019.2892241","National Key R&D Program; National Natural Science Foundation of China; Key Science and Technology Project of Anhui Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606997","Speech separation;deep learning;label permutation problem;computational auditory scene analysis","Deep learning;Speech processing;Training;Neural networks;Image analysis;Extraterrestrial measurements;Task analysis","autoregressive processes;learning (artificial intelligence);speech recognition","online autoregressive approach;multispeaker monaural speech separation;causal temporal context;learned listening;grouping architecture;computational auditory scene analysis;grouping stage;label permutation problem;segment levels;signal-to-distortion ratio;speech quality scores;frame levels;WSJ0-2mix benchmark;deep learning power;closed-set evaluation;open-set evaluation","","1","54","","","","","IEEE","IEEE Journals"
"Group affinity guided deep hypergraph model for person re-identification","Q. Zeng; H. Yu","Zhejiang University, People's Republic of China; Zhejiang University, People's Republic of China","Electronics Letters","","2019","55","4","186","188","Person re-identification aims at searching in a large gallery image database for images of the same identity as probe image, which can also be treated as a retrieval task in which the pairwise affinity is often used to rank the retrieved images. However, most existing methods of person re-identification only consider pairwise affinity but ignore the group affinity information. Some frameworks incorporate group affinity into the testing phase, which is not end-to-end trainable for deep neural networks. In this Letter, a powerful deep learning based framework for person re-identification is presented, which aims at fully utilising group affinity information for more discriminative features. Specifically, hypergraph model for group information mining is novelly proposed in the training phase, which significantly improves the performance of features acquired by the baseline networks. Extensive experiments on two popular benchmarks demonstrate the effectiveness of the authors' model and applicability on different baseline networks.","","","10.1049/el.2018.7791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643896","","","data mining;feature extraction;graph theory;image retrieval;learning (artificial intelligence);neural nets;visual databases","probe image;retrieval task;pairwise affinity;group affinity information;deep neural networks;group information mining;person re-identification;deep learning based framework;group affinity guided deep hypergraph model;large gallery image database;image retrieval","","","9","","","","","IET","IET Journals"
"Category-Based Deep CCA for Fine-Grained Venue Discovery From Multimodal Data","Y. Yu; S. Tang; K. Aizawa; A. Aizawa","Digital Content and Media Sciences Research Division, National Institute of Informatics, Tokyo, Japan; Graduate School of Informatics and Engineering, The University of Electro-Communications, Tokyo, Japan; Department of Information and Communication Engineering, The University of Tokyo, Tokyo, Japan; Digital Content and Media Sciences Research Division, National Institute of Informatics, Tokyo, Japan","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","4","1250","1258","In this work, travel destinations and business locations are taken as venues. Discovering a venue by a photograph is very important for visual context-aware applications. Unfortunately, few efforts paid attention to complicated real images such as venue photographs generated by users. Our goal is fine-grained venue discovery from heterogeneous social multimodal data. To this end, we propose a novel deep learning model, category-based deep canonical correlation analysis. Given a photograph as input, this model performs: 1) exact venue search (find the venue where the photograph was taken) and 2) group venue search (find relevant venues that have the same category as the photograph), by the cross-modal correlation between the input photograph and textual description of venues. In this model, data in different modalities are projected to a same space via deep networks. Pairwise correlation (between different modality data from the same venue) for exact venue search and category-based correlation (between different modality data from different venues with the same category) for group venue search are jointly optimized. Because a photograph cannot fully reflect rich text description of a venue, the number of photographs per venue in the training phase is increased to capture more aspects of a venue. We build a new venue-aware multimodal data set by integrating Wikipedia featured articles and Foursquare venue photographs. Experimental results on this data set confirm the feasibility of the proposed method. Moreover, the evaluation over another publicly available data set confirms that the proposed method outperforms state of the arts for cross-modal retrieval between image and text.","","","10.1109/TNNLS.2018.2856253","JSPS KAKENHI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8432497","Category-based deep canonical correlation analysis (C-DCCA);cross-modal retrieval;fine-grained venue discovery;multimodal data","Correlation;Visualization;Internet;Business;Data models;Pairwise error probability;Feature extraction","correlation methods;image retrieval;learning (artificial intelligence);social networking (online);text analysis","fine-grained venue discovery;category-based deep canonical correlation analysis;exact venue search;group venue search;venue-aware multimodal data;Foursquare venue photographs;category-based deep CCA;modality data;heterogeneous social multimodal data;cross-modal retrieval","","10","29","","","","","IEEE","IEEE Journals"
"Intelligent Resource Scheduling for 5G Radio Access Network Slicing","M. Yan; G. Feng; J. Zhou; Y. Sun; Y. Liang","National Key Laboratory of Science and Technology on Communications and the Center for Intelligent Networking and Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Science and Technology on Communications and the Center for Intelligent Networking and Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Science and Technology on Communications and the Center for Intelligent Networking and Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Science and Technology on Communications and the Center for Intelligent Networking and Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Science and Technology on Communications and the Center for Intelligent Networking and Communications, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Vehicular Technology","","2019","68","8","7691","7703","It is widely acknowledged that network slicing can tackle the diverse use cases and connectivity services of the forthcoming next-generation mobile networks (5G). Resource scheduling is of vital importance for improving resource-multiplexing gain among slices while meeting specific service requirements for radio access network (RAN) slicing. Unfortunately, due to the performance isolation, diversified service requirements, and network dynamics (including user mobility and channel states), resource scheduling in RAN slicing is very challenging. In this paper, we propose an intelligent resource scheduling strategy (iRSS) for 5G RAN slicing. The main idea of an iRSS is to exploit a collaborative learning framework that consists of deep learning (DL) in conjunction with reinforcement learning (RL). Specifically, DL is used to perform large time-scale resource allocation, whereas RL is used to perform online resource scheduling for tackling small time-scale network dynamics, including inaccurate prediction and unexpected network states. Depending on the amount of available historical traffic data, an iRSS can flexibly adjust the significance between the prediction and online decision modules for assisting RAN in making resource scheduling decisions. Numerical results show that the convergence of an iRSS satisfies online resource scheduling requirements and can significantly improve resource utilization while guaranteeing performance isolation between slices, compared with other benchmark algorithms.","","","10.1109/TVT.2019.2922668","National Natural Science Foundation of China; Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736403","RAN slicing;resource scheduling;deep learning;reinforcement learning","5G mobile communication;Vehicle dynamics;Dynamic scheduling;Radio access networks;Resource management;Machine learning","5G mobile communication;learning (artificial intelligence);radio access networks;resource allocation;telecommunication computing;telecommunication traffic","resource utilization;performance isolation;5G radio access network slicing;next-generation mobile networks;resource-multiplexing gain;specific service requirements;diversified service requirements;iRSS;collaborative learning framework;deep learning;reinforcement learning;time-scale resource allocation;time-scale network dynamics;unexpected network states;online decision modules;resource scheduling decisions;online resource scheduling requirements;user mobility;5G RAN slicing;intelligent resource scheduling","","1","34","Traditional","","","","IEEE","IEEE Journals"
"Image Recognition by Predicted User Click Feature With Multidomain Multitask Transfer Deep Network","M. Tan; J. Yu; H. Zhang; Y. Rui; D. Tao","Key Laboratory of Complex Systems Modeling and Simulation, School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Key Laboratory of Complex Systems Modeling and Simulation, School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Key Laboratory of Complex Systems Modeling and Simulation, School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Lenovo (Beijing) Co., Ltd., Beijing, China; UBTECH Sydney Artificial Intelligence Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW, Australia","IEEE Transactions on Image Processing","","2019","28","12","6047","6062","The click feature of an image, defined as a user click count vector based on click data, has been demonstrated to be effective for reducing the semantic gap for image recognition. Unfortunately, most of the traditional image recognition datasets do not contain click data. To address this problem, researchers have begun to develop a click prediction model using assistant datasets containing click information and have adapted this predictor to a common click-free dataset for different tasks. This method can be customized to our problem, but it has two main limitations: 1) the predicted click feature often performs badly in the recognition task since the prediction model is constructed independently of the subsequent recognition problem and 2) transferring the predictor from one dataset to another is challenging due to the large cross-domain diversity. In this paper, we devise a multitask and multidomain deep network with varied modals (MTMDD-VM) to formulate image recognition and click prediction tasks in a unified framework. Datasets with and without click information are integrated in the training. Furthermore, a nonlinear word embedding with a position-sensitive loss function is designed to discover the visual click correlation. We evaluate the proposed method on three public dog breed image datasets, and we utilize the Clickture-Dog dataset as the auxiliary dataset that provides click data. The experimental results show that: 1) the nonlinear word embedding and position-sensitive loss function largely enhance the predicted click feature in the recognition task, realizing a 32% improvement in accuracy; 2) the multitask learning framework improves accuracies in both image recognition and click prediction; and 3) the unified training using the combined dataset with and without click data further improves the performance. Compared with the state-of-the-art methods, the proposed approach not only performs much better in accuracy but also achieves good scalability and one-shot learning ability.","","","10.1109/TIP.2019.2921861","Natural Science Foundation of Zhejiang Province; National Natural Science Foundation of China; Australian Research Council; Zhejiang Provincial Key Science and Technology Project Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8751129","Image recognition;click prediction;transfer deep learning;multitask learning;word embedding","Image recognition;Task analysis;Visualization;Predictive models;Adaptation models;Correlation;Computational modeling","image recognition;learning (artificial intelligence)","predicted user click feature;multidomain multitask transfer deep network;user click count vector;click data;click prediction model;assistant datasets;click information;common click-free dataset;multitask network;multidomain deep network;click prediction tasks;position-sensitive loss function;visual click correlation;public dog breed image datasets;Clickture-Dog dataset","","3","45","","","","","IEEE","IEEE Journals"
"Unsupervised Deep Video Hashing via Balanced Code for Large-Scale Video Retrieval","G. Wu; J. Han; Y. Guo; L. Liu; G. Ding; Q. Ni; L. Shao","School of Computing and Communication, Lancaster University, Lancaster, U.K.; School of Computing and Communication, Lancaster University, Lancaster, U.K.; School of Software, Tsinghua University, Beijing, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; School of Software, Tsinghua University, Beijing, China; School of Computing and Communication, Lancaster University, Lancaster, U.K.; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates","IEEE Transactions on Image Processing","","2019","28","4","1993","2007","This paper proposes a deep hashing framework, namely, unsupervised deep video hashing (UDVH), for large-scale video similarity search with the aim to learn compact yet effective binary codes. Our UDVH produces the hash codes in a self-taught manner by jointly integrating discriminative video representation with optimal code learning, where an efficient alternating approach is adopted to optimize the objective function. The key differences from most existing video hashing methods lie in: 1) UDVH is an unsupervised hashing method that generates hash codes by cooperatively utilizing feature clustering and a specifically designed binarization with the original neighborhood structure preserved in the binary space and 2) a specific rotation is developed and applied onto video features such that the variance of each dimension can be balanced, thus facilitating the subsequent quantization step. Extensive experiments performed on three popular video datasets show that the UDVH is overwhelmingly better than the state of the arts in terms of various evaluation metrics, which makes it practical in real-world applications.","","","10.1109/TIP.2018.2882155","Royal Society; Shenzhen Government; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540456","Video hashing;balanced rotation;similarity retrieval;feature representation;deep learning","Quantization (signal);Feature extraction;Binary codes;Hamming distance;Training;Nickel","binary codes;cryptography;feature extraction;image representation;pattern clustering;unsupervised learning;video coding;video retrieval","unsupervised deep video hashing;balanced code;large-scale video retrieval;deep hashing framework;UDVH;large-scale video similarity search;binary codes;hash codes;discriminative video representation;optimal code learning;unsupervised hashing method;video features;video hashing methods;video datasets;feature clustering","","6","74","","","","","IEEE","IEEE Journals"
"Online Incremental Machine Learning Platform for Big Data-Driven Smart Traffic Management","D. Nallaperuma; R. Nawaratne; T. Bandaragoda; A. Adikari; S. Nguyen; T. Kempitiya; D. De Silva; D. Alahakoon; D. Pothuhera","Research Centre for Data Analytics and Cognition, La Trobe Business School, Bundoora, VIC, Australia; Research Centre for Data Analytics and Cognition, La Trobe Business School, Bundoora, VIC, Australia; Research Centre for Data Analytics and Cognition, La Trobe Business School, Bundoora, VIC, Australia; Research Centre for Data Analytics and Cognition, La Trobe Business School, Bundoora, VIC, Australia; Research Centre for Data Analytics and Cognition, La Trobe Business School, Bundoora, VIC, Australia; Research Centre for Data Analytics and Cognition, La Trobe Business School, Bundoora, VIC, Australia; Research Centre for Data Analytics and Cognition, La Trobe Business School, Bundoora, VIC, Australia; Research Centre for Data Analytics and Cognition, La Trobe Business School, Bundoora, VIC, Australia; Department of Information Management and Technology, VicRoads, Kew, VIC, Australia","IEEE Transactions on Intelligent Transportation Systems","","2019","20","12","4679","4690","The technological landscape of intelligent transport systems (ITS) has been radically transformed by the emergence of the big data streams generated by the Internet of Things (IoT), smart sensors, surveillance feeds, social media, as well as growing infrastructure needs. It is timely and pertinent that ITS harness the potential of an artificial intelligence (AI) to develop the big data-driven smart traffic management solutions for effective decision-making. The existing AI techniques that function in isolation exhibit clear limitations in developing a comprehensive platform due to the dynamicity of big data streams, high-frequency unlabeled data generation from the heterogeneous data sources, and volatility of traffic conditions. In this paper, we propose an expansive smart traffic management platform (STMP) based on the unsupervised online incremental machine learning, deep learning, and deep reinforcement learning to address these limitations. The STMP integrates the heterogeneous big data streams, such as the IoT, smart sensors, and social media, to detect concept drifts, distinguish between the recurrent and non-recurrent traffic events, and impact propagation, traffic flow forecasting, commuter sentiment analysis, and optimized traffic control decisions. The platform is successfully demonstrated on 190 million records of smart sensor network traffic data generated by 545,851 commuters and corresponding social media data on the arterial road network of Victoria, Australia.","","","10.1109/TITS.2019.2924883","Australian Government Research Training Program Scholarship; Data to Decisions Cooperative Research Centres; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759919","Smart traffic management;concept drift;unsupervised incremental learning;deep learning;deep reinforcement learning;impact propagation;traffic optimization;traffic forecasting;traffic control;social media analytics","Social networking (online);Roads;Real-time systems;Clustering algorithms;Big Data;Machine learning algorithms;Data models","","","","1","49","CCBY","","","","IEEE","IEEE Journals"
"Optimized Computation Offloading Performance in Virtual Edge Computing Systems Via Deep Reinforcement Learning","X. Chen; H. Zhang; C. Wu; S. Mao; Y. Ji; M. Bennis","VTT Technical Research Centre of Finland, Oulu, Finland; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Graduate School of Informatics and Engineering, University of Electro-Communications, Tokyo, Japan; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA; Information Systems Architecture Research Division, National Institute of Informatics, Tokyo, Japan; Centre for Wireless Communications, University of Oulu, Oulu, Finland","IEEE Internet of Things Journal","","2019","6","3","4005","4018","To improve the quality of computation experience for mobile devices, mobile-edge computing (MEC) is a promising paradigm by providing computing capabilities in close proximity within a sliced radio access network (RAN), which supports both traditional communication and MEC services. Nevertheless, the design of computation offloading policies for a virtual MEC system remains challenging. Specifically, whether to execute a computation task at the mobile device or to offload it for MEC server execution should adapt to the time-varying network dynamics. This paper considers MEC for a representative mobile user in an ultradense sliced RAN, where multiple base stations (BSs) are available to be selected for computation offloading. The problem of solving an optimal computation offloading policy is modeled as a Markov decision process, where our objective is to maximize the long-term utility performance whereby an offloading decision is made based on the task queue state, the energy queue state as well as the channel qualities between mobile user and BSs. To break the curse of high dimensionality in state space, we first propose a double deep Q-network (DQN)-based strategic computation offloading algorithm to learn the optimal policy without knowing a priori knowledge of network dynamics. Then motivated by the additive structure of the utility function, a Q-function decomposition technique is combined with the double DQN, which leads to a novel learning algorithm for the solving of stochastic computation offloading. Numerical experiments show that our proposed learning algorithms achieve a significant improvement in computation offloading performance compared with the baseline policies.","","","10.1109/JIOT.2018.2876279","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Zhejiang Key Research and Development Plan; National Science Foundation; Auburn University; JSPS KAKENHI; Academy of Finland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493155","Deep reinforcement learning;Markov decision process (MDP);mobile-edge computing (MEC);network slicing;network virtualization;Q-function decomposition;radio access networks (RANs)","Task analysis;Mobile handsets;Wireless communication;Heuristic algorithms;Servers;Stochastic processes;Electronic mail","cellular radio;cloud computing;learning (artificial intelligence);Markov processes;mobile computing;optimisation;power aware computing;radio access networks;resource allocation;stochastic processes","stochastic computation offloading;optimized computation offloading performance;computation experience;mobile device;mobile-edge computing;computing capabilities;sliced radio access network;virtual MEC system;MEC server execution;double deep Q-network-based strategic computation offloading algorithm;offloading decision;representative mobile user;time-varying network dynamics","","21","42","","","","","IEEE","IEEE Journals"
"Joint Weakly and Semi-Supervised Deep Learning for Localization and Classification of Masses in Breast Ultrasound Images","S. Y. Shin; S. Lee; I. D. Yun; S. M. Kim; K. M. Lee","Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, South Korea; Department of Electronic Engineering, Soonchunhyang University, Asan, South Korea; Division of Computer and Electronic Systems Engineering, Hankuk University of Foreign Studies, Yongin, South Korea; Department of Radiology, Seoul National University Bundang Hospital, Seoul National University, Seongnam, South Korea; Department of Electrical and Computer Engineering, Automation and Systems Research Institute, Seoul National University, Seoul, South Korea","IEEE Transactions on Medical Imaging","","2019","38","3","762","774","We propose a framework for localization and classification of masses in breast ultrasound images. We have experimentally found that training convolutional neural network-based mass detectors with large, weakly annotated datasets presents a non-trivial problem, while overfitting may occur with those trained with small, strongly annotated datasets. To overcome these problems, we use a weakly annotated dataset together with a smaller strongly annotated dataset in a hybrid manner. We propose a systematic weakly and semi-supervised training scenario with appropriate training loss selection. Experimental results show that the proposed method can successfully localize and classify masses with less annotation effort. The results trained with only 10 strongly annotated images along with weakly annotated images were comparable to results trained from 800 strongly annotated images, with the 95% confidence interval (CI) of difference -3%-5%, in terms of the correct localization (CorLoc) measure, which is the ratio of images with intersection over union with ground truth higher than 0.5. With the same number of strongly annotated images, additional weakly annotated images can be incorporated to give a 4.5% point increase in CorLoc, from 80% to 84.50% (with 95% CIs 76%-83.75% and 81%-88%). The effects of different algorithmic details and varied amount of data are presented through ablative analysis.","","","10.1109/TMI.2018.2872031","National Research Foundation of Korea; Korean Government (MSIT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8471199","Breast ultrasound;convolutional neural networks;mass classification;mass localization;semi-supervised learning;weakly supervised learning","Training;Cancer;Ultrasonic imaging;Machine learning;Breast;Image segmentation;Lesions","biomedical ultrasonics;image classification;learning (artificial intelligence);medical image processing;neural nets","semisupervised deep;masses;breast ultrasound images;classification;training convolutional neural network-based mass detectors;weakly annotated dataset;strongly annotated datasets;appropriate training loss selection;annotation effort;correct localization measure;additional weakly annotated images;strongly annotated images;CorLoc;ablative analysis","","2","45","","","","","IEEE","IEEE Journals"
"Using Generalized Gaussian Distributions to Improve Regression Error Modeling for Deep Learning-Based Speech Enhancement","L. Chai; J. Du; Q. Liu; C. Lee","National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","12","1919","1931","From a statistical perspective, the conventional minimum mean squared error (MMSE) criterion can be considered as the maximum likelihood (ML) solution under an assumed homoscedastic Gaussian error model. However, in this paper, a statistical analysis reveals the super-Gaussian and heteroscedastic properties of the prediction errors in nonlinear regression deep neural network (DNN)-based speech enhancement when estimating clean log-power spectral (LPS) components at DNN outputs with noisy LPS features in DNN input vectors. Accordingly, we propose treating all dimensions of the prediction error vector as statistically independent random variables and model them with generalized Gaussian distributions (GGDs). Then, the objective function with the GGD error model is derived according to the ML criterion. Experiments on the TIMIT corpus corrupted by simulated additive noises show consistent improvements of our proposed DNN framework over the conventional DNN framework in terms of various objective quality measures under 14 unseen noise types evaluated and at various signal-to-noise ratio levels. Furthermore, the ML optimization objective with GGD outperforms the conventional MMSE criterion, achieving improved generalization and robustness.","","","10.1109/TASLP.2019.2935803","National Key R&D Program of China; National Natural Science Foundation of China; Key Science and Technology Project of Anhui Province; Huawei Noah's Ark Lab; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805084","Speech enhancement;deep neural network;prediction error modeling;generalized Gaussian distribution;maximum likelihood estimation","Speech enhancement;Linear programming;Predictive models;Training;Task analysis;Gaussian distribution","Gaussian distribution;Gaussian processes;learning (artificial intelligence);least mean squares methods;maximum likelihood estimation;mean square error methods;neural nets;noise;regression analysis;speech enhancement;speech processing","statistical perspective;conventional minimum;squared error criterion;maximum likelihood solution;assumed homoscedastic Gaussian error model;statistical analysis;super-Gaussian properties;heteroscedastic properties;prediction errors;nonlinear regression deep neural network-based speech enhancement;clean log-power spectral components;DNN outputs;noisy LPS features;DNN input vectors;prediction error vector;statistically independent random variables;generalized Gaussian distributions;GGD error model;ML criterion;consistent improvements;conventional DNN framework;ML optimization objective;conventional MMSE criterion;improved generalization;robustness;regression error modeling;deep learning-based speech enhancement","","","75","Traditional","","","","IEEE","IEEE Journals"
"Bayesian Weight Decay on Bounded Approximation for Deep Convolutional Neural Networks","J. Park; S. Jo","School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","9","2866","2875","This paper determines the weight decay parameter value of a deep convolutional neural network (CNN) that yields a good generalization. To obtain such a CNN in practice, numerical trials with different weight decay values are needed. However, the larger the CNN architecture is, the higher is the computational cost of the trials. To address this problem, this paper formulates an analytical solution for the decay parameter through a proposed objective function in conjunction with Bayesian probability distributions. For computational efficiency, a novel method to approximate this solution is suggested. This method uses a small amount of information in the Hessian matrix. Theoretically, the approximate solution is guaranteed by a provable bound and is obtained by a proposed algorithm, where its time complexity is linear in terms of both the depth and width of the CNN. The bound provides a consistent result for the proposed learning scheme. By reducing the computational cost of determining the decay value, the approximation allows for the fast investigation of a deep CNN (DCNN) which yields a small generalization error. Experimental results show that our assumption verified with different DCNNs is suitable for real-world image data sets. In addition, the proposed method significantly reduces the time cost of learning with setting the weight decay parameter while achieving good classification performances.","","","10.1109/TNNLS.2018.2886995","Institute of Information & Communications Technology Planning & Evaluation (IITP); Korea government (MSIT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613900","Bayesian method;convolutional neural networks (CNNs);inverse Hessian matrix;regularization;weight decay","Bayes methods;Training;Linear programming;Learning systems;Convolutional neural networks;Time complexity","approximation theory;Bayes methods;convolutional neural nets;Hessian matrices;learning (artificial intelligence);probability;statistical distributions","numerical trials;CNN architecture;computational cost;analytical solution;Bayesian probability distributions;computational efficiency;approximate solution;decay value;deep CNN;Bayesian weight decay;bounded approximation;deep convolutional neural network;weight decay parameter value;weight decay values","","","51","","","","","IEEE","IEEE Journals"
"Remote Sensing Scene Classification Using Convolutional Features and Deep Forest Classifier","Y. Boualleg; M. Farah; I. R. Farah","SIIVT–RIADI Laboratory, National School of Computer Science, University of Manouba, Manouba, Tunisia; SIIVT–RIADI Laboratory, National School of Computer Science, University of Manouba, Manouba, Tunisia; SIIVT–RIADI Laboratory, National School of Computer Science, University of Manouba, Manouba, Tunisia","IEEE Geoscience and Remote Sensing Letters","","2019","16","12","1944","1948","High-resolution remote sensing scene classification (HR-RSSC) plays an increasingly important role since it aims to enhance the scene semantic understanding. Recently, convolutional neural networks (CNNs) proved their effectiveness in learning powerful feature representations for various visual recognition tasks. However, in the RS domain, the performance of CNN is still limited due to the lack of sufficient labeled data. In this letter, we propose an HR-RSSC method based on CNN transfer learning (TL) for feature extraction (FE) and deep forest (DF) for classification. In fact, we extract deep features from the last convolutional layer in order to avoid the use of the fully connected layers (FCLs) which need many parameters to tune. Moreover, we train a DF model that is based on ensemble learning that can achieve better performances than single classifiers and is easy to train with few parameters. We evaluate the proposed method on two RS image. Compared to full-training, fine-tuning, and state-of-the-art CNN TL methods, the results demonstrate the effectiveness of the DF model for HR-RSSC based on CNN TL in terms of overall accuracy and training time.","","","10.1109/LGRS.2019.2911855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8709807","Convolutional neural network (CNN);deep forest (DF);remote sensing (RS);scene classification;transfer learning (TL)","Feature extraction;Forestry;Training;Computational modeling;Data models;Remote sensing;Task analysis","computer vision;convolutional neural nets;feature extraction;image classification;image representation;image resolution;learning (artificial intelligence);remote sensing","feature extraction;deep forest;CNN transfer learning;HR-RSSC method;visual recognition tasks;feature representations;convolutional neural networks;scene semantic understanding;high-resolution remote sensing scene classification;ensemble learning;DF model;fully connected layers","","1","16","IEEE","","","","IEEE","IEEE Journals"
"A Deep Learning-Based Compression Algorithm for 9-DOF Inertial Measurement Unit Signals Along With an Error Compensating Mechanism","M. Sepahvand; F. Abdali-Mohammadi","Department of Computer Engineering and Information Technology, Razi University, Kermanshah, Iran; Department of Computer Engineering and Information Technology, Razi University, Kermanshah, Iran","IEEE Sensors Journal","","2019","19","2","632","640","Inertial sensors with microelectromechanical systems technology are an integral part of many modern electronic devices such as wearable medical products, which are inherently subject to memory, bandwidth, and energy constraints due to their size and purpose. One of the important challenges for the progress in this area is the storage, transmission, and processing of large quantities of inertial sensors signal. To address this issue, this paper presents a method for near-lossless compression of multi-axis inertial signals. To improve the inertial signal compression capability, the proposed compression method employs the independent component analysis method with a principal component analysis preprocessing step to extract independent components from the signals. A deep autoencoder is used to compress the independent components and later to estimate them in the reconstruction phase. The reconstruction error is also quantized and coded using arithmetic coding and transmitted alongside the compressed components. This paper also proposes a new approach for improving the quality of the reconstructed signals. In this approach, on the receiver side, the reconstruction error is fed to the Madgwick filter as an external noise and is compensated using this filter. The experimental results demonstrate the high compression rate and low reconstruction error of the proposed method compared to the state-of-the-art methods.","","","10.1109/JSEN.2018.2877360","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502108","Deep autoencoder;estimator;independent component analysis;inertial signal;near-lossless compression;wearable computing","Compression algorithms;Magnetic sensors;Filtering algorithms;Integrated circuits;Principal component analysis;Estimation","arithmetic codes;data compression;error compensation;feature extraction;filtering theory;independent component analysis;learning (artificial intelligence);microsensors;signal reconstruction","deep learning-based compression algorithm;inertial measurement unit signals;error compensating mechanism;microelectromechanical systems technology;integral part;modern electronic devices;wearable medical products;energy constraints;inertial sensors signal;near-lossless compression;multiaxis inertial signals;inertial signal compression capability;compression method;independent component analysis method;principal component analysis;independent components;deep autoencoder;reconstruction phase;compressed components;high compression rate;low reconstruction error","","","39","","","","","IEEE","IEEE Journals"
"Two-Stage Deep Learning for Noisy-Reverberant Speech Enhancement","Y. Zhao; Z. Wang; D. Wang","Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","1","53","62","In real-world situations, speech reaching our ears is commonly corrupted by both room reverberation and background noise. These distortions are detrimental to speech intelligibility and quality, and also pose a serious problem to many speech-related applications, including automatic speech and speaker recognition. In order to deal with the combined effects of noise and reverberation, we propose a two-stage strategy to enhance corrupted speech, where denoising and dereverberation are conducted sequentially using deep neural networks. In addition, we design a new objective function that incorporates clean phase during model training to better estimate spectral magnitudes, which would in turn yield better phase estimates when combined with iterative phase reconstruction. The two-stage model is then jointly trained to optimize the proposed objective function. Systematic evaluations and comparisons show that the proposed algorithm improves objective metrics of speech intelligibility and quality substantially, and significantly outperforms previous one-stage enhancement systems.","","","10.1109/TASLP.2018.2870725","Air Force Office of Scientific Research; NIH; Starkey research gift; Ohio Supercomputer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466616","Deep neural networks;denoising;dereverberation;phase;ideal ratio mask","Noise measurement;Noise reduction;Speech enhancement;Training;Reverberation;Linear programming;Time-domain analysis","iterative methods;learning (artificial intelligence);neural nets;reverberation;signal reconstruction;speaker recognition;speech enhancement;speech intelligibility","automatic speech;speaker recognition;deep neural networks;objective function;phase estimates;iterative phase reconstruction;two-stage model;speech intelligibility;one-stage enhancement systems;noisy-reverberant speech enhancement;room reverberation;background noise;speech quality;two-stage deep learning;speech denoising;speech dereverberation;spectral magnitudes estimation","","5","46","","","","","IEEE","IEEE Journals"
"Deep Deterministic Policy Gradient (DDPG)-Based Energy Harvesting Wireless Communications","C. Qiu; Y. Hu; Y. Chen; B. Zeng","Center for Intelligent Networking and Communications, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Center for Intelligent Networking and Communications, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Internet of Things Journal","","2019","6","5","8577","8588","To overcome the difficulties of charging the wireless sensors in the wild with conventional energy supply, more and more researchers have focused on the sensor networks with renewable generations. Considering the uncertainty of the renewable generations, an effective energy management strategy is necessary for the sensors. In this paper, we propose a novel energy management algorithm based on the reinforcement learning. By utilizing deep deterministic policy gradient (DDPG), the proposed algorithm is applicable for the continuous states and realizes the continuous energy management. We also propose a state normalization algorithm to help the neural network initialize and learn. With only one day's real solar data and the simulative channel data for training, the proposed algorithm shows excellent performance in the validation with about 800 days length of real solar data. Compared with the state-of-the-art algorithms, the proposed algorithm achieves better performance in terms of long-term average net bit rate.","","","10.1109/JIOT.2019.2921159","National Natural Science Foundation of China; 111 Project; Thousand Youth Talents Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8731635","Deep deterministic policy gradient (DDPG);energy harvesting;Lyapunov optimization;Markov decision process (MDP);reinforcement learning;sensor networks","Energy harvesting;Wireless communication;Wireless sensor networks;Energy management;Reinforcement learning;Optimization;Sensors","energy harvesting;energy management systems;gradient methods;learning (artificial intelligence);neural nets;telecommunication computing;telecommunication power management;wireless channels;wireless sensor networks","deep deterministic policy gradient-based energy;DDPG;renewable generations;reinforcement learning;state normalization algorithm;solar data;energy harvesting wireless communications;wireless sensor networks;neural network;continuous energy management algorithm","","1","31","","","","","IEEE","IEEE Journals"
"Learning Cross-Modality Representations From Multi-Modal Images","G. van Tulder; M. de Bruijne","Biomedical Imaging Group, Erasmus MC, Rotterdam, The Netherlands; Biomedical Imaging Group, Erasmus MC, Rotterdam, The Netherlands","IEEE Transactions on Medical Imaging","","2019","38","2","638","648","Machine learning algorithms can have difficulties adapting to data from different sources, for example from different imaging modalities. We present and analyze three techniques for unsupervised cross-modality feature learning, using a shared auto-encoder-like convolutional network that learns a common representation from multi-modal data. We investigate a form of feature normalization, a learning objective that minimizes cross-modality differences, and modality dropout, in which the network is trained with varying subsets of modalities. We measure the same-modality and cross-modality classification accuracies and explore whether the models learn modality-specific or shared features. This paper presents experiments on two public data sets, with knee images from two MRI modalities, provided by the Osteoarthritis Initiative, and brain tumor segmentation on four MRI modalities from the BRATS challenge. All three approaches improved the cross-modality classification accuracy, with modality dropout and per-feature normalization giving the largest improvement. We observed that the networks tend to learn a combination of cross-modality and modality-specific features. Overall, a combination of all three methods produced the most cross-modality features and the highest cross-modality classification accuracy, while maintaining most of the same-modality accuracy.","","","10.1109/TMI.2018.2868977","Nederlandse Organisatie voor Wetenschappelijk Onderzoek; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456579","Representation learning;transfer learning;autoencoders;deep learning","Magnetic resonance imaging;Biomedical imaging;Encoding;Decoding;Training;Image reconstruction;Computed tomography","biomedical MRI;brain;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;tumours","cross-modality features;highest cross-modality classification accuracy;same-modality accuracy;cross-modality representations;multimodal images;machine learning algorithms;unsupervised cross-modality feature;learning objective;modality dropout;MRI modalities;per-feature normalization;modality-specific features;auto-encoder-like convolutional network","","","25","","","","","IEEE","IEEE Journals"
"Real-Time Vehicle Detection Using an Effective Region Proposal-Based Depth and 3-Channel Pattern","V. D. Nguyen; D. T. Tran; J. Y. Byun; J. W. Jeon","Department of Electrical and Computer Engineering, College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; Image Processing Group, UlikeKorea Company, Inc., Seoul, South Korea; Department of Electrical and Computer Engineering, College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Transactions on Intelligent Transportation Systems","","2019","20","10","3634","3646","Traditional deep learning-based vehicle detection methods are often designed using a pyramid of filters with multiple scales and sizes; therefore, the processing time is slow due to the large number of scales used and because the classifier runs at all scales. Recently, a deep learning-based region proposal network was introduced to detect vehicles that only employ the network one time regardless of the size of the input image. In object detection, deep learning-based region proposal networks have achieved state-of-the-art performance in terms of accuracy. These systems achieve a very high accuracy under normal driving conditions; however, their performance decreases under difficult driving conditions such as in snow, rain, or fog. In addition, the current state-of-the-art system-based region proposal networks still fail to satisfy the real-time requirements of the driving assistant systems. More recently, the identification of local patterns has been shown to improve the performance of the traditional deep-learning systems; hence, this paper investigates local patterns in region proposal networks to improve their accuracy. Depth information is also investigated to improve the processing time of current region proposal networks. Our experimental results show that the proposed system obtains better performance than the state-of-the-art object region detection systems in terms of both accuracy and processing time.","","","10.1109/TITS.2018.2877200","MSIP, South Korea, under the G-ITRC Support Program; Basic Science Research Program through NRF of South Korea; MOE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8534313","Deep learning;region proposal network;local pattern","Proposals;Vehicle detection;Real-time systems;Object detection;Neural networks;Cameras","driver information systems;image classification;learning (artificial intelligence);object detection;road vehicles;traffic engineering computing","object detection;deep learning-based region proposal network;deep-learning systems;current region proposal networks;traditional deep learning-based vehicle detection methods;real-time vehicle detection;region proposal-based depth;3-channel pattern;object region detection systems;multiple scales;system-based region proposal networks;driving assistant systems","","","50","","","","","IEEE","IEEE Journals"
"3-D Deformable Object Manipulation Using Deep Neural Networks","Z. Hu; T. Han; P. Sun; J. Pan; D. Manocha","Department of Biomedical Engineering, City University of Hong Kong, Hong Kong; Department of Biomedical Engineering, City University of Hong Kong, Hong Kong; Department of Computer Science, The University of Hong Kong, Hong Kong; Department of Computer Science, The University of Hong Kong, Hong Kong; Department of Computer Science, University of Maryland, College Park, MD, USA","IEEE Robotics and Automation Letters","","2019","4","4","4255","4261","Due to its high dimensionality, deformable object manipulation is a challenging problem in robotics. In this letter, we present a deep neural network based controller to servo control the position and shape of deformable objects with unknown deformation properties. In particular, a multi-layer neural network is used to map between the robotic end-effector's movement and the object's deformation measurement using an online learning strategy. In addition, we introduce a novel feature to describe deformable objects' deformation used in visual servoing. This feature is directly extracted from the 3-D point cloud rather from the 2-D image as in previous work. In addition, we perform simultaneous tracking and reconstruction for the deformable object to resolve the partial observation problem during the deformable object manipulation. We validate the performance of our algorithm and controller on a set of deformable object manipulation tasks and demonstrate that our method can achieve effective and accurate servo control for general deformable objects with a wide variety of goal settings. Experiment videos are available at https://sites.google.com/view/mso-deep.","","","10.1109/LRA.2019.2930476","HKSAR Research Grants Council General Research Fund; NSFC/RGC Joint Research Scheme; Innovation and Technology Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8769898","Deep Learning in Robotics and Automation;Visual Servoing;Dual Arm Manipulation;RGB-D Perception;Model Learning for Control","Strain;Deformable models;Three-dimensional displays;Neural networks;Histograms;Feature extraction;Robots","end effectors;learning (artificial intelligence);neurocontrollers;position control;robot vision;visual servoing","deformable object manipulation tasks;general deformable objects;deep neural network;unknown deformation properties;multilayer neural network;robotic end-effector movement;object deformation measurement;online learning strategy;visual servoing;3-D point cloud;2-D image","","","24","Traditional","","","","IEEE","IEEE Journals"
"Landmine Detection Using Multispectral Images","J. S. Silva; I. F. L. Guerra; J. Bioucas-Dias; T. Gasche","Portuguese Military Academy and CINAMIL, Rua Gomes Freire, Lisbon, Portugal; Portuguese Military Academy and CINAMIL, Rua Gomes Freire, Lisbon, Portugal; Instituto de Telecomunicações and Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; Portuguese Military Academy and CINAMIL, Rua Gomes Freire, Lisbon, Portugal","IEEE Sensors Journal","","2019","19","20","9341","9351","The conditions in which images are obtained to perform multispectral detection of landmines have a direct influence on the methods that are used to perform automatic detection of landmines. In this paper, two methodologies are proposed: one using traditional classifiers and the other using deep learning, namely, a convolutional neuronal network (CNN). In the first methodology, classifier fusion techniques are also used. The performance of the first methodology was evaluated as a function of the number of landmine features, the environment, and the depth of the mine. In deep learning, a study was carried out based on the feature map, the type of landmine, and the environment. A quantitative analysis shows that traditional classifiers achieved an overall accuracy of above 97% in indoor and outdoor environments for the detection of landmines. The adopted deep learning methodology presented an increase in the performance for larger mines and a decrease for smaller ones. These experimental results shed light on the factors that influence the detection of mines and into the advantages and disadvantages of CNN compared with the classical classifier methods.","","","10.1109/JSEN.2019.2925203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746208","Landmine;detection;classifier combination;deep learning;convolutional neuronal network","Landmine detection;Explosives;Soil;Sensors;Feature extraction;Deep learning;Acoustics","convolutional neural nets;feature extraction;image classification;landmine detection;learning (artificial intelligence)","convolutional neuronal network;CNN;classifier fusion techniques;landmine features;feature map;indoor environments;outdoor environments;adopted deep learning methodology;landmine detection;multispectral images;quantitative analysis","","1","32","","","","","IEEE","IEEE Journals"
"Learning content-adaptive feature pooling for facial depression recognition in videos","X. Zhou; P. Huang; H. Liu; S. Niu","Beijing University of Posts and Telecommunications, People's Republic of China; Beijing University of Posts and Telecommunications, People's Republic of China; Peking University, People's Republic of China; Peking University People's Hospital, People's Republic of China","Electronics Letters","","2019","55","11","648","650","Recently, a deep representation of facial depression built on convolutional neural networks has shown impressive performance in video-based depression recognition. However, most existing approaches either fix the weights or using a certain heuristics to integrate the frame-level facial features, resulting in suboptimal feature aggregation in encoding the helpful while discarding noisy information in videos. To address this issue, the authors introduce the memory attention mechanism in a regression network to learn a deep discriminative depression representation, where the residual network module aims at learning frame-level deep feature, while the attention module acts as a pooling layer by adaptively learning the weights emphasising or suppressing face images with varying poses and imaging conditions. They empirically evaluate the proposed approach on a benchmark depression dataset, and the results demonstrate the superiority of their approach over the state-of-the-art alternatives.","","","10.1049/el.2019.0443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730891","","","image representation;image classification;feature extraction;learning (artificial intelligence);face recognition;neural nets","content-adaptive feature pooling;facial depression recognition;videos;deep representation;convolutional neural networks;impressive performance;video-based depression recognition;frame-level facial features;suboptimal feature aggregation;noisy information;memory attention mechanism;regression network;deep discriminative depression representation;residual network module;learning frame-level deep feature;attention module acts;pooling layer;benchmark depression dataset","","","11","","","","","IET","IET Journals"
"Remote Sensor Design for Visual Recognition With Convolutional Neural Networks","L. Jaffe; M. Zelinski; W. Sakla","Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","9090","9108","While deep learning technologies for computer vision have developed rapidly since 2012, modeling of remote sensing systems has remained focused around human vision. In particular, remote sensing systems are usually constructed to optimize sensing cost-quality tradeoffs with respect to human image interpretability. While some recent studies have explored remote sensing system design as a function of simple computer vision algorithm performance, there has been little work relating this design to the state of the art in computer vision: deep learning with convolutional neural networks. We develop experimental systems to conduct this analysis, showing results with modern deep learning algorithms and recent overhead image data. Our results are compared to standard image quality measurements based on human visual perception, and we conclude not only that machine and human interpretability differ significantly but also that computer vision performance is largely self-consistent across a range of disparate conditions. This paper is presented as a cornerstone for a new generation of sensor design systems that focus on computer algorithm performance instead of human visual perception.","","","10.1109/TGRS.2019.2925813","U.S. Department of Energy; Lawrence Livermore National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809353","Convolutional neural network (CNN);deep learning;image system design;remote sensing;satellite imagery;transfer learning","Measurement;Visualization;Image recognition;Image quality;Remote sensing;Signal to noise ratio;Mathematical model","computer vision;computerised instrumentation;convolutional neural nets;image recognition;image sensors;learning (artificial intelligence);open systems;remote sensing","visual recognition;convolutional neural networks;deep learning technologies;human vision;sensing cost-quality tradeoffs;human image interpretability;remote sensing system design;deep learning algorithms;standard image quality measurements;human visual perception;human interpretability;computer vision algorithm performance;overhead image data","","","57","CCBY","","","","IEEE","IEEE Journals"
"Robust Discriminative Metric Learning for Image Representation","Z. Ding; M. Shao; W. Hwang; S. Suh; J. Han; C. Choi; Y. Fu","Department of Computer, Information and Technology, Indiana University–Purdue University Indianapolis, Indianapolis, IN, USA; Computer and Information Science, University of Massachusetts Dartmouth, North Dartmouth, MA, USA; Department of Software and Computer Engineering, College of Information Technology, Ajou University, Suwon, South Korea; Software Solution Laboratory, Samsung Advanced Institute of Technology, Suwon, South Korea; Software Solution Laboratory, Samsung Advanced Institute of Technology, Suwon, South Korea; Software Solution Laboratory, Samsung Advanced Institute of Technology, Suwon, South Korea; Department of Electrical and Computer Engineering, College of Computer and Information Science, Northeastern University, Boston, MA, USA","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","11","3173","3183","Metric learning has attracted significant attention in the past decades, because of its appealing advances in various real-world tasks, e.g., person re-identification and face recognition. Traditional supervised metric learning attempts to seek a discriminative metric, which could minimize the pairwise distance of within-class data samples, while maximizing the pairwise distance of data samples from various classes. However, it is still a challenge to build a robust and discriminative metric, especially for corrupted data in the real-world application. In this paper, we propose a Robust Discriminative Metric Learning algorithm through fast low-rank representation and denoising strategy. To be specific, the metric learning problem is guided by a discriminative regularization by incorporating the pair-wise or class-wise information. Moreover, the low-rank basis learning is jointly optimized with the metric to better uncover the global data structure and remove noise. Furthermore, the fast low-rank representation is implemented to mitigate the computational burden and ensure the scalability on large-scale datasets. Finally, we evaluate our learned metric on several challenging tasks, e.g., face recognition/verification, object recognition, image clustering, and person re-identification. The experimental results verify the effectiveness of our proposed algorithm in comparison to many metric learning algorithms, even deep learning ones.","","","10.1109/TCSVT.2018.2879626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8522031","Robust metric learning;fast low-rank representation","Measurement;Data models;Noise reduction;Optimization;Face recognition;Feature extraction;Machine learning algorithms","data structures;face recognition;image denoising;image representation;learning (artificial intelligence);pattern clustering","image representation;person re-identification;pairwise distance;within-class data samples;corrupted data;low-rank representation;denoising strategy;discriminative regularization;class-wise information;low-rank basis learning;global data structure;deep learning;robust discriminative metric learning;face recognition;pair-wise information;face verification;object recognition;image clustering","","","53","","","","","IEEE","IEEE Journals"
"Unsupervised Difference Representation Learning for Detecting Multiple Types of Changes in Multitemporal Remote Sensing Images","P. Zhang; M. Gong; H. Zhang; J. Liu; Y. Ban","Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Division of Geoinformatics, KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","4","2277","2289","With the rapid increase of remote sensing images in temporal, spectral, and spatial resolutions, it is urgent to develop effective techniques for joint interpretation of spatial-temporal images. Multitype change detection (CD) is a significant research topic in multitemporal remote sensing image analysis, and its core is to effectively measure the difference degree and represent the difference among the multitemporal images. In this paper, we propose a novel difference representation learning (DRL) network and present an unsupervised learning framework for multitype CD task. Deep neural networks work well in representation learning but rely too much on labeled data, while clustering is a widely used classification technique free from supervision. However, the distribution of real remote sensing data is often not very friendly for clustering. To better highlight the changes and distinguish different types of changes, we combine difference measurement, DRL, and unsupervised clustering into a unified model, which can be driven to learn Gaussian-distributed and discriminative difference representations for nonchange and different types of changes. Furthermore, the proposed model is extended into an iterative framework to imitate the bottom-up aggregative clustering procedure, in which similar change types are gradually merged into the same classes. At the same time, the training samples are updated and reused to ensure that it converges to a stable solution. The experimental studies on four pairs of multispectral data sets demonstrate the effectiveness and superiority of the proposed model on multitype CD.","","","10.1109/TGRS.2018.2872509","National Natural Science Foundation of China; National Program for Support of Top-Notch Young Professionals of China; National Key Research and Development Program of China; Key Research and Development Program of Shaanxi Province; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8506472","Change detection (CD);deep neural networks (DNNs);difference representation (DR);multiclass changes;multitemporal image analysis;remote sensing","Remote sensing;Training;Task analysis;Earth;Machine learning;Spatial resolution;Image analysis","Gaussian distribution;geophysical image processing;image classification;image resolution;neural nets;pattern clustering;remote sensing;unsupervised learning","unsupervised difference representation learning;temporal resolutions;spatial resolutions;multitype change detection;multitemporal remote sensing image analysis;unsupervised learning framework;deep neural networks;unsupervised clustering;discriminative difference representations;aggregative clustering procedure;spectral resolutions;DRL network;multiple type detection;difference representation learning network;Gaussian-distributed representations","","1","43","","","","","IEEE","IEEE Journals"
"PhaseNet: A Deep Convolutional Neural Network for Two-Dimensional Phase Unwrapping","G. E. Spoorthi; S. Gorthi; R. K. S. S. Gorthi","Department of Electrical Engineering, Indian Institute of Technology Tirupati, Tirupati, India; Department of Electrical Engineering, Indian Institute of Technology Tirupati, Tirupati, India; Department of Electrical Engineering, Indian Institute of Technology Tirupati, Tirupati, India","IEEE Signal Processing Letters","","2019","26","1","54","58","Phase unwrapping is a crucial signal processing problem in several applications that aims to restore original phase from the wrapped phase. In this letter, we propose a novel framework for unwrapping the phase using deep fully convolutional neural network termed as PhaseNet. We reformulate the problem definition of directly obtaining continuous original phase as obtaining the wrap-count (integer jump of 2 π) at each pixel by semantic segmentation and this is accomplished through a suitable deep learning framework. The proposed architecture consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The relationship between the absolute phase and the wrap-count is leveraged in generating abundant simulated data of several random shapes. This deliberates the network on learning continuity in wrapped phase maps rather than specific patterns in the training data. We compare the proposed framework with the widely adapted quality-guided phase unwrapping algorithm and also with the well-known MATLAB's unwrap function for varying noise levels. The proposed framework is found to be robust to noise and computationally fast. The results obtained highlight that deep convolutional neural network can indeed be effectively applied for phase unwrapping, and the proposed framework will hopefully pave the way for the development of a new set of deep learning based phase unwrapping methods.","","","10.1109/LSP.2018.2879184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519608","Decoder;deep convolutional neural network;encoder;phase unwrapping;semantic segmentation","Training;Decoding;Signal processing algorithms;Semantics;Matlab;Shape","convolution;feedforward neural nets;image classification;image coding;image segmentation;learning (artificial intelligence)","deep convolutional neural network;deep fully convolutional neural network;PhaseNet;encoder network;pixel-wise classification layer;wrapped phase maps;deep learning framework;decoder network;quality-guided phase unwrapping algorithm;MATLAB unwrap function;two-dimensional phase unwrapping;semantic segmentation","","","25","","","","","IEEE","IEEE Journals"
"Sliding-Window Temporal Attention Based Deep Learning System for Robust Sensor Modality Fusion for UGV Navigation","H. U. Unlu; N. Patel; P. Krishnamurthy; F. Khorrami","Controls/Robotics Research Laboratory, Department of Electrical and Computer Engineering, NYU Tandon School of Engineering, Brooklyn, NY, USA; Controls/Robotics Research Laboratory, Department of Electrical and Computer Engineering, NYU Tandon School of Engineering, Brooklyn, NY, USA; Controls/Robotics Research Laboratory, Department of Electrical and Computer Engineering, NYU Tandon School of Engineering, Brooklyn, NY, USA; Controls/Robotics Research Laboratory, Department of Electrical and Computer Engineering, NYU Tandon School of Engineering, Brooklyn, NY, USA","IEEE Robotics and Automation Letters","","2019","4","4","4216","4223","We propose a novel temporal attention based neural network architecture for robotics tasks that involve fusion of time series of sensor data, and evaluate the performance improvements in the context of autonomous navigation of unmanned ground vehicles (UGVs) in uncertain environments. The architecture generates feature vectors by fusing raw pixel and depth values collected by camera(s) and LiDAR(s), stores a history of the generated feature vectors, and incorporates the temporally attended history with current features to predict a steering command. The experimental studies show the robust performance in unknown and cluttered environments. Furthermore, the temporal attention is resilient to noise, bias, blur, and occlusions in the sensor signals. We trained the network on indoor corridor datasets (that will be publicly released) from our UGV. The datasets have LiDAR depth measurements, camera images, and human tele-operation commands.","","","10.1109/LRA.2019.2930475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770056","Autonomous vehicle navigation;sensor fusion;deep learning in robotics and automation","Robot sensing systems;Navigation;Laser radar;History;Cameras;Computer architecture;Microsoft Windows","cameras;learning (artificial intelligence);navigation;neurocontrollers;optical radar;path planning;radar clutter;remotely operated vehicles;sensor fusion;spatial variables measurement;time series;vectors","steering command;cluttered environments;LiDAR depth measurements;sliding-window temporal attention;robust sensor modality fusion;UGV navigation;neural network architecture;robotics tasks;time series;autonomous navigation;unmanned ground vehicles;feature vector generation;deep learning system;temporal attention based neural network architecture;camera;indoor corridor datasets;human teleoperation commands","","","35","Traditional","","","","IEEE","IEEE Journals"
"Learning Affordance Segmentation for Real-World Robotic Manipulation via Synthetic Images","F. Chu; R. Xu; P. A. Vela","Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA; Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA; Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Robotics and Automation Letters","","2019","4","2","1140","1147","This letter presents a deep learning framework to predict the affordances of object parts for robotic manipulation. The framework segments affordance maps by jointly detecting and localizing candidate regions within an image. Rather than requiring annotated real-world images, the framework learns from synthetic data and adapts to real-world data without supervision. The method learns domain-invariant region proposal networks and task-level domain adaptation components with regularization on the predicted domains. A synthetic version of the UMD data set is collected for autogenerating annotated, synthetic input data. Experimental results show that the proposed method outperforms an unsupervised baseline, and achieves performance close to state-of-the-art supervised approaches. An ablation study establishes the performance gap between the proposed method and the supervised equivalent (30%). Real-world manipulation experiments demonstrate use of the affordance segmentations for task execution, which achieves the same performance with supervised approaches.","","","10.1109/LRA.2019.2894439","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620534","Perception for Grasping and Manipulation;Deep Learning in Robotics and Automation;RGB-D Perception","Image segmentation;Task analysis;Adaptation models;Robots;Feature extraction;Data models;Training","computer vision;image representation;image segmentation;learning (artificial intelligence);unsupervised learning","synthetic data;real-world data;domain-invariant region proposal networks;task-level domain adaptation components;synthetic version;UMD data set;synthetic input data;supervised equivalent;real-world manipulation experiments;affordance segmentation;real-world robotic manipulation;deep learning framework;framework segments;real-world images","","","55","","","","","IEEE","IEEE Journals"
"Deep Collaborative Embedding for Social Image Understanding","Z. Li; J. Tang; T. Mei","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; JD AI Research Building A, North-Star Century Center, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","9","2070","2083","In this work, we investigate the problem of learning knowledge from the massive community-contributed images with rich weakly-supervised context information, which can benefit multiple image understanding tasks simultaneously, such as social image tag refinement and assignment, content-based image retrieval, tag-based image retrieval and tag expansion. Towards this end, we propose a Deep Collaborative Embedding (DCE) model to uncover a unified latent space for images and tags. The proposed method incorporates the end-to-end learning and collaborative factor analysis in one unified framework for the optimal compatibility of representation learning and latent space discovery. A nonnegative and discrete refined tagging matrix is learned to guide the end-to-end learning. To collaboratively explore the rich context information of social images, the proposed method integrates the weakly-supervised image-tag correlation, image correlation and tag correlation simultaneously and seamlessly. The proposed model is also extended to embed new tags in the uncovered space. To verify the effectiveness of the proposed method, extensive experiments are conducted on two widely-used social image benchmarks for multiple social image understanding tasks. The encouraging performance of the proposed method over the state-of-the-art approaches demonstrates its superiority.","","","10.1109/TPAMI.2018.2852750","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8403294","Image understanding;embedding;deep learning;tag completion;image retrieval","Correlation;Task analysis;Image retrieval;Semantics;Visualization;Image annotation;Collaboration","content-based retrieval;image representation;image retrieval;matrix decomposition;supervised learning","end-to-end learning;rich context information;weakly-supervised image-tag correlation;massive community-contributed images;weakly-supervised context information;content-based image retrieval;tag-based image retrieval;tag expansion;unified latent space;collaborative factor analysis;representation learning;multiple social image understanding;DCE model;deep collaborative embedding model;discrete refined tagging matrix;nonnegative tagging matrix;latent space discovery","","11","60","","","","","IEEE","IEEE Journals"
"Robust Object Tracking Using Manifold Regularized Convolutional Neural Networks","H. Hu; B. Ma; J. Shen; H. Sun; L. Shao; F. Porikli","Beijing Laboratory of Intelligent Information Technology, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Research School of Engineering, Australian National University, Canberra, ACT, Australia","IEEE Transactions on Multimedia","","2019","21","2","510","521","In visual tracking, usually only a small number of samples are labeled, and most existing deep learning based trackers ignore abundant unlabeled samples that could provide additional information for deep trackers to boost their tracking performance. An intuitive way to explain unlabeled data is to incorporate manifold regularization into the common classification loss functions, but the high computational cost may prohibit those deep trackers from practical applications. To overcome this issue, we propose a two-stage approach to a deep tracker that takes into account both labeled and unlabeled samples. The annotation of unlabeled samples is propagated from its labeled neighbors first by exploring the manifold space that these samples are assumed to lie in. Then, we refine it by training a deep convolutional neural network using both labeled and unlabeled data in a supervised manner. Online visual tracking is further carried out under the framework of particle filters with the presented manifold regularized deep model being updated every few frames. Experimental results on different tracking datasets demonstrate that our tracker outperforms most existing tracking approaches. The source code and results are available at: https://github.com/shenjianbing/MRCNNTracking.","","","10.1109/TMM.2018.2859831","National Natural Science Foundation of China; Beijing Natural Science Foundation; Australian Research Council; Specialized Fund for Joint Building Program of Beijing Municipal Education Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419331","Convolutional neural networks;deep learning;deep tracker;manifold regularization;object tracking;online tracking","Visualization;Manifolds;Target tracking;Laplace equations;Training;Feature extraction","image representation;learning (artificial intelligence);neural nets;object tracking;particle filtering (numerical methods);pattern classification","labeled neighbors;manifold space;deep convolutional neural network;unlabeled data;online visual tracking;existing tracking approaches;robust object tracking;manifold regularized convolutional neural networks;abundant unlabeled samples;deep tracker;tracking performance;incorporate manifold regularization;common classification loss functions;high computational cost;deep learning based trackers","","2","48","","","","","IEEE","IEEE Journals"
"Enhanced Device-Free Human Detection: Efficient Learning From Phase and Amplitude of Channel State Information","S. Fang; C. Li; W. Lu; Z. Xu; Y. Chien","Department of Electrical Engineering, Yuan Ze University, Taoyuan, Taiwan; Department of Electrical Engineering, Yuan Ze University, Taoyuan, Taiwan; Department of Electrical Engineering, Yuan Ze University, Taoyuan, Taiwan; School of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Department of Electrical Engineering, National Ilan University, Yilan City, Taiwan","IEEE Transactions on Vehicular Technology","","2019","68","3","3048","3051","With the rapidly increasing demand for security and E-health applications, device-free human detection has attracted interest because it does not require a wearable device or camera setup. This paper proposes a deep-learning-based approach that monitors wireless signals to learn three human modes, i.e., absence, working, and sleeping, in realistic indoor environments. This paper integrates the amplitude and phase of channel state information to propose a hybrid complex feature; this facilitates robust and efficient human detection even with fewer data samples. Experiments conducted in two unmodified WiFi networks demonstrate the effectiveness of the proposed algorithms. Four machine-learning algorithms provide satisfactory performance with sufficient data, and deep neural networks perform the best. Results show that by using 6% training samples, the proposed hybrid feature still achieves 93% accuracy and can even outperform three typical machine learning algorithms that use full training samples. Moreover, the proposed feature significantly improves detection accuracy by 11.62%-27.76% than traditional amplitude feature with fewer training samples.","","","10.1109/TVT.2019.2892563","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8610237","Channel state information;device-free detection;human detection;WiFi application","Training;Feature extraction;Data models;Testing;Channel state information;Wireless fidelity;Machine learning","biomedical communication;health care;learning (artificial intelligence);medical signal processing;neural nets;object detection;patient monitoring;wireless channels;wireless LAN","efficient learning;channel state information;E-health applications;wireless signals;human modes;realistic indoor environments;hybrid complex feature;unmodified WiFi networks;machine-learning algorithms;deep neural networks;hybrid feature;data samples;training samples;enhanced device-free human detection;deep-learning-based approach;absence mode;working mode;sleeping mode","","1","25","","","","","IEEE","IEEE Journals"
"GenSynth: a generative synthesis approach to learning generative machines for generate efficient neural networks","A. Wong; M. Javad Shafiee; B. Chwyl; F. Li","Waterloo Artificial Intelligence Institute, Canada; Waterloo Artificial Intelligence Institute, Canada; DarwinAI Corp., Canada; DarwinAI Corp., Canada","Electronics Letters","","2019","55","18","986","989","The tremendous potential exhibited by deep learning is often offset by architectural and computational complexity, making widespread deployment a challenge for edge scenarios such as mobile and other consumer devices. To tackle this challenge, we explore the following idea: Can we learn generative machines to automatically generate deep neural networks with efficient network architectures? In this study, we introduce the idea of generative synthesis, which is premised on the intricate interplay between a generator-inquisitor pair that work in tandem to garner insights and learn to generate highly efficient deep neural networks that best satisfies operational requirements. Experimental results for image classification, semantic segmentation, and object detection tasks illustrate the efficacy of generative synthesis (GenSynth) in producing generators that automatically generate highly efficient deep neural networks (which we nickname FermiNets with higher model efficiency and lower computational costs (reaching $\gt 10\times $>10× more efficient and fewer multiply-accumulate operations than several tested state-of-the-art networks), as well as higher energy efficiency (reaching $\gt 4\times $>4× improvements in image inferences per joule consumed on a Nvidia Tegra X2 mobile processor). As such, GenSynth can be a powerful, generalised approach for accelerating and improving the building of deep neural networks for on-device edge scenarios.","","","10.1049/el.2019.1719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8822909","","","computational complexity;image classification;learning (artificial intelligence);neural nets;object detection","consumer devices;operational requirements;architectural complexity;deep learning;efficient neural networks;generative synthesis approach;higher energy efficiency;tested state-of-the-art networks;higher model efficiency;unique highly efficient deep neural networks;generator-inquisitor pair;GenSynth;efficient network architectures;generative machines;computational complexity","","","25","","","","","IET","IET Journals"
"Effective Building Extraction From High-Resolution Remote Sensing Images With Multitask Driven Deep Neural Network","J. Hui; M. Du; X. Ye; Q. Qin; J. Sui","School of Earth and Space Sciences, Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, China; School of Mathematical Sciences, Peking University, Beijing, China; School of Earth and Space Sciences, Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, China; School of Earth and Space Sciences, Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, China; School of Earth and Space Sciences, Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","5","786","790","Building extraction from high-resolution remote sensing images has widely been studied for its great significance in obtaining geographic information. Many methods based on deep learning have been tried for the task; however, there is still much to explore about designing layers or modules for remote sensing data and taking full use of the unique features of buildings like shape and boundary. In this letter, an end-to-end network architecture based on U-Net is proposed. The U-Net architecture is modified with Xception module for remote sensing images to extract effective features. Also, multitask learning is adopted to incorporate the structure information of buildings. Two standard data sets (Massachusetts building data set and Vaihingen Data set) of high-resolution remote sensing images are selected to test our model and it achieves state-of-the-art results.","","","10.1109/LGRS.2018.2880986","National Key Technologies Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554076","Building extraction;deep neural network;multitask learning;remote sensing image;Xception module","Feature extraction;Remote sensing;Task analysis;Buildings;Neural networks;Training;Decoding","feature extraction;geographic information systems;geophysical image processing;image resolution;learning (artificial intelligence);neural nets;remote sensing","high-resolution remote sensing images;multitask driven deep neural network;remote sensing data;end-to-end network architecture;geographic information;deep learning;building extraction;Xception module;U-Net","","1","19","","","","","IEEE","IEEE Journals"
"Remote Sensing Image Superresolution Using Deep Residual Channel Attention","J. M. Haut; R. Fernandez-Beltran; M. E. Paoletti; J. Plaza; A. Plaza","Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Institute of New Imaging Technologies Universitat Jaume I, Castellón de la Plana, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","9277","9289","The current trend in remote sensing image superresolution (SR) is to use supervised deep learning models to effectively enhance the spatial resolution of airborne and satellite-based optical imagery. Nonetheless, the inherent complexity of these architectures/data often makes these methods very difficult to train. Despite these recent advances, the huge amount of network parameters that must be fine-tuned and the lack of suitable high-resolution remotely sensed imagery in actual operational scenarios still raise some important challenges that may become relevant limitations in the existent earth observation data production environments. To address these problems, we propose a new remote sensing SR approach that integrates a visual attention mechanism within a residual-based network design in order to allow the SR process to focus on those features extracted from land-cover components that require more computations to be superresolved. As a result, the network training process is significantly improved because it aims at learning the most relevant high-frequency information while the proposed architecture allows neglecting the low-frequency features extracted from spatially uninformative earth surface areas by means of several levels of skip connections. Our experimental assessment, conducted using the University of California at Merced and GaoFen-2 remote sensing image collections, three scaling factors, and eight different SR methods, demonstrates that our newly proposed approach exhibits competitive performance in the task of superresolving remotely sensed imagery.","","","10.1109/TGRS.2019.2924818","Ministerio de Educación, Cultura y Deporte; Consejería de Educación y Empleo, Junta de Extremadura; Generalitat Valenciana; Secretaría de Estado de Investigación, Desarrollo e Innovación; European Social Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770258","Deep learning;remote sensing;single-image superresolution (SR);visual attention (VA)","Remote sensing;Spatial resolution;Feature extraction;Visualization;Earth;Training","feature extraction;geophysical image processing;geophysical techniques;image classification;image resolution;learning (artificial intelligence);remote sensing","remote sensing image superresolution;deep residual channel attention;supervised deep learning models;spatial resolution;optical imagery;network parameters;suitable high-resolution remotely;remote sensing SR approach;visual attention mechanism;residual-based network design;SR process;network training process;spatially uninformative earth surface areas;earth observation data production environments","","","70","","","","","IEEE","IEEE Journals"
"Local Semantic-Aware Deep Hashing With Hamming-Isometric Quantization","Y. Wang; J. Liang; D. Cao; Z. Sun","Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2019","28","6","2665","2679","Hashing is a promising approach for compact storage and efficient retrieval of big data. Compared to the conventional hashing methods using handcrafted features, emerging deep hashing approaches employ deep neural networks to learn both feature representations and hash functions, which have been proven to be more powerful and robust in real-world applications. Currently, most of the existing deep hashing methods construct pairwise or triplet-wise constraints to obtain similar binary codes between a pair of similar data points or relatively similar binary codes within a triplet. However, we argue that some critical local structures have not been fully exploited. So, this paper proposes a novel deep hashing method named local semantic-aware deep hashing with Hamming-isometric quantization (LSDH), aiming to make full use of local similarity in hash function learning. Specifically, the potential semantic relation is exploited to robustly preserve local similarity of data in the Hamming space. In addition to reducing the error introduced by binary quantizing, a Hamming-isometric objective is designed to maximize the consistency of similarity between the pairwise binary-like features and corresponding binary codes pair, which is shown to be able to improve the quality of binary codes. Extensive experimental results on several benchmark datasets, including three single-label datasets and one multi-label dataset, demonstrate that the proposed LSDH achieves better performance than the latest state-of-the-art hashing methods.","","","10.1109/TIP.2018.2889269","National Natural Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8585128","Image retrieval;deep hashing;similarity-preserving;local structures;Hamming-isometric","Binary codes;Quantization (signal);Image retrieval;Semantics;Benchmark testing;Automation","binary codes;file organisation;information retrieval;learning (artificial intelligence);neural nets","deep neural networks;similar data points;critical local structures;deep hashing method;local similarity;hash function learning;potential semantic relation;Hamming-isometric quantization;semantic-aware deep hashing;pairwise binary-like features;Hamming-isometric objective;binary codes pair;similar binary codes","","","70","","","","","IEEE","IEEE Journals"
"Self-Supervised Feature Learning With CRF Embedding for Hyperspectral Image Classification","Y. Wang; J. Mei; L. Zhang; B. Zhang; P. Zhu; Y. Li; X. Li","School of Land Science and Technology, China University of Geosciences, Beijing, China; Faculty of Geographical Science, State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China; Faculty of Geographical Science, State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China; Faculty of Geographical Science, State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China; Faculty of Geographical Science, State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China; Faculty of Geographical Science, State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","5","2628","2642","The challenges in hyperspectral image (HSI) classification lie in the existence of noisy spectral information and lack of contextual information among pixels. Considering the three different levels in HSIs, i.e., subpixel, pixel, and superpixel, offer complementary information, we develop a novel HSI feature learning network (HSINet) to learn consistent features by self-supervision for HSI classification. HSINet contains a three-layer deep neural network and a multifeature convolutional neural network. It automatically extracts the features such as spatial, spectral, color, and boundary as well as context information. To boost the performance of self-supervised feature learning with the likelihood maximization, the conditional random field (CRF) framework is embedded into HSINet. The potential terms of unary, pairwise, and higher order in CRF are constructed by the corresponding subpixel, pixel, and superpixel. Furthermore, the feedback information derived from these terms are also fused into the different-level feature learning process, which makes the HSINet-CRF be a trainable end-to-end deep learning model with the back-propagation algorithm. Comprehensive evaluations are performed on three widely used HSI data sets and our method outperforms the state-of-the-art methods.","","","10.1109/TGRS.2018.2875943","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8548579","Conditional random field (CRF);convolutional neural network (CNN);feature learning;hyperspectral image (HSI) classification;self-supervision","Feature extraction;Shape;Semantics;Image segmentation;Convolutional neural networks;Hyperspectral sensors","backpropagation;convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;random processes;supervised learning","multifeature convolutional neural network;context information;self-supervised feature learning;conditional random field framework;HSINet-CRF;trainable end-to-end deep learning model;CRF embedding;noisy spectral information;three-layer deep neural network;hyperspectral image classification;HSI feature learning network;back-propagation algorithm","","","56","","","","","IEEE","IEEE Journals"
"u-DeepHand: FMCW Radar-Based Unsupervised Hand Gesture Feature Learning Using Deep Convolutional Auto-Encoder Network","Z. Zhang; Z. Tian; Y. Zhang; M. Zhou; B. Wang","School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; Department of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan, China","IEEE Sensors Journal","","2019","19","16","6811","6821","Recently, although radar sensors have been widely applied for hand gesture recognition (HGR) tasks, conventional radar-based HGR systems still have two major challenges. First, these systems rely on supervised learning approaches to learn gesture features, which normally require a large-scale labeled dataset to address the overfitting problem. However, the acquisition of such dataset is time-consuming. Second, the radar signature of hand movement is often influenced by micromotion caused by other body parts, which leads to distorted motion features, resulting in poor identification accuracy. To overcome these problems, we propose an unsupervised hand gesture feature learning method using the deep convolutional auto-encoder network to analyze hand gesture signal collected by a frequency modulated continuous wave (FMCW) radar sensor. First, via a convolutional encoder sub-network, input radar range profiles are transformed into lower dimensional representations. Then, the representations are expanded to reconstruct the corresponding input profiles by a deconvolutional decoder sub-network. In addition, to investigate the mechanisms of the proposed network and evaluate its performance, we conduct an in-depth study of the feature maps learned from various hand gesture experimental data and evaluate the corresponding classification performance. The results demonstrate that the proposed convolutional auto-encoder network is able to achieve high recognition accuracy with low training sample cost, which outperforms the state-of-the-art hand gesture recognition systems based on transfer learning VGGNet and fully connected-based auto-encoder network.","","","10.1109/JSEN.2019.2910810","National Natural Science Foundation of China; Program for Changjiang Scholars and Innovative Research Team in University; Special Fund of Chongqing Key Laboratory (CSTC), through Fundamental and Frontier Research Project of Chongqing; University Outstanding Achievement Transformation Project of Chongqing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8689114","Hand gesture recognition;FMCW radar;convolutional auto-encoder network;transfer learning VGGNet;feature maps","Sensors;Doppler radar;Training;Feature extraction;Convolution;Task analysis","convolutional neural nets;CW radar;feature extraction;FM radar;gesture recognition;image classification;radar imaging;unsupervised learning","FMCW radar-based unsupervised hand gesture feature learning;deep convolutional auto-encoder network;radar sensors;hand gesture recognition tasks;radar signature;hand movement;distorted motion features;hand gesture signal;convolutional encoder sub-network;input radar range profiles;deconvolutional decoder sub-network;feature maps;hand gesture experimental data;frequency modulated continuous wave radar sensor;radar-based HGR systems;performance evaluation;classification performance","","","52","","","","","IEEE","IEEE Journals"
"Smart Manufacturing Scheduling With Edge Computing Using Multiclass Deep Q Network","C. Lin; D. Deng; Y. Chih; H. Chiu","Department of Industrial Engineering and Management, National Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science and Information Engineering, National Changhua University of Education, Changhua, Taiwan; Department of Industrial Engineering and Management, National Chiao Tung University, Hsinchu, Taiwan; Department of Industrial Engineering and Management, National Chiao Tung University, Hsinchu, Taiwan","IEEE Transactions on Industrial Informatics","","2019","15","7","4276","4284","Manufacturing is involved with complex job shop scheduling problems (JSP). In smart factories, edge computing supports computing resources at the edge of production in a distributed way to reduce response time of making production decisions. However, most works on JSP did not consider edge computing. Therefore, this paper proposes a smart manufacturing factory framework based on edge computing, and further investigates the JSP under such a framework. With recent success of some AI applications, the deep Q network (DQN), which combines deep learning and reinforcement learning, has showed its great computing power to solve complex problems. Therefore, we adjust the DQN with an edge computing framework to solve the JSP. Different from the classical DQN with only one decision, this paper extends the DQN to address the decisions of multiple edge devices. Simulation results show that the proposed method performs better than the other methods using only one dispatching rule.","","","10.1109/TII.2019.2908210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676376","Deep Q network;edge computing;job shop scheduling;multiple dispatching rules;smart manufacturing","Production facilities;Dispatching;Edge computing;Job shop scheduling;Smart manufacturing;Task analysis","distributed processing;job shop scheduling;learning (artificial intelligence);production engineering computing","computing resources;job shop scheduling problems;dispatching rule;smart factories;multiclass deep Q network;smart manufacturing scheduling;edge computing framework;reinforcement learning;deep learning;JSP","","","30","","","","","IEEE","IEEE Journals"
"Intelligent Driving Data Recorder in Smartphone Using Deep Neural Network-Based Speedometer and Scene Understanding","Y. Gu; Q. Wang; S. Kamijo","Institute of Industrial Science, The University of Tokyo, Tokyo, Japan; Graduate School of Interdisciplinary Information Studies, The University of Tokyo, Tokyo, Japan; Interfaculty Initiative in Information Studies, The University of Tokyo, Tokyo, Japan","IEEE Sensors Journal","","2019","19","1","287","296","This paper proposes a smartphone-based Driving Data Recorder (DDR). The proposed DDR has the functions of accurate speed estimation and intelligent traffic scene understanding. DDRs are used to store the relevant driving data to provide feedback on driver behavior for accident analysis, insurance issue, and so on. The conventional DDRs are standalone devices with multiple sensors. The current DDR products record many useless data or lose important information. On the other hand, the widely used smartphones already have the hardware conditions to replace the conventional DDR products. This paper proposes to develop the intelligent DDR in the smartphones. Considering the requirements of the DDRs, two functions are developed in this paper: motion sensor-based speedometer and vision sensor-based scene understanding. The proposed speedometer function adopts double-layered Long Short-Term Memory (LSTM) network as the model, which can estimate the vehicle speed directly from gyroscope and accelerometer of a smartphone. The scene understanding function can detect road facilities such as traffic lights, crosswalks, and stop lines. The driving data recorded in those areas are very important for analyzing driver behaviors. In the development of the scene understanding function, maintaining high detection accuracy with reduced computation cost is significant due to the limitation of smartphones' processing resources. This paper uses a lightweight architecture deep learning network to achieve the goal. The proposed system has been evaluated using the real traffic data. Speed estimation function only has 1.8 km/h of speed mean error. In addition, there is no accumulated error even for a long time driving. The evaluation of the scene understanding function indicates that the proposed method can provide a high-accuracy detection at 2 FPS, which is faster than the state-of-the-art method.","","","10.1109/JSEN.2018.2874665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8486998","Driving data recorder;smartphone;deep learning;speedometer;scene understanding;LSTM;CNN compression","Accelerometers;Sensors;Estimation;Vehicles;Gyroscopes;Roads;Machine learning","accelerometers;computer vision;driver information systems;feature extraction;image sensors;learning (artificial intelligence);neural nets;object detection;road safety;road traffic;road vehicles;smart phones","accurate speed estimation;intelligent traffic scene understanding;relevant driving data;driver behavior;conventional DDR products;intelligent DDR;motion sensor-based speedometer;vision sensor-based scene understanding;speedometer function;scene understanding function;smartphones;lightweight architecture deep learning network;traffic data;speed estimation function;long short-term memory network;intelligent driving data recorder;LSTM","","1","47","","","","","IEEE","IEEE Journals"
"Selective Ensemble Classification of Image Steganalysis Via Deep Q Network","D. Ni; G. Feng; L. Shen; X. Zhang","School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China","IEEE Signal Processing Letters","","2019","26","7","1065","1069","Currently many important advances in digital media field have been achieved through the ensemble method. In image steganalysis, the application of classifiers has evolved from the early single classifier to the ensemble classifiers. The performance of the ensemble classifier is better than that of a single classifier, but the classifiers may have a certain degree of redundancy. Therefore, it is of great significance to study how to reduce the number of the ensemble classifiers under the premise of ensuring the classification performance. In this letter, we propose a selective ensemble method in image steganalysis based on deep Q network, which combines reinforcement learning with convolutional neural network and are seldom seen in ensemble pruning. This method improves the generalization performance of the model, and reduces the size of ensemble as well. The experimental results show that the proposed method has a certain degree of effect on the ensemble classification optimization of image steganalysis in both spatial and frequency domains.","","","10.1109/LSP.2019.2913018","National Natural Science Foundation of China; Natural Science Foundation of Shanghai; Shanghai Excellent Academic Leader Plan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698276","Steganalysis;ensemble classification;reinforcement learning;Deep Q Network","Feature extraction;Training;Signal processing algorithms;Reinforcement learning;Optimization;Convolutional neural networks","convolutional neural nets;image classification;learning (artificial intelligence);steganography","deep Q network;image steganalysis;ensemble classifier;selective ensemble method;ensemble pruning;selective ensemble classification;digital media;reinforcement learning;convolutional neural network;image steganography","","","17","","","","","IEEE","IEEE Journals"
"Two-Stream Collaborative Learning With Spatial-Temporal Attention for Video Classification","Y. Peng; Y. Zhao; J. Zhang","Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","3","773","786","Video classification is highly important and has widespread applications, such as video search and intelligent surveillance. Video naturally contains both static and motion information, which can be represented by frames and optical flow, respectively. Recently, researchers have generally adopted deep networks to capture the static and motion information separately, which has two main limitations. First, the coexistence relationship between spatial and temporal attention is ignored, although they should be jointly modeled as the spatial and temporal evolutions of video to learn discriminative video features. Second, the strong complementarity between static and motion information is ignored, although they should be collaboratively learned to enhance each other. To address the above two limitations, this paper proposes the two-stream collaborative learning with spatial-temporal attention (TCLSTA) approach, which consists of two models. First, for the spatial-temporal attention model, the spatial-level attention emphasizes the salient regions in a frame, and the temporal-level attention exploits the discriminative frames in a video. They are mutually enhanced to jointly learn the discriminative static and motion features for better classification performance. Second, for the static-motion collaborative model, it not only achieves mutual guidance between static and motion information to enhance the feature learning but also adaptively learns the fusion weights of static and motion streams, thus exploiting the strong complementarity between static and motion information to improve video classification. Experiments on four widely used data sets show that our TCLSTA approach achieves the best performance compared with more than 10 state-of-the-art methods.","","","10.1109/TCSVT.2018.2808685","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8300648","Video classification;static-motion collaborative learning;spatial-temporal attention;adaptively weighted learning","Feature extraction;Adaptation models;Video sequences;Collaboration;Semantics;Collaborative work;Weapons","feature extraction;image classification;image sequences;learning (artificial intelligence);video signal processing","video classification;video search;static motion information;spatial attention;spatial evolutions;temporal evolutions;discriminative video features;spatial-temporal attention model;spatial-level attention;temporal-level attention;static-motion collaborative model;static motion streams;two-stream collaborative learning;intelligent surveillance;optical flow;deep networks;discriminative static motion feature learning;video discriminative frames;adaptively learning;fusion weights","","9","80","","","","","IEEE","IEEE Journals"
"Dictionary Learning for Adaptive GPR Landmine Classification","F. Giovanneschi; K. V. Mishra; M. A. Gonzalez-Huici; Y. C. Eldar; J. H. G. Ender","Fraunhofer Institute for High Frequency Physics and Radar Techniques, Wachtberg, Germany; Department of Electrical and Computer Engineering, The University of Iowa, Iowa City, IA, USA; Fraunhofer Institute for High Frequency Physics and Radar Techniques, Wachtberg, Germany; Weizmann Institute of Science, Rehovot, Israel; Centre for Sensor Systems (ZESS), University of Siegen, Siegen, Germany","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","12","10036","10055","Ground-penetrating radar (GPR) target detection and classification is a challenging task. Here, we consider online dictionary learning (DL) methods to obtain sparse representations (SR) of the GPR data to enhance feature extraction for target classification via support vector machines. Online methods are preferred because traditional batch DL like K-times singular value decomposition (K-SVD) is not scalable to high-dimensional training sets and infeasible for real-time operation. We also develop Drop-Off MINi-batch Online Dictionary Learning (DOMINODL), which exploits the fact that a lot of the training data may be correlated. The DOMINODL algorithm iteratively considers elements of the training set in small batches and drops off samples which become less relevant. For the case of abandoned anti-personnel landmines classification, we compare the performance of K-SVD with three online algorithms: classical online dictionary learning (ODL), its correlation-based variant, and DOMINODL. Our experiments with real data from L-band GPR show that online DL methods reduce learning time by 36%-93% and increase mine detection by 4%-28% over K-SVD. Our DOMINODL is the fastest and retains similar classification performance as the other two online DL approaches. We use a Kolmogorov-Smirnoff test distance and the Dvoretzky-Kiefer-Wolfowitz inequality for the selection of DL input parameters leading to enhanced classification results. To further compare with the state-of-the-art classification approaches, we evaluate a convolutional neural network (CNN) classifier, which performs worse than the proposed approach. Moreover, when the acquired samples are randomly reduced by 25%, 50%, and 75%, sparse decomposition-based classification with DL remains robust while the CNN accuracy is drastically compromised.","","","10.1109/TGRS.2019.2931134","Lady Davis Post-Doctoral Fellowship; Andrew and Erna Finci Viterbi Postdoctoral Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809354","Adaptive radar;deep learning;ground-penetrating radar (GPR);online dictionary learning (ODL);radar target classification;sparse decomposition","Ground penetrating radar;Landmine detection;Support vector machines;Training;Machine learning;Dictionaries","convolutional neural nets;feature extraction;ground penetrating radar;image classification;image enhancement;iterative methods;landmine detection;learning (artificial intelligence);object detection;singular value decomposition;support vector machines","feature extraction;target classification;support vector machines;K-times singular value decomposition;K-SVD;DOMINODL algorithm;correlation-based variant;online DL methods;mine detection;enhanced classification results;sparse decomposition-based classification;anti-personnel landmines classification;drop-off MINi-batch online dictionary learning;ground-penetrating radar;target detection;iterative methods;Kolmogorov-Smirnoff test;Dvoretzky-Kiefer-Wolfowitz inequality;convolutional neural network;GPR landmine classification","","","72","IEEE","","","","IEEE","IEEE Journals"
"Learning to Reconstruct Computed Tomography Images Directly From Sinogram Data Under A Variety of Data Acquisition Conditions","Y. Li; K. Li; C. Zhang; J. Montoya; G. Chen","Department of Medical Physics, University of Wisconsin–Madison, Madison, WI, USA; Department of Medical Physics, University of Wisconsin–Madison, Madison, WI, USA; Department of Medical Physics, University of Wisconsin–Madison, Madison, WI, USA; Department of Medical Physics, University of Wisconsin–Madison, Madison, WI, USA; Department of Medical Physics, University of Wisconsin–Madison, Madison, WI, USA","IEEE Transactions on Medical Imaging","","2019","38","10","2469","2481","Computed tomography (CT) is widely used in medical diagnosis and non-destructive detection. Image reconstruction in CT aims to accurately recover pixel values from measured line integrals, i.e., the summed pixel values along straight lines. Provided that the acquired data satisfy the data sufficiency condition as well as other conditions regarding the view angle sampling interval and the severity of transverse data truncation, researchers have discovered many solutions to accurately reconstruct the image. However, if these conditions are violated, accurate image reconstruction from line integrals remains an intellectual challenge. In this paper, a deep learning method with a common network architecture, termed iCT-Net, was developed and trained to accurately reconstruct images for previously solved and unsolved CT reconstruction problems with high quantitative accuracy. Particularly, accurate reconstructions were achieved for the case when the sparse view reconstruction problem (i.e., compressed sensing problem) is entangled with the classical interior tomographic problems.","","","10.1109/TMI.2019.2910760","NIH; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8688568","Image reconstruction;deep learning;sparse-view;interior tomography","Image reconstruction;Computed tomography;Kernel;Convolution;Training;Deep learning","computerised tomography;data acquisition;image reconstruction;medical image processing","compressed sensing problem;iCT-Net;sparse view reconstruction problem;unsolved CT reconstruction problems;deep learning method;image reconstruction;transverse data truncation;view angle sampling interval;data sufficiency condition;summed pixel values;measured line integrals;nondestructive detection;medical diagnosis;data acquisition conditions;sinogram data;computed tomography images","","1","71","","","","","IEEE","IEEE Journals"
"BE-CALF: Bit-Depth Enhancement by Concatenating All Level Features of DNN","J. Liu; W. Sun; Y. Su; P. Jing; X. Yang","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Image Processing","","2019","28","10","4926","4940","There is a growing demand for monitors to provide high-quality visualization with more bits representing each rendered pixel. However, since most existing images and videos are of low bit-depth (LBD), transforming LBD images to visually pleasant high bit-depth (HBD) versions is of significant value. Most existing bit-depth enhancement methods generate unsatisfactory HBD images with annoying false contour artifacts or blurry details, and some algorithms are also time-consuming. To overcome these drawbacks, we propose a bit-depth enhancement framework via concatenating all level features of deep neural networks (DNNs). A novel deep learning network is proposed based on the deep convolutional variational auto-encoders (VAEs), and skip connections that concatenate every two layers are applied to pass low-level and high-level features to consequent layers, easing the gradient vanishing problem. Meanwhile, the proposed network is optimized to generate the residual between original images and its quantized ones, which performs better than recovering HBD images directly. The experimental results show that the proposed algorithm can eliminate false contour artifacts of the recovered HBD images with low time consumption, and can achieve dramatic restoration performance gains compared with state-of-the-art methods both subjectively and objectively.","","","10.1109/TIP.2019.2912294","National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8713480","Bit-depth enhancement;deep learning;convolutional neural network;high dynamic range imaging;skip connections","Image reconstruction;Feature extraction;Image color analysis;Deep learning;Task analysis;Signal processing algorithms;Neural networks","convolutional neural nets;data visualisation;image enhancement;image resolution;rendering (computer graphics)","recovered HBD images;low time consumption;high-quality visualization;rendered pixel;low bit-depth;LBD images;bit-depth enhancement framework;deep neural networks;deep learning network;deep convolutional variational auto-encoders;high-level features;bit-depth enhancement methods;BE-CAL;gradient vanishing problem","","","40","","","","","IEEE","IEEE Journals"
"Force Sensitive Robotic End-Effector Using Embedded Fiber Optics and Deep Learning Characterization for Dexterous Remote Manipulation","J. I. Kim; D. Kim; M. Krebs; Y. S. Park; Y. Park","Department of Mechanical and Aerospace Engineering; Soft Robotics Research Center; Institute of Advanced Machines and Design, Seoul National University, Seoul, South Korea; Department of Mechanical and Aerospace Engineering; Soft Robotics Research Center; Institute of Advanced Machines and Design, Seoul National University, Seoul, South Korea; Argonne National laboratory, Lemont, IL, USA; Argonne National laboratory, Lemont, IL, USA; Department of Mechanical and Aerospace Engineering; Soft Robotics Research Center; Institute of Advanced Machines and Design, Seoul National University, Seoul, South Korea","IEEE Robotics and Automation Letters","","2019","4","4","3481","3488","Many of the tasks that require a high level of autonomy in complex and dangerous situations are still done by human operators with a high risk of accidents. Although various remotely controlled robot systems have been proposed, the remote operation has limitations in performance and efficiency compared with on-site operations. This letter proposes the design of a new force and tactile sensing mechanism for a robotic end-effector suitable for deployment in harsh environments with integrated force sensing based on fiber optic sensors embedded in a simple and rugged structure. The proposed end-effector was able to detect the magnitude and location of the applied force accurately for high-performance tele-manipulation using hierarchical deep neural network (root mean square errors of 0.43 and 1.11 mm for estimating the contact location in the x-axis and the y-axis, respectively, and 1.16 N for estimating the magnitude of the contact force). Gaussian smoothing was used to support the performance, reducing the error levels by 25%. Also, learning feasibility was performed based on the auto-encoder. Using preliminary bilateral remote control experiments, we demonstrated the feasibility of the telemanipulation with dexterity.","","","10.1109/LRA.2019.2926959","International Nuclear Energy Research Initiative; National Research Foundation; Ministry of Science and ICT of Republic of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8755461","Force and tactile sensing;dexterous manipulation;telerobotics;teleoperation","Force;Robot sensing systems;Strain;End effectors;Mathematical model","dexterous manipulators;end effectors;fibre optic sensors;force sensors;learning (artificial intelligence);neural nets;tactile sensors;telerobotics","force sensitive robotic end-effector;embedded fiber optics;deep learning characterization;dexterous remote manipulation;complex situations;dangerous situations;human operators;remotely controlled robot systems;remote operation;on-site operations;tactile sensing mechanism;integrated force;fiber optic sensors;high-performance tele-manipulation;hierarchical deep neural network;contact location;contact force;error levels;learning feasibility;preliminary bilateral remote control experiments;size 0.43 mm;size 1.11 mm","","","35","Traditional","","","","IEEE","IEEE Journals"
"Deep learning approach for segmentation of plain carbon steel microstructure images","A. Panda; R. Naskar; S. Pal","Department of Computer Science and Engineering, National Institute of Technology, India; Department of Computer Science and Engineering, National Institute of Technology, India; Department of Metallurgical and Material Sciences, National Institute of Technology, India","IET Image Processing","","2019","13","9","1516","1524","To bring about variation in the physical and structural properties or grade of a metal, it is made to undergo specific heat treatment procedures; which can be customized to make the metal microstructure evolve desirably, to obtain specific targeted properties. Recently, computer-based simulations of such heat treatment procedures have become popular, however, such simulations are feasible only if the digital microstructure images are available in suitable forms (optimal digital forms of the microstructure images means the distinct grains identified and the grain boundaries demarcated, i.e., segmentation of microstructure images). To this end, the authors propose a deep learning based Generative Adversarial Network (GAN) architecture for steel microstructure image segmentation. The authors’ experimental results prove the performance efficiency of the proposed GAN model, as compared to the state-of-the-art. However, the proposed network architecture requires large volumes of training data, in the form of annotated ground truth segmentation masks. The current literature lacks sufficient segmented steel microstructure images for this training, to the best of their knowledge. Hence, their second contribution in this study is the development of a Convolutional Neural Network-based framework for sufficient ground truths generation, to aid in the proposed segmentation network training.","","","10.1049/iet-ipr.2019.0404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768525","","","neural nets;carbon steel;construction industry;learning (artificial intelligence);metallurgy;steel;image segmentation","deep learning approach;plain carbon steel microstructure images;grade/quality customised;construction industry;transportation;quality/grade;specific heat treatment procedures;specific desired properties;computer-based simulations;metallurgy industry;manual experimentation errors;metal heat treatment processes;digital microstructure images;suitable forms;optimal digital forms;simulation models;raw metal microstructure image;Generative Adversarial Network architecture;steel microstructure image segmentation;authors;GAN model;conventional deep learning models;annotated ground truth segmentation masks;sufficient segmented steel microstructure images;sufficient ground truths generation;segmentation network training;related metal microstructure image processing researches;experiments","","","","","","","","IET","IET Journals"
"Deep Learning for In Situ and Real-Time Quality Monitoring in Additive Manufacturing Using Acoustic Emission","S. A. Shevchik; G. Masinelli; C. Kenel; C. Leinenbach; K. Wasmer","Laboratory for Advanced Materials Processing, Empa—Swiss Federal Laboratories for Materials Science and Technology, Dübendorf, Switzerland; Laboratory for Advanced Materials Processing, Empa—Swiss Federal Laboratories for Materials Science and Technology, Dübendorf, Switzerland; Laboratory for Advanced Materials Processing, Empa—Swiss Federal Laboratories for Materials Science and Technology, Dübendorf, Switzerland; Laboratory for Advanced Materials Processing, Empa—Swiss Federal Laboratories for Materials Science and Technology, Dübendorf, Switzerland; Laboratory for Advanced Materials Processing, Empa—Swiss Federal Laboratories for Materials Science and Technology, Dübendorf, Switzerland","IEEE Transactions on Industrial Informatics","","2019","15","9","5194","5203","Additive manufacturing (AM) is considered as a revolution in manufacturing. However, the high expectations face technical difficulties that prevent further penetration into wider industries. The main reason is the lack of process reproducibility and the absence of a reliable and cost-effective process monitoring. This paper is a supplement to existing studies in this field and proposes a unique combination of highly sensitive acoustic sensor and machine learning for process monitoring. The acoustic signals from a real powder-bed fusion AM process were collected using a fiber Bragg grating. The process parameters are intentionally tuned to achieve three levels of quality categories, which are related to the porosity contents inside the workpiece. The quality categories are defined as high, medium, and poor quality and their corresponding porosity contents are 0.07%, 0.30%, and 1.42%, respectively. Wavelet spectrograms of the signals and their encoded label representations, obtained from spectral clustering, are taken as features. A deep convolutional neural network is used to classify the features from each category and the classification accuracy ranges between 78% and 91%. Hence, the proposed method has significant industrial potentials for in situ and real-time quality monitoring of AM processes since it requires minimum modifications of commercially available industrial machines.","","","10.1109/TII.2019.2910524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8688503","Acoustic emission (AE);additive manufacturing (AM);fiber Bragg grating (FBG);fiber optical sensors;  $M$  -band wavelets;powder-bed fusion AM;process monitoring;spectral convolutional neural networks (SCNNs)","Monitoring;Spectrogram;Real-time systems;Acoustics;Temperature measurement;Time-frequency analysis;Bragg gratings","acoustic emission;acoustic signal processing;Bragg gratings;convolutional neural nets;feature extraction;learning (artificial intelligence);porosity;process monitoring;production engineering computing;quality management;rapid prototyping (industrial);real-time systems;signal classification;signal representation;three-dimensional printing","deep learning;real-time quality monitoring;additive manufacturing;acoustic emission;machine learning;acoustic signals;powder-bed fusion;fiber Bragg grating;deep convolutional neural network;in situ quality monitoring;process monitoring;acoustic sensor;porosity content;wavelet spectrograms;spectral clustering;feature classification","","","35","Traditional","","","","IEEE","IEEE Journals"
"Cloud Deployment of High-Resolution Medical Image Analysis With TOMAAT","F. Milletari; J. Frei; M. Aboulatta; G. Vivar; S. Ahmadi","Nvidia Inc., Santa Clara, USA; German Center for Vertigo and Balance Disorders (DSGZ), Ludwig-Maximilians University, Munich, Germany; German Center for Vertigo and Balance Disorders (DSGZ), Ludwig-Maximilians University, Munich, Germany; German Center for Vertigo and Balance Disorders (DSGZ), Ludwig-Maximilians University, Munich, Germany; German Center for Vertigo and Balance Disorders (DSGZ), Ludwig-Maximilians University, Munich, Germany","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","969","977","Background: Deep learning has been recently applied to a multitude of computer vision and medical image analysis problems. Although recent research efforts have improved the state of the art, most of the methods cannot be easily accessed, compared or used by other researchers or clinicians. Even if developers publish their code and pre-trained models on the internet, integration in stand-alone applications and existing workflows is often not straightforward, especially for clinical research partners. In this paper, we propose an open-source framework to provide AI-enabled medical image analysis through the network. Methods: TOMAAT provides a cloud environment for general medical image analysis, composed of three basic components: (i) an announcement service, maintaining a public registry of (ii) multiple distributed server nodes offering various medical image analysis solutions, and (iii) client software offering simple interfaces for users. Deployment is realized through HTTP-based communication, along with an API and wrappers for common image manipulations during preand post-processing. Results: We demonstrate the utility and versatility of TOMAAT on several hallmark medical image analysis tasks: segmentation, diffeomorphic deformable atlas registration, landmark localization, and workflow integration. Through TOMAAT, the high hardware demands, setup and model complexity of demonstrated approaches are transparent to users, who are provided with simple client interfaces. We present example clients in three-dimensional Slicer, in the web browser, on iOS devices and in a commercially available, certified medical image analysis suite. Conclusion: TOMAAT enables deployment of state-of-the-art image segmentation in the cloud, fostering interaction among deep learning researchers and medical collaborators in the clinic. Currently, a public announcement service is hosted by the authors, and several ready-to-use services are registered and enlisted at http://tomaat.cloud.","","","10.1109/JBHI.2018.2885214","German Federal Ministry of Education and Health; German Center for Vertigo and Balance Disorders; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8561278","Deep learning;medical image analysis;segmentation;registration;cloud deployment;clinical translation","Servers;Image analysis;Deep learning;Prediction algorithms;Medical diagnostic imaging;Image segmentation","application program interfaces;client-server systems;cloud computing;computer vision;hypermedia;image segmentation;Internet;iOS (operating system);learning (artificial intelligence);medical image processing;online front-ends;public domain software;transport protocols","cloud deployment;high-resolution medical image analysis;clinical research partners;AI-enabled medical image analysis;image segmentation;deep learning;TOMAAT;computer vision;Internet;open-source framework;public registry;distributed server;client software;HTTP;API;client interfaces;three-dimensional slicer;web browser;iOS devices;public announcement service","","","27","","","","","IEEE","IEEE Journals"
"Behavioral Modeling and Linearization of Wideband RF Power Amplifiers Using BiLSTM Networks for 5G Wireless Systems","J. Sun; W. Shi; Z. Yang; J. Yang; G. Gui","Key Laboratory of Broadband Wireless Communication and Sensor Network Technology, Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China; School of New Energy and Electronic Engineering, Yancheng Teachers University, Yancheng, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Key Laboratory of Broadband Wireless Communication and Sensor Network Technology, Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China; Key Laboratory of Broadband Wireless Communication and Sensor Network Technology, Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Transactions on Vehicular Technology","","2019","68","11","10348","10356","Characterization and linearization of RF power amplifiers (PAs) are key issues of fifth-generation wireless communication systems, especially when high peak-to-average ratio waveforms are introduced. Recently, deep learning methods have achieved great success in numerous domains including wireless physical-layer. However, there has been limited work in using deep learning for PAs behavioral modeling and linearization. In this paper, we make a bridge between memory effects of the nonlinear PAs and memory of bidirectional long short-term memory (BiLSTM) neural networks. We then build a BiLSTM-based behavioral modeling architecture and its accompanying digital predistortion (DPD) model by reconciling a non causality concern. Next, an additional model is proposed in this paper to mitigate uncertainty of the tested PA when transforming phases. The experimental results demonstrate the effectiveness of the proposed scheme, in which the adequately trained networks are capable of characterizing the PA, and the artificial intelligence-based DPD shows promising linearization performance when considering the tested PAs inherent unpredictability.","","","10.1109/TVT.2019.2925562","Priority Academic Program Development of Jiangsu Higher Education Institutions; National Natural Science Foundation of China; Jiangsu Specially Appointed Professor Program; Summit of the Six Top Talents Program of Jiangsu; Program for High-Level Entrepreneurial and Innovative Talents Introduction; Nanjing University of Posts and Telecommunications; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8750814","Behavioral modeling;digital predistortion;deep learning;BiLSTM neural networks;phase ambiguity","Mathematical model;Radio frequency;Wideband;5G mobile communication;Wireless communication;Deep learning;Computational modeling","learning (artificial intelligence);neural nets;power amplifiers;radiofrequency amplifiers","artificial intelligence-based DPD;linearization performance;wideband RF power amplifiers;BiLSTM networks;5G wireless systems;fifth-generation wireless communication systems;high peak-to-average ratio waveforms;nonlinear PA;short-term memory neural networks;BiLSTM-based behavioral modeling architecture;digital predistortion model;noncausality concern;adequately trained networks;tested PA inherent unpredictability;memory effects;PA behavioral modeling;wireless physical-layer;deep learning methods","","18","57","IEEE","","","","IEEE","IEEE Journals"
"Video Summarization by Learning Deep Side Semantic Embedding","Y. Yuan; T. Mei; P. Cui; W. Zhu","Tsinghua-Berkeley Shenzhen Institute, Tsinghua University, Shenzhen, China; Microsoft Research, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua-Berkeley Shenzhen Institute, Tsinghua National Laboratory for Information Science and Technology, Beijing Key Laboratory of Networked Multimedia, Tsinghua University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","1","226","237","With the rapid growth of video content, video summarization, which focuses on automatically selecting important and informative parts from videos, is becoming increasingly crucial. However, the problem is challenging due to its subjectiveness. Previous research, which predominantly relies on manually designed criteria or resourcefully expensive human annotations, often fails to achieve satisfying results. We observe that the side information associated with a video (e.g., surrounding text such as titles, queries, descriptions, comments, and so on) represents a kind of human-curated semantics of video content. This side information, although valuable for video summarization, is overlooked in existing approaches. In this paper, we present a novel deep side semantic embedding (DSSE) model to generate video summaries by leveraging the freely available side information. The DSSE constructs a latent subspace by correlating the hidden layers of the two uni-modal autoencoders, which embed the video frames and side information, respectively. Specifically, by interactively minimizing the semantic relevance loss and the feature reconstruction loss of the two uni-modal autoencoders, the comparable common information between video frames and side information can be more completely learned. Therefore, their semantic relevance can be more effectively measured. Finally, semantically meaningful segments are selected from videos by minimizing their distances to the side information in the constructed latent subspace. We conduct experiments on two datasets (Thumb1K and TVSum50) and demonstrate the superior performance of DSSE to the several state-of-the-art approaches to video summarization.","","","10.1109/TCSVT.2017.2771247","National Natural Science Foundation of China; National Program on Key Basic Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8101557","Deep learning;embedding;side semantics;video summarization","Semantics;Visualization;Loss measurement;Extraterrestrial measurements;Image reconstruction;Data models;Decoding","learning (artificial intelligence);video coding;video signal processing","feature reconstruction loss;human-curated semantics;semantic relevance loss;video frames;uni-modal autoencoders;video summaries;deep side semantic embedding model;video summarization","","2","53","","","","","IEEE","IEEE Journals"
"Detection of Basic Human Physical Activities With Indoor–Outdoor Information Using Sigma-Based Features and Deep Learning","G. Memiş; M. Sert","Computer Engineering Department, Başkent University, Ankara, Turkey; Computer Engineering Department, Başkent University, Ankara, Turkey","IEEE Sensors Journal","","2019","19","17","7565","7574","The devices created on account of the developments in wearable technology are increasingly becoming a part of our daily lives. In particular, sensors have enhanced the usefulness of such devices. The aim of this paper is to detect human physical activity along with indoor/outdoor information by using mobile phones and a separate oxygen saturation sensor. There is no relevant dataset in the literature for this type of detection. For this purpose, data from four different types of human physical activity was collected through mobile phone and oxygen saturation sensors; 12 people aged between 20-65 years participated in the study. During the data collection process, different physical activities under different environmental conditions were performed by the subjects in 10 min. As a next step, a novel deep neural network (DNN) model specifically designed for physical activity recognition was proposed. In order to improve accuracy and reduce the computational complexity, standard deviation (sigma)-based features were introduced. To evaluate its efficacy, we conducted comparisons with selected machine learning algorithms on our proposed dataset. The results on our dataset indicate that the multimodal sigma-based features give the best classification accuracy of 81.60% using our proposed DNN method. Furthermore, the accuracy of the classification made with our proposed DNN method without sigma-based features was 79.04%.","","","10.1109/JSEN.2019.2916393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8713510","Wearable sensors;deep learning;physical activity database;physical activity recognition","Smart phones;Temperature sensors;Feature extraction;Biomedical monitoring;Monitoring","biology computing;feature extraction;learning (artificial intelligence);mobile computing;neural nets;pattern classification;sensor fusion;wearable computers","indoor-outdoor information;wearable technology;mobile phone;oxygen saturation sensors;deep neural network model;physical activity recognition;standard deviation-based features;multimodal sigma-based features;oxygen saturation sensor;data collection;human physical activity detection","","","52","","","","","IEEE","IEEE Journals"
"Deep Learning Models Unveiled Functional Difference Between Cortical Gyri and Sulci","S. Zhang; H. Liu; H. Huang; Y. Zhao; X. Jiang; B. Bowers; L. Guo; X. Hu; M. Sanchez; T. Liu","University of Georgia; Northwestern Polytechnical University; Northwestern Polytechnical University; University of Georgia; University of Electronic Science and Technology of China; University of Georgia; Northwestern Polytechnical University; University of California, Riverside; Emory University; University of Georgia, Athens, GA, USA","IEEE Transactions on Biomedical Engineering","","2019","66","5","1297","1308","It is largely unknown whether there is functional role difference between cortical gyral and sulcal regions. Recent advancements in neuroimaging studies demonstrate clear difference of structural connection profiles in gyral and sulcal areas, suggesting possible functional role difference in these convex and concave cortical regions. To explore and confirm such possible functional difference, we design and apply a powerful deep learning model of convolutional neural networks (CNN) that has been proven to be superior in learning discriminative and meaningful patterns on fMRI. By using the CNN model, gyral and sulcal fMRI signals are learned and predicted, and the prediction performance is adopted to demonstrate the functional difference between gyri and sulci. By using the Human Connectome Project (HCP) fMRI data and macaque brain fMRI data, an average of 83% and 90% classification accuracy has been achieved to separate gyral/sulcal HCP task fMRI signals at the population and individual subject level, respectively; 81% and 86% classification accuracy for resting state fMRI signals at the group and individual subject level, respectively. In addition, 78% classification accuracy has been achieved to separate gyral/sulcal resting state fMRI signals in macaque brains. Importantly, further analysis reveals that the discriminative features that are learned by CNNs to differentiate gyral/sulcal fMRI signals can be meaningfully interpreted, thus unveiling the fundamental functional difference between cortical gyri and sulci. That is, gyri are more global functional integration centers with simpler lower frequency signal components, while sulci are more local processing units with more complex higher frequency signal components.","","","10.1109/TBME.2018.2872726","National Institutes of Health; National Science Foundation; NSFC; Special Fund for Basic Scientific Research of Central Colleges; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8476182","Cortical gyri;cortical sulci;classification;brain function;convolutional neural network","Functional magnetic resonance imaging;Task analysis;Diffusion tensor imaging;Machine learning;Brain modeling","biomedical MRI;brain;convolutional neural nets;image classification;learning (artificial intelligence);medical image processing;neurophysiology","cortical gyral regions;neuroimaging studies;structural connection profiles;gyral areas;sulcal areas;concave cortical regions;convolutional neural networks;discriminative patterns;meaningful patterns;CNN model;gyral signals;prediction performance;macaque brain fMRI data;individual subject level;fundamental functional difference;global functional integration centers;simpler lower frequency signal components;complex higher frequency signal components;deep learning model;classification accuracy;resting state fMRI signals;cortical sulcal regions;functional role difference;convex cortical regions;gyral-sulcal HCP task fMRI signals;Human Connectome Project fMRI data","","2","57","","","","","IEEE","IEEE Journals"
"MoDL: Model-Based Deep Learning Architecture for Inverse Problems","H. K. Aggarwal; M. P. Mani; M. Jacob","Department of Electrical and Computer Engineering, The University of Iowa, Iowa City, IA, USA; Department of Radiology, The University of Iowa, Iowa City, IA, USA; Department of Electrical and Computer Engineering, The University of Iowa, Iowa City, IA, USA","IEEE Transactions on Medical Imaging","","2019","38","2","394","405","We introduce a model-based image reconstruction framework with a convolution neural network (CNN)-based regularization prior. The proposed formulation provides a systematic approach for deriving deep architectures for inverse problems with the arbitrary structure. Since the forward model is explicitly accounted for, a smaller network with fewer parameters is sufficient to capture the image information compared to direct inversion approaches. Thus, reducing the demand for training data and training time. Since we rely on end-to-end training with weight sharing across iterations, the CNN weights are customized to the forward model, thus offering improved performance over approaches that rely on pre-trained denoisers. Our experiments show that the decoupling of the number of iterations from the network complexity offered by this approach provides benefits, including lower demand for training data, reduced risk of overfitting, and implementations with significantly reduced memory footprint. We propose to enforce data-consistency by using numerical optimization blocks, such as conjugate gradients algorithm within the network. This approach offers faster convergence per iteration, compared to methods that rely on proximal gradients steps to enforce data consistency. Our experiments show that the faster convergence translates to improved performance, primarily when the available GPU memory restricts the number of iterations.","","","10.1109/TMI.2018.2865356","National Institute of Biomedical Imaging and Bioengineering; ONR-N000141310202; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8434321","Deep learning;parallel imaging;convolutional neural network","Image reconstruction;Training data;Training;Optimization;Imaging;Numerical models;Machine learning","convolutional neural nets;image denoising;image reconstruction;inverse problems;learning (artificial intelligence);neural net architecture;optimisation","MoDL;inverse problems;convolution neural network-based regularization;image information;end-to-end training;data consistency;model-based image reconstruction;pretrained denoisers;numerical optimization","","3","45","","","","","IEEE","IEEE Journals"
"RAPIDO: a rejuvenating adaptive PID-type optimiser for deep neural networks","S. Kim; D. J. Park; D. E. Chang","KAIST, Republic of Korea; KAIST, Republic of Korea; KAIST, Republic of Korea","Electronics Letters","","2019","55","16","899","901","The authors present a novel gradient descent algorithm called RAPIDO for deep learning. It adapts over time and performs optimisation using current, past and future information similar to the PID controller. The proposed method is suited for optimising deep neural networks that consist of activation functions such as sigmoid, hyperbolic tangent and ReLU functions because it can adapt appropriately to sudden changes in gradients. They experimentally study the authors' method and show the performance results by comparing with other methods on the quadratic objective function and the MNIST classification task. The proposed method shows better performance than the other methods.","","","10.1049/el.2019.1593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789805","","","gradient methods;learning (artificial intelligence);neural nets;optimisation;pattern classification;three-term control","MNIST classification task;gradient descent algorithm;quadratic objective function;ReLU functions;hyperbolic tangent functions;sigmoid functions;activation functions;PID controller;deep learning;deep neural networks;rejuvenating adaptive PID-type optimiser;RAPIDO","","","6","","","","","IET","IET Journals"
"LPCCNet: A Lightweight Network for Point Cloud Classification","M. Li; Y. Hu; N. Zhao; L. Guo","State Key Laboratory of Pulsed Power Laser Technology, College of Electronic Engineering, National University of Defence Technology, Hefei, China; State Key Laboratory of Pulsed Power Laser Technology, College of Electronic Engineering, National University of Defence Technology, Hefei, China; State Key Laboratory of Pulsed Power Laser Technology, College of Electronic Engineering, National University of Defence Technology, Hefei, China; State Key Laboratory of Pulsed Power Laser Technology, College of Electronic Engineering, National University of Defence Technology, Hefei, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","6","962","966","Deep learning has achieved much in image and natural language processing, and related research has also expanded into the field of point cloud processing, and deep networks dedicated to point clouds have emerged; however, in many points cloud applications, networks with fewer parameters and lower computational burdens are required to satisfy the requirements of miniaturisation of devices, so research into lightweight point cloud deep networks is justified. In this letter, we described the design of a lightweight point cloud classification deep learning architecture, lightweight point cloud classification network, with the self-designed block structure as a unit, using a novel index fully dense connectivity method that interconnects all convolutional layers of the network, improving the utilisation of features, thereby reducing the parameter size of each layer; at the same time, the network applies grouping convolution and pruning to the convolutional layer and the fully connected layer, further reducing the amount of parameters and calculation burden. The experimental results show the effectiveness of the network that can achieve comparable results with existing large networks, such as PointNet++, with a smaller amount of parameters. The proposed network offers great promise in mobile device deployment and real-time processing.","","","10.1109/LGRS.2018.2889472","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8607072","Classification;deep learning;group convolution (GC);network compression;point cloud","Three-dimensional displays;Feature extraction;Task analysis;Convolution;Deep learning;Indexes;Atmospheric modeling","computer graphics;convolutional neural nets;pattern classification","lightweight network;natural language processing;point cloud processing;points cloud applications;lightweight point cloud deep networks;deep learning architecture;lightweight point cloud classification network;convolutional layer;mobile device deployment;real-time processing;LPCCNet;index fully dense connectivity method","","","22","","","","","IEEE","IEEE Journals"
"Multitask Policy Adversarial Learning for Human-Level Control With Large State Spaces","J. P. Wang; Y. K. Shi; W. S. Zhang; I. Thomas; S. H. Duan","Laboratory of Precision Sensing and Control Center, Institute of Automation, Chinese Academy of Science, Beijing, China; Communications Standards Research Institute, China Academy of Telecommunication Research, MIIT, Beijing, China; Laboratory of Precision Sensing and Control Center, Institute of Automation, Chinese Academy of Science, Beijing, China; Fujitsu RunMyProcess, Asnieres-sur-Seine, France; Communications Standards Research Institute, China Academy of Telecommunication Research, MIIT, Beijing, China","IEEE Transactions on Industrial Informatics","","2019","15","4","2395","2404","The sequential decision-making problem with large-scale state spaces is an important and challenging topic for multitask reinforcement learning (MTRL). Training near-optimality policies across tasks suffers from prior knowledge deficiency in discrete-time nonlinear environment, especially for continuous task variations, requiring scalability approaches to transfer prior knowledge among new tasks when considering large number of tasks. This paper proposes a multitask policy adversarial learning (MTPAL) method for learning a nonlinear feedback policy that generalizes across multiple tasks, making cognizance ability of robot much closer to human-level decision making. The key idea is to construct a parametrized policy model directly from large high-dimensional observations by deep function approximators, and then train optimal of sequential decision policy for each new task by an adversarial process, in which simultaneously two models are trained: a multitask policy generator transforms samples drawn from a prior distribution into samples from a complex data distribution with higher dimensionality, and a multitask policy discriminator decides whether the given sample is prior distribution from human-level empirically derived or from the generator. All the related human-level empirically derived are integrated into the sequential decision policy, transferring human-level policy at every layer in a deep policy network. Extensive experimental testing result of four different WeiChai Power manufacturing data sets shows that our approach can surpass human performance simultaneously from cart-pole to production assembly control.","","","10.1109/TII.2018.2881266","National Key R&D Program of China; National Natural Science Foundation of China; Beijing Municipal Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8534402","Deep multitask reinforcement learning;flexible manufacturing;industrial big data;sequential decision making (SDM)","Task analysis;Aerospace electronics;Data models;Computational modeling;Decision making;Training","assembling;data handling;decision making;feedback;function approximation;learning (artificial intelligence);manufacturing data processing","knowledge deficiency;WeiChai Power manufacturing data sets;cart-pole;production assembly control;multitask policy discriminator;multitask policy generator;sequential decision policy;high-dimensional observations;parametrized policy model;human-level decision making;nonlinear feedback policy;multitask policy adversarial;continuous task variations;discrete-time nonlinear environment;training near-optimality policies;multitask reinforcement learning;large-scale state spaces;sequential decision-making problem;human-level control;policy adversarial learning;human performance;deep policy network;human-level policy","","","27","","","","","IEEE","IEEE Journals"
"Pose-Invariant Embedding for Deep Person Re-Identification","L. Zheng; Y. Huang; H. Lu; Y. Yang","Research School of Computer Science, The Australian National University, Canberra, ACT, Australia; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; Center for Artificial Intelligence, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Image Processing","","2019","28","9","4500","4509","Pedestrian misalignment, which mainly arises from detector errors and pose variations, is a critical problem for a robust person re-identification (re-ID) system. With poor alignment, the feature learning and matching process might be largely compromised. To address this problem, this paper introduces pose-invariant embedding (PIE) as a pedestrian descriptor. First, in order to align pedestrians to a standard pose, the PoseBox structure is introduced, which is generated through pose estimation followed by affine transformations. Second, to reduce the impact of pose estimation errors and information loss during the PoseBox construction, we design a PoseBox fusion (PBF) CNN architecture that takes the original image, the PoseBox, and the pose estimation confidence as input. The proposed PIE descriptor is thus defined as the fully connected layer of the PBF network for the retrieval task. Experiments are conducted on the Market-1501, CUHK03-NP, and DukeMTMC-reID datasets. We show that PoseBox alone yields decent re-ID accuracy and that when integrated in the PBF network, the learned PIE descriptor produces competitive performance compared with state-of-the-art approaches.","","","10.1109/TIP.2019.2910414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693885","Pose invariant embedding;PoseBox;PoseBox fusion network;person re-identification","Pose estimation;Head;Shoulder;Hip;Legged locomotion;Standards;Deep learning","affine transforms;convolutional neural nets;face recognition;feature extraction;image matching;image representation;learning (artificial intelligence);pose estimation","PoseBox fusion CNN architecture;estimation confidence;PBF network;learned PIE descriptor;deep person re-identification;feature learning;matching process;affine transformation;retrieval task;Market-1501 dataset;CUHK03-NP dataset;DukeMTMC-reID dataset;PoseBox construction;information loss;pose estimation;PoseBox structure;pedestrian descriptor;robust person re-identification;pedestrian misalignment;pose-invariant embedding","","8","62","","","","","IEEE","IEEE Journals"
"Seismic Signal Denoising and Decomposition Using Deep Neural Networks","W. Zhu; S. M. Mousavi; G. C. Beroza","Department of Geophysics, Stanford University, Stanford, CA, USA; Department of Geophysics, Stanford University, Stanford, CA, USA; Department of Geophysics, Stanford University, Stanford, CA, USA","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","9476","9488","Frequency filtering is widely used in routine processing of seismic data to improve the signal-to-noise ratio (SNR) of recorded signals and by doing so to improve subsequent analyses. In this paper, we develop a new denoising/decomposition method, DeepDenoiser, based on a deep neural network. This network is able to simultaneously learn a sparse representation of data in the time-frequency domain and a non-linear function that maps this representation into masks that decompose input data into a signal of interest and noise (defined as any non-seismic signal). We show that DeepDenoiser achieves impressive denoising of seismic signals even when the signal and noise share a common frequency band. Because the noise statistics are automatically learned from data and require no assumptions, our method properly handles white noise, a variety of colored noise, and non-earthquake signals. DeepDenoiser can significantly improve the SNR with minimal changes in the waveform shape of interest, even in the presence of high noise levels. We demonstrate the effect of our method on improving earthquake detection. There are clear applications of DeepDenoiser to seismic imaging, micro-seismic monitoring, and preprocessing of ambient noise data. We also note that the potential applications of our approach are not limited to these applications or even to earthquake data and that our approach can be adapted to diverse signals and applications in other settings.","","","10.1109/TGRS.2019.2926772","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802278","Convolutional neural networks;decomposition;deep learning;seismic denoising","Noise reduction;Neural networks;Noise measurement;Transforms;Time-domain analysis;Earthquakes;Deep learning","earthquakes;filtering theory;geophysical signal processing;geophysical techniques;neural nets;nonlinear functions;seismology;signal denoising;signal detection;signal representation;time-frequency analysis","seismic signal denoising;deep neural network;seismic data;signal-to-noise ratio;DeepDenoiser;sparse representation;time-frequency domain;nonseismic signal;seismic signals;nonearthquake signals;seismic imaging;microseismic monitoring;ambient noise data;earthquake data","","1","64","","","","","IEEE","IEEE Journals"
"Direct Segmentation-Based Full Quantification for Left Ventricle via Deep Multi-Task Regression Learning Network","X. Du; R. Tang; S. Yin; Y. Zhang; S. Li","School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; Department of Medical Imaging, Western University, London, ON, Canada","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","942","948","Quantitative analysis of the heart is extremely necessary and significant for detecting and diagnosing heart disease, yet there are still some challenges. In this study, we propose a new end-to-end segmentation-based deep multi-task regression learning model (Indices-JSQ) to make a holonomic quantitative analysis of the left ventricle (LV), which contains a segmentation network (Img2Contour) and multi-task regression network (Contour2Indices). First, Img2Contour, which contains a deep convolutional encoder-decoder module, is designed to obtain the LV contour. Then, the predicted contour is fed as input to Contour2Indices for full quantification. On the whole, we take into account the relationship between different tasks, which can serve as a complementary advantage. Meanwhile, instead of using images directly from the original dataset, we creatively use the segmented contour of the original image to estimate the cardiac indices to achieve better and more accurate results. We make experiments on MR sequences of 145 subjects and gain the experimental results of 157 mm2, 2.43 mm, 1.29 mm, and 0.87 on areas, dimensions, regional wall thicknesses, and Dice Metric, respectively. It intuitively shows that the proposed method outperforms the other state-of-the-art methods and demonstrates that our method has a great potential in cardiac MR images segmentation, comprehensive clinical assessment, and diagnosis.","","","10.1109/JBHI.2018.2879188","National Science Foundation of China; Anhui Provincial Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519630","Left ventricle;full quantification;multi-task regression learning","Image segmentation;Estimation;Feature extraction;Decoding;Task analysis;Heart;Convolution","biomedical MRI;cardiology;diseases;image segmentation;image sequences;learning (artificial intelligence);medical image processing;regression analysis","heart disease;end-to-end segmentation-based deep multitask regression;Indices-JSQ;holonomic quantitative analysis;left ventricle;segmentation network;Img2Contour;multitask regression network;Contour2Indices;deep convolutional encoder-decoder module;LV contour;segmented contour;cardiac indices;direct segmentation;MR sequences;cardiac MR images segmentation","","","23","","","","","IEEE","IEEE Journals"
"Approximating the Ideal Observer and Hotelling Observer for Binary Signal Detection Tasks by Use of Supervised Learning Methods","W. Zhou; H. Li; M. A. Anastasio","Department of Electrical and Systems Engineering, Washington University in St. Louis, St. Louis, MO, USA; Department of Bioengineering, University of Illinois at Urbana–Champaign, Urbana, IL, USA; Department of Bioengineering, University of Illinois at Urbana–Champaign, Urbana, IL, USA","IEEE Transactions on Medical Imaging","","2019","38","10","2456","2468","It is widely accepted that the optimization of medical imaging system performance should be guided by task-based measures of image quality (IQ). Task-based measures of IQ quantify the ability of an observer to perform a specific task, such as detection or estimation of a signal (e.g., a tumor). For binary signal detection tasks, the Bayesian Ideal Observer (IO) sets an upper limit of observer performance and has been advocated for use in optimizing medical imaging systems and data-acquisition designs. Except in special cases, the determination of the IO test statistic is analytically intractable. Markov-chain Monte Carlo (MCMC) techniques can be employed to approximate the IO detection performance, but their reported applications have been limited to relatively simple object models. In cases where the IO test statistic is difficult to compute, the Hotelling Observer (HO) can be employed. To compute the HO test statistic, potentially large covariance matrices must be accurately estimated and subsequently inverted, which can present computational challenges. This paper investigates the supervised learning-based methodologies for approximating the IO and HO test statistics. Convolutional neural networks (CNNs) and single-layer neural networks (SLNNs) are employed to approximate the IO and HO test statistics, respectively. The numerical simulations were conducted for both signal-known-exactly (SKE) and signal-known-statistically (SKS) signal detection tasks. The considered background models include the lumpy object model and the clustered lumpy object model. The measurement noise models considered are Gaussian, Laplacian, and mixed Poisson-Gaussian. The performances of the supervised learning methods are assessed via receiver operating characteristic (ROC) analysis, and the results are compared to those produced by the use of traditional numerical methods or analytical calculations when feasible. The potential advantages of the proposed supervised learning approaches for approximating the IO and HO test statistics are discussed.","","","10.1109/TMI.2019.2911211","National Institutes of Health; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691467","Imaging system optimization;numerical observers;Bayesian Ideal Observer;Hotelling Observer;task-based image quality;supervised learning;deep learning","Task analysis;Observers;Signal detection;Covariance matrices;Imaging;Supervised learning;Numerical models","Bayes methods;convolutional neural nets;covariance matrices;Gaussian noise;learning (artificial intelligence);Markov processes;Monte Carlo methods;signal detection","Hotelling Observer;binary signal detection tasks;supervised learning methods;image quality;Bayesian Ideal Observer;medical imaging systems;IO test statistic;Markov-chain Monte Carlo techniques;HO test statistic;single-layer neural networks;signal-known-statistically signal detection;clustered lumpy object model;measurement noise models;data-acquisition;IO detection performance;covariance matrices;convolutional neural networks;IO test statistics;signal-known-exactly signal detection;Gaussian measurement noise model;Laplacian measurement noise model;mixed Poisson-Gaussian measurement noise model;receiver operating characteristic analysis;ROC analysis","","","58","","","","","IEEE","IEEE Journals"
"A Parallel Gaussian–Bernoulli Restricted Boltzmann Machine for Mining Area Classification With Hyperspectral Imagery","K. Tan; F. Wu; Q. Du; P. Du; Y. Chen","Key Laboratory for Land Environment and Disaster Monitoring of NASG, China University of Mining and Technology, Xuzhou, China; Key Laboratory for Land Environment and Disaster Monitoring of NASG, China University of Mining and Technology, Xuzhou, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; Key Laboratory for Satellite Mapping Technology and Applications of NASG, Nanjing University, Nanjing, China; Key Laboratory for Land Environment and Disaster Monitoring of NASG, China University of Mining and Technology, Xuzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","2","627","636","In this paper, a novel feature extraction method is proposed for hyperspectral image classification using a Gaussian-Bernoulli restricted Boltzmann machine (GBRBM) in parallel. The proposed approach employs several GBRBMs with different hidden layers to extract deep features from hyperspectral images, which are nonlinear and local invariant. Based on the learned deep features, a logistic regression layer is trained for classification. The proposed approaches are carried out on two public hyperspectral datasets: Pavia University dataset and Salinas dataset, and a new dataset obtained by HySpex imaging spectrometer in the mining area in Xuzhou. The obtained results reveal that the proposed approach offers superior performance compared to traditional classifiers. The advantage of the proposed GBRBM is that it can extract deep features in an unsupervised way and reduce the prediction time by using GPU. In particular, the classification results of the mining area provide valuable suggestions to improve environmental protection.","","","10.1109/JSTARS.2019.2892975","National Natural Science Foundation of China; Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636983","Deep learning;Gaussian–Bernoulli restricted Boltzmann machine (GBRBM);hyperspectral image classification","Feature extraction;Hyperspectral imaging;Deep learning;Training;Computational modeling","Boltzmann machines;feature extraction;Gaussian processes;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);regression analysis;remote sensing","parallel Gaussian-Bernoulli restricted Boltzmann machine;mining area classification;hyperspectral imagery;hyperspectral image classification;GBRBM;hyperspectral images;learned deep features;logistic regression layer;public hyperspectral datasets;Pavia University dataset;Salinas dataset;HySpex imaging spectrometer;classification results;feature extraction method;hidden layers;GPU","","","34","","","","","IEEE","IEEE Journals"
"NeuroTrajectory: A Neuroevolutionary Approach to Local State Trajectory Learning for Autonomous Vehicles","S. M. Grigorescu; B. Trasnea; L. Marina; A. Vasilcoi; T. Cocias","Elektrobit Automotive and the Robotics, Vision and Control Lab (ROVIS) (www.rovislab.com), Transilvania University of Braşov, Braşov, Romania; Elektrobit Automotive and the Robotics, Vision and Control Lab (ROVIS) (www.rovislab.com), Transilvania University of Braşov, Braşov, Romania; Elektrobit Automotive and the Robotics, Vision and Control Lab (ROVIS) (www.rovislab.com), Transilvania University of Braşov, Braşov, Romania; Elektrobit Automotive and the Robotics, Vision and Control Lab (ROVIS) (www.rovislab.com), Transilvania University of Braşov, Braşov, Romania; Elektrobit Automotive and the Robotics, Vision and Control Lab (ROVIS) (www.rovislab.com), Transilvania University of Braşov, Braşov, Romania","IEEE Robotics and Automation Letters","","2019","4","4","3441","3448","Autonomous vehicles are controlled today either based on sequences of decoupled perception-planning-action operations, either based on End2End or deep reinforcement learning (DRL) systems. Current deep learning solutions for autonomous driving are subject to several limitations (e.g., they estimate driving actions through a direct mapping of sensors to actuators, or require complex reward shaping methods). Although the cost function used for training can aggregate multiple weighted objectives, the gradient descent step is computed by the backpropagation algorithm using a single-objective loss. To address these issues, we introduce NeuroTrajectory, which is a multiobjective neuroevolutionary approach to local state trajectory learning for autonomous driving, where the desired state trajectory of the ego-vehicle is estimated over a finite prediction horizon by a perception-planning deep neural network. In comparison to DRL methods, which predict optimal actions for the upcoming sampling time, we estimate a sequence of optimal states that can be used for motion control. We propose an approach which uses genetic algorithms for training a population of deep neural networks, where each network individual is evaluated based on a multi-objective fitness vector, with the purpose of establishing a so-called Pareto front of optimal deep neural networks. The performance of an individual is given by a fitness vector composed of three elements. Each element describes the vehicle's travel path, lateral velocity and longitudinal speed, respectively. The same network structure can be trained on synthetic, as well as on real-world data sequences. We have benchmarked our system against a baseline Dynamic Window Approach (DWA), as well as against an End2End supervised learning method.","","","10.1109/LRA.2019.2926224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752412","","Trajectory;Autonomous vehicles;Neural networks;Task analysis;Estimation;Training;Pipelines","backpropagation;control engineering computing;genetic algorithms;gradient methods;learning (artificial intelligence);mobile robots;motion control;neural nets;robot dynamics;traffic engineering computing;vectors;vehicle dynamics;vehicles","NeuroTrajectory;local state trajectory learning;autonomous vehicles;decoupled perception-planning-action operations;End2End;deep reinforcement learning systems;autonomous driving;multiple weighted objectives;gradient descent step;single-objective loss;multiobjective neuroevolutionary approach;finite prediction horizon;perception-planning deep neural network;DRL methods;multiobjective fitness vector;backpropagation algorithm;genetic algorithms;motion control;dynamic window approach;deep learning","","","18","Traditional","","","","IEEE","IEEE Journals"
"Deep Continuous Conditional Random Fields With Asymmetric Inter-Object Constraints for Online Multi-Object Tracking","H. Zhou; W. Ouyang; J. Cheng; X. Wang; H. Li","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; The University of Sydney, Sydney, NSW, Australia; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","4","1011","1022","Online multi-object tracking (MOT) is a challenging problem and has many important applications including intelligence surveillance, robot navigation, and autonomous driving. In existing MOT methods, individual object's movements and inter-object relations are mostly modeled separately and relations between them are still manually tuned. In addition, inter-object relations are mostly modeled in a symmetric way, which we argue is not an optimal setting. To tackle those difficulties, in this paper, we propose a deep continuous conditional random field (DCCRF) for solving the online MOT problem in a track-by-detection framework. The DCCRF consists of unary and pairwise terms. The unary terms estimate tracked objects' displacements across time based on visual appearance information. They are modeled as deep convolution neural networks, which are able to learn discriminative visual features for tracklet association. The asymmetric pairwise terms model inter-object relations in an asymmetric way, which encourages high-confidence tracklets to help correct errors of low-confidence tracklets and not to be affected by low-confidence ones much. The DCCRF is trained in an end-to-end manner for better adapting the influences of visual information as well as inter-object relations. Extensive experimental comparisons with state-of-the-arts as well as detailed component analysis of our proposed DCCRF on two public benchmarks demonstrate the effectiveness of our proposed MOT framework.","","","10.1109/TCSVT.2018.2825679","SenseTime Group Limited; General Research Fund through the Research Grants Council of Hong Kong; Hong Kong Innovation and Technology Support Programme; National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8335792","Multi-object tracking;deep neural networks;continuous conditional random fields;asymmetric pairwise terms","Tracking;Trajectory;Visualization;Neural networks;Machine learning;Mathematical model;Feature extraction","feature extraction;image motion analysis;image representation;learning (artificial intelligence);neural nets;object detection;object tracking;optimisation;target tracking;video signal processing","online multiobject tracking;deep continuous conditional random field;DCCRF;online MOT problem;track-by-detection framework;deep-continuous conditional random fields;asymmetric interobject constraints;unary term;asymmetric pairwise terms;deep convolution neural network;discriminative visual features;tracklet association","","3","45","","","","","IEEE","IEEE Journals"
"Attention Residual Learning for Skin Lesion Classification","J. Zhang; Y. Xie; Y. Xia; C. Shen","National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Medical Imaging","","2019","38","9","2092","2103","Automated skin lesion classification in dermoscopy images is an essential way to improve the diagnostic performance and reduce melanoma deaths. Although deep convolutional neural networks (DCNNs) have made dramatic breakthroughs in many image classification tasks, accurate classification of skin lesions remains challenging due to the insufficiency of training data, inter-class similarity, intra-class variation, and the lack of the ability to focus on semantically meaningful lesion parts. To address these issues, we propose an attention residual learning convolutional neural network (ARL-CNN) model for skin lesion classification in dermoscopy images, which is composed of multiple ARL blocks, a global average pooling layer, and a classification layer. Each ARL block jointly uses the residual learning and a novel attention learning mechanisms to improve its ability for discriminative representation. Instead of using extra learnable layers, the proposed attention learning mechanism aims to exploit the intrinsic self-attention ability of DCNNs, i.e., using the feature maps learned by a high layer to generate the attention map for a low layer. We evaluated our ARL-CNN model on the ISIC-skin 2017 dataset. Our results indicate that the proposed ARL-CNN model can adaptively focus on the discriminative parts of skin lesions, and thus achieve the state-of-the-art performance in skin lesion classification.","","","10.1109/TMI.2019.2893944","National Natural Science Foundation of China; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620285","Attention learning;residual learning;skin lesion classification;dermoscopy images","Lesions;Skin;Melanoma;Learning systems;Task analysis;Visualization;Computational modeling","cancer;convolutional neural nets;feature extraction;image classification;image representation;learning (artificial intelligence);medical image processing;skin","dermoscopy images;image classification tasks;convolutional neural network model;classification layer;attention learning mechanism;intrinsic self-attention ability;ARL-CNN model;attention residual learning;skin lesion classification;global average pooling layer;discriminative representation","","2","55","","","","","IEEE","IEEE Journals"
"Idle Time Window Prediction in Cellular Networks with Deep Spatiotemporal Modeling","L. Fang; X. Cheng; H. Wang; L. Yang","Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, CO, USA; State Key Laboratory of Advanced Optical Communication Systems and Network, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; Department of Statistics, Colorado State University, Fort Collins, CO, USA; Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, CO, USA","IEEE Journal on Selected Areas in Communications","","2019","37","6","1441","1454","Idle time windows (ITWs) consist of one critical trigger for various functions in green intelligent network management and traffic scheduling in mobile networks. In this paper, we study the ITW prediction in mobile networks based on network subscribers' demand and mobility behaviors observed by network operators. We first innovatively formulate the ITW prediction into a regression problem with an ITW presence confidence index that facilitates direct ITW detection and estimation. Feature extraction on the demand and mobility history is then proposed to capture the current trends of subscribers' demand and mobility as well as to account for the periodicity underlying subscribers' demand and mobility patterns as exogenous inputs. In light of feature engineering, a deep learning-based ITW prediction model is proposed, which consists of two components, namely the representation learning network and the output network. The representation learning network is aimed to learn effective patterns, whereas the output network is designed to produce the desired ITW presence confidence index and the ITW estimate by integrating the learned representation and exogenous inputs. In this paper, a novel temporal graph convolutional network (TGCN) for the representation learning network is proposed to effectively capture the graph-based spatiotemporal input features. The experiment results validate the proposed direct ITW prediction formulation and demonstrate the superiority of the proposed TGCN in terms of both ITW detection and ITW estimation performance, which can achieve a significant intersection-over-union (IoU) improvement compared with baselines.","","","10.1109/JSAC.2019.2904367","National Natural Science Foundation of China; Shenzhen Fundamental Research Fund; Shenzhen Peacock Plan; Guangdong Province; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667305","Machine learning;mobile communication;predictive models","Predictive models;Time series analysis;Estimation;Feature extraction;Spatiotemporal phenomena;Indexes;Microprocessors","feature extraction;graph theory;learning (artificial intelligence);regression analysis;spatiotemporal phenomena;telecommunication network management;telecommunication traffic","cellular networks;deep spatiotemporal modeling;idle time windows;green intelligent network management;traffic scheduling;mobile networks;network subscribers;network operators;mobility history;exogenous inputs;deep learning-based ITW prediction model;representation learning network;output network;ITW estimate;ITW prediction formulation;spatiotemporal input features;temporal graph convolutional network;ITW presence confidence index;mobility patterns","","1","35","","","","","IEEE","IEEE Journals"
"System Statistics Learning-Based IoT Security: Feasibility and Suitability","F. Li; A. Shinde; Y. Shi; J. Ye; X. Li; W. Song","Center for Cyber-Physical Systems, University of Georgia, Athens, GA, USA; Center for Cyber-Physical Systems, University of Georgia, Athens, GA, USA; Center for Cyber-Physical Systems, University of Georgia, Athens, GA, USA; Center for Cyber-Physical Systems, University of Georgia, Athens, GA, USA; School of Computer Science, University of Science and Technology of China, Hefei, China; Center for Cyber-Physical Systems, University of Georgia, Athens, GA, USA","IEEE Internet of Things Journal","","2019","6","4","6396","6403","Cyber attacks and malfunctions challenge the wide applications of Internet of Things (IoT). Since they are generally designed as embedded systems, typical auto-sustainable IoT devices usually have a limited capacity and a low processing power. Because of the limited computation resources, it is difficult to apply the traditional techniques designed for personal computers or super computers, like traffic analyzers and antivirus software. In this paper, we propose to leverage statistical learning methods to characterize the device behavior and flag deviations as anomalies. Because the system statistics, such as CPU usage cycles, disk usage, etc., can be obtained by IoT application program interfaces, the proposed framework is platform and deviceindependent. Considering IoT applications, we train multiple machine learning models to evaluate their feasibility and suitability. For the target auto-sustainable IoT devices, which operate well-planned processes, the normal system performances can be modeled accurately. Based on time series analysis methods, such as local outlier factor, cumulative sum, and the proposed adaptive online thresholding, the anomalous behaviors can be effectively detected. Comparing their performances on detecting anomalies as well as the computation sources required, we conclude that relatively simple machine learning models are more suitable for IoT security, and a data-driven anomaly detection method is preferred.","","","10.1109/JIOT.2019.2897063","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633365","Anomaly detection;deep learning;failure and intrusion detection;Internet of Things (IoT);machine learning","Internet of Things;Security;Anomaly detection;Predictive models;Time series analysis;Adaptation models;Computational modeling","application program interfaces;embedded systems;Internet of Things;learning (artificial intelligence);security of data;statistical analysis;time series","time series analysis methods;data-driven anomaly detection method;system statistics learning-based IoT security;embedded systems;IoT application program interfaces;multiple machine learning models;statistical learning methods;auto-sustainable IoT devices","","2","41","","","","","IEEE","IEEE Journals"
"Attention-Based Pedestrian Attribute Analysis","Z. Tan; Y. Yang; J. Wan; H. Hang; G. Guo; S. Z. Li","Center for Biometrics and Security Research, Chinese Academy of Sciences, Beijing, China; Center for Biometrics and Security Research, Chinese Academy of Sciences, Beijing, China; Center for Biometrics and Security Research, Chinese Academy of Sciences, Beijing, China; Institute of Statistics and Big Data (ISBD), Renmin University of China, Beijing, China; Baidu Research, Institute of Deep Learning, Beijing, China; Center for Biometrics and Security Research, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2019","28","12","6126","6140","Recognizing the pedestrian attributes in surveillance scenes is an inherently challenging task, especially for the pedestrian images with large pose variations, complex backgrounds, and various camera viewing angles. To select important and discriminative regions or pixels against the variations, three attention mechanisms are proposed, including parsing attention, label attention, and spatial attention. Those attentions aim at accessing effective information by considering problems from different perspectives. To be specific, the parsing attention extracts discriminative features by learning not only where to turn attention to but also how to aggregate features from different semantic regions of human bodies, e.g., head and upper body. The label attention aims at targetedly collecting the discriminative features for each attribute. Different from the parsing and label attention mechanisms, the spatial attention considers the problem from a global perspective, aiming at selecting several important and discriminative image regions or pixels for all attributes. Then, we propose a joint learning framework formulated in a multi-task-like way with these three attention mechanisms learned concurrently to extract complementary and correlated features. This joint learning framework is named Joint Learning of Parsing attention, Label attention, and Spatial attention for Pedestrian Attributes Analysis (JLPLS-PAA, for short). Extensive comparative evaluations conducted on multiple large-scale benchmarks, including PA-100K, RAP, PETA, Market-1501, and Duke attribute datasets, further demonstrate the effectiveness of the proposed JLPLS-PAA framework for pedestrian attribute analysis.","","","10.1109/TIP.2019.2919199","National Key Research and Development Plan; National Natural Science Foundation of China; Science and Technology Development Fund of Macau; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8755326","Pedestrian attribute analysis;attention mechanism;pedestrian parsing","Feature extraction;Task analysis;Deep learning;Image recognition;Aggregates;Semantics;Biometrics (access control)","feature extraction;feature selection;learning (artificial intelligence);pedestrians;pose estimation","pedestrian attribute analysis;pedestrian images;label attention;spatial attention;feature extraction;pedestrian pose variations;Joint Learning of Parsing attention;discriminative image region selection","","3","65","","","","","IEEE","IEEE Journals"
"Deep Learning-Inspired Message Passing Algorithm for Efficient Resource Allocation in Cognitive Radio Networks","M. Liu; T. Song; J. Hu; J. Yang; G. Gui","National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; College of Telecommunication and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Telecommunication and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Transactions on Vehicular Technology","","2019","68","1","641","653","Energy efficiency (EE) and spectrum efficiency (SE) have received significant attentions on optimizing the network performance in cognitive radio networks. In this paper, an EE+SE tradeoff based target is considered for the primary users (PUs) and the secondary users (SUs). First of all, considering the orthogonal frequency division multiple access-based resource allocation (RA) for the underlying SUs, we formulate an objective function through minimizing a weighted sum of the secondary interference power, where the network performance of both PUs and SUs are guaranteed by the constraints on quality of service, power consumption and data rate. However, it is a NP-hard problem. In order to solve it, we propose a damped three dimensional (D3D) message-passing algorithm (MPA) based on deep learning. Specifically, a feed-forward neural network is devised and an analogous back propagation algorithm is developed to learn the optimal parameters of the D3D-MPA. To improve the computational efficiency of the allocation and the learning, a suboptimal RA scheme is deduced based on a damped two dimensional MPA. Finally, simulation results are provided to confirm the effectiveness of our proposed scheme.","","","10.1109/TVT.2018.2883669","National Natural Science Foundation of China; Jiangsu Specially Appointed Professor; Innovation and Entrepreneurship of Jiangsu High-level Talent; Summit of the Six Top Talents Program of Jiangsu; Science and Technology Research Project of Chongqing Municipal Education Commission; NJUPTSF; “1311 Talent Plan” of Nanjing University of Posts and Telecommunications; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8546798","Cognitive radio;resource allocation;efficiency tradeoff;message passing;deep learning","Interference;Message passing;Resource management;Optimization;Neural networks;IP networks","backpropagation;cognitive radio;computational complexity;energy conservation;feedforward neural nets;frequency division multiple access;message passing;OFDM modulation;optimisation;radiofrequency interference;resource allocation;telecommunication power management","NP-hard problem;energy efficiency;D3D MPA;damped 3D message-passing algorithm;damped 2D MPA;orthogonal frequency division multiple access;resource allocation;secondary users;primary users;EE+SE tradeoff based target;spectrum efficiency;cognitive radio networks;message passing algorithm;computational efficiency;analogous back propagation algorithm;feed-forward neural network;deep learning;power consumption;secondary interference power","","44","43","","","","","IEEE","IEEE Journals"
"Unsupervised Semantic-Preserving Adversarial Hashing for Image Search","C. Deng; E. Yang; T. Liu; J. Li; W. Liu; D. Tao","School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; Tencent AI Lab, Shenzhen, China; UBTECH Sydney Artificial Intelligence Centre, School of Computer Science, Faculty of Engineering and Information Technologies, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Image Processing","","2019","28","8","4032","4044","Hashing plays a pivotal role in nearest-neighbor searching for large-scale image retrieval. Recently, deep learning-based hashing methods have achieved promising performance. However, most of these deep methods involve discriminative models, which require large-scale, labeled training datasets, thus hindering their real-world applications. In this paper, we propose a novel strategy to exploit the semantic similarity of the training data and design an efficient generative adversarial framework to learn binary hash codes in an unsupervised manner. Specifically, our model consists of three different neural networks: an encoder network to learn hash codes from images, a generative network to generate images from hash codes, and a discriminative network to distinguish between pairs of hash codes and images. By adversarially training these networks, we successfully learn mutually coherent encoder and generative networks, and can output efficient hash codes from the encoder network. We also propose a novel strategy, which utilizes both feature and neighbor similarities, to construct a semantic similarity matrix, then use this matrix to guide the hash code learning process. Integrating the supervision of this semantic similarity matrix into the adversarial learning framework can efficiently preserve the semantic information of training data in Hamming space. The experimental results on three widely used benchmarks show that our method not only significantly outperforms several state-of-the-art unsupervised hashing methods, but also achieves comparable performance with popular supervised hashing methods.","","","10.1109/TIP.2019.2903661","National Natural Science Foundation of China; Key R&D Program-The Key Industry Innovation Chain of Shaanxi; National Key R&D Program of China; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666767","Hashing;image search;adversarial learning;deep learning","Semantics;Hash functions;Binary codes;Image reconstruction;Generators;Gallium nitride;Generative adversarial networks","binary codes;file organisation;image coding;image retrieval;matrix algebra;neural nets;unsupervised learning","unsupervised semantic-preserving adversarial hashing;image search;nearest-neighbor searching;large-scale image retrieval;deep learning-based hashing methods;discriminative models;labeled training datasets;training data;efficient generative adversarial framework;binary hash codes;encoder network;generative network;discriminative network;output efficient hash codes;neighbor similarities;semantic similarity matrix;hash code learning process;adversarial learning framework;semantic information;supervised hashing methods;neural networks","","1","72","","","","","IEEE","IEEE Journals"
"Deep CNNs for Object Detection Using Passive Millimeter Sensors","S. López-Tapia; R. Molina; N. P. de la Blanca","Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain; Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain; Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","9","2580","2589","Passive millimeter wave images (PMMWIs) can be used to detect and localize objects concealed under clothing. Unfortunately, the quality of the acquired images and the unknown position, shape, and size of the hidden objects render these tasks challenging. In this paper, we discuss a deep learning approach to this detection/localization problem. The effect of the nonstationary acquisition noise on different architectures is analyzed and discussed. A comparison with shallow architectures is also presented. The achieved detection accuracy defines a new state of the art in object detection on PMMWIs. The low computational training and testing costs of the solution allow its use in real-time applications.","","","10.1109/TCSVT.2017.2774927","Spanish Ministry of Economy and Competitiveness (MINECO); FEDER Funds; Junta de Andalucía; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8114272","Classification;deep learning;millimeter wave imaging;object detection;security","Computer architecture;Image segmentation;Feature extraction;Convolution;Image sensors;Sensors","convolutional neural nets;image sensors;learning (artificial intelligence);millimetre wave detectors;millimetre wave imaging;millimetre wave measurement;object detection","PMMWI;deep CNN;image acquisition;nonstationary acquisition noise;deep learning approach;passive millimeter wave images;passive millimeter sensors;object detection","","","43","","","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Event-Driven Multi-Agent Decision Processes","K. Menda; Y. Chen; J. Grana; J. W. Bono; B. D. Tracey; M. J. Kochenderfer; D. Wolpert","Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA; Anderson School of Management, University of California at Los Angeles, Los Angeles, CA, USA; Santa Fe Institute, Santa Fe, NM, USA; Economists Inc., San Francisco, CA, USA; Santa Fe Institute, Santa Fe, NM, USA; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA; Resident Faculty, Santa Fe Institute, Santa Fe, NM, USA","IEEE Transactions on Intelligent Transportation Systems","","2019","20","4","1259","1268","The incorporation of macro-actions (temporally extended actions) into multi-agent decision problems has the potential to address the curse of dimensionality associated with such decision problems. Since macro-actions last for stochastic durations, multiple agents executing decentralized policies in cooperative environments must act asynchronously. We present an algorithm that modifies generalized advantage estimation for temporally extended actions, allowing a state-of-the-art policy optimization algorithm to optimize policies in Dec-POMDPs in which agents act asynchronously. We show that our algorithm is capable of learning optimal policies in two cooperative domains, one involving real-time bus holding control and one involving wildfire fighting with unmanned aircraft. Our algorithm works by framing problems as “event-driven decision processes,” which are scenarios in which the sequence and timing of actions and events are random and governed by an underlying stochastic process. In addition to optimizing policies with continuous state and action spaces, our algorithm also facilitates the use of event-driven simulators, which do not require time to be discretized into time-steps. We demonstrate the benefit of using event-driven simulation in the context of multiple agents taking asynchronous actions. We show that fixed time-step simulation risks obfuscating the sequence in which closely separated events occur, adversely affecting the policies learned. In addition, we show that arbitrarily shrinking the time-step scales poorly with the number of agents.","","","10.1109/TITS.2018.2848264","National Aeronautics and Space Administration; Army Research Office; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419722","Artificial intelligence;autonomous vehicles;discrete event simulation;distributed decision-making;neural networks;multi-agent systems","Aerospace electronics;Aircraft;Learning (artificial intelligence);Computational modeling;Atmospheric modeling;Machine learning;Estimation","aircraft;autonomous aerial vehicles;decision theory;learning (artificial intelligence);Markov processes;multi-agent systems;stochastic processes;wildfires","continuous state;event-driven simulation;multiple agents;asynchronous actions;fixed time-step simulation risks;closely separated events;deep reinforcement;event-driven multiagent decision processes;temporally extended actions;multiagent decision problems;stochastic durations;decentralized policies;generalized advantage estimation;optimal policies;real-time bus;involving wildfire;event-driven decision processes","","","18","","","","","IEEE","IEEE Journals"
"Sparse Self-Attention LSTM for Sentiment Lexicon Construction","D. Deng; L. Jing; J. Yu; S. Sun","School of Computer and Information Technology, Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; School of Management, Xi’an Jiaotong University, Xi’an, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","11","1777","1790","Sentiment lexicon is a very important resource for opinion mining. Recently, many state-of-the-art works employ deep learning techniques to construct sentiment lexicons. In general, they firstly learn sentiment-aware word embeddings, and then use it as word features to construct sentiment lexicons. However, these methods do not consider the importance of each word to the distinguish of documents' sentiment polarities. As we know, most words among a document do not contribute to understand documents' semantic or sentiment. For example, in the tweet It's a good day, but i can't feel it. I'm really unhappy. The words `unhappy', `feel' and `can't' are much more important than the words `good', `day' in predicting the sentiment polarity of this twitter. Meanwhile, many words, such as `the', `in', `it' and `I'm' are uninformative. In this paper, we propose a novel sparse self-attention LSTM (SSALSTM) to efficiently capture the above intuitive facts, and then construct a large scale sentiment lexicons in twitter. In SSALSTM, we use a novel self-attention mechanism to capture the importance of each words to the distinguish of documents' sentiment polarities. In addition, a $L_1$ regularize is applied in the attentions which can ensure the sparsity characters that most words in a document are semantic and sentiment indistinguishable. Once we learn an efficient sentiment-aware word embedding, we train a classifier which uses sentiment-aware word embedding as features to predict the sentiment polarities of words. Extensive experiments on four publicly available datasets, SemEval 2013-2016, indicate that the sentiment lexicon generated by our proposed model achieves state-of-the-art performance on both supervised and unsupervised sentiment classification tasks.","","","10.1109/TASLP.2019.2933326","National Natural Science Foundation of China; Beijing Natural Science Foundation; Beijing Municipal Science and Technology Commission; National Key Research and Development Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8788581","Sentiment lexicon;sentiment analysis;sparsity;deep learning;text mining","Semantics;Task analysis;Dictionaries;Deep learning;Sentiment analysis;Speech processing;Social networking (online)","data mining;feature extraction;learning (artificial intelligence);natural language processing;neural nets;pattern classification;social networking (online);text analysis","supervised sentiment classification tasks;unsupervised sentiment classification tasks;sparse self-attention LSTM;sentiment lexicon construction;word features;opinion mining;deep learning techniques;sentiment-aware word embeddings;document sentiment polarities;large scale sentiment lexicons;SSALSTM;sparsity characters;publicly available datasets","","","41","Traditional","","","","IEEE","IEEE Journals"
"An Inductive Content-Augmented Network Embedding Model for Edge Artificial Intelligence","B. Yuan; J. Panneerselvam; L. Liu; N. Antonopoulos; Y. Lu","Key Laboratory of Embedded System and Service Computing, Tongji University, Shanghai, China; School of Electronics, Computing and Mathematics, University of Derby, Derby, U.K.; Department of Informatics, University of Leicester, Leicester, U.K.; Edinburgh Napier University, Edinburgh, U.K.; School of Electronics, Computing and Mathematics, University of Derby, Derby, U.K.","IEEE Transactions on Industrial Informatics","","2019","15","7","4295","4305","Real-time data processing applications demand dynamic resource provisioning and efficient service discovery, which is particularly challenging in resource-constraint edge computing environments. Network embedding techniques can potentially aid effective resource discovery services in edge environments, by achieving a proximity-preserving representation of the network resources. Most of the existing techniques of network embedding fail to capture accurate proximity information among the network nodes and further lack exploiting information beyond the second-order neighbourhood. This paper leverages artificial intelligence for network representation and proposes a deep learning model, named inductive content augmented network embedding (ICANE), which integrates the network structure and resource content attributes into a feature vector. Secondly, a hierarchical aggregation approach is introduced to explicitly learn the network representation through sampling the nodes and aggregating features from the higher-order neighbourhood. A semantic proximity search model is then designed to generate the top-k ranking of relevant nodes using the learned network representation. Experiments conducted on real-world datasets demonstrate the superiority of the proposed model over the existing popular methods in terms of resource discovery and the query resolving performance.","","","10.1109/TII.2019.2902877","Natural Science Foundation of Jiangsu Province; UK–Jiangsu 20-20 World-Class University Initiative; UK–Jiangsu 20-20 Initiative Pump Priming; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658146","Artificial intelligence (AI);deep learning;edge computing;network embedding;resource discovery","Computational modeling;Informatics;Edge computing;Semantics;Deep learning;Mathematics;Task analysis","artificial intelligence;graph theory;learning (artificial intelligence);query processing;resource allocation","network structure;semantic proximity search model;inductive content-augmented network embedding model;edge artificial intelligence;real-time data processing applications;dynamic resource provisioning;resource-constraint edge computing environments;network embedding techniques;edge environments;proximity-preserving representation;network resources;network nodes;second-order neighbourhood;deep learning model;network representation;inductive content;proximity information;resource discovery services;service discovery","","","35","","","","","IEEE","IEEE Journals"
"Unsupervised Detection of Anomalous Sound Based on Deep Learning and the Neyman–Pearson Lemma","Y. Koizumi; S. Saito; H. Uematsu; Y. Kawachi; N. Harada","NTT Media Intelligence Laboratories, NTT Corporation, Tokyo, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Tokyo, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Tokyo, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Tokyo, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Tokyo, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","1","212","224","This paper proposes a novel optimization principle and its implementation for unsupervised anomaly detection in sound (ADS) using an autoencoder (AE). The goal of the unsupervised-ADS is to detect unknown anomalous sounds without training data of anomalous sounds. The use of an AE as a normal model is a state-of-the-art technique for the unsupervised-ADS. To decrease the false positive rate (FPR), the AE is trained to minimize the reconstruction error of normal sounds, and the anomaly score is calculated as the reconstruction error of the observed sound. Unfortunately, since this training procedure does not take into account the anomaly score for anomalous sounds, the true positive rate (TPR) does not necessarily increase. In this study, we define an objective function based on the Neyman-Pearson lemma by considering the ADS as a statistical hypothesis test. The proposed objective function trains the AE to maximize the TPR under an arbitrary low FPR condition. To calculate the TPR in the objective function, we consider that the set of anomalous sounds is the complementary set of normal sounds and simulate anomalous sounds by using a rejection sampling algorithm. Through experiments using synthetic data, we found that the proposed method improved the performance measures of the ADS under low FPR conditions. In addition, we confirmed that the proposed method could detect anomalous sounds in real environments.","","","10.1109/TASLP.2018.2877258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8501554","Anomaly detection in sound;Neyman-Pearson lemma;deep learning;and autoencoder","Linear programming;Task analysis;Probability density function;Speech processing;Training data;Feature extraction","acoustic signal detection;learning (artificial intelligence);optimisation;sampling methods;security of data;statistical analysis;unsupervised learning","true positive rate;Neyman-Pearson lemma;simulate anomalous sounds;anomaly score;reconstruction error;unknown anomalous sounds;unsupervised-ADS;unsupervised anomaly detection;anomalous sound;unsupervised detection","","4","39","","","","","IEEE","IEEE Journals"
"Deep Clustering via Weighted $k$-Subspace Network","W. Huang; M. Yin; J. Li; S. Xie","Guangdong Key Laboratory of IoT Information Processing, School of Automation, Guangdong University of Technology, Guangzhou, China; Guangdong Key Laboratory of IoT Information Processing, School of Automation, Guangdong University of Technology, Guangzhou, China; Guangdong Key Laboratory of IoT Information Processing, School of Automation, Guangdong University of Technology, Guangzhou, China; Guangdong Key Laboratory of IoT Information Processing, School of Automation, Guangdong University of Technology, Guangzhou, China","IEEE Signal Processing Letters","","2019","26","11","1628","1632","Subspace clustering aims to separate the data into clusters under the hypothesis that the samples within the same cluster will lie in the same low-dimensional subspace. Due to the tough pairwise constraints, k-subspace clustering is sensitive to outliers and initialization. In this letter, we present a novel deep architecture for k-subspace clustering to address this issue, called as Deep Weighted k-Subspace Clustering (DWSC). Specifically, our framework consists of autoencoder and weighted k-subsapce network. We first use the autoencoder to non-linearly compress the samples into the low-dimensional latent space. In the weighted k-subspace network, we feed the latent representation into the assignment network to output soft assignments which represent the probability of data belonging to the according subspace. Subsequently, the optimal k subspaces are identified by minimizing the projection residuals of the latent representations to all subspaces, using the learned soft assignments as a weighting vector. Finally, we jointly optimize the representation learning and clustering in a unified framework. Experimental results show that our approach outperforms the state-of-the-art subspace clustering methods on two benchmark datasets.","","","10.1109/LSP.2019.2941368","National Science Foundation China; Science and Technology Planning Project of Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8836663","Deep clustering;subspace clustering;weighted;autoencoder","Feature extraction;Training;Clustering algorithms;Signal processing algorithms;Neural networks;Decoding;Linear programming","data structures;learning (artificial intelligence);network theory (graphs);neural nets;pattern clustering","deep clustering;weighted k-subspace network;deep architecture;representation learning;deep weighted k-subspace clustering;data clusters;autoencoder","","","37","Traditional","","","","IEEE","IEEE Journals"
"Residual Learning of Deep Convolutional Neural Network for Seismic Random Noise Attenuation","F. Wang; S. Chen","School of Earth Sciences, Zhejiang University, Hangzhou, China; School of Earth Sciences, Zhejiang University, Hangzhou, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","8","1314","1318","Over the last decades, seismic random noise attenuation has been dominated by transform-based denoising methods over the last decades. However, these methods usually need to estimate the noise level and select an optimal transformation in advance, and they may generate some artifacts in the denoising result (e.g., nonsmooth edges and pseudo-Gibbs phenomena). To overcome these disadvantages, we trained a deep convolutional neural network (CNN) with residual learning for seismic data denoising. We used synthetic seismic data for network training rather than seismic images, and we adopted a method to preprocess the seismic data before it was inputted in the network to help network training. We demonstrate the performance of the deep CNN in seismic random noise attenuation based on the synthetic seismic data. Results of numerical experiments show that our network adaptively and effectively suppresses noise of different levels and exhibits a competitive performance in comparison with the traditional transform-based methods.","","","10.1109/LGRS.2019.2895702","Key Laboratory of Geological Information Technology, Ministry of Natural Resources of the People’s Republic of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746563","Convolutional neural network (CNN);residual learning;seismic data denoising","Noise reduction;Transforms;Training;Testing;Noise level;Convolutional neural networks;Biological neural networks","convolutional neural nets;geophysical image processing;image denoising;learning (artificial intelligence);random noise;seismology","residual learning;deep convolutional neural network;seismic random noise attenuation;noise level;seismic data denoising;synthetic seismic data;seismic images","","","19","","","","","IEEE","IEEE Journals"
"Deep Cooperative Sensing: Cooperative Spectrum Sensing Based on Convolutional Neural Networks","W. Lee; M. Kim; D. Cho","Department of Information and Communication Engineering, Institute of Marine Industry, Gyeongsang National University, Tongyeong, South Korea; Department of Communication Systems, EURECOM, Sophia-Antipolis, France; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Vehicular Technology","","2019","68","3","3005","3009","In this paper, we investigate cooperative spectrum sensing (CSS) in a cognitive radio network (CRN) where multiple secondary users (SUs) cooperate in order to detect a primary user, which possibly occupies multiple bands simultaneously. Deep cooperative sensing (DCS), which constitutes the first CSS framework based on a convolutional neural network (CNN), is proposed. In DCS, instead of the explicit mathematical modeling of CSS, the strategy for combining the individual sensing results of the SUs is learned autonomously with a CNN using training sensing samples regardless of whether the individual sensing results are quantized or not. Moreover, both spectral and spatial correlation of individual sensing outcomes are taken into account such that an environment-specific CSS is enabled in DCS. Through simulations, we show that the performance of CSS can be greatly improved by the proposed DCS.","","","10.1109/TVT.2019.2891291","National Research Foundation of Korea; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8604101","Cognitive radio network;cooperative spectrum sensing;deep learning;convolutional neural network;correlation","Sensors;Cascading style sheets;Correlation;Fading channels;Convolutional neural networks;Deep learning;Wireless sensor networks","convolutional neural nets;cooperative communication;radio spectrum management;signal detection;signal sampling;telecommunication computing","convolutional neural network;deep cooperative sensing;spectrum sensing;cognitive radio network;multiple secondary users;primary user;multiple bands;CSS framework","","6","15","","","","","IEEE","IEEE Journals"
"Cost-Effective Object Detection: Active Sample Mining With Switchable Selection Criteria","K. Wang; L. Lin; X. Yan; Z. Chen; D. Zhang; L. Zhang","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","3","834","850","Though quite challenging, leveraging large-scale unlabeled or partially labeled data in learning systems (e.g., model/classifier training) has attracted increasing attentions due to its fundamental importance. To address this problem, many active learning (AL) methods have been proposed that employ up-to-date detectors to retrieve representative minority samples according to predefined confidence or uncertainty thresholds. However, these AL methods cause the detectors to ignore the remaining majority samples (i.e., those with low uncertainty or high prediction confidence). In this paper, by developing a principled active sample mining (ASM) framework, we demonstrate that cost-effective mining samples from these unlabeled majority data are a key to train more powerful object detectors while minimizing user effort. Specifically, our ASM framework involves a switchable sample selection mechanism for determining whether an unlabeled sample should be manually annotated via AL or automatically pseudolabeled via a novel self-learning process. The proposed process can be compatible with minibatch-based training (i.e., using a batch of unlabeled or partially labeled data as a one-time input) for object detection. In this process, the detector, such as a deep neural network, is first applied to the unlabeled samples (i.e., object proposals) to estimate their labels and output the corresponding prediction confidences. Then, our ASM framework is used to select a number of samples and assign pseudolabels to them. These labels are specific to each learning batch based on the confidence levels and additional constraints introduced by the AL process and will be discarded afterward. Then, these temporarily labeled samples are employed for network fine-tuning. In addition, a few samples with low-confidence predictions are selected and annotated via AL. Notably, our method is suitable for object categories that are not seen in the unlabeled data during the learning process. Extensive experiments on two public benchmarks (i.e., the PASCAL VOC 2007/2012 data sets) clearly demonstrate that our ASM framework can achieve performance comparable to that of the alternative methods but with significantly fewer annotations.","","","10.1109/TNNLS.2018.2852783","National Ten Thousand Talents Program; Guangdong “Climbing Program” Special Funds; National Natural Science Foundation of China; Ministry of Public Security Science and Technology Police Foundation; Science and Technology Planning Project of Guangdong Province; Hong Kong RGC General Research Fund; Hong Kong Polytechnic University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8421608","Active learning (AL);large-scale object detection;neural networks;self-driven learning;semisupervised learning","Training;Detectors;Object detection;Task analysis;Data mining;Switches;Proposals","data mining;image retrieval;learning (artificial intelligence);object detection","cost-effective object detection;switchable selection criteria;learning systems;active learning methods;cost-effective mining samples;ASM framework;minibatch-based training;temporarily labeled samples;object categories;self-learning process;representative minority sample retrieval;active sample mining;switchable sample selection;network fine-tuning","","2","44","","","","","IEEE","IEEE Journals"
"DeepStore: An Interaction-Aware Wide&Deep Model for Store Site Recommendation With Attentional Spatial Embeddings","Y. Liu; B. Guo; N. Li; J. Zhang; J. Chen; D. Zhang; Y. Liu; Z. Yu; S. Zhang; L. Yao","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Alibaba Group, Hangzhou, China; Département Réseaux et Services Multimédia Mobiles, Institut Mines-Télécom/Télécom SudParis, Évry, France; Alibaba Group, Hangzhou, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Alibaba Group, Hangzhou, China; School of Computer Science and Engineering, University of New South Wales, Sydney, NSW, Australia","IEEE Internet of Things Journal","","2019","6","4","7319","7333","Store site recommendation is one of the essential business services in smart cities for brick-and-mortar enterprises. In recent years, the proliferation of multisource data in cities has fostered unprecedented opportunities to the data-driven store site recommendation, which aims at leveraging large-scale user-generated data to analyze and mine users' preferences for identifying the optimal location for a new store. However, most works in store site recommendation pay more attention to a single data source which lacks some significant data (e.g., consumption data and user profile data). In this paper, we aim to study the store site recommendation in a fine-grained manner. Specifically, we predict the consumption level of different users at the store based on multisource data, which can not only help the store placement but also benefit analyzing customer behavior in the store at different time periods. To solve this problem, we design a novel model based on the deep neural network, named DeepStore, which learns low- and high-order feature interactions explicitly and implicitly from dense and sparse features simultaneously. In particular, DeepStore incorporates three modules: 1) the cross network; 2) the deep network; and 3) the linear component. In addition, to learn the latent feature representation from multisource data, we propose two embedding methods for different types of data: 1) the filed embedding and 2) attention-based spatial embedding. Extensive experiments are conducted on a real-world dataset including store data, user data, and point-of-interest data, the results demonstrate that DeepStore outperforms the state-of-the-art models.","","","10.1109/JIOT.2019.2916143","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Northwestern Polytechnical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8712550","Attention mechanism;data analytics;deep learning;spatial embedding;store site recommendation","Data models;Internet of Things;Business;Neural networks;Feature extraction;Urban areas;Data mining","data analysis;data mining;electronic commerce;feature extraction;learning (artificial intelligence);neural nets;recommender systems","user-generated data analysis;data source;interaction-aware wide-and-deep model;attentional spatial embeddings;brick-and-mortar enterprises;customer behavior;deep neural network;linear component;mine users preferences;optimal location;data-driven store site recommendation;store placement;multisource data;user profile data;consumption data","","","48","","","","","IEEE","IEEE Journals"
"Sparse Coding Guided Spatiotemporal Feature Learning for Abnormal Event Detection in Large Videos","W. Chu; H. Xue; C. Yao; D. Cai","State Key Lab of CAD&CG, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; State Key Lab of CAD&CG, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; State Key Lab of CAD&CG, College of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Multimedia","","2019","21","1","246","255","Abnormal event detection in large videos is an important task in research and industrial applications, which has attracted considerable attention in recent years. Existing methods usually solve this problem by extracting local features and then learning an outlier detection model on training videos. However, most previous approaches merely employ hand-crafted visual features, which is a clear disadvantage due to their limited representation capacity. In this paper, we present a novel unsupervised deep feature learning algorithm for the abnormal event detection problem. To exploit the spatiotemporal information of the inputs, we utilize the deep three-dimensional convolutional network (C3D) to perform feature extraction. Then, the key problem is how to train the C3D network without any category labels. Here, we employ the sparse coding results of the hand-crafted features generated from the inputs to guide the unsupervised feature learning. Specifically, we define a multilevel similarity relationship between these inputs according to the statistical information of the shared atoms. In the following, we introduce the quadruplet concept to model the multilevel similarity structure, which could be used to construct a generalized triplet loss for training the C3D network. Furthermore, the C3D network could be utilized to generate the features for sparse coding again, and this pipeline could be iterated for several times. By jointly optimizing between the sparse coding and the unsupervised feature learning, we can obtain robust and rich feature representations. Based on the learned representations, the sparse reconstruction error is applied to predicting the anomaly score of each testing input. Experiments on several publicly available video surveillance datasets in comparison with a number of existing works demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods.","","","10.1109/TMM.2018.2846411","National Natural Science Foundation of China; National Youth Top-notch Talent Support Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8379443","Video analysis;unsupervised feature learning;sparse coding;anomaly detection","Feature extraction;Videos;Spatiotemporal phenomena;Event detection;Encoding;Anomaly detection;Task analysis","convolution;feature extraction;feedforward neural nets;image representation;object detection;unsupervised learning;video signal processing;video surveillance","unsupervised feature learning;robust feature representations;learned representations;sparse reconstruction error;sparse coding guided spatiotemporal feature learning;industrial applications;local features;outlier detection model;training videos;hand-crafted visual features;unsupervised deep feature learning algorithm;abnormal event detection problem;spatiotemporal information;three-dimensional convolutional network;feature extraction;sparse coding results;hand-crafted features;multilevel similarity relationship;video surveillance datasets","","6","74","","","","","IEEE","IEEE Journals"
"STRAINet: Spatially Varying sTochastic Residual AdversarIal Networks for MRI Pelvic Organ Segmentation","D. Nie; L. Wang; Y. Gao; J. Lian; D. Shen","Department of Computer Science, The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology, BRIC, The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Shanghai United Imaging Intelligence Company Ltd., Shanghai, China; Department of Radiation Oncology, The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology, BRIC, The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","5","1552","1564","Accurate segmentation of pelvic organs is important for prostate radiation therapy. Modern radiation therapy starts to use a magnetic resonance image (MRI) as an alternative to computed tomography image because of its superior soft tissue contrast and also free of risk from radiation exposure. However, segmentation of pelvic organs from MRI is a challenging problem due to inconsistent organ appearance across patients and also large intrapatient anatomical variations across treatment days. To address such challenges, we propose a novel deep network architecture, called “Spatially varying sTochastic Residual AdversarIal Network” (STRAINet), to delineate pelvic organs from MRI in an end-to-end fashion. Compared to the traditional fully convolutional networks (FCN), the proposed architecture has two main contributions: 1) inspired by the recent success of residual learning, we propose an evolutionary version of the residual unit, i.e., stochastic residual unit, and use it to the plain convolutional layers in the FCN. We further propose long-range stochastic residual connections to pass features from shallow layers to deep layers; and 2) we propose to integrate three previously proposed network strategies to form a new network for better medical image segmentation: a) we apply dilated convolution in the smallest resolution feature maps, so that we can gain a larger receptive field without overly losing spatial information; b) we propose a spatially varying convolutional layer that adapts convolutional filters to different regions of interest; and c) an adversarial network is proposed to further correct the segmented organ structures. Finally, STRAINet is used to iteratively refine the segmentation probability maps in an autocontext manner. Experimental results show that our STRAINet achieved the state-of-the-art segmentation accuracy. Further analysis also indicates that our proposed network components contribute most to the performance.","","","10.1109/TNNLS.2018.2870182","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8486982","Adversarial learning;dilation;pelvic organ segmentation;stochastic residual learning","Image segmentation;Magnetic resonance imaging;Biological systems;Feature extraction;Biomedical imaging;Task analysis;Manuals","biological organs;biological tissues;biomedical MRI;computerised tomography;image segmentation;learning (artificial intelligence);medical image processing","computed tomography image;radiation exposure;pelvic organs;deep network architecture;STRAINet;residual learning;stochastic residual unit;long-range stochastic residual connections;deep layers;network strategies;medical image segmentation;convolutional layer;segmented organ structures;segmentation probability maps;state-of-the-art segmentation accuracy;network components;sTochastic residual adversarial networks;MRI pelvic organ segmentation;prostate radiation therapy;magnetic resonance image;soft tissue contrast;fully convolutional networks;regions-of-interest","","","54","","","","","IEEE","IEEE Journals"
"Feedback Deep Deterministic Policy Gradient With Fuzzy Reward for Robotic Multiple Peg-in-Hole Assembly Tasks","J. Xu; Z. Hou; W. Wang; B. Xu; K. Zhang; K. Chen","State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Department of Mechanical Engineering, Tsinghua University, Beijing, China; State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Department of Mechanical Engineering, Tsinghua University, Beijing, China; State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Department of Mechanical Engineering, Tsinghua University, Beijing, China; Department of Mechanical Engineering, Dalian Jiaotong University, Dalian, China; State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Department of Mechanical Engineering, Tsinghua University, Beijing, China; State Key Laboratory of Tribology, Beijing Key Laboratory of Precision/Ultra-Precision Manufacturing Equipment Control, Department of Mechanical Engineering, Tsinghua University, Beijing, China","IEEE Transactions on Industrial Informatics","","2019","15","3","1658","1667","The automatic completion of multiple peg-in-hole assembly tasks by robots remains a formidable challenge because the traditional control strategies require a complex analysis of the contact model. In this paper, the assembly task is formulated as a Markov decision process, and a model-driven deep deterministic policy gradient algorithm is proposed to accomplish the assembly task through the learned policy without analyzing the contact states. In our algorithm, the learning process is driven by a simple traditional force controller. In addition, a feedback exploration strategy is proposed to ensure that our algorithm can efficiently explore the optimal assembly policy and avoid risky actions, which can address the data efficiency and guarantee stability in realistic assembly scenarios. To improve the learning efficiency, we utilize a fuzzy reward system for the complex assembly process. Then, simulations and realistic experiments of a dual peg-in-hole assembly demonstrate the effectiveness of the proposed algorithm. The advantages of the fuzzy reward system and feedback exploration strategy are validated by comparing the performances of different cases in simulations and experiments.","","","10.1109/TII.2018.2868859","National Natural Science Foundation of China; State Key Laboratory of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454796","Continuous actions control;feedback exploration;fuzzy reward;intelligent assembly;multiple peg-in-hole;reinforcement learning","Task analysis;Robotic assembly;Training;Service robots;Informatics;Analytical models","feedback;force control;gradient methods;learning (artificial intelligence);Markov processes;robotic assembly","feedback deep deterministic policy gradient;multiple peg-in-hole assembly tasks;Markov decision process;model-driven deep deterministic policy gradient algorithm;learned policy;learning process;feedback exploration strategy;fuzzy reward system;traditional force controller;robots","","2","22","","","","","IEEE","IEEE Journals"
"Training Lightweight Deep Convolutional Neural Networks Using Bag-of-Features Pooling","N. Passalis; A. Tefas","Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","6","1705","1715","Convolutional neural networks (CNNs) are predominantly used for several challenging computer vision tasks achieving state-of-the-art performance. However, CNNs are complex models that require the use of powerful hardware, both for training and deploying them. To this end, a quantization-based pooling method is proposed in this paper. The proposed method is inspired from the bag-of-features model and can be used for learning more lightweight deep neural networks. Trainable radial basis function neurons are used to quantize the activations of the final convolutional layer, reducing the number of parameters in the network and allowing for natively classifying images of various sizes. The proposed method employs differentiable quantization and aggregation layers leading to an end-to-end trainable CNN architecture. Furthermore, a fast linear variant of the proposed method is introduced and discussed, providing new insight for understanding convolutional neural architectures. The ability of the proposed method to reduce the size of CNNs and increase the performance over other competitive methods is demonstrated using seven data sets and three different learning tasks (classification, regression, and retrieval).","","","10.1109/TNNLS.2018.2872995","Horizon 2020 Framework Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8506624","Bag-of-features (BoF);convolutional neural networks (CNNs);lightweight neural networks;pooling operators","Feature extraction;Computer architecture;Training;Task analysis;Computational modeling;Quantization (signal);Backpropagation algorithms","computer vision;convolutional neural nets;data compression;image classification;image coding;learning (artificial intelligence);neural net architecture;radial basis function networks","complex models;quantization-based pooling method;bag-of-features model;lightweight deep neural networks;trainable radial basis function neurons;final convolutional layer;differentiable quantization;aggregation layers;end-to-end trainable CNN architecture;computer vision tasks;learning tasks;lightweight deep convolutional neural network training;convolutional neural architectures;bag-of-feature pooling;image classification;fast linear variant","","3","64","","","","","IEEE","IEEE Journals"
"Cascaded Deep Networks With Multiple Receptive Fields for Infrared Image Super-Resolution","Z. He; S. Tang; J. Yang; Y. Cao; M. Ying Yang; Y. Cao","State Key Laboratory of Fluid Power and Mechatronic Systems and Laboratory of Advanced Manufacturing Technology of Zhejiang Province, School of Mechanical Engineering, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of Fluid Power and Mechatronic Systems and Laboratory of Advanced Manufacturing Technology of Zhejiang Province, School of Mechanical Engineering, Zhejiang University, Hangzhou, China; State Key Laboratory of Fluid Power and Mechatronic Systems and Laboratory of Advanced Manufacturing Technology of Zhejiang Province, School of Mechanical Engineering, Zhejiang University, Hangzhou, China; Scene Understanding Group, ITC, Universiteit Twente, Enschede, Netherlands; State Key Laboratory of Fluid Power and Mechatronic Systems and Laboratory of Advanced Manufacturing Technology of Zhejiang Province, School of Mechanical Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","8","2310","2322","Infrared images have a wide range of military and civilian applications, including night vision, surveillance, and robotics. However, high-resolution infrared detectors are difficult to fabricate and their manufacturing cost is expensive. In this paper, we present a cascaded architecture of deep neural networks with multiple receptive fields to increase the spatial resolution of infrared images by a large scale factor (x8). Instead of reconstructing a high-resolution image from its low-resolution version using a single complex deep network, the key idea of our approach is to set up a mid-point (scale x2) between scale x1 and x8 such that lost information can be divided into two components. Lost information within each component contains similar patterns thus can be more accurately recovered even using a simpler deep network. In our proposed cascaded architecture, two consecutive deep networks with different receptive fields are jointly trained through a multi-scale loss function. The first network with a large receptive field is applied to recover largescale structure information, while the second one uses a relatively smaller receptive field to reconstruct small-scale image details. Our proposed method is systematically evaluated using realistic infrared images. Compared with state-of-the-art super-resolution methods, our proposed cascaded approach achieves improved reconstruction accuracy using significantly fewer parameters.","","","10.1109/TCSVT.2018.2864777","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8432397","Infrared imaging;super-resolution;cascaded architecture;deep networks;receptive fields","Image reconstruction;Spatial resolution;Image restoration;Dictionaries;Machine learning;Training","image reconstruction;image resolution;infrared detectors;infrared imaging;neural nets","high-resolution infrared detectors;manufacturing cost;cascaded architecture;deep neural networks;multiple receptive fields;spatial resolution;high-resolution image;low-resolution version;single complex deep network;simpler deep network;consecutive deep networks;multiscale loss function;largescale structure information;relatively smaller receptive field;small-scale image details;infrared image super-resolution;large scale factor;cascaded deep networks;reconstruction accuracy","","1","39","","","","","IEEE","IEEE Journals"
"Patch-Based Output Space Adversarial Learning for Joint Optic Disc and Cup Segmentation","S. Wang; L. Yu; X. Yang; C. Fu; P. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Medical Imaging","","2019","38","11","2485","2495","Glaucoma is a leading cause of irreversible blindness. Accurate segmentation of the optic disc (OD) and optic cup (OC) from fundus images is beneficial to glaucoma screening and diagnosis. Recently, convolutional neural networks demonstrate promising progress in the joint OD and OC segmentation. However, affected by the domain shift among different datasets, deep networks are severely hindered in generalizing across different scanners and institutions. In this paper, we present a novel patch-based output space adversarial learning framework (pOSAL) to jointly and robustly segment the OD and OC from different fundus image datasets. We first devise a lightweight and efficient segmentation network as a backbone. Considering the specific morphology of OD and OC, a novel morphology-aware segmentation loss is proposed to guide the network to generate accurate and smooth segmentation. Our pOSAL framework then exploits unsupervised domain adaptation to address the domain shift challenge by encouraging the segmentation in the target domain to be similar to the source ones. Since the whole-segmentation-based adversarial loss is insufficient to drive the network to capture segmentation details, we further design the pOSAL in a patch-based fashion to enable fine-grained discrimination on local segmentation details. We extensively evaluate our pOSAL framework and demonstrate its effectiveness in improving the segmentation performance on three public retinal fundus image datasets, i.e., Drishti-GS, RIM-ONE-r3, and REFUGE. Furthermore, our pOSAL framework achieved the first place in the OD and OC segmentation tasks in the MICCAI 2018 Retinal Fundus Glaucoma Challenge.","","","10.1109/TMI.2019.2899910","National Basic Research Program of China (973 Program); Shenzhen Science and Technology Program; Hong Kong Research Grants Council through the General Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643416","Optic disc segmentation;optic cup segmentation;deep learning;domain adaptation;adversarial learning","Image segmentation;Retina;Optical imaging;Biomedical optical imaging;Task analysis;Testing;Training","biomedical optical imaging;convolutional neural nets;diseases;eye;image segmentation;medical image processing;unsupervised learning;vision defects","optic cup segmentation;irreversible blindness;glaucoma screening;convolutional neural networks;deep networks;smooth segmentation;pOSAL framework;unsupervised domain adaptation;whole-segmentation-based adversarial loss;local segmentation details;segmentation performance;public retinal fundus image datasets;MICCAI 2018 Retinal Fundus Glaucoma Challenge;morphology-aware segmentation loss;patch-based output space adversarial learning framework;joint optic disc segmentation;glaucoma diagnosis;domain shift;RIM-ONE-r3;Drishti-GS;REFUGE","","","52","","","","","IEEE","IEEE Journals"
"Through-Wall Remote Human Voice Recognition Using Doppler Radar With Transfer Learning","R. Khanna; D. Oh; Y. Kim","Department of Electrical and Computer Engineering, California State University, Fresno, CA, USA; Advanced Radar Research Division, Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea; Department of Electrical and Computer Engineering, California State University, Fresno, CA, USA","IEEE Sensors Journal","","2019","19","12","4571","4576","We investigated the feasibility of using Doppler radar to recognize human voices by capturing the micro-Doppler signatures of vibrations from the larynx and mouth. The signatures produced through the vibrations of a human being's vocal cords generate unique micro-Doppler signatures, depending on the letters pronounced. These can then be used to classify and recognize different words and letters. In this paper, we could successfully capture echo signals using the Doppler radar when a human subject spoke seven musical notes from Do to Ti and alphabet letters from A to Z. Spectrogram analysis was conducted for classification purposes, and the deep convolutional neural networks employed could classify the 26 letters to an accuracy of 94%. To overcome the deficiency of the measured data and improve the classification accuracy, transfer learning was introduced. Using the VGG-16 model, its accuracy was improved up to 97%. Additional experiments were conducted to ascertain the radar's capability to detect the human voice through a barrier between the human and the radar. In this paper, we demonstrated the possibility of remote voice recognition using Doppler information, with or without a barrier.","","","10.1109/JSEN.2019.2901271","Daegu Gyeongbuk Institute of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653899","Human voice recognition;micro-Doppler signatures;deep learning;convolutional neural network;transfer learning;AlexNet;VGG-16","Doppler radar;Vibrations;Speech recognition;Human voice;Doppler effect;Spectrogram","convolutional neural nets;Doppler radar;learning (artificial intelligence);radar computing;signal classification;speech recognition","wall remote human voice recognition;Doppler radar;transfer learning;microDoppler signatures;vibrations;human subject;alphabet letters;remote voice recognition;Doppler information;classification purposes;deep convolutional neural networks;spectrogram analysis","","","17","","","","","IEEE","IEEE Journals"
"Placement Delivery Array Design via Attention-Based Sequence-to-Sequence Model With Deep Neural Network","Z. Zhang; M. Hua; C. Li; Y. Huang; L. Yang","National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, School of Information Science and Engineering, Southeast University, Nanjing, China","IEEE Wireless Communications Letters","","2019","8","2","372","375","Recently, coded caching scheme was proposed as the ability of alleviating the load of networks. Especially, the placement delivery array (PDA) used for characterizing the coded caching scheme has attracted vast attention. In this letter, a deep neural architecture is first proposed to learn the construction of PDAs for reducing the computational complexity. The problem of variable size of PDAs is solved using mechanism of neural attention and reinforcement learning. Different from previous works using combined optimization algorithms to get PDAs, our proposed deep neural architecture uses sequence-to-sequence model to learn construct PDAs. Numerical results are given to demonstrate that the proposed method can effectively implement coded caching meanwhile reducing the computational complexity.","","","10.1109/LWC.2018.2873334","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478380","Coded caching;placement delivery array;deep learning;neural attention","Handheld computers;Computational modeling;Computer architecture;Servers;Bipartite graph;Training;Dictionaries","computational complexity;learning (artificial intelligence);neural nets;optimisation","attention-based sequence-to-sequence model;deep neural network;coded caching scheme;computational complexity;reinforcement learning;placement delivery array;PDA;optimization algorithm","","1","15","","","","","IEEE","IEEE Journals"
"Weakly Supervised Adversarial Domain Adaptation for Semantic Segmentation in Urban Scenes","Q. Wang; J. Gao; X. Li","School of Computer Science and the Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; School of Computer Science and the Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; School of Computer Science and the Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Image Processing","","2019","28","9","4376","4386","Semantic segmentation, a pixel-level vision task, is rapidly developed by using convolutional neural networks (CNNs). Training CNNs requires a large amount of labeled data, but manually annotating data is difficult. For emancipating manpower, in recent years, some synthetic datasets are released. However, they are still different from real scenes, which causes that training a model on the synthetic data (source domain) cannot achieve a good performance on real urban scenes (target domain). In this paper, we propose a weakly supervised adversarial domain adaptation to improve the segmentation performance from synthetic data to real scenes, which consists of three deep neural networks. A detection and segmentation (DS) model focuses on detecting objects and predicting segmentation map; a pixel-level domain classifier (PDC) tries to distinguish the image features from which domains; and an object-level domain classifier (ODC) discriminates the objects from which domains and predicts object classes. PDC and ODC are treated as the discriminators, and DS is considered as the generator. By the adversarial learning, DS is supposed to learn domain-invariant features. In experiments, our proposed method yields the new record of mIoU metric in the same problem.","","","10.1109/TIP.2019.2910667","National Natural Science Foundation of China; Natural Science Foundation of Shaanxi Province; Project of Special Zone for National Defense Science and Technology Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693661","Semantic segmentation;domain adaptation;adversarial learning;weakly supervision","Image segmentation;Semantics;Training;Task analysis;Feature extraction;Adaptation models;Streaming media","computer vision;convolutional neural nets;feature extraction;image classification;image segmentation;learning (artificial intelligence);object detection;town and country planning","segmentation performance;synthetic data;deep neural networks;pixel-level domain classifier;object-level domain classifier;domain-invariant features;weakly supervised adversarial domain adaptation;semantic segmentation;urban scenes;pixel-level vision task;convolutional neural networks;synthetic datasets;source domain;target domain;segmentation map;CNN training;adversarial learning","","3","42","","","","","IEEE","IEEE Journals"
"Pixelwise Deep Sequence Learning for Moving Object Detection","Y. Chen; J. Wang; B. Zhu; M. Tang; H. Lu","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","9","2567","2579","Moving object detection is an essential, well-studied but still open problem in computer vision and plays a fundamental role in many applications. Traditional approaches usually reconstruct background images with hand-crafted visual features, such as color, texture, and edge. Due to lack of prior knowledge or semantic information, it is difficult to deal with complicated and rapid changing scenes. To exploit the temporal structure of the pixel-level semantic information, in this paper, we propose an end-to-end deep sequence learning architecture for moving object detection. First, the video sequences are input into a deep convolutional encoder-decoder network for extracting pixel-wise semantic features. Then, to exploit the temporal context, we propose a novel attention long short-term memory (Attention ConvLSTM) to model pixelwise changes over time. A spatial transformer network and a conditional random field layer are finally appended to reduce the sensitivity to camera motion and smooth the foreground boundaries. A multi-task loss is proposed to jointly optimization for frame-based classification and temporal prediction in an end-to-end network. Experimental results on CDnet 2014 and LASIESTA show 12.15% and 16.71% improvement to the state of the art, respectively.","","","10.1109/TCSVT.2017.2770319","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8097419","Moving object detection;background modeling;moving object proposal;convolutional neural networks","Semantics;Feature extraction;Object detection;Cameras;Image color analysis;Lighting","computer vision;convolutional neural nets;feature extraction;Gaussian processes;image classification;image motion analysis;image segmentation;image sequences;learning (artificial intelligence);object detection;recurrent neural nets;video signal processing","temporal structure;pixel-level semantic information;end-to-end deep sequence learning architecture;object detection;video sequences;deep convolutional encoder-decoder network;pixel-wise semantic features;temporal context;pixelwise changes;spatial transformer network;end-to-end network;computer vision;hand-crafted visual features;Attention ConvLSTM;attention long short-term memory;multitask loss;frame-based classification","","10","51","","","","","IEEE","IEEE Journals"
"Plant Phenotyping by Deep-Learning-Based Planner for Multi-Robots","C. Wu; R. Zeng; J. Pan; C. C. L. Wang; Y. Liu","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science, The University of Hong Kong, Hong Kong; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Technology, Beijing National Research Center of Information Science and Technology, Tsinghua University, Beijing, China","IEEE Robotics and Automation Letters","","2019","4","4","3113","3120","Manual plant phenotyping is slow, error prone, and labor intensive. In this letter, we present an automated robotic system for fast, precise, and noninvasive measurements using a new deep-learning-based next-best view planning pipeline. Specifically, we first use a deep neural network to estimate a set of candidate voxels for the next scanning. Next, we cast rays from these voxels to determine the optimal viewpoints. We empirically evaluate our method in simulations and real-world robotic experiments with up to three robotic arms to demonstrate its efficiency and effectiveness. One advantage of our new pipeline is that it can be easily extended to a multi-robot system where multiple robots move simultaneously according to the planned motions. Our system significantly outperforms the single robot in flexibility and planning time. High-throughput phenotyping can be made practically.","","","10.1109/LRA.2019.2924125","National Natural Science Foundation of China; Royal Society-Newton Advanced Fellowship; MOE-Key Laboratory of Pervasive Computing; CUHK Direct Research Grant; Science and Technology Department of Jiangsu Province, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8743431","Agricultural automation;multi-robot systems;computer vision for automation","Planning;Robot sensing systems;Manipulators;Multi-robot systems;Three-dimensional displays","agriculture;industrial manipulators;learning (artificial intelligence);mobile robots;multi-robot systems;neural nets;path planning;planning (artificial intelligence);service robots","precise measurements;high-throughput phenotyping;planning time;single robot;planned motions;multiple robots;multirobot system;robotic arms;real-world robotic experiments;optimal viewpoints;candidate voxels;deep neural network;view planning pipeline;noninvasive measurements;fast measurements;automated robotic system;manual plant phenotyping;multirobots;deep-learning-based planner","","","38","Traditional","","","","IEEE","IEEE Journals"
"Pyramid-Structured Depth MAP Super-Resolution Based on Deep Dense-Residual Network","L. Huang; J. Zhang; Y. Zuo; Q. Wu","Fuzhou University, Fuzhou, China; Fuzhou University, Fuzhou, China; Jiangxi University of Fiance and Economics, Jiangxi, China; Faculty of Engineering and Information Technology, University of Technology Sydney, Sydney, NSW, Australia","IEEE Signal Processing Letters","","2019","26","12","1723","1727","Although deep convolutional neural networks (DCNN) show significant improvement for single depth map (SD) super-resolution (SR) over the traditional counterparts, most SDSR DCNNs do not reuse the hierarchical features for depth map SR resulting in blurred high-resolution (HR) depth maps. They always stack convolutional layers to make network deeper and wider. In addition, most SDSR networks generate HR depth maps at a single level, which is not suitable for large up-sampling factors. To solve these problems, we present pyramid-structured depth map super-resolution based on deep dense-residual network. Specially, our networks are made up of dense residual blocks that use densely connected layers and residual learning to model the mapping between high-frequency residuals and low-resolution (LR) depth map. Furthermore, based on the pyramid structure, our network can progressively generate depth maps of various levels by taking advantages of features from different levels. The proposed network adopts a deep supervision scheme to reduce the difficulty of model training and further improve the performance. The proposed method is evaluated on Middlebury datasets which shows improved performance compared with 6 state-of-the-art methods.","","","10.1109/LSP.2019.2944646","National Natural Science Foundation of China; Major Science and Technology Projects in Fujian, China; Natural Science Foundation of Jiangxi; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8853318","Depth map super-resolution;residual learning;dense connection;deep convolutional neural networks","Training;Convolution;Interpolation;Feature extraction;Computational modeling","convolutional neural nets;image reconstruction;image representation;image resolution;learning (artificial intelligence);stereo image processing","pyramid-structured depth map super-resolution;deep dense-residual network;deep convolutional neural networks;single depth map super-resolution;depth map SR;blurred high-resolution depth maps;SDSR networks;dense residual blocks;low-resolution depth map;Middlebury dataset","","","24","Traditional","","","","IEEE","IEEE Journals"
"Driver Drowsiness Detection Using Condition-Adaptive Representation Learning Framework","J. Yu; S. Park; S. Lee; M. Jeon","School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea; School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea; Department of Information Communication Engineering, Mokwon University, Daejeon, South Korea; School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea","IEEE Transactions on Intelligent Transportation Systems","","2019","20","11","4206","4218","We propose a condition-adaptive representation learning framework for driver drowsiness detection based on a 3D-deep convolutional neural network. The proposed framework consists of four models: spatio-temporal representation learning, scene condition understanding, feature fusion, and drowsiness detection. Spatio-temporal representation learning extracts features that can describe motions and appearances in video simultaneously. Scene condition understanding classifies the scene conditions related to various conditions about the drivers and driving situations, such as statuses of wearing glasses, illumination condition of driving, and motion of facial elements, such as head, eye, and mouth. Feature fusion generates a condition-adaptive representation using two features extracted from the above models. The drowsiness detection model recognizes driver drowsiness status using the condition-adaptive representation. The condition-adaptive representation learning framework can extract more discriminative features focusing on each scene condition than the general representation so that the drowsiness detection method can provide more accurate results for the various driving situations. The proposed framework is evaluated with the NTHU drowsy driver detection video dataset. The experimental results show that our framework outperforms the existing drowsiness detection methods based on visual analysis.","","","10.1109/TITS.2018.2883823","Institute for Information and communications Technology Promotion; National Research Foundation of Korea; Ministry of Environment; Ministry of Health and Welfare; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8580568","Representation learning;adaptive learning;convolutional neural network;driver drowsiness detection","Feature extraction;Visualization;Vehicle crash testing;Sensors;Adaptation models;Automobiles","convolutional neural nets;driver information systems;face recognition;feature extraction;learning (artificial intelligence);object detection;video signal processing","condition-adaptive representation learning framework;3D-deep convolutional neural network;feature fusion;spatio-temporal representation learning;driver drowsiness status;NTHU drowsy driver detection video dataset;driver drowsiness detection method;feature extraction","","4","58","","","","","IEEE","IEEE Journals"
"SeqViews2SeqLabels: Learning 3D Global Features via Aggregating Sequential Views by RNN With Attention","Z. Han; M. Shang; Z. Liu; C. Vong; Y. Liu; M. Zwicker; J. Han; C. L. P. Chen","School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Aeronautics, Northwestern Polytechnical University, Xi’an, China; Department of Computer and Information Science, University of Macau, Macau, China; School of Software, Tsinghua University, Beijing, China; University of Maryland at College Park, College Park, MD, USA; School of Automation, Northwestern Polytechnical University, Xi’an, China; Department of Computer and Information Science, University of Macau, Macau, China","IEEE Transactions on Image Processing","","2019","28","2","658","672","Learning 3D global features by aggregating multiple views has been introduced as a successful strategy for 3D shape analysis. In recent deep learning models with end-to-end training, pooling is a widely adopted procedure for view aggregation. However, pooling merely retains the max or mean value over all views, which disregards the content information of almost all views and also the spatial information among the views. To resolve these issues, we propose Sequential Views To Sequential Labels (SeqViews2SeqLabels) as a novel deep learning model with an encoder-decoder structure based on recurrent neural networks (RNNs) with attention. SeqViews2SeqLabels consists of two connected parts, an encoder-RNN followed by a decoder-RNN, that aim to learn the global features by aggregating sequential views and then performing shape classification from the learned global features, respectively. Specifically, the encoder-RNN learns the global features by simultaneously encoding the spatial and content information of sequential views, which captures the semantics of the view sequence. With the proposed prediction of sequential labels, the decoder-RNN performs more accurate classification using the learned global features by predicting sequential labels step by step. Learning to predict sequential labels provides more and finer discriminative information among shape classes to learn, which alleviates the overfitting problem inherent in training using a limited number of 3D shapes. Moreover, we introduce an attention mechanism to further improve the discriminative ability of SeqViews2SeqLabels. This mechanism increases the weight of views that are distinctive to each shape class, and it dramatically reduces the effect of selecting the first view position. Shape classification and retrieval results under three large-scale benchmarks verify that SeqViews2SeqLabels learns more discriminative global features by more effectively aggregating sequential views than state-of-the-art methods.","","","10.1109/TIP.2018.2868426","National Key R&D Program of China; National Natural Science Foundation of China; Swiss National Science Foundation; NWPU Basic Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453813","3D feature learning;sequential views;sequential labels;view aggregation;RNN;attention","Three-dimensional displays;Shape;Machine learning;Solid modeling;Training;Semantics;Recurrent neural networks","feature extraction;image classification;learning (artificial intelligence);recurrent neural nets","SeqViews2SeqLabels;multiple views;3D shape analysis;encoder-RNN;shape classification;discriminative global features;deep learning models;sequential views aggregation;content information;spatial information;sequential views to sequential labels;decoder-RNN","","6","54","","","","","IEEE","IEEE Journals"
"Exploiting Negative Evidence for Deep Latent Structured Models","T. Durand; N. Thome; M. Cord","Sorbonne Universités, Paris, France; CEDRIC-Conservatoire National des Arts et Métiers, Paris, France; Sorbonne Universités, Paris, France","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","2","337","351","The abundance of image-level labels and the lack of large scale detailed annotations (e.g. bounding boxes, segmentation masks) promotes the development of weakly supervised learning (WSL) models. In this work, we propose a novel framework for WSL of deep convolutional neural networks dedicated to learn localized features from global image-level annotations. The core of the approach is a new latent structured output model equipped with a pooling function which explicitly models negative evidence, e.g. a cow detector should strongly penalize the prediction of the bedroom class. We show that our model can be trained end-to-end for different visual recognition tasks: multi-class and multi-label classification, and also structured average precision (AP) ranking. Extensive experiments highlight the relevance of the proposed method: our model outperforms state-of-the art results on six datasets. We also show that our framework can be used to improve the performance of state-of-the-art deep models for large scale image classification on ImageNet. Finally, we evaluate our model for weakly supervised tasks: in particular, a direct adaptation for weakly supervised segmentation provides a very competitive model.","","","10.1109/TPAMI.2017.2788435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8242666","Weakly supervised learning;convolutional networks;structured outputs;image classification;ranking;localization","Computational modeling;Predictive models;Training;Image segmentation;Semantics;Analytical models","feedforward neural nets;image classification;image representation;image segmentation;learning (artificial intelligence);pattern classification","deep latent structured models;image-level labels;scale detailed annotations;bounding boxes;segmentation masks;weakly supervised learning models;WSL;deep convolutional neural networks;localized features;global image-level annotations;latent structured output model;pooling function;models negative evidence;cow detector;bedroom class;multilabel classification;average precision ranking;state-of-the-art deep models;scale image classification;weakly supervised tasks;weakly supervised segmentation;visual recognition tasks","","","79","","","","","IEEE","IEEE Journals"
"Evaluate the Malignancy of Pulmonary Nodules Using the 3-D Deep Leaky Noisy-OR Network","F. Liao; M. Liang; Z. Li; X. Hu; S. Song","School of Medicine, Tsinghua University, Beijing, China; Institute for Artificial Intelligence, Tsinghua University, Beijing, China; School of Medicine, Tsinghua University, Beijing, China; Institute for Artificial Intelligence, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","11","3484","3495","Automatic diagnosing lung cancer from computed tomography scans involves two steps: detect all suspicious lesions (pulmonary nodules) and evaluate the whole-lung/pulmonary malignancy. Currently, there are many studies about the first step, but few about the second step. Since the existence of nodule does not definitely indicate cancer, and the morphology of nodule has a complicated relationship with cancer, the diagnosis of lung cancer demands careful investigations on every suspicious nodule and integration of information of all nodules. We propose a 3-D deep neural network to solve this problem. The model consists of two modules. The first one is a 3-D region proposal network for nodule detection, which outputs all suspicious nodules for a subject. The second one selects the top five nodules based on the detection confidence, evaluates their cancer probabilities, and combines them with a leaky noisy-OR gate to obtain the probability of lung cancer for the subject. The two modules share the same backbone network, a modified U-net. The overfitting caused by the shortage of the training data is alleviated by training the two modules alternately. The proposed model won the first place in the Data Science Bowl 2017 competition.","","","10.1109/TNNLS.2019.2892409","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642524","3-D convolutional neural network (CNN);deep learning;nodule malignancy evaluation;noisy-OR model;pulmonary nodule detection","Cancer;Solid modeling;Noise measurement;Lung;Proposals;Task analysis;Object detection","cancer;computerised tomography;lung;medical image processing;neural nets;patient diagnosis;probability","pulmonary nodules;3-D deep leaky noisy;automatic diagnosing lung cancer;computed tomography scans;suspicious lesions;suspicious nodule;3-D deep neural network;3-D region proposal network;nodule detection;suspicious nodules;detection confidence;cancer probabilities;backbone network","","2","42","","","","","IEEE","IEEE Journals"
"CoinNet: Copy Initialization Network for Multispectral Imagery Semantic Segmentation","B. Pan; Z. Shi; X. Xu; T. Shi; N. Zhang; X. Zhu","Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Shanghai Aerospace Electronic Technology Institute, Shanghai, China; Shanghai Aerospace Electronic Technology Institute, Shanghai, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","5","816","820","Remote sensing imagery semantic segmentation refers to assigning a label to every pixel. Recently, deep convolutional neural networks (CNNs)-based methods have presented an impressive performance in this task. Due to the lack of sufficient labeled remote sensing images, researchers usually utilized transfer learning (TL) strategies to fine tune networks which were pretrained in huge RGB-scene data sets. Unfortunately, this manner may not work if the target images are multispectral/hyperspectral. The basic assumption of TL is that the low-level features extracted by the former layers are similar in most data sets, hence users only require to train the parameters in the last layers that are specific to different tasks. However, if one should use a pretrained deep model in RGB data for multispectral /hyperspectral imagery semantic segmentation, the structure of the input layer has to be adjusted. In this case, the first convolutional layer has to be trained using the multispectral /hyperspectral data sets which are much smaller. Apparently, the feature representation ability of the first convolutional layer will decrease and it may further harm the following layers. In this letter, we propose a new deep learning model, COpy INitialization Network (CoinNet), for multispectral imagery semantic segmentation. The major advantage of CoinNet is that it can make full use of the initial parameters in the pretrained network's first convolutional layer. Comparison experiments on a challenging multispectral data set have demonstrated the effectiveness of the proposed improvement. The demo and a trained network will be published in our homepage.","","","10.1109/LGRS.2018.2880756","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Shanghai Association for Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8558115","CoinNet;deep convolutional network;semantic segmentation;transfer learning (TL)","Semantics;Image segmentation;Task analysis;Hyperspectral imaging;Testing","feature extraction;geophysical image processing;image colour analysis;image representation;image segmentation;learning (artificial intelligence);neural nets;remote sensing","pretrained deep model;RGB data;input layer;deep learning model;CoinNet;multispectral imagery semantic segmentation;pretrained network;remote sensing imagery semantic segmentation;transfer learning strategies;fine tune networks;target images;hyperspectral data set;multispectral data set;deep convolutional neural networks;remote sensing images;low-level features extraction;RGB-scene data sets;copy initialization network;hyperspectral imagery semantic segmentation","","3","17","","","","","IEEE","IEEE Journals"
"Structured behaviour prediction of on-road vehicles via deep forest","L. Mou; S. Mao; H. Xie; Y. Chen","Beijing University of Technology, People's Republic of China; Xidian University, People's Republic of China; Beijing University of Technology, People's Republic of China; Beijing University of Technology, People's Republic of China","Electronics Letters","","2019","55","8","452","455","Vision-based vehicle behaviour analysis has drawn increasing research efforts as an interesting and challenging issue in recent years. Although a variety of approaches have been taken to characterise on-road behaviour, there still lacks a general model for interpreting the behaviour of vehicles on the road. In this Letter, the authors propose a new method that effectively predicts the vehicle behaviour based on structured deep forest modelling. Inspired by structured learning, the structure information of vehicle behaviour is extracted from the detected vehicle, and then the corresponding structured label is constructed. Especially, the structured label visually expresses the vehicle behaviour as contrast to the discrete numerical label. With the structured label, a structured deep forest model is proposed to predict the vehicle behaviour. Experimental results illustrate that the proposed method successfully obtains the implication of semantic interpretation of vehicle behaviour by the predicted structured labels, and meanwhile it achieves comparable performance with traditional methods.","","","10.1049/el.2019.0472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704686","","","computer vision;learning (artificial intelligence);traffic engineering computing;automobiles","vision-based vehicle behaviour analysis;structured deep forest modelling;structured behaviour prediction;on-road vehicles;structured learning;structured label","","","15","","","","","IET","IET Journals"
"A Graph-Based Semisupervised Deep Learning Model for PolSAR Image Classification","H. Bi; J. Sun; Z. Xu","Institute for Information and System Sciences and the National Engineering Laboratory for Big Data Algorithm and Analysis Technology, Xi’an Jiaotong University, Xi’an, China; Institute for Information and System Sciences and the National Engineering Laboratory for Big Data Algorithm and Analysis Technology, Xi’an Jiaotong University, Xi’an, China; Institute for Information and System Sciences and the National Engineering Laboratory for Big Data Algorithm and Analysis Technology, Xi’an Jiaotong University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","4","2116","2132","Aiming at improving the classification accuracy with limited numbers of labeled pixels in polarimetric synthetic aperture radar (PolSAR) image classification task, this paper presents a graph-based semisupervised deep learning model for PolSAR image classification. It models the PolSAR image as an undirected graph, where the nodes correspond to the labeled and unlabeled pixels, and the weighted edges represent similarities between the pixels. Upon the graph, we design an energy function incorporating a semisupervision term, a convolutional neural network (CNN) term, and a pairwise smoothness term. The employed CNN extracts abstract and data-driven polarimetric features and outputs class label predictions to the graph model. The semisupervision term enforces the category label constraints on the human-labeled pixels. The pairwise smoothness term encourages class label smoothness and the alignment of class label boundaries with the image edges. Starting from an initialized class label map generated based on K-Wishart distribution hypothesis or superpixel segmentation of PauliRGB images, we iteratively and alternately optimize the defined energy function until it converges. We conducted experiments on two real benchmark PolSAR images, and extensive experiments demonstrated that our approach achieved the state-of-the-art results for PolSAR image classification.","","","10.1109/TGRS.2018.2871504","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8486693","Convolutional neural network (CNN);graph model;polarimetric synthetic aperture radar (PolSAR) image classification;semisupervised method","Feature extraction;Machine learning;Task analysis;Image edge detection;Image segmentation;Neural networks;Prediction algorithms","convolutional neural nets;graph theory;image classification;image segmentation;radar imaging;radar polarimetry;supervised learning;synthetic aperture radar","deep learning model;PolSAR image classification;polarimetric synthetic aperture radar image classification task;benchmark PolSAR images;PauliRGB images;initialized class label map;image edges;class label boundaries;class label smoothness;human-labeled pixels;graph model;data-driven polarimetric features;pairwise smoothness term;convolutional neural network term;semisupervision term;unlabeled pixels;undirected graph","","7","59","","","","","IEEE","IEEE Journals"
"Synchrophasor Recovery and Prediction: A Graph-Based Deep Learning Approach","J. J. Q. Yu; D. J. Hill; V. O. K. Li; Y. Hou","Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong; Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong; Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong","IEEE Internet of Things Journal","","2019","6","5","7348","7359","Data integrity of power system states is critical to modern power grid operation and control due to communication latency, state measurements are not immediately available at the control center, rendering slow responses of time-sensitive applications. In this paper, a new graph-based deep learning approach is proposed to recover and predict the states ahead of time utilizing the power network topology and existing measurements. A graph-convolutional recurrent adversarial network is devised to process available information and extract graphical and temporal data correlations. This approach overcomes drawbacks of the existing synchrophasor recovery and prediction implementation to improve the overall system performance. Additionally, the approach offers an adaptive data processing method to handle power grids of various sizes. Case studies demonstrate the outstanding recovery and prediction accuracy of the proposed approach, and investigations are conducted to illustrate its robustness against bad communication conditions, measurement noise, and system topology changes.","","","10.1109/JIOT.2019.2899395","Research Grants Council, University Grants Committee; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642404","Communication latency;deep learning;internet of Things;prediction system;state estimation;wide-area measurement system","Power systems;Transmission line measurements;Internet of Things;Network topology;Phasor measurement units;Neural networks","convolutional neural nets;data analysis;graph theory;phasor measurement;power engineering computing;power generation control;power grids;recurrent neural nets","graph-convolutional recurrent adversarial network;graphical data correlations;temporal data correlations;prediction implementation;adaptive data processing method;prediction accuracy;graph-based deep learning approach;data integrity;power system states;modern power grid operation;state measurements;control center;power network topology;synchrophasor recovery;communication latency;slow responses;measurement noise;system topology changes","","","42","","","","","IEEE","IEEE Journals"
"A CNN With Multiscale Convolution and Diversified Metric for Hyperspectral Image Classification","Z. Gong; P. Zhong; Y. Yu; W. Hu; S. Li","National Key Laboratory of Science and Technology on ATR, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, National University of Defense Technology, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","6","3599","3618","Recently, researchers have shown the powerful ability of deep methods with multilayers to extract high-level features and to obtain better performance for hyperspectral image classification. However, a common problem of traditional deep models is that the learned deep models might be suboptimal because of the limited number of training samples, especially for the image with large intraclass variance and low interclass variance. In this paper, novel convolutional neural networks (CNNs) with multiscale convolution (MS-CNNs) are proposed to address this problem by extracting deep multiscale features from the hyperspectral image. Moreover, deep metrics usually accompany with MS-CNNs to improve the representational ability for the hyperspectral image. However, the usual metric learning would make the metric parameters in the learned model tend to behave similarly. This similarity leads to obvious model's redundancy and, thus, shows negative effects on the description ability of the deep metrics. Traditionally, determinantal point process (DPP) priors, which encourage the learned factors to repulse from one another, can be imposed over these factors to diversify them. Taking advantage of both the MS-CNNs and DPP-based diversity-promoting deep metrics, this paper develops a CNN with multiscale convolution and diversified metric to obtain discriminative features for hyperspectral image classification. Experiments are conducted over four real-world hyperspectral image data sets to show the effectiveness and applicability of the proposed method. Experimental results show that our method is better than original deep models and can produce comparable or even better classification performance in different hyperspectral image data sets with respect to spectral and spectral-spatial features.","","","10.1109/TGRS.2018.2886022","National Natural Science Foundation of China; Foundation for the Author of National Excellent Doctoral Dissertation of the People's Republic of China; Program for New Century Excellent Talents in University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598950","Convolutional neural network (CNN);deep metric learning;determinantal point process (DPP);hyperspectral image;image classification;multiscale features","Hyperspectral imaging;Measurement;Feature extraction;Training;Convolution;Task analysis","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);remote sensing","multiscale convolution;hyperspectral image classification;learned deep models;low interclass variance;MS-CNNs;deep multiscale features;DPP-based diversity-promoting deep metrics;high-level features extraction;convolutional neural networks;hyperspectral image data sets;determinantal point process;spectral-spatial features","","7","46","","","","","IEEE","IEEE Journals"
"Action Parsing-Driven Video Summarization Based on Reinforcement Learning","J. Lei; Q. Luan; X. Song; X. Liu; D. Tao; M. Song","Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Information Science and Engineering, Yunnan University, Kunming, China; Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","7","2126","2137","How to manage, store, and index large numbers of videos is an urgent problem to be solved. Although there are many video summarization models achieving good results, models based on low-level features cannot summarize important semantic information and models based on semantic analysis need related text descriptions that do not exist for most videos. As a consequence, the mining semantic information contained in the video itself is a more feasible way. In this paper, we propose an action parsing-driven video summarization model based on reinforcement learning. The model is mainly divided into two parts, video cut by action parsing and video summarization based on reinforcement learning. In the first part, a sequential multiple instance learning model is trained with weakly annotated data to solve the problem of full annotation's time consuming and weak annotation's ambiguity. In the second part, we design a deep recurrent neural network-based video summarization model that selects the most distinguishable frames comparing with other actions. Meanwhile, the quality of the extracted key frames could be evaluated by the categorization accuracy. Experiments and comparison with state-of-the-art methods demonstrate the advantage of the proposed approach.","","","10.1109/TCSVT.2018.2860797","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; National Key Research and Development Program; Key Research and Development Program of Zhejiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8421619","Weakly-annotated;action parsing;sequential multiple instance learning;video cut;video summarization","Semantics;Learning (artificial intelligence);Training;Analytical models;Labeling;Event detection;Indexes","data mining;feature extraction;learning (artificial intelligence);recurrent neural nets;text analysis;video signal processing","reinforcement learning;video summarization models;low-level features;semantic analysis need;action parsing-driven video summarization model;video cut;sequential multiple instance learning model;weakly annotated data;deep recurrent neural network-based video summarization model;semantic information mining","","","45","","","","","IEEE","IEEE Journals"
"Re-Ranking High-Dimensional Deep Local Representation for NIR-VIS Face Recognition","C. Peng; N. Wang; J. Li; X. Gao","State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Image Processing","","2019","28","9","4553","4565","Heterogeneous face recognition refers to matching facial images captured from different sensors or sources, which has wide applications in public security and law enforcement. Because of the great differences in sensing and creating procedure, there is a huge feature gap between heterogeneous facial images. The existing methods merely focus on comparing the probe image with the gallery in feature space, while the true target may not appear at the first rank due to the appearance variations caused by different sensing patterns. In order to exploit valuable information from the initial ranking result, this paper proposes to re-rank high-dimensional deep local representation for matching near-infrared (NIR) and visual (VIS) facial images, i.e., NIR-VIS face recognition. A high-dimensional deep local representation is first constructed by extracting and concatenating deep features on local facial patches via a convolutional neural network (CNN). The initial NIR-VIS recognition ranking results can be obtained by comparing the compressed deep features. We then propose a novel and efficient locally linear re-ranking (LLRe-Rank) technique to refine the initial ranking results, which can explore valuable information from the initial ranking result. The proposed re-ranking method does not require any human interaction or data annotation and can be served as an unsupervised postprocessing technique. The experimental results on the most challenging Oulu-CASIA NIR-VIS database and CASIA NIR-VIS 2.0 database demonstrate the effectiveness of our method.","","","10.1109/TIP.2019.2912360","National Natural Science Foundation of China; National Key Research and Development Program of China; Key Industrial Innovation Chain in Industrial Domain; National High-Level Talents Special Support Program of China; Young Elite Scientists Sponsorship Program by CAST; Young Talent fund of University Association for Science and Technology in Shaanxi, China; CCF-Tencent Open Fund; China 111 Project; China Postdoctoral Science Foundation; Xidian University-Intellifusion Joint Innovation Laboratory of Artificial Intelligence; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8699090","Heterogeneous face recognition;NIR-VIS face matching;re-ranking","Face recognition;Face;Databases;Sensors;Image generation;Deep learning;Lighting","convolutional neural nets;face recognition;feature extraction;image matching;image representation","NIR-VIS face recognition;heterogeneous face recognition;public security;law enforcement;heterogeneous facial images;local facial patches;compressed deep features;re-ranking method;sensing patterns;high-dimensional deep local representation re-ranking;deep feature extraction;facial image matching;near-infrared facial images;visual facial images;high-dimensional deep local representation;convolutional neural network;LLRe-Rank technique;locally linear re-ranking technique;unsupervised post processing technique;Oulu-CASIA NIR-VIS database;CASIA NIR-VIS 2.0 database","","","54","","","","","IEEE","IEEE Journals"
"Joint Transceiver Optimization for Wireless Communication PHY Using Neural Network","B. Zhu; J. Wang; L. He; J. Song","Electronic Engineering Department, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Journal on Selected Areas in Communications","","2019","37","6","1364","1373","Deep learning has a wide application in the area of natural language processing and image processing due to its strong ability of generalization. In this paper, we propose a novel neural network structure for jointly optimizing the transmitter and receiver in communication physical layer under fading channels. We build up a convolutional autoencoder to simultaneously conduct the role of modulation, equalization, and demodulation. The proposed system is able to design different mapping scheme from input bit sequences of arbitrary length to constellation symbols according to different channel environments. The simulation results show that the performance of neural network-based system is superior to traditional modulation and equalization methods in terms of time complexity and bit error rate under fading channels. The proposed system can also be combined with other coding techniques to further improve the performance. Furthermore, the proposed system network is more robust to channel variation than traditional communication methods.","","","10.1109/JSAC.2019.2904361","National Key R&D Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664650","Deep learning;modulation;equalization;autoencoder;frequency selective fading","Receivers;Convolutional codes;Communication systems;Deep learning;Neurons;Modulation;Transmitters","computational complexity;error statistics;fading channels;learning (artificial intelligence);natural language processing;neural nets;radio transceivers","wireless communication PHY;deep learning;natural language processing;image processing;communication physical layer;fading channels;convolutional autoencoder;arbitrary length;constellation symbols;neural network-based system;traditional modulation;time complexity;bit error rate;system network;traditional communication methods;transceiver optimization;neural network structure;equalization methods;mapping scheme;channel environments;coding techniques","","3","36","","","","","IEEE","IEEE Journals"
"Scene Context-Driven Vehicle Detection in High-Resolution Aerial Images","C. Tao; L. Mi; Y. Li; J. Qi; Y. Xiao; J. Zhang","School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","10","7339","7351","As the spatial resolution of remote sensing images is improving gradually, it is feasible to realize “scene-object” collaborative image interpretation. Unfortunately, this idea is not fully utilized in vehicle detection from high-resolution aerial images, and most of the existing methods may be promoted by considering the variability of vehicle spatial distribution in different image scenes and treating vehicle detection tasks scene-specific. With this motivation, a scene context-driven vehicle detection method is proposed in this paper. At first, we perform scene classification using the deep learning method and, then, detect vehicles in roads and parking lots separately through different vehicle detectors. Afterward, we further optimize the detection results using different postprocessing rules according to different scene types. Experimental results show that the proposed approach outperforms the state-of-the-art algorithms in terms of higher detection accuracy rate and lower false alarm rate.","","","10.1109/TGRS.2019.2912985","National Natural Science Foundation of China; Natural Science Foundation of Hunan Province; Young Elite Scientists Sponsorship Program by Hunan Province of China; Land and Resource Department Scientific Research Program of Hunan Province, China; Hunan Science and Technology Department Innovation Platform Open Fund Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8728170","Aerial image;deep learning;scene context-driven;vehicle detection","Feature extraction;Vehicle detection;Roads;Remote sensing;Support vector machines;Deep learning;Automobiles","feature extraction;geophysical image processing;image classification;image resolution;image segmentation;learning (artificial intelligence);object detection;remote sensing;traffic engineering computing","treating vehicle detection tasks scene-specific;scene context-driven vehicle detection method;scene classification;deep learning method;different scene types;higher detection accuracy rate;high-resolution aerial images;spatial resolution;remote sensing images;scene-object;collaborative image interpretation;vehicle spatial distribution;vehicle detectors;image scenes","","1","32","","","","","IEEE","IEEE Journals"
"A Generative Discriminatory Classified Network for Change Detection in Multispectral Imagery","M. Gong; Y. Yang; T. Zhan; X. Niu; S. Li","School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Science and Engineering, Southeast University, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","1","321","333","Multispectral image change detection based on deep learning generally needs a large amount of training data. However, it is difficult and expensive to mark a large amount of labeled data. To deal with this problem, we propose a generative discriminatory classified network (GDCN) for multispectral image change detection, in which labeled data, unlabeled data, and new fake data generated by generative adversarial networks are used. The GDCN consists of a discriminatory classified network (DCN) and a generator. The DCN divides the input data into changed class, unchanged class, and extra class, i.e., fake class. The generator recovers the real data from input noises to provide additional training samples so as to boost the performance of the DCN. Finally, the bitemporal multispectral images are input to the DCN to get the final change map. Experimental results on the real multispectral imagery datasets demonstrate that the proposed GDCN trained by unlabeled data and a small amount of labeled data can achieve competitive performance compared with existing methods.","","","10.1109/JSTARS.2018.2887108","National Natural Science Foundation of China; National Key Research and Development Program of China; Key Research and Development Program of Shaanxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600384","Change detection;deep learning;generative adversarial networks (GANs);multispectral imagery","Gallium nitride;Training;Remote sensing;Feature extraction;Deep learning;Generators;Task analysis","image classification;learning (artificial intelligence);remote sensing","GDCN;DCN;bitemporal multispectral images;multispectral imagery datasets;generative discriminatory classified network;multispectral image change detection;generative adversarial networks;discriminatory classified network;deep learning","","","44","","","","","IEEE","IEEE Journals"
"Conv-TasNet: Surpassing Ideal Time–Frequency Magnitude Masking for Speech Separation","Y. Luo; N. Mesgarani","Department of Electrical Engineering, Columbia University, New York, NY, USA; Department of Electrical Engineering, Columbia University, New York, NY, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","8","1256","1266","Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time-frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network consisting of stacked one-dimensional dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time-frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time-frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications. This study, therefore, represents a major step toward the realization of speech separation systems for real-world speech processing technologies.","","","10.1109/TASLP.2019.2915167","National Institute of Health; National Institute on Deafness and Other Communication Disorders; National Science Foundation; Pew Charitable Trusts; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8707065","Source separation;single-channel;time-domain;deep learning;real-time","Convolution;Time-frequency analysis;Time-domain analysis;Speech processing;Spectrogram;Decoding;Deep learning","learning (artificial intelligence);linear codes;signal representation;source separation;speech coding;time-frequency analysis","time-frequency representation;mixed signal;fully convolutional time-domain audio separation network;end-to-end time-domain speech separation;linear encoder;speech waveform;speaker separation;modified encoder representations;temporal convolutional network;stacked one-dimensional dilated convolutional blocks;speech signal;Conv-TasNet system;three-speaker mixtures;two-speaker speech separation;real-time speech separation applications;speech separation systems;ideal time-frequency magnitude masking;single-channel speaker-independent speech separation methods;spectrograms;deep learning framework;speech waveform representation;weighting functions;linear decoder;objective distortion measures;subjective quality assessment;real-world speech processing technologies","","3","65","","","","","IEEE","IEEE Journals"
"AAIoT: Accelerating Artificial Intelligence in IoT Systems","J. Zhou; Y. Wang; K. Ota; M. Dong","Embedded System and Internet of Things Technical Lab, School of Computer Science and Technology, Soochow University, Suzhou, China; Embedded System and Internet of Things Technical Lab, School of Computer Science and Technology, Soochow University, Suzhou, China; Department of Information and Electronic Engineering, Muroran Institute of Technology, Muroran, Japan; Department of Information and Electronic Engineering, Muroran Institute of Technology, Muroran, Japan","IEEE Wireless Communications Letters","","2019","8","3","825","828","Existing deep learning systems in the Internet of Things (IoT) environments lack the ability of assigning compute tasks reasonably which leads to resources wasting. In this letter, we propose an AAIoT, a method to allocate the inference computation of each network layer to each device in multi-layer IoT system. To our best knowledge, this is the first attempt to solve this problem. We design a dynamic programming algorithm to minimize the response time when weighing the cost of computation and transmission. Simulation results show that our approach makes significant improvements in system response time.","","","10.1109/LWC.2019.2894703","Japan Society for the Promotion of Science; Kokusai Denshin Denwa IDO (KDDI) Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624458","Internet of Things;deep learning;resource scheduling","Time factors;Internet of Things;Resource management;Neural networks;Computational modeling;Deep learning;Bandwidth","dynamic programming;inference mechanisms;Internet of Things;learning (artificial intelligence)","compute tasks;AAIoT;inference computation;network layer;multilayer IoT system;dynamic programming algorithm;system response time;artificial intelligence;deep learning systems;Internet of Things environments","","4","12","","","","","IEEE","IEEE Journals"
"Deep Learning Empowered Task Offloading for Mobile Edge Computing in Urban Informatics","K. Zhang; Y. Zhu; S. Leng; Y. He; S. Maharjan; Y. Zhang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Wolfson School of Mechanical, Electrical, and Manufacturing Engineering, Loughborough University, Loughborough, U.K.; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; College of Electronics and Information Engineering, Shenzhen University, Shenzhen, China; Simula Metropolitan Center for Digital Engineering, Oslo, Norway; Simula Metropolitan Center for Digital Engineering, Oslo, Norway","IEEE Internet of Things Journal","","2019","6","5","7635","7647","Led by industrialization of smart cities, numerous interconnected mobile devices, and novel applications have emerged in the urban environment, providing great opportunities to realize industrial automation. In this context, autonomous driving is an attractive issue, which leverages large amounts of sensory information for smart navigation while posing intensive computation demands on resource constrained vehicles. Mobile edge computing (MEC) is a potential solution to alleviate the heavy burden on the devices. However, varying states of multiple edge servers as well as a variety of vehicular offloading modes make efficient task offloading a challenge. To cope with this challenge, we adopt a deep Q-learning approach for designing optimal offloading schemes, jointly considering selection of target server and determination of data transmission mode. Furthermore, we propose an efficient redundant offloading algorithm to improve task offloading reliability in the case of vehicular data transmission failure. We evaluate the proposed schemes based on real traffic data. Results indicate that our offloading schemes have great advantages in optimizing system utilities and improving offloading reliability.","","","10.1109/JIOT.2019.2903191","Fundamental Research Funds for the Central Universities, China; European Unions Horizon 2020 Research and Innovation Programme under the Marie Skłodowska-Curie; Xi’an Key Laboratory of Mobile Edge Computing and Security; Key Research and Development Plan of Shaanxi province; Science and Technology Program of Sichuan Province; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660505","Offloading;Q-learning;reliability;vehicular edge computing","Task analysis;Servers;Edge computing;Reliability;Computational modeling;Cellular networks;Internet of Things","cloud computing;data communication;internetworking;learning (artificial intelligence);mobile computing;optimisation;telecommunication traffic;vehicular ad hoc networks","mobile edge computing;urban informatics;smart cities;industrial automation;autonomous driving;sensory information;smart navigation;resource constrained vehicles;multiple edge servers;vehicular offloading modes;deep Q-learning approach;optimal offloading schemes;data transmission mode;task offloading reliability;vehicular data transmission failure;redundant offloading algorithm","","4","30","","","","","IEEE","IEEE Journals"
"Half a Percent of Labels is Enough: Efficient Animal Detection in UAV Imagery Using Deep CNNs and Active Learning","B. Kellenberger; D. Marcos; S. Lobry; D. Tuia","Laboratory of GeoInformation Science and Remote Sensing, Wageningen University, Wageningen, The Netherlands; Laboratory of GeoInformation Science and Remote Sensing, Wageningen University, Wageningen, The Netherlands; Laboratory of GeoInformation Science and Remote Sensing, Wageningen University, Wageningen, The Netherlands; Laboratory of GeoInformation Science and Remote Sensing, Wageningen University, Wageningen, The Netherlands","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","12","9524","9533","We present an Active Learning (AL) strategy for reusing a deep Convolutional Neural Network (CNN)-based object detector on a new data set. This is of particular interest for wildlife conservation: given a set of images acquired with an Unmanned Aerial Vehicle (UAV) and manually labeled ground truth, our goal is to train an animal detector that can be reused for repeated acquisitions, e.g., in follow-up years. Domain shifts between data sets typically prevent such a direct model application. We thus propose to bridge this gap using AL and introduce a new criterion called Transfer Sampling (TS). TS uses Optimal Transport (OT) to find corresponding regions between the source and the target data sets in the space of CNN activations. The CNN scores in the source data set are used to rank the samples according to their likelihood of being animals, and this ranking is transferred to the target data set. Unlike conventional AL criteria that exploit model uncertainty, TS focuses on very confident samples, thus allowing quick retrieval of true positives in the target data set, where positives are typically extremely rare and difficult to find by visual inspection. We extend TS with a new window cropping strategy that further accelerates sample retrieval. Our experiments show that with both strategies combined, less than half a percent of oracle-provided labels are enough to find almost 80% of the animals in challenging sets of UAV images, beating all baselines by a margin.","","","10.1109/TGRS.2019.2927393","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807383","Active Learning (AL);animal census;convolutional neural networks;domain adaptation;object detection;Optimal Transport (OT);unmanned aerial vehicles","Animals;Detectors;Data models;Unmanned aerial vehicles;Adaptation models;Biological system modeling;Predictive models","autonomous aerial vehicles;convolutional neural nets;ecology;environmental science computing;learning (artificial intelligence);mobile robots;object detection","animal detection;UAV imagery;active learning strategy;wildlife conservation;animal detector;CNN activations;window cropping strategy;deep convolutional neural network-based object detector;transfer sampling;sample retrieval;optimal transport","","1","31","IEEE","","","","IEEE","IEEE Journals"
"Intelligent Labeling Based on Fisher Information for Medical Image Segmentation Using Deep Learning","J. Sourati; A. Gholipour; J. G. Dy; X. Tomas-Fernandez; S. Kurugol; S. K. Warfield","Radiology Department, Computational Radiology Laboratory, Boston Children’s Hospital, Boston, MA, USA; Radiology Department, Computational Radiology Laboratory, Boston Children’s Hospital, Boston, MA, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Radiology Department, Computational Radiology Laboratory, Boston Children’s Hospital, Boston, MA, USA; Radiology Department, Computational Radiology Laboratory, Boston Children’s Hospital, Boston, MA, USA; Radiology Department, Computational Radiology Laboratory, Boston Children’s Hospital, Boston, MA, USA","IEEE Transactions on Medical Imaging","","2019","38","11","2642","2653","Deep convolutional neural networks (CNN) have recently achieved superior performance at the task of medical image segmentation compared to classic models. However, training a generalizable CNN requires a large amount of training data, which is difficult, expensive, and time-consuming to obtain in medical settings. Active Learning (AL) algorithms can facilitate training CNN models by proposing a small number of the most informative data samples to be annotated to achieve a rapid increase in performance. We proposed a new active learning method based on Fisher information (FI) for CNNs for the first time. Using efficient backpropagation methods for computing gradients together with a novel low-dimensional approximation of FI enabled us to compute FI for CNNs with a large number of parameters. We evaluated the proposed method for brain extraction with a patch-wise segmentation CNN model in two different learning scenarios: universal active learning and active semi-automatic segmentation. In both scenarios, an initial model was obtained using labeled training subjects of a source data set and the goal was to annotate a small subset of new samples to build a model that performs well on the target subject(s). The target data sets included images that differed from the source data by either age group (e.g. newborns with different image contrast) or underlying pathology that was not available in the source data. In comparison to several recently proposed AL methods and brain extraction baselines, the results showed that FI-based AL outperformed the competing methods in improving the performance of the model after labeling a very small portion of target data set (<; 0.25%).","","","10.1109/TMI.2019.2907805","National Institutes of Health; Research Grant from the Boston Children’s Hospital Translational Research Program; National Institutes of Health; McKnight Foundation; National Science Foundation; Crohn's and Colitis Foundation of America; American Gastroenterological Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675438","Convolutional neural network;active learning;Fisher Information;brain extraction;patch-wise segmentation","Image segmentation;Uncertainty;Data models;Biomedical imaging;Computational modeling;Labeling;Brain modeling","approximation theory;backpropagation;brain;convolutional neural nets;feature extraction;image segmentation;medical image processing","intelligent labeling;Fisher information;medical image segmentation;deep Learning;convolutional neural networks;active learning method;backpropagation methods;low-dimensional approximation;patch-wise segmentation CNN model;semiautomatic segmentation;image contrast","","","54","","","","","IEEE","IEEE Journals"
"Scattering Networks for Hybrid Representation Learning","E. Oyallon; S. Zagoruyko; G. Huang; N. Komodakis; S. Lacoste-Julien; M. Blaschko; E. Belilovsky","CentraleSupelec CVN, Gif-sur-Yvette, France; Ecole des Ponts, Champs-sur-Marne, France; University of Montreal, Montreal, Canada; Ecole des Ponts, Champs-sur-Marne, France; University of Montreal, Montreal, Canada; KU Leuven, Leuven, Belgium; University of Montreal, Montreal, Canada","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","9","2208","2221","Scattering networks are a class of designed Convolutional Neural Networks (CNNs) with fixed weights. We argue they can serve as generic representations for modelling images. In particular, by working in scattering space, we achieve competitive results both for supervised and unsupervised learning tasks, while making progress towards constructing more interpretable CNNs. For supervised learning, we demonstrate that the early layers of CNNs do not necessarily need to be learned, and can be replaced with a scattering network instead. Indeed, using hybrid architectures, we achieve the best results with predefined representations to-date, while being competitive with end-to-end learned CNNs. Specifically, even applying a shallow cascade of small-windowed scattering coefficients followed by 1 × 1-convolutions results in AlexNet accuracy on the ILSVRC2012 classification task. Moreover, by combining scattering networks with deep residual networks, we achieve a single-crop top-5 error of 11.4 percent on ILSVRC2012. Also, we show they can yield excellent performance in the small sample regime on CIFAR-10 and STL-10 datasets, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. For unsupervised learning, scattering coefficients can be a competitive representation that permits image recovery. We use this fact to train hybrid GANs to generate images. Finally, we empirically analyze several properties related to stability and reconstruction of images from scattering coefficients.","","","10.1109/TPAMI.2018.2855738","ERC; Internal Funds KU Leuven; DIGITEO; NSERC; Research Foundation - Flanders; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8413168","Scattering transform;wavelets;deep neural networks;invariance","Scattering;Wavelet transforms;Task analysis;Pipelines;Gray-scale;Hybrid power systems","convolutional neural nets;image classification;image representation;learning (artificial intelligence);unsupervised learning","hybrid representation learning;unsupervised learning;supervised learning;scattering network;small-windowed scattering coefficients;ILSVRC2012 classification task;competitive representation;convolutional neural networks;interpretable CNN;image recovery;image generation","","1","60","","","","","IEEE","IEEE Journals"
"Self-Supervised Drivable Area and Road Anomaly Segmentation Using RGB-D Data For Robotic Wheelchairs","H. Wang; Y. Sun; M. Liu","Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","IEEE Robotics and Automation Letters","","2019","4","4","4386","4393","The segmentation of drivable areas and road anomalies are critical capabilities to achieve autonomous navigation for robotic wheelchairs. The recent progress of semantic segmentation using deep learning techniques has presented effective results. However, the acquisition of large-scale datasets with hand-labeled ground truth is time-consuming and labor-intensive, making the deep learning-based methods often hard to implement in practice. We contribute to the solution of this problem for the task of drivable area and road anomaly segmentation by proposing a self-supervised learning approach. We develop a pipeline that can automatically generate segmentation labels for drivable areas and road anomalies. Then, we train RGB-D data-based semantic segmentation neural networks and get predicted labels. Experimental results show that our proposed automatic labeling pipeline achieves an impressive speed-up compared to manual labeling. In addition, our proposed self-supervised approach exhibits more robust and accurate results than the state-of-the-art traditional algorithms as well as the state-of-the-art self-supervised algorithms.","","","10.1109/LRA.2019.2932874","National Natural Science Foundation of China; Hong Kong SAR Government, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8786197","Semantic scene understanding;deep learning in robotics and automation;RGB-D perception","Roads;Wheelchairs;Mobile robots;Image segmentation;Cameras;Pipelines","image colour analysis;image segmentation;learning (artificial intelligence);mobile robots","self-supervised drivable area;road anomaly segmentation;robotic wheelchairs;road anomalies;deep learning techniques;hand-labeled ground truth;deep learning-based methods;self-supervised learning approach;segmentation labels;RGB-D data-based semantic segmentation neural networks;automatic labeling pipeline;manual labeling;state-of-the-art self-supervised algorithms","","","29","Traditional","","","","IEEE","IEEE Journals"
"Deep Mixture of Diverse Experts for Large-Scale Visual Recognition","T. Zhao; Q. Chen; Z. Kuang; J. Yu; W. Zhang; J. Fan","Department of Computer Science, University of North Carolina at Charlotte, NC; Department of Computer Science, University of North Carolina at Charlotte, NC; School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science, Fudan University, Shanghai, China; Department of Computer Science, University of North Carolina at Charlotte, NC","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","5","1072","1087","In this paper, a deep mixture of diverse experts algorithm is developed to achieve more efficient learning of a huge (mixture) network for large-scale visual recognition application. First, a two-layer ontology is constructed to assign large numbers of atomic object classes into a set of task groups according to the similarities of their learning complexities, where certain degrees of inter-group task overlapping are allowed to enable sufficient inter-group message passing. Second, one particular base deep CNNs with $M+1$   M + 1    outputs is learned for each task group to recognize its $M$  M   atomic object classes and identify one special class of “not-in-group”, where the network structure (numbers of layers and units in each layer) of the well-designed deep CNNs (such as AlexNet, VGG, GoogleNet, ResNet) is directly used to configure such base deep CNNs. For enhancing the separability of the atomic object classes in the same task group, two approaches are developed to learn more discriminative base deep CNNs: (a) our deep multi-task learning algorithm that can effectively exploit the inter-class visual similarities; (b) our two-layer network cascade approach that can improve the accuracy rates for the hard object classes at certain degrees while effectively maintaining the high accuracy rates for the easy ones. Finally, all these complementary base deep CNNs with diverse but overlapped outputs are seamlessly combined to generate a mixture network with larger outputs for recognizing tens of thousands of atomic object classes. Our experimental results have demonstrated that our deep mixture of diverse experts algorithm can achieve very competitive results on large-scale visual recognition.","","","10.1109/TPAMI.2018.2828821","Shaanxi Province Innovative Research Team; Changjiang Scholars and Innovative Research Team in University; National Science Foundation; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8344459","Deep mixture of diverse experts;base deep CNNs;mixture network;deep multi-task learning;large-scale visual recognition","Task analysis;Visualization;Training;Complexity theory;Image recognition;Prediction algorithms;Diversity reception","","","","","76","","","","","IEEE","IEEE Journals"
"Discriminative Neural Embedding Learning for Short-Duration Text-Independent Speaker Verification","S. Wang; Z. Huang; Y. Qian; K. Yu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","11","1686","1696","Short duration text-independent speaker verification remains a hot research topic in recent years, and deep neural network based embeddings have shown impressive results in such conditions. Good speaker embeddings require the property of both small intra-class variation and large inter-class difference, which is critical for the ability of discrimination and generalization. Current embedding learning strategies can be grouped into two frameworks: “Cascade embedding learning” with multiple stages and “direct embedding learning” from spectral feature directly. We propose new approaches to achieve more discriminant speaker embeddings. Within the cascade framework, a neural network based deep discriminant analysis (DDA) is proposed to project i-vector to more discriminative embeddings. Within the direct embedding framework, a deep model with more advanced center loss and A-softmax loss is used, the focal loss is also investigated in this framework. Moreover, the traditional i-vector and neural embeddings are finally combined with neural network based DDA to achieve further gain. Main experiments are carried out on a short-duration text-independent speaker verification dataset generated from the SRE corpus. The results show that the newly proposed method is promising for short-duration text-independent speaker verification, and it is consistently better than traditional i-vector and neural embedding baselines. The best embeddings achieve roughly 30% relative EER reduction compared to the i-vector baseline, which could be further enhanced when combined with the i-vector system.","","","10.1109/TASLP.2019.2928128","China NSFC project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759958","Short-duration text-independent speaker verification;center loss;triplet loss;angular softmax;speaker neural embedding","Neural networks;Speech processing;Training;Feature extraction;Optimization;Analytical models;Linear discriminant analysis","Gaussian processes;learning (artificial intelligence);neural nets;speaker recognition","discriminative neural embedding learning;short duration text-independent speaker verification;deep neural network based embeddings;good speaker embeddings;current embedding learning strategies;direct embedding learning;discriminant speaker embeddings;deep discriminant analysis;discriminative embeddings;direct embedding framework;neural embeddings;short-duration text-independent speaker verification dataset;neural embedding baselines;cascade embedding learning","","","56","Traditional","","","","IEEE","IEEE Journals"
"Curriculum Learning for Speech Emotion Recognition From Crowdsourced Labels","R. Lotfian; C. Busso","Erik Jonsson School of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA; Erik Jonsson School of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","4","815","826","This study introduces a method to design a curriculum for machine-learning to maximize the efficiency during the training process of deep neural networks (DNNs) for speech emotion recognition. Previous studies in other machine-learning problems have shown the benefits of training a classifier following a curriculum where samples are gradually presented in increasing level of difficulty. For speech emotion recognition, the challenge is to establish a natural order of difficulty in the training set to create the curriculum. We address this problem by assuming that, ambiguous samples for humans are also ambiguous for computers. Speech samples are often annotated by multiple evaluators to account for differences in emotion perception across individuals. While some sentences with clear emotional content are consistently annotated, sentences with more ambiguous emotional content present important disagreement between individual evaluations. We propose to use the disagreement between evaluators as a measure of difficulty for the classification task. We propose metrics that quantify the inter-evaluation agreement to define the curriculum for regression problems and binary and multi-class classification problems. The experimental results consistently show that relying on a curriculum based on agreement between human judgments leads to statistically significant improvements over baselines trained without a curriculum.","","","10.1109/TASLP.2019.2898816","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8638999","Curriculum learning;speech emotion recognition;inter-evaluator agreement","Training;Speech recognition;Task analysis;Emotion recognition;Speech processing;Machine learning;Computers","emotion recognition;learning (artificial intelligence);neural nets;pattern classification;regression analysis;speech recognition","classification task;multiclass classification;human judgments;regression problems;DNNs;deep neural networks;crowdsourced labels;emotional content;machine learning problems;emotion perception;speech samples;speech emotion recognition;curriculum learning","","2","71","","","","","IEEE","IEEE Journals"
"Revisiting Jump-Diffusion Process for Visual Tracking: A Reinforcement Learning Approach","X. Liu; Q. Xu; T. Chau; Y. Mu; L. Zhu; S. Yan","Department of Computer Science, San Diego State University, San Diego, CA, USA; XreLab Inc., San Diego, CA, USA; Department of Computer Science, San Diego State University, San Diego, CA, USA; Institute of Computer Science and Technology, Peking University, Beijing, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","8","2431","2441","In this paper, we revisit the classical stochastic jump-diffusion process and develop an effective variant for estimating visibility statuses of objects while tracking them in videos. Dealing with partial or full occlusions is a long standing problem in computer vision but largely remains unsolved. In this paper, we cast the above problem as a Markov decision process and develop a policy-based jump-diffusion method to jointly track object locations in videos and estimate their visibility statuses. Our method employs a set of jump dynamics to change visibility statuses of objects and a set of diffusion dynamics to track objects in videos. Different from the traditional jump-diffusion process that stochastically generates dynamics, we utilize deep policy functions to determine the best dynamic for the present state and learn the optimal policies using reinforcement learning methods. Our method is capable of tracking objects with full or partial occlusions in crowded scenes. We evaluate the proposed method over challenging video sequences and compare it to alternative tracking methods. Significant improvements are made particularly for videos with frequent interactions or occlusions.","","","10.1109/TCSVT.2018.2862891","NJUST Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information; Beijing Municipal Science and Technology Commission; Defense Advanced Research Projects Agency; National Science Foundation; ONR; San Diego State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8425080","Machine learning;neural networks;computer vision","Videos;Markov processes;Visualization;Task analysis;Proposals;Learning (artificial intelligence);Computer vision","computer vision;diffusion;learning (artificial intelligence);Markov processes;object tracking;stochastic processes","classical stochastic jump-diffusion process;effective variant;visibility statuses;partial occlusions;full occlusions;long standing problem;computer vision;Markov decision process;policy-based jump-diffusion method;object locations;jump dynamics;diffusion dynamics;traditional jump-diffusion process;deep policy functions;optimal policies;reinforcement learning methods;challenging video sequences;alternative tracking methods;visual tracking;reinforcement learning approach","","","54","","","","","IEEE","IEEE Journals"
"Semi-Supervised SAR ATR via Multi-Discriminator Generative Adversarial Network","C. Zheng; X. Jiang; X. Liu","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Sensors Journal","","2019","19","17","7525","7533","As a supervised deep learning algorithm well-suited for image processing, convolutional neural network (CNN) has shown great potential on synthetic aperture radar (SAR) automatic target recognition (ATR) and achieved superior performance in recent years. However, the training of the deep convolution network depends heavily on sufficient labeled samples while the SAR images are scarce and difficult to obtain, and it is time-consuming to artificially annotate labels for raw images. In this paper, a semi-supervised recognition method combining generative adversarial network (GAN) with CNN is proposed. We generated unlabeled images with GAN and set them as the input of CNN together with original labeled images, so as to implement the effective training and recognition with limited training samples. In order to address the instability training issue caused by the adversarial principal of GAN, a dynamic adjustable multi-discriminator GAN (MGAN) architecture is introduced in the proposed framework. Meanwhile, the label smoothing regularization (LSR) is applied to regularize the semi-supervised recognition model of the CNN. Experiments carried out on the moving and stationary target acquisition and recognition (MSTAR) dataset have indicated that the proposed method possesses the ability to improves the accuracy and robustness of CNN system, especially when the training dataset is limited.","","","10.1109/JSEN.2019.2915379","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8707959","SAR target recognition;dynamic multi-discriminator GAN;semi-supervised learning;label smoothing regularization","Gallium nitride;Generative adversarial networks;Training;Generators;Synthetic aperture radar;Deep learning;Radar polarimetry","convolutional neural nets;image classification;image recognition;radar imaging;radar target recognition;supervised learning;synthetic aperture radar","dynamic adjustable multidiscriminator GAN architecture;label smoothing regularization;stationary target acquisition;CNN system;semisupervised SAR ATR;multidiscriminator generative adversarial network;supervised deep learning algorithm;image processing;convolutional neural network;deep convolution network;SAR images;semisupervised recognition method;unlabeled images;synthetic aperture radar;automatic target recognition;LSR;MGAN","","","35","","","","","IEEE","IEEE Journals"
"Discriminative Feature Learning With Distance Constrained Stacked Sparse Autoencoder for Hyperspectral Target Detection","Y. Shi; J. Lei; Y. Yin; K. Cao; Y. Li; C. Chang","State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; Department of Computer Science and Information Engineering, National Yunlin University of Science and Technology, Douliu, Taiwan","IEEE Geoscience and Remote Sensing Letters","","2019","16","9","1462","1466","Target detection (TD) is one of the major tasks in hyperspectral image (HSI) processing, and its performance is greatly affected by the background. Feature extraction (FE) has been an effective way to mine discriminative information, especially FE based on deep learning, which can learn the intrinsic properties of data to further improve the detection performance. Unlike supervised networks, unsupervised stacked sparse autoencoders (SSAEs) can learn deep and nonlinear features without any labeled data. However, SSAEs usually require a supervised fine-tuned model to obtain better discrimination, which is not feasible for TD, since the prior information is generally insufficient. In this letter, we introduce a distance constraint that is added to the SSAE to form a new distance constrained SSAE (DCSSAE) network. Specifically, the distance constraint maximizes the distinction between the target pixels and other background pixels in the feature space. Then, using the discriminative features learned from the DCSSAE, a simple detector using radial basis function kernel is derived for background suppression. Experiments on two HSIs demonstrate that the deep spectral features learned from the DCSSAE are more distinguishable, and our proposed detector, namely, the DCSSAE detector, outperforms several popular detectors, especially in background suppression.","","","10.1109/LGRS.2019.2901019","National Natural Science Foundation of China; 111 Project; Fundamental Research Funds for the Central Universities; Natural Science Basic Research Plan in Shaanxi Province of China; China Postdoctoral Science Foundation; Yangtse Rive Scholar Bonus Schemes and Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666997","Discriminative feature;distance constraint;hyperspectral target detection (TD);stacked sparse autoencoder (SSAE)","Feature extraction;Iron;Hyperspectral imaging;Detectors;Training;Kernel","feature extraction;geophysical image processing;image classification;image representation;learning (artificial intelligence);object detection;radial basis function networks","hyperspectral target detection;hyperspectral image processing;feature extraction;discriminative information;deep learning;detection performance;supervised networks;sparse autoencoders;deep features;nonlinear features;fine-tuned model;distance constrained stacked sparse autoencoder;discriminative feature learning;DCSSAE detector;deep spectral features;background suppression;discriminative features;feature space;background pixels;target pixels;SSAE network;distance constraint","","","19","","","","","IEEE","IEEE Journals"
"Refinet: A Deep Segmentation Assisted Refinement Network for Salient Object Detection","K. Fu; Q. Zhao; I. Y. Gu","College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden","IEEE Transactions on Multimedia","","2019","21","2","457","469","Compared to conventional saliency detection by handcrafted features, deep convolutional neural networks (CNNs) recently have been successfully applied to saliency detection field with superior performance on locating salient objects. However, due to repeated sub-sampling operations inside CNNs such as pooling and convolution, many CNN-based saliency models fail to maintain fine-grained spatial details and boundary structures of objects. To remedy this issue, this paper proposes a novel end-to-end deep learning-based refinement model named Refinet, which is based on fully convolutional network augmented with segmentation hypotheses. Intermediate saliency maps that are edge-aware are computed from segmentation-based pooling and then feed to a two-tier fully convolutional network for effective fusion and refinement, leading to more precise object details and boundaries. In addition, the resolution of feature maps in the proposed Refinet is carefully designed to guarantee sufficient boundary clarity of the refined saliency output. Compared to widely employed dense conditional random field, Refinet is able to enhance coarse saliency maps generated by existing models with more accurate spatial details, and its effectiveness is demonstrated by experimental results on seven benchmark datasets.","","","10.1109/TMM.2018.2859746","National Science Foundation; Fundamental Research Funds for the Central Universities; National Key Research and Development Program of China; National Key Scientific Instrument and Equipment Development Projects of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419317","Salient object detection;refinement;convolu-tional network;image segmentation","Image segmentation;Object detection;Feature extraction;Saliency detection;Convolution;Machine learning;Task analysis","convolutional neural nets;image segmentation;learning (artificial intelligence);object detection;self-organising feature maps;statistical analysis","Refinet;deep segmentation assisted refinement network;salient object detection;conventional saliency detection;handcrafted features;deep convolutional neural networks;CNNs;saliency detection field;repeated sub-sampling operations;CNN-based saliency models;fine-grained spatial details;end-to-end deep learning-based refinement model;segmentation hypotheses;intermediate saliency maps;segmentation-based pooling;two-tier fully convolutional network;feature maps;coarse saliency maps;dense conditional random field;refined saliency output boundary clarity","","1","75","","","","","IEEE","IEEE Journals"
"A Machine Learning Approach for Classifying Ischemic Stroke Onset Time From Imaging","K. C. Ho; W. Speier; H. Zhang; F. Scalzo; S. El-Saden; C. W. Arnold","Departments of Bioengineering and Radiological Sciences, University of California at Los Angeles, Los Angeles, CA, USA; Computational Integrated Diagnostics Laboratory, University of California at Los Angeles, Los Angeles, CA, USA; Departments of Bioengineering and Radiological Sciences, University of California at Los Angeles, Los Angeles, CA, USA; Departments of Neurology and Computer Science, University of California at Los Angeles, Los Angeles, CA, USA; Computational Integrated Diagnostics Laboratory, University of California at Los Angeles, Los Angeles, CA, USA; Computational Integrated Diagnostics Laboratory, University of California at Los Angeles, Los Angeles, CA, USA","IEEE Transactions on Medical Imaging","","2019","38","7","1666","1676","Current clinical practice relies on clinical history to determine the time since stroke (TSS) onset. Imaging-based determination of acute stroke onset time could provide critical information to clinicians in deciding stroke treatment options, such as thrombolysis. The patients with unknown or unwitnessed TSS are usually excluded from thrombolysis, even if their symptoms began within the therapeutic window. In this paper, we demonstrate a machine learning approach for TSS classification using routinely acquired imaging sequences. We develop imaging features from the magnetic resonance (MR) images and train machine learning models to classify the TSS. We also propose a deep-learning model to extract hidden representations for the MR perfusion-weighted images and demonstrate classification improvement by incorporating these additional deep features. The cross-validation results show that our best classifier achieved an area under the curve of 0.765, with a sensitivity of 0.788 and a negative predictive value of 0.609, outperforming existing methods. We show that the features generated by our deep-learning algorithm correlate with the MR imaging features, and validate the robustness of the model on imaging parameter variations (e.g., year of imaging). This paper advances magnetic resonance imaging analysis one-step-closer to an operational decision support tool for stroke treatment guidance.","","","10.1109/TMI.2019.2901445","National Institute of Neurological Disorders and Stroke; UCLA Radiology Department Exploratory; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8651325","Deep learning;autoencoder;acute ischemic stroke;stroke onset time;MR perfusion imaging","Deep learning;Stroke (medical condition);Feature extraction;Magnetic resonance imaging;Biomedical imaging","","","","","60","","","","","IEEE","IEEE Journals"
"Deep feature based efficient regularised ensemble for engagement recognition","Y. -. Park; G. -. Lee; H. -. Yang","Korea Advanced Institute of Science and Technology, Daejon, Republic of Korea; Korea Advanced Institute of Science and Technology, Daejon, Republic of Korea; Korea Advanced Institute of Science and Technology, Daejon, Republic of Korea","Electronics Letters","","2019","55","24","1281","1283","Over the years, open education in online environments, such as Massive Online Open Courses, has grown rapidly. While the trend is expected to bridge the educational gap among students, the new environment has also created new challenges such as the lack of feedback and difficulties in interaction. The authors propose an automated engagement recognition system to alleviate this problem, driven by the recent developments in computer vision and artificial neural networks. The authors' proposed system extracts deep features from a facial image and employs a combination of multiple regularised shallow networks to recognise engagement. They verified the system in a public data set. The proposed system has faster learning speed and better accuracy than single deep network based approaches do.","","","10.1049/el.2019.2783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8918196","","","computer aided instruction;educational courses;feature extraction;graph theory;learning (artificial intelligence);neural nets","computer vision;artificial neural networks;facial image;multiple regularised shallow networks;single deep network based approaches;open education;online environments;educational gap;feedback;automated engagement recognition system;deep feature extraction;massive online open courses;deep feature based efficient regularised ensemble;public data set","","","12","","","","","IET","IET Journals"
"PRIMAL: Pathfinding via Reinforcement and Imitation Multi-Agent Learning","G. Sartoretti; J. Kerr; Y. Shi; G. Wagner; T. K. S. Kumar; S. Koenig; H. Choset","Robotics Institute at Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute at Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute at Carnegie Mellon University, Pittsburgh, PA, USA; Commonwealth Scientific and Industrial Research Organisation, Pullenvale, QLD, Australia; Computer Science Department, University of Southern California, Los Angeles, CA, USA; Computer Science Department, University of Southern California, Los Angeles, CA, USA; Robotics Institute at Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Robotics and Automation Letters","","2019","4","3","2378","2385","Multi-agent path finding (MAPF) is an essential component of many large-scale, real-world robot deployments, from aerial swarms to warehouse automation. However, despite the community's continued efforts, most state-of-the-art MAPF planners still rely on centralized planning and scale poorly past a few hundred agents. Such planning approaches are maladapted to realworld deployments, where noise and uncertainty often require paths be recomputed online, which is impossible when planning times are in seconds to minutes. We present PRIMAL, a novel framework for MAPF that combines reinforcement and imitation learning to teach fully decentralized policies, where agents reactively plan paths online in a partially observable world while exhibiting implicit coordination. This framework extends our previous work on distributed learning of collaborative policies by introducing demonstrations of an expert MAPF planner during training, as well as careful reward shaping and environment sampling. Once learned, the resulting policy can be copied onto any number of agents and naturally scales to different team sizes and world dimensions. We present results on randomized worlds with up to 1024 agents and compare success rates against state-of-the-art MAPF planners. Finally, we experimentally validate the learned policies in a hybrid simulation of a factory mockup, involving both real world and simulated robots.","","","10.1109/LRA.2019.2903261","National Science Foundation; CMU Manufacturing Futures Initiative; Richard King Mellon Foundation; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661608","Path planning for multiple mobile robots or agents;deep learning in robotics and automation;distributed robot systems;AI-based methods;factory automation","Planning;Robot kinematics;Training;Robot sensing systems;Production facilities;Multi-agent systems","learning (artificial intelligence);multi-agent systems;multi-robot systems;path planning;warehouse automation","PRIMAL;multiagent path finding;essential component;real-world robot deployments;aerial swarms;warehouse automation;community;centralized planning;planning approaches;noise;uncertainty;planning times;imitation learning;fully decentralized policies;partially observable world;distributed learning;collaborative policies;expert MAPF planner;environment sampling;world dimensions;randomized worlds;learned policies;simulated robots;team sizes;reward shaping;reinforcement learning","","3","43","","","","","IEEE","IEEE Journals"
"Deep Visual MPC-Policy Learning for Navigation","N. Hirose; F. Xia; R. Martín-Martín; A. Sadeghian; S. Savarese","Department of Computer Science, Stanford University, Stanford, CA, USA; Department of Computer Science, Stanford University, Stanford, CA, USA; Department of Computer Science, Stanford University, Stanford, CA, USA; Department of Computer Science, Stanford University, Stanford, CA, USA; Department of Computer Science, Stanford University, Stanford, CA, USA","IEEE Robotics and Automation Letters","","2019","4","4","3184","3191","Humans can routinely follow a trajectory defined by a list of images/landmarks. However, traditional robot navigation methods require accurate mapping of the environment, localization, and planning. Moreover, these methods are sensitive to subtle changes in the environment. In this letter, we propose PoliNet, a deep visual model predictive control-policy learning method that can perform visual navigation while avoiding collisions with unseen objects on the navigation path. PoliNet takes in as input a visual trajectory and 360° images from robot's current view and outputs velocity commands fora planning horizon of N steps that optimally balance between trajectory following and obstacle avoidance. PoliNet is trained using a differentiable neural image predictive model and a traversability estimation model in an model predictive control setup, with minimal human supervision. PoliNet can be applied to visual trajectory in new scenes without retraining. We show experimentally that the robot can follow a visual trajectory even if it does not start from the exact same position and in the presence of previously unseen obstacles. We validated our algorithm with tests both in a realistic simulation environment and in the real world outperforming state-of-the-art baselines under similar conditions in success rate, coverage rate of the trajectory, and with lower computational load. We also show that we can generate visual trajectory in simulation and execute the corresponding path in the real environment.","","","10.1109/LRA.2019.2925731","TOYOTA Central R&D Labs., Inc.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8750823","Visual-Based Navigation;Omnidirectional Vision;Collision Avoidance","Visualization;Navigation;Trajectory;Robots;Predictive models;Cameras;Collision avoidance","collision avoidance;image sensors;learning (artificial intelligence);mobile robots;path planning;predictive control;robot vision","deep visual MPC-policy;PoliNet;deep visual model predictive control-policy learning method;visual navigation;navigation path;visual trajectory;differentiable neural image predictive model;traversability estimation model;model predictive control setup;robot navigation methods","","","44","Traditional","","","","IEEE","IEEE Journals"
"Deep Online Video Stabilization With Multi-Grid Warping Transformation Learning","M. Wang; G. Yang; J. Lin; S. Zhang; A. Shamir; S. Lu; S. Hu","Beihang University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Interdisciplinary Center Herzliya, Herzliya, Israel; Nankai University, Tianjin, China; Beihang University, Beijing, China","IEEE Transactions on Image Processing","","2019","28","5","2283","2292","Video stabilization techniques are essential for most hand-held captured videos due to high-frequency shakes. Several 2D-, 2.5D-, and 3D-based stabilization techniques have been presented previously, but to the best of our knowledge, no solutions based on deep neural networks had been proposed to date. The main reason for this omission is shortage in training data as well as the challenge of modeling the problem using neural networks. In this paper, we present a video stabilization technique using a convolutional neural network. Previous works usually propose an off-line algorithm that smoothes a holistic camera path based on feature matching. Instead, we focus on low-latency, real-time camera path smoothing that does not explicitly represent the camera path and does not use future frames. Our neural network model, called StabNet, learns a set of mesh-grid transformations progressively for each input frame from the previous set of stabilized camera frames and creates stable corresponding latent camera paths implicitly. To train the network, we collect a dataset of synchronized steady and unsteady video pairs via a specially designed hand-held hardware. Experimental results show that our proposed online method performs comparatively to the traditional off-line video stabilization methods without using future frames while running about 10 times faster. More importantly, our proposed StabNet is able to handle low-quality videos, such as night-scene videos, watermarked videos, blurry videos, and noisy videos, where the existing methods fail in feature extraction or matching.","","","10.1109/TIP.2018.2884280","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Research Grant of the Beijing Higher Institution Engineering Research Center; Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology; Israel Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554287","Video stabilization;video processing","Cameras;Streaming media;Training;Synchronization;Neural networks;Training data;Real-time systems","cameras;convolutional neural nets;feature extraction;image matching;image motion analysis;image sequences;learning (artificial intelligence);smoothing methods;transforms;video signal processing","deep online video stabilization;multigrid warping transformation learning;deep neural networks;convolutional neural network;mesh-grid transformations;camera path smoothing;StabNet;feature extraction;feature matching","","1","30","","","","","IEEE","IEEE Journals"
"Deep Objective Quality Assessment Driven Single Image Super-Resolution","B. Yan; B. Bare; C. Ma; K. Li; W. Tan","School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China","IEEE Transactions on Multimedia","","2019","21","11","2957","2971","Single-image super-resolution (SISR) is a classic problem in the image processing community, which aims at generating a high-resolution image from a low-resolution one. In recent years, deep learning based SISR methods emerged and achieved a performance leap than previous methods. However, because the evaluation metrics of SISR methods is peak signal-to-noise ratio (PSNR), previous methods usually choose L2-norm as the loss function. This leads to a significant improvement in the final PSNR value but little improvement in perceptual quality. In this paper, in order to achieve better results in both perceptual quality and PSNR values, we propose an objective quality assessment driven SISR method. First, we propose a novel full-reference image quality assessment approach for SISR and employ it as a loss function, namely super-resolution image quality assessment (SR-IQA) loss. Then, we combine SR-IQA loss with L2-norm to guide our proposed SISR method to achieve better results. Besides that, our proposed SISR method consists of several proposed highway units. Furthermore, in order to verify the generalization ability of our new kind of loss function, we integrate SR-IQA loss to generative adversarial networks based SR method and achieve better perceptual quality. Experimental results prove that our proposed SISR method achieves better performance than other methods both qualitatively and quantitatively in most of the cases.","","","10.1109/TMM.2019.2914883","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8705363","Single image super-resolution;full-reference quality assessment;generative adversarial networks;image enhancement","Feature extraction;Image resolution;Image quality;Quality assessment;Signal resolution;Deep learning;Measurement","image resolution;learning (artificial intelligence);neural nets","objective quality assessment driven single image super-resolution;image processing community;loss function;full-reference image quality assessment approach;SR-IQA loss;peak signal-to-noise ratio;PSNR;super-resolution image quality assessment loss;generative adversarial networks based SR method;deep learning based SISR methods","","1","90","Traditional","","","","IEEE","IEEE Journals"
"Unsupervised Two-Path Neural Network for Cell Event Detection and Classification Using Spatiotemporal Patterns","H. T. H. Phan; A. Kumar; D. Feng; M. Fulham; J. Kim","School of Information Technologies (J12), The University of Sydney, Sydney, NSW, Australia; School of Information Technologies (J12), The University of Sydney, Sydney, NSW, Australia; School of Information Technologies (J12), The University of Sydney, Sydney, NSW, Australia; Sydney Medical School, The University of Sydney, Sydney, NSW, Australia; School of Information Technologies (J12), The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Medical Imaging","","2019","38","6","1477","1487","Automatic event detection in cell videos is essential for monitoring cell populations in biomedicine. Deep learning methods have advantages over traditional approaches for cell event detection due to their ability to capture more discriminative features of cellular processes. Supervised deep learning methods, however, are inherently limited due to the scarcity of annotated data. Unsupervised deep learning methods have shown promise in general (non-cell) videos because they can learn the visual appearance and motion of regularly occurring events. Cell videos, however, can have rapid, irregular changes in cell appearance and motion, such as during cell division and death, which are often the events of most interest. We propose a novel unsupervised two-path input neural network architecture to capture these irregular events with three key elements: 1) a visual encoding path to capture regular spatiotemporal patterns of observed objects with convolutional long short-term memory units; 2) an event detection path to extract information related to irregular events with max-pooling layers; and 3) integration of the hidden states of the two paths to provide a comprehensive representation of the video that is used to simultaneously locate and classify cell events. We evaluated our network in detecting cell division in densely packed stem cells in phase-contrast microscopy videos. Our unsupervised method achieved higher or comparable accuracy to standard and state-of-the-art supervised methods.","","","10.1109/TMI.2018.2885572","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8567937","Molecular and cellular imaging;machine learning;cell;unsupervised neural network;pattern recognition and classification","Computer architecture;Microprocessors;Videos;Event detection;Neural networks;Feature extraction","biology computing;cellular biophysics;feature extraction;image classification;learning (artificial intelligence);neural nets;neurophysiology;unsupervised learning","cell division;densely packed stem cells;unsupervised method;two-path neural network;cell event detection;automatic event detection;cell videos;cell populations;supervised deep learning methods;unsupervised deep learning methods;visual appearance;cell appearance;visual encoding path;regular spatiotemporal patterns;event detection path;unsupervised two-path input neural network architecture","","","59","","","","","IEEE","IEEE Journals"
"Deep Active Localization","S. K. Gottipati; K. Seo; D. Bhatt; V. Mai; K. Murthy; L. Paull","Mila and DIRO, Universite de Montreal, Montreal, QC, Canada; SAIT, Samsung Electronics Co. Ltd. Suwon, Gyeonggi-do, South Korea; Mila and DIRO, Universite de Montreal, Montreal, QC, Canada; Mila and DIRO, Universite de Montreal, Montreal, QC, Canada; Mila and DIRO, Universite de Montreal, Montreal, QC, Canada; Mila and DIRO, Universite de Montreal, Montreal, QC, Canada","IEEE Robotics and Automation Letters","","2019","4","4","4394","4401","Active localization consists of generating robot actions that allow it to maximally disambiguate its pose within a reference map. Traditional approaches use an information-theoretic criterion for action selection and hand-crafted perceptual models. In this work we propose an end-to-end differentiable method for learning to take informative actions that is trainable entirely in simulation and then transferable to real robot hardware with zero refinement. The system is composed of two learned modules: a convolutional neural network for perception, and a deep reinforcement learned planning module. We leverage a multi-scale approach in the perceptual model since the accuracy needed to take actions using reinforcement learning is much less than the accuracy needed for robot control. We demonstrate that the resulting system outperforms traditional approach for either perception or planning. We also demonstrate our approach's robustness to different map configurations and other nuisance parameters through the use of domain randomization in training. The code has been released: https://github.com/montrealrobotics/dal and is compatible with the OpenAI gym framework, as well as the Gazebo simulator.","","","10.1109/LRA.2019.2932575","Samsung Advanced Institute of Technology; Natural Sciences and Engineering Research Council of Canada; Fonds de recherche nature et technologie Québec; Canada CIFAR AI Chairs Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784238","Localization;deep learning in robotics and automation;motion and path planning;AI-based methods","Robot localization;Robot sensing systems;Deep learning;Motion planning;Path planning","","","","","35","Traditional","","","","IEEE","IEEE Journals"
"A Deep Neural Network-Based Permanent Magnet Localization for Tongue Tracking","N. Sebkhi; N. Sahadat; S. Hersek; A. Bhavsar; S. Siahpoushan; M. Ghoovanloo; O. T. Inan","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Sensors Journal","","2019","19","20","9324","9331","Permanent magnet localization (PML) is a nascent method of wireless motion tracking that can estimate the 5D state (3D position and 2D orientation) of a cylindrical magnet from its magnetic field. PML is well suited for applications where a wireless tracking is crucial, and in particular for tongue motion which opens up many new interesting applications. To allow its usage outside of a research lab, our tongue tracking system relies on the PML to avoid impeding tongue's natural motion due to its wireless tracking method and ensure safe use inside the mouth. Our tracking module is embedded in a headset to be portable and simple to use while being affordable by relying on the mass-produced components. The classical implementation of PML has many shortcomings that limit its practicality for realworld applications because it is computationally intensive due to its iterative algorithm, subject to local minimum convergence issues, and sensitive to its initial state and calibration parameters. Additionally, its physical model is an approximation that is only valid in limited tracking conditions. In this paper, we investigated the potential of deep learning to create a PML model for tongue tracking by training a feedforward neural network (3 layers, 100 neurons per layer) on a dataset composed of ~1.7 million states spanning a volume of 10 × 10 × 10 cm3. Our PML was validated on 337000 states in a 4 × 6 × 7 cm3 volume and tested on 100000 samples emulating a more natural tongue motion with curves and twists. A fully automated 5D positional stage was engineered by our team to collect these large datasets of the 5D magnet states. Our PML prediction, limited to the magnet's 3D positions in this paper, reaches positional errors of 1.4 mm (median) and 1.8 mm (Q3).","","","10.1109/JSEN.2019.2923585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8737998","Algorithm;articulograph;biomedical device;deep learning;machine learning;magnetometers;motion tracking;neural network;nonlinear optimization;permanent magnet localization;speech;tongue tracking;wearable","Magnetometers;Tongue;Magnetic recording;Magnetic resonance imaging;Tracking;Sensors;Two dimensional displays","feedforward neural nets;interactive systems;iterative methods;learning (artificial intelligence);magnetic sensors;permanent magnets","fully automated 5D positional stage;5D magnet states;PML prediction;deep neural network-based permanent magnet localization;nascent method;wireless motion tracking;cylindrical magnet;magnetic field;tongue tracking system;wireless tracking method;tracking module;realworld applications;local minimum convergence issues;calibration parameters;tracking conditions;PML model;feedforward neural network;natural tongue motion","","","30","","","","","IEEE","IEEE Journals"
"Drug Repositioning for Schizophrenia and Depression/Anxiety Disorders: A Machine Learning Approach Leveraging Expression Data","K. Zhao; H. So","School of Biomedical Sciences, The Chinese University of Hong Kong, Sha Tin, Hong Kong; School of Biomedical Sciences, The Chinese University of Hong Kong, Sha Tin, Hong Kong","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","1304","1315","Development of new medications is a lengthy and costly process, and drug repositioning might help to shorten the development cycle. We present a machine learning (ML) workflow to drug discovery or repositioning by predicting indication for a particular disease based on drug expression profiles, with a focus on applications in psychiatry. Drugs that are not originally indicated for the disease but with high predicted probabilities serve as candidates for repurposing. This approach is widely applicable to any chemicals or drugs with expression profiles measured, even if drug targets are unknown. It is also highly flexible as virtually any supervised learning algorithms can be used. We employed the ML approach to identify repositioning opportunities for schizophrenia as well as depression and anxiety disorders. We applied various state-of-the-art ML approaches, including deep neural networks (DNNs), support vector machines (SVMs), elastic net regression, random forest, and gradient boosted trees. The predictive performance of the five approaches in cross validation did not differ substantially, with SVM slightly outperforming the others. However, other methods also reveal literature-supported repositioning candidates of different mechanisms of actions. As a further validation, we showed that the repositioning hits are enriched for psychiatric medications considered in clinical trials. We also examined the correlation between predicted probabilities of treatment potential and the number of related research articles, and found significant correlations for all methods, especially DNN. Finally, we propose that ML may provide a new avenue to exploring drug mechanisms via examining the variable importance of gene features.","","","10.1109/JBHI.2018.2856535","Lo Kwee Seong Biomedical Research Fund; Chinese University of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411482","Anxiety disorder;drug repositioning;machine learning;major depressive disorder;schizophrenia","Drugs;Support vector machines;Diseases;Machine learning;Chemicals;Supervised learning;Correlation","diseases;drugs;gradient methods;medical computing;medical disorders;neural nets;patient treatment;psychology;random forests;regression analysis;support vector machines","drug repositioning;schizophrenia;drug discovery;drug expression profiles;support vector machines;depression;machine learning;supervised learning algorithms;anxiety disorders;psychiatry;deep neural networks;elastic net regression;random forest;gradient boosted trees;psychiatric medications","","6","105","","","","","IEEE","IEEE Journals"
"Deep Multiview Heartwave Authentication","C. L. P. Lim; W. L. Woo; S. S. Dlay; D. Wu; B. Gao","School of Engineering, Newcastle University, Newcastle upon Tyne, U.K.; School of Engineering, Newcastle University, Newcastle upon Tyne, U.K.; School of Engineering, Newcastle University, Newcastle upon Tyne, U.K.; School of Engineering, Newcastle University, Newcastle upon Tyne, U.K.; School of Engineering, Newcastle University, Newcastle upon Tyne, U.K.","IEEE Transactions on Industrial Informatics","","2019","15","2","777","786","This paper presents a heartwave based authentication method that utilizes an ensemble of deep belief networks (DBNs) under different parameters to increase the reliability of feature extraction. The multiview outputs are further embedded into a single view before inputting into a stacked DBN for classification. The result of the proposed novel architecture achieved a classification rate of 98.3% with 30% training data. Importantly, it is able to perform user classification using heartwave signals acquired under intense physical exercise where heart rate ranges from 50 bpm to as high as 180 bpm. Under extreme physical duress, the heartwave from an individual experiences extreme morphological variations that render conventional classification approaches nonapplicable.","","","10.1109/TII.2018.2874477","Nanyang Polytechnic, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8485298","Authentication;deep belief network (DBN);deep learning;discrete wavelet transformation (DWT);heartwave;multiview spectrum","Authentication;Feature extraction;Discrete wavelet transforms;Reliability;Training;Senior citizens;Informatics","belief networks;cardiology;learning (artificial intelligence);medical signal processing;patient monitoring;pattern classification","heartwave authentication;heartwave based authentication method;deep belief networks;feature extraction;multiview outputs;stacked DBN;user classification;heartwave signals;intense physical exercise;heart rate;extreme physical duress;individual experiences extreme morphological variations;render conventional classification approaches","","","29","","","","","IEEE","IEEE Journals"
"Semi-Supervised Gait Generation With Two Microfluidic Soft Sensors","D. Kim; M. Kim; J. Kwon; Y. Park; S. Jo","School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Mechanical Engineering and Aerospace Engineering and Institue of Advanced Machines and Design, Seoul National University, Seoul, South Korea; Department of Mechanical Engineering and Aerospace Engineering and Institue of Advanced Machines and Design, Seoul National University, Seoul, South Korea; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Robotics and Automation Letters","","2019","4","3","2501","2507","Nowadays, the use of deep learning for the calibration of soft wearable sensors has addressed the typical drawbacks of the microfluidic soft sensors, such as hysteresis and nonlinearity. However, previous studies have not yet resolved some of the design constraints such as the sensors are needed to be attached to the joints and many sensors are needed to track the human motion. Moreover, the previous methods also demand an excessive amount of data for sensor calibration which make the system impractical. In this letter, we present a gait motion generating method using only two microfluidic sensors. We select appropriate sensor positions with consideration of the deformation patterns of the lower-limb skins and mutual interference with soft actuators. Moreover, a semi-supervised deep learning model is proposed to reduce the size of calibration data. We evaluated the performance of the proposed model with various walking speeds. From the experiment, the proposed method showed a higher performance with smaller calibration dataset comparing to the other methods that are based on the supervised deep learning.","","","10.1109/LRA.2019.2907431","National Research Foundation funded by the Korean Government (MSIT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8678818","Wearable robots;soft robot applications;deep learning in robotics and automation","Calibration;Sensor phenomena and characterization;Wearable sensors;Capacitive sensors;Data acquisition;Strain","calibration;gait analysis;image motion analysis;learning (artificial intelligence);microactuators;microfluidics;microsensors;skin","semisupervised gait generation;microfluidic soft sensors;soft actuators;deep learning model;supervised deep learning;sensor positions;soft wearable sensor calibration;design constraints;human motion tracking;gait motion generating method;lower-limb skin deformation patterns;mutual interference;calibration data size reduction","","","29","","","","","IEEE","IEEE Journals"
"RF-PUF: Enhancing IoT Security Through Authentication of Wireless Nodes Using In-Situ Machine Learning","B. Chatterjee; D. Das; S. Maity; S. Sen","School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA","IEEE Internet of Things Journal","","2019","6","1","388","398","Traditional authentication in radio-frequency (RF) systems enable secure data communication within a network through techniques such as digital signatures and hash-based message authentication codes (HMAC), which suffer from key-recovery attacks. State-of-the-art Internet of Things networks such as Nest also use open authentication (OAuth 2.0) protocols that are vulnerable to cross-site-recovery forgery (CSRF), which shows that these techniques may not prevent an adversary from copying or modeling the secret IDs or encryption keys using invasive, side channel, learning or software attacks. Physical unclonable functions (PUFs), on the other hand, can exploit manufacturing process variations to uniquely identify silicon chips which makes a PUF-based system extremely robust and secure at low cost, as it is practically impossible to replicate the same silicon characteristics across dies. Taking inspiration from human communication, which utilizes inherent variations in the voice signatures to identify a certain speaker, we present RF-PUF: a deep neural network-based framework that allows real-time authentication of wireless nodes, using the effects of inherent process variation on RF properties of the wireless transmitters (Tx), detected through in-situ machine learning at the receiver (Rx) end. The proposed method utilizes the already-existing asymmetric RF communication framework and does not require any additional circuitry for PUF generation or feature extraction. The burden of device identification is completely shifted to the gateway Rx, similar to the operation of a human listener's brain. Simulation results involving the process variations in a standard 65-nm technology node, and features such as local oscillator offset and I-Q imbalance detected with a neural network having 50 neurons in the hidden layer indicate that the framework can distinguish up to 4800 Tx(s) with an accuracy of 99.9% [≈99% for 10000 Tx(s)] under varying channel conditions, and without the need for traditional preambles. The proposed scheme can be used as a stand-alone security feature, or as a part of traditional multifactor authentication.","","","10.1109/JIOT.2018.2849324","National Science Foundation; Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8390918","Artificial neural networks (ANNs);authentication;deep neural network;device signatures;Internet-of-Things (IoT);machine learning (ML);physical unclonable function (PUF);radio frequency (RF);security","Receivers;Radio frequency;Authentication;Radio transmitters;Machine learning;Object recognition","authorisation;cryptographic protocols;cryptography;digital signatures;Internet of Things;learning (artificial intelligence);neural nets;radio receivers","RF-PUF;deep neural network-based framework;real-time authentication;wireless nodes;inherent process variation;RF properties;wireless transmitters;in-situ machine;receiver end;asymmetric RF communication framework;feature extraction;human listener;stand-alone security feature;traditional multifactor authentication;radio-frequency systems;secure data communication;digital signatures;message authentication codes;open authentication;OAuth 2;cross-site-recovery forgery;encryption keys;software attacks;physical unclonable functions;PUFs;silicon chips;PUF-based system;silicon characteristics;human communication;voice signatures;IoT security;key-recovery attacks;Internet of Things networks","","4","47","","","","","IEEE","IEEE Journals"
"Weakly Supervised Salient Object Detection With Spatiotemporal Cascade Neural Networks","Y. Tang; W. Zou; Z. Jin; Y. Chen; Y. Hua; X. Li","Shenzhen Key Laboratory of Advanced Telecommunication and Information Processing, College of Information Engineering, Shenzhen University, Shenzhen, China; Shenzhen Key Laboratory of Advanced Telecommunication and Information Processing, College of Information Engineering, Shenzhen University, Shenzhen, China; Shenzhen Key Laboratory of Advanced Telecommunication and Information Processing, College of Information Engineering, Shenzhen University, Shenzhen, China; Shenzhen Key Laboratory of Advanced Telecommunication and Information Processing, College of Information Engineering, Shenzhen University, Shenzhen, China; EEECS/ECIT, Queen’s University Belfast, Belfast, U.K.; Shenzhen Key Laboratory of Advanced Telecommunication and Information Processing, College of Information Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","7","1973","1984","Recently, deep learning techniques have substantially boosted the performance of salient object detection in still images. However, the salient object detection in videos by using traditional handcrafted features or deep learning features is not fully investigated, probably due to the lack of sufficient manually labeled video data for saliency modeling, especially for the data-driven deep learning. This paper proposes a novel weakly supervised approach to the salient object detection in a video, which can learn a robust saliency prediction model by using very limited manually labeled data and a large amount of weakly labeled data that could be easily generated in a supervised approach. Furthermore, we propose a spatiotemporal cascade neural network architecture for saliency modeling, in which two fully convolutional networks are cascaded to evaluate the visual saliency from both spatial and temporal cues to lead the optimal video saliency prediction. The proposed approach is extensively evaluated on the widely used challenging data sets, and the experiments demonstrate that our proposed approach substantially outperforms the state-of-the-art salient object detection models.","","","10.1109/TCSVT.2018.2859773","National Natural Science Foundation of China; Natural Science Foundation of Shenzhen; Shenzhen University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419765","Video saliency;weakly supervised learning;spatiotemporal prior fusion;cascade fully convolutional network","Videos;Object detection;Spatiotemporal phenomena;Neural networks;Saliency detection;Machine learning;Feature extraction","convolutional neural nets;feature extraction;image classification;image segmentation;learning (artificial intelligence);object detection;video signal processing","spatiotemporal cascade neural networks;deep learning techniques;saliency modeling;data-driven deep learning;supervised approach;robust saliency prediction model;manually labeled data;weakly labeled data;spatiotemporal cascade neural network architecture;optimal video saliency prediction;weakly supervised salient object detection","","2","61","","","","","IEEE","IEEE Journals"
"A Machine Learning-Based Fast-Forward Solver for Ground Penetrating Radar With Application to Full-Waveform Inversion","I. Giannakis; A. Giannopoulos; C. Warren","School of Engineering, The University of Edinburgh, Edinburgh, U.K.; School of Engineering, The University of Edinburgh, Edinburgh, U.K.; Department of Mechanical and Construction Engineering, Northumbria University, Newcastle upon Tyne, U.K.","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","7","4417","4426","The simulation, or forward modeling, of ground penetrating radar (GPR) is becoming a more frequently used approach to facilitate the interpretation of complex real GPR data, and as an essential component of full-waveform inversion (FWI). However, general full-wave 3-D electromagnetic (EM) solvers, such as the ones based on the finite-difference time-domain (FDTD) method, are still computationally demanding for simulating realistic GPR problems. We have developed a novel near-real-time, forward modeling approach for GPR that is based on a machine learning (ML) architecture. The ML framework uses an innovative training method that combines a predictive principal component analysis technique, a detailed model of the GPR transducer, and a large data set of modeled GPR responses from our FDTD simulation software. The ML-based forward solver is parameterized for a specific GPR application, but the framework can be applied to many different classes of GPR problems. To demonstrate the novelty and computational efficiency of our ML-based GPR forward solver, we used it to carry out FWI for a common infrastructure assessment application-determining the location and diameter of reinforcement bars in concrete. We tested our FWI with synthetic and real data and found a good level of accuracy in determining the rebar location, size, and surrounding material properties from both data sets. The combination of the near-real-time computation, which is orders of magnitude less than what is achievable by traditional full-wave 3-D EM solvers, and the accuracy of our ML-based forward model is a significant step toward commercially viable applications of FWI of GPR.","","","10.1109/TGRS.2019.2891206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630654","Concrete;deep learning;finite-difference time-domain (FDTD);full-waveform inversion (FWI);ground penetrating radar (GPR);machine learning (ML);neural networks;non-destructive testing (NDT);rebar","Ground penetrating radar;Computational modeling;Training;Data models;Neural networks;Finite difference methods;Time-domain analysis","finite difference time-domain analysis;ground penetrating radar;learning (artificial intelligence);principal component analysis;radar computing;rebar","full-waveform inversion;FWI;electromagnetic solvers;finite-difference time-domain method;machine learning architecture;ML framework;innovative training method;predictive principal component analysis technique;GPR transducer;data set;modeled GPR responses;FDTD simulation software;specific GPR application;ML-based GPR forward;near-real-time computation;ML-based forward model;machine learning-based fast-forward solver;ground penetrating radar;infrastructure assessment application","","2","51","","","","","IEEE","IEEE Journals"
"Neural Networks for Deep Radiotherapy Dose Analysis and Prediction of Liver SBRT Outcomes","B. Ibragimov; D. A. S. Toesca; Y. Yuan; A. C. Koong; D. T. Chang; L. Xing","Department of Radiation Oncology, Stanford University, Palo Alto, CA, USA; Department of Radiation Oncology, Stanford University, Palo Alto, CA, USA; Department of Electrical Engineering, City University of Hong Kong, Hong Kong; Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA; Department of Radiation Oncology, Stanford University, Palo Alto, CA, USA; Department of Radiation Oncology, Stanford University, Palo Alto, CA, USA","IEEE Journal of Biomedical and Health Informatics","","2019","23","5","1821","1833","Stereotactic body radiation therapy (SBRT) is a relatively novel treatment modality, with little post-treatment prognostic information reported. This study proposes a novel neural network based paradigm for accurate prediction of liver SBRT outcomes. We assembled a database of patients treated with liver SBRT at our institution. Together with a three-dimensional (3-D) dose delivery plans for each SBRT treatment, other variables such as patients' demographics, quantified abdominal anatomy, history of liver comorbidities, other liver-directed therapies, and liver function tests were collected. We developed a multi-path neural network with the convolutional path for 3-D dose plan analysis and fully connected path for other variables analysis, where the network was trained to predict postSBRT survival and local cancer progression. To enhance the network robustness, it was initially pre-trained on a large database of computed tomography images. Following n-fold cross-validation, the network automatically identified patients that are likely to have longer survival or late cancer recurrence, i.e., patients with the positive predicted outcome (PPO) of SBRT, and vice versa, i.e., negative predicted outcome (NPO). The predicted results agreed with actual SBRT outcomes with 56% of PPO patients and 0% NPO patients with primary liver cancer survived more than two years after SBRT. Similarly, 82% of PPO patients and 0% of NPO patients with metastatic liver cancer survived twoyear threshold. The obtained results were superior to the performance of support vector machine and random forest classifiers. Furthermore, the network was able to identify the critical-to-spare liver regions, and the critical clinical features associated with the highest risks of negative SBRT outcomes.","","","10.1109/JBHI.2019.2904078","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664101","Liver cancer;SBRT;survival prediction;local progression prediction;deep learning;convolutional neural networks","Liver;Cancer;Three-dimensional displays;Tumors;Neural networks;Computed tomography;Deep learning","cancer;computerised tomography;dosimetry;liver;medical image processing;neural nets;patient diagnosis;patient treatment;radiation therapy;support vector machines;tumours","neural networks;deep radiotherapy dose analysis;liver SBRT outcomes;stereotactic body radiation therapy;relatively novel treatment modality;post-treatment prognostic information;novel neural network;dose delivery plans;SBRT treatment;liver comorbidities;liver-directed therapies;liver function tests;multipath neural network;variables analysis;local cancer progression;network robustness;longer survival;positive predicted outcome;actual SBRT outcomes;PPO patients;primary liver cancer;metastatic liver cancer;critical-to-spare liver regions;3D dose plan analysis;negative SBRT;NPO patients","","","59","Traditional","","","","IEEE","IEEE Journals"
"Robust correlation filter tracking with deep semantic supervision","W. Wang; Z. Chen; L. Douadji; M. Shi","Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, People's Republic of China; Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, People's Republic of China; Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, People's Republic of China; Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, People's Republic of China","IET Image Processing","","2019","13","5","754","760","Traditional correlation filter (CF) tracking has achieved high tracking performance and speed. However, it easily falls into tracking failures in some cases of target occlusion, deformation, rotation etc. Tracking failure also contaminates the CF model and makes it less discriminative. To tackle these problems, the authors propose a deep semantic supervision tracking framework. This framework integrates the advantages of multiple features and tracking methods into an evaluation and redetection tracking mechanism. In this work, customised deep convolutional neural network (CNN) with particle filtering (PF) resampling was employed to alleviate the contamination of the CF model and improve tracking performance. The authors also adopted a mixed decision mechanism for CF tracking results evaluation. Furthermore, based on the observation that most tracking frames can be easily tracked by a CF tracker using handcrafted features, authors' tracking method achieves real-time performance. It should be noted that the proposed framework is flexible and extensible to improve other existing trackers. In authors' extensive experiments on large benchmark datasets including OTB2013 and OTB2015, the proposed tracker performed favourably compared to the state-of-the-art methods.","","","10.1049/iet-ipr.2018.5314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8689152","","","convolutional neural nets;learning (artificial intelligence);particle filtering (numerical methods);target tracking","OTB2015 benchmark datasets;OTB2013 benchmark datasets;real-time performance;handcrafted features;target occlusion;tracking frames;deep convolutional neural network;CF tracker;particle filtering resampling;redetection tracking mechanism;deep semantic supervision tracking framework;tracking failure;high tracking performance;robust correlation filter tracking","","","39","","","","","IET","IET Journals"
"Adversarial Learning for Constrained Image Splicing Detection and Localization Based on Atrous Convolution","Y. Liu; X. Zhu; X. Zhao; Y. Cao","State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","","2019","14","10","2551","2566","Constrained image splicing detection and localization (CISDL), which investigates two input suspected images and identifies whether one image has suspected regions pasted from the other, is a newly proposed challenging task for image forensics. In this paper, we propose a novel adversarial learning framework to learn a deep matching network for CISDL. Our framework mainly consists of three building blocks. First, a deep matching network based on atrous convolution (DMAC) aims to generate two high-quality candidate masks, which indicate suspected regions of the two input images. In DMAC, atrous convolution is adopted to extract features with rich spatial information, a correlation layer based on a skip architecture is proposed to capture hierarchical features, and atrous spatial pyramid pooling is constructed to localize tampered regions at multiple scales. Second, a detection network is designed to rectify inconsistencies between the two corresponding candidate masks. Finally, a discriminative network drives the DMAC network to produce masks that are hard to distinguish from ground-truth ones. The detection network and the discriminative network collaboratively supervise the training of DMAC in an adversarial way. Besides, a sliding window-based matching strategy is investigated for high-resolution images matching. Extensive experiments, conducted on five groups of datasets, demonstrate the effectiveness of the proposed framework and the superior performance of DMAC.","","","10.1109/TIFS.2019.2902826","National Natural Science Foundation of China; National Key Technology R&D Program; Beijing Municipal Science and Technology Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658131","Image forensics;constrained image splicing detection and localization;adversarial learning;deep matching;atrous convolution","Feature extraction;Convolution;Correlation;Task analysis;Training;Computer architecture;Image forensics","feature extraction;image forensics;image matching;image resolution;image segmentation;learning (artificial intelligence)","adversarial learning;constrained image splicing detection;atrous convolution;image forensics;deep matching network;high-quality candidate masks;atrous spatial pyramid pooling;detection network;discriminative network;DMAC network;sliding window-based matching strategy;constrained image splicing localization;feature extraction;spatial information;correlation layer;hierarchical features;high-resolution images matching","","","66","","","","","IEEE","IEEE Journals"
"Deep-Learning-Based Fault Classification Using Hilbert–Huang Transform and Convolutional Neural Network in Power Distribution Systems","M. Guo; N. Yang; W. Chen","College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China","IEEE Sensors Journal","","2019","19","16","6905","6913","Fault classification is important for the fault cause analysis and faster power supply restoration. A deep-learning-based fault classification method in small current grounding power distribution systems is presented in this paper. The current and voltage signals are sampled at a substation when a fault occurred. The time-frequency energy matrix is constructed via applying Hilbert-Huang transform (HHT) band-pass filter to those sampled fault signals. Regarding the time-frequency energy matrix as the pixel matrix of digital image, a method for image similarity recognition based on convolution neural network (CNN) is used for fault classification. The presented method can extract the features of fault signals and accurately classify ten types of short-circuit faults, simultaneously. Two simulation models are established in the PSCAD/EMTDC and physical system environment, respectively. The performance of the presented method is studied in the MATLAB environment. Various kinds of fault conditions and factors including asynchronous sampling, different network structures, distribution generators access, and so on are considered to verify the adaptability of the presented method. The results of investigation show that the presented method has the characteristics of high accuracy and adaptability in fault classification of power distribution systems.","","","10.1109/JSEN.2019.2913006","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698326","Power distribution systems;fault classification;Hilbert-Huang transform band-pass filter;time-frequency energy matrix;deep learning;convolution neural network","Circuit faults;Time-frequency analysis;Transforms;Power distribution;Feature extraction;Band-pass filters;Fuzzy logic","band-pass filters;convolutional neural nets;distributed power generation;earthing;fault diagnosis;fault location;Hilbert transforms;learning (artificial intelligence);power distribution faults;time-frequency analysis","Hilbert-Huang transform;convolutional neural network;fault cause analysis;faster power supply restoration;fault classification method;current grounding power distribution systems;current voltage signals;time-frequency energy matrix;sampled fault signals;convolution neural network;short-circuit faults;fault conditions;CNN;PSCAD;EMTDC;physical system environment;MATLAB environment","","1","23","","","","","IEEE","IEEE Journals"
"Deep Crisp Boundaries: From Boundaries to Higher-Level Tasks","Y. Wang; X. Zhao; Y. Li; K. Huang","Center for Research on Intelligent System and Engineering, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent System and Engineering, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Biostatistics and Medical Informatics and the Department of Computer Sciences, University of Wisconsin–Madison, Madison, WI, USA; University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2019","28","3","1285","1298","Edge detection has made significant progress with the help of deep convolutional networks (ConvNet). These ConvNet-based edge detectors have approached human level performance on standard benchmarks. We provide a systematical study of these detectors' outputs. We show that the detection results did not accurately localize edge pixels, which can be adversarial for tasks that require crisp edge inputs. As a remedy, we propose a novel refinement architecture to address the challenging problem of learning a crisp edge detector using ConvNet. Our method leverages a top-down backward refinement pathway, and progressively increases the resolution of feature maps to generate crisp edges. Our results achieve superior performance, surpassing human accuracy when using standard criteria on BSDS500, and largely outperforming the state-of-the-art methods when using more strict criteria. More importantly, we demonstrate the benefit of crisp edge maps for several important applications in computer vision, including optical flow estimation, object proposal generation, and semantic segmentation.","","","10.1109/TIP.2018.2874279","National Key Research and Development Program of China; National Natural Science Foundation of China; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8485388","Boundary detection;deep learning","Image edge detection;Detectors;Task analysis;Proposals;Optical imaging;Semantics;Standards","computer vision;convolution;edge detection;feature extraction;image representation;image segmentation;image sequences;learning (artificial intelligence);object detection","higher-level tasks;edge detection;deep convolutional networks;ConvNet-based edge detectors;human level performance;standard benchmarks;edge pixels;crisp edge inputs;crisp edge detector;backward refinement pathway;human accuracy;deep crisp boundaries;object proposal generation;crisp edge maps;standard criteria","","2","62","","","","","IEEE","IEEE Journals"
"Toward Generalized Change Detection on Planetary Surfaces With Convolutional Autoencoders and Transfer Learning","H. R. Kerner; K. L. Wagstaff; B. D. Bue; P. C. Gray; J. F. Bell III; H. Ben Amor","Department of Geographical Sciences, University of Maryland, College Park, MD, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Nicholas School of the Environment, Duke University, Durham, NC, USA; School of Earth and Space Exploration, Arizona State University, Tempe, AZ, USA; School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","10","3900","3918","Ongoing planetary exploration missions are returning large volumes of image data. Identifying surface changes in these images, e.g., new impact craters, is critical for investigating many scientific hypotheses. Traditional approaches to change detection rely on image differencing and manual feature engineering. These methods can be sensitive to irrelevant variations in illumination or image quality and typically require before and after images to be coregistered, which itself is a major challenge. Additionally, most prior change detection studies have been limited to remote sensing images of earth. We propose a new deep learning approach for binary patch-level change detection involving transfer learning and nonlinear dimensionality reduction using convolutional autoencoders. Our experiments on diverse remote sensing datasets of Mars, the moon, and earth show that our methods can detect meaningful changes with high accuracy using a relatively small training dataset despite significant differences in illumination, image quality, imaging sensors, coregistration, and surface properties. We show that the latent representations learned by a convolutional autoencoder yield the most general representations for detecting change across surface feature types, scales, sensors, and planetary bodies.","","","10.1109/JSTARS.2019.2936771","Jet Propulsion Laboratory; California Institute of Technology; Internal Strategic University Research Partnerships; National Aeronautics and Space Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827273","Change detection algorithms;earth;machine learning;mars;moon;neural networks;remote sensing;supervised learning;unsupervised learning","Remote sensing;Earth;Feature extraction;Mars;Neural networks;Deep learning;Moon","","","","","82","IEEE","","","","IEEE","IEEE Journals"
"GETNET: A General End-to-End 2-D CNN Framework for Hyperspectral Image Change Detection","Q. Wang; Z. Yuan; Q. Du; X. Li","School of Computer Science, the Center for OPTical IMagery Analysis and Learning, Unmanned System Research Institute, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and the Center for OPTical IMagery Analysis and Learning, Northwestern Polytechnical University, Xi’an, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","1","3","13","Change detection (CD) is an important application of remote sensing, which provides timely change information about large-scale Earth surface. With the emergence of hyperspectral imagery, CD technology has been greatly promoted, as hyperspectral data with high spectral resolution are capable of detecting finer changes than using the traditional multispectral imagery. Nevertheless, the high dimension of the hyperspectral data makes it difficult to implement traditional CD algorithms. Besides, endmember abundance information at subpixel level is often not fully utilized. In order to better handle high-dimension problem and explore abundance information, this paper presents a general end-to-end 2-D convolutional neural network (CNN) framework for hyperspectral image CD (HSI-CD). The main contributions of this paper are threefold: 1) mixed-affinity matrix that integrates subpixel representation is introduced to mine more cross-channel gradient features and fuse multisource information; 2) 2-D CNN is designed to learn the discriminative features effectively from the multisource data at a higher level and enhance the generalization ability of the proposed CD algorithm; and 3) the new HSI-CD data set is designed for objective comparison of different methods. Experimental results on real hyperspectral data sets demonstrate that the proposed method outperforms most of the state of the arts.","","","10.1109/TGRS.2018.2849692","National Key R&D Program of China; National Natural Science Foundation of China; Natural Science Foundation of Shaanxi Province; Fundamental Research Funds for the Central Universities; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418840","2-D convolutional neural network (CNN);change detection (CD);deep learning;hyperspectral image (HSI);mixed-affinity matrix;spectral unmixing","Hyperspectral imaging;Machine learning;Task analysis;Neural networks;Principal component analysis","convolution;data mining;feedforward neural nets;geophysical image processing;geophysical techniques;hyperspectral imaging;image fusion;image resolution;image segmentation;learning (artificial intelligence);remote sensing","hyperspectral image change detection;hyperspectral imagery;CD technology;hyperspectral image CD;fuse multisource information;HSI-CD data;hyperspectral data sets;multispectral imagery;GETNET;general End-to-End 2-D CNN;cross-channel gradient features;general end-to-end 2-D convolutional neural network","","47","35","","","","","IEEE","IEEE Journals"
"DeepISP: Toward Learning an End-to-End Image Processing Pipeline","E. Schwartz; R. Giryes; A. M. Bronstein","School of Electrical Engineering, Tel-Aviv University, Tel Aviv, Israel; School of Electrical Engineering, Tel-Aviv University, Tel Aviv, Israel; Department of Computer Science, Technion–Israel Institute of Technology, Haifa, Israel","IEEE Transactions on Image Processing","","2019","28","2","912","923","We present DeepISP, a full end-to-end deep neural model of the camera image signal processing pipeline. Our model learns a mapping from the raw low-light mosaiced image to the final visually compelling image and encompasses low-level tasks, such as demosaicing and denoising, as well as higher-level tasks, such as color correction and image adjustment. The training and evaluation of the pipeline were performed on a dedicated data set containing pairs of low-light and well-lit images captured by a Samsung S7 smartphone camera in both raw and processed JPEG formats. The proposed solution achieves the state-of-the-art performance in objective evaluation of peak signal-to-noise ratio on the subtask of joint denoising and demosaicing. For the full end-to-end pipeline, it achieves better visual quality compared to the manufacturer ISP, in both a subjective human assessment and when rated by a deep model trained for assessing image quality.","","","10.1109/TIP.2018.2872858","H2020 European Research Council; H2020 European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478390","Image processing;deep learning;ISP;denoising;demosaicing;color correction","Task analysis;Pipelines;Noise reduction;Image color analysis;Cameras;Image resolution","cameras;feature extraction;image classification;image coding;image colour analysis;image denoising;image segmentation;image sensors;learning (artificial intelligence);neural nets;smart phones","DeepISP;end-to-end image processing pipeline;end-to-end deep neural model;camera image signal;raw low-light mosaiced image;final visually compelling image;low-level tasks;demosaicing;higher-level tasks;color correction;image adjustment;well-lit images;Samsung S7 smartphone camera;raw processed JPEG formats;peak signal-to-noise ratio;joint denoising;end-to-end pipeline;deep model;image quality","","1","47","","","","","IEEE","IEEE Journals"
"Deep Supervision with Intermediate Concepts","C. Li; M. Z. Zia; Q. Tran; X. Yu; G. D. Hager; M. Chandraker","Computer Science, Johns Hopkins University, Baltimore, MD, USA; Hololens, Microsoft, Redmond, WA, USA; Media Analytics, NEC Laboratories America Inc, Cupertino, CA, USA; Media Analytics, NEC Laboratories America Inc, Cupertino, CA, USA; Department of Computer Science, The Johns Hopkins University, Baltimore, MD, USA; Media Analytics, NEC Laboratories America Inc, Cupertino, CA, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","8","1828","1843","Recent data-driven approaches to scene interpretation predominantly pose inference as an end-to-end black-box mapping, commonly performed by a Convolutional Neural Network (CNN). However, decades of work on perceptual organization in both human and machine vision suggest that there are often intermediate representations that are intrinsic to an inference task, and which provide essential structure to improve generalization. In this work, we explore an approach for injecting prior domain structure into neural network training by supervising hidden layers of a CNN with intermediate concepts that normally are not observed in practice. We formulate a probabilistic framework which formalizes these notions and predicts improved generalization via this deep supervision method. One advantage of this approach is that we are able to train only from synthetic CAD renderings of cluttered scenes, where concept values can be extracted, but apply the results to real images. Our implementation achieves the state-of-the-art performance of 2D/3D keypoint localization and image classification on real image benchmarks including KITTI, PASCALVOC, PASCAL3D+, IKEA, and CIFAR100. We provide additional evidence that our approach outperforms alternative forms of supervision, such as multi-task networks.","","","10.1109/TPAMI.2018.2863285","NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8434117","Deep learning;multi-task learning;single image 3D structure prediction;object pose estimation","Three-dimensional displays;Solid modeling;Task analysis;Shape;Two dimensional displays;Rendering (computer graphics);Training data","CAD;computer vision;convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);probability;rendering (computer graphics)","machine vision;3D keypoint localization;2D keypoint localization;multitask networks;image classification;concept values;cluttered scenes;synthetic CAD renderings;deep supervision method;probabilistic framework;hidden layers;neural network training;prior domain structure;inference task;intermediate representations;perceptual organization;CNN;Convolutional Neural Network;end-to-end black-box mapping;scene interpretation;recent data-driven approaches;intermediate concepts","","1","60","","","","","IEEE","IEEE Journals"
"Deep Memory Network for Cross-Modal Retrieval","G. Song; D. Wang; X. Tan","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Transactions on Multimedia","","2019","21","5","1261","1275","With the explosive growth of multimedia data on the Internet, cross-modal retrieval has attracted a great deal of attention in both computer vision and multimedia communities. However, this task is challenging due to the heterogeneity gap between different modalities. Current approaches typically involve a common representation learning process that maps data from different modalities into a common space by linear or nonlinear embedding. Yet, most of them only handle the dual-modal situation and generalize poorly to complex cases that involve multiple modalities. In addition, they often require expensive fine-grained alignment of training data among diverse modalities. In this paper, we address these with a novel cross-modal memory network (CMMN), in which memory contents across modalities are simultaneously learned from end to end without the need of exact alignment. We further account for the diversity across multiple modalities using the strategy of adversarial learning. Extensive experimental results on several large-scale datasets demonstrate that the proposed CMMN approach achieves state-of-the-art performance in the task of cross-modal retrieval.","","","10.1109/TMM.2018.2877122","National Natural Science Foundation of China; Equipments of China; Qing Lan Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8506385","Cross-modal retrieval;memory network;deep learning","Semantics;Sports;Task analysis;Visualization;Neural networks;Correlation;Binary codes","computer vision;information retrieval;Internet;learning (artificial intelligence);multimedia systems","deep memory network;cross-modal retrieval;multimedia data;cross-modal memory network;Internet;computer vision;nonlinear embedding;linear embedding;CMMN;adversarial learning","","","57","","","","","IEEE","IEEE Journals"
"A Deep Collaborative Framework for Face Photo–Sketch Synthesis","M. Zhu; J. Li; N. Wang; X. Gao","State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","10","3096","3108","Great breakthroughs have been made in the accuracy and speed of face photo-sketch synthesis in recent years. Regression-based methods have gained increasing attention, which benefit from deeper and faster end-to-end convolutional neural networks. However, most of these models typically formulate the mapping from photo domain X to sketch domain Y as a unidirectional feedforward mapping, G : X → Y, and vice versa, F : Y → X; thus, the utilization of mutual interaction between two opposite mappings is lacking. Therefore, we proposed a collaborative framework for face photo-sketch synthesis. The concept behind our model was that a middle latent domain Z̃ between the photo domain X and the sketch domain Y can be learned during the learning procedure of G : X → Y and F : Y → X by introducing a collaborative loss that makes full use of two opposite mappings. This strategy can constrain the two opposite mappings and make them more symmetrical, thus making the network more suitable for the photo-sketch synthesis task and obtaining higher quality generated images. Qualitative and quantitative experiments demonstrated the superior performance of our model in comparison with the existing state-of-the-art solutions.","","","10.1109/TNNLS.2018.2890018","National Natural Science Foundation of China; National Key Research and Development Program of China; Key Industrial Innovation Chain in Industrial Domain; National High-Level Talents Special Support Program of China; Young Elite Scientists Sponsorship Program by CAST; Natural Science Basic Research Plan in the Shaanxi Province of China; Young Talent Fund of the University Association for Science and Technology in Shaanxi, China; Fundamental Research Funds for the Central Universities; Xidian University; CCF-Tencent Open Fund; Xidian University—Intellifusion Joint Innovation Laboratory of Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8621611","Collaborative loss;deep collaborative nets (Col-Nets);face photo–sketch synthesis;generative adversarial nets","Face;Collaboration;Image reconstruction;Gallium nitride;Task analysis;Training;Law enforcement","computer vision;convolutional neural nets;face recognition;feedforward;learning (artificial intelligence);object detection;regression analysis","deep collaborative framework;face photo-sketch synthesis;end-to-end convolutional neural networks;unidirectional feedforward mapping;opposite mappings;sketch domain;photo-sketch synthesis task;photo domain;middle latent domain;regression-based methods","","1","59","","","","","IEEE","IEEE Journals"
"Progressive Spatial Recurrent Neural Network for Intra Prediction","Y. Hu; W. Yang; M. Li; J. Liu","Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China","IEEE Transactions on Multimedia","","2019","21","12","3024","3037","Intra prediction is an important component of modern video codecs, which is able to efficiently squeeze out the spatial redundancy in video frames. With preceding pixels as the context, traditional intra prediction schemes generate linear predictions based on several predefined directions (i.e., modes) for blocks to be encoded. However, these modes are relatively simple and their predictions may fail when facing blocks with complex textures, which leads to additional bits encoding the residue. In this paper, we design a progressive spatial recurrent neural network (PS-RNN) that learns to conduct intra prediction. Specifically, our PS-RNN consists of three spatial recurrent units and progressively generates predictions by passing information along from preceding contents to blocks to be encoded. To make our network generate predictions considering both distortion and bit rate, we propose using sum of absolute transformed difference (SATD) as the loss function to train PS-RNN since SATD is able to measure rate-distortion cost of encoding a residue block. Moreover, our method supports variable-block-size for intra prediction, which is more practical in real coding conditions. The proposed intra prediction scheme achieves on average 2.5% bit-rate reduction on variable-block-size settings under the same reconstruction quality compared with HEVC.","","","10.1109/TMM.2019.2920603","National Natural Science Foundation of China; Natural Science Foundation of Beijing Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8727931","Video Coding;intra prediction;deep learning;spatial RNN;SATD loss;HEVC","Codecs;Video coding;Deep learning;Deep learning;Rate-distortion;Recurrent neural networks","","","","","48","IEEE","","","","IEEE","IEEE Journals"
"A Deep Neural Framework for Continuous Sign Language Recognition by Iterative Training","R. Cui; H. Liu; C. Zhang","Tsinghua University (THUAI), State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Department of Automation, Institute for Artificial Intelligence, Tsinghua University, Beijing, China; Tsinghua University (THUAI), State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Department of Automation, Institute for Artificial Intelligence, Tsinghua University, Beijing, China; Tsinghua University (THUAI), State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Department of Automation, Institute for Artificial Intelligence, Tsinghua University, Beijing, China","IEEE Transactions on Multimedia","","2019","21","7","1880","1891","This work develops a continuous sign language (SL) recognition framework with deep neural networks, which directly transcribes videos of SL sentences to sequences of ordered gloss labels. Previous methods dealing with continuous SL recognition usually employ hidden Markov models with limited capacity to capture the temporal information. In contrast, our proposed architecture adopts deep convolutional neural networks with stacked temporal fusion layers as the feature extraction module, and bidirectional recurrent neural networks as the sequence learning module. We propose an iterative optimization process for our architecture to fully exploit the representation capability of deep neural networks with limited data. We first train the end-to-end recognition model for alignment proposal, and then use the alignment proposal as strong supervisory information to directly tune the feature extraction module. This training process can run iteratively to achieve improvements on the recognition performance. We further contribute by exploring the multimodal fusion of RGB images and optical flow in sign language. Our method is evaluated on two challenging SL recognition benchmarks, and outperforms the state of the art by a relative improvement of more than 15% on both databases.","","","10.1109/TMM.2018.2889563","National Natural Science Foundation of China; Beijing Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598757","Continuous sign language recognition;sequence learning;iterative training;multimodal fusion","Feature extraction;Training;Videos;Hidden Markov models;Convolutional neural networks;Gesture recognition","feature extraction;hidden Markov models;image representation;image sequences;learning (artificial intelligence);recurrent neural nets;sign language recognition","iterative training;continuous sign language recognition framework;deep neural networks;SL sentences;ordered gloss labels;continuous SL recognition;Markov models;temporal information;deep convolutional neural networks;stacked temporal fusion layers;feature extraction module;bidirectional recurrent neural networks;sequence learning module;iterative optimization process;end-to-end recognition model;alignment proposal;training process;recognition performance;SL recognition benchmarks","","3","48","","","","","IEEE","IEEE Journals"
"Multi-Task Hierarchical Feature Learning for Real-Time Visual Tracking","Y. Kuai; G. Wen; D. Li","ATR Key Laboratory, National University of Defense Technology, Changsha, China; ATR Key Laboratory, National University of Defense Technology, Changsha, China; ATR Key Laboratory, National University of Defense Technology, Changsha, China","IEEE Sensors Journal","","2019","19","5","1961","1968","Recently, the tracking community leads a fashion of end-to-end feature learning using convolutional neural networks (CNNs) for visual object tracking. Traditional trackers extract feature maps from the last convolutional layer of CNNs for feature representation. This single-layer representation ignores target information captured in the earlier convolutional layers. In this paper, we propose a novel hierarchical feature learning framework, which captures both high-level semantics and low-level spatial details using multi-task learning. Particularly, feature maps extracted from both the shallow layer and the deep layer are input into a correlation filter layer to encode fine-grained geometric cues and coarse-grained semantic cues, respectively. Our network performs these two feature learning tasks with a multi-task learning strategy. We conduct extensive experiments on three popular tracking datasets, including OTB, UAV123, and VOT2016. Experimental results show that our method achieves remarkable performance improvement while running in real time.","","","10.1109/JSEN.2018.2883593","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8548575","Visual tracking;correlation filter;hierarchical feature;multi-task learning","Correlation;Feature extraction;Target tracking;Task analysis;Semantics;Visualization;Convolutional neural networks","feature extraction;image representation;learning (artificial intelligence);object detection;object tracking","multitask hierarchical feature learning;real-time visual tracking;tracking community;end-to-end feature;convolutional neural networks;CNNs;visual object tracking;traditional trackers;feature maps;convolutional layer;feature representation;single-layer representation;target information;earlier convolutional layers;novel hierarchical feature;high-level semantics;low-level spatial details;shallow layer;deep layer;correlation filter layer;fine-grained geometric cues;coarse-grained semantic cues;learning tasks;multitask learning strategy;popular tracking datasets","","","45","","","","","IEEE","IEEE Journals"
"Zero-Shot Learning via Category-Specific Visual-Semantic Mapping and Label Refinement","L. Niu; J. Cai; A. Veeraraghavan; L. Zhang","Electric and Computer Engineering Department, Rice University, Houston, TX, USA; School of Computer Engineering, Nanyang Technological University, Singapore; Electric and Computer Engineering Department, Rice University, Houston, TX, USA; Computer Science and Engineering Department, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Image Processing","","2019","28","2","965","979","Zero-shot learning (ZSL) aims to classify a test instance from an unseen category based on the training instances from seen categories in which the gap between seen categories and unseen categories is generally bridged via visual-semantic mapping between the low-level visual feature space and the intermediate semantic space. However, the visual-semantic mapping (i.e., projection) learnt based on seen categories may not generalize well to unseen categories, which is known as the projection domain shift in ZSL. To address this projection domain shift issue, we propose a method named adaptive embedding ZSL (AEZSL) to learn an adaptive visual-semantic mapping for each unseen category, followed by progressive label refinement. Moreover, to avoid learning visual-semantic mapping for each unseen category in the large-scale classification task, we additionally propose a deep adaptive embedding model named deep AEZSL sharing the similar idea (i.e., visual-semantic mapping should be category specific and related to the semantic space) with AEZSL, which only needs to be trained once, but can be applied to arbitrary number of unseen categories. Extensive experiments demonstrate that our proposed methods achieve the state-of-the-art results for image classification on three small-scale benchmark datasets and one large-scale benchmark dataset.","","","10.1109/TIP.2018.2872916","Key Basic Research Program of Shanghai; National Basic Research Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8476580","Zero-shot learning (ZSL);domain adaptation","Semantics;Visualization;Training;Task analysis;Adaptation models;Feature extraction;Terminology","image classification;learning (artificial intelligence)","zero-shot learning;low-level visual feature space;intermediate semantic space;projection domain shift;adaptive embedding ZSL method;adaptive category-specific visual-semantic mapping;progressive label refinement;large-scale classification task;deep adaptive embedding model;deep AEZSL method;image classification;large-scale benchmark dataset","","2","54","","","","","IEEE","IEEE Journals"
"Interpreting Deep Visual Representations via Network Dissection","B. Zhou; D. Bau; A. Oliva; A. Torralba","CSAIL, MIT, Cambridge, MA, USA; CSAIL, MIT, Cambridge, MA, USA; CSAIL, MIT, Cambridge, MA, USA; CSAIL, MIT, Cambridge, MA, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","9","2131","2145","The success of recent deep convolutional neural networks (CNNs) depends on learning hidden representations that can summarize the important factors of variation behind the data. In this work, we describe Network Dissection, a method that interprets networks by providing meaningful labels to their individual units. The proposed method quantifies the interpretability of CNN representations by evaluating the alignment between individual hidden units and visual semantic concepts. By identifying the best alignments, units are given interpretable labels ranging from colors, materials, textures, parts, objects and scenes. The method reveals that deep representations are more transparent and interpretable than they would be under a random equivalently powerful basis. We apply our approach to interpret and compare the latent representations of several network architectures trained to solve a wide range of supervised and self-supervised tasks. We then examine factors affecting the network interpretability such as the number of the training iterations, regularizations, different initialization parameters, as well as networks depth and width. Finally we show that the interpreted units can be used to provide explicit explanations of a given CNN prediction for an image. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into what hierarchical structures can learn.","","","10.1109/TPAMI.2018.2858759","DARPA XAI; NSF; NSF; ONR; MIT Big Data Initiative at CSAIL; Toyota Research Institute MIT CSAIL Joint Research Center; Google; Amazon; Nvidia; Facebook; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417924","Convolutional neural networks;network interpretability;visual recognition;interpretable machine learning","Visualization;Detectors;Training;Image color analysis;Task analysis;Image segmentation;Semantics","convolutional neural nets;image classification;image representation;learning (artificial intelligence)","network dissection;deep convolutional neural networks;CNN prediction;network depth;hidden units;deep visual representations;interpreted units;network interpretability;network architectures;latent representations;random equivalently powerful basis;deep representations;interpretable labels;visual semantic concepts;meaningful labels;hidden representations","","","57","","","","","IEEE","IEEE Journals"
"Deep Binary Reconstruction for Cross-Modal Hashing","D. Hu; F. Nie; X. Li","School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Multimedia","","2019","21","4","973","985","To satisfy the huge storage space and organization capacity requirements in addressing big multimodal data, hashing techniques have been widely employed to learn binary representations in cross-modal retrieval tasks. However, optimizing the hashing objective under the necessary binary constraint is truly a difficult problem. A common strategy is to relax the constraint and perform individual binarizations over the learned real-valued representations. In this paper, in contrast to conventional two-stage methods, we propose to directly learn the binary codes, where the model can be easily optimized by a standard gradient descent optimizer. However, before that, we present a theoretical guarantee of the effectiveness of the multimodal network in preserving the inter- and intra-modal consistencies. Based on this guarantee, a novel multimodal deep binary reconstruction model is proposed, which can be trained to simultaneously model the correlation across modalities and learn the binary hashing codes. To generate binary codes and to avoid the tiny gradient problem, a novel activation function first scales the input activations to suitable scopes and, then, feeds them to the tanh function to build the hashing layer. Such a composite function is named adaptive tanh. Both linear and nonlinear scaling methods are proposed and shown to generate efficient codes after training the network. Extensive ablation studies and comparison experiments are conducted for the image2text and text2image retrieval tasks; the method is found to outperform several state-of-the-art deep-learning methods with respect to different evaluation metrics.","","","10.1109/TMM.2018.2866771","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8447211","Cross-modal hashing;binary reconstruction","Semantics;Task analysis;Binary codes;Correlation;Image reconstruction;Sparse matrices;Training","binary codes;file organisation;gradient methods;image reconstruction;image retrieval;learning (artificial intelligence)","text2image retrieval tasks;organization capacity requirements;big multimodal data;binary representations;cross-modal retrieval tasks;hashing objective;learned real-valued representations;binary codes;standard gradient descent optimizer;multimodal network;intra-modal consistencies;binary hashing codes;tiny gradient problem;tanh function;hashing layer;nonlinear scaling methods;activation function;multimodal deep binary reconstruction model;image2text retrieval tasks","","","55","","","","","IEEE","IEEE Journals"
"Iterative Joint Image Demosaicking and Denoising Using a Residual Denoising Network","F. Kokkinos; S. Lefkimmiatis","Center for Computational and Data-Intensive Science and Engineering, Skolkovo Institute of Science and Technology, Moscow, Russia; Center for Computational and Data-Intensive Science and Engineering, Skolkovo Institute of Science and Technology, Moscow, Russia","IEEE Transactions on Image Processing","","2019","28","8","4177","4188","Modern digital cameras rely on the sequential execution of separate image processing steps to produce realistic images. The first two steps are usually related to denoising and demosaicking, where the former aims to reduce noise from the sensor and the latter converts a series of light intensity readings to color images. Modern approaches try to jointly solve these problems, i.e., joint denoising-demosaicking, which is an inherently ill-posed problem given that two-thirds of the intensity information is missing and the rest is perturbed by noise. While there are several machine learning systems that have been recently introduced to solve this problem, the majority of them rely on generic network architectures, which do not explicitly consider the physical image model. In this paper, we propose a novel algorithm that is inspired by powerful classical image regularization methods, large-scale optimization, and deep learning techniques. Consequently, our derived iterative optimization algorithm, which involves a trainable denoising network, has a transparent and clear interpretation compared with other black-box data driven approaches. Our extensive experimentation line demonstrates that our proposed method outperforms any previous approaches for both noisy and noise-free data across many different datasets. This improvement in reconstruction quality is attributed to the rigorous derivation of an iterative solution and the principled way we design our denoising network architecture, which as a result requires fewer trainable parameters than the current state-of-the-art solution, and furthermore can be efficiently trained by using a significantly smaller number of training data than existing deep demosaicking networks.","","","10.1109/TIP.2019.2905991","CDISE Departmet of SKOLTECH University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668795","Deep learning;denoising;demosaicking;image restoration;proximal methods;majorization-minimization","Noise reduction;Image reconstruction;Noise measurement;Pipelines;Image color analysis;Training","cameras;computer vision;image colour analysis;image denoising;image reconstruction;image restoration;image segmentation;iterative methods;learning (artificial intelligence);object recognition;optimisation","light intensity readings;intensity information;machine learning systems;generic network architectures;physical image model;large-scale optimization;deep learning techniques;trainable denoising network;black-box data;noise-free data;denoising network architecture;deep demosaicking networks;iterative joint image demosaicking;residual denoising network;modern digital cameras;sequential execution;joint denoising-demosaicking;image regularization methods;iterative optimization algorithm;image processing steps","","","56","","","","","IEEE","IEEE Journals"
"EEG-Based Age and Gender Prediction Using Deep BLSTM-LSTM Network Model","P. Kaushik; A. Gupta; P. P. Roy; D. P. Dogra","Department of Computer Science and Engineering, Indian Institute of Technology, Roorkee, India; Department of Computer Science and Engineering, Indian Institute of Technology, Roorkee, India; Department of Computer Science and Engineering, Indian Institute of Technology, Roorkee, India; School of Electrical Sciences, IIT Bhubaneswar, Bhubaneswar, India","IEEE Sensors Journal","","2019","19","7","2634","2641","With the rapid development of brain-computer interfaces (BCI), the number of applications that use BCI technology is increasingly thick and fast. Prediction of age and gender of a person through EEG analysis is a new application of BCI that has been proposed in this paper. An industry standard EEG recording device has been used to record cerebral activities of 60 subjects (both male and female) in relaxed position with closed eyes. Deep BLSTM-LSTM network has been used to construct a hybrid learning framework for the aforementioned analysis. Accuracy of 93.7% and 97.5% have been recorded for age and gender classification problems respectively. These values are better than the state-of-the-art methods. Our analysis also reveals that the beta band frequencies are better in predicting the age and gender as compared to other frequency bands of the EEG signals. The proposed method has several applications, including biometric, health-care, entertainment, and targeted advertisements.","","","10.1109/JSEN.2018.2885582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8567950","Age detection;BCI;BLSTM;deep learning;EEG;gender detection","Electroencephalography;Brain modeling;Feature extraction;Support vector machines;Sensors;Task analysis","brain;brain-computer interfaces;electroencephalography;learning (artificial intelligence);medical signal processing;neurophysiology;signal classification","deep BLSTM-LSTM network model;brain-computer interfaces;BCI technology;EEG analysis;industry standard EEG recording device;cerebral activities;hybrid learning framework;gender classification problems;beta band frequencies;frequency bands;EEG-based age prediction;EEG-based gender prediction;EEG signals","","1","49","","","","","IEEE","IEEE Journals"
"A Hierarchical Deep Domain Adaptation Approach for Fault Diagnosis of Power Plant Thermal System","X. Wang; H. He; L. Li","Department of Computer, North China Electric Power University, Baoding, China; Department of Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, USA; Department of Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, USA","IEEE Transactions on Industrial Informatics","","2019","15","9","5139","5148","Fault diagnosis of a thermal system under varying operating conditions is of great importance for the safe and reliable operation of a power plant involved in peak shaving. However, it is a difficult task due to the lack of sufficient labeled data under some operating conditions. In practical applications, the model built on the labeled data under one operating condition will be extended to such operating conditions. Data distribution discrepancy can be triggered by variation of operating conditions and may degenerate the performance of the model. Considering the fact that data distributions are different but related under different operating conditions, this paper proposes a hierarchical deep domain adaptation (HDDA) approach to transfer a classifier trained on labeled data under one loading condition to identify faults with unlabeled data under another loading condition. In HDDA, a hierarchical structure is developed to reveal the effective information for final diagnosis by layerwisely capturing representative features. HDDA learns domain-invariant and discriminative features with the hierarchical structure by reducing distribution discrepancy and preserving discriminative information hidden in raw process data. For practical applications, the Taguchi method is used to obtain the optimized model parameters. Experimental results and comprehensive comparison analysis demonstrate its superiority.","","","10.1109/TII.2019.2899118","Natural Science Foundation of Hebei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641448","Classification;domain adaptation (DA);deep learning;fault diagnosis;power plant;thermal system","Feature extraction;Fault diagnosis;Adaptation models;Power generation;Data models;Load modeling;Data mining","data handling;fault diagnosis;learning (artificial intelligence);pattern classification;power engineering computing;Taguchi methods;thermal power stations","hierarchical deep domain adaptation approach;fault diagnosis;power plant thermal system;operating condition;data distributions;loading condition;hierarchical structure;HDDA learning;discriminative features;domain-invariant features;Taguchi method","","1","35","Traditional","","","","IEEE","IEEE Journals"
"MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation","X. Zhang; Y. Sugano; M. Fritz; A. Bulling","Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany; Graduate School of Information Science and Technology, Osaka University, Osaka, Japan; Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany; Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","1","162","175","Learning-based methods are believed to work well for unconstrained gaze estimation, i.e. gaze estimation from a monocular RGB camera without assumptions regarding user, environment, or camera. However, current gaze datasets were collected under laboratory conditions and methods were not evaluated across multiple datasets. Our work makes three contributions towards addressing these limitations. First, we present the MPIIGaze dataset, which contains 213,659 full face images and corresponding ground-truth gaze positions collected from 15 users during everyday laptop use over several months. An experience sampling approach ensured continuous gaze and head poses and realistic variation in eye appearance and illumination. To facilitate cross-dataset evaluations, 37,667 images were manually annotated with eye corners, mouth corners, and pupil centres. Second, we present an extensive evaluation of state-of-the-art gaze estimation methods on three current datasets, including MPIIGaze. We study key challenges including target gaze range, illumination conditions, and facial appearance variation. We show that image resolution and the use of both eyes affect gaze estimation performance, while head pose and pupil centre information are less informative. Finally, we propose GazeNet, the first deep appearance-based gaze estimation method. GazeNet improves on the state of the art by 22 percent (from a mean error of 13.9 degrees to 10.8 degrees) for the most challenging cross-dataset evaluation.","","","10.1109/TPAMI.2017.2778103","Cluster of Excellence on Multimodal Computing and Interaction (MMCI); Saarland University; Alexander von Humboldt Postdoctoral Fellowship; JST CREST Research; Max Planck Institute for Informatics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8122058","Unconstrained gaze estimation;cross-dataset evaluation;convolutional neural network;deep learning","Estimation;Head;Lighting;Cameras;Magnetic heads;Three-dimensional displays;Data models","cameras;computer vision;convolution;face recognition;gaze tracking;image colour analysis;image resolution;learning (artificial intelligence);motion estimation;neural nets;object detection;pose estimation","target gaze range;cross-dataset evaluation;face images;extensive evaluation;eye corners;cross-dataset evaluations;eye appearance;continuous gaze;experience sampling approach;corresponding ground-truth gaze positions;MPIIGaze dataset;multiple datasets;laboratory conditions;current gaze datasets;monocular RGB camera;unconstrained gaze estimation;learning-based methods;real-world dataset;deep appearance-based gaze estimation method;gaze estimation performance;facial appearance variation;illumination conditions","","4","77","","","","","IEEE","IEEE Journals"
"Spectrum Sharing in Vehicular Networks Based on Multi-Agent Reinforcement Learning","L. Liang; H. Ye; G. Y. Li","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Journal on Selected Areas in Communications","","2019","37","10","2282","2292","This paper investigates the spectrum sharing problem in vehicular networks based on multi-agent reinforcement learning, where multiple vehicle-to-vehicle (V2V) links reuse the frequency spectrum preoccupied by vehicle-to-infrastructure (V2I) links. Fast channel variations in high mobility vehicular environments preclude the possibility of collecting accurate instantaneous channel state information at the base station for centralized resource management. In response, we model the resource sharing as a multi-agent reinforcement learning problem, which is then solved using a fingerprint-based deep Q-network method that is amenable to a distributed implementation. The V2V links, each acting as an agent, collectively interact with the communication environment, receive distinctive observations yet a common reward, and learn to improve spectrum and power allocation through updating Q-networks using the gained experiences. We demonstrate that with a proper reward design and training mechanism, the multiple V2V agents successfully learn to cooperate in a distributed way to simultaneously improve the sum capacity of V2I links and payload delivery rate of V2V links.","","","10.1109/JSAC.2019.2933962","Intel Corporation; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792382","Vehicular networks;distributed spectrum access;spectrum and power allocation;multi-agent reinforcement learning","Resource management;Vehicle-to-everything;Reliability;Safety;Fading channels;Reinforcement learning;3GPP","frequency allocation;learning (artificial intelligence);multi-agent systems;telecommunication computing;vehicular ad hoc networks;wireless channels","vehicular networks;spectrum sharing problem;frequency spectrum;fast channel variations;high mobility vehicular environments;accurate instantaneous channel state information;base station;centralized resource management;multiagent reinforcement learning problem;fingerprint-based deep Q-network method;multiple V2V agents;vehicle-to-vehicle links;vehicle-to-infrastructure links","","3","37","CCBY","","","","IEEE","IEEE Journals"
"Decision Directed Channel Estimation Based on Deep Neural Network  $k$ -Step Predictor for MIMO Communications in 5G","M. Mehrabi; M. Mohammadkarimi; M. Ardakani; Y. Jing","Faculty of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Faculty of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Faculty of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Faculty of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada","IEEE Journal on Selected Areas in Communications","","2019","37","11","2443","2456","We consider the use of deep neural network (DNN) to develop a decision-directed (DD)-channel estimation (CE) algorithm for multiple-input multiple-output (MIMO)-space-time block coded systems in highly dynamic vehicular environments. We propose the use of DNN for k -step channel prediction for space-time block code (STBC), and show that deep learning (DL)-based DD-CE can remove the need for Doppler rate estimation in fast time-varying quasi stationary channels, where the Doppler rate varies from one packet to another. Doppler rate estimation in this kind of vehicular channels is remarkably challenging and requires a large number of pilots and preambles, leading to lower power and spectral efficiency. We train two DNNs which learn the real and imaginary parts of the MIMO fading channels over a wide range of Doppler rates. We demonstrate that by these DNNs, DD-CE can be realized with only priori knowledge about Doppler rate range and not the exact value. For the proposed DD-CE algorithm, we also analytically derive the maximum likelihood (ML) decoding algorithm for STBC transmission. The proposed DL-based DD-CE is a promising solution for reliable communication over vehicular MIMO fading channels without accurate mathematical models. This is because DNNs can intelligently learn the statistics of the fading channels. Our simulation results show that the proposed DL-based DD-CE algorithm exhibits lower error propagation compared to existing DD-CE algorithms which require perfect knowledge of the Doppler rate.","","","10.1109/JSAC.2019.2934004","Huawei Innovation Research Program (HIRP); TELUS Communications Company; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798971","MIMO communication;channel estimation;deep learning;decision directed;mmWave communications","Doppler effect;Channel estimation;MIMO communication;Fading channels;Mathematical model;Estimation;Prediction algorithms","5G mobile communication;channel coding;channel estimation;Doppler effect;fading channels;learning (artificial intelligence);maximum likelihood decoding;MIMO communication;neural nets;radio spectrum management;space-time block codes;telecommunication computing;telecommunication network reliability;telecommunication power management;time-varying channels","deep neural network k -step predictor;highly dynamic vehicular environments;Doppler rate estimation;vehicular channels;Doppler rate range;vehicular MIMO fading channels;DD-CE algorithm exhibits;DD-CE algorithms;time-varying quasistationary channels;5G networks;decision directed channel estimation;MIMO communications;space-time block coded systems;STBC;spectral efficiency;maximum likelihood decoding algorithm","","1","31","","","","","IEEE","IEEE Journals"
"Deep Neural Network Initialization With Decision Trees","K. D. Humbird; J. L. Peterson; R. G. Mcclarren","Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Department of Aerospace and Mechanical Engineering, University of Notre Dame, Notre Dame, IN, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","5","1286","1295","In this paper, a novel, automated process for constructing and initializing deep feedforward neural networks based on decision trees is presented. The proposed algorithm maps a collection of decision trees trained on the data into a collection of initialized neural networks with the structures of the networks determined by the structures of the trees. The tree-informed initialization acts as a warm-start to the neural network training process, resulting in efficiently trained, accurate networks. These models, referred to as “deep jointly informed neural networks” (DJINN), demonstrate high predictive performance for a variety of regression and classification data sets and display comparable performance to Bayesian hyperparameter optimization at a lower computational cost. By combining the user-friendly features of decision tree models with the flexibility and scalability of deep neural networks, DJINN is an attractive algorithm for training predictive models on a wide range of complex data sets.","","","10.1109/TNNLS.2018.2869694","U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478232","Decision trees;multilayer neural networks;neural networks","Biological neural networks;Decision trees;Vegetation;Neurons;Training;Data models","decision trees;feedforward neural nets;learning (artificial intelligence);optimisation;pattern classification;regression analysis","automated process;deep feedforward neural networks;tree-informed initialization;neural network training process;deep jointly informed neural networks;classification data sets;decision tree models;neural network initialization;regression method;Bayesian hyperparameter optimization","","1","46","","","","","IEEE","IEEE Journals"
"An Efficient Residual Learning Neural Network for Hyperspectral Image Superresolution","W. Liu; J. Lee","Artificial Intelligence Lab, Department of Computer Engineering, Chonbuk National University, Jeonju-si, South Korea; Artificial Intelligence Lab, Department of Computer Engineering, Chonbuk National University, Jeonju-si, South Korea","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","4","1240","1253","Deep learning, especially a discriminative model for image reconstruction, has shown great potential for single image superresolution (SR) of hyperspectral images (HSI). For HSI SR task, it is crucial to predicting each pixel according to the surrounding context, exploiting both spatial and spectral correlation information simultaneously. In this paper, an efficient three-dimensional (3-D) HSI SR convolution neural network (CNN) based on residual learning is proposed. The network builds convolutional layers in low-resolution (LR) space and extracts the features along both spatial and spectral dimensions using 3-D dilated kernel. Then, 3-D deconvolution is employed at the last layer, which enlarges the image to the desired size. By employing multibranch and multiscale fusion in the architecture, the network can learn a better and more complex LR to high-resolution mapping. The overall network combines the global with local residual learning to reduce training difficulty and improve the performance. The design philosophy of our model is to find the best tradeoff between performance and computational cost. We train the network in an end-to-end fashion, and the experimental results of the quantitative and qualitative evaluation show that our proposed method yields satisfactory SR performance.","","","10.1109/JSTARS.2019.2901752","Korea Energy Technology Evaluation and Planning; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668479","Convolutional neural network (CNN);hyperspectral image (HSI);residual learning;superresolution (SR);three-dimensional (3-D) deconvolution;3-D dilated convolution;3-D subpixel convolution","Convolution;Spatial resolution;Kernel;Image reconstruction;Neural networks;Hyperspectral imaging","convolutional neural nets;geophysical image processing;geophysical signal processing;hyperspectral imaging;image reconstruction;image resolution;learning (artificial intelligence)","efficient residual learning neural network;hyperspectral image superresolution;deep learning;discriminative model;image reconstruction;great potential;single image superresolution;hyperspectral images;HSI SR task;surrounding context;spatial correlation information;spectral correlation information;HSI SR convolution neural network;convolutional layers;low-resolution space;spatial dimensions;spectral dimensions;3-D dilated kernel;multibranch fusion;multiscale fusion;complex LR;high-resolution mapping;local residual learning;satisfactory SR performance","","","55","","","","","IEEE","IEEE Journals"
"Pattern Sensitive Prediction of Traffic Flow Based on Generative Adversarial Framework","Y. Lin; X. Dai; L. Li; F. Wang","State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Qingdao Academy of Intelligent Industries, Qingdao, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Intelligent Transportation Systems","","2019","20","6","2395","2400","Traffic flow prediction is one of the most popular topics in the field of the intelligent transportation system due to its importance. Powered by advanced machine learning techniques, especially the deep learning method, prediction accuracy noticeably increases in recent years. However, most existing methods applied a data-driven paradigm and tend to ignore the outliers, which result in poor performance while handling burst phenomena in the traffic system. To overcome this problem, the prediction model needs to recognize different patterns and handle them in different ways. In this paper, we propose a new prediction model (called pattern sensitive network) that can handle different traffic patterns automatically. By using adversarial training, our model can make more accurate predictions in unusual states without compromising its performance in usual states. Experiments demonstrate that our method can work well in both usual traffic states and unusual traffic states.","","","10.1109/TITS.2018.2857224","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8438991","Traffic flow prediction;deep learning;generative adversarial network","Predictive models;Biological system modeling;Data models;Automation;Machine learning;Industries","intelligent transportation systems;learning (artificial intelligence);road traffic","data-driven paradigm;burst phenomena;traffic system;prediction model;pattern sensitive network;different traffic patterns;adversarial training;usual traffic states;unusual traffic states;pattern sensitive prediction;generative adversarial framework;traffic flow prediction;intelligent transportation system;advanced machine learning techniques;deep learning method;prediction accuracy","","2","46","","","","","IEEE","IEEE Journals"
"Patch orientation-specified network for learning-based image super-resolution","S. B. Yoo; M. Han","Electronics and Telecommunications Research Institute (ETRI), Intelligent Convergence Research Laboratory, Daejeon, Republic of Korea; Electronics and Telecommunications Research Institute (ETRI), Intelligent Convergence Research Laboratory, Daejeon, Republic of Korea","Electronics Letters","","2019","55","23","1233","1235","Learning-based image super-resolution is considered as a promising solution to reconstruct a high-resolution image from a low-resolution image. To improve the super-resolution performance dramatically, this Letter focuses on the effect of training dataset on the performance and proposes an image super-resolution scheme based on patch orientation-specified network. In particular, a deep neural network is trained using patches with a specific orientation and angular transformation is combined with the neural network to cope with various orientations in input patches. Experimental results show the suggested network model is superior to existing state-of-the-art super-resolution alternatives.","","","10.1049/el.2019.1219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8896716","","","image resolution;learning (artificial intelligence);neural nets","super-resolution performance;image super-resolution scheme;patch orientation-specified network;deep neural network;existing state-of-the-art super-resolution alternatives;learning-based image super-resolution;high-resolution image;low-resolution image","","","8","","","","","IET","IET Journals"
"A Deep Scene Representation for Aerial Scene Classification","X. Zheng; Y. Yuan; X. Lu","Key Laboratory of Spectral Imaging Technology CAS, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China; Center for Optical Imagery Analysis and Learning, School of the Computer Science, Northwestern Polytechnical University, Xi’an, China; Key Laboratory of Spectral Imaging Technology CAS, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","7","4799","4809","As a fundamental problem in earth observation, aerial scene classification tries to assign a specific semantic label to an aerial image. In recent years, the deep convolutional neural networks (CNNs) have shown advanced performances in aerial scene classification. The successful pretrained CNNs can be transferable to aerial images. However, global CNN activations may lack geometric invariance and, therefore, limit the improvement of aerial scene classification. To address this problem, this paper proposes a deep scene representation to achieve the invariance of CNN features and further enhance the discriminative power. The proposed method: 1) extracts CNN activations from the last convolutional layer of pretrained CNN; 2) performs multiscale pooling (MSP) on these activations; and 3) builds a holistic representation by the Fisher vector method. MSP is a simple and effective multiscale strategy, which enriches multiscale spatial information in affordable computational time. The proposed representation is particularly suited at aerial scenes and consistently outperforms global CNN activations without requiring feature adaptation. Extensive experiments on five aerial scene data sets indicate that the proposed method, even with a simple linear classifier, can achieve the state-of-the-art performance.","","","10.1109/TGRS.2019.2893115","State Key Program of National Natural Science of China; National Natural Science Foundation of China; Chinese Academy of Sciences; State Key Laboratory of Transient Optics and Photonics; CAS “Light of West China” Program; Xi’an Postdoctoral Innovation Base Scientific Research Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636541","Aerial scene classification;convolutional neural networks (CNNs);Fisher vector (FV);multiscale representation","Feature extraction;Encoding;Strain;Task analysis;Remote sensing;Training;Semantics","convolutional neural nets;geophysical image processing;image classification;image representation;learning (artificial intelligence);remote sensing;vectors","aerial scene classification;aerial image;global CNN activations;deep scene representation;aerial scene data sets;deep convolutional neural networks;deep CNN;multiscale pooling;Fisher vector method;Earth observation","","2","53","","","","","IEEE","IEEE Journals"
"Large Scale Image Segmentation with Structured Loss Based Deep Learning for Connectome Reconstruction","J. Funke; F. Tschopp; W. Grisaitis; A. Sheridan; C. Singh; S. Saalfeld; S. C. Turaga","Institut de Robotica i Informatica Industrial, Universitat Politecnica de Catalunya, Barcelona, Spain; Institut fur Neuroinformatik UZH/ETH, Zurich, Switzerland; Turaga Lab, Janelia Research Campus, Howard Hughes Medical Institute, Ashburn, VA; Turaga Lab, Janelia Research Campus, Howard Hughes Medical Institute, Ashburn, VA; Turaga Lab, Janelia Research Campus, Howard Hughes Medical Institute, Ashburn, VA; Saalfeld Lab, Janelia Research Campus, Howard Hughes Medical Institute, Ashburn, VA; Turaga Lab, Janelia Research Campus, Howard Hughes Medical Institute, Ashburn, VA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","7","1669","1680","We present a method combining affinity prediction with region agglomeration, which improves significantly upon the state of the art of neuron segmentation from electron microscopy (EM) in accuracy and scalability. Our method consists of a 3D U-Net, trained to predict affinities between voxels, followed by iterative region agglomeration. We train using a structured loss based on Malis, encouraging topologically correct segmentations obtained from affinity thresholding. Our extension consists of two parts: First, we present a quasi-linear method to compute the loss gradient, improving over the original quadratic algorithm. Second, we compute the gradient in two separate passes to avoid spurious gradient contributions in early training stages. Our predictions are accurate enough that simple learning-free percentile-based agglomeration outperforms more involved methods used earlier on inferior predictions. We present results on three diverse EM datasets, achieving relative improvements over previous results of 27, 15, and 250 percent. Our findings suggest that a single method can be applied to both nearly isotropic block-face EM data and anisotropic serial sectioned EM data. The runtime of our method scales linearly with the size of the volume and achieves a throughput of ~2.6 seconds per megavoxel, qualifying our method for the processing of very large datasets.","","","10.1109/TPAMI.2018.2835450","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8364622","Connectomics;electron microscopy;deep learning;structured loss;segmentation;affinity prediction;agglomeration","Three-dimensional displays;Training;Image segmentation;Image reconstruction;Neurons;Microscopy;Prediction algorithms","electron microscopy;gradient methods;image segmentation;learning (artificial intelligence);medical image processing;neural net architecture;neurophysiology;stereo image processing","structured loss;connectome reconstruction;neuron segmentation;iterative region agglomeration;quasilinear method;image segmentation;3D electron microscopy;3D U-Net architecture;biological nervous systems;Malis loss function","","","19","","","","","IEEE","IEEE Journals"
"An End-to-End Load Balancer Based on Deep Learning for Vehicular Network Traffic Control","J. Li; G. Luo; N. Cheng; Q. Yuan; Z. Wu; S. Gao; Z. Liu","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Electrical and Computer Engineering Department, University of Waterloo, Waterloo, Canada; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Internet of Things Journal","","2019","6","1","953","966","The infrastructure to vehicle (I2V) communication boosts a large number of prevailing vehicular services, which can provide vehicles with external information, storage, and computing power located at both mobile edge server (MES) and remote cloud. However, vehicle distribution is imbalanced due to the spatial inhomogeneity and temporal dynamics. As a consequence, the communication load for MES is imbalanced and vehicles may suffer from poor I2V communications where the MES is overloaded. In this paper, we propose a novel proactively load balancing approach that enables efficient cooperation among MESs, which is referred to as end-to-end load balancer (E2LB). E2LB schedules the cached data among MESs based on the predicted road traffic situation. First, a convolutional neural network (CNN) is applied to efficiently learn the spatio-temporal correlation in order to predict the road traffic situation. Then, we formulate the load balancing problem as a nonlinear programming (NLP) problem and a novel framework based on CNN is adopted to approximate the NLP optimization. Finally, we connect the above neural networks into an end-to-end neural network to jointly optimize the performance, where the input is the historical traffic situation while the output is the balanced scheduling solution. E2LB can guarantee the real-time scheduling, since the calling of a well-trained neural network only requires a small number of simple operations. Experiments on the trajectories of taxis and buses in Beijing demonstrate the efficiency and effectiveness of E2LB.","","","10.1109/JIOT.2018.2866435","National Science and Technology Major Project of the Ministry of Science and Technology of China; Natural Science Foundation of Beijing Municipality; National Natural Science Foundation of China; BUPT Excellent Ph.D. Students Foundation; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8443082","Convolutional neural network (CNN);deep learning;end-to-end;load balance;network traffic control","Load management;Quality of service;Neural networks;Computer architecture;Real-time systems;Correlation;Microprocessors","learning (artificial intelligence);neural nets;nonlinear programming;resource allocation;road traffic;scheduling","vehicle distribution;communication load;poor I2V communications;end-to-end load balancer;convolutional neural network;load balancing problem;end-to-end neural network;historical traffic situation;balanced scheduling solution;vehicular network traffic control;vehicular services;remote cloud;road traffic situation;E2LB schedules;proactively load balancing approach;nonlinear programming problem;cached data","","2","45","","","","","IEEE","IEEE Journals"
"Cross-Data Set Hyperspectral Image Classification Based on Deep Domain Adaptation","X. Ma; X. Mou; J. Wang; X. Liu; H. Wang; B. Yin","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; School of Information Science and Technology, Dalian Maritime University, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","12","10164","10174","For hyperspectral image classification, there is a large gap between the theoretical method and the practical application. Hyperspectral image classification in theoretical research trains a new classifier for each data set, which is ineffective and even infeasible in large-scale applications. In this paper, we make a preliminary attempt to recycle the classification model to new data sets in an unsupervised way. Specially, we propose a cross-data set hyperspectral image classification method based on deep domain adaptation. The proposed method contains three modules: domain alignment module that learns to minimize the domain discrepancy with the guide of an irrelevant task, task allocation module that learns to classify on the source domain with the regulation of domain alignment, and domain adaptation module that transfers both the alignment ability and classification ability to the target domain by an adaptation strategy. As a result, with the information of an irrelevant task on dual-domain data sets, we can minimize the domain discrepancy and transfer the task-relevant knowledge from the source domain to the target domain in an unsupervised way. Extensive experiments on three hyperspectral images demonstrate the effectiveness of our method compared with other related methods when dealing with new data sets.","","","10.1109/TGRS.2019.2931730","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Natural Science Foundation of Liaoning Province; Northwestern Polytechnical University; Dalian Science and Technology Innovation Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809350","Cross-data set classification;domain adaptation;hyperspectral image;neural networks","Task analysis;Hyperspectral imaging;Training;Deep learning;Feature extraction","geophysical image processing;image classification;learning (artificial intelligence)","deep domain adaptation;classification model;cross-data set hyperspectral image classification method;domain alignment module;domain discrepancy;task allocation module;source domain;domain adaptation module;alignment ability;classification ability;adaptation strategy;dual-domain data sets","","","43","IEEE","","","","IEEE","IEEE Journals"
"Quadruplet Network With One-Shot Learning for Fast Visual Object Tracking","X. Dong; J. Shen; D. Wu; K. Guo; X. Jin; F. Porikli","Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Alibaba Group, Hangzhou, China; Key Laboratory of CAD&CG, Zhejiang University, Hangzhou, China; Research School of Engineering, The Australia National University, Canberra, ACT, Australia","IEEE Transactions on Image Processing","","2019","28","7","3516","3527","In the same vein of discriminative one-shot learning, Siamese networks allow recognizing an object from a single exemplar with the same class label. However, they do not take advantage of the underlying structure of the data and the relationship among the multitude of samples as they only rely on the pairs of instances for training. In this paper, we propose a new quadruplet deep network to examine the potential connections among the training instances, aiming to achieve a more powerful representation. We design a shared network with four branches that receive a multi-tuple of instances as inputs and are connected by a novel loss function consisting of pair loss and triplet loss. According to the similarity metric, we select the most similar and the most dissimilar instances as the positive and negative inputs of triplet loss from each multi-tuple. We show that this scheme improves the training performance. Furthermore, we introduce a new weight layer to automatically select suitable combination weights, which will avoid the conflict between triplet and pair loss leading to worse performance. We evaluate our quadruplet framework by model-free tracking-by-detection of objects from a single initial exemplar in several visual object tracking benchmarks. Our extensive experimental analysis demonstrates that our tracker achieves superior performance with a real-time processing speed of 78 frames/s. Our source code is available.","","","10.1109/TIP.2019.2898567","Natural Science Foundation of Beijing Municipality; Key Research and Development Program of Zhejiang Province; National Natural Science Foundation of China; Australian Research Council; Fok Ying-Tong Education Foundation for Young Teachers; Beijing Municipal Commission of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8638788","Quadruplet deep network;visual object tracking;Siamese deep network","Training;Visualization;Object tracking;Correlation;Real-time systems;Task analysis;Image processing","image representation;learning (artificial intelligence);object tracking","one-shot learning;fast visual object tracking;Siamese networks;single exemplar;class label;quadruplet deep network;potential connections;training instances;shared network;multituple;pair loss;triplet loss;similarity metric;dissimilar instances;positive inputs;negative inputs;training performance;weight layer;quadruplet framework;single initial exemplar;visual object tracking benchmarks;loss function;combination weights","","7","51","","","","","IEEE","IEEE Journals"
"Open-World Person Re-Identification With Deep Hash Feature Embedding","Y. Zhao; Y. Li; S. Wang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Signal Processing Letters","","2019","26","12","1758","1762","Most existing person re-identification (re-id) methods are designed based on the artificial closed-set assumption that the probe and gallery identities are exactly overlapped with a small search pool. This leads to poor scalability in real-world applications where the task is often to re-id a small set of target people (i.e., watch-list) among a large search pool with unknown ID overlap, namely, an open-set deployment setting. In this paper, we firstly propose a new person re-id setting called Watch-List based Open-Set (WLOS) person re-id, which is characterised by the above open-set deployment and a watch-list available at the training stage. Then, we address such a under-studied WLOS problem by formulating a novel Task Dedicated Deep Hashing (TDDH) approach which learning a purpose-specific deep hash model particularly for the given target people in an efficient end-to-end manner. Extensive experiments on three large-scale re-id benchmarks are conducted to demonstrate the advantages and superiority of the TDDH over a wide range of the state-of-the-art hashing and re-id methods under the more realistic open-set setting.","","","10.1109/LSP.2019.2946965","State Key Development Program in 13th Five-Year; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8876675","Person re-identification;deep hashing;open-set;large-scale","Probes;Task analysis;Training;Cameras;Measurement;Benchmark testing","cryptography;image matching;learning (artificial intelligence)","watch-list;novel Task Dedicated Deep Hashing approach;purpose-specific deep hash model;large-scale re-id benchmarks;re-id methods;realistic open-set setting;Open-World Person Re-Identification;Deep hash feature;closed-set assumption;gallery identities;search pool;real-world applications;unknown ID overlap;open-set deployment setting;person re-id setting;Open-Set person re-id","","","15","IEEE","","","","IEEE","IEEE Journals"
"Deep supervised hashing network with integrated regularisation","J. Liao; B. Li; D. Yang; J. Wang; Q. Qi; J. Wang","Institute of Network Technology, Beijing University of Post and Telecommunications, People's Republic of China; Institute of Network Technology, Beijing University of Post and Telecommunications, People's Republic of China; China Unicom Software Research Institute, People's Republic of China; Institute of Network Technology, Beijing University of Post and Telecommunications, People's Republic of China; Institute of Network Technology, Beijing University of Post and Telecommunications, People's Republic of China; Institute of Network Technology, Beijing University of Post and Telecommunications, People's Republic of China","IET Image Processing","","2019","13","12","2143","2151","Hashing has been widely deployed to approximate nearest neighbour search for large-scale multimedia retrieval tasks due to storage and retrieval efficiency. State-of-the-art supervised hashing methods for image retrieval construct deep structures to simultaneously learn image representation and generate good hash codes, and the key step among them is simultaneously learned feature representation and binary hash code. Existing methods use similarity and regularity loss to train deep hashing systems, but these two functions usually work together but not cooperative, which may lead to inadequate performance of the whole system. In this study, a new method for training deep hashing system to learn compact binary codes is presented. The deep supervised hashing network with integrated regularisation (DSHIR) system develop the zero division restriction as a new part of the loss function, which settles the problem of cooperatively guiding the system generate similarity preserving binary codes. DSHIR system also modifies the similarity handling loss to better extract features from image data, which promotes the performance compared to existing end-to-end deep hashing systems. Experiments show that DSHIR yields about 10 per cent higher mean average precision on CIFAR-10 dataset, and also promote on other evaluation indexes compared with state-of-the-art systems.","","","10.1049/iet-ipr.2018.6644","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870588","","","binary codes;feature extraction;file organisation;image classification;image representation;image retrieval;indexing;information retrieval;learning (artificial intelligence)","compact binary codes;deep hashing system;existing methods;binary hash code;good hash codes;image representation;deep structures;image retrieval;hashing methods;retrieval efficiency;storage;large-scale multimedia retrieval tasks;approximate nearest neighbour search;deep supervised hashing network;state-of-the-art systems;existing end-to-end deep hashing systems;DSHIR system;integrated regularisation system","","","34","","","","","IET","IET Journals"
"Deep Saliency With Channel-Wise Hierarchical Feature Responses for Traffic Sign Detection","C. Li; Z. Chen; Q. M. J. Wu; C. Liu","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada; School of Control Science and Engineering, Shandong University, Jinan, China","IEEE Transactions on Intelligent Transportation Systems","","2019","20","7","2497","2509","Traffic sign detection is challenging in cases of a complex background, occlusions, distortions, and so on. To overcome the above-mentioned challenges, this paper pays close attention to channel-wise feature responses to propose an end-to-end deep learning-based saliency traffic sign detection method. Our model contains three main components: channel-wise coarse feature extraction (CCFE), channel-wise hierarchical feature refinement (CHFR), and hierarchical feature map fusion (HFMF). In addition, it is based on the squeeze-and-excitation-residual network to explicitly model the inter dependences between the channels of its convolution features at a slight computational cost. We first apply CCFE to produce coarse feature maps with much information loss. To make full use of spatial information and fine details, CHFR is executed to refine hierarchical features. After that, HFMF is used to fuse hierarchical feature maps to generate the final traffic sign saliency map. Compared with other five traffic sign detection methods, the experimental results demonstrate the efficiency (a real-time speed) and superior performance of the proposed method according to comprehensive evaluations over three benchmark data sets.","","","10.1109/TITS.2018.2867183","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Research Fund of Guangxi Key Lab of Multi-source Information Mining & Security; Shenzhen Science and Technology Research and Development Funds; Shandong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8476588","Deep saliency;channel-wise feature responses;squeeze-and-excitation-residual network;hierarchical feature refinement;traffic sign detection","Feature extraction;Visualization;Image color analysis;Convolutional neural networks;Saliency detection;Kernel;Shape","feature extraction;image colour analysis;image segmentation;learning (artificial intelligence);object detection;traffic engineering computing","deep saliency;channel-wise hierarchical feature responses;end-to-end deep learning-based saliency traffic sign detection method;hierarchical feature map fusion;convolution features;coarse feature maps;traffic sign detection methods;channel-wise coarse feature extraction;traffic sign saliency map;squeeze-and-excitation-residual network;CCFE;HFMF;CHFR","","1","33","","","","","IEEE","IEEE Journals"
"High-Accuracy Entity State Prediction Method Based on Deep Belief Network Toward IoT Search","P. Zhang; X. Kang; D. Wu; R. Wang","School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Wireless Communications Letters","","2019","8","2","492","495","The state of physical entity in the Internet of Things (IoT) has an obvious time-varying characteristic. Preliminarily selecting candidate entities by predicting their current state when searching match entities from massive ones can effectively reduce the communication overhead of IoT search system. The existing methods are all based on shallow learning theories whose performances are very limited. Thus, a high-accuracy entity state prediction method (HESPM) based on deep learning theory is proposed. The model of HESPM is built by utilizing the deep belief network. Then the contrastive divergence algorithm is adopted to train the model. Therefore, the dynamic evolution trend of entity state can be accurately perceived and the future entity state can be precisely predicted. Simulation results demonstrate the effectiveness of HESPM in enhancing the prediction accuracy and communication overhead performances.","","","10.1109/LWC.2018.2877639","National Natural Science Foundation of China; Program for Innovation Team Building at Institutions of Higher Education in Chongqing; Chongqing Municipal Education Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502822","IoT;IoT search;entity state prediction;DBN","Neurons;Probability distribution;Artificial neural networks;Internet of Things;Predictive models;Logic gates","belief networks;Internet of Things;learning (artificial intelligence);search problems","deep belief network;physical entity;candidate entities;searching match entities;IoT search system;high-accuracy entity state prediction method;deep learning theory;prediction accuracy;time-varying characteristic;shallow learning;contrastive divergence algorithm;HESPM;communication overhead","","22","16","","","","","IEEE","IEEE Journals"
"Joint Deep and Depth for Object-Level Segmentation and Stereo Tracking in Crowds","J. Li; L. Wei; F. Zhang; T. Yang; Z. Lu","School of Telecommunications Engineering, Xidian University, Xi’an, China; School of Telecommunications Engineering, Xidian University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Telecommunications Engineering, Xidian University, Xi’an, China","IEEE Transactions on Multimedia","","2019","21","10","2531","2544","Tracking multiple people in crowds is a fundamental and essential task in the multimedia field. It is often hindered by difficulties, such as dynamic occlusion between objects, cluttered background, and abrupt illumination changes. To respond to this need, in this paper, we combine deep and depth to build a stereo tracking system for crowds. The core of the system is the fusion of the advantages of deep learning and depth information, which is exploited to achieve object segmentation and improve the multiobject tracking performance in severe occlusion. More specifically, first, to obtain more accurate detection observations in the tracking system, we present a novel object-level segmentation method. This method combines the effective detection results of deep learning with depth information to obtain precise object segmentation results. Then, we integrate the segmentation results and three-dimensional (3-D) information to extract 2-D and 3-D characteristics to represent the target, and design three similarity models to realize a stereo tracking method through data association in crowds. Finally, we build a diverse stereo dataset including various challenging indoor and outdoor scenes. The comprehensive experiments verify the effective and robust tracking performance of our system in various scenarios, and the system has rich output results including segmentation results, target distance, and tracking results. Moreover, the qualitative and quantitative comparison results show that the proposed algorithm not only has good object segmentation performance but also improves the tracking performance of completely and partially occluded objects, which is superior to the tested state-of-the-art tracking approaches.","","","10.1109/TMM.2019.2908350","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676320","Multiple object tracking;object-level segmentation;stereo vision;severe occlusion","Target tracking;Image segmentation;Three-dimensional displays;Deep learning;Solid modeling;Data models;Trajectory","image motion analysis;image segmentation;object detection;object tracking;stereo image processing","partially occluded objects;fundamental task;multimedia field;dynamic occlusion;cluttered background;abrupt illumination changes;stereo tracking system;deep learning;depth information;multiobject tracking performance;severe occlusion;accurate detection observations;object-level segmentation method;precise object segmentation results;stereo tracking method;diverse stereo dataset;challenging indoor scenes;outdoor scenes;effective tracking performance;robust tracking performance;qualitative comparison results;quantitative comparison results;object segmentation performance","","1","41","Traditional","","","","IEEE","IEEE Journals"
"Cost-Effective Foliage Penetration Human Detection Under Severe Weather Conditions Based on Auto-Encoder/Decoder Neural Network","Y. Huang; Y. Zhong; Q. Wu; E. Dutkiewicz; T. Jiang","Global Big Data Technologies Centre, School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; Global Big Data Technologies Centre, School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; Global Big Data Technologies Centre, School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; Global Big Data Technologies Centre, School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; Key Laboratory of Universal Wireless Communication, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Internet of Things Journal","","2019","6","4","6190","6200","Military surveillance events and rescue activities are vital missions for the Internet-of-Things. To this end, foliage penetration for human detection plays an important role. However, although the feasibility of that mission has been validated, we observe that it still cannot perform promisingly under severe weather conditions, such as rainy, foggy, and snowy days. Therefore, in this paper, experiments are conducted under severe weather conditions based on a proposed deep learning approach. We present an auto-encoder/decoder (Auto-ED) deep neural network that can learn the deep representation and conduct classification task concurrently. Since the property of cost-effective, the device-free sensing techniques are used to address human detection in our case. As we pursue the signal-based mission, two components are involved in the proposed Auto-ED approach. First, an encoder is utilized that encode signal-based inputs into higher dimensional tensors by fractionally strided convolution operations. Then, a decoder is leveraged with convolution operations to extract deep representations and learn the classifier simultaneously. To verify the effectiveness of the proposed approach, we compare it with several machine learning approaches under different weather conditions. Also, a simulation experiment is conducted by adding additive white Gaussian noise to the original target signals with different signal to noise ratios. Experimental results demonstrate that the proposed approach can best tackle the challenge of human detection under severe weather conditions in the high-clutter foliage environment, which indicates its potential application values in the near future.","","","10.1109/JIOT.2018.2878880","National Natural Science Foundation of China; Australian Government Research Training Program Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516280","Auto-encoder and decoder (Auto-ED);deep learning;device-free sensing (DFS);human detection","Meteorology;Sensors;Neural networks;Radio transmitters;Iron;Receivers","AWGN;image coding;Internet of Things;learning (artificial intelligence);neural nets;object detection","signal-based mission;Auto-ED approach;cost-effective foliage penetration human detection;deep learning approach;deep representation;autoencoder/decoder neural network;military surveillance events;Internet-of-Things;rescue activities;fractionally strided convolution operations;additive white Gaussian noise","","","36","","","","","IEEE","IEEE Journals"
"Cross-Modal Surface Material Retrieval Using Discriminant Adversarial Learning","W. Zheng; H. Liu; B. Wang; F. Sun","State Key Laboratory of Reliability and Intelligence of Electrical Equipment, School of Electrical Engineering, Hebei University of Technology, Tianjin, China; Department of Computer Science and Technology, State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; State Key Laboratory of Reliability and Intelligence of Electrical Equipment, School of Electrical Engineering, Hebei University of Technology, Tianjin, China; Department of Computer Science and Technology, State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Industrial Informatics","","2019","15","9","4978","4987","The surface properties of an object play a vital role in the tasks of robotic manipulation or interaction with its surrounding environment. Tactile sensing can provide rich information about the surface properties of an object through physical contact. Hence, how to convey and interpret the tactile information to the user is a significant problem during the human-machine interaction. To this end, a visual-tactile cross-modal retrieval framework is proposed for perceptual estimation by associating tactile information to visual information of material surfaces. Namely, we can use tactile information of an unknown material surface to retrieve perceptually similar surfaces from an available surface visual sample set. For the proposed framework, we develop a discriminant adversarial learning method, which incorporates intramodal discriminant, cross-modal correlation, and intermodal consistency into a deep learning network for common feature representation learning. Experimental results on the publicly available data set show that the proposed framework and the method are effective.","","","10.1109/TII.2019.2895602","National Natural Science Foundation of China; Natural Science Foundation of Hebei Province; Hebei University of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8627975","Cross-modal retrieval;discriminant adversarial learning (DAL);surface material","Visualization;Training;Material properties;Correlation;Gallium nitride;Informatics;Robots","haptic interfaces;human-robot interaction;learning (artificial intelligence);manipulators;neural nets","cross-modal surface material retrieval;robotic manipulation;tactile sensing;tactile information;human-machine interaction;visual-tactile cross-modal retrieval framework;material surfaces;discriminant adversarial learning method;cross-modal correlation;common feature representation learning;deep learning network","","","38","Traditional","","","","IEEE","IEEE Journals"
"Deep Latent Low-Rank Representation for Face Sketch Synthesis","M. Zhang; N. Wang; Y. Li; X. Gao","State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","10","3109","3123","Face sketch synthesis is useful and profitable in digital entertainment. Most existing face sketch synthesis methods rely on the assumption that facial photographs/sketches form a low-dimensional manifold. Once the training data are insufficient, the manifold could not characterize the identity-specific information that is included in a test photograph but excluded in the training data. Thus, the synthesized sketch would lose this information, such as glasses, earrings, hairstyles, and hairpins. To provide the sufficient data and satisfy the assumption on manifold, we propose a novel face sketch synthesis framework based on deep latent low-rank representation (DLLRR) in this paper. The DLLRR induces the hidden training sketches with the identity-specific information as the hidden data to the insufficient original training sketches as the observed data. And it searches the lowest rank representation on the candidates of a test photograph from the both hidden and observed data. For the strong representational capability of the coupled autoencoder, we leverage it to reveal the hidden data. Experiment results on face photograph-sketch database illustrate that the proposed method can successfully provide the sufficient training data with the identity-specific information. And compared to the state of the arts, the proposed method synthesizes more clean and vivid face sketches.","","","10.1109/TNNLS.2018.2890017","National Natural Science Foundation of China; National Key Research and Development Program of China; National High-Level Talents Special Support Program of China; Young Elite Scientists Sponsorship Program by CAST; China Postdoctoral Science Foundation; Opening Project of Science and Technology on Reliability Physics and Application Technology of Electronic Component Laboratory; Natural Science Basic Research Plan in Shaanxi Province of China; Young Talent Fund of the University Association for Science and Technology in Shaanxi, China; Fundamental Research Funds for the Central Universities; CCF-Tencent Open Fund; 111 Project; Yangtse Rive Scholar Bonus Schemes; Ten Thousand Talent Program; Xidian University-Intellifusion Joint Innovation Laboratory of Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8621606","Coupled autoencoder;deep latent low-rank representation (DLLRR);face sketch synthesis","Face;Training;Training data;Manifolds;Dictionaries;Learning systems;Entertainment industry","face recognition;image representation;neural nets","identity-specific information;low-dimensional manifold;face sketch synthesis framework;hidden training sketches;hidden data;insufficient original training sketches;face sketch synthesis methods;representational capability;deep latent low-rank representation;digital entertainment;facial photographs;DLLRR;coupled autoencoder;face photograph-sketch database","","1","39","","","","","IEEE","IEEE Journals"
"Learning From Humans How to Grasp: A Data-Driven Architecture for Autonomous Grasping With Anthropomorphic Soft Hands","C. D. Santina; V. Arapi; G. Averta; F. Damiani; G. Fiore; A. Settimi; M. G. Catalano; D. Bacciu; A. Bicchi; M. Bianchi","Centro di Ricerca “Enrico Piaggio,” and the Dipartimento di Ingegneria dell’Informazione, Università di Pisa, Pisa, Italy; Centro di Ricerca “Enrico Piaggio,”, Università di Pisa, Pisa, Italy; Centro di Ricerca “Enrico Piaggio,” and the Dipartimento di Ingegneria dell’Informazione, Università di Pisa, Pisa, Italy; Centro di Ricerca “Enrico Piaggio,”, Università di Pisa, Pisa, Italy; Centro di Ricerca “Enrico Piaggio,”, Università di Pisa, Pisa, Italy; Centro di Ricerca “Enrico Piaggio,”, Università di Pisa, Pisa, Italy; Soft Robotics for Human Cooperation and Rehabilitation, Fondazione Istituto Italiano di Tecnologia, Genova, Italy; Dipartimento di Informatica, Università di Pisa, Pisa, Italy; Centro di Ricerca “Enrico Piaggio,” and the Dipartimento di Ingegneria dell’Informazione, Università di Pisa, Pisa, Italy; Centro di Ricerca “Enrico Piaggio,” and the Dipartimento di Ingegneria dell’Informazione, Università di Pisa, Pisa, Italy","IEEE Robotics and Automation Letters","","2019","4","2","1533","1540","Soft hands are robotic systems that embed compliant elements in their mechanical design. This enables an effective adaptation with the items and the environment, and ultimately, an increase in their grasping performance. These hands come with clear advantages in terms of ease-to-use and robustness if compared with classic rigid hands, when operated by a human. However, their potential for autonomous grasping is still largely unexplored, due to the lack of suitable control strategies. To address this issue, in this letter, we propose an approach to enable soft hands to autonomously grasp objects, starting from the observations of human strategies. A classifier realized through a deep neural network takes as input the visual information on the object to be grasped, and predicts which action a human would perform to achieve the goal. This information is hence used to select one among a set of human-inspired primitives, which define the evolution of the soft hand posture as a combination of anticipatory action and touch-based reactive grasp. The architecture is completed by the hardware component, which consists of an RGB camera to look at the scene, a 7-DoF manipulator, and a soft hand. The latter is equipped with inertial measurement units at the fingernails for detecting contact with the object. We extensively tested the proposed architecture with 20 objects, achieving a success rate of 81.1% over 111 grasps.","","","10.1109/LRA.2019.2896485","European Union's Horizon 2020 research and innovation; Italian Ministry of Education and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629968","Natural Machine Motion;Deep Learning in Robotics and Automation;Modeling, Control, and Learning for Soft Robots;Grasping","Grasping;Videos;Neural networks;Robot sensing systems;Uncertainty;Computer architecture","cameras;learning (artificial intelligence);manipulators;mobile robots;neural nets;robot vision","control strategies;robotic systems;mechanical design;deep neural network;visual information;anticipatory action;touch-based reactive grasp;RGB camera;7-DoF manipulator;inertial measurement units;fingernails;contact detection;reactive grasp;soft hand posture;human-inspired primitives;human strategies;grasp objects;classic rigid hands;grasping performance;anthropomorphic soft hands;autonomous grasping;data-driven architecture","","1","25","","","","","IEEE","IEEE Journals"
"On the Representational Power of Restricted Boltzmann Machines for Symmetric Functions and Boolean Functions","L. Gu; J. Huang; L. Yang","Guangdong Provincial Key Laboratory of Computational Science, School of Mathematics, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Computational Science, School of Mathematics, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Computational Science, School of Mathematics, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","5","1335","1347","Restricted Boltzmann machines (RBMs) are used to build deep-belief networks that are widely thought to be one of the first effective deep learning neural networks. This paper studies the ability of RBMs to represent distributions over (0, 1in via softplus/hardplus RBM networks. It is shown that any distribution whose density depends on the number of 1's in their input can be approximated with arbitrarily high accuracy by an RBM of size 2n + 1, which improves the result of a previous study by reducing the size from n2 to 2n + 1. A theorem for representing partially symmetric Boolean functions by softplus RBM networks is established. Accordingly, the representational power of RBMs for distributions whose mass represents the Boolean functions is investigated in comparison with that of threshold circuits and polynomial threshold functions. It is shown that a distribution over [0, 1]n whose mass represents a Boolean function can be computed with a given margin δ by an RBM of size and parameters bounded by polynomials in n, if and only if it can be computed by a depth-2 threshold circuit with size and parameters bounded by polynomials in n.","","","10.1109/TNNLS.2018.2868809","National Natural Science Foundation of China; Guangdong Province Key; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8475002","Boolean functions;representational power;restricted Boltzmann machines (RBMs);symmetric functions;threshold circuits","Boolean functions;Learning systems;Markov processes;Computational modeling;Analytical models;Geometry;Probability distribution","belief networks;Boltzmann machines;Boolean functions;learning (artificial intelligence);polynomials","polynomial threshold functions;Boolean function;depth-2 threshold circuit;representational power;restricted Boltzmann machines;symmetric functions;Restricted Boltzmann machines;deep-belief networks;effective deep learning neural networks;softplus/hardplus RBM networks;arbitrarily high accuracy;partially symmetric Boolean functions;softplus RBM networks","","","25","","","","","IEEE","IEEE Journals"
"Learning Converged Propagations With Deep Prior Ensemble for Image Enhancement","R. Liu; L. Ma; Y. Wang; L. Zhang","Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, DUT-RU International School of Information Science & Engineering, Dalian University of Technology, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, DUT-RU International School of Information Science & Engineering, Dalian University of Technology, Dalian, China; Institute of Atmospheric Sciences, Fudan University, Shanghai, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Image Processing","","2019","28","3","1528","1543","Enhancing the visual qualities of images plays very important roles in various vision and learning applications. In the past few years, both knowledge-driven maximum a posterior (MAP) with prior modelings and fully data-dependent convolutional neural network (CNN) techniques have been investigated to address specific enhancement tasks. In this paper, by exploiting the advantages of these two types of mechanisms within a complementary propagation perspective, we propose a unified framework, named deep prior ensemble (DPE) for solving various image enhancement tasks. Specifically, we first establish the basic propagation scheme based on the fundamental image modeling cues and then introduce residual CNNs to help predicting the propagation direction at each stage. By designing prior projections to perform feedback control, we theoretically prove that even with experience-inspired CNNs, DPE is definitely converged and the output will always satisfy our fundamental task constraints. The main advantage against the conventional optimization-based MAP approaches is that our descent directions are learned from collected training data, thus are much more robust to unwanted local minimums. While, compared with existing CNN type networks, which are often designed in heuristic manners without theoretical guarantees, DPE is able to gain advantages from rich task cues investigated on the bases of domain knowledges. Therefore, the DPE actually provides a generic ensemble methodology to integrate both knowledge and data-based cues for different image enhancement tasks. More importantly, our theoretical investigations verify that the feed-forward propagations of DPE are properly controlled toward our desired solution. Experimental results demonstrate that the proposed DPE outperforms the state-of-the-arts on a variety of image enhancement tasks in terms of both quantitative measure and visual perception quality.","","","10.1109/TIP.2018.2875568","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8492421","Image enhancement;visual propagation;prior model;residual CNN;non-convex optimization","Task analysis;Image enhancement;Visualization;Feedback control;Optimization;Neural networks;Image restoration","feedback;feedforward neural nets;gradient methods;image enhancement;learning (artificial intelligence);optimisation","data-dependent convolutional neural network techniques;specific enhancement tasks;complementary propagation perspective;DPE;basic propagation scheme;fundamental image modeling cues;propagation direction;fundamental task constraints;conventional optimization-based MAP approaches;collected training data;rich task cues;generic ensemble methodology;data-based cues;theoretical investigations;feed-forward propagations;visual perception quality;converged propagations;learning applications;knowledge-driven maximum;deep prior ensemble;CNN type networks;image enhancement tasks","","3","64","","","","","IEEE","IEEE Journals"
"Deep Q Learning Driven CT Pancreas Segmentation With Geometry-Aware U-Net","Y. Man; Y. Huang; J. Feng; X. Li; F. Wu","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Medical Imaging","","2019","38","8","1971","1980","The segmentation of pancreas is important for medical image analysis, yet it faces great challenges of class imbalance, background distractions, and non-rigid geometrical features. To address these difficulties, we introduce a deep Q network (DQN) driven approach with deformable U-Net to accurately segment the pancreas by explicitly interacting with contextual information and extract anisotropic features from pancreas. The DQN-based model learns a context-adaptive localization policy to produce a visually tightened and precise localization bounding box of the pancreas. Furthermore, deformable U-Net captures geometry-aware information of pancreas by learning geometrically deformable filters for feature extraction. The experiments on NIH dataset validate the effectiveness of the proposed framework in pancreas segmentation.","","","10.1109/TMI.2019.2911588","Natural Science Foundation of Zhejiang Province; Zhejiang University; Tencent AI Lab Rhino-Bird Joint Research Program; Baidu AI Frontier Technology Joint Research Program; National Basic Research Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8692647","Computational and artificial intelligence;image processing;image segmentation;imaging tomography;computed tomography;pancreas segmentation;DQN;deformable U-net","Pancreas;Image segmentation;Computed tomography;Feature extraction;Three-dimensional displays;Biomedical imaging;Shape","computerised tomography;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;neural nets","geometry-aware U-Net;medical image analysis;nonrigid geometrical features;DQN-based model;context-adaptive localization policy;precise localization bounding box;anisotropic feature extraction;deep Q-learning driven CT pancreas segmentation;class imbalance;background distractions;contextual information;deformable U-Net;geometry-aware information","","3","36","","","","","IEEE","IEEE Journals"
"Space/Aerial-Assisted Computing Offloading for IoT Applications: A Learning-Based Approach","N. Cheng; F. Lyu; W. Quan; C. Zhou; H. He; W. Shi; X. Shen","School of Telecommunication, Xidian University, Xi’an, China; Electrical and Computer Engineering Department, University of Waterloo, Waterloo, Canada; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; Electrical and Computer Engineering Department, University of Waterloo, Waterloo, Canada; School of Information Engineering, Zhejiang University, Hangzhou, China; Electrical and Computer Engineering Department, University of Waterloo, Waterloo, Canada; Electrical and Computer Engineering Department, University of Waterloo, Waterloo, Canada","IEEE Journal on Selected Areas in Communications","","2019","37","5","1117","1129","Internet of Things (IoT) computing offloading is a challenging issue, especially in remote areas where common edge/cloud infrastructure is unavailable. In this paper, we present a space-air-ground integrated network (SAGIN) edge/cloud computing architecture for offloading the computation-intensive applications considering remote energy and computation constraints, where flying unmanned aerial vehicles (UAVs) provide near-user edge computing and satellites provide access to the cloud computing. First, for UAV edge servers, we propose a joint resource allocation and task scheduling approach to efficiently allocate the computing resources to virtual machines (VMs) and schedule the offloaded tasks. Second, we investigate the computing offloading problem in SAGIN and propose a learning-based approach to learn the optimal offloading policy from the dynamic SAGIN environments. Specifically, we formulate the offloading decision making as a Markov decision process where the system state considers the network dynamics. To cope with the system dynamics and complexity, we propose a deep reinforcement learning-based computing offloading approach to learn the optimal offloading policy on-the-fly, where we adopt the policy gradient method to handle the large action space and actor-critic method to accelerate the learning process. Simulation results show that the proposed edge VM allocation and task scheduling approach can achieve near-optimal performance with very low complexity and the proposed learning-based computing offloading algorithm not only converges fast but also achieves a lower total cost compared with other offloading approaches.","","","10.1109/JSAC.2019.2906789","National Natural Science Foundation of China; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672604","Computing offloading;edge computing;space-air-ground;IoT;reinforcement learning","Task analysis;Servers;Internet of Things;Edge computing;Delays;Resource management;Computer architecture","autonomous aerial vehicles;cloud computing;decision making;gradient methods;Internet of Things;learning (artificial intelligence);Markov processes;neural nets;resource allocation;scheduling;virtual machines","flying unmanned aerial vehicles;near-user edge computing;satellites;UAV edge servers;joint resource allocation;task scheduling approach;computing offloading problem;learning-based approach;optimal offloading policy;dynamic SAGIN environments;network dynamics;system dynamics;actor-critic method;space/aerial-assisted computing offloading;IoT applications;common edge/cloud infrastructure;space-air-ground integrated network edge/cloud computing architecture;deep reinforcement learning-based computing offloading approach;Internet of Things computing offloading;virtual machines;Markov decision process;policy gradient method","","15","40","","","","","IEEE","IEEE Journals"
"Learning-Based Wireless Powered Secure Transmission","D. He; C. Liu; H. Wang; T. Q. S. Quek","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore","IEEE Wireless Communications Letters","","2019","8","2","600","603","In this letter, we propose a learning-based wireless powered secure transmission, in which a source utilizes energy harvested from a power beacon to communicate with a legitimate receiver, in the presence of an eavesdropper. In order to confuse the eavesdropper, we assume that the source transmits the artificial noise signals, in addition to the information signals. We first characterize the effective secrecy throughput of our system, showing its dependence on the transmission parameters, including the fraction of time allocated for wireless power transfer, the fraction of power allocated to the information signals, as well as the wiretap code rates. We then leverage the deep feedforward neural network to learn how the optimal transmission parameters that jointly maximize the effective secrecy throughput can be obtained. Through numerical results, we demonstrate that our learning-based scheme can achieve almost the same secrecy performance as the optimal solution obtained from the exhaustive search, while requiring much less computational complexity.","","","10.1109/LWC.2018.2881976","China Scholarship Council; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540070","Wireless power transfer;artificial noise;physical layer security;deep feedforward neural network","Wireless communication;Communication system security;Training;Throughput;Feedforward neural networks;Computational complexity","computational complexity;energy harvesting;feedforward neural nets;learning (artificial intelligence);optimisation;power transmission;telecommunication security;wireless channels","learning-based wireless powered secure transmission;power beacon;artificial noise signals;information signals;wireless power transfer;optimal transmission parameters;learning-based scheme;wiretap code rates;computational complexity;deep feedforward neural network;secrecy throughput","","1","14","","","","","IEEE","IEEE Journals"
"Multimodal medical image registration via common representations learning and differentiable geometric constraints","C. Liu; L. Ma; Z. Lu; X. Jin; J. Xu","Zhejiang University, People's Republic of China; Zhejiang University, People's Republic of China; Zhejiang University, People's Republic of China; 1st Affiliated Hospital of Wenzhou Medical University, People's Republic of China; Huzhou University, People's Republic of China","Electronics Letters","","2019","55","6","316","318","Multimodal medical image registration remains a challenging problem when strong appearance variations and imprecise alignment exist in images. Previous deep network approaches cannot handle such a high degree of variability and prohibit the use of strong geometric constraints. The authors introduce a novel deep architecture that not only produces image representations that are well-suited for this challenging task, but also leverages knowledge of the geometry constraints for robust registration. By enforcing the representations of different modalities living in a common semantic space, they obtain convolutional features tending to respond to object parts consistently across modality. This yields a unique description for all object fragments and allows the end user to know the model's decision process, whereas most existing models remain unclear and difficult to explain. By using the differentiable spatial transformer to compensate transform, they integrate geometric consensus into the cost function to enable end-to-end model optimisation which has not yet been exploited before. The authors evaluate their method on a very challenging medical image dataset. Experiments demonstrate that the proposed method provides a plausible representation and outperforms state-of-the-art approaches by a significant margin.","","","10.1049/el.2018.6713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666210","","","convolutional neural nets;geometry;image registration;image representation;learning (artificial intelligence);medical image processing;optimisation","multimodal medical image registration;differentiable geometric constraints;strong geometric constraints;image representations;differentiable spatial transformer;geometric consensus;end-to-end model optimisation;plausible representation;representations learning;deep network approaches;semantic space;convolutional features;object fragments","","","7","","","","","IET","IET Journals"
"Scene Classification With Recurrent Attention of VHR Remote Sensing Images","Q. Wang; S. Liu; J. Chanussot; X. Li","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Grenoble Images Speech Signals and Automatics Laboratory, Centre National de la Recherche Scientifique, Grenoble Institute of Technology, Grenoble Alpes University, Grenoble, France; School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","2","1155","1167","Scene classification of remote sensing images has drawn great attention because of its wide applications. In this paper, with the guidance of the human visual system (HVS), we explore the attention mechanism and propose a novel end-to-end attention recurrent convolutional network (ARCNet) for scene classification. It can learn to focus selectively on some key regions or locations and just process them at high-level features, thereby discarding the noncritical information and promoting the classification performance. The contributions of this paper are threefold. First, we design a novel recurrent attention structure to squeeze high-level semantic and spatial features into several simplex vectors for the reduction of learning parameters. Second, an end-to-end network named ARCNet is proposed to adaptively select a series of attention regions and then to generate powerful predictions by learning to process them sequentially. Third, we construct a new data set named OPTIMAL-31, which contains more categories than popular data sets and gives researchers an extra platform to validate their algorithms. The experimental results demonstrate that our model makes great promotion in comparison with the state-of-the-art approaches.","","","10.1109/TGRS.2018.2864987","National Key Research and Development Program of China; National Natural Science Foundation of China; Natural Science Foundation of Shaanxi Province; Fundamental Research Funds for the Central Universities; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454883","Attention;convolutional neural network (CNN);deep learning;long short-term memory (LSTM);remote sensing;recurrent neural networks (RNN);scene classification","Remote sensing;Feature extraction;Training;Saliency detection;Machine learning;Electronic mail;Task analysis","convolutional neural nets;feature extraction;geophysical image processing;image classification;learning (artificial intelligence);recurrent neural nets;remote sensing","learning parameters reduction;high-level semantic;recurrent attention structure;spatial features;high-level features;ARCNet;end-to-end attention recurrent convolutional network;human visual system;VHR remote sensing images;scene classification","","24","53","","","","","IEEE","IEEE Journals"
"Learning Bilevel Layer Priors for Single Image Rain Streaks Removal","P. Mu; J. Chen; R. Liu; X. Fan; Z. Luo","School of Mathematical Sciences, Dalian University of Technology, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, Dalian, China; DUT-RU International School of Information Science & Engineering, Dalian University of Technology, Dalian, China","IEEE Signal Processing Letters","","2019","26","2","307","311","Rain streaks removal is an important issue of the outdoor vision system and recently has been investigated extensively. In the past decades, maximum a posterior and network-based architecture have been attracting considerable attention for this problem. However, it is challenging to establish effective regularization priors and the cost function with complex prior is hard to optimize. On the other hand, it is still hard to incorporate data-dependent information into conventional numerical iterations. To partially address the above limits and inspired by the leader-follower gaming perspective, we introduce an unrolling strategy to incorporate data-dependent network architectures into the established iterations, i.e., a learning bilevel layer priors method to jointly investigate the learnable feasibility and optimality of rain streaks removal problem. Both visual and quantitative comparison results demonstrate that our method outperforms the state of the art.","","","10.1109/LSP.2018.2889277","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8586910","Learning-based optimization;rain streaks removal;deep unrolling;bilevel layer priors","Rain;Optimization;Network architecture;Manganese;Convolution;Fans;Visualization","filtering theory;image denoising;image enhancement;image reconstruction;image representation;image restoration;iterative methods;learning (artificial intelligence);rain;unsupervised learning;video signal processing","leader-follower gaming perspective;data-dependent information;cost function;effective regularization priors;maximum a posterior network-based architecture;outdoor vision system;single image rain streaks removal;removal problem;optimality;learning bilevel layer priors method;data-dependent network architectures","","1","37","","","","","IEEE","IEEE Journals"
"Deep FisherNet for Image Classification","P. Tang; X. Wang; B. Shi; X. Bai; W. Liu; Z. Tu","School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Department of Cognitive Science, University of California at San Diego, San Diego, CA, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","7","2244","2250","Despite the great success of convolutional neural networks (CNNs) for the image classification task on data sets such as Cifar and ImageNet, CNN's representation power is still somewhat limited in dealing with images that have a large variation in size and clutter, where Fisher vector (FV) has shown to be an effective encoding strategy. FV encodes an image by aggregating local descriptors with a universal generative Gaussian mixture model (GMM). FV, however, has limited learning capability and its parameters are mostly fixed after constructing the codebook. To combine together the best of the two worlds, we propose in this brief a neural network structure with FV layer being part of an end-to-end trainable system that is differentiable; we name our network FisherNet that is learnable using back propagation. Our proposed FisherNet combines CNN training and FV encoding in a single end-to-end structure. We observe a clear advantage of FisherNet over plain CNN and standard FV in terms of both classification accuracy and computational efficiency on the challenging PASCAL visual object classes object classification and emotion image classification tasks.","","","10.1109/TNNLS.2018.2874657","National Natural Science Foundation of China; National Program for Support of Top-Notch Young Professionals; Program for HUST Academic Frontier Youth Team; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8522050","Convolutional neural networks (CNNs);end to end;Fisher layer;Fisher vector (FV);image classification","Feature extraction;Image representation;Task analysis;Aggregates;Support vector machines;Training;Learning systems","convolutional neural nets;Gaussian processes;image classification;image representation;learning (artificial intelligence);mixture models;object detection","deep FisherNet;convolutional neural networks;CNN's representation power;Fisher vector;effective encoding strategy;local descriptors;universal generative Gaussian mixture model;neural network structure;FV layer;end-to-end trainable system;CNN training;single end-to-end structure;emotion image classification tasks;PASCAL visual object classes","","1","46","","","","","IEEE","IEEE Journals"
"Activity Recognition for Cognitive Assistance Using Body Sensors Data and Deep Convolutional Neural Network","M. Z. Uddin; M. M. Hassan","Department of Informatics, University of Oslo, Oslo, Norway; Chair of Pervasive and Mobile Computing and Information Systems Department, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia","IEEE Sensors Journal","","2019","19","19","8413","8419","In this paper, we propose a robust activity recognition approach for smart healthcare using body sensors and deep convolutional neural network (CNN). We analyze signals from different body sensors for healthcare, such as ECG, magnetometer, accelerometer, and gyroscope sensors. After extracting salient features from the sensor data based on Gaussian kernel-based principal component analysis and Z-score normalization, a deep activity CNN is trained based on the features. Finally, the trained deep CNN is used for recognizing the activities in testing data. The proposed approach is applied to a publicly available standard data set and then compared with other conventional approaches. The experimental results show that the proposed approach is superior than others, indicating the robustness of the approach to be adopted for cognitive assistance in body sensor-based smart healthcare systems.","","","10.1109/JSEN.2018.2871203","Deanship of Scientific Research, King Saud University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8469048","Body sensors;deep CNN","Intelligent sensors;Magnetic sensors;Feature extraction;Medical services;Principal component analysis;Machine learning","accelerometers;body sensor networks;cognition;convolutional neural nets;electrocardiography;feature extraction;Gaussian processes;gyroscopes;health care;magnetometers;medical signal processing;principal component analysis","cognitive assistance;body sensors data;deep convolutional neural network;robust activity recognition approach;gyroscope sensors;sensor data;Gaussian kernel-based principal component analysis;deep activity CNN;trained deep CNN;body sensor-based smart healthcare systems;body sensors;salient feature extraction;ECG;magnetometer;accelerometer;z-score normalization","","4","34","","","","","IEEE","IEEE Journals"
"Independent Deeply Learned Matrix Analysis for Determined Audio Source Separation","N. Makishima; S. Mogami; N. Takamune; D. Kitamura; H. Sumino; S. Takamichi; H. Saruwatari; N. Ono","Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; National Institute of Technology, Kagawa College, Kagawa, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Graduate School of System Design, Tokyo Metropolitan University, Tokyo, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","10","1601","1615","In this paper, we propose a new framework called independent deeply learned matrix analysis (IDLMA), which unifies a deep neural network (DNN) and independence-based multichannel audio source separation. IDLMA utilizes both pretrained DNN source models and statistical independence between sources for the separation, where the time-frequency structures of each source are iteratively optimized by a DNN while enhancing the estimation accuracy of the spatial demixing filters. As the source generative model, we introduce a complex heavy-tailed distribution to improve the separation performance. In addition, we address a semi-supervised situation; namely, a solo-recorded audio dataset can be prepared for only one source in the mixture signal. To solve the limited-data problem, we propose an appropriate data augmentation method to adapt the DNN source models to the observed signal, which enables IDLMA to work even in the semi-supervised situation. Experiments are conducted using music signals with a training dataset in both supervised and semi-supervised situations. The results show the validity of the proposed method in terms of the separation accuracy.","","","10.1109/TASLP.2019.2925450","SECOM Science and Technology Foundation and JSPS KAKENHI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747523","Audio source separation;independent component analysis;deep neural networks;semi-supervised learning","Power capacitors;Source separation;Time-frequency analysis;Covariance matrices;Spectrogram;Data models;Estimation","audio signal processing;iterative methods;matrix algebra;neural nets;source separation;spatial filters;supervised learning","independent deeply learned matrix analysis;IDLMA;independence-based multichannel audio source separation;source generative model;solo-recorded audio dataset;DNN source models;deep neural network;data augmentation method","","","44","CCBY","","","","IEEE","IEEE Journals"
"Constrained Learned Feature Extraction for Acoustic Scene Classification","T. Zhang; J. Wu","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","8","1216","1228","Deep neural networks (DNNs) have been proven to be powerful models for acoustic scene classification tasks. State-of-the-art DNNs have millions of connections and are computationally intensive, making them difficult to deploy on systems with limited resources. With a focus on acoustic scene classification, we describe a new learnable module, the simulated Fourier transform module, which allows deep neural networks to implement the discrete Fourier transform operation 8x faster on a graphics processing unit (GPU). We frame the signal processing procedure as an adaptive machine learning problem and introduce learnable parameters in the module to facilitate fast adaptation for the complex and variable acoustic signal. This module gives neural networks the ability to model audio signals from raw waveforms, without extra fast Fourier transform and filter bank patches. Then, we use the temporal transformer module, which has been previously published, to alleviate the information loss caused by the simulated Fourier transform module. These techniques can be integrated into an existing fully connected neural network (FCNN), convolutional neural network (CNN), or recurrent neural network (RNN) models. We evaluate the proposed strategy using four acoustic scene datasets (LITIS Rouen, DCASE2016, DCASE2017, and DCASE2018) as target tasks. We show that the proposed approach significantly outperforms the vanilla FCNN, CNN, and RNN approach on both efficiency and performance. For instance, the proposed approach can reduce inference time by 8× while reducing the classification error on LITIS Rouen dataset from 3.21% to 1.81%.","","","10.1109/TASLP.2019.2913091","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698305","Deep neural networks;Fourier transform;acoustic scene classification","Neural networks;Acoustics;Computational modeling;Task analysis;Discrete Fourier transforms;Graphics processing units","discrete Fourier transforms;feature extraction;image classification;learning (artificial intelligence);neural nets;signal processing","deep neural networks;simulated Fourier transform module;graphics processing unit;signal processing procedure;temporal transformer module;constrained learned feature extraction;acoustic scene classification;discrete Fourier transform;adaptive machine learning;fully connected neural network;convolutional neural network;recurrent neural network","","","50","","","","","IEEE","IEEE Journals"
"Siamese Convolutional Neural Networks for Remote Sensing Scene Classification","X. Liu; Y. Zhou; J. Zhao; R. Yao; B. Liu; Y. Zheng","School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","8","1200","1204","The convolutional neural networks (CNNs) have shown powerful feature representation capability, which provides novel avenues to improve scene classification of remote sensing imagery. Although we can acquire large collections of satellite images, the lack of rich label information is still a major concern in the remote sensing field. In addition, remote sensing data sets have their own limitations, such as the small scale of scene classes and lack of image diversity. To mitigate the impact of the existing problems, a Siamese CNN, which combines the identification and verification models of CNNs, is proposed in this letter. A metric learning regularization term is explicitly imposed on the features learned through CNNs, which enforce the Siamese networks to be more robust. We carried out experiments on three widely used remote sensing data sets for performance evaluation. Experimental results show that our proposed method outperforms the existing methods.","","","10.1109/LGRS.2019.2894399","National Natural Science Foundation of China; State’s Key Project of Research and Development Plan of China; Six Talent Peaks Project in Jiangsu Province; Natural Science Foundation of Jiangsu Province; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642394","Classification;convolutional neural networks (CNNs);deep learning;remote sensing","Remote sensing;Feature extraction;Computational modeling;Data models;Measurement;Deep learning;Computer architecture","convolutional neural nets;feature extraction;image classification;image representation;learning (artificial intelligence);remote sensing","Siamese convolutional neural networks;remote sensing scene classification;remote sensing imagery;satellite images;remote sensing data sets;image diversity;Siamese CNN;CNN;feature representation;metric learning regularization","","","29","","","","","IEEE","IEEE Journals"
"Exploiting Images for Video Recognition: Heterogeneous Feature Augmentation via Symmetric Adversarial Learning","F. Yu; X. Wu; J. Chen; L. Duan","Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Image Processing","","2019","28","11","5308","5321","Training deep models of video recognition usually requires sufficient labeled videos in order to achieve good performance without over-fitting. However, it is quite labor-intensive and time-consuming to collect and annotate a large amount of videos. Moreover, training deep neural networks on large-scale video datasets always demands huge computational resources which further hold back many researchers and practitioners. To resolve that, collecting and training on annotated images are much easier. However, thoughtlessly applying images to help recognize videos may result in noticeable performance degeneration due to the well-known domain shift and feature heterogeneity. This proposes a novel symmetric adversarial learning approach for heterogeneous image-to-video adaptation, which augments deep image and video features by learning domain-invariant representations of source images and target videos. Primarily focusing on an unsupervised scenario where the labeled source images are accompanied by unlabeled target videos in the training phrase, we present a data-driven approach to respectively learn the augmented features of images and videos with superior transformability and distinguishability. Starting with learning a common feature space (called image-frame feature space) between images and video frames, we then build new symmetric generative adversarial networks (Sym-GANs) where one GAN maps image-frame features to video features and the other maps video features to image-frame features. Using the Sym-GANs, the source image feature is augmented with the generated video-specific representation to capture the motion dynamics while the target video feature is augmented with the image-specific representation to take the static appearance information. Finally, the augmented features from the source domain are fed into a network with fully connected layers for classification. Thanks to an end-to-end training procedure of the Sym-GANs and the classification network, our approach achieves better results than other state-of-the-arts, which is clearly validated by experiments on two video datasets, i.e., the UCF101 and HMDB51 datasets.","","","10.1109/TIP.2019.2917867","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721715","Heterogeneous domain adaptation;feature augmentation;symmetric GANs;image-to-video adaptation","Training;Neural networks;Image recognition;Feature extraction;Generative adversarial networks;Gallium nitride;Computational modeling","feature extraction;image capture;image classification;image motion analysis;image representation;image sequences;learning (artificial intelligence);neural nets;video signal processing","video recognition;heterogeneous feature augmentation;training deep models;deep neural networks;large-scale video datasets;annotated images;noticeable performance degeneration;domain shift;feature heterogeneity;image-to-video adaptation;deep image;domain-invariant representations;labeled source images;unlabeled target videos;training phrase;augmented features;video frames;symmetric generative adversarial networks;Sym-GANs;source image feature;generated video-specific representation;target video feature;image-specific representation;end-to-end training procedure;labeled videos;training;feature space;video features;image-frame features;symmetric adversarial learning approach;motion dynamics;classification network;UCF101 datasets;HMDB51 datasets","","","70","","","","","IEEE","IEEE Journals"
"Deep Hierarchical Encoder–Decoder Network for Image Captioning","X. Xiao; L. Wang; K. Ding; S. Xiang; C. Pan","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Multimedia","","2019","21","11","2942","2956","Encoder-decoder models have been widely used in image captioning, and most of them are designed via single long short term memory (LSTM). The capacity of single-layer network, whose encoder and decoder are integrated together, is limited for such a complex task of image captioning. Moreover, how to effectively increase the “vertical depth” of encoder-decoder remains to be solved. To deal with these problems, a novel deep hierarchical encoder-decoder network is proposed for image captioning, where a deep hierarchical structure is explored to separate the functions of encoder and decoder. This model is capable of efficiently exerting the representation capacity of deep networks to fuse high level semantics of vision and language in generating captions. Specifically, visual representations in top levels of abstraction are simultaneously considered, and each of these levels is associated to one LSTM. The bottom-most LSTM is applied as the encoder of textual inputs. The application of the middle layer in encoder-decoder is to enhance the decoding ability of top-most LSTM. Furthermore, depending on the introduction of semantic enhancement module of image feature and distribution combine module of text feature, variants of architectures of our model are constructed to explore the impacts and mutual interactions among the visual representation, textual representations, and the output of the middle LSTM layer. Particularly, the framework is training under a reinforcement learning method to address the exposure bias problem between the training and the testing by the policy gradient optimization. Qualitative analyses indicate the process that our model “translates” image to sentence and further visualization presents the evolution of the hidden states from different hierarchical LSTMs over time. Extensive experiments demonstrate that our model outperforms current state-of-the-art models on three benchmark datasets: Flickr8K, Flickr30K, and MSCOCO. On both image captioning and retrieval tasks, our method achieves the best results. On MSCOCO captioning Leaderboard, our method also achieves superior performance.","","","10.1109/TMM.2019.2915033","National Natural Science Foundation of China; Beijing Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8710315","Deep hierarchical structure;encoder–decoder;LSTM;image captioning;retrieval;vision-sentence","Visualization;Semantics;Hidden Markov models;Decoding;Logic gates;Training;Computer architecture","data visualisation;gradient methods;image enhancement;image fusion;image representation;image retrieval;learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis","deep hierarchical encoder-decoder network;image captioning;deep hierarchical structure;visual representation;image feature enhancement;encoder-decoder models;MSCOCO dataset;single long short term memory;Flickr30K dataset;Flickr8K dataset;bottom-most LSTM;visualization","","","76","Traditional","","","","IEEE","IEEE Journals"
"Load Balancing for Ultradense Networks: A Deep Reinforcement Learning-Based Approach","Y. Xu; W. Xu; Z. Wang; J. Lin; S. Cui","Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Shenzhen Research Institute of Big Data and School of Science and Engineering, Chinese University of Hong Kong, Shenzhen, China","IEEE Internet of Things Journal","","2019","6","6","9399","9412","In this article, we propose a deep reinforcement learning (DRL)-based mobility load balancing (MLB) algorithm along with a two-layer architecture to solve the large-scale load balancing problem for ultradense networks (UDNs). Our contribution is threefold. First, this article proposes a two-layer architecture to solve the large-scale load balancing problem in a self-organized manner. The proposed architecture can alleviate the global traffic variations by dynamically grouping small cells into self-organized clusters according to their historical loads, and further adapt to local traffic variations through intracluster load balancing afterwards. Second, for the intracluster load balancing, this article proposes an off-policy DRL-based MLB algorithm to autonomously learn the optimal MLB policy under an asynchronous parallel learning framework, without any prior knowledge assumed over the underlying UDN environments. Moreover, the algorithm enables joint exploration with multiple behavior policies, such that the traditional MLB methods can be used to guide the learning process thereby improving the learning efficiency and stability. Third, this article proposes an offline-evaluation-based safeguard mechanism to ensure that the online system can always operate with the optimal and well-trained MLB policy, which not only stabilizes the online performance but also enables the exploration beyond current policies to make full use of machine learning in a safe way. Empirical results verify that the proposed framework outperforms the existing MLB methods in general UDN environments featured with irregular network topologies, coupled interferences, and random user movements, in terms of the load balancing performance.","","","10.1109/JIOT.2019.2935010","National Natural Science Foundation of China; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8796404","Deep reinforcement learning (DRL);load balancing;self-organizing networks (SONs);ultradense networks (UDNs)","","","","","","34","IEEE","","","","IEEE","IEEE Journals"
"DeepPool: Distributed Model-Free Algorithm for Ride-Sharing Using Deep Reinforcement Learning","A. O. Al-Abbasi; A. Ghosh; V. Aggarwal","School of Industrial Engineering, Purdue University, West Lafayette, IN, USA; School of Industrial Engineering, Purdue University, West Lafayette, IN, USA; School of Industrial Engineering, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Intelligent Transportation Systems","","2019","20","12","4714","4727","The success of modern ride-sharing platforms crucially depends on the profit of the ride-sharing fleet operating companies, and how efficiently the resources are managed. Further, ride-sharing allows sharing costs and, hence, reduces the congestion and emission by making better use of vehicle capacities. In this paper, we develop a distributed model-free, DeepPool, that uses deep Q-network (DQN) techniques to learn optimal dispatch policies by interacting with the environment. Further, DeepPool efficiently incorporates travel demand statistics and deep learning models to manage dispatching vehicles for improved ride sharing services. Using real-world dataset of taxi trip records in New York, DeepPool performs better than other strategies, proposed in the literature, that do not consider ride sharing or do not dispatch the vehicles to regions where the future demand is anticipated. Finally, DeepPool can adapt rapidly to dynamic environments since it is implemented in a distributed manner in which each vehicle solves its own DQN individually without coordination.","","","10.1109/TITS.2019.2931830","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793143","Ride-sharing;deep Q-network (DQN);vehicle dispatch;road network;distributed algorithm","Automobiles;Dispatching;Public transportation;Reinforcement learning;Vehicle dynamics;Real-time systems","","","","2","55","IEEE","","","","IEEE","IEEE Journals"
"Image-to-Image Learning to Predict Traffic Speeds by Considering Area-Wide Spatio-Temporal Dependencies","D. Jo; B. Yu; H. Jeon; K. Sohn","Laboratory of Big-data applications in public sector, Department of urban engineering, Chung-Ang University, Seoul, South Korea; Laboratory of Big-data applications in public sector, Department of urban engineering, Chung-Ang University, Seoul, South Korea; Laboratory of Big-data applications in public sector, Department of urban engineering, Chung-Ang University, Seoul, South Korea; Laboratory of Big-data applications in public sector, Department of urban engineering, Chung-Ang University, Seoul, South Korea","IEEE Transactions on Vehicular Technology","","2019","68","2","1188","1197","Spatio-temporal dependencies are the key to predicting the traffic parameters of an urban arterial network. However, their inclusion in forecasting traffic states has been hampered due to both the absence of a robust model and the computational burden. Recently, an innovative way to tackle the problem was developed by adopting a convolutional neural network (CNN) to deal with map images representing traffic states. Unlike previous studies that utilized map images only for input, the present study adopted images for both the input and the output of a CNN model to predict traffic speeds. The results show that the performance of the proposed model based on image-to-image learning is superior to that of the existing models.","","","10.1109/TVT.2018.2885366","Chung-Ang University Excellent Student Scholarship; National Research Foundation of Korea; Korean Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8565993","Deep convolutional neural network (CNN), machine learning;spatio-temporal dependency;traffic speed","Roads;Predictive models;Feature extraction;Data models;Forecasting;Machine learning;Image segmentation","learning (artificial intelligence);neural nets;road traffic;traffic engineering computing","convolutional neural network;robust model;urban arterial network;traffic parameters;area-wide spatio-temporal dependencies;image-to-image learning;predict traffic speeds;CNN model;traffic states","","","38","","","","","IEEE","IEEE Journals"
"Hyperspectral Image Denoising Employing a Spatial–Spectral Deep Residual Convolutional Neural Network","Q. Yuan; Q. Zhang; J. Li; H. Shen; L. Zhang","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","2","1205","1218","Hyperspectral image (HSI) denoising is a crucial preprocessing procedure to improve the performance of the subsequent HSI interpretation and applications. In this paper, a novel deep learning-based method for this task is proposed, by learning a nonlinear end-to-end mapping between the noisy and clean HSIs with a combined spatial-spectral deep convolutional neural network (HSID-CNN). Both the spatial and spectral information are simultaneously assigned to the proposed network. In addition, multiscale feature extraction and multilevel feature representation are, respectively, employed to capture both the multiscale spatial-spectral feature and fuse different feature representations for the final restoration. The simulated and real-data experiments demonstrate that the proposed HSID-CNN outperforms many of the mainstream methods in both the quantitative evaluation indexes, visual effects, and HSI classification accuracy.","","","10.1109/TGRS.2018.2865197","National Key Research and Development Program of China; National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Natural Science Foundation of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454887","Hyperspectral image (HSI) denoising;spatial–spectral;convolutional neural network (CNN);multiscale feature extraction","Noise reduction;Feature extraction;Noise measurement;Hyperspectral imaging;Image denoising;Convolutional neural networks","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image denoising;image representation;learning (artificial intelligence);neural nets","nonlinear end-to-end mapping;noisy HSI;feature representations;deep learning-based method;crucial preprocessing procedure;spatial-spectral deep residual convolutional neural network;hyperspectral image denoising;HSI classification accuracy;multiscale spatial-spectral feature;multilevel feature representation;multiscale feature extraction;spectral information;spatial information;HSID-CNN;combined spatial-spectral deep convolutional neural network","","14","54","","","","","IEEE","IEEE Journals"
"Adaptive Spatial Modulation MIMO Based on Machine Learning","P. Yang; Y. Xiao; M. Xiao; Y. L. Guan; S. Li; W. Xiang","National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Sichuan, China; National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Sichuan, China; Information Science and Engineering (ISE) Department, School of Electrical Engineering, KTH Royal Institute of Technology, Stockholm, Sweden; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Sichuan, China; College of Science and Engineering, James Cook University, Cairns, QLD, Australia","IEEE Journal on Selected Areas in Communications","","2019","37","9","2117","2131","In this paper, we propose a novel framework of low-cost link adaptation for spatial modulation multiple-input multiple-output (SM-MIMO) systems-based upon the machine learning paradigm. Specifically, we first convert the problems of transmit antenna selection (TAS) and power allocation (PA) in SM-MIMO to ones-based upon data-driven prediction rather than conventional optimization-driven decisions. Then, supervised-learning classifiers (SLC), such as the K -nearest neighbors (KNN) and support vector machine (SVM) algorithms, are developed to obtain their statistically-consistent solutions. Moreover, for further comparison we integrate deep neural networks (DNN) with these adaptive SM-MIMO schemes, and propose a novel DNN-based multi-label classifier for TAS and PA parameter evaluation. Furthermore, we investigate the design of feature vectors for the SLC and DNN approaches and propose a novel feature vector generator to match the specific transmission mode of SM. As a further advance, our proposed approaches are extended to other adaptive index modulation (IM) schemes, e.g., adaptive modulation (AM) aided orthogonal frequency division multiplexing with IM (OFDM-IM). Our simulation results show that the SLC and DNN-based adaptive SM-MIMO systems outperform many conventional optimization-driven designs and are capable of achieving a near-optimal performance with a significantly lower complexity.","","","10.1109/JSAC.2019.2929404","National Natural Science Foundation of China; National Key Laboratory of Science and Technology on Communications; Fundamental Research Funds for the Central Universities; SSF project “High-Reliable Low-Latency Industrial Wireless Communications”; EU Marie Sklodowska-Curie Actions Project “High-Reliability Low-Latency Communications With Network Coding,” and ERA-NET, “SMART-MLA.”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768319","Index modulation;SM-MIMO;machine learning;neural network;link adaptation","Machine learning;MIMO communication;Adaptive systems;Indexes;Modulation;Complexity theory;Support vector machines","adaptive modulation;learning (artificial intelligence);MIMO communication;neural nets;OFDM modulation;pattern classification;support vector machines;telecommunication computing;transmitting antennas","novel DNN-based multilabel classifier;TAS;feature vectors;SLC;DNN approaches;novel feature vector generator;adaptive modulation;DNN-based adaptive SM-MIMO systems;conventional optimization-driven designs;adaptive spatial modulation MIMO;low-cost link adaptation;machine learning paradigm;power allocation;data-driven prediction;supervised-learning classifiers;adaptive SM-MIMO schemes;spatial modulation multiple-input multiple-output systems;support vector machine algorithms;optimization-driven decisions;transmit antenna selection","","","43","","","","","IEEE","IEEE Journals"
"Enhanced Cyber-Physical Security in Internet of Things Through Energy Auditing","F. Li; Y. Shi; A. Shinde; J. Ye; W. Song","Center for Cyber-Physical Systems, University of Georgia, Athens, GA, USA; Center for Cyber-Physical Systems, University of Georgia, Athens, GA, USA; Center for Cyber-Physical Systems, University of Georgia, Athens, GA, USA; Center for Cyber-Physical Systems, University of Georgia, Athens, GA, USA; Center for Cyber-Physical Systems, University of Georgia, Athens, GA, USA","IEEE Internet of Things Journal","","2019","6","3","5224","5231","Internet of Things (IoT) are vulnerable to both cyber and physical attacks. Therefore, a cyber-physical security system against different kinds of attacks is in high demand. Traditionally, attacks are detected via monitoring system logs. However, the system logs, such as network statistics and file access records, can be forged. Furthermore, existing solutions mainly target cyber attacks. This paper proposes the first energy auditing and analytics-based IoT monitoring mechanism. To our best knowledge, this is the first attempt to detect and identify IoT cyber and physical attacks based on energy auditing. Using the energy meter readings, we develop a dual deep learning (DL) model system, which adaptively learns the system behaviors in a normal condition. Unlike the previous single DL models for energy disaggregation, we propose a disaggregation-aggregation architecture. The innovative design makes it possible to detect both cyber and physical attacks. The disaggregation model analyzes the energy consumptions of system subcomponents, e.g., CPU, network, disk, etc., to identify cyber attacks, while the aggregation model detects the physical attacks by characterizing the difference between the measured power consumption and prediction results. Using energy consumption data only, the proposed system identifies both cyber and physical attacks. The system and algorithm designs are described in detail. In the hardware simulation experiments, the proposed system exhibits promising performances.","","","10.1109/JIOT.2019.2899492","Southern Company; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642398","Cyber and physical attack detection;deep learning (DL);energy audit;Internet of Things (IoT)","Internet of Things;Deep learning;Power demand;Measurement;Cyberattack;Energy consumption","computer network security;energy consumption;Internet of Things;power aware computing;power meters","physical attacks;cyber attacks;energy consumption data;enhanced cyber-physical security;energy auditing;cyber-physical security system;system logs;analytics-based IoT monitoring mechanism;IoT cyber;energy meter readings;dual deep learning model system;energy disaggregation","","2","34","","","","","IEEE","IEEE Journals"
"Acoustic Impedance Deblurring With a Deep Convolution Neural Network","I. Sacramento; E. Trindade; M. Roisenberg; F. Bordignon; B. B. Rodrigues","Department of Computer Science, Informatics and Statistics Institute, Federal University of Santa Catarina, Florianópolis, Brazil; Petrobras, Rio de Janeiro, Brazil; Department of Computer Science, Informatics and Statistics Institute, Federal University of Santa Catarina, Florianópolis, Brazil; Cognitive and Connectionism Laboratory, Federal University of Santa Catarina, Florianópolis, Brazil; Petrobras, Rio de Janeiro, Brazil","IEEE Geoscience and Remote Sensing Letters","","2019","16","2","315","319","Domain-specific methods for deblurring particular sorts of objects have gained increasing attention due to the ineffectiveness of generic methods. We present a simple and effective convolutional neural network that deblurs postinversion acoustic impedance images. The architecture of our model consists of a convolutional layer that highlights edges and contours related to interfaces between rock layers; a locally connected layer that performs a convolutional step with unshared weights; and, finally, two fully connected layers that perform a nonlinear estimation of acoustic impedance values. We use the updated Standford VI reservoir model as training data set, which is composed of 150 acoustic impedance sections, each section with 200 traces. In this letter, we adopt a strong supervised learning that exploit, trace by trace, the data set of the inverted and ground truth impedance images. We also present an analysis comparing the frequency bandwidth among the latent, blurry, and deblurred images. Furthermore, the peak signal-to-noise ratio is calculated and compared with a classical deblurring method. We additionally address the requirement of deep learning for the huge amount of training examples by inserting rectified linear units and keeping the network architecture simple. The experimental results demonstrate the efficacy of the proposed method.","","","10.1109/LGRS.2018.2870732","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8481415","Acoustic impedance;convolutional neural network (CNN);deblurring;frequency recovering;seismic inversion","Impedance;Acoustics;Kernel;Convolution;Data models;Reservoirs;Bandwidth","acoustic impedance;feedforward neural nets;geophysical techniques;image restoration;learning (artificial intelligence)","classical deblurring method;deep learning;network architecture simple;acoustic impedance deblurring;deep convolution neural network;domain-specific methods;generic methods;simple network;effective convolutional neural network;postinversion acoustic impedance images;convolutional layer;highlights edges;rock layers;locally connected layer;convolutional step;unshared weights;fully connected layers;nonlinear estimation;acoustic impedance values;updated Standford VI reservoir model;training data set;strong supervised learning;deblurred images;peak signal-to-noise ratio;acoustic impedance sections","","","22","","","","","IEEE","IEEE Journals"
"Robot Motion Planning in Learned Latent Spaces","B. Ichter; M. Pavone","Google Brain, Mountain View, CA, USA; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA","IEEE Robotics and Automation Letters","","2019","4","3","2407","2414","This letter presents latent sampling-based motion planning (L-SBMP), a methodology toward computing motion plans for complex robotic systems by learning a plannable latent representation. Recent works in control of robotic systems have effectively leveraged local, low-dimensional embeddings of high-dimensional dynamics. In this letter, we combine these recent advances with techniques from sampling-based motion planning (SBMP) in order to design a methodology capable of planning for high-dimensional robotic systems beyond the reach of traditional approaches (e.g., humanoids, or even systems where planning occurs in the visual space). Specifically, the learned latent space is constructed through an autoencoding network, a dynamics network, and a collision checking network, which mirror the three main algorithmic primitives of SBMP, namely state sampling, local steering, and collision checking. Notably, these networks can be trained through only raw data of the system's states and actions along with a supervising collision checker. Building upon these networks, an RRT-based algorithm is used to plan motions directly in the latent space-we refer to this exploration algorithm as learned latent RRT. This algorithm globally explores the latent space and is capable of generalizing to new environments. The overall methodology is demonstrated on two planning problems, namely a visual planning problem, whereby planning happens in the visual (pixel) space, and a humanoid robot planning problem.","","","10.1109/LRA.2019.2901898","NASA; King Abdulaziz City for Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653875","Motion and path planning;deep learning in robotics and automation","Planning;Robots;Aerospace electronics;Trajectory;Visualization;Heuristic algorithms;Dynamics","collision avoidance;humanoid robots;learning (artificial intelligence);mobile robots;sampling methods","visual space;learned latent space;autoencoding network;dynamics network;collision checking network;main algorithmic primitives;state sampling;supervising collision checker;RRT-based algorithm;learned latent RRT;planning problems;visual planning problem;humanoid robot planning problem;robot motion planning;latent sampling-based motion planning;L-SBMP;motion plans;complex robotic systems;plannable latent representation;low-dimensional embeddings;high-dimensional dynamics;high-dimensional robotic systems;latent space","","","23","","","","","IEEE","IEEE Journals"
"Channel Splitting Network for Single MR Image Super-Resolution","X. Zhao; Y. Zhang; T. Zhang; X. Zou","School of Life Science and Technology, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; School of Life Science and Technology, University of Electronic Science and Technology of China (UESTC), Chengdu, China; School of Life Science and Technology, University of Electronic Science and Technology of China (UESTC), Chengdu, China","IEEE Transactions on Image Processing","","2019","28","11","5649","5662","High resolution magnetic resonance (MR) imaging is desirable in many clinical applications due to its contribution to more accurate subsequent analyses and early clinical diagnoses. Single image super-resolution (SISR) is an effective and cost efficient alternative technique to improve the spatial resolution of MR images. In the past few years, SISR methods based on deep learning techniques, especially convolutional neural networks (CNNs), have achieved the state-of-the-art performance on natural images. However, the information is gradually weakened and training becomes increasingly difficult as the network deepens. The problem is more serious for medical images because lacking high quality and effective training samples makes deep models prone to underfitting or overfitting. Nevertheless, many current models treat the hierarchical features on different channels equivalently, which is not helpful for the models to deal with the hierarchical features discriminatively and targetedly. To this end, we present a novel channel splitting network (CSN) to ease the representational burden of deep models. The proposed CSN model divides the hierarchical features into two branches, i.e., residual branch and dense branch, with different information transmissions. The residual branch is able to promote feature reuse, while the dense branch is beneficial to the exploration of new features. Besides, we also adopt the merge-and-run mapping to facilitate information integration between different branches. The extensive experiments on various MR images, including proton density (PD), T1, and T2 images, show that the proposed CSN model achieves superior performance over other state-of-the-art SISR methods.","","","10.1109/TIP.2019.2921882","Sichuan Science and Technology Program; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736987","Convolutional neural network;channel splitting;feature fusion;magnetic resonance imaging;super-resolution","Training;Spatial resolution;Imaging;Deep learning;Signal resolution;Two dimensional displays","biomedical MRI;convolutional neural nets;image resolution;learning (artificial intelligence);medical image processing","training samples;channel splitting network;CSN model;information transmissions;proton density;clinical applications;high resolution magnetic resonance imaging;single MR image super-resolution;state-of-the-art SISR methods;dense branch;residual branch;hierarchical features;deep models;medical images;natural images;convolutional neural networks;deep learning techniques;MR images;spatial resolution;early clinical diagnoses","","1","58","","","","","IEEE","IEEE Journals"
"Deep Residual Network for Steganalysis of Digital Images","M. Boroumand; M. Chen; J. Fridrich","Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY, USA; Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY, USA; Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY, USA","IEEE Transactions on Information Forensics and Security","","2019","14","5","1181","1193","Steganography detectors built as deep convolutional neural networks have firmly established themselves as superior to the previous detection paradigm - classifiers based on rich media models. Existing network architectures, however, still contain elements designed by hand, such as fixed or constrained convolutional kernels, heuristic initialization of kernels, the thresholded linear unit that mimics truncation in rich models, quantization of feature maps, and awareness of JPEG phase. In this work, we describe a deep residual architecture designed to minimize the use of heuristics and externally enforced elements that is universal in the sense that it provides state-of-the-art detection accuracy for both spatial-domain and JPEG steganography. The key part of the proposed architecture is a significantly expanded front part of the detector that “computes noise residuals” in which pooling has been disabled to prevent suppression of the stego signal. Extensive experiments show the superior performance of this network with a significant improvement, especially in the JPEG domain. Further performance boost is observed by supplying the selection channel as a second channel.","","","10.1109/TIFS.2018.2871749","National Science Foundation; Air Force Office of Scientific Research; Defense Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8470101","Steganography;steganalysis;convolutional neural network;deep residual network;selection channel;SRNet","Detectors;Transform coding;Kernel;Machine learning;Feature extraction;Training;Convolution","convolutional neural nets;image coding;object detection;steganography","noise residuals;feature map quantization;JPEG domain;expanded front part;JPEG steganography;spatial-domain;state-of-the-art detection accuracy;deep residual architecture;JPEG phase;thresholded linear unit;constrained convolutional kernels;network architectures;rich media models;deep convolutional neural networks;steganography detectors;digital images;steganalysis;deep residual network","","10","68","","","","","IEEE","IEEE Journals"
"Learning Aggregated Transmission Propagation Networks for Haze Removal and Beyond","R. Liu; X. Fan; M. Hou; Z. Jiang; Z. Luo; L. Zhang","Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province and the DUT–RU International School of Information Science & Engineering, Dalian University of Technology, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province and the DUT–RU International School of Information Science & Engineering, Dalian University of Technology, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province and the DUT–RU International School of Information Science & Engineering, Dalian University of Technology, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province and the DUT–RU International School of Information Science & Engineering, Dalian University of Technology, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province and the DUT–RU International School of Information Science & Engineering, Dalian University of Technology, Dalian, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","10","2973","2986","Single-image dehazing is an important low-level vision task with many applications. Early studies have investigated different kinds of visual priors to address this problem. However, they may fail when their assumptions are not valid on specific images. Recent deep networks also achieve a relatively good performance in this task. But unfortunately, due to the disappreciation of rich physical rules in hazes, a large amount of data are required for their training. More importantly, they may still fail when there exist completely different haze distributions in testing images. By considering the collaborations of these two perspectives, this paper designs a novel residual architecture to aggregate both prior (i.e., domain knowledge) and data (i.e., haze distribution) information to propagate transmissions for scene radiance estimation. We further present a variational energy-based perspective to investigate the intrinsic propagation behavior of our aggregated deep model. In this way, we actually bridge the gap between prior-driven models and data-driven networks and leverage advantages but avoid limitations of previous dehazing approaches. A lightweight learning framework is proposed to train our propagation network. Finally, by introducing a task-aware image separation formulation with a flexible optimization scheme, we extend the proposed model for more challenging vision tasks, such as underwater image enhancement and single-image rain removal. Experiments on both synthetic and real-world images demonstrate the effectiveness and efficiency of the proposed framework.","","","10.1109/TNNLS.2018.2862631","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Hong Kong Scholar Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8450630","Haze and rain removal;residual networks (ResNets);transmission propagation;underwater image enhancement","Task analysis;Training;Atmospheric modeling;Image enhancement;Image color analysis;Training data;Rain","computer vision;image enhancement;image restoration;learning (artificial intelligence);neural nets;optimisation","residual architecture;optimization scheme;single-image rain removal;underwater image enhancement;task-aware image separation formulation;propagation network;lightweight learning framework;prior-driven models;aggregated deep model;intrinsic propagation behavior;variational energy-based perspective;scene radiance estimation;haze distribution;low-level vision task;single-image dehazing;haze removal;aggregated transmission propagation networks","","6","42","","","","","IEEE","IEEE Journals"
"Image Reconstruction Based on Convolutional Neural Network for Electrical Resistance Tomography","C. Tan; S. Lv; F. Dong; M. Takei","Tianjin Key Laboratory of Process Measurement and Control, School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Process Measurement and Control, School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Process Measurement and Control, School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Graduate School of Engineering, Chiba University, Chiba, Japan","IEEE Sensors Journal","","2019","19","1","196","204","Image reconstruction is a key problem for electrical resistance tomography (ERT). Because of the soft-field nature and the ill-posed problem in solving inverse problem, traditional image reconstruction methods cannot achieve high accuracy and the process is usually time consuming. Since deep learning is good at mapping complicated nonlinear function, a deep learning method based on convolutional neural network (CNN) is proposed for image reconstruction of ERT. To establish the database, 41122 samples were generated with numerical simulations. 10-fold cross validation was used to divide all samples into training set and validation set. The network structure was based on LeNet, and refined by applying dropout layer and moving average. After 346 training epochs, the image correlation coefficient (ICC) on validation set was 0.95. When white Gaussian noise with a signal-to-noise ratio of 30, 40, and 50 were added to validation set, the ICC was 0.79, 0.89, and 0.93, respectively, which proved the anti-noise capability of the network. The reconstruction results on samples which have more inclusions, different conductivity, and other shapes explained the network has good generalization ability. Furthermore, experimental data from a 16-electrode industrial ERT system was used to compare the accuracy of the proposed model with some typical reconstruction methods. Results show that the proposed CNN method has better reconstruction results than LBP, Tikhonov, and Landweber.","","","10.1109/JSEN.2018.2876411","National Natural Science Foundation of China; National Natural Science Foundation of China; Natural Science Foundation of Tianjin City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493596","Electrical resistance tomography;image reconstruction;convolutional neural network;deep learning","Image reconstruction;Training;Conductivity;Machine learning;Mathematical model;Sensors;Voltage measurement","convolution;feedforward neural nets;Gaussian noise;image reconstruction;inverse problems;learning (artificial intelligence);nonlinear functions;tomography","convolutional neural network;electrical resistance tomography;ERT;soft-field nature;inverse problem;deep learning method;10-fold cross validation;image correlation coefficient;CNN method;image reconstruction;ICC;white Gaussian noise;nonlinear function","","10","34","","","","","IEEE","IEEE Journals"
"Predicting Human Saccadic Scanpaths Based on Iterative Representation Learning","C. Xia; J. Han; F. Qi; G. Shi","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Image Processing","","2019","28","7","3502","3515","Visual attention is a dynamic process of scene exploration and information acquisition. However, existing research on attention modeling has concentrated on estimating static salient locations. In contrast, dynamic attributes presented by saccade have not been well explored in previous attention models. In this paper, we address the problem of saccadic scanpath prediction by introducing an iterative representation learning framework. Within the framework, saccade can be interpreted as an iterative process of predicting one fixation according to the current representation and updating the representation based on the gaze shift. In the predicting phase, we propose a Bayesian definition of saccade to combine the influence of perceptual residual and spatial location on the selection of fixations. In implementation, we compute the representation error of an autoencoder-based network to measure perceptual residuals of each area. Simultaneously, we integrate saccade amplitude and center-weighted mechanism to model the influence of spatial location. Based on estimating the influence of two parts, the final fixation is defined as the point with the largest posterior probability of gaze shift. In the updating phase, we update the representation pattern for the subsequent calculation by retraining the network with samples extracted around the current fixation. In the experiments, the proposed model can replicate the fundamental properties of psychophysics in visual search. In addition, it can achieve superior performance on several benchmark eye-tracking data sets.","","","10.1109/TIP.2019.2897966","National Key R&D Program of China; National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8637020","Visual attention;saccade;scanpath;deep learning;representation learning","Predictive models;Estimation;Feature extraction;Visualization;Brain modeling;Computational modeling;Data models","Bayes methods;biomechanics;eye;feature extraction;learning (artificial intelligence);object detection;probability;robot vision;visual perception","iterative representation learning framework;saccadic scanpath prediction;dynamic attributes;static salient locations;attention modeling;information acquisition;scene exploration;dynamic process;visual attention;predicting human saccadic scanpaths;current fixation;representation pattern;updating phase;final fixation;perceptual residuals;autoencoder-based network;representation error;spatial location;saccade;predicting phase;gaze shift","","","50","","","","","IEEE","IEEE Journals"
"Long-Term Traffic Speed Prediction Based on Multiscale Spatio-Temporal Feature Learning Network","D. Zang; J. Ling; Z. Wei; K. Tang; J. Cheng","Department of Computer Science and Technology, Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China; Department of Computer Science and Technology, Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China; Department of Computer Science and Technology, Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China; Department of Traffic Information Engineering and Control, Tongji University, Shanghai, China; Department of Computer Science and Technology, Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China","IEEE Transactions on Intelligent Transportation Systems","","2019","20","10","3700","3709","Speed plays a significant role in evaluating the evolution of traffic status, and predicting speed is one of the fundamental tasks for the intelligent transportation system. There exists a large number of works on speed forecast; however, the problem of long-term prediction for the next day is still not well addressed. In this paper, we propose a multiscale spatio-temporal feature learning network (MSTFLN) as the model to handle the challenging task of long-term traffic speed prediction for elevated highways. Raw traffic speed data collected from loop detectors every 5 min are transformed into spatial-temporal matrices; each matrix represents the one-day speed information, rows of the matrix indicate the numbers of loop detectors, and time intervals are denoted by columns. To predict the traffic speed of a certain day, nine speed matrices of three historical days with three different time scales are served as the input of MSTFLN. The proposed MSTFLN model consists of convolutional long short-term memories and convolutional neural networks. Experiments are evaluated using the data of three main elevated highways in Shanghai, China. The presented results demonstrate that our approach outperforms the state-of-the-art work and it can effectively predict the long-term speed information.","","","10.1109/TITS.2018.2878068","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8528888","Traffic speed prediction;multiscale spatio-temporal feature learning network;deep learning;intelligent transportation system","Predictive models;Detectors;Data models;Road transportation;Recurrent neural networks;Feature extraction","convolutional neural nets;intelligent transportation systems;learning (artificial intelligence);matrix algebra;recurrent neural nets","long-term traffic speed prediction;multiscale spatio-temporal feature learning network;long-term prediction;loop detectors;speed matrices;long-term speed information;MSTFLN;convolutional long short-term memories;convolutional neural networks","","1","34","","","","","IEEE","IEEE Journals"
"Learning-Based Computation Offloading for IoT Devices With Energy Harvesting","M. Min; L. Xiao; Y. Chen; P. Cheng; D. Wu; W. Zhuang","Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; School of Data and Computer Science, Guangdong Key Laboratory of Big Data Analysis and Processing, Sun Yat-sen University, Guangzhou, Guangzhou, ChinaChina; Department Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Vehicular Technology","","2019","68","2","1930","1941","Internet of Things (IoT) devices can apply mobile edge computing (MEC) and energy harvesting (EH) to provide high-level experiences for computational intensive applications and concurrently to prolong the lifetime of the battery. In this paper, we propose a reinforcement learning (RL) based offloading scheme for an IoT device with EH to select the edge device and the offloading rate according to the current battery level, the previous radio transmission rate to each edge device, and the predicted amount of the harvested energy. This scheme enables the IoT device to optimize the offloading policy without knowledge of the MEC model, the energy consumption model, and the computation latency model. Further, we present a deep RL-based offloading scheme to further accelerate the learning speed. Their performance bounds in terms of the energy consumption, computation latency, and utility are provided for three typical offloading scenarios and verified via simulations for an IoT device that uses wireless power transfer for energy harvesting. Simulation results show that the proposed RL-based offloading scheme reduces the energy consumption, computation latency, and task drop rate, and thus increases the utility of the IoT device in the dynamic MEC in comparison with the benchmark offloading schemes.","","","10.1109/TVT.2018.2890685","National Natural Science Foundation of China; National Mobile Communications Research Laboratory, Southeast University; National Natural Science Foundation of China; National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Guangdong Special Support Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598893","Mobile edge computing;energy harvesting;reinforcement learning;Internet of Things;computation offloading","Computational modeling;Energy consumption;Task analysis;Batteries;Performance evaluation;Vehicle dynamics;Mobile handsets","energy consumption;energy harvesting;Internet of Things;learning (artificial intelligence);mobile computing;power aware computing","deep RL;Internet of Things devices;deep reinforcement learning;energy consumption reduction;mobile edge computing;computation offloading;energy harvesting;energy consumption model;offloading policy;IoT device","","17","37","","","","","IEEE","IEEE Journals"
"An End-to-End Deep Learning Histochemical Scoring System for Breast Cancer TMA","J. Liu; B. Xu; C. Zheng; Y. Gong; J. Garibaldi; D. Soria; A. Green; I. O. Ellis; W. Zou; G. Qiu","College of Information Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China; Ningbo Yongxin Optics Co., Ltd., Zhejiang, China; College of Information Engineering, Shenzhen University, Shenzhen, China; School of Computer Science, University of Nottingham, Nottingham, U.K.; Department of Computer Science, University of Westerminster, London, U.K.; Faculty of Medicine & Health Sciences, University of Nottingham, Nottingham, U.K.; Faculty of Medicine & Health Sciences, University of Nottingham, Nottingham, U.K.; College of Information Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Medical Imaging","","2019","38","2","617","628","One of the methods for stratifying different molecular classes of breast cancer is the Nottingham prognostic index plus, which uses breast cancer relevant biomarkers to stain tumor tissues prepared on tissue microarray (TMA). To determine the molecular class of the tumor, pathologists will have to manually mark the nuclei activity biomarkers through a microscope and use a semi-quantitative assessment method to assign a histochemical score (H-Score) to each TMA core. Manually marking positively stained nuclei is a time-consuming, imprecise, and subjective process, which will lead to inter-observer and intra-observer discrepancies. In this paper, we present an end-to-end deep learning system, which directly predicts the H-Score automatically. Our system imitates the pathologists' decision process and uses one fully convolutional network (FCN) to extract all nuclei region (tumor and non-tumor), a second FCN to extract tumor nuclei region, and a multi-column convolutional neural network, which takes the outputs of the first two FCNs and the stain intensity description image as an input and acts as the high-level decision making mechanism to directly output the H-Score of the input TMA image. To the best of our knowledge, this is the first end-to-end system that takes a TMA image as the input and directly outputs a clinical score. We will present experimental results, which demonstrate that the H-Scores predicted by our model have very high and statistically significant correlation with experienced pathologists' scores and that the H-Score discrepancy between our algorithm and the pathologists is on par with the inter-subject discrepancy between the pathologists.","","","10.1109/TMI.2018.2868333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453832","H-Score;immunohistochemistry;diaminobenzidine;convolutional neural network;breast cancer","Tumors;Biological tissues;Breast cancer;Solid modeling;Biomarkers;Image segmentation;Image color analysis","biological tissues;cancer;convolutional neural nets;decision making;gynaecology;image classification;image segmentation;learning (artificial intelligence);medical image processing;tumours","breast cancer relevant biomarkers;tumor tissues;tissue microarray;molecular class;nuclei activity biomarkers;semiquantitative assessment method;histochemical score;TMA core;positively stained nuclei;subjective process;intra-observer discrepancies;end-to-end deep learning system;fully convolutional network;FCN;nuclei region;nontumor;multicolumn convolutional neural network;stain intensity description image;high-level decision making mechanism;input TMA image;end-to-end system;clinical score;experienced pathologists;H-Score discrepancy;inter-subject discrepancy;histochemical scoring system;breast cancer TMA;Nottingham prognostic index plus;molecular classes","","","46","","","","","IEEE","IEEE Journals"
"Channel Estimation for Cell-Free mmWave Massive MIMO Through Deep Learning","Y. Jin; J. Zhang; S. Jin; B. Ai","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, P. R. China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, P. R. China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, P. R. China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, P. R. China","IEEE Transactions on Vehicular Technology","","2019","68","10","10325","10329","The combination of cell-free massive multiple-input multiple-output (MIMO) systems along with millimeter-wave (mmWave) bands is indeed one of most promising technological enablers of the envisioned wireless Gbit/s experience. However, both massive antennas at access points and large bandwidth at mmWave induce high computational complexity to exploit an accurate estimation of channel state information. Considering the sparse mmWave channel matrix as a natural image, we propose a practical and accurate channel estimation framework based on the fast and flexible denoising convolutional neural network (FFDNet). In contrast to previous deep learning based channel estimation methods, FFDNet is suitable a wide range of signal-to-noise ratio levels with a flexible noise level map as the input. More specifically, we provide a comprehensive investigation to optimize the FFDNet based channel estimator. Extensive simulation results validate that the training speed of FFDNet is faster than state-of-the-art channel estimators without sacrificing normalized mean square error performance, which makes FFDNet as an practical channel estimator for cell-free mmWave massive MIMO systems.","","","10.1109/TVT.2019.2937543","Beijing Natural Haidian Joint Fund; National Key Research and Development Program; Royal Society Newton Advanced Fellowship; Major projects of Beijing Municipal Science and Technology Commission; State Key Lab of Rail Traffic Control and Safety; National Natural Science Foundation of China; Natural Science Foundation of Beijing Municipality; State Key Laboratory of Integrated Services Networks; Key Laboratory of Optical Communication and Networks; Engineering Research Center of Mobile Communications, Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8815888","Channel estimation;cell-free massive MIMO;FFDNet;millimeter wave","Channel estimation;Noise level;Sparse matrices;Convolution;Transmission line matrix methods;Noise reduction","antenna arrays;channel estimation;convolutional neural nets;learning (artificial intelligence);mean square error methods;MIMO communication;telecommunication computing;wireless channels","accurate channel estimation framework;fast denoising convolutional neural network;flexible denoising convolutional neural network;deep learning;FFDNet;cell-free mmWave massive MIMO systems;cell-free massive multiple-input multiple-output systems;millimeter-wave bands;massive antennas;channel state information;sparse mmWave channel matrix;practical channel estimation framework","","","19","Traditional","","","","IEEE","IEEE Journals"
"Deep-Learning Schemes for Full-Wave Nonlinear Inverse Scattering Problems","Z. Wei; X. Chen","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","4","1849","1860","This paper is devoted to solving a full-wave inverse scattering problem (ISP), which is aimed at retrieving permittivities of dielectric scatterers from the knowledge of measured scattering data. ISPs are highly nonlinear due to multiple scattering, and iterative algorithms with regularizations are often used to solve such problems. However, they are associated with heavy computational cost, and consequently, they are often time-consuming. This paper proposes the convolutional neural network (CNN) technique to solve full-wave ISPs. We introduce and compare three training schemes based on U-Net CNN, including direct inversion, backpropagation, and dominant current schemes (DCS). Several representative tests are carried out, including both synthetic and experimental data, to evaluate the performances of the proposed methods. It is demonstrated that the proposed DCS outperforms the other two schemes in terms of accuracy and is able to solve typical ISPs quickly within 1 s. The proposed deep-learning inversion scheme is promising in providing quantitative images in real time.","","","10.1109/TGRS.2018.2869221","National Research Foundation Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8476623","Convolutional neural network (CNN);dielectric scatterers;inverse scattering","Scattering;Inverse problems;Optimization;Iterative methods;Permittivity;Current measurement;Neural networks","backpropagation;convolutional neural nets;electromagnetic wave scattering;image classification;image reconstruction;inverse problems;iterative methods;learning (artificial intelligence)","full-wave nonlinear inverse scattering problems;ISP;dielectric scatterers;iterative algorithms;U-Net CNN;direct inversion;dominant current schemes;deep-learning inversion scheme;convolutional neural network;full-wave ISP;image classification;image reconstruction;backpropagation","","7","37","","","","","IEEE","IEEE Journals"
"Fast Single-Image Super-Resolution via Deep Network With Component Learning","C. Xie; W. Zeng; X. Lu","School of Automation, Southeast University, Nanjing, China; College of Civil Aviation, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Automation, Southeast University, Nanjing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","12","3473","3486","Driven by the spectacular success of deep learning, several advanced models based on neural networks have recently been proposed for single-image super-resolution, incrementally revealing their superiority over their alternatives. In this paper, we pursue this latest line of research and present an improved network structure by taking advantage of the proposed component learning. The core idea and difference of this learning strategy are to use the residual extracted from the input to predict its counterpart in the corresponding output. To this end, a global decomposition procedure is designed on the basis of convolutional sparse coding and performed on the input for extracting the low-resolution (LR) residual component from it. Owing to the properties of this decomposition, the represented residual component still stays in the LR space so that the subsequent part is capable of operating it economically in terms of computational complexity. Thorough experimental results demonstrate the merit and effectiveness of the proposed component learning strategy, and our trained model outperforms many state-of-the-art methods in terms of both speed and reconstruction quality.","","","10.1109/TCSVT.2018.2883771","National Natural Science Foundation of China; Key Research and Development Program of Jiangsu Province; Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550684","Single image super-resolution;component learning;deep convolutional neural networks","Computational modeling;Image reconstruction;Training;Image resolution;Convolutional codes;Encoding","","","","","64","IEEE","","","","IEEE","IEEE Journals"
"WISERNet: Wider Separate-Then-Reunion Network for Steganalysis of Color Images","J. Zeng; S. Tan; G. Liu; B. Li; J. Huang","Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, and National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, and National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, and National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing and Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen Key Laboratory of Media Security, and National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University, Shenzhen, China","IEEE Transactions on Information Forensics and Security","","2019","14","10","2735","2748","Until recently, deep steganalyzers in the spatial domain have been all designed for gray-scale images. In this paper, we propose the wider separate-then-reunion network (WISERNet) for steganalysis of color images. We provide theoretical rationale to claim that the summation in normal convolution is one sort of linear collusion attack which reserves strong correlated patterns while impairs uncorrelated noises. Therefore, in the bottom convolutional layer which aims at suppressing correlated image contents, we adopt separate channel-wise convolution without summation instead. Conversely, in the upper convolutional layers, we believe that the summation in normal convolution is beneficial. Therefore, we adopt united normal convolution in those layers and make them remarkably wider to reinforce the effect of linear collusion attack. As a result, our proposed wide-and-shallow, separate-then-reunion network structure is specifically suitable for color image steganalysis. We have conducted extensive experiments on color image datasets generated from BOSSBase raw images and another large-scale dataset that contains 100, 000 raw images, with different demosaicking algorithms and down-sampling algorithms. The experimental results show that our proposed network outperforms other state-of-the-art color image steganalytic models either hand crafted or learned using deep networks in the literature by a clear margin. Specifically, it is noted that the detection performance gain is achieved with less than half the complexity compared to the most advanced deep-learning steganalyzer as far as we know, which is scarce in the literature.","","","10.1109/TIFS.2019.2904413","National Natural Science Foundation of China; Shenzhen R&D Program; Alibaba Group through Alibaba Innovative Research (AIR) Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664587","Steganalysis;steganography;deep learning;convolutional neural network","Color;Convolution;Image color analysis;Correlation;Gray-scale;Feature extraction;Kernel","convolution;image colour analysis;image denoising;image sampling;image segmentation;learning (artificial intelligence);object detection;steganography","normal convolution;linear collusion attack;strong correlated patterns;bottom convolutional layer;upper convolutional layers;color image steganalysis;color image datasets;BOSSBase raw images;large-scale dataset;deep networks;advanced deep-learning steganalyzer;WISERNet;spatial domain;gray-scale images;uncorrelated noises;channel-wise convolution;wider separate-then-reunion network structure;correlated image content suppression;demosaicking algorithms;down-sampling algorithms","","3","40","","","","","IEEE","IEEE Journals"
"Overlapping Community Deep Exploring-Based Relay Selection Method Toward Multi-Hop D2D Communication","P. Zhang; X. Kang; X. Li; Y. Liu; D. Wu; R. Wang","School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Wireless Communications Letters","","2019","8","5","1357","1360","Cellular D2D networks consist of numerous D2D user equipments (UEs) carried by human beings with multiple social attributes, which accordingly connotes an overlapping community (OC) structure. Accurately detecting OC D2D UEs can effectively improve the efficiency of multi-hop D2D communication. Existing relay selection methods overlook the OC structure characteristic of cellular D2D networks, which results in limited relay efficiency. Therefore, we propose an OC deep exploring-based relay selection method. First, we build the social tie matrix between D2D UEs and then based on deep learning theory we extract the features of social tie matrix to further precisely detect OC D2D UEs. Moreover, by reasonably utilizing detected OC D2D UEs, we design the effective relay selection method. Simulation results demonstrate that the proposed method can largely enhance the delivery rate and power consumption performances of cellular D2D networks.","","","10.1109/LWC.2019.2917907","National Natural Science Foundation of China; Program for Innovation Team Building at Institutions of Higher Education in Chongqing; Chongqing Municipal Education Commission Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718793","Multi-hop D2D communication;relay selection;overlapping community;deep exploration","Device-to-device communication;Relays;Feature extraction;Sparse matrices;Spread spectrum communication;Encoding;Decoding","cellular radio;cooperative communication;learning (artificial intelligence)","Cellular D2D networks;numerous D2D user equipments;multiple social attributes;overlapping community structure;OC D2D;multihop D2D communication;relay selection methods;OC structure characteristic;cellular D2D networks;relay efficiency;OC deep exploring-based relay selection method;social tie matrix;effective relay selection method;community deep exploring-based relay selection method;MultiHop D2D Communication;OC","","12","11","","","","","IEEE","IEEE Journals"
"Cascaded Detection Framework Based on a Novel Backbone Network and Feature Fusion","Z. Tian; W. Wang; R. Zhan; Z. He; J. Zhang; Z. Zhuang","Science and Technology on Automatic Target Recognition Laboratory, National University of Defense Technology, Changsha, China; Science and Technology on Automatic Target Recognition Laboratory, National University of Defense Technology, Changsha, China; Science and Technology on Automatic Target Recognition Laboratory, National University of Defense Technology, Changsha, China; Science and Technology on Automatic Target Recognition Laboratory, National University of Defense Technology, Changsha, China; Science and Technology on Automatic Target Recognition Laboratory, National University of Defense Technology, Changsha, China; Science and Technology on Automatic Target Recognition Laboratory, National University of Defense Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","9","3480","3491","Due to the ability of powerful feature representation, deep-learning-based object detection has attracted considerable research attention, and many methods have been proposed for remote sensing images. However, there are still some problems that need to be addressed. In this paper, a novel and effective detection framework based on faster region-based convolutional neural network is designed. Specifically, first, in order to locate the boundaries of large objects and find the missing small objects, DetNet is incorporated into the detection framework as the backbone network. DetNet fixes the spatial resolution in deep layers and adopts dilated bottleneck with convolution projection to increase the divergence between input and output feature maps. Then, the proposed framework uses the backbone network to extract the scene features and region features simultaneously, which are both mapped to feature vectors and then fused together. The feature fusion operation can improve the feature representation of the generated region. Last, to improve the performance of localization, the cascade structure is adopted in the framework. The cascade structure has multiple phases and every phase has independent classifier and regressor. The results obtained from the previous phase are used as the regions of interest in the next phase. Therefore, the multiphase detector can increase the detection accuracy phase by phase. Comprehensive evaluations on a public ten-class object detection dataset demonstrate the effectiveness of the proposed framework. Moreover, ablation experiments are also implemented to show the respective influence of different parts of the framework on the performance improvement.","","","10.1109/JSTARS.2019.2924086","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754690","Convolutional neural network (CNN);feature fusion;object detection;remote sensing images","Feature extraction;Object detection;Remote sensing;Detectors;Task analysis;Support vector machines;Deep learning","convolutional neural nets;feature extraction;image classification;image fusion;image representation;learning (artificial intelligence);object detection;regression analysis","public ten-class object detection dataset;deep-learning-based object detection;remote sensing images;faster region-based convolutional neural network;DetNet;deep layers;convolution projection;output feature maps;scene features;region features;feature fusion operation;generated region;cascade structure;feature representation;detection framework","","","27","Traditional","","","","IEEE","IEEE Journals"
"Retro-Remote Sensing: Generating Images From Ancient Texts","M. B. Bejiga; F. Melgani; A. Vascotto","Department of Computer Science and Information Engineering, University of Trento, Trento, Italy; Department of Computer Science and Information Engineering, University of Trento, Trento, Italy; Department of Computer Science and Information Engineering, University of Trento, Trento, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","3","950","960","The data available in the world come in various modalities, such as audio, text, image, and video. Each data modality has different statistical properties. Understanding each modality, individually, and the relationship between the modalities is vital for a better understanding of the environment surrounding us. Multimodal learning models allow us to process and extract useful information from multimodal sources. For instance, image captioning and text-to-image synthesis are examples of multimodal learning, which require mapping between texts and images. In this paper, we introduce a research area that has never been explored by the remote sensing community, namely the synthesis of remote sensing images from text descriptions. More specifically, in this paper, we focus on exploiting ancient text descriptions of geographical areas, inherited from previous civilizations, to generate equivalent remote sensing images. From a methodological perspective, we propose to rely on generative adversarial networks (GANs) to convert the text descriptions into equivalent pixel values. GANs are a recently proposed class of generative models that formulate learning the distribution of a given dataset as an adversarial competition between two networks. The learned distribution is represented using the weights of a deep neural network and can be used to generate more samples. To fulfill the purpose of this paper, we collected satellite images and ancient texts to train the network. We present the interesting results obtained and propose various future research paths that we believe are important to further develop this new research area.","","","10.1109/JSTARS.2019.2895693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660422","Convolutional neural networks (CNN);deep learning;generative adversarial networks (GAN);multimodal learning;remote sensing;text-to-image synthesis","Remote sensing;Gallium nitride;Earth;Sensors;Generators;Satellites;Technological innovation","convolutional neural nets;geophysical image processing;image retrieval;learning (artificial intelligence);remote sensing;statistical analysis;text analysis","retro-remote sensing;ancient texts;data modality;multimodal learning models;text-to-image synthesis;ancient text descriptions;generative adversarial networks;GANs;deep neural network;satellite images;image generation;statistical properties;image captioning;information extraction","","","34","","","","","IEEE","IEEE Journals"
"Deep Color Guided Coarse-to-Fine Convolutional Network Cascade for Depth Image Super-Resolution","Y. Wen; B. Sheng; P. Li; W. Lin; D. D. Feng","School of Electronics, Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronics, Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Faculty of Information Technology, Macau University of Science and Technology, Macau, China; School of Electronics, Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Image Processing","","2019","28","2","994","1006","Depth image super-resolution is a significant yet challenging task. In this paper, we introduce a novel deep color guided coarse-to-fine convolutional neural network (CNN) framework to address this problem. First, we present a data-driven filter method to approximate the ideal filter for depth image super-resolution instead of hand-designed filters. Based on large data samples, the filter learned is more accurate and stable for upsampling depth image. Second, we introduce a coarse-to-fine CNN to learn different sizes of filter kernels. In the coarse stage, larger filter kernels are learned by the CNN to achieve crude high-resolution depth image. As to the fine stage, the crude high-resolution depth image is used as the input so that smaller filter kernels are learned to gain more accurate results. Benefit from this network, we can progressively recover the high frequency details. Third, we construct a color guidance strategy that fuses color difference and spatial distance for depth image upsampling. We revise the interpolated high-resolution depth image according to the corresponding pixels in high-resolution color maps. Guided by color information, the depth of high-resolution image obtained can alleviate texture copying artifacts and preserve edge details effectively. Quantitative and qualitative experimental results demonstrate our state-of-the-art performance for depth map super-resolution.","","","10.1109/TIP.2018.2874285","National Natural Science Foundation of China; National Key Research and Development Program of China; Science and Technology Commission of Shanghai Municipality; Fundo para o Desenvolvimento das Ciências e da Tecnologia; Shanghai “The Belt and Road” Young Scholar Exchange Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8485321","Depth super-resolution;color guidance;coarse-to-fine convolutional neural network;filter kernel learning","Image color analysis;Image edge detection;Color;Spatial resolution;Kernel;Sensors","convolution;feedforward neural nets;image colour analysis;image filtering;image resolution;image sampling;image texture;interpolation;learning (artificial intelligence)","crude high-resolution depth image;depth image upsampling;interpolated high-resolution depth image;high-resolution color maps;high-resolution image;depth map super-resolution;depth image super-resolution;data-driven filter method;hand-designed filters;deep color guided coarse-to-fine convolutional network cascade;filter kernels;deep color guided coarse-to-fine convolutional neural network framework;CNN framework;color guidance strategy;texture copying artifacts;color information","","5","58","","","","","IEEE","IEEE Journals"
"DIOD: Fast, Semi-Supervised Deep ISAR Object Detection","B. Xue; N. Tong; X. Xu","Graduate School, Air Force Engineering University, Xi’an, China; Graduate School, Air Force Engineering University, Xi’an, China; Shaanxi Rural Commercial Bank Co., Ltd., Shangluo, China","IEEE Sensors Journal","","2019","19","3","1073","1081","Inverse synthetic aperture radar (ISAR) object detection is one of the most challenging problems in computer vision, and most existing ISAR object detection algorithms are complicated and perform poorly. To provide a convenient and high-quality ISAR object detection method, we propose a fast semi-supervised method, called DIOD, which is based on fully convolutional region candidate networks (FCRCNs) and deep convolutional neural networks. First, a region candidate is used to localize potential objects in most of the best detection methods, but this approach often results in the most intractable computational bottleneck. Thus, to perform localization robustly and accurately in minimal time, we propose an FCRCN with “seed” boxes at multiple scales and aspect ratios. This approach offers almost cost-free candidate computation and achieves excellent performance. Second, to overcome the lack of labeled training data, the model undergoes an efficient semi-supervised pretraining process followed by fine-tuning, which produces successful results. Finally, to further improve the accuracy and speed of the detection system, we introduce a novel sharing mechanism and a joint learning strategy that extract more discriminative and comprehensive features while simultaneously learning the latent shared and individual features and their correlations. Extensive experiments are conducted on two real-world ISAR datasets, and the results show that DIOD outperforms the existing state-of-the-art methods.","","","10.1109/JSEN.2018.2879669","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8525285","Object detection;semisupervised;region candidate;deep convolutional neural network;inverse synthetic aperture radar","Object detection;Feature extraction;Convolutional codes;Microsoft Windows;Training;Sensors;Task analysis","computer vision;convolutional neural nets;feature extraction;object detection;radar computing;radar imaging;supervised learning;synthetic aperture radar","semisupervised pretraining process;real-world ISAR datasets;semisupervised deep ISAR object detection;inverse synthetic aperture radar object detection;computer vision;fully convolutional region candidate networks;deep convolutional neural networks;DIOD","","1","26","","","","","IEEE","IEEE Journals"
"Label Propagation Ensemble for Hyperspectral Image Classification","Y. Zhang; G. Cao; A. Shafique; P. Fu","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","9","3623","3636","The imbalance between limited labeled pixels and high dimensionality of hyperspectral data can easily give rise to Hughes phenomenon. Semisupervised learning (SSL) methods provide promising solutions to address the aforementioned issue. Graph-based SSL algorithms, also called label propagation methods, have obtained increasing attention in hyperspectral image (HSI) classification. However, the graphs constructed by utilizing the geometrical structure similarity of samples are unreliable due to the high dimensionality and complexity of the HSIs, especially for the case of very limited labeled pixels. Our motivation is to construct label propagation ensemble (LPE) model, then use the decision fusion of multiple label propagations to obtain pseudolabeled pixels with high classification confidence. In LPE, random subspace method is introduced to partition the feature space into multiple subspaces, then several label propagation models are constructed on corresponding subspaces, finally the results of different label propagation models are fused at decision level, and only the unlabeled pixels whose label propagation results are the same will be assigned with pseudolabels. Meanwhile extreme learning machine classifiers are trained on the labeled and pseudolabeled samples during the iteration. Compared with traditional label propagation methods, our proposed method can deal with the situation of very limited labeled samples by providing pseudolabeled pixels with high classification confidence, consequently, the accurate base classifiers are obtained. To demonstrate the effectiveness of the proposed method, LPE is compared with several state-of-the-art methods on four hyperspectral datasets. In addition, the method that only use label propagation is investigated to show the importance of ensemble technique in LPE. The experimental results demonstrate that the proposed method can provide competitive solution for HSI classification.","","","10.1109/JSTARS.2019.2926123","Postgraduate Research and Practice Innovation Program of Jiangsu Province; National Natural Science Foundation of China; National Key Research and Development Plan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765736","Ensemble learning;extreme learning machine (ELM);hyperspectral image (HSI) classification;label propagation","Hyperspectral imaging;Feature extraction;Image reconstruction;Deep learning","feedforward neural nets;geophysical image processing;graph theory;hyperspectral imaging;image classification;supervised learning","hyperspectral image classification;labeled pixels;label propagation ensemble model;LPE;multiple label propagations;pseudolabeled pixels;random subspace method;extreme learning machine classifiers;hyperspectral datasets;HSI classification;graph-based SSL algorithms;Hughes phenomenon","","1","70","Traditional","","","","IEEE","IEEE Journals"
"DAEN: Deep Autoencoder Networks for Hyperspectral Unmixing","Y. Su; J. Li; A. Plaza; A. Marinoni; P. Gamba; S. Chakravortty","Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, Center of Integrated Geographic Information Analysis, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Physics and Technology, Earth Observation Group, Centre for Integrated Remote Sensing and Forecasting for Arctic Operations, UiT–The Arctic University of Norway, Tromsø, Norway; Department of Electrical, Computer and Biomedical Engineering, Telecommunications and Remote Sensing Laboratory, University of Pavia, Pavia, Italy; Department of Information Technology, Maulana Abul Kalam Azad University of Technology, Kolkata, India","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","7","4309","4321","Spectral unmixing is a technique for remotely sensed image interpretation that expresses each (possibly mixed) pixel as a combination of pure spectral signatures (endmembers) and their fractional abundances. In this paper, we develop a new technique for unsupervised unmixing which is based on a deep autoencoder network (DAEN). Our newly developed DAEN consists of two parts. The first part of the network adopts stacked autoencoders (SAEs) to learn spectral signatures, so as to generate a good initialization for the unmixing process. In the second part of the network, a variational autoencoder (VAE) is employed to perform blind source separation, aimed at obtaining the endmember signatures and abundance fractions simultaneously. By taking advantage from the SAEs, the robustness of the proposed approach is remarkable as it can unmix data sets with outliers and low signal-to-noise ratio. Moreover, the multihidden layers of the VAE ensure the required constraints (nonnegativity and sum-to-one) when estimating the abundances. The effectiveness of the proposed method is evaluated using both synthetic and real hyperspectral data. When compared with other unmixing methods, the proposed approach demonstrates very competitive performance.","","","10.1109/TGRS.2018.2890633","National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8628241","Deep autoencoder network (DAEN);deep learning;endmember identification;hyperspectral unmixing;variational autoencoder (VAE)","Hyperspectral imaging;Estimation;Computers;Training;Noise reduction","geophysical image processing;hyperspectral imaging;remote sensing;spectral analysis","deep autoencoder network;hyperspectral unmixing;remotely sensed image interpretation;pure spectral signatures;fractional abundances;unsupervised unmixing;newly developed DAEN;unmixing process;variational autoencoder;endmember signatures;synthetic data;real hyperspectral data;unmixing methods","","1","55","","","","","IEEE","IEEE Journals"
"Pixel-to-Pixel Learning With Weak Supervision for Single-Stage Nucleus Recognition in Ki67 Images","F. Xing; T. C. Cornish; T. Bennett; D. Ghosh; L. Yang","Department of Biostatistics and Informatics and the Data Science to Patient Value Initiative, University of Colorado Anschutz Medical Campus, Aurora, CO, USA; Department of PathologyUniversity of Colorado Anschutz Medical Campus; Department of Pediatrics and the Data Science to Patient Value InitiativeUniversity of Colorado Anschutz Medical Campus; Department of Biostatistics and Informatics and the Data Science to Patient Value InitiativeUniversity of Colorado Anschutz Medical Campus; Department of Electrical and Computer Engineering, Department of Computer and Information Science and Engineering, and the J. Crayton Pruitt Family Department of Biomedical EngineeringUniversity of Florida","IEEE Transactions on Biomedical Engineering","","2019","66","11","3088","3097","Objective: Nucleus recognition is a critical yet challenging step in histopathology image analysis, for example, in Ki67 immunohistochemistry stained images. Although many automated methods have been proposed, most use a multi-stage processing pipeline to categorize nuclei, leading to cumbersome, low-throughput, and error-prone assessments. To address this issue, we propose a novel deep fully convolutional network for single-stage nucleus recognition. Methods: Instead of conducting direct pixel-wise classification, we formulate nucleus identification as a deep structured regression model. For each input image, it produces multiple proximity maps, each of which corresponds to one nucleus category and exhibits strong responses in central regions of the nuclei. In addition, by taking into consideration the nucleus distribution in histopathology images, we further introduce an auxiliary task, region of interest (ROI) extraction, to assist and boost the nucleus quantification with weak ROI annotation. The proposed network can be learned in an end-to-end, pixel-to-pixel manner for simultaneous nucleus detection and classification. Results: We have evaluated this network on a pancreatic neuroendocrine tumor Ki67 image dataset, and the experiments demonstrate that our method outperforms recent state-of-the-art approaches. Conclusion: We present a new, pixel-to-pixel deep neural network with two sibling branches for effective nucleus recognition and observe that learning with another relevant task, ROI extraction, can further boost individual nucleus localization and classification. Significance: Our method provides a clean, single-stage nucleus recognition pipeline for histopathology image analysis, especially a new perspective for Ki67 image quantification, which would potentially benefit individual object quantification in whole-slide images.","","","10.1109/TBME.2019.2900378","The University of Colorado Anschutz Medical Campus; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8649708","Nucleus classification;nucleus detection;fully convolutional networks;Ki67;neuroendocrine tumor;microscopy images","Tumors;Task analysis;Image recognition;Immune system;Feature extraction;Microscopy;Image analysis","cellular biophysics;convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);medical image processing;object detection;regression analysis;tumours","histopathology image analysis;Ki67 image quantification;whole-slide images;pixel-to-pixel learning;Ki67 images;Ki67 immunohistochemistry;multistage processing pipeline;deep fully convolutional network;pixel-wise classification;nucleus identification;deep structured regression model;nucleus category;nucleus distribution;nucleus quantification;weak ROI annotation;simultaneous nucleus detection;pancreatic neuroendocrine tumor Ki67 image dataset;pixel-to-pixel deep neural network;nucleus localization;single-stage nucleus recognition pipeline;nucleus recognition;ROI extraction;object quantification","","","55","Traditional","","","","IEEE","IEEE Journals"
"Triple Verification Network for Generalized Zero-Shot Learning","H. Zhang; Y. Long; Y. Guan; L. Shao","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Open Lab, School of Computing, University of Newcastle, Newcastle upon Tyne, U.K.; Open Lab, School of Computing, University of Newcastle, Newcastle upon Tyne, U.K.; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates","IEEE Transactions on Image Processing","","2019","28","1","506","517","Conventional zero-shot learning approaches often suffer from severe performance degradation in the generalized zero-shot learning (GZSL) scenario, i.e., to recognize test images that are from both seen and unseen classes. This paper studies the Class-level Over-fitting (CO) and empirically shows its effects to GZSL. We then address ZSL as a triple verification problem and propose a unified optimization of regression and compatibility functions, i.e., two main streams of existing ZSL approaches. The complementary losses mutually regularizes the same model to mitigate the CO problem. Furthermore, we implement a deep extension paradigm to linear models and significantly outperform state-of-the-art methods in both GZSL and ZSL scenarios on the four standard benchmarks.","","","10.1109/TIP.2018.2869696","National Natural Science Foundation of China; National Defense Pre-research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8464092","Generalized zero shot learning;triple verification;orthogonal projection;dual regression","Visualization;Training;Task analysis;Semantics;Benchmark testing;Degradation;Image recognition","learning (artificial intelligence);object recognition;optimisation;regression analysis","generalized zero-shot learning;zero-shot learning scenario;GZSL;test images;Class-level Over-fitting;triple verification problem;ZSL approaches;ZSL scenarios;triple verification network;complementary losses;deep extension paradigm","","2","58","","","","","IEEE","IEEE Journals"
"Multi-Pseudo Regularized Label for Generated Data in Person Re-Identification","Y. Huang; J. Xu; Q. Wu; Z. Zheng; Z. Zhang; J. Zhang","Global Big Data Technologies Centre, School of Electrical and Data Engineering, University of Technology Sydney, Ultimo, NSW, Australia; Global Big Data Technologies Centre, School of Electrical and Data Engineering, University of Technology Sydney, Ultimo, NSW, Australia; Global Big Data Technologies Centre, School of Electrical and Data Engineering, University of Technology Sydney, Ultimo, NSW, Australia; Centre for Artificial Intelligence, School of Software, University of Technology Sydney, Ultimo, NSW, Australia; Research Center for Brain-Inspired Intelligence, CAS Center for Excellence in Brain Science and Intelligence Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Global Big Data Technologies Centre, School of Electrical and Data Engineering, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Image Processing","","2019","28","3","1391","1403","Sufficient training data normally is required to train deeply learned models. However, due to the expensive manual process for a labeling large number of images (i.e., annotation), the amount of available training data (i.e., real data) is always limited. To produce more data for training a deep network, generative adversarial network can be used to generate artificial sample data (i.e., generated data). However, the generated data usually does not have annotation labels. To solve this problem, in this paper, we propose a virtual label called Multi-pseudo Regularized Label (MpRL) and assign it to the generated data. With MpRL, the generated data will be used as the supplementary of real training data to train a deep neural network in a semi-supervised learning fashion. To build the corresponding relationship between the real data and generated data, MpRL assigns each generated data a proper virtual label which reflects the likelihood of the affiliation of the generated data to pre-defined training classes in the real data domain. Unlike the traditional label which usually is a single integral number, the virtual label proposed in this paper is a set of weight-based values each individual of which is a number in (0,1] called multi-pseudo label and reflects the degree of relation between each generated data to every pre-defined class of real data. A comprehensive evaluation is carried out by adopting two state-of-the-art convolutional neural networks (CNNs) in our experiments to verify the effectiveness of MpRL. Experiments demonstrate that by assigning MpRL to generated data, we can further improve the person re-ID performance on five re-ID datasets, i.e., Market-1501, DukeMTMC-reID, CUHK03, VIPeR, and CUHK01. The proposed method obtains +6.29%, +6.30%, +5.58%, +5.84%, and +3.48% improvements in rank-1 accuracy over a strong CNN baseline on the five datasets, respectively, and outperforms state-of-the-art methods.","","","10.1109/TIP.2018.2874715","Australian Government Research Training Program Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8485730","Person re-identification;generated data;virtual label;semi-supervised learning","Training;Gallium nitride;Semisupervised learning;Training data;Data models;Machine learning;Task analysis","data handling;feedforward neural nets;image recognition;learning (artificial intelligence)","multipseudo regularized label;person re-identification;training data;deep network training;virtual label;MpRL;semisupervised learning;predefined training classes;data domain;convolutional neural networks;Market-1501 dataset;DukeMTMC-reID dataset;CUHK03 dataset;VIPeR dataset;CUHK01 dataset","","8","64","","","","","IEEE","IEEE Journals"
"Adversarial Learning Semantic Volume for 2D/3D Face Shape Regression in the Wild","H. Zhang; Q. Li; Z. Sun","Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China","IEEE Transactions on Image Processing","","2019","28","9","4526","4540","Regression-based methods have revolutionized 2D landmark localization with the exploitation of deep neural networks and massive annotated datasets in the wild. However, it remains challenging for 3D landmark localization due to the lack of annotated datasets and the ambiguous nature of landmarks under the 3D perspective. This paper revisits regression-based methods and proposes an adversarial voxel and coordinate regression framework for 2D and 3D facial landmark localization in real-world scenarios. First, a semantic volumetric representation is introduced to encode the per-voxel likelihood of positions being the 3D landmarks. Then, an end-to-end pipeline is designed to jointly regress the proposed volumetric representation and the coordinate vector. Such a pipeline not only enhances the robustness and accuracy of the predictions but also unifies the 2D and 3D landmark localization so that the 2D and 3D datasets could be utilized simultaneously. Further, an adversarial learning strategy is exploited to distill 3D structure learned from synthetic datasets to real-world datasets under weakly supervised settings, where an auxiliary regression discriminator is proposed to encourage the network to produce plausible predictions for both the synthetic and real-world images. The effectiveness of our method is validated on benchmark datasets 3DFAW and AFLW2000-3D for both 2D and 3D facial landmark localization tasks. The experimental results show that the proposed method achieves significant improvements over the previous state-of-the-art methods.","","","10.1109/TIP.2019.2911114","National Natural Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8694006","2D/3D facial landmark localization;semantic volumetric representation;joint voxel and coordinate regression;auxiliary regression adversarial learning","Three-dimensional displays;Two dimensional displays;Face;Heating systems;Shape;Semantics;Task analysis","face recognition;image representation;learning (artificial intelligence);neural nets;regression analysis;shape recognition;stereo image processing","adversarial learning semantic volume;2D/3D face shape regression;regression framework;semantic volumetric representation;adversarial learning strategy;auxiliary regression discriminator;3D facial landmark localization;2D facial landmark localization;deep neural networks","","","63","","","","","IEEE","IEEE Journals"
"Visual tracking with tree-structured appearance model for online learning","Y. Lv; K. Liu; F. Cheng; W. Li","School of Computer Science and Technology, Xidian University, People's Republic of China; School of Computer Science and Technology, Xidian University, People's Republic of China; School of Computer Science and Technology, Xidian University, People's Republic of China; School of Computer Science and Technology, Xidian University, People's Republic of China","IET Image Processing","","2019","13","12","2106","2115","Deep learning has been widely used in many visual recognition tasks owing to its powerful representation ability. However, online learning is a bottleneck to obstruct the application of deep learning in visual tracking. Although many algorithms have discarded the process of online learning during tracking, they demonstrate poor robustness to the online adaptation to appearance changes of the target. In this study, the authors design a tree structure specifically for online learning, which enables the appearance model to be updated smoothly. Once the target appearance has changed severely, a new branch is generated to avoid the fuzzy boundary of classification. In addition, active learning technique and artificial data are employed in the update to make the best of the limited knowledge about the interesting object during the tracking process. The proposed algorithm is evaluated on OTB2013 and VOT2017 benchmark and outperforms many state-of-the-art methods.","","","10.1049/iet-ipr.2018.6517","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870579","","","image representation;learning (artificial intelligence);object tracking;trees (mathematics)","active learning technique;online adaptation;visual tracking;visual recognition tasks;deep learning;online learning;tree-structured appearance model","","","43","","","","","IET","IET Journals"
"Adaptive Multiscale Deep Fusion Residual Network for Remote Sensing Image Classification","G. Li; L. Li; H. Zhu; X. Liu; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","8506","8521","With the development of remote sensing imaging technology, remote sensing images with high-resolution and complex structure can be acquired easily. The classification of remote sensing images is always a hot and challenging problem. In order to improve the performance of remote sensing image classification, we propose an adaptive multiscale deep fusion residual network (AMDF-ResNet). The AMDF-ResNet consists of a backbone network and a fusion network. The backbone network including several residual blocks generates multiscale hierarchy features, which contain semantic information from low to high levels. In the fusion network, the adaptive feature fusion module proposed can emphasize useful information and suppress useless information by learning the weights, which represent the importance of the features. The AMDF-ResNet can make full use of the multiscale hierarchy features and the extracted feature is discriminative. In addition, we propose a samples selection method named important samples selection strategy (ISSS). Based on superpixels segmentation result, gradient information and spatial distribution are used as two references to determine the selection numbers and select samples. Compared with the random selection strategy, training samples selected by ISSS are more representative and diverse. The experimental results on four data sets demonstrate that the AMDF-ResNet and ISSS are effective.","","","10.1109/TGRS.2019.2921342","National Natural Science Foundation of China; National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746806","Deep learning (DL);feature extraction;image classification;multispectral (MS) images;remote sensing","Feature extraction;Training;Semantics;Image segmentation;Adaptive systems;Hyperspectral sensors","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);neural nets;remote sensing","backbone network;fusion network;multiscale hierarchy features;adaptive feature fusion module;AMDF-ResNet;adaptive multiscale deep fusion residual network;remote sensing image classification;remote sensing imaging technology;important samples selection strategy;superpixel segmentation","","","54","","","","","IEEE","IEEE Journals"
"CloudSegNet: A Deep Network for Nychthemeron Cloud Image Segmentation","S. Dev; A. Nautiyal; Y. H. Lee; S. Winkler","ADAPT SFI Research Centre, Trinity College Dublin, Dublin 2, Ireland; ADAPT SFI Research Centre, Trinity College Dublin, Dublin 2, Ireland; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; AI Singapore and the School of Computing, National University of Singapore, Singapore","IEEE Geoscience and Remote Sensing Letters","","2019","16","12","1814","1818","We analyze clouds in the earth's atmosphere using ground-based sky cameras. An accurate segmentation of clouds in the captured sky/cloud image is difficult, owing to the fuzzy boundaries of clouds. Several techniques have been proposed, which use color as the discriminatory feature for cloud detection. In the existing literature, however, analysis of daytime and nighttime images is considered separately, mainly because of differences in image characteristics and applications. In this letter, we propose a lightweight deep-learning architecture called CloudSegNet. It is the first that integrates daytime and nighttime (also known as nychthemeron) image segmentation in a single framework and achieves state-of-the-art results on public databases.","","","10.1109/LGRS.2019.2912140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8727442","Cloud segmentation;deep learning;nychthemeron;whole sky imager (WSI)","Clouds;Image segmentation;Image color analysis;Computer architecture;Cameras;Image resolution;Convolution","cameras;clouds;image segmentation;learning (artificial intelligence)","CloudSegNet;deep network;nychthemeron cloud image segmentation;ground-based sky cameras;fuzzy boundaries;cloud detection;image characteristics","","1","20","IEEE","","","","IEEE","IEEE Journals"
"Deep Neural Generative Model of Functional MRI Images for Psychiatric Disorder Diagnosis","T. Matsubara; T. Tashiro; K. Uehara","Graduate School of System Informatics, Kobe University, Hyogo, Japan; Graduate School of System InformaticsKobe University; Graduate School of System InformaticsKobe University","IEEE Transactions on Biomedical Engineering","","2019","66","10","2768","2779","Accurate diagnosis of psychiatric disorders plays a critical role in improving the quality of life for patients and potentially supports the development of new treatments. Many studies have been conducted on machine learning techniques that seek brain imaging data for specific biomarkers of disorders. These studies have encountered the following dilemma: A direct classification overfits to a small number of high-dimensional samples but unsupervised feature-extraction has the risk of extracting a signal of no interest. In addition, such studies often provided only diagnoses for patients without presenting the reasons for these diagnoses. This study proposed a deep neural generative model of resting-state functional magnetic resonance imaging (fMRI) data. The proposed model is conditioned by the assumption of the subject's state and estimates the posterior probability of the subject's state given the imaging data, using Bayes' rule. This study applied the proposed model to diagnose schizophrenia and bipolar disorders. Diagnostic accuracy was improved by a large margin over competitive approaches, namely classifications of functional connectivity, discriminative/generative models of regionwise signals, and those with unsupervised feature-extractors. The proposed model visualizes brain regions largely related to the disorders, thus motivating further biological investigation.","","","10.1109/TBME.2019.2895663","JSPS KAKENHI; MIC/SCOPE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8627955","Deep learning;generative model;functional magnetic resonance imaging;psychiatric-disorder diagnosis;schizophrenia;bipolar disorder","Feature extraction;Functional magnetic resonance imaging;Brain modeling;Biological system modeling;Hidden Markov models;Task analysis","biomedical MRI;brain;feature extraction;learning (artificial intelligence);medical disorders;medical image processing;neural nets;neurophysiology","unsupervised feature-extraction;high-dimensional samples;direct classification;specific biomarkers;brain imaging data;psychiatric disorders;psychiatric disorder diagnosis;functional MRI images;unsupervised feature-extractors;functional connectivity;bipolar disorders;resting-state functional magnetic resonance imaging data;deep neural generative model","","","75","Traditional","","","","IEEE","IEEE Journals"
"Multi-Channel Decomposition in Tandem With Free-Energy Principle for Reduced-Reference Image Quality Assessment","W. Zhu; G. Zhai; X. Min; M. Hu; J. Liu; G. Guo; X. Yang","MoE Key Laboratory of Artificial Intelligence, Artificial Intelligence Institute and with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai, China; MoE Key Laboratory of Artificial Intelligence, Artificial Intelligence Institute and with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai, China; MoE Key Laboratory of Artificial Intelligence, Artificial Intelligence Institute and with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Institute of Deep Learning, Baidu Research and with the National Engineering Laboratory for Deep Learning Technology and Application, Beijing, China; MoE Key Laboratory of Artificial Intelligence, Artificial Intelligence Institute and with the Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Multimedia","","2019","21","9","2334","2346","The visual quality of perceptions is highly correlated with the mechanisms of the human brain and visual system. Recently, the free-energy principle, which has been widely researched in brain theory and neuroscience, is introduced to quantize the perception, action, and learning in human brain. In the field of image quality assessment (IQA), on one hand, the free-energy principle can resort to the internal generative model to simulate the visual stimulus of the human beings. On the other hand, abundant psychological and neurobiological studies reveal that different frequency and orientation components of one visual stimulus arouse different neurons in the striate cortex, and the striate cortex processes visual information in the cerebral cortex. Motivated by these two aspects, a novel reduce-reference IQA metric called the multi-channel free-energy based reduced-reference quality metric is proposed in this paper. First, a two-level discrete Haar wavelet transform is used to decompose the input reference and distorted images. Next, to simulate the generative model in the human brain, the sparse representation is leveraged to extract the free-energy-based features in subband images. Finally, the overall quality metric is obtained through the support vector regressor. Extensive experimental comparisons on four benchmark image quality databases (LIVE, CSIQ, TID2008, and TID2013) demonstrate that the proposed method is highly competitive with the representative reduced-reference and classical full-reference models.","","","10.1109/TMM.2019.2902484","National Natural Science Foundation of China; National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Science and Technology Commission of Shanghai Municipality; Equipment Pre-research Joint Research Program of Ministry of Education; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8656492","Image quality assessment (IQA);reduced-reference (RR);multi-channel decomposition;free-energy principle;human visual system","Visualization;Wavelet transforms;Image quality;Feature extraction;Measurement;Brain modeling","brain;feature extraction;Haar transforms;image representation;neurophysiology;regression analysis;support vector machines;visual databases;visual perception;wavelet transforms","image quality databases;reduce-reference IQA;LIVE;CSIQ;TID2008;TID2013;visual quality;two-level discrete Haar wavelet transform;support vector regressor;sparse representation;full-reference models;representative reduced-reference;free-energy-based features;reduced-reference quality metric;multichannel free-energy;striate cortex processes visual information;human beings;visual stimulus;neuroscience;brain theory;visual system;human brain;reduced-reference image quality assessment;multichannel decomposition","","","50","Traditional","","","","IEEE","IEEE Journals"
"Hashing with Mutual Information","F. Cakir; K. He; S. A. Bargal; S. Sclaroff","FirstFuel Software; Facebook Reality Labs; Department of Computer Science, Boston University, Boston, MA, USA; Department of Computer Science, Boston University, Boston, MA, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","10","2424","2437","Binary vector embeddings enable fast nearest neighbor retrieval in large databases of high-dimensional objects, and play an important role in many practical applications, such as image and video retrieval. We study the problem of learning binary vector embeddings under a supervised setting, also known as hashing. We propose a novel supervised hashing method based on optimizing an information-theoretic quantity, mutual information. We show that optimizing mutual information can reduce ambiguity in the induced neighborhood structure in the learned Hamming space, which is essential in obtaining high retrieval performance. To this end, we optimize mutual information in deep neural networks with minibatch stochastic gradient descent, with a formulation that maximally and efficiently utilizes available supervision. Experiments on four image retrieval benchmarks, including ImageNet, confirm the effectiveness of our method in learning high-quality binary embeddings for nearest neighbor retrieval.","","","10.1109/TPAMI.2019.2914897","BU IGNITION; US NSF; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8705282","Hashing;deep learning;nearest neighbor retrieval;mutual information","Mutual information;Task analysis;Hash functions;Optimization;Quantization (signal);Deep learning;Neural networks","file organisation;gradient methods;image retrieval;learning (artificial intelligence);neural nets","binary vector embeddings;high-dimensional objects;video retrieval;supervised setting;supervised hashing method;information-theoretic quantity;image retrieval benchmarks;nearest neighbor retrieval;high-quality binary embeddings;Hamming space;mutual information","","1","71","","","","","IEEE","IEEE Journals"
"Multi-Scale Dense Networks for Hyperspectral Remote Sensing Image Classification","C. Zhang; G. Li; S. Du","School of Civil Engineering, Hefei University of Technology, Hefei, China; School of Civil Engineering, Hefei University of Technology, Hefei, China; Institute of Remote Sensing and GIS, Peking University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","9201","9222","For hyperspectral remote sensing image (HSI) classification, the learning process of deep neural networks has been progressively advanced in depth, but the fine features are often largely lost or even disappear in the process of depth transfer. With the increase in feature aggregation and connectivity, the complexity of the network and the training parameters increases greatly, requiring more training time. This paper proposed a multi-scale dense network (MSDN) for HSI classification that made full use of different scale information in the network structure and combined scale information throughout the network. It implemented feature extraction of HSIs in two dimensions, including the features at fine and coarse levels. In the horizontal direction, it considered the deep extraction of HSI features, and the 3-D dense connection structure was used for aggregating features at different levels. In the vertical direction, scale information was considered, and three-scale feature maps at low, middle, and high levels were generated based on the first layer of the network. The MSDN used stride convolution for downsampling and combined feature information at different scale levels. The MSDN extracted features along the diagonal line. The network implemented the reconstruction of deep feature extraction and multi-scale fusion for HSI classification. The MSDN model performed well on representative HSI datasets, namely, the Indian Pines, Pavia University, Salinas, Botswana, and Kennedy Space Center datasets. It improved the training speed and accuracy for HSI classification and especially improved the convergence speed, which effectively saved computing resources and had high stability.","","","10.1109/TGRS.2019.2925615","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784389","3-D convolutional neural network (3-D CNN);3-D DenseNet;hyperspectral remote sensing image (HSI) classification;multi-scale dense network (MSDN);spectral–spatial information","Feature extraction;Training;Convolution;Remote sensing;Convergence;Data mining;Deep learning","feature extraction;image classification;image fusion;learning (artificial intelligence);neural nets;remote sensing","deep feature extraction;HSI classification;representative HSI datasets;multiscale dense network;hyperspectral remote sensing image classification;deep neural networks;feature aggregation;MSDN;deep extraction;3D dense connection structure","","","28","","","","","IEEE","IEEE Journals"
"Varifocal-Net: A Chromosome Classification Approach Using Deep Convolutional Networks","Y. Qin; J. Wen; H. Zheng; X. Huang; J. Yang; N. Song; Y. Zhu; L. Wu; G. Yang","Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China; Center for Medical Genetics, School of Life Sciences, Central South University, Changsha, China; Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of ReproductiveMedicine, School of Medicine, Shanghai Jiao Tong University, Shanghai, China; INSA Lyon, CNRS, INSERM, CREATIS UMR 5220, U1206, University of Lyon, Lyon, France; Center for Medical Genetics, School of Life Sciences, Central South University, Changsha, China; Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.","IEEE Transactions on Medical Imaging","","2019","38","11","2569","2581","Chromosome classification is critical for karyotyping in abnormality diagnosis. To expedite the diagnosis, we present a novel method named Varifocal-Net for simultaneous classification of chromosome's type and polarity using deep convolutional networks. The approach consists of one global-scale network (G-Net) and one local-scale network (L-Net). It follows three stages. The first stage is to learn both global and local features. We extract global features and detect finer local regions via the G-Net. By proposing a varifocal mechanism, we zoom into local parts and extract local features via the L-Net. Residual learning and multi-task learning strategies are utilized to promote high-level feature extraction. The detection of discriminative local parts is fulfilled by a localization subnet of the G-Net, whose training process involves both supervised and weakly supervised learning. The second stage is to build two multi-layer perceptron classifiers that exploit features of both two scales to boost classification performance. The third stage is to introduce a dispatch strategy of assigning each chromosome to a type within each patient case, by utilizing the domain knowledge of karyotyping. The evaluation results from 1909 karyotyping cases showed that the proposed Varifocal-Net achieved the highest accuracy per patient case (%) of 99.2 for both type and polarity tasks. It outperformed state-of-the-art methods, demonstrating the effectiveness of our varifocal mechanism, multi-scale feature ensemble, and dispatch strategy. The proposed method has been applied to assist practical karyotype diagnosis.","","","10.1109/TMI.2019.2905841","National Natural Science Foundation of China; National Key R&D Program of China; 863 Plan of China; National Basic Research Program of China (973 Program); 1000-Talent Plan (Young Program) and Committee of Science and Technology, Shanghai, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8669741","Chromosome classification;varifocal mechanism;feature ensemble;convolutional networks;dispatch strategy","Biological cells;Feature extraction;Microscopy;Task analysis;Support vector machines;Training;Indexes","convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);medical diagnostic computing;multilayer perceptrons;pattern classification","local features;L-Net;residual learning;multitask learning strategies;high-level feature extraction;discriminative local parts;localization subnet;weakly supervised learning;multilayer perceptron classifiers;classification performance;dispatch strategy;patient case;Varifocal-Net;polarity tasks;varifocal mechanism;multiscale feature ensemble;practical karyotype diagnosis;chromosome classification approach;deep convolutional networks;abnormality diagnosis;simultaneous classification;global-scale network;G-Net;local-scale network;global features;finer local regions;karyotyping cases","","","53","","","","","IEEE","IEEE Journals"
"Computer-Aided Diagnosis of Label-Free 3-D Optical Coherence Microscopy Images of Human Cervical Tissue","Y. Ma; T. Xu; X. Huang; X. Wang; C. Li; J. Jerwick; Y. Ning; X. Zeng; B. Wang; Y. Wang; Z. Zhang; X. Zhang; C. Zhou","School of Computer ScienceWuhan University; Department of Electrical and Computer EngineeringLehigh University; College of Information Sciences and TechnologyPenn State University; Department of Electrical and Computer EngineeringLehigh University; Department of Electrical and Computer EngineeringLehigh University; Department of BioengineeringDepartment of Electrical and Computer EngineeringLehigh University; Department of Electrical and Computer EngineeringLehigh University; Department of Electrical and Computer EngineeringLehigh University; Third Affiliated Hospital of Zhengzhou University; Department of Pathology and Laboratory MedicineRhode Island Hospital/Warren Alpert Medical School of Brown University; Third Affiliated Hospital of Zhengzhou University; Third Affiliated Hospital of Zhengzhou University; Department of BioengineeringDepartment of Electrical and Computer EngineeringLehigh University","IEEE Transactions on Biomedical Engineering","","2019","66","9","2447","2456","Objective: Ultrahigh-resolution optical coherence microscopy (OCM) has recently demonstrated its potential for accurate diagnosis of human cervical diseases. One major challenge for clinical adoption, however, is the steep learning curve clinicians need to overcome to interpret OCM images. Developing an intelligent technique for computer-aided diagnosis (CADx) to accurately interpret OCM images will facilitate clinical adoption of the technology and improve patient care. Methods: 497 high-resolution three-dimensional (3-D) OCM volumes (600 cross-sectional images each) were collected from 159 ex vivo specimens of 92 female patients. OCM image features were extracted using a convolutional neural network (CNN) model, concatenated with patient information [e.g., age and human papillomavirus (HPV) results], and classified using a support vector machine classifier. Ten-fold cross-validations were utilized to test the performance of the CADx method in a five-class classification task and a binary classification task. Results: An 88.3 ± 4.9% classification accuracy was achieved for five fine-grained classes of cervical tissue, namely normal, ectropion, low-grade and high-grade squamous intraepithelial lesions (LSIL and HSIL), and cancer. In the binary classification task [low-risk (normal, ectropion, and LSIL) versus high-risk (HSIL and cancer)], the CADx method achieved an area-under-the-curve value of 0.959 with an 86.7 ± 11.4% sensitivity and 93.5 ± 3.8% specificity. Conclusion: The proposed deep-learning-based CADx method outperformed four human experts. It was also able to identify morphological characteristics in OCM images that were consistent with histopathological interpretations. Significance: Label-free OCM imaging, combined with deep-learning-based CADx methods, holds a great promise to be used in clinical settings for the effective screening and diagnosis of cervical diseases.","","","10.1109/TBME.2018.2890167","National Basic Research Program of China (973 Program); Lehigh University; U.S. National Science Foundation; U.S. National Institutes of Health; Medical Science and Technology; Henan Province Medical Technology Challenger Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598821","Cervical cancer;optical coherence tomography;optical coherence microscopy;deep learning;computer-aided diagnosis","Feature extraction;Biomedical imaging;Lesions;Cervical cancer","biological organs;biological tissues;biomedical optical imaging;cancer;convolutional neural nets;diseases;feature extraction;gynaecology;image classification;learning (artificial intelligence);medical image processing;optical tomography;patient care;patient diagnosis;stereo image processing;support vector machines","Label-free OCM imaging;deep-learning-based CADx methods;computer-aided diagnosis;human cervical tissue;Ultrahigh-resolution optical coherence microscopy;human cervical diseases;steep learning curve clinicians;patient care;convolutional neural network model;patient information;support vector machine classifier;CADx method;binary classification task;high-grade squamous intraepithelial lesions;Label-free 3-D optical coherence microscopy images;convolutional neural network;OCM image feature extraction","","","43","CCBY","","","","IEEE","IEEE Journals"
"MR Image Reconstruction Using Deep Density Priors","K. C. Tezcan; C. F. Baumgartner; R. Luechinger; K. P. Pruessmann; E. Konukoglu","Computer Vision Laboratory, ETH Zürich, Zürich, Switzerland; Computer Vision Laboratory, ETH Zürich, Zürich, Switzerland; Institute for Biomedical Engineering, ETH Zürich, Zürich, Switzerland; Institute for Biomedical Engineering, ETH Zürich, Zürich, Switzerland; Computer Vision Laboratory, ETH Zürich, Zürich, Switzerland","IEEE Transactions on Medical Imaging","","2019","38","7","1633","1642","Algorithms for magnetic resonance (MR) image reconstruction from undersampled measurements exploit prior information to compensate for missing k-space data. Deep learning (DL) provides a powerful framework for extracting such information from existing image datasets, through learning, and then using it for reconstruction. Leveraging this, recent methods employed DL to learn mappings from undersampled to fully sampled images using paired datasets, including undersampled and corresponding fully sampled images, integrating prior knowledge implicitly. In this letter, we propose an alternative approach that learns the probability distribution of fully sampled MR images using unsupervised DL, specifically variational autoencoders (VAE), and use this as an explicit prior term in reconstruction, completely decoupling the encoding operation from the prior. The resulting reconstruction algorithm enjoys a powerful image prior to compensate for missing k-space data without requiring paired datasets for training nor being prone to associated sensitivities, such as deviations in undersampling patterns used in training and test time or coil settings. We evaluated the proposed method with T1 weighted images from a publicly available dataset, multi-coil complex images acquired from healthy volunteers ( ${N}=8$ ), and images with white matter lesions. The proposed algorithm, using the VAE prior, produced visually high quality reconstructions and achieved low RMSE values, outperforming most of the alternative methods on the same dataset. On multi-coil complex data, the algorithm yielded accurate magnitude and phase reconstruction results. In the experiments on images with white matter lesions, the method faithfully reconstructed the lesions.","","","10.1109/TMI.2018.2887072","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8579232","Reconstruction;MRI;prior probability;machine learning;deep learning;unsupervised learning;density estimation","Image reconstruction;Reconstruction algorithms;Training;Data models;Signal processing algorithms;Mathematical model","","","","","56","","","","","IEEE","IEEE Journals"
"DDLA: dual deep learning architecture for classification of plant species","A. P. Sundara Sobitha Raj; S. K. Vajravelu","Madras Institute of Technology, Anna University, India; Madras Institute of Technology, Anna University, India","IET Image Processing","","2019","13","12","2176","2182","Plant species recognition is performed using a dual deep learning architecture (DDLA) approach. DDLA consists of MobileNet and DenseNet-121 architectures. The feature vectors obtained from individual architectures are concatenated to form a final feature vector. The extracted features are then classified using machine learning (ML) classifiers such as linear discriminant analysis, multinomial logistic regression (LR), Naive Bayes, classification and regression tree, k-nearest neighbour, random forest classifier, bagging classifier and multi-layer perceptron. The dataset considered in the studies is standard (Flavia, Folio, and Swedish Leaf) and custom collected (Leaf-12) dataset. The MobileNet and DenseNet-121 architectures are also used as a feature extractor and a classifier. It is observed that the DDLA architecture with LR classifier produced the highest accuracies of 98.71, 96.38, 99.41, and 99.39% for Flavia, Folio, Swedish leaf, and Leaf-12 datasets. The observed accuracy for DDLA + LR is higher compared with other approaches (DDLA + ML classifiers, MobileNet + ML classifiers, DenseNet-121 + ML classifiers, MobileNet + fully connected layer (FCL), DenseNet-121 + FCL). It is also observed that the DDLA architecture with LR classifier achieves higher accuracy in comparable computation time with other approaches.","","","10.1049/iet-ipr.2019.0346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870607","","","Bayes methods;biology computing;botany;feature extraction;image classification;multilayer perceptrons;nearest neighbour methods;random forests;regression analysis","bagging classifier;k-nearest neighbour;naive Bayes;plant species recognition;dual deep learning architecture approach;DenseNet-121 architectures;feature vectors;machine learning classifiers;linear discriminant analysis;multinomial logistic regression;regression tree;random forest classifier;multilayer perceptron;feature extractor;DDLA architecture;LR classifier;Swedish leaf;Leaf-12 datasets;plant species classification;ML classifiers;MobileNet;fully connected layer;feature extraction","","","43","","","","","IET","IET Journals"
"Model-Free Training of End-to-End Communication Systems","F. A. Aoudia; J. Hoydis","Nokia Bell Labs, Paris-Saclay, Nozay, France; Nokia Bell Labs, Paris-Saclay, Nozay, France","IEEE Journal on Selected Areas in Communications","","2019","37","11","2503","2516","The idea of end-to-end learning of communication systems through neural network (NN)-based autoencoders has the shortcoming that it requires a differentiable channel model. We present in this paper a novel learning algorithm which alleviates this problem. The algorithm enables training of communication systems with an unknown channel model or with non-differentiable components. It iterates between training of the receiver using the true gradient, and training of the transmitter using an approximation of the gradient. We show that this approach works as well as model-based training for a variety of channels and tasks. Moreover, we demonstrate the algorithm's practical viability through hardware implementation on software defined radios (SDRs) where it achieves state-of-the-art performance over a coaxial cable and wireless channel.","","","10.1109/JSAC.2019.2933891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792076","Autoencoder;deep learning;end-to-end learning;neural network;software-defined radio;reinforcement learning","Training;Receivers;Channel models;Radio transmitters;Communication systems;Approximation algorithms","approximation theory;gradient methods;learning (artificial intelligence);neural nets;software radio;telecommunication computing","model-based training;model-free training;end-to-end communication systems;end-to-end learning;neural network-based autoencoders;learning algorithm;software defined radios;gradient approximation","","1","29","","","","","IEEE","IEEE Journals"
"Trajectory-Pooled Spatial-Temporal Architecture of Deep Convolutional Neural Networks for Video Event Detection","Y. Li; R. Ge; Y. Ji; S. Gong; C. Liu","School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","9","2683","2692","Nowadays content-based video event detection faces great challenges due to complex scenes and blurred actions in surveillance videos. To alleviate these challenges, we propose a novel spatial-temporal architecture of deep convolutional neural networks for this task. By taking advantage of spatial-temporal information, we fine-tune two-stream networks, and then, fuse spatial and temporal features at convolution layers using a 2D pooling fusion method to enforce the consistence of spatial-temporal information. Based on the two-stream networks and spatial-temporal layer, a triple-channel model is obtained. Furthermore, we implement trajectory-constrained pooling to deep features and hand-crafted features to combine their merits. A fusion method on triple-channel yields the final detection result. The experiments on two benchmark surveillance video data sets including VIRAT 1.0 and VIRAT 2.0, which involve a suit of challenging events, such as person loading an object to a vehicle or person opening a vehicle trunk, manifest that the proposed method can achieve superior performance compared with the state-of-the-art methods on these event benchmarks.","","","10.1109/TCSVT.2017.2759299","Natural Science Foundation of Jiangsu Province; Six Talent Peaks Project in Jiangsu Province; National Natural Science Foundation of China; Jilin University; Natural Science Foundation of Zhejiang Province; Postgraduate Research & Practice Innovation Program of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8057866","Trajectory-pooled;triple-channel;convolutional neural networks;spatial-temporal;event detection;deep feature","Trajectory;Event detection;Feature extraction;Computer vision;Computer architecture;Image motion analysis;Fuses","convolutional neural nets;feature extraction;image fusion;image representation;image resolution;learning (artificial intelligence);object detection;video signal processing;video surveillance","trajectory-pooled spatial-temporal architecture;deep convolutional neural networks;video event detection;spatial-temporal information;fine-tune two-stream networks;spatial features;temporal features;convolution layers;2D pooling fusion method;spatial-temporal layer;triple-channel model;trajectory-constrained pooling;deep features;hand-crafted features;final detection result;benchmark surveillance video data sets;VIRAT 1;VIRAT 2;event benchmarks;spatial-temporal architecture","","1","42","","","","","IEEE","IEEE Journals"
"Optimized Input for CNN-Based Hyperspectral Image Classification Using Spatial Transformer Network","X. He; Y. Chen","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","12","1884","1888","Deep learning-based methods, especially deep convolutional neural networks (CNNs), have shown their effectiveness for hyperspectral image (HSI) classification. In previous deep CNN-based HSI classification methods, a cuboid is empirically determined as the input. The dimensionalities of the cuboid, including height and weight, are crucial to the final classification results. Unfortunately, these superparameters (i.e., the dimensionalities of input cube) are hand-crafted, which means the inputs of a classifier are not optimized according to the specific hyperspectral dataset. In this letter, spatial transformation network (STN) is explored to obtain the optimal input for CNN-based HSI classification for the first time. STN is used to translate, rotate, and scale the original input to obtain optimized input for the following CNN. Moreover, in order to mitigate the overfitting problem in CNN-based HSI classification, DropBlock is introduced as a regularization technique for HSI accurate classification. Compared with dropout, which is a popular regularization technique, DropBlock obtains better classification accuracy. The proposed methods are tested on two widely used hyperspectral data sets (i.e., Salinas and Kennedy Space Center). The obtained experimental results show that the proposed methods provide competitive results compared with state-of-the-art methods including deep CNN-based methods.","","","10.1109/LGRS.2019.2911322","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8710262","Convolutional neural network (CNN);deep learning;hyperspectral image (HSI) classification;spatial transformer network (STN)","Feature extraction;Training;Hyperspectral imaging;Convolution;Deep learning;Task analysis","convolutional neural nets;hyperspectral imaging;image classification;optimisation","CNN-based HSI classification;HSI accurate classification;CNN-based hyperspectral image classification;spatial transformer network;deep convolutional neural networks;regularization technique;DropBlock","","","24","IEEE","","","","IEEE","IEEE Journals"
"Multiview Multitask Gaze Estimation With Deep Convolutional Neural Networks","D. Lian; L. Hu; W. Luo; Y. Xu; L. Duan; J. Yu; S. Gao","School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","10","3010","3023","Gaze estimation, which aims to predict gaze points with given eye images, is an important task in computer vision because of its applications in human visual attention understanding. Many existing methods are based on a single camera, and most of them only focus on either the gaze point estimation or gaze direction estimation. In this paper, we propose a novel multitask method for the gaze point estimation using multiview cameras. Specifically, we analyze the close relationship between the gaze point estimation and gaze direction estimation, and we use a partially shared convolutional neural networks architecture to simultaneously estimate the gaze direction and gaze point. Furthermore, we also introduce a new multiview gaze tracking data set that consists of multiview eye images of different subjects. As far as we know, it is the largest multiview gaze tracking data set. Comprehensive experiments on our multiview gaze tracking data set and existing data sets demonstrate that our multiview multitask gaze point estimation solution consistently outperforms existing methods.","","","10.1109/TNNLS.2018.2865525","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454246","Convolutional neural networks (CNNs);gaze tracking;multitask learning (MTL);multiview learning","Estimation;Gaze tracking;Head;Task analysis;Feature extraction;Cameras;Robustness","computer vision;convolutional neural nets;estimation theory;gaze tracking","multiview cameras;multiview gaze tracking data;convolutional neural networks architecture;multiview multitask gaze point estimation solution;multiview eye images;gaze direction estimation;deep convolutional neural networks","","","53","","","","","IEEE","IEEE Journals"
"Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks","W. Lai; J. Huang; N. Ahuja; M. Yang","Department of Electrical and Engineering and Computer Science, University of California, Merced, CA, USA; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Champaign, IL, USA; Department of Electrical and Engineering and Computer Science, University of California, Merced, CA, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","11","2599","2613","Convolutional neural networks have recently demonstrated high-quality reconstruction for single image super-resolution. However, existing methods often require a large number of network parameters and entail heavy computational loads at runtime for generating high-accuracy super-resolution results. In this paper, we propose the deep Laplacian Pyramid Super-Resolution Network for fast and accurate image super-resolution. The proposed network progressively reconstructs the sub-band residuals of high-resolution images at multiple pyramid levels. In contrast to existing methods that involve the bicubic interpolation for pre-processing (which results in large feature maps), the proposed method directly extracts features from the low-resolution input space and thereby entails low computational loads. We train the proposed network with deep supervision using the robust Charbonnier loss functions and achieve high-quality image reconstruction. Furthermore, we utilize the recursive layers to share parameters across as well as within pyramid levels, and thus drastically reduce the number of parameters. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of run-time and image quality.","","","10.1109/TPAMI.2018.2865304","NSF CAREER; NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8434354","Single-image super-resolution;deep convolutional neural networks;Laplacian pyramid","Image reconstruction;Feature extraction;Convolution;Spatial resolution;Laplace equations;Interpolation","feature extraction;image reconstruction;image representation;image resolution;interpolation;learning (artificial intelligence);neural nets","high-quality image reconstruction;low computational loads;low-resolution input space;multiple pyramid levels;high-resolution images;image super-resolution;fast image super-resolution;deep Laplacian pyramid super-resolution network;network parameters;single image super-resolution;high-quality reconstruction;convolutional neural networks;deep Laplacian Pyramid networks;image quality","","16","53","","","","","IEEE","IEEE Journals"
"Joint prominent expression feature regions in auxiliary task learning network for facial expression recognition","W. Chen; H. Hu","Sun Yat-Sen University, People's Republic of China; Sun Yat-Sen University, People's Republic of China","Electronics Letters","","2019","55","1","22","24","The key issue for facial expression recognition (FER) is to concentrate on the prominent expression feature regions where the expression changes. In this Letter, the authors propose a novel and effective FER framework jointing prominent expression feature regions in an auxiliary task learning network (ATLN). The proposed approach consists of two deep learning neural networks, one of which is the main network whose inputs are complete face images, and the other is the auxiliary learning network whose inputs are pre-processed face images containing prominent expression feature regions. The main network structure of the ATLN improves its ability to focus on regions with prominent expressions changing by sharing parameters with their auxiliary network structure. They carry out experiments on two public facial expression databases, namely, CK+ and MMI. Experimental results demonstrate the superior performance of proposed method.","","","10.1049/el.2018.7235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8605323","","","face recognition;feature extraction;learning (artificial intelligence);neural nets","joint prominent expression feature regions;auxiliary task learning network;facial expression recognition;expression changes;main network whose inputs;auxiliary learning network whose inputs;main network structure;prominent expressions;auxiliary network structure;public facial expression databases","","","12","","","","","IET","IET Journals"
"Pattern Classification for Gastrointestinal Stromal Tumors by Integration of Radiomics and Deep Convolutional Features","Z. Ning; J. Luo; Y. Li; S. Han; Q. Feng; Y. Xu; W. Chen; T. Chen; Y. Zhang","School of Biomedical Engineering, Southern Medical University, Guangzhou, China; School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Department of General Surgery, Guangdong General Hospital, Guangdong Academy of Medical Science, Guangzhou, China; Department of General Surgery Zhujiang Hospital, Southern Medical University, Guangzhou, China; School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Medical Image Center, Nanfang Hospital, Southern Medical University, Guangzhou, China; School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Department of General Surgery Nanfang Hospital, Southern Medical University, Guangdong Provincial Engineering Technology Research Center of Minimally Invasive Surgery, Guangzhou, China; School of Biomedical Engineering, Southern Medical University, Guangzhou, China","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","1181","1191","Predicting malignant potential is one of the most critical components of a computer-aided diagnosis system for gastrointestinal stromal tumors (GISTs). These tumors have been studied only on the basis of subjective computed tomography findings. Among various methodologies, radiomics, and deep learning algorithms, specifically convolutional neural networks (CNNs), have recently been confirmed to achieve significant success by outperforming the state-of-the-art performance in medical image pattern classification and have rapidly become leading methodologies in this field. However, the existing methods generally use radiomics or deep convolutional features independently for pattern classification, which tend to take into account only global or local features, respectively. In this paper, we introduce and evaluate a hybrid structure that includes different features selected with radiomics model and CNNs and integrates these features to deal with GISTs classification. The Radiomics model and CNNs are constructed for global radiomics and local convolutional feature selection, respectively. Subsequently, we utilize distinct radiomics and deep convolutional features to perform pattern classification for GISTs. Specifically, we propose a new pooling strategy to assemble the deep convolutional features of 54 three-dimensional patches from the same case and integrate these features with the radiomics features for independent case, followed by random forest classifier. Our method can be extensively evaluated using multiple clinical datasets. The classification performance (area under the curve (AUC): 0.882; 95% confidence interval (CI): 0.816-0.947) consistently outperforms those of independent radiomics (AUC: 0.807; 95% CI: 0.724-0.892) and CNNs (AUC: 0.826; 95% CI: 0.795-0.856) approaches.","","","10.1109/JBHI.2018.2841992","National Natural Science Foundation of China; Science and Technology Program of Guangdong Province; Guangdong Provincial Key Laboratory of Medical Image Processing; Science and Technology Program of Guangzhou; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8368327","Radiomics;convolutional neural network;feature integration;gastrointestinal stromal tumors","Feature extraction;Tumors;Computed tomography;Cancer;Electron tubes;Medical diagnostic imaging","cancer;computerised tomography;convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);medical image processing;tumours","CNNs;medical image pattern classification;deep convolutional features;GISTs classification;global radiomics;local convolutional feature selection;distinct radiomics;radiomics features;gastrointestinal stromal tumors;computer-aided diagnosis system;deep learning algorithms;convolutional neural networks;random forest classifier","","3","48","","","","","IEEE","IEEE Journals"
"Audio–Visual Deep Clustering for Speech Separation","R. Lu; Z. Duan; C. Zhang","State Key Lab of Intelligent Technologies and SystemsBeijing National Research Center for Information Science and Technology (BNRist)Institute for Artificial Intelligence; Department of Electrical and Computer Engineering, Department of Computer Science, University of Rochester, Rochester, NY, USA; State Key Lab of Intelligent Technologies and SystemsBeijing National Research Center for Information Science and Technology (BNRist)Institute for Artificial Intelligence","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","11","1697","1712","Speech separation aims to separate individual voices from an audio mixture of multiple simultaneous talkers. Audio-only approaches show unsatisfactory performance when the speakers are of the same gender or share similar voice characteristics. This is due to challenges on learning appropriate feature representations for separating voices in single frames and streaming voices across time. Visual signals of speech (e.g., lip movements), if available, can be leveraged to learn better feature representations for separation. In this paper, we propose a novel audio-visual deep clustering model (AVDC) to integrate visual information into the process of learning better feature representations (embeddings) for Time-Frequency (T-F) bin clustering. It employs a two-stage audio-visual fusion strategy where speaker-wise audio-visual T-F embeddings are first computed after the first-stage fusion to model the audio-visual correspondence for each speaker. In the second-stage fusion, audio-visual embeddings of all speakers and audio embeddings calculated by deep clustering from the audio mixture are concatenated to form the final T-F embedding for clustering. Through a series of experiments, the proposed AVDC model is shown to outperform the audio-only deep clustering and utterance-level permutation invariant training baselines and three other state-of-the-art audio-visual approaches. Further analyses show that the AVDC model learns a better T-F embedding for alleviating the source permutation problem across frames. Other experiments show that the AVDC model is able to generalize across different numbers of speakers between training and testing and shows some robustness when visual information is partially missing.","","","10.1109/TASLP.2019.2928140","National Natural Science Foundation of China; NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762221","Speaker-independent speech separation;deep clustering;audio-visual fusion","Visualization;Lips;Spectrogram;Hidden Markov models;Training;Speech processing;Robustness","audio signal processing;audio-visual systems;blind source separation;feature extraction;learning (artificial intelligence);pattern clustering;speech processing","two-stage audio-visual fusion strategy;audio-visual deep clustering model;time-frequency bin clustering;visual signals;streaming voices;single frames;appropriate feature representations;similar voice characteristics;audio-only approaches;multiple simultaneous talkers;individual voices;speech separation;visual information;state-of-the-art audio-visual approaches;AVDC model;audio mixture;audio embeddings;audio-visual embeddings;second-stage fusion;audio-visual correspondence;first-stage fusion;speaker-wise audio-visual","","","60","Traditional","","","","IEEE","IEEE Journals"
"Online Asymmetric Metric Learning With Multi-Layer Similarity Aggregation for Cross-Modal Retrieval","Y. Wu; S. Wang; G. Song; Q. Huang","Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences (UCAS), Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2019","28","9","4299","4312","Cross-modal retrieval has attracted intensive attention in recent years, where a substantial yet challenging problem is how to measure the similarity between heterogeneous data modalities. Despite using modality-specific representation learning techniques, most existing shallow or deep models treat different modalities equally and neglect the intrinsic modality heterogeneity and information imbalance among images and texts. In this paper, we propose an online similarity function learning framework to learn the metric that can well reflect the cross-modal semantic relation. Considering that multiple CNN feature layers naturally represent visual information from low-level visual patterns to high-level semantic abstraction, we propose a new asymmetric image-text similarity formulation which aggregates the layer-wise visual-textual similarities parameterized by different bilinear parameter matrices. To effectively learn the aggregated similarity function, we develop three different similarity combination strategies, i.e., average kernel, multiple kernel learning, and layer gating. The former two kernel-based strategies assign uniform weights on different layers to all data pairs; the latter works on the original feature representation and assigns instance-aware weights on different layers to different data pairs, and they are all learned by preserving the bi-directional relative similarity expressed by a large number of cross-modal training triplets. The experiments conducted on three public datasets well demonstrate the effectiveness of our methods.","","","10.1109/TIP.2019.2908774","National Natural Science Foundation of China; National Basic Research Program of China (973 Program); China Postdoctoral Science Foundation; Key Research Program of Frontier Sciences of CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8680035","Cross-modal retrieval;asymmetric metric;online learning;multi-layer aggregation","Kernel;Visualization;Measurement;Semantics;Training;Feature extraction;Correlation","feature extraction;information retrieval;learning (artificial intelligence);text analysis","online asymmetric metric learning;multilayer similarity aggregation;cross-modal retrieval;heterogeneous data modalities;modality-specific representation learning techniques;intrinsic modality heterogeneity;information imbalance;online similarity function;cross-modal semantic relation;visual information;low-level visual patterns;high-level semantic abstraction;asymmetric image-text similarity formulation;layer-wise visual-textual similarities;aggregated similarity function;layer gating;bi-directional relative similarity;cross-modal training triplets;CNN feature layers;kernel learning;feature representation;data pairs;instance-aware weights;similarity combination strategies;bilinear parameter matrices","","","67","","","","","IEEE","IEEE Journals"
"Large-Scale Multi-Class Image-Based Cell Classification With Deep Learning","N. Meng; E. Y. Lam; K. K. Tsia; H. K. So","Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong; Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong; Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong; Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong","IEEE Journal of Biomedical and Health Informatics","","2019","23","5","2091","2098","Recent advances in ultra-high-throughput microscopy have enabled a new generation of cell classification methodologies using image-based cell phenotypes alone. In contrast to current single-cell analysis techniques that rely solely on slow and costly genetic/epigenetic analysis, these image-based analyses allow morphological profiling and screening of thousands or even millions of single cells at a fraction of the cost, and have been proven to demonstrate the statistical significance required for understanding the role of cell heterogeneity in diverse biological applications, ranging from cancer screening to drug candidate identification/validation processes. This paper examines the efficacies and opportunities presented by machine learning algorithms in processing large scale datasets with millions of label-free cell images. An automatic single-cell classification framework using convolutional neural network (CNN) has been developed. A comparative analysis of its efficiency in classifying large datasets against conventional k-nearest neighbors (kNN) and support vector machine (SVM) based methods are also presented. Experiments have shown that our proposed framework can efficiently identify multiple types cells with over 99% accuracy based on the phenotypic label-free bright-field images; and CNN-based models perform well and relatively stable against data volume compared with kNN and SVM.","","","10.1109/JBHI.2018.2878878","Research Grants Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516291","Cell classification;convolutional neural network;bright field imaging;multiclass classification","Computer architecture;Microprocessors;Feature extraction;Imaging;Informatics;Support vector machines;Throughput","cancer;cellular biophysics;convolutional neural nets;genetics;image classification;learning (artificial intelligence);medical image processing;support vector machines","k-nearest neighbors based methods;ultra-high-throughput microscopy;single-cell analysis techniques;convolutional neural network-based models;support vector machine based methods;phenotypic label-free bright-field images;single-cell classification framework;label-free cell images;cancer screening;cell heterogeneity;morphological profiling;deep learning;scale multiclass image-based cell classification","","2","44","Traditional","","","","IEEE","IEEE Journals"
"VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control","J. Zhang; L. Tai; P. Yun; Y. Xiong; M. Liu; J. Boedecker; W. Burgard","Department of Computer Science, University of Freiburg, Breisgau, Germany; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Computer Science, University of Freiburg, Breisgau, Germany; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Computer Science, University of Freiburg, Breisgau, Germany; Department of Computer Science, University of Freiburg, Breisgau, Germany","IEEE Robotics and Automation Letters","","2019","4","2","1148","1155","In this letter, we deal with the reality gap from a novel perspective, targeting transferring deep reinforcement learning (DRL) policies learned in simulated environments to the real-world domain for visual control tasks. Instead of adopting the common solutions to the problem by increasing the visual fidelity of synthetic images output from simulators during the training phase, we seek to tackle the problem by translating the real-world image streams back to the synthetic domain during the deployment phase, to make the robot feel at home. We propose this as a lightweight, flexible, and efficient solution for visual control, as first, no extra transfer steps are required during the expensive training of DRL agents in simulation; second, the trained DRL agents will not be constrained to being deployable in only one specific real-world environment; and third, the policy training and the transfer operations are decoupled, and can be conducted in parallel. Besides this, we propose a simple yet effective shift loss that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs. We validate the shift loss for artistic style transfer for videos and domain adaptation, and validate our visual control approach in indoor and outdoor robotics experiments.","","","10.1109/LRA.2019.2894216","Shenzhen Science and Technology Innovation Commission; German Research Foundation; Research Grant Council of Hong Kong SAR Government, China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620258","Deep learning in robotics and automation;visual-based navigation;model learning for control","Visualization;Training;Robots;Adaptation models;Semantics;Task analysis;Navigation","control engineering computing;learning (artificial intelligence);mobile robots;robot vision;virtual reality","VR-goggles;real-to-sim domain adaptation;deep reinforcement learning policies;simulated environments;real-world domain;visual control tasks;visual fidelity;synthetic images output;training phase;real-world image streams;synthetic domain;expensive training;trained DRL agents;real-world environment;policy training;transfer operations;consistent policy outputs;artistic style transfer;visual control approach;indoor robotics experiments;outdoor robotics experiments;synthetic image output;real-world image stream translation;DRL agent training;shift loss;domain adaptation","","","30","","","","","IEEE","IEEE Journals"
"CamStyle: A Novel Data Augmentation Method for Person Re-Identification","Z. Zhong; L. Zheng; Z. Zheng; S. Li; Y. Yang","Cognitive Science Department, Xiamen University, Xiamen, China; Research School of Computer Science, The Australian National University, Canberra, ACT, Australia; Centre for Artificial Intelligence, University of Technology Sydney, Ultimo, NSW, Australia; Cognitive Science Department, Xiamen University, Xiamen, China; Centre for Artificial Intelligence, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Image Processing","","2019","28","3","1176","1190","Person re-identification (re-ID) is a cross-camera retrieval task that suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle). CamStyle can serve as a data augmentation approach that reduces the risk of deep network overfitting and that smooths the CamStyle disparities. Specifically, with a style transfer model, labeled training images can be style transferred to each camera, and along with the original training samples, form the augmented training set. This method, while increasing data diversity against overfitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few camera systems in which overfitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of overfitting. We also report competitive accuracy compared with the state of the art on Market-1501 and DukeMTMC-re-ID. Importantly, CamStyle can be employed to the challenging problems of one view learning and unsupervised domain adaptation (UDA) in person re-identification (re-ID), both of which have critical research and application significance. The former only has labeled data in one camera view and the latter only has labeled data in the source domain. Experimental results show that CamStyle significantly improves the performance of the baseline in the two problems. Specially, for UDA, CamStyle achieves state-of-the-art accuracy based on a baseline deep re-ID model on Market-1501 and DukeMTMC-reID. Our code is available at: https://github.com/zhunzhong07/CamStyle.","","","10.1109/TIP.2018.2874313","National Natural Science Foundation of China; Fujian Province 2011 Collaborative Innovation Center of TCM Health Management; Collaborative Innovation Center of Chinese Oolong Tea Industry-Collaborative Innovation Center (2011) of Fujian Province; Fund for Integration of Cloud Computing and Big Data; Innovation of Science and Education; Data to Decisions CRC (D2D CRC); Cooperative Research Centre Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8485427","Person re-identification;CamStyle;one-view learning;unsupervised domain adaptation","Cameras;Training;Task analysis;Adaptation models;Machine learning;Data models;Australia","cameras;image recognition;learning (artificial intelligence)","person re-identification;cross-camera retrieval task;image style variations;camera-invariant descriptor subspace;camera style;data augmentation approach;deep network overfitting;style transfer model;data diversity;label smooth regularization;LSR;camera systems;DukeMTMC-re-ID;view learning;baseline deep re-ID model;Market-1501;UDA;unsupervised domain adaptation;CamStyle;data augmentation method","","10","87","","","","","IEEE","IEEE Journals"
"Taxi-Based Mobility Demand Formulation and Prediction Using Conditional Generative Adversarial Network-Driven Learning Approaches","H. Yu; X. Chen; Z. Li; G. Zhang; P. Liu; J. Yang; Y. Yang","Department of Civil and Environmental Engineering, University of Hawaii, Honolulu, HI, USA; School of Automation, Northwestern Polytechnical University, Xi’an, China; Department of Civil and Environmental Engineering, University of Hawaii, Honolulu, HI, USA; Department of Civil and Environmental Engineering, University of Hawaii, Honolulu, HI, USA; School of Transportation, Southeast University, Nanjing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Department of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, USA","IEEE Transactions on Intelligent Transportation Systems","","2019","20","10","3888","3899","In this paper, a deep learning (DL) framework was proposed to predict the taxi-passenger demand while the spatial, the temporal, and external dependencies were considered simultaneously. The proposed DL framework combined a modified density-based spatial clustering algorithm with noise (DBSCAN) and a conditional generative adversarial network (CGAN) model. More specifically, the modified DBSCAN model was applied to produce a number of sub-networks considering the spatial correlation of taxi pick-up events in the road network. And the CGAN model, fed with the historical taxi passenger demand and other conditional information, was capable to predict the taxi-passenger demands. The proposed CGAN model was made up with two long short-term memory (LSTM) neural networks, which are termed as the generative network G and the discriminative network D, respectively. Adversarial training process was conducted to the two LSTMs. In the numerical experiment, different model layouts were compared. It was found that different network layouts provided reasonable accuracy. With limited training data, more LSTM layers in the generator network resulted in not only higher accuracy, but also more difficulties in training. Comparisons were also conducted between the proposed prediction model and four typical approaches, including the moving average method, the autoregressive integrated moving method, the neural network model, and the LSTM neural network model. The comparison results showed that the proposed model outperformed all the other methods. And the repeated experiment indicated that the proposed CGAN model provided significant better predictions than the LSTM model did. Future research was recommended to include more datasets for testing the model and more information for improving predictive performance.","","","10.1109/TITS.2019.2923964","National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759890","Taxi system;machine learning;demand forecast;big transportation data","Public transportation;Predictive models;Generators;Training;Roads;Numerical models;Data models","autoregressive moving average processes;learning (artificial intelligence);neural nets;pattern clustering;public transport;regression analysis;traffic engineering computing","conditional generative adversarial network-driven learning approaches;deep learning framework;taxi-passenger demand;density-based spatial clustering algorithm;modified DBSCAN model;spatial correlation;taxi pick-up events;road network;CGAN model;historical taxi passenger demand;short-term memory neural networks;generative network;adversarial training process;prediction model;LSTM neural network model;predictive performance;taxi-based mobility demand formulation;discriminative network;autoregressive integrated moving method","","","38","","","","","IEEE","IEEE Journals"
"Fine-Grained Image Classification Using Modified DCNNs Trained by Cascaded Softmax and Generalized Large-Margin Losses","W. Shi; Y. Gong; X. Tao; D. Cheng; N. Zheng","Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Department of Computer Science, School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","3","683","694","We develop a fine-grained image classifier using a general deep convolutional neural network (DCNN). We improve the fine-grained image classification accuracy of a DCNN model from the following two aspects. First, to better model the h-level hierarchical label structure of the fine-grained image classes contained in the given training data set, we introduce h fully connected (fc) layers to replace the top fc layer of a given DCNN model and train them with the cascaded softmax loss. Second, we propose a novel loss function, namely, generalized large-margin (GLM) loss, to make the given DCNN model explicitly explore the hierarchical label structure and the similarity regularities of the fine-grained image classes. The GLM loss explicitly not only reduces between-class similarity and within-class variance of the learned features by DCNN models but also makes the subclasses belonging to the same coarse class be more similar to each other than those belonging to different coarse classes in the feature space. Moreover, the proposed fine-grained image classification framework is independent and can be applied to any DCNN structures. Comprehensive experimental evaluations of several general DCNN models (AlexNet, GoogLeNet and VGG) using three benchmark data sets (Stanford car, fine-grained visual classification-aircraft and CUB-200-2011) for the fine-grained image classification task demonstrate the effectiveness of our method.","","","10.1109/TNNLS.2018.2852721","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419081","Cascaded softmax loss;deep convolutional neural network (DCNN);fine-grained image classification;generalized large-margin (GLM) loss;hierarchical label structure","Training;Feature extraction;Data models;Measurement;Automobiles;Learning systems;Benchmark testing","feature extraction;image classification;learning (artificial intelligence);neural nets","coarse class;fine-grained image classification framework;DCNN structures;general DCNN models;fine-grained visual classification-aircraft;fine-grained image classification task;fine-grained image classifier;general deep convolutional neural network;fine-grained image classification accuracy;h-level hierarchical label structure;fine-grained image classes;cascaded softmax loss;loss function;large-margin loss;GLM loss;between-class similarity;DCNN model","","6","44","","","","","IEEE","IEEE Journals"
"Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval","L. Wu; Y. Wang; L. Shao","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; Faculty of Electronic Engineering, Dalian University of Technology, Dalian, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates","IEEE Transactions on Image Processing","","2019","28","4","1602","1612","In this paper, we propose a novel deep generative approach to cross-modal retrieval to learn hash functions in the absence of paired training samples through the cycle consistency loss. Our proposed approach employs adversarial training scheme to learn a couple of hash functions enabling translation between modalities while assuming the underlying semantic relationship. To induce the hash codes with semantics to the input-output pair, cycle consistency loss is further delved into the adversarial training to strengthen the correlation between the inputs and corresponding outputs. Our approach is generative to learn hash functions, such that the learned hash codes can maximally correlate each input-output correspondence and also regenerate the inputs so as to minimize the information loss. The learning to hash embedding is thus performed to jointly optimize the parameters of the hash functions across modalities as well as the associated generative models. Extensive experiments on a variety of large-scale cross-modal data sets demonstrate that our proposed method outperforms the state of the arts.","","","10.1109/TIP.2018.2878970","National Natural Science Foundation of China; Shenzhen Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517125","Cross-modal retrieval;generative hash;cycle-consistency","Semantics;Binary codes;Training;Correlation;Gallium nitride;Data models","cryptography;file organisation;information retrieval;learning (artificial intelligence)","hash functions;input-output pair;cycle consistency loss;learned hash codes;input-output correspondence;associated generative models;large-scale cross-modal data sets;cycle-consistent deep generative hashing;cross-modal retrieval;deep generative approach;adversarial training scheme","","10","60","","","","","IEEE","IEEE Journals"
"Low-dose computed tomography scheme incorporating residual learning-based denoising with iterative reconstruction","Y. Ding; T. Hu","College of Information Science and Electronic Engineering, Zhejiang University, People's Republic of China; College of Information Science and Electronic Engineering, Zhejiang University, People's Republic of China","Electronics Letters","","2019","55","4","174","176","Low-dose computed tomography has been highly desirable because of the health concern about excessive radiation dose, but also challenging due to insufficient or noisy projection data. Compared with post-processing methods by directly denoising filtered back-projection images, iterative reconstruction achieves excellent performance but consumes a large number of iterations. In this Letter, a two-stage method is proposed by incorporating residual learning-based denoising with iterative reconstruction. First, an intermediate image is reconstructed by compressed sensing iterative reconstruction. Then, the image is denoised by a deep neural network. Specially, a network performing two-level residual learning is designed to strengthen denoising effect. Experimental results show that the proposed method outperforms iterative reconstruction with better numeric results and comparable visual performance while consuming fewer iterations.","","","10.1049/el.2018.6449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643890","","","computerised tomography;image denoising;image reconstruction;iterative methods;learning (artificial intelligence);medical image processing;neural nets","excessive radiation dose;low-dose computed tomography scheme incorporating residual;fewer iteration;denoising effect;two-level residual learning;residual learning-based;iterative reconstruction;back-projection images;post-processing methods;noisy projection data;insufficient projection data","","","10","","","","","IET","IET Journals"
"LiDAR Data Classification Using Spatial Transformation and CNN","X. He; A. Wang; P. Ghamisi; G. Li; Y. Chen","Higher Education Key Laboratory for Measure and Control Technology and Instrumentations of Heilongjiang, Harbin University of Science and Technology, Harbin, China; Higher Education Key Laboratory for Measure and Control Technology and Instrumentations of Heilongjiang, Harbin University of Science and Technology, Harbin, China; Helmholtz Institute Freiberg for Resource Technology, Exploration, Helmholtz-Zentrum Dresden-Rossendorf, Freiberg, Germany; State Key Laboratory of Frozen Soil Engineering, Cold and Arid Regions Environmental and Engineering Research Institute, Chinese Academy of Sciences, Lanzhou, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","1","125","129","Light detection and ranging (LiDAR) is a useful data acquisition technique, which is widely used in a variety of practical applications. The classification of LiDAR-derived rasterized digital surface model (LiDAR-DSM) is a fundamental technique in LiDAR data processing. In recent years, deep learning methods, especially convolutional neural networks (CNNs), have shown their capability in remote sensing areas, including LiDAR data processing. Traditional deep models empirically use a fixed neighborhood system as input to the network. Therefore, the weight and height of the input rectangle may not be optimal. In order to modify such handcrafted setting, a spatial transformation network is used here to identify optimal inputs. The transformed inputs are fed into a well-designed CNN to obtain the final classification results. Furthermore, morphological profiles are combined with spatial transformation CNN to further improve the classification accuracy. The proposed frameworks are tested on two LiDAR-DSMs (i.e., the Recology and Houston data sets). The experimental results show that the proposed models provide competitive results compared to the state-of-the-art methods. Furthermore, the proposed optimal input identification approach can also be found beneficial for other remote sensing applications.","","","10.1109/LGRS.2018.2868378","National Natural Science Foundation of China; State Key Laboratory of Frozen Soil Engineering; “High Potential Program” of Helmholtz-Zentrum Dresden-Rossendorf; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8480863","Convolutional neural networks (CNNs);deep learning;feature extraction;light detection and ranging (LiDAR);morphological profile (MP);spatial transformation network (STN)","Laser radar;Shape;Generators;Remote sensing;Feature extraction;Data mining;Spatial databases","data acquisition;feedforward neural nets;geophysical image processing;image classification;learning (artificial intelligence);optical radar;remote sensing","LiDAR data classification;useful data acquisition technique;LiDAR-derived rasterized digital surface model;LiDAR-DSM;fundamental technique;LiDAR data processing;deep learning methods;convolutional neural networks;remote sensing areas;traditional deep models;spatial transformation network;final classification results;spatial transformation CNN;classification accuracy;optimal input identification approach;remote sensing applications","","","13","","","","","IEEE","IEEE Journals"
"Deep Metadata Fusion for Traffic Light to Lane Assignment","T. Langenberg; T. Lüddecke; F. Wörgötter","Daimler AG, Mercedes-Benz Cars, Research and Development, Sindelfingen, Germany; Third Physical Institute, Georg-August University Göttingen, Göttingen, Germany; Third Physical Institute, Georg-August University Göttingen, Göttingen, Germany","IEEE Robotics and Automation Letters","","2019","4","2","973","980","We present a deep metadata fusion approach that connects image data and heterogeneous metadata inside a Convolutional Neural Network (CNN). This approach enables us to assign all relevant traffic lights to their associated lanes. To achieve this, a common CNN topology is trained by down-sampled and transformed input images to predict an indication vector. The indication vector contains the column positions of all the relevant traffic lights that are associated with lanes. In parallel, we fuse prepared and adaptively weighted Metadata Feature Maps (MFM) with the convolutional feature map input of a selected convolutional layer. The results are compared to rule-based, only-metadata, and only-vision approaches. In addition, human performance of the traffic light to ego-vehicle lane assignment has been measured by a subjective test. The proposed approach outperforms all other approaches. It achieves about 93.0% average precision for a real-world dataset. In a more complex dataset, 87.1% average precision is achieved. In particular, the new approach reaches significantly higher results with 93.7% to 91.0% average accuracy for a real-world dataset in contrast to lower human performance.","","","10.1109/LRA.2019.2893446","H2020 Leadership in Enabling and Industrial Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613841","Intelligent transportation systems;computer vision for transportation;deep learning in robotics and automation","Metadata;Cameras;Image color analysis;Convolutional neural networks;Detectors;Roads;Semantics","computer vision;convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);meta data;road traffic;road traffic control;road vehicles","deep metadata fusion approach;image data;heterogeneous metadata;common CNN topology;input images;indication vector;convolutional feature map input;only-metadata;only-vision approaches;ego-vehicle lane assignment;convolutional neural network;traffic lights;adaptively weighted metadata feature maps","","","33","","","","","IEEE","IEEE Journals"
"Saliency-Guided Deep Neural Networks for SAR Image Change Detection","J. Geng; X. Ma; X. Zhou; H. Wang","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","10","7365","7377","Change detection is an important task to identify land-cover changes between the acquisitions at different times. For synthetic aperture radar (SAR) images, inherent speckle noise of the images can lead to false changed points, which affects the change detection performance. Besides, the supervised classifier in change detection framework requires numerous training samples, which are generally obtained by manual labeling. In this paper, a novel unsupervised method named saliency-guided deep neural networks (SGDNNs) is proposed for SAR image change detection. In the proposed method, to weaken the influence of speckle noise, a salient region that probably belongs to the changed object is extracted from the difference image. To obtain pseudotraining samples automatically, hierarchical fuzzy C-means (HFCM) clustering is developed to select samples with higher probabilities to be changed and unchanged. Moreover, to enhance the discrimination of sample features, DNNs based on the nonnegative- and Fisher-constrained autoencoder are applied for final detection. Experimental results on five real SAR data sets demonstrate the effectiveness of the proposed approach.","","","10.1109/TGRS.2019.2913095","Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8713939","Change detection;deep neural networks (DNNs);synthetic aperture radar (SAR) image;unsupervised learning","Radar polarimetry;Speckle;Feature extraction;Training;Noise reduction;Neural networks;Synthetic aperture radar","fuzzy set theory;geophysical image processing;geophysical techniques;image classification;neural nets;radar imaging;remote sensing by radar;speckle;synthetic aperture radar;unsupervised learning","change detection performance;change detection framework;SAR image change detection;changed object;synthetic aperture radar images;false changed points;saliency-guided deep neural networks;land-cover;speckle noise;supervised classifier;unsupervised method;SGDNNs;sample feature discrimination;hierarchical fuzzy C-means clustering;HFCM clustering;probabilities;Fisher-constrained autoencoder;nonnegative-constrained autoencoder","","2","51","","","","","IEEE","IEEE Journals"
"Quality Evaluation of Image Dehazing Methods Using Synthetic Hazy Images","X. Min; G. Zhai; K. Gu; Y. Zhu; J. Zhou; G. Guo; X. Yang; X. Guan; W. Zhang","Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Beijing Key Laboratory of Computational Intelligence and Intelligent System, Faculty of Information Technology, Beijing University of Technology, Beijing, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer and Information ScienceFaculty of Science and Technology; Institute of Deep Learning and with the National Engineering Laboratory for Deep Learning Technology and Application, Baidu Research, Beijing, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Multimedia","","2019","21","9","2319","2333","To enhance the visibility and usability of images captured in hazy conditions, many image dehazing algorithms (DHAs) have been proposed. With so many image DHAs, there is a need to evaluate and compare these DHAs. Due to the lack of the reference haze-free images, DHAs are generally evaluated qualitatively using real hazy images. But it is possible to perform quantitative evaluation using synthetic hazy images since the reference haze-free images are available and full-reference (FR) image quality assessment (IQA) measures can be utilized. In this paper, we follow this strategy and study DHA evaluation using synthetic hazy images systematically. We first build a synthetic haze removing quality (SHRQ) database. It consists of two subsets: regular and aerial image subsets, which include 360 and 240 dehazed images created from 45 and 30 synthetic hazy images using 8 DHAs, respectively. Since aerial imaging is an important application area of dehazing, we create an aerial image subset specifically. We then carry out subjective quality evaluation study on these two subsets. We observe that taking DHA evaluation as an exact FR IQA process is questionable, and the state-of-the-art FR IQA measures are not effective for DHA evaluation. Thus, we propose a DHA quality evaluation method by integrating some dehazing-relevant features, including image structure recovering, color rendition, and over-enhancement of low-contrast areas. The proposed method works for both types of images, but we further improve it for aerial images by incorporating its specific characteristics. Experimental results on two subsets of the SHRQ database validate the effectiveness of the proposed measures.","","","10.1109/TMM.2019.2902097","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Macau Science and Technology Development Fund; Research Committee at the University of Macau; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8654007","Image dehazing;dehazing algorithm evaluation;quality assessment;synthetic haze;regular/aerial image","Atmospheric modeling;Image color analysis;Atmospheric measurements;Databases;Scattering;Computational modeling;Imaging","feature extraction;image capture;image colour analysis;image denoising;image enhancement;image restoration","image dehazing algorithms;image DHAs;image structure recovering;aerial images;image quality assessment;DHA quality evaluation;image capture;image enhancement;synthetic haze removing quality database;dehazing-relevant features","","4","62","Traditional","","","","IEEE","IEEE Journals"
"Unsupervised Change Detection Based on a Unified Framework for Weighted Collaborative Representation With RDDL and Fuzzy Clustering","G. Yang; H. Li; W. Wang; W. Yang; W. J. Emery","Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; School of Electronic Information, Wuhan University, Wuhan, China; Department of Aerospace Engineering Sciences, University of Colorado, Boulder, CO, USA","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","8890","8903","In this paper, we propose a novel unsupervised change detection method of remote sensing (RS) images based on a unified framework for weighted collaborative representation (WCR) with robust deep dictionary learning (RDDL) and fuzzy clustering. Specifically, WCR is employed to collaboratively represent neighborhood features with lower computational complexity, for which the RDDL model is built to learn more effective and representative overcomplete dictionary and enhance the robustness against the noise and outliers. Meanwhile, in order to make the resulting collaborative coefficients more beneficial for clustering, the unified framework for WCR with RDDL and fuzzy clustering is designed. By doing so, our framework not only precludes the utilization of third-party clustering algorithm, but also achieves better detection performance. Subsequently, the spatial constraint is enforced on the membership matrix to yield the updated one for further improving the accuracy of change detection. Finally, a binary change mask (CM) is achieved by assigning the pixels into the changed and unchanged classes. Experiments are performed on five pairs of RS images, and experimental results demonstrate the effectiveness of the proposed method.","","","10.1109/TGRS.2019.2923643","National Natural Science Foundation of China; Frontier Intersection Basic Research Project for the Central Universities; Southwest Jiaotong University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778743","Collaborative representation;deep learning 23 (DL);dictionary learning;fuzzy clustering;remote sensing (RS) 24 image;unsupervised change detection","Collaboration;Dictionaries;Clustering algorithms;Change detection algorithms;Feature extraction;Machine learning;Principal component analysis","computational complexity;fuzzy set theory;image representation;object detection;pattern clustering;remote sensing","robust deep dictionary learning;fuzzy clustering;WCR;RDDL model;third-party clustering algorithm;binary change mask;RS images;weighted collaborative representation;unsupervised change detection method;remote sensing images;membership matrix","","","59","","","","","IEEE","IEEE Journals"
"Hierarchical Features Driven Residual Learning for Depth Map Super-Resolution","C. Guo; C. Li; J. Guo; R. Cong; H. Fu; P. Han","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; College of Electronic Information and Automation, Civil Aviation University of China, Tianjin, China","IEEE Transactions on Image Processing","","2019","28","5","2545","2557","Rapid development of affordable and portable consumer depth cameras facilitates the use of depth information in many computer vision tasks such as intelligent vehicles and 3D reconstruction. However, depth map captured by low-cost depth sensors (e.g., Kinect) usually suffers from low spatial resolution, which limits its potential applications. In this paper, we propose a novel deep network for depth map super-resolution (SR), called DepthSR-Net. The proposed DepthSR-Net automatically infers a high-resolution (HR) depth map from its low-resolution (LR) version by hierarchical features driven residual learning. Specifically, DepthSR-Net is built on residual U-Net deep network architecture. Given LR depth map, we first obtain the desired HR by bicubic interpolation upsampling and then construct an input pyramid to achieve multiple level receptive fields. Next, we extract hierarchical features from the input pyramid, intensity image, and encoder-decoder structure of U-Net. Finally, we learn the residual between the interpolated depth map and the corresponding HR one using the rich hierarchical features. The final HR depth map is achieved by adding the learned residual to the interpolated depth map. We conduct an ablation study to demonstrate the effectiveness of each component in the proposed network. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art methods. In addition, the potential usage of the proposed network in other low-level vision problems is discussed.","","","10.1109/TIP.2018.2887029","National Natural Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8579111","Convolutional neural network (CNN);depth map super-resolution (SR);residual learning;image reconstruction","Color;Feature extraction;Image reconstruction;Spatial resolution;Task analysis;Cameras","cameras;computerised instrumentation;decoding;feature extraction;image coding;image resolution;image sampling;interpolation;learning (artificial intelligence);spatial variables measurement","interpolated depth map;low-level vision problems;residual learning;portable consumer depth cameras;high-resolution depth map;consumer depth cameras;depth sensors;LR depth map;depth map superresolution;DepthSR-Net;U-net deep network architecture;computer vision;intelligent vehicles;3D reconstruction;encoder-decoder structure;bicubic interpolation upsampling","","4","54","","","","","IEEE","IEEE Journals"
"Deep Ordinal Hashing With Spatial Attention","L. Jin; X. Shu; K. Li; Z. Li; G. Qi; J. Tang","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Facebook, Menlo Park, CA, USA; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Computer Science, University of Central Florida, Orlando, FL, USA; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Image Processing","","2019","28","5","2173","2186","Hashing has attracted increasing research attention in recent years due to its high efficiency of computation and storage in image retrieval. Recent works have demonstrated the superiority of simultaneous feature representations and hash functions learning with deep neural networks. However, most existing deep hashing methods directly learn the hash functions by encoding the global semantic information, while ignoring the local spatial information of images. The loss of local spatial structure makes the performance bottleneck of hash functions, therefore limiting its application for accurate similarity retrieval. In this paper, we propose a novel deep ordinal hashing (DOH) method, which learns ordinal representations to generate ranking-based hash codes by leveraging the ranking structure of feature space from both local and global views. In particular, to effectively build the ranking structure, we propose to learn the rank correlation space by exploiting the local spatial information from fully convolutional network and the global semantic information from the convolutional neural network simultaneously. More specifically, an effective spatial attention model is designed to capture the local spatial information by selectively learning well-specified locations closely related to target objects. In such hashing framework, the local spatial and global semantic nature of images is captured in an end-to-end ranking-to-hashing manner. Experimental results conducted on three widely used datasets demonstrate that the proposed DOH method significantly outperforms the state-of-the-art hashing methods.","","","10.1109/TIP.2018.2883522","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550736","Hashing;image retrieval;ranking structure;fully convolutional network;convolutional neural network;local spatial;global semantic information","Semantics;Image retrieval;Correlation;Convolutional neural networks;Visualization;Convolutional codes","convolutional neural nets;file organisation;image representation;image retrieval","deep ordinal hashing;image retrieval;deep neural networks;global semantic information;local spatial information;convolutional neural network;end-to-end ranking-to-hashing manner;feature representations;hash function learning;rank correlation space learning;DOH;fully convolutional network","","2","58","","","","","IEEE","IEEE Journals"
"RNN-Based Path Prediction of Obstacle Vehicles With Deep Ensemble","K. Min; D. Kim; J. Park; K. Huh","Department of Automotive Engineering, Hanyang University, Seoul, South Korea; Department of Automotive Engineering, Hanyang University, Seoul, South Korea; Department of Automotive Engineering, Hanyang University, Seoul, South Korea; Department of Automotive Engineering, Hanyang University, Seoul, South Korea","IEEE Transactions on Vehicular Technology","","2019","68","10","10252","10256","In this paper, a new approach for obstacle vehicle path prediction, which is important for advanced driver assistance systems (ADAS) and autonomous vehicles, is proposed based on a deep neural network. In order to analyze sequential sensor data, a recurrent neural network (RNN) is used and the input data for RNN is drawn from three sensors: LIDAR, camera and GPS. These sensor data are obtained experimentally with real vehicles. In addition, deep ensemble is used for robustness of the estimation and acquisition of the uncertainty. The predicted path of the proposed method is continuous and it predicts both short-term and long-term path with a single algorithm. The size of the network model is small, but it shows good performance in predicting future trajectory of obstacle vehicles.","","","10.1109/TVT.2019.2933232","Industrial Strategic Technology Development Program; Ministry of Trade, Industry and Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8788650","Path prediction;deep learning;ADAS;ensemble","Trajectory;Standards;Hidden Markov models;Recurrent neural networks;Uncertainty;Predictive models;Training","cameras;driver information systems;Global Positioning System;optical radar;recurrent neural nets;road vehicles","deep ensemble;obstacle vehicle path prediction;autonomous vehicles;deep neural network;sequential sensor data;recurrent neural network;camera;network model;RNN-based path prediction;LIDAR;GPS;advanced driver assistance systems","","","27","Traditional","","","","IEEE","IEEE Journals"
"Fast and Accurate Sparse Coding of Visual Stimuli With a Simple, Ultralow-Energy Spiking Architecture","W. Woods; C. Teuscher","Department of Electrical and Computer Engineering, Portland State University, Portland, OR, USA; Department of Electrical and Computer Engineering, Portland State University, Portland, OR, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","7","2173","2187","Memristive crossbars have become a popular means for realizing unsupervised and supervised learning techniques. In previous neuromorphic architectures with leaky integrate-and-fire neurons, the crossbar itself has been separated from the neuron capacitors to preserve mathematical rigor. In this paper, we sought to design a simplified sparse coding circuit without this restriction, resulting in a fast circuit that approximated a sparse coding operation at a minimal loss in accuracy. We showed that connecting the neurons directly to the crossbar resulted in a more energy-efficient sparse coding architecture and alleviated the need to prenormalize receptive fields. This paper provides derivations for the design of such a network, named the simple spiking locally competitive algorithm, as well as CMOS designs and results on the CIFAR and MNIST data sets. Compared to a nonspiking, nonapproximate model which scored 33% on CIFAR-10 with a single-layer classifier, this hardware scored 32% accuracy. When used with a state-of-the-art deep learning classifier, the nonspiking model achieved 82% and our simplified, spiking model achieved 80% while compressing the input data by 92%. Compared to a previously proposed spiking model, our proposed hardware consumed 99% less energy to do the same work at 21× the throughput. Accuracy held out with online learning to a write variance of 3%, suitable for the often reported 4-bit resolution required for neuromorphic algorithms, with offline learning to a write variance of 27%, and with read variance to 40%. The proposed architecture's excellent accuracy, throughput, and significantly lower energy usage demonstrate the utility of our innovations.","","","10.1109/TNNLS.2018.2878002","National Science Foundation; Defense Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8541104","Locally competitive algorithm (LCA);memristors;neuromorphic architecture;sparse coding;spiking architecture","Encoding;Throughput;Hardware;Neuromorphics;Computer architecture;Neurons;Visualization","memristors;neural chips;unsupervised learning","visual stimuli;ultralow-energy spiking architecture;memristive crossbars;unsupervised learning techniques;crossbar;neuron capacitors;simplified sparse coding circuit;fast circuit;sparse coding operation;minimal loss;energy-efficient sparse coding architecture;receptive fields;simple spiking locally competitive algorithm;nonspiking model;CIFAR-10;single-layer classifier;spiking model;online learning;write variance;neuromorphic algorithms;sparse coding;leaky integrate-and-fire neurons;deep learning classifier;energy usage;CIFAR-10 dataset;MNIST dataset","","","40","","","","","IEEE","IEEE Journals"
"R3-Net: A Deep Network for Multioriented Vehicle Detection in Aerial Images and Videos","Q. Li; L. Mou; Q. Xu; Y. Zhang; X. X. Zhu","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Remote Sensing Technology Institute, German Aerospace Center, Wessling, Germany; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Geodesy and Geomatics Engineering, Canada Research Chair Laboratory in Advanced Geomatics Image Processing, University of New Brunswick, Fredericton, NB, Canada; Remote Sensing Technology Institute, German Aerospace Center, Wessling, Germany","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","7","5028","5042","Vehicle detection is a significant and challenging task in aerial remote sensing applications. Most existing methods detect vehicles with regular rectangle boxes and fail to offer the orientation of vehicles. However, the orientation information is crucial for several practical applications, such as the trajectory and motion estimation of vehicles. In this paper, we propose a novel deep network, called a rotatable region-based residual network (R3-Net), to detect multioriented vehicles in aerial images and videos. More specially, R3-Net is utilized to generate rotatable rectangular target boxes in a half coordinate system. First, we use a rotatable region proposal network (R-RPN) to generate rotatable region of interests (R-RoIs) from feature maps produced by a deep convolutional neural network. Here, a proposed batch averaging rotatable anchor strategy is applied to initialize the shape of vehicle candidates. Next, we propose a rotatable detection network (R-DN) for the final classification and regression of the R-RoIs. In R-DN, a novel rotatable position-sensitive pooling is designed to keep the position and orientation information simultaneously while downsampling the feature maps of R-RoIs. In our model, R-RPN and R-DN can be trained jointly. We test our network on two open vehicle detection image data sets, namely, DLR 3K Munich Data set and VEDAI Data set, demonstrating the high precision and robustness of our method. In addition, further experiments on aerial videos show the good generalization capability of the proposed method and its potential for vehicle tracking in aerial videos. The demo video is available at https://youtu.be/xCYD-tYudN0.","","","10.1109/TGRS.2019.2895362","National Defense Science and Technology Innovation Zone; National Natural Science Foundation of China; China Scholarship Council; H2020 European Research Council; Helmholtz-Gemeinschaft; Bayerische Akademie der Wissenschaften; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8651485","Aerial images and videos;deep learning;multioriented detection;remote sensing;vehicle detection","Feature extraction;Vehicle detection;Remote sensing;Videos;Task analysis;Automobiles;Object detection","convolutional neural nets;feature extraction;image classification;image segmentation;motion estimation;object detection;remote sensing;traffic engineering computing;vehicles;video signal processing","deep network;multioriented vehicle detection;aerial images;aerial remote sensing applications;regular rectangle boxes;orientation information;rotatable region-based residual network;multioriented vehicles;rotatable rectangular target boxes;rotatable region proposal network;R-RPN;R-RoIs;feature maps;deep convolutional neural network;batch averaging rotatable anchor strategy;vehicle candidates;rotatable detection network;R-DN;rotatable position-sensitive pooling;open vehicle detection image data sets;DLR 3K Munich Data;aerial videos;vehicle tracking;demo video;trajectory estimation;vehicle motion estimation;R3-Net","","1","58","","","","","IEEE","IEEE Journals"
"Wasserstein CNN: Learning Invariant Features for NIR-VIS Face Recognition","R. He; X. Wu; Z. Sun; T. Tan","National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Center for Excellence in Brain Science and Intelligence Technology, CASIA, CAS, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Center for Excellence in Brain Science and Intelligence Technology, CASIA, CAS, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Center for Excellence in Brain Science and Intelligence Technology, CASIA, CAS, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Center for Excellence in Brain Science and Intelligence Technology, CASIA, CAS, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","7","1761","1773","Heterogeneous face recognition (HFR) aims at matching facial images acquired from different sensing modalities with mission-critical applications in forensics, security and commercial sectors. However, HFR presents more challenging issues than traditional face recognition because of the large intra-class variation among heterogeneous face images and the limited availability of training samples of cross-modality face image pairs. This paper proposes the novel Wasserstein convolutional neural network (WCNN) approach for learning invariant features between near-infrared (NIR) and visual (VIS) face images (i.e., NIR-VIS face recognition). The low-level layers of the WCNN are trained with widely available face images in the VIS spectrum, and the high-level layer is divided into three parts: the NIR layer, the VIS layer and the NIR-VIS shared layer. The first two layers aim at learning modality-specific features, and the NIR-VIS shared layer is designed to learn a modality-invariant feature subspace. The Wasserstein distance is introduced into the NIR-VIS shared layer to measure the dissimilarity between heterogeneous feature distributions. W-CNN learning is performed to minimize the Wasserstein distance between the NIR distribution and the VIS distribution for invariant deep feature representations of heterogeneous face images. To avoid the over-fitting problem on small-scale heterogeneous face data, a correlation prior is introduced on the fully-connected WCNN layers to reduce the size of the parameter space. This prior is implemented by a low-rank constraint in an end-to-end network. The joint formulation leads to an alternating minimization for deep feature representation at the training stage and an efficient computation for heterogeneous data at the testing stage. Extensive experiments using three challenging NIR-VIS face recognition databases demonstrate the superiority of the WCNN method over state-of-the-art methods.","","","10.1109/TPAMI.2018.2842770","State Key Development Program; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370677","Heterogeneous face recognition;VIS-NIR face matching;feature representation","Face;Face recognition;Sensors;Databases;Training;Correlation;Feature extraction","convolutional neural nets;face recognition;feature extraction;image matching;image representation;learning (artificial intelligence)","Wasserstein CNN;invariant features;NIR-VIS face recognition;heterogeneous face recognition;HFR;facial images;security;commercial sectors;traditional face recognition;heterogeneous face images;cross-modality face image pairs;neural network approach;low-level layers;high-level layer;NIR-VIS shared layer;modality-specific features;modality-invariant feature subspace;Wasserstein distance;heterogeneous feature distributions;w-CNN learning;NIR distribution;invariant deep feature representations;small-scale heterogeneous face data;WCNN layers;deep feature representation;recognition databases;sensing modalities","","6","59","","","","","IEEE","IEEE Journals"
"Impact of Data Loss for Prediction of Traffic Flow on an Urban Road Using Neural Networks","T. Pamuła","Faculty of Transport, Silesian University of Technology, Gliwice, Poland","IEEE Transactions on Intelligent Transportation Systems","","2019","20","3","1000","1009","The deployment of intelligent transport systems requires efficient means of assessing the traffic situation. This involves gathering real traffic data from the road network and predicting the evolution of traffic parameters, in many cases based on incomplete or false data from vehicle detectors. Traffic flows in the network follow spatiotemporal patterns and this characteristic is used to suppress the impact of missing or erroneous data. The application of multilayer perceptrons and deep learning networks using autoencoders for the prediction task is evaluated. Prediction sensitivity to false data is estimated using traffic data from an urban traffic network.","","","10.1109/TITS.2018.2836141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370052","Deep learning;traffic flow prediction;sensitivity to loss of data","Roads;Neurons;Neural networks;Machine learning;Predictive models;Data models;Adaptation models","learning (artificial intelligence);multilayer perceptrons;road traffic;traffic engineering computing","road network;traffic parameters;false data;missing data;erroneous data;deep learning networks;prediction task;prediction sensitivity;traffic data;urban traffic network;data loss;traffic flow;urban road;neural networks;intelligent transport systems;traffic situation","","1","35","","","","","IEEE","IEEE Journals"
"uDAS: An Untied Denoising Autoencoder With Sparsity for Spectral Unmixing","Y. Qu; H. Qi","Department of Electrical Engineering and Computer Science, Advanced Imaging and Collaborative Information Processing Group, University of Tennessee, Knoxville, TN, USA; Department of Electrical Engineering and Computer Science, Advanced Imaging and Collaborative Information Processing Group, University of Tennessee, Knoxville, TN, USA","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","3","1698","1712","Linear spectral unmixing is the practice of decomposing the mixed pixel into a linear combination of the constituent endmembers and the estimated abundances. This paper focuses on unsupervised spectral unmixing where the endmembers are unknown a priori. Conventional approaches use either geometrical- or statistical-based approaches. In this paper, we address the challenges of spectral unmixing with unsupervised deep learning models, in specific, the autoencoder models, where the decoder serves as the endmembers and the hidden layer output serves as the abundances. In several recent attempts, part-based autoencoders have been designed to solve the unsupervised spectral unmixing problem. However, the performance has not been satisfactory. In this paper, we first discuss some important findings we make on issues with part-based autoencoders. By proof of counterexample, we show that all existing part-based autoencoder networks with nonnegative and tied encoder and decoder are inherently defective by making these inappropriate assumptions on the network structure. As a result, they are not suitable for solving the spectral unmixing problem. We propose a so-called untied denoising autoencoder with sparsity, in which the encoder and decoder of the network are independent, and only the decoder of the network is enforced to be nonnegative. Furthermore, we make two critical additions to the network design. First, since denoising is an essential step for spectral unmixing, we propose to incorporate the denoising capacity into the network optimization in the format of a denoising constraint rather than cascading another denoising preprocessor in order to avoid the introduction of additional reconstruction error. Second, to be more robust to the inaccurate estimation of a number of endmembers, we adopt an $l_{21}$ -norm on the encoder of the network to reduce the redundant endmembers while decreasing the reconstruction error simultaneously. The experimental results demonstrate that the proposed approach outperforms several state-of-the-art methods, especially for highly noisy data.","","","10.1109/TGRS.2018.2868690","Intelligence Advanced Research Projects Activity; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8476591","Deep learning;denoising autoencoder;hyperspectral image (HSI);spectral unmixing","Decoding;Noise reduction;Machine learning;Estimation;Collaboration;Minimization;Matrix decomposition","hyperspectral imaging;image coding;image denoising;remote sensing;unsupervised learning","untied denoising autoencoder;linear spectral unmixing;mixed pixel;linear combination;unsupervised deep learning models;autoencoder models;hidden layer output;unsupervised spectral unmixing problem;nonnegative tied encoder;network structure;network design;denoising capacity;network optimization;denoising preprocessor;autoencoder network","","4","76","","","","","IEEE","IEEE Journals"
"Optimizing a Parameterized Plug-and-Play ADMM for Iterative Low-Dose CT Reconstruction","J. He; Y. Yang; Y. Wang; D. Zeng; Z. Bian; H. Zhang; J. Sun; Z. Xu; J. Ma","School of Biomedical Engineering, Southern Medical University, Guangzhou, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Biomedical Engineering, Southern Medical University, Guangzhou, China; School of Biomedical Engineering, Southern Medical University, Guangzhou, China; School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Department of Biomedical Engineering, Johns Hopkins University, Baltimore, MD, USA; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Biomedical Engineering, Southern Medical University, Guangzhou, China","IEEE Transactions on Medical Imaging","","2019","38","2","371","382","Reducing the exposure to X-ray radiation while maintaining a clinically acceptable image quality is desirable in various CT applications. To realize low-dose CT (LdCT) imaging, model-based iterative reconstruction (MBIR) algorithms are widely adopted, but they require proper prior knowledge assumptions in the sinogram and/or image domains and involve tedious manual optimization of multiple parameters. In this paper, we propose a deep learning (DL)-based strategy for MBIR to simultaneously address prior knowledge design and MBIR parameter selection in one optimization framework. Specifically, a parameterized plug-and-play alternating direction method of multipliers (3pADMM) is proposed for the general penalized weighted least-squares model, and then, by adopting the basic idea of DL, the parameterized plug-and-play (3p) prior and the related parameters are optimized simultaneously in a single framework using a large number of training data. The main contribution of this paper is that the 3p prior and the related parameters in the proposed 3pADMM framework can be supervised and optimized simultaneously to achieve robust LdCT reconstruction performance. Experimental results obtained on clinical patient datasets demonstrate that the proposed method can achieve promising gains over existing algorithms for LdCT image reconstruction in terms of noise-induced artifact suppression and edge detail preservation.","","","10.1109/TMI.2018.2865202","National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; Science and Technology Program of Guangdong, China; Science and Technology Program of Guangzhou, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8434327","Low-dose CT;parameterized plug-and-play ADMM;deep learning","Image reconstruction;Computed tomography;Optimization;Biomedical imaging;Machine learning;X-ray imaging","computerised tomography;image denoising;image reconstruction;iterative methods;learning (artificial intelligence);least squares approximations;medical image processing;optimisation","multiple parameters;MBIR parameter selection;optimization framework;3pADMM framework;LdCT image reconstruction;iterative low-dose CT reconstruction;CT applications;sinogram;deep learning;alternating direction method of multipliers;image quality;optimization;penalized weighted least-squares model;parameterized plug-and-play;DL;clinical patient datasets;noise-induced artifact suppression;edge detail preservation;X-ray radiation;model-based iterative reconstruction","","2","46","","","","","IEEE","IEEE Journals"
"Multi-Grained Deep Feature Learning for Robust Pedestrian Detection","C. Lin; J. Lu; J. Zhou","Department of Automation, State Key Laboratory of Intelligent Technologies and Systems, Tsinghua University, Beijing, China; Department of Automation, State Key Laboratory of Intelligent Technologies and Systems, Tsinghua University, Beijing, China; Department of Automation, State Key Laboratory of Intelligent Technologies and Systems, Tsinghua University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","12","3608","3621","In this paper, we address the challenging problem of detecting pedestrians, which are heavily occluded and/or far from cameras. Unlike most existing pedestrian detection methods which only use coarse-resolution feature maps with fixed receptive fields, our approach exploits multi-grained deep features to make the detector robust to visible parts of occluded pedestrians and small-size targets. Specifically, we jointly train a multi-scale network and a human parsing network in a weakly supervised manner with only bounding box annotations. We carefully design the multi-scale network to predict pedestrians of particular scales with the most appropriate feature maps, by matching their receptive fields with the target sizes. The human parsing network generates a fine-grained attention map, which helps guide the detector to focus on the visible parts of occluded pedestrians and small-size instances. Both networks are computed in parallel and form a unified single stage pedestrian detector, which assures a suitable tradeoff between accuracy and speed. Moreover, we introduce an adversarial hiding network to make our detector more robust to occlusion situations, which generates occlusions on pedestrians with the goal to fool the detector that in turn adapts itself to learn to localize these adversarial instances. Experiments on three challenging pedestrian detection benchmarks show that our proposed method achieves a state-of-the-art performance and executes  $2\times $  faster than the competitive methods.","","","10.1109/TCSVT.2018.2883558","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Shenzhen Fundamental Research Fund (Subject Arrangement); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550692","Pedestrian detection;human parsing;attention;deep feature learning","Feature extraction;Detectors;Image resolution;Semantics;Proposals;Task analysis;Generative adversarial networks","","","","","73","IEEE","","","","IEEE","IEEE Journals"
"Synthetic Aperture Radar Image Generation With Deep Generative Models","K. Wang; G. Zhang; Y. Leng; H. Leung","School of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Electrical and Computer Engineering, University of Calgary, Calgary, AB, Canada","IEEE Geoscience and Remote Sensing Letters","","2019","16","6","912","916","A variety of machine learning approaches have been applied to synthetic aperture radar (SAR) automatic target recognition. The performances of these approaches rely strongly on the quality and quantity of training data. In real-world applications, however, it is challenging to obtain sufficient data suitable for these approaches. To alleviate this problem, a novel deep generative model for SAR image generation is proposed, which is an extension of Wasserstein autoencoder. The network structure and reconstruction loss function of the model have been improved according to the characteristics of SAR images. The experimental results demonstrate that our model is superior to other classical generative models in SAR image generation. The generated images can be directly used as training samples, thereby extending the training data set and improving the recognition accuracy.","","","10.1109/LGRS.2018.2884898","National Natural Science Foundation of China; Fundamental Research Funds for the Central University, China; Natural Science Foundation of Jiangsu Province; Base Research Foundation; Funding of Key Laboratory of Radar Imaging and Microwave Photonics (Nanjing University of Aeronautics and Astronautics), Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8580384","Deep generative model;synthetic aperture radar automatic target recognition (SAR-ATR);Wasserstein autoencoder (WAE)","Synthetic aperture radar;Training;Image reconstruction;Solid modeling;Decoding;Image generation;Computational modeling","image classification;image reconstruction;learning (artificial intelligence);neural nets;radar computing;radar imaging;synthetic aperture radar","machine learning approaches;synthetic aperture radar automatic target recognition;training data;deep generative model;SAR image generation;network structure;reconstruction loss function;classical generative models;generated images;synthetic aperture radar image generation;Wasserstein autoencoder","","1","22","","","","","IEEE","IEEE Journals"
"Multiuser Resource Control With Deep Reinforcement Learning in IoT Edge Computing","L. Lei; H. Xu; X. Xiong; K. Zheng; W. Xiang; X. Wang","College of Science and Engineering, James Cook University, Cairns, QLD, Australia; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China; Intelligent Computing and Communication Laboratory, Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Intelligent Computing and Communication Laboratory, Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; College of Science and Engineering, James Cook University, Cairns, QLD, Australia; Department of Electrical and Computer Engineering, Western University, London, ON, Canada","IEEE Internet of Things Journal","","2019","6","6","10119","10133","By leveraging the concept of mobile edge computing (MEC), massive amount of data generated by a large number of Internet of Things (IoT) devices could be offloaded to MEC server at the edge of wireless network for further computational intensive processing. However, due to the resource constraint of IoT devices and wireless network, both communications and computation resources need to be allocated and scheduled efficiently for better system performance. In this article, we propose a joint computation offloading and multiuser scheduling algorithm for IoT edge computing system to minimize the long-term average weighted sum of delay and power consumption under stochastic traffic arrival. We formulate the dynamic optimization problem as an infinite-horizon average-reward continuous-time Markov decision process (CTMDP) model. One critical challenge in solving this MDP problem for the multiuser resource control is the curse-of-dimensionality problem, where the state space of the MDP model and the computation complexity increase exponentially with the growing number of users or IoT devices. In order to overcome this challenge, we use the deep reinforcement learning (RL) techniques and propose a neural network architecture to approximate the value functions for the post-decision system states. The designed algorithm to solve the CTMDP problem supports semidistributed auction-based implementation, where the IoT devices submit bids to the BS to make the resource control decisions centrally. The simulation results show that the proposed algorithm provides significant performance improvement over the baseline algorithms, and also outperforms the RL algorithms based on other neural network architectures.","","","10.1109/JIOT.2019.2935543","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802256","Deep reinforcement learning (DRL);Internet of Things (IoT);mobile edge computing (MEC)","","","","","","41","IEEE","","","","IEEE","IEEE Journals"
"Regression Convolutional Neural Network for Automated Pediatric Bone Age Assessment From Hand Radiograph","X. Ren; T. Li; X. Yang; S. Wang; S. Ahmad; L. Xiang; S. R. Stone; L. Li; Y. Zhan; D. Shen; Q. Wang","Institute for Medical Imaging Technology, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Radiology, Shanghai Children's Hospital, Shanghai Jiao Tong University, Shanghai, China; Department of Radiology, Shanghai Children's Hospital, Shanghai Jiao Tong University, Shanghai, China; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, NC, Chapel Hill, USA; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, NC, Chapel Hill, USA; Institute for Medical Imaging Technology, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; Aberdeen Biomedical Imaging Centre (ABIC), University of Aberdeen, Aberdeen, U.K.; Department of Radiology, Shanghai Children's Hospital, Shanghai Jiao Tong University, Shanghai, China; Institute for Medical Imaging Technology, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, NC, Chapel Hill, USA; Institute for Medical Imaging Technology, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Journal of Biomedical and Health Informatics","","2019","23","5","2030","2038","Skeletal bone age assessment is a common clinical practice to investigate endocrinology, and genetic and growth disorders of children. However, clinical interpretation and bone age analyses are time-consuming, labor intensive, and often subject to inter-observer variability. This advocates the need of a fully automated method for bone age assessment. We propose a regression convolutional neural network (CNN) to automatically assess the pediatric bone age from hand radiograph. Our network is specifically trained to place more attention to those bone age related regions in the X-ray images. Specifically, we first adopt the attention module to process all images and generate the coarse/fine attention maps as inputs for the regression network. Then, the regression CNN follows the supervision of the dynamic attention loss during training; thus, it can estimate the bone age of the hard (or “outlier”) images more accurately. The experimental results show that our method achieves an average discrepancy of 5.2-5.3 months between clinical and automatic bone age evaluations on two large datasets. In conclusion, we propose a fully automated deep learning solution to process X-ray images of the hand for bone age assessment, with the accuracy comparable to human experts but with much better efficiency.","","","10.1109/JBHI.2018.2876916","Shanghai Jiao Tong University; National Natural Science Foundation of China; National Key R&D Program of China; Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8500181","Bone age assessment;deep learning;regression convolutional neural network;hand radiograph","Bones;Training;X-ray imaging;Machine learning;Feature extraction;Radiography;Convolutional neural networks","bone;convolutional neural nets;diagnostic radiography;learning (artificial intelligence);medical disorders;medical image processing;paediatrics;regression analysis","regression convolutional neural network;automated pediatric bone age assessment;hand radiograph;skeletal bone age assessment;clinical interpretation;fully automated method;bone age related regions;X-ray images;regression network;regression CNN;clinical bone age evaluations;automatic bone age evaluations;fully automated deep learning solution;clinical practice;endocrinology;genetic disorders;growth disorders;bone age analyses;pediatric bone age","","1","41","Traditional","","","","IEEE","IEEE Journals"
"Hand-Gesture Recognition Using Two-Antenna Doppler Radar With Deep Convolutional Neural Networks","S. Skaria; A. Al-Hourani; M. Lech; R. J. Evans","School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia; School of Engineering, RMIT University, Melbourne, VIC, Australia; Department of Electrical and Electronic Engineering, The University of Melbourne, Melbourne, VIC, Australia","IEEE Sensors Journal","","2019","19","8","3041","3048","Low-cost consumer radar integrated circuits combined with recent advances in machine learning have opened up a range of new possibilities in smart sensing. In this paper, we use a miniature radar sensor to capture Doppler signatures of 14 different hand gestures and train a deep convolutional neural network (DCNN) to classify these captured gestures. We utilize two receiving antennas of a continuous-wave Doppler radar capable of producing the in-phase and quadrature components of the beat signals. We map these two beat signals into three input channels of a DCNN as two spectrograms and an angle of arrival matrix. The classification results of the proposed architecture show a gesture classification accuracy exceeding 95% and a very low confusion between different gestures. This is almost 10% improvement over the single-channel Doppler methods reported in the literature.","","","10.1109/JSEN.2019.2892073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8610109","Radar sensors;deep convolutional neural networks;radar signal processing;hand-gesture recognition;Doppler radar;multi-antenna radar;millimeter-wave radar","Doppler radar;Doppler effect;Sensors;Spectrogram;Receiving antennas;Gesture recognition","convolutional neural nets;Doppler radar;feature extraction;gesture recognition;image classification;learning (artificial intelligence);radar antennas;radar imaging;receiving antennas","captured gestures;receiving antennas;beat signals;DCNN;gesture classification accuracy;single-channel Doppler methods;two-antenna doppler radar;deep convolutional neural network;low-cost consumer radar integrated circuits;machine learning;smart sensing;miniature radar sensor;Doppler signatures;continuous-wave Doppler radar;hand-gesture recognition;quadrature components","","3","30","","","","","IEEE","IEEE Journals"
"Deep Video Dehazing With Semantic Segmentation","W. Ren; J. Zhang; X. Xu; L. Ma; X. Cao; G. Meng; W. Liu","State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Medical School, University of Chinese Academy of Sciences, Beijing, China; SenseTime Research, Beijing, China; Tencent AI Laboratory, Shenzhen, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Tencent AI Laboratory, Shenzhen, China","IEEE Transactions on Image Processing","","2019","28","4","1895","1908","Recent research have shown the potential of using convolutional neural networks (CNNs) to accomplish single image dehazing. In this paper, we take one step further to explore the possibility of exploiting a network to perform haze removal for videos. Unlike single image dehazing, video-based approaches can take advantage of the abundant information that exists across neighboring frames. In this paper, assuming that a scene point yields highly correlated transmission values between adjacent video frames, we develop a deep learning solution for video dehazing, where a CNN is trained end-to-end to learn how to accumulate information across frames for transmission estimation. The estimated transmission map is subsequently used to recover a haze-free frame via atmospheric scattering model. In addition, as the semantic information of a scene provides a strong prior for image restoration, we propose to incorporate global semantic priors as input to regularize the transmission maps so that the estimated maps can be smooth in the regions of the same object and only discontinuous across the boundaries of different objects. To train this network, we generate a dataset consisted of synthetic hazy and haze-free videos for supervision based on the NYU depth dataset. We show that the features learned from this dataset are capable of removing haze that arises in outdoor scenes in a wide range of videos. Extensive experiments demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods on both synthetic and real-world videos.","","","10.1109/TIP.2018.2876178","National Key R&D Program of China; National Natural Science Foundation of China; Chinese Academy of Sciences; Joint Foundation Program of the Chinese Academy of Sciences for equipment pre-feasibility study; National Natural Science Foundation of China; Equipment Research Program of the Chinese Academy of Sciences; Open Projects Program of National Laboratory of Pattern Recognition; CCF-Tencent Open Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8492451","Video dehazing;defogging;transmission map;convolutional neural network","Semantics;Atmospheric modeling;Convolutional neural networks;Image segmentation;Image color analysis;Coherence;Task analysis","feature extraction;feedforward neural nets;image denoising;image enhancement;image restoration;image segmentation;learning (artificial intelligence);video signal processing","deep video dehazing;semantic segmentation;convolutional neural networks;single image dehazing;haze removal;video-based approaches;neighboring frames;scene point;adjacent video frames;deep learning solution;transmission estimation;haze-free frame;atmospheric scattering model;semantic information;image restoration;transmission maps;haze-free videos;real-world videos","","8","61","","","","","IEEE","IEEE Journals"
"Deep Learning Based Robotic Tool Detection and Articulation Estimation With Spatio-Temporal Layers","E. Colleoni; S. Moccia; X. Du; E. De Momi; D. Stoyanov","Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy; Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy; Wellcome/EPSRC Centre for Interventional and Surgical Sciences, UCL Robotics Institute, University College London, London, U.K.; Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy; Wellcome/EPSRC Centre for Interventional and Surgical Sciences, UCL Robotics Institute, University College London, London, U.K.","IEEE Robotics and Automation Letters","","2019","4","3","2714","2721","Surgical-tool joint detection from laparoscopic images is an important but challenging task in computer-assisted minimally invasive surgery. Illumination levels, variations in background and the different number of tools in the field of view, all pose difficulties to algorithm and model training. Yet, such challenges could be potentially tackled by exploiting the temporal information in laparoscopic videos to avoid per frame handling of the problem. In this letter, we propose a novel encoder-decoder architecture for surgical instrument joint detection and localization that uses three-dimensional convolutional layers to exploit spatio-temporal features from laparoscopic videos. When tested on benchmark and custom-built datasets, a median Dice similarity coefficient of 85.1% with an interquartile range of 4.6% highlights performance better than the state of the art based on single-frame processing. Alongside novelty of the network architecture, the idea for inclusion of temporal information appears to be particularly useful when processing images with unseen backgrounds during the training phase, which indicates that spatio-temporal features for joint detection help to generalize the solution.","","","10.1109/LRA.2019.2917163","Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS) at UCL; EPSRC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715379","Surgical-tool detection;medical robotics;computer assisted interventions;minimally invasive surgery;surgical vision","Videos;Joints;Three-dimensional displays;Tools;Instruments;Robots;Surgery","decoding;image segmentation;learning (artificial intelligence);medical robotics;object tracking;surgery","training phase;encoder-decoder architecture;median dice similarity coefficient;joint detection;deep learning based robotic tool detection;unseen backgrounds;network architecture;single-frame processing;custom-built datasets;spatio-temporal features;three-dimensional convolutional layers;localization;surgical instrument joint detection;frame handling;laparoscopic videos;temporal information;illumination levels;computer-assisted minimally invasive surgery;laparoscopic images;surgical-tool joint detection;spatio-temporal layers;articulation estimation","","1","27","","","","","IEEE","IEEE Journals"
"Bayesian Polytrees With Learned Deep Features for Multi-Class Cell Segmentation","H. Fehri; A. Gooya; Y. Lu; E. Meijering; S. A. Johnston; A. F. Frangi","Center for Computational Imaging Simulation Technologies in Biomedicine, The University of Sheffield, Sheffield, U.K.; Center for Computational Imaging Simulation Technologies in Biomedicine, The University of Sheffield, Sheffield, U.K.; Center for Computational Imaging Simulation Technologies in Biomedicine, The University of Sheffield, Sheffield, U.K.; Biomedical Imaging Group Rotterdam, Erasmus University Medical Center, Rotterdam, The Netherlands; Bateson Center, Firth Court, The University of Sheffield, Sheffield, U.K.; Center for Computational Imaging Simulation Technologies in Biomedicine, The University of Sheffield, Sheffield, U.K.","IEEE Transactions on Image Processing","","2019","28","7","3246","3260","The recognition of different cell compartments, the types of cells, and their interactions is a critical aspect of quantitative cell biology. However, automating this problem has proven to be non-trivial and requires solving multi-class image segmentation tasks that are challenging owing to the high similarity of objects from different classes and irregularly shaped structures. To alleviate this, graphical models are useful due to their ability to make use of prior knowledge and model inter-class dependences. Directed acyclic graphs, such as trees, have been widely used to model top-down statistical dependences as a prior for improved image segmentation. However, using trees, a few inter-class constraints can be captured. To overcome this limitation, we propose polytree graphical models that capture label proximity relations more naturally compared to tree-based approaches. A novel recursive mechanism based on two-pass message passing was developed to efficiently calculate closed-form posteriors of graph nodes on polytrees. The algorithm is evaluated on simulated data and on two publicly available fluorescence microscopy datasets, outperforming directed trees and three state-of-the-art convolutional neural networks, namely, SegNet, DeepLab, and PSPNet. Polytrees are shown to outperform directed trees in predicting segmentation error by highlighting areas in the segmented image that do not comply with prior knowledge. This paves the way to uncertainty measures on the resulting segmentation and guide subsequent segmentation refinement.","","","10.1109/TIP.2019.2895455","University of Sheffield; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8626539","Hierarchical graphs;cell and nucleus segmentation;multi-class segmentation;error prediction","Image segmentation;Graphical models;Biomedical imaging;Inference algorithms;Feature extraction;Convolutional neural networks","Bayes methods;directed graphs;image classification;image segmentation;learning (artificial intelligence);neural nets;trees (mathematics)","learned deep features;multiclass cell segmentation;different cell compartments;quantitative cell biology;multiclass image segmentation tasks;irregularly shaped structures;model inter-class dependences;directed acyclic graphs;inter-class constraints;polytree graphical models;capture label proximity relations;tree-based approaches;graph nodes;outperform directed trees;segmentation error;segmented image;resulting segmentation;image segmentation;fluorescence microscopy datasets;segmentation refinement;Bayesian polytrees","","3","53","","","","","IEEE","IEEE Journals"
"Learning and Adapting Robust Features for Satellite Image Segmentation on Heterogeneous Data Sets","S. Ghassemi; A. Fiandrotti; G. Francini; E. Magli","Electronics and Telecommunication Department, Polytechnic University of Turin, Turin, Italy; Electronics and Telecommunication Department, Polytechnic University of Turin, Turin, Italy; Telecom Italia, Turin, Italy; Electronics and Telecommunication Department, Polytechnic University of Turin, Turin, Italy","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","9","6517","6529","This paper addresses the problem of training a deep neural network for satellite image segmentation so that it can be deployed over images whose statistics differ from those used for training. For example, in postdisaster damage assessment, the tight time constraints make it impractical to train a network from scratch for each image to be segmented. We propose a convolutional encoder-decoder network able to learn visual representations of increasing semantic level as its depth increases, allowing it to generalize over a wider range of satellite images. Then, we propose two additional methods to improve the network performance over each specific image to be segmented. First, we observe that updating the batch normalization layers' statistics over the target image improves the network performance without human intervention. Second, we show that refining a trained network over a few samples of the image boosts the network performance with minimal human intervention. We evaluate our architecture over three data sets of satellite images, showing the state-of-the-art performance in binary segmentation of previously unseen images and competitive performance with respect to more complex techniques in a multiclass segmentation task.","","","10.1109/TGRS.2019.2906689","TIM; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693644","Convolutional neural network (CNN);deep learning;domain adaptation;encoder–decoder architecture;satellite image segmentation","Image segmentation;Satellites;Training;Semantics;Feature extraction;Labeling;Computer architecture","convolutional neural nets;geophysical image processing;image segmentation;learning (artificial intelligence);statistics","multiclass segmentation task;satellite image segmentation;deep neural network;convolutional encoder-decoder network;satellite images;network performance;batch normalization layers;binary segmentation;statistics","","","52","","","","","IEEE","IEEE Journals"
"SketchHelper: Real-Time Stroke Guidance for Freehand Sketch Retrieval","J. Choi; H. Cho; J. Song; S. M. Yoon","HCI Lab, College of Computer Science, Kookmin University, Seoul, South Korea; HCI Lab, College of Computer Science, Kookmin University, Seoul, South Korea; HCI Lab, College of Computer Science, Kookmin University, Seoul, South Korea; HCI Lab, College of Computer Science, Kookmin University, Seoul, South Korea","IEEE Transactions on Multimedia","","2019","21","8","2083","2092","Text-based retrieval systems have been popular, but content-based retrieval systems have gained widespread acceptance in recent years to directly retrieve diverse media based on their visual content, such as color, texture, and shape. Among many content-based retrieval systems, sketch-based media retrieval systems have attracted attention recently with the proliferation of tablet PCs and smart mobile devices. Sketch-based retrieval requires the user to draw a freehand sketch query, but freehand drawing can be challenging for those with limited drawing skills. This degrades retrieval performance, since successful retrieval depends on the quality of the sketch query image drawn by the user. To address this issue, we propose a real-time stroke guidance for freehand sketch retrieval that continuously displays next-stroke shadow sketches on the canvas based on the user's step-by-step partial strokes. We train a stroke guidance network that learns the mapping between the step-wise stroke relations to predict the user's next stroke. The proposed stroke guidance for freehand sketch retrieval system runs on a five step next-stroke prediction model that identifies candidate next-stroke sketches from a database of millions of sketches. The system retrieves variable number of sketch object classes at different drawing stages. During the initial sketching stage, diverse drawing possibilities are covered by retrieving multiple sketch classes; as the sketching progresses, the intended sketch class is narrowed down to one. Deep binary hashing is employed for efficient similarity matching of relevant next-stroke sketches. We extend the Google QuickDraw dataset to create a five step sketch stroke database. Qualitative and quantitative experiments are conducted to verify the effectiveness of the proposed system, which can be utilized for drawing guidance, tracing, and sketch retrieval. Tracing refers to the act of copying the shadowed line of a guiding image by drawing over its lines.","","","10.1109/TMM.2019.2892301","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8607060","Stroke-based modeling;sketch based sketch retrieval;shadow-guided drawing;deep learning","Three-dimensional displays;Feature extraction;Databases;Real-time systems;Shape;Media;Deep learning","CAD;computer graphics;content-based retrieval;image retrieval;learning (artificial intelligence);query processing;solid modelling","sketch object classes;initial sketching stage;diverse drawing possibilities;multiple sketch classes;sketching progresses;intended sketch class;step sketch stroke database;drawing guidance;real-time stroke guidance;text-based retrieval systems;sketch-based media retrieval systems;freehand sketch query;freehand drawing;retrieval performance;successful retrieval;sketch query image;next-stroke shadow sketches;step-by-step partial strokes;stroke guidance network;step-wise stroke relations;freehand sketch retrieval system;step next-stroke prediction model;candidate next-stroke sketches","","","34","Traditional","","","","IEEE","IEEE Journals"
"Real-Time Instrument Segmentation in Robotic Surgery Using Auxiliary Supervised Deep Adversarial Learning","M. Islam; D. A. Atputharuban; R. Ramesh; H. Ren","NUS Graduate School for Integrative Sciences and Engineering and Department of Biomedical Engineering, National University of Singapore, Singapore; Department of Biomedical Engineering, National University of Singapore, Singapore; Department of Biomedical Engineering, National University of Singapore, Singapore; Department of Biomedical Engineering, National University of Singapore, Singapore","IEEE Robotics and Automation Letters","","2019","4","2","2188","2195","Robot-assisted surgery is an emerging technology that has undergone rapid growth with the development of robotics and imaging systems. Innovations in vision, haptics, and accurate movements of robot arms have enabled surgeons to perform precise minimally invasive surgeries. Real-time semantic segmentation of the robotic instruments and tissues is a crucial step in robot-assisted surgery. Accurate and efficient segmentation of the surgical scene not only aids in the identification and tracking of instruments but also provides contextual information about the different tissues and instruments being operated with. For this purpose, we have developed a light-weight cascaded convolutional neural network to segment the surgical instruments from high-resolution videos obtained from a commercial robotic system. We propose a multi-resolution feature fusion module to fuse the feature maps of different dimensions and channels from the auxiliary and main branch. We also introduce a novel way of combining auxiliary loss and adversarial loss to regularize the segmentation model. Auxiliary loss helps the model to learn low-resolution features, and adversarial loss improves the segmentation prediction by learning higher order structural information. The model also consists of a light-weight spatial pyramid pooling unit to aggregate rich contextual information in the intermediate stage. We show that our model surpasses existing algorithms for pixelwise segmentation of surgical instruments in both prediction accuracy and segmentation time of high-resolution videos.","","","10.1109/LRA.2019.2900854","Singapore Academic Research Fund; NMRC Bedside & Bench; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648150","Deep learning in robotics and automation;visual tracking;object detection;segmentation and categorization","Instruments;Robots;Surgery;Real-time systems;Image segmentation;Tools;Tracking","","","","","50","","","","","IEEE","IEEE Journals"
"Where-and-When to Look: Deep Siamese Attention Networks for Video-Based Person Re-Identification","L. Wu; Y. Wang; J. Gao; X. Li","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; Dalian University of Technology, Dalian, China; Discipline of Business Analytics, The University of Sydney Business School, The University of Sydney, Sydney, NSW, Australia; University of Queensland, St Lucia, QLD, Australia","IEEE Transactions on Multimedia","","2019","21","6","1412","1424","Video-based person re-identification (re-id) is a central application in surveillance systems with a significant concern in security. Matching persons across disjoint camera views in their video fragments are inherently challenging due to the large visual variations and uncontrolled frame rates. There are two steps crucial to person re-id, namely, discriminative feature learning and metric learning. However, existing approaches consider the two steps independently, and they do not make full use of the temporal and spatial information in the videos. In this paper, we propose a Siamese attention architecture that jointly learns spatiotemporal video representations and their similarity metrics. The network extracts local convolutional features from regions of each frame and enhances their discriminative capability by focusing on distinct regions when measuring the similarity with another pedestrian video. The attention mechanism is embedded into spatial gated recurrent units to selectively propagate relevant features and memorize their spatial dependencies through the network. The model essentially learns which parts (where) from which frames (when) are relevant and distinctive for matching persons and attaches higher importance therein. The proposed Siamese model is end-to-end trainable to jointly learn comparable hidden representations for paired pedestrian videos and their similarity value. Extensive experiments on three benchmark datasets show the effectiveness of each component of the proposed deep network while outperforming state-of-the-art methods.","","","10.1109/TMM.2018.2877886","Australian Research Council; National Natural Science Foundation of China; ARC Discovery Projects funding; University of Sydney Business School ARC Bridging Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8506428","Video-based person re-identification;gated recurrent units;spatial correlations;visual attention","Feature extraction;Measurement;Visualization;Spatiotemporal phenomena;Convolution;Computer architecture;Logic gates","convolution;feature extraction;image recognition;image representation;learning (artificial intelligence);object detection;object recognition;pedestrians;video signal processing","local convolutional features;spatial gated recurrent units;paired pedestrian videos;deep Siamese attention networks;matching persons;disjoint camera views;video fragments;discriminative feature learning;metric learning;temporal information;spatial information;spatiotemporal video representations;video-based person reidentification","","9","73","","","","","IEEE","IEEE Journals"
"Parallel Architecture of Convolutional Bi-Directional LSTM Neural Networks for Network-Wide Metro Ridership Prediction","X. Ma; J. Zhang; B. Du; C. Ding; L. Sun","Beijing Key Laboratory for Cooperative Vehicle Infrastructure System and Safety Control, School of Transportation Science and Engineering, Beihang University, Beijing, China; Beijing Key Laboratory for Cooperative Vehicle Infrastructure System and Safety Control, School of Transportation Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, Beijing Advanced Innovation Center for Big Data and Brain Computing, School of Computer Science and Engineering, Beihang University, Beijing, China; Beijing Key Laboratory for Cooperative Vehicle Infrastructure System and Safety Control, School of Transportation Science and Engineering, Beihang University, Beijing, China; School of Economics and Management, Tsinghua University, Beijing, China","IEEE Transactions on Intelligent Transportation Systems","","2019","20","6","2278","2288","Accurate metro ridership prediction can guide passengers in efficiently selecting their departure time and transferring from station to station. An increasing number of deep learning algorithms are being utilized to forecast metro ridership due to the development of computational intelligence. However, limited efforts have been exerted to consider spatiotemporal features, which are important in forecasting ridership through deep learning methods, in large-scale metro networks. To fill this gap, this paper proposes a parallel architecture comprising convolutional neural network (CNN) and bi-directional long short-term memory network (BLSTM) to extract spatial and temporal features, respectively. Metro ridership data are transformed into ridership images and time series. Spatial features can be learned from ridership image data by using CNN, which demonstrates favorable performance in video detection. Time series data are input into the BLSTM which considers the historical and future impacts of ridership in temporal feature extraction. The two networks are concatenated in parallel and prevented from interfering with each other. Joint spatiotemporal features are fed into a fully connected network for metro ridership prediction. The Beijing metro network is used to demonstrate the efficiency of the proposed algorithm. The proposed model outperforms traditional statistical models, deep learning architectures, and sequential structures, and is suitable for ridership prediction in large-scale metro networks. Metro authorities can thus effectively allocate limited resources to overcrowded areas for service improvement.","","","10.1109/TITS.2018.2867042","National Natural Science Foundation of China; Natural Science Foundation of Beijing Municipality; Young Elite Scientist Sponsorship Program by the China Association for Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8536904","Metro ridership prediction;spatiotemporal features;convolutional neural network;bi-directional long short-term memory network;parallel structure","Feature extraction;Predictive models;Data models;Spatiotemporal phenomena;Forecasting;Neural networks;Time series analysis","convolutional neural nets;feature extraction;learning (artificial intelligence);parallel architectures;public transport;railway engineering;recurrent neural nets;regression analysis;time series;transportation;video signal processing","parallel architecture;network-wide metro ridership prediction;accurate metro ridership prediction;departure time;deep learning algorithms;forecasting ridership;deep learning methods;large-scale metro networks;CNN;short-term memory network;spatial features;temporal features;metro ridership data;ridership image data;time series data;temporal feature extraction;joint spatiotemporal features;fully connected network;Beijing metro network;deep learning architectures;metro authorities;convolutional bi-directional LSTM neural networks","","1","43","","","","","IEEE","IEEE Journals"
"Splenomegaly Segmentation on Multi-Modal MRI Using Deep Convolutional Networks","Y. Huo; Z. Xu; S. Bao; C. Bermudez; H. Moon; P. Parvathaneni; T. K. Moyo; M. R. Savona; A. Assad; R. G. Abramson; B. A. Landman","Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, USA; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, USA; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, USA; Department of Biomedical Engineering, Vanderbilt University, Nashville, TN, USA; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, USA; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, USA; Department of Medicine, Vanderbilt University Medical Center, Nashville, TN, USA; Department of Medicine, Vanderbilt University Medical Center, Nashville, TN, USA; Incyte Corporation, Wilmington, DE, USA; Department of Radiology and Radiological Science, Vanderbilt University Medical Center, Nashville, TN, USA; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, USA","IEEE Transactions on Medical Imaging","","2019","38","5","1185","1196","The findings of splenomegaly, abnormal enlargement of the spleen, is a non-invasive clinical biomarker for liver and spleen diseases. Automated segmentation methods are essential to efficiently quantify splenomegaly from clinically acquired abdominal magnetic resonance imaging (MRI) scans. However, the task is challenging due to: 1) large anatomical and spatial variations of splenomegaly; 2) large inter- and intra-scan intensity variations on multi-modal MRI; and 3) limited numbers of labeled splenomegaly scans. In this paper, we propose the Splenomegaly Segmentation Network (SS-Net) to introduce the deep convolutional neural network (DCNN) approaches in multi-modal MRI splenomegaly segmentation. Large convolutional kernel layers were used to address the spatial and anatomical variations, while the conditional generative adversarial networks were employed to leverage the segmentation performance of SS-Net in an end-to-end manner. A clinically acquired cohort containing both T1-weighted (T1w) and T2-weighted (T2w) MRI splenomegaly scans was used to train and evaluate the performance of multi-atlas segmentation (MAS), 2D DCNN networks, and a 3-D DCNN network. From the experimental results, the DCNN methods achieved superior performance to the state-of-the-art MAS method. The proposed SS-Net method has achieved the highest median and mean Dice scores among the investigated baseline DCNN methods.","","","10.1109/TMI.2018.2881110","National Science Foundation; National Institutes of Health; Vanderbilt-Incyte Research Alliance Grant (Savona/Abramson/Landman); National Institute on Aging; National Institutes of Health; Advanced Computing Center for Research and Education (ACCRE), Vanderbilt University, Nashville, TN, USA; ViSE/VICTR VR3029; National Center for Research Resources; National Center for Advancing Translational Sciences; National Institutes of Health; Vanderbilt IDEAS Grant (Holly-Bockelmann, Walker, Meliler, Palmeri, Weller); Vanderbilt University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8533359","Spleen segmentation;MRI;deep convolutional neural network;multi-contrast;splenomegaly","Image segmentation;Magnetic resonance imaging;Kernel;Gallium nitride;Two dimensional displays;Three-dimensional displays;Generative adversarial networks","biomedical MRI;convolutional neural nets;diseases;image segmentation;learning (artificial intelligence);liver;medical image processing","deep convolutional networks;splenomegaly enlargement;abnormal enlargement;noninvasive clinical biomarker;spleen diseases;automated segmentation methods;labeled splenomegaly scans;Splenomegaly Segmentation Network;deep convolutional neural network;multimodal MRI splenomegaly segmentation;convolutional kernel layers;anatomical variations;segmentation performance;multiatlas segmentation;SS-Net method;baseline DCNN method;intrascan intensity variation;abdominal magnetic resonance imaging scan;2D DCNN networks;liver disease","","","59","","","","","IEEE","IEEE Journals"
"Multi-Scale Attention Deep Neural Network for Fast Accurate Object Detection","K. Song; H. Yang; Z. Yin","State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","10","2972","2985","Object detection remains a challenging task in computer vision due to the tremendous extent of changes in the appearances of objects caused by clustered backgrounds, occlusion, truncation, and scale change. Current deep neural network (DNN)-based object detection methods cannot simultaneously achieve a high accuracy and a high efficiency. To overcome this limitation, in this paper, we propose a novel multi-scale attention (MSA) DNN for accurate object detection with high efficiency. The proposed MSA-DNN method utilizes a novel multi-scale feature fusion module (MSFFM) to construct high-level semantic features. Subsequently, a novel MSA module (MSAM) based on the fused layers of the MSFFM is introduced to exploit the global semantic information of image-level labels to guide detection. On the one hand, MSAM can capture global semantic information to further enhance the semantic feature representation of the fused layers constructed by the MSFFM, thereby improving the detection accuracy. On the other hand, the MSA maps generated by MSAM can be employed to rapidly and coarsely locate objects at different scales. In addition, an attention-based hard negative mining strategy is introduced to filter out negative samples to reduce the search space, dramatically alleviating the severe class imbalance problem. Extensive experimental results on the challenging PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO datasets demonstrate that MSA-DNN achieves a state-of-the-art detection accuracy while maintaining a high efficiency. Furthermore, MSA-DNN significantly improves the small-object detection accuracy.","","","10.1109/TCSVT.2018.2875449","Major Project Foundation of Hubei Province; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489974","Object detection;attention model;feature fusion;deep neural network","Semantics;Object detection;Task analysis;Feature extraction;Proposals;Neural networks;Computational modeling","computer vision;data mining;feature extraction;learning (artificial intelligence);neural nets;object detection","attention-based hard negative mining strategy;state-of-the-art detection accuracy;small-object detection accuracy;multiscale attention deep neural network;fast accurate object detection;scale change;novel multiscale attention DNN;MSA-DNN method;MSFFM;high-level semantic features;novel MSA module;MSAM;fused layers;global semantic information;image-level labels;semantic feature representation;MSA maps;multiscale feature fusion module;deep neural network-based object detection","","","55","","","","","IEEE","IEEE Journals"
"VSSA-NET: Vertical Spatial Sequence Attention Network for Traffic Sign Detection","Y. Yuan; Z. Xiong; Q. Wang","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Image Processing","","2019","28","7","3423","3434","Although traffic sign detection has been studied for years and great progress has been made with the rise of deep learning technique, there are still many problems remaining to be addressed. For complicated real-world traffic scenes, there are two main challenges. First, traffic signs are usually small-sized objects, which makes them more difficult to detect than large ones; second, it is hard to distinguish false targets which resemble real traffic signs in complex street scenes without context information. To handle these problems, we propose a novel end-to-end deep learning method for traffic sign detection in complex environments. Our contributions are as follows: 1) we propose a multi-resolution feature fusion network architecture which exploits densely connected deconvolution layers with skip connections, and can learn more effective features for a small-size object and 2) we frame the traffic sign detection as a spatial sequence classification and regression task, and propose a vertical spatial sequence attention module to gain more context information for better detection performance. To comprehensively evaluate the proposed method, we experiment on several traffic sign datasets as well as the general object detection dataset, and the results have shown the effectiveness of our proposed method.","","","10.1109/TIP.2019.2896952","National Natural Science Foundation of China; State Key Program of National Natural Science Foundation of China; Natural Science Foundation of Shaanxi Province; Projects of Special Zone for National Defense Science and Technology Innovation, Fundamental Research Funds for the Central Universities; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8632977","Traffic sign detection;context modeling;small object;sequence attention model","Feature extraction;Object detection;Deep learning;Task analysis;Image color analysis;Detectors;Deconvolution","feature extraction;image classification;learning (artificial intelligence);object detection;traffic engineering computing","end-to-end deep learning method;traffic sign detection;traffic sign datasets;vertical spatial sequence attention network;complicated real-world traffic scenes;complex environments;multiresolution feature fusion network architecture;general object detection dataset","","3","41","","","","","IEEE","IEEE Journals"
"Dynamic Multicontext Segmentation of Remote Sensing Images Based on Convolutional Networks","K. Nogueira; M. Dalla Mura; J. Chanussot; W. R. Schwartz; J. A. dos Santos","Department of Computer Science, Universidade Federal de Minas Gerais, Belo Horizonte, Brazil; CNRS, Grenoble INP, GIPSA-lab, University of Grenoble Alpes, Grenoble, France; Inria, CNRS, Grenoble INP, LJK, Univ. Grenoble Alpes, Grenoble, France; Department of Computer Science, Universidade Federal de Minas Gerais, Belo Horizonte, Brazil; Department of Computer Science, Universidade Federal de Minas Gerais, Belo Horizonte, Brazil","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","10","7503","7520","Semantic segmentation requires methods capable of learning high-level features while dealing with large volume of data. Toward such goal, convolutional networks can learn specific and adaptable features based on the data. However, these networks are not capable of processing a whole remote sensing image, given its huge size. To overcome such limitation, the image is processed using fixed size patches. The definition of the input patch size is usually performed empirically (evaluating several sizes) or imposed (by network constraint). Both strategies suffer from drawbacks and could not lead to the best patch size. To alleviate this problem, several works exploited multicontext information by combining networks or layers. This process increases the number of parameters, resulting in a more difficult model to train. In this paper, we propose a novel technique to perform semantic segmentation of remote sensing images that exploits a multicontext paradigm without increasing the number of parameters while defining, in training time, the best patch size. The main idea is to train a dilated network with distinct patch sizes, allowing it to capture multicontext characteristics from heterogeneous contexts. While processing these varying patches, the network provides a score for each patch size, helping in the definition of the best size for the current scenario. A systematic evaluation of the proposed algorithm is conducted using four high-resolution remote sensing data sets with very distinct properties. Our results show that the proposed algorithm provides improvements in pixelwise classification accuracy when compared to the state-of-the-art methods.","","","10.1109/TGRS.2019.2913861","Pró-Reitoria de Pesquisa, Universidade Federal de Minas Gerais; Conselho Nacional de Desenvolvimento Científico e Tecnológico; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; Fundação de Amparo à Pesquisa do Estado de Minas Gerais; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8727958","Convolutional networks (ConvNets);deep learning;multicontext;multiscale;remote sensing;semantic segmentation","Semantics;Remote sensing;Image segmentation;Task analysis;Training;Deep learning;Aggregates","feature extraction;geophysical image processing;image classification;image representation;image resolution;image segmentation;learning (artificial intelligence);remote sensing","adaptable features;specific features;high-level features;convolutional networks;dynamic multicontext segmentation;high-resolution remote sensing data sets;varying patches;multicontext characteristics;distinct patch sizes;dilated network;multicontext paradigm;remote sensing image;semantic segmentation;multicontext information;network constraint;input patch size;fixed size patches;huge size","","1","52","","","","","IEEE","IEEE Journals"
"Inverting the Generator of a Generative Adversarial Network","A. Creswell; A. A. Bharath","BICV, Imperial College London, London, U.K.; BICV, Imperial College London, London, U.K.","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","7","1967","1974","Generative adversarial networks (GANs) learn a deep generative model that is able to synthesize novel, high-dimensional data samples. New data samples are synthesized by passing latent samples, drawn from a chosen prior distribution, through the generative model. Once trained, the latent space exhibits interesting properties that may be useful for downstream tasks such as classification or retrieval. Unfortunately, GANs do not offer an “inverse model,” a mapping from data space back to latent space, making it difficult to infer a latent representation for a given data sample. In this paper, we introduce a technique, inversion, to project data samples, specifically images, to the latent space using a pretrained GAN. Using our proposed inversion technique, we are able to identify which attributes of a data set a trained GAN is able to model and quantify GAN performance, based on a reconstruction loss. We demonstrate how our proposed inversion technique may be used to quantitatively compare the performance of various GAN models trained on three image data sets. We provide codes for all of our experiments in the website (https://github.com/ToniCreswell/InvertingGAN).","","","10.1109/TNNLS.2018.2875194","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520899","Backpropagation;feature extraction;image generation;multilayer neural network;pattern recognition;unsupervised learning","Gallium nitride;Generators;Image reconstruction;Generative adversarial networks;Data models;Training;Learning systems","image coding;image reconstruction;inverse problems;learning (artificial intelligence)","high-dimensional data samples;inversion technique;deep generative adversarial network;GAN;GAN training models;codes","","4","27","CCBY","","","","IEEE","IEEE Journals"
"Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling","S. Feng; T. Lee","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","12","2000","2011","This research addresses the problem of acoustic modeling of low-resource languages for which transcribed training data is absent. The goal is to learn robust frame-level feature representations that can be used to identify and distinguish subword-level speech units. The proposed feature representations comprise various types of multilingual bottleneck features (BNFs) that are obtained via multi-task learning of deep neural networks (MTL-DNN). One of the key problems is how to acquire high-quality frame labels for untranscribed training data to facilitate supervised DNN training. It is shown that learning of robust BNF representations can be achieved by effectively leveraging transcribed speech data and well-trained automatic speech recognition (ASR) systems from one or more out-of-domain (resource-rich) languages. Out-of-domain ASR systems can be applied to perform speaker adaptation with untranscribed training data of the target language, and to decode the training speech into frame-level labels for DNN training. It is also found that better frame labels can be generated by considering temporal dependency in speech when performing frame clustering. The proposed methods of feature learning are evaluated on the standard task of unsupervised subword modeling in Track 1 of the ZeroSpeech 2017 Challenge. The best performance achieved by our system is 9.7% in terms of across-speaker triphone minimal-pair ABX error rate, which is comparable to the best systems reported recently. Lastly, our investigation reveals that the closeness between target languages and out-of-domain languages and the amount of available training data for individual target languages could have significant impact on the goodness of learned features.","","","10.1109/TASLP.2019.2937953","Hong Kong Research Grants Council GRF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818297","Zero resource;unsupervised learning;robust features;speaker adaptation;multi-task learning","Hidden Markov models;Training;Acoustics;Adaptation models;Clustering algorithms;Speech processing;Task analysis","learning (artificial intelligence);neural nets;speech recognition","training speech;frame-level labels;frame clustering;feature learning;across-speaker triphone minimal-pair;target language;speaker adaptation;out-of-domain ASR systems;resource-rich;automatic speech recognition systems;transcribed speech data;robust BNF representations;supervised DNN training;untranscribed training data;high-quality frame labels;MTL-DNN;deep neural networks;multitask learning;multilingual bottleneck features;subword-level speech units;robust frame-level feature representations;transcribed training data;low-resource languages;acoustic modeling;unsupervised subword modeling;phonetic diversity;cross-lingual speaker;individual target languages;out-of-domain languages;training data","","","54","Traditional","","","","IEEE","IEEE Journals"
"Generalising multistain immunohistochemistry tissue segmentation using end-to-end colour deconvolution deep neural networks","A. Lahiani; J. Gildenblat; I. Klaman; N. Navab; E. Klaiman","Pathology and Tissue Analytics, Pharma Research and Early Development, Roche Innovation Center Munich, Germany; DeePathology Ltd., Israel; Pathology and Tissue Analytics, Pharma Research and Early Development, Roche Innovation Center Munich, Germany; Computer Aided Medical Procedures, Technische Universität München, Germany; Pathology and Tissue Analytics, Pharma Research and Early Development, Roche Innovation Center Munich, Germany","IET Image Processing","","2019","13","7","1066","1073","A key challenge in cancer immunotherapy biomarker research is quantification of pattern changes in microscopic whole slide images of tumour biopsies. Drug development requires a correlative analysis of various biomarkers. To enable that, tissue slides are manually annotated by pathologists, which is a tedious and error-prone task. Automation of this annotation process can improve accuracy and consistency while reducing workload and cost. The authors present a deep learning method to automatically segment digitised slide images with multiple stainings into compartments of tumour, healthy tissue, necrosis, and background. The method is based on using a fully convolutional neural network including a colour deconvolution segment learned end-to-end and helping the network to converge faster and deal with the dataset staining variability. They evaluate the performance of the proposed method using the F1 score, which is the harmonic mean between precision and recall. They report a testing F1 score of 0.88, 0.9, 0.8, and 0.99 for tumour, tissue, necrosis, and background, respectively. They address the task in the context of drug development where multiple stains exist and look into solutions for generalisations over these image populations. They also apply visualisation techniques to help understand the network decisions and gain more trust from pathologists.","","","10.1049/iet-ipr.2018.6513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733278","","","biomedical optical imaging;cancer;convolutional neural nets;drugs;image segmentation;learning (artificial intelligence);medical image processing;tumours","annotation process;deep learning method;segment digitised slide images;fully convolutional neural network;colour deconvolution segment;drug development;image populations;cancer immunotherapy biomarker research;microscopic whole slide images;tumour biopsies;biomarkers;colour deconvolution deep neural networks;multistain immunohistochemistry tissue segmentation","","1","63","","","","","IET","IET Journals"
"Learning to Exploit the Prior Network Knowledge for Weakly Supervised Semantic Segmentation","C. Redondo-Cabrera; M. Baptista-Ríos; R. J. López-Sastre","Department of Signal Theory and Communications, Research Group GRAM, University of Alcalá, Alcalá de Henares, Spain; Department of Signal Theory and Communications, Research Group GRAM, University of Alcalá, Alcalá de Henares, Spain; Department of Signal Theory and Communications, Research Group GRAM, University of Alcalá, Alcalá de Henares, Spain","IEEE Transactions on Image Processing","","2019","28","7","3649","3661","Training a convolutional neural network for semantic segmentation typically requires collecting a large amount of accurate pixel-level annotations and is a hard and expensive task. In contrast, simple image tags are easier to gather. In this paper, we introduce a novel weakly supervised semantic segmentation model which is able to learn from image labels and just image labels. Our model uses the prior knowledge of a network trained for image recognition, employing these image annotations as an attention mechanism to identify semantic regions in the images. We then present a methodology that builds accurate class-specific segmentation masks from these regions, where neither external objectness nor saliency algorithms are required. We describe how to incorporate this mask generation strategy into a fully end-to-end trainable process, where the network jointly learns to classify and segment images. Our experiments on PASCAL VOC 2012 dataset show that exploiting these generated class-specific masks in conjunction with our novel end-to-end learning process outperforms several recent weakly supervised semantic segmentation methods that use image tags only, and even some models that leverage additional supervision or training data.","","","10.1109/TIP.2019.2901393","Ministerio de Economía y Competitividad; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8651353","Semantic segmentation;weakly supervised;deep learning","Image segmentation;Semantics;Training;Task analysis;Data models;Training data;Tools","convolutional neural nets;image annotation;image classification;image recognition;image segmentation;learning (artificial intelligence)","image labels;image recognition;image annotations;attention mechanism;mask generation strategy;end-to-end trainable process;segment images;end-to-end learning process;prior network knowledge;convolutional neural network;weakly supervised semantic segmentation methods;class-specific segmentation masks;image tags;pixel-level annotations;PASCAL VOC 2012 dataset","","2","40","","","","","IEEE","IEEE Journals"
"EEG-Based Spatio–Temporal Convolutional Neural Network for Driver Fatigue Evaluation","Z. Gao; X. Wang; Y. Yang; C. Mu; Q. Cai; W. Dang; S. Zuo","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Key Laboratory of Mechanism Theory and Equipment Design, Ministry of Education, Tianjin University, Tianjin, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","9","2755","2763","Driver fatigue evaluation is of great importance for traffic safety and many intricate factors would exacerbate the difficulty. In this paper, based on the spatial-temporal structure of multichannel electroencephalogram (EEG) signals, we develop a novel EEG-based spatial-temporal convolutional neural network (ESTCNN) to detect driver fatigue. First, we introduce the core block to extract temporal dependencies from EEG signals. Then, we employ dense layers to fuse spatial features and realize classification. The developed network could automatically learn valid features from EEG signals, which outperforms the classical two-step machine learning algorithms. Importantly, we carry out fatigue driving experiments to collect EEG signals from eight subjects being alert and fatigue states. Using 2800 samples under within-subject splitting, we compare the effectiveness of ESTCNN with eight competitive methods. The results indicate that ESTCNN fulfills a better classification accuracy of 97.37% than these compared methods. Furthermore, the spatial-temporal structure of this framework advantages in computational efficiency and reference time, which allows further implementations in the brain-computer interface online systems.","","","10.1109/TNNLS.2018.2886414","National Natural Science Foundation of China; Natural Science Foundation of Tianjin City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8607897","Brain–computer interface (BCI);convolutional neural network (CNN);deep learning (DL);electroencephalogram (EEG);fatigue driving;spatio–temporal data","Electroencephalography;Fatigue;Convolution;Feature extraction;Brain modeling;Task analysis;Physiology","convolutional neural nets;driver information systems;electroencephalography;learning (artificial intelligence);medical signal processing;sensor fusion;signal classification","EEG signals;fatigue driving experiments;fatigue states;ESTCNN;spatial-temporal structure;spatio-temporal convolutional neural network;driver fatigue evaluation;traffic safety;multichannel electroencephalogram signals;spatial-temporal convolutional neural network;spatial feature fusion;brain-computer interface online systems;two-step machine learning","","4","45","","","","","IEEE","IEEE Journals"
"A Novel Neural Network for Remote Sensing Image Matching","H. Zhu; L. Jiao; W. Ma; F. Liu; W. Zhao","Key Laboratory of Intelligent Perception and Image Understanding, International Research Center of Intelligent Perception and Computation, Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, International Research Center of Intelligent Perception and Computation, Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, International Research Center of Intelligent Perception and Computation, Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, International Research Center of Intelligent Perception and Computation, Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, International Research Center of Intelligent Perception and Computation, Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","9","2853","2865","Rapid development of remote sensing (RS) imaging technology makes the acquired images have larger size, higher resolution, and more complex structure, which goes beyond the reach of classical hand-crafted feature-based matching. In this paper, we propose a feature learning approach based on two-branch networks to transform the image matching task into a two-class classification problem. To match two key points, two image patches centered at the key points are entered into the proposed network. The network aims to learn discriminative feature representations for patch matching, so that more matching pairs can be obtained on the premise of maintaining higher subpixel matching accuracy. The proposed network adopts a two-stage training mode to deal with the complex characteristics of RS images. An adaptive sample selection strategy is proposed to determine the size of each patch by the scale of its central key point. Thus, each patch can preserve the texture structure around its key point rather than all patches have a predetermined size. In the matching prediction stage, two strategies, namely, superpixel-based sample graded strategy and superpixel-based ordered spatial matching, are designed to improve the matching efficiency and matching accuracy, respectively. The experimental results and theoretical analysis demonstrate the feasibility, robustness, and effectiveness of the proposed method.","","","10.1109/TNNLS.2018.2888757","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613838","Deep learning (DL);image matching;neural network;remote sensing (RS) image","Feature extraction;Image matching;Dogs;Training;Neural networks;Remote sensing;Imaging","feature extraction;image classification;image matching;image representation;learning (artificial intelligence);neural nets;remote sensing","spatial matching;remote sensing imaging technology;two-branch networks;two-class classification problem;image patches;discriminative feature representations;patch matching;two-stage training mode;complex characteristics;RS images;adaptive sample selection strategy;central key point;texture structure;predetermined size;matching prediction stage;superpixel-based sample graded strategy;neural network;remote sensing image matching;subpixel matching accuracy;classical hand-crafted feature-based matching;feature learning approach","","1","48","","","","","IEEE","IEEE Journals"
"Optimal Combination of Image Denoisers","J. H. Choi; O. A. Elgendy; S. H. Chan","School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Image Processing","","2019","28","8","4016","4031","Given a set of image denoisers, each having a different denoising capability, is there a provably optimal way of combining these denoisers to produce an overall better result? An answer to this question is fundamental to designing an ensemble of weak estimators for complex scenes. In this paper, we present an optimal combination scheme by leveraging the deep neural networks and the convex optimization. The proposed framework, called the Consensus Neural Network (CsNet), introduces three new concepts in image denoising: 1) a provably optimal procedure to combine the denoised outputs via convex optimization; 2) a deep neural network to estimate the mean squared error (MSE) of denoised images without needing the ground truths; and 3) an image boosting procedure using a deep neural network to improve the contrast and to recover the lost details of the combined images. Experimental results show that CsNet can consistently improve the denoising performance for both deterministic and neural network denoisers.","","","10.1109/TIP.2019.2903321","Division of Computing and Communication Foundations; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663454","Image denoising;optimal combination;convex optimization;deep learning;convolutional neural networks","Neural networks;Noise reduction;Noise level;Convex functions;Gaussian noise;Standards;Deep learning","filtering theory;image denoising;mean square error methods;neural nets;optimisation","image denoisers;different denoising capability;optimal combination scheme;deep neural network;convex optimization;Consensus Neural Network;image denoising;provably optimal procedure;denoised outputs;denoised images;image boosting procedure;combined images;denoising performance;deterministic network denoisers;neural network denoisers","","1","68","","","","","IEEE","IEEE Journals"
"A Particle Swarm Optimization-Based Flexible Convolutional Autoencoder for Image Classification","Y. Sun; B. Xue; M. Zhang; G. G. Yen","School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Electrical and Computer Engineering, Oklahoma State University–Stillwater, Stillwater, OK, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","8","2295","2309","Convolutional autoencoders (CAEs) have shown their remarkable performance in stacking to deep convolutional neural networks (CNNs) for classifying image data during the past several years. However, they are unable to construct the state-of-the-art CNNs due to their intrinsic architectures. In this regard, we propose a flexible CAE (FCAE) by eliminating the constraints on the numbers of convolutional layers and pooling layers from the traditional CAE. We also design an architecture discovery method by exploiting particle swarm optimization, which is capable of automatically searching for the optimal architectures of the proposed FCAE with much less computational resource and without any manual intervention. We test the proposed approach on four extensively used image classification data sets. Experimental results show that our proposed approach in this paper significantly outperforms the peer competitors including the state-of-the-art algorithms.","","","10.1109/TNNLS.2018.2881143","Marsden Fund; Huawei Industry Fund; Victoria University of Wellington; National Natural Science Fund of China for Distinguished Young Scholar; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8571181","Convolutional autoencoder (CAE);deep learning;image classification;neural networks;particle swarm optimization (PSO)","Computer architecture;Neural networks;Convolutional codes;Optimization;Convolution;Particle swarm optimization;Stacking","convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);particle swarm optimisation","particle swarm optimization-based flexible convolutional autoencoder;image classification;convolutional autoencoders;remarkable performance;deep convolutional neural networks;image data;state-of-the-art CNNs;intrinsic architectures;flexible CAE;FCAE;convolutional layers;pooling layers;traditional CAE;architecture discovery method;optimal architectures;state-of-the-art algorithms","","2","72","","","","","IEEE","IEEE Journals"
"Deep Pyramidal Residual Networks for Spectral–Spatial Hyperspectral Image Classification","M. E. Paoletti; J. M. Haut; R. Fernandez-Beltran; J. Plaza; A. J. Plaza; F. Pla","Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón, Spain","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","2","740","754","Convolutional neural networks (CNNs) exhibit good performance in image processing tasks, pointing themselves as the current state-of-the-art of deep learning methods. However, the intrinsic complexity of remotely sensed hyperspectral images still limits the performance of many CNN models. The high dimensionality of the HSI data, together with the underlying redundancy and noise, often makes the standard CNN approaches unable to generalize discriminative spectral-spatial features. Moreover, deeper CNN architectures also find challenges when additional layers are added, which hampers the network convergence and produces low classification accuracies. In order to mitigate these issues, this paper presents a new deep CNN architecture specially designed for the HSI data. Our new model pursues to improve the spectral-spatial features uncovered by the convolutional filters of the network. Specifically, the proposed residual-based approach gradually increases the feature map dimension at all convolutional layers, grouped in pyramidal bottleneck residual blocks, in order to involve more locations as the network depth increases while balancing the workload among all units, preserving the time complexity per layer. It can be seen as a pyramid, where the deeper the blocks, the more feature maps can be extracted. Therefore, the diversity of high-level spectral-spatial attributes can be gradually increased across layers to enhance the performance of the proposed network with the HSI data. Our experiments, conducted using four well-known HSI data sets and 10 different classification techniques, reveal that our newly developed HSI pyramidal residual model is able to provide competitive advantages (in terms of both classification accuracy and computational time) over the state-of-the-art HSI classification methods.","","","10.1109/TGRS.2018.2860125","Ministerio de Educación (Resolución de 26 de diciembre de 2014 y de 19 de noviembre de 2015, de la Secretaría de Estado de Educación, Formación Profesional y Universidades, por la que se convocan ayudas para la formación de profesorado universitario, de los subprogramas de Formación y de Movilidad incluidos en el Programa Estatal de Promoción del Talento y su Empleabilidad, en el marco del Plan Estatal de Investigación Científica y Técnica y de Innovación 2013–2016; Consejería de Educación y Empleo, Junta de Extremadura; Generalitat Valenciana; Spanish Ministry of Economy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445697","Convolutional neural networks (CNNs);hyperspectral imaging (HSI);residual networks (ResNets)","Feature extraction;Hyperspectral imaging;Machine learning;Data models;Training","convolutional neural nets;feature extraction;hyperspectral imaging;image classification","HSI pyramidal residual model;classification methods;classification accuracy;high-level spectral-spatial attributes;feature maps;network depth;pyramidal bottleneck residual blocks;convolutional layers;feature map dimension;residual-based approach;convolutional filters;deep CNN architecture;network convergence;deeper CNN architectures;HSI data;CNN models;remotely sensed hyperspectral images;deep learning methods;image processing tasks;convolutional neural networks;spectral-spatial hyperspectral image classification;pyramidal residual networks","","7","67","","","","","IEEE","IEEE Journals"
"EuroCity Persons: A Novel Benchmark for Person Detection in Traffic Scenes","M. Braun; S. Krebs; F. Flohr; D. M. Gavrila","Daimler AG, Ulm 89081, Germany; Daimler AG, Ulm 89081, Germany; Daimler AG, Ulm 89081, Germany; TU Delft, Delft, CD, Netherlands","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","8","1844","1861","Big data has had a great share in the success of deep learning in computer vision. Recent works suggest that there is significant further potential to increase object detection performance by utilizing even bigger datasets. In this paper, we introduce the EuroCity Persons dataset, which provides a large number of highly diverse, accurate and detailed annotations of pedestrians, cyclists and other riders in urban traffic scenes. The images for this dataset were collected on-board a moving vehicle in 31 cities of 12 European countries. With over 238,200 person instances manually labeled in over 47,300 images, EuroCity Persons is nearly one order of magnitude larger than datasets used previously for person detection in traffic scenes. The dataset furthermore contains a large number of person orientation annotations (over 211,200). We optimize four state-of-the-art deep learning approaches (Faster R-CNN, R-FCN, SSD and YOLOv3) to serve as baselines for the new object detection benchmark. In experiments with previous datasets we analyze the generalization capabilities of these detectors when trained with the new dataset. We furthermore study the effect of the training set size, the dataset diversity (day- versus night-time, geographical region), the dataset detail (i.e., availability of object orientation information) and the annotation quality on the detector performance. Finally, we analyze error sources and discuss the road ahead.","","","10.1109/TPAMI.2019.2897684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8634919","Object detection;benchmarking","Proposals;Benchmark testing;Object detection;Feature extraction;Urban areas;Deep learning;Training","Big Data;computer vision;convolutional neural nets;learning (artificial intelligence);object detection;recurrent neural nets;road traffic;road vehicles;traffic engineering computing","object orientation information;person detection;computer vision;EuroCity Persons dataset;urban traffic scenes;person orientation annotations;object detection benchmark;Big Data;deep learning approaches;moving vehicle;Faster R-CNN;R-FCN;SSD;YOLOv3","","10","77","","","","","IEEE","IEEE Journals"
"Detecting Small Objects in Urban Settings Using SlimNet Model","Z. Yang; Y. Liu; L. Liu; X. Tang; J. Xie; X. Gao","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; Land Satellite Remote Sensing Application Center, Ministry of Natural Resources (MNR), Beijing, China; Land Satellite Remote Sensing Application Center, Ministry of Natural Resources (MNR), Beijing, China; Land Satellite Remote Sensing Application Center, Ministry of Natural Resources (MNR), Beijing, China; Land Satellite Remote Sensing Application Center, Ministry of Natural Resources (MNR), Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","8445","8457","The automatic extraction of small objects such as roadside milestones, small traffic signs, and other urban furniture remains a technical challenge. This study focuses on methods of deep learning to detect small urban elements in mobile mapping system (MMS) images. Based on images obtained by an MMS in urban areas, we create an urban element detection (UED) data set containing several kinds of small objects found in a city. A simple feature extraction convolution neural network (CNN) called SlimNet is proposed and combined with an optimized faster R-CNN framework. The resulting deep learning method can automatically extract small objects commonly found in cities, including manhole covers, milestones, and license plates. Experiments on the UED data set show that SlimNet has the highest accuracy compared with other popular networks, including VGG, MobileNet, ResNet and YOLOv3. The SlimNet model can achieve a mean average precision (AP) that is up to 12.3% higher than that of the lowest ResNet-152 network and can accelerate both training and detection owing to its relative simplicity. Moreover, k-means clustering is used to choose the dimensions of the anchor box for detection. We ran k-means clustering for different numbers of clusters, and the results show that at least four clusters are needed for detection using a small data set such as the UED. We also propose a method to use templates of different scales for anchors to further improve small object detection; this approach improved the AP by 3%-4% in our experiments.","","","10.1109/TGRS.2019.2921111","High-Resolution Earth Observation System Major Project (Civil Part); Active and Passive Composite Mapping and Application Technology With Visible, Infrared and Laser Sensors; National Key Research and Development Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746784","Convolution neural network (CNN);mobile mapping;small object detection;urban element detection (UED)","Feature extraction;Licenses;Urban areas;Deep learning;Three-dimensional displays;Object detection;Roads","convolutional neural nets;feature extraction;learning (artificial intelligence);mobile computing;object detection;pattern clustering","urban elements;mobile mapping system images;MMS;urban areas;urban element detection data set;optimized faster R-CNN framework;deep learning method;UED data;SlimNet model;mean average precision;object detection;urban settings;automatic extraction;roadside milestones;traffic signs;urban furniture;ResNet-152 network;feature extraction convolution neural network","","","51","","","","","IEEE","IEEE Journals"
"MSFgNet: A Novel Compact End-to-End Deep Network for Moving Object Detection","P. W. Patil; S. Murala","Department of Electrical Engineering, Computer Vision and Pattern Recognition Laboratory, IIT Ropar, Rupnagar, India; Department of Electrical Engineering, Computer Vision and Pattern Recognition Laboratory, IIT Ropar, Rupnagar, India","IEEE Transactions on Intelligent Transportation Systems","","2019","20","11","4066","4077","Moving object detection (MOD) in videos is a challenging task. Estimation of accurate background is the key to extracting the foreground from video frames. In this paper, we have proposed a novel compact end-to-end convolutional neural network architecture, motion saliency foreground network (MSFgNet), to estimate the background and to extract the foreground from video frames. Initially, the long streaming video is divided into a number of small video streams (SVS). The proposed network takes the SVS as an input and estimates the background frame for each SVS. Second, the saliency map is extracted using the current video frame and estimated background. Furthermore, a compact encoder-decoder network is proposed to extract the foreground from the estimated saliency maps. The performance of the proposed MSFgNet is tested on three benchmark datasets (CDnet-2014, LASIESTA, and PTIS) for MOD. The computational complexity (handling of number of parameters and execution time) and the performance of the proposed MSFgNet are compared with the existing state-of-the-art methods for MOD in terms of precision, recall, and F-measure. Performance analysis shows that the proposed network is very compact and outperforms the existing state-of-the-art methods for MOD in videos.","","","10.1109/TITS.2018.2880096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8546771","Background estimation;saliency;foreground;deep learning;CNN","Videos;Object detection;Estimation;Feature extraction;Lighting;Image color analysis;Saliency detection","computational complexity;computer vision;convolutional neural nets;Gaussian processes;image motion analysis;image sequences;learning (artificial intelligence);object detection;video signal processing;video streaming;video surveillance","MSFgNet;moving object detection;MOD;video frames;compact end-to-end convolutional neural network architecture;motion saliency foreground network;SVS;saliency maps;compact encoder-decoder network;video streaming;small video streams;compact end-to-end deep network;computational complexity","","2","48","","","","","IEEE","IEEE Journals"
"DeepCrack: Learning Hierarchical Convolutional Features for Crack Detection","Q. Zou; Z. Zhang; Q. Li; X. Qi; Q. Wang; S. Wang","School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; Shenzhen Key Laboratory of Spatial Smart Sensing and Service, Shenzhen University, Shenzhen, China; Shenzhen Research Institute of Big Data, Shenzhen, China; School of Computer Science, Wuhan University, Wuhan, China; Department of Computer Science and Engineering, University of South Carolina, Columbia, SC, USA","IEEE Transactions on Image Processing","","2019","28","3","1498","1512","Cracks are typical line structures that are of interest in many computer-vision applications. In practice, many cracks, e.g., pavement cracks, show poor continuity and low contrast, which bring great challenges to image-based crack detection by using low-level features. In this paper, we propose DeepCrack-an end-to-end trainable deep convolutional neural network for automatic crack detection by learning high-level features for crack representation. In this method, multi-scale deep convolutional features learned at hierarchical convolutional stages are fused together to capture the line structures. More detailed representations are made in larger scale feature maps and more holistic representations are made in smaller scale feature maps. We build DeepCrack net on the encoder-decoder architecture of SegNet and pairwisely fuse the convolutional features generated in the encoder network and in the decoder network at the same scale. We train DeepCrack net on one crack dataset and evaluate it on three others. The experimental results demonstrate that DeepCrack achieves F-measure over 0.87 on the three challenging datasets in average and outperforms the current state-of-the-art methods.","","","10.1109/TIP.2018.2878966","National Natural Science Foundation of China; National Key Research and Development Program of China; Natural Science Foundation of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517148","Line detection;edge detection;contour grouping;crack detection;convolutional neural network","Image edge detection;Feature extraction;Decoding;Image segmentation;Convolutional neural networks;Computer architecture","computer vision;crack detection;edge detection;feature extraction;image fusion;image representation;learning (artificial intelligence);neural nets","line structures;high-level feature learning;small scale feature maps;large scale feature maps;end-to-end trainable deep convolutional neural network;DeepCrack net;encoder-decoder architecture;SegNet;convolutional feature fusion;F-measure;crack dataset;decoder network;encoder network;holistic representations;detailed representations;hierarchical convolutional stages;multiscale deep convolutional features;crack representation;automatic crack detection;low-level features;image-based crack detection;pavement cracks;computer-vision applications;hierarchical convolutional features","","12","49","","","","","IEEE","IEEE Journals"
"Achieving Super-Resolution Remote Sensing Images via the Wavelet Transform Combined With the Recursive Res-Net","W. Ma; Z. Pan; J. Guo; B. Lei","Chinese Academy of Sciences, Institute of Electronics, Beijing, China; Chinese Academy of Sciences, Institute of Electronics, Beijing, China; Chinese Academy of Sciences, Institute of Electronics, Beijing, China; Chinese Academy of Sciences, Institute of Electronics, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","6","3512","3527","Deep learning (DL) has been successfully applied to single image super-resolution (SISR), which aims at reconstructing a high-resolution (HR) image from its low-resolution (LR) counterpart. Different from most current DL-based methods, which perform reconstruction in the spatial domain, we use a scheme based in the frequency domain to reconstruct the HR image at various frequency bands. Further, we propose a method that incorporates the wavelet transform (WT) and the recursive Res-Net. The WT is applied to the LR image to divide it into various frequency components. Then, an elaborately designed network with recursive residual blocks is used to predict high-frequency components. Finally, the reconstructed image is obtained via the inverse WT. This paper has three main contributions: 1) an SISR scheme based on the frequency domain is proposed under a DL framework to fully exploit the potential to depict images at different frequency bands; 2) recursive block and residual learning in global and local manners are adopted to ease the training of the deep network, and the batch normalization layer is removed to increase the flexibility of the network, save memory, and promote speed; and 3) the low-frequency wavelet component is replaced by an LR image with more details to further improve performance. To validate the effectiveness of the proposed method, extensive experiments are performed using the NWPU-RESISC45 data set, and the results demonstrate that the proposed method outperforms several state-of-the-art methods in terms of both objective evaluation and subjective perspective.","","","10.1109/TGRS.2018.2885506","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600724","Recursive network;remote sensing image;residual learning;super resolution;wavelet transform (WT)","Spatial resolution;Image reconstruction;Remote sensing;Discrete wavelet transforms;Signal resolution","geophysical image processing;image reconstruction;image resolution;learning (artificial intelligence);remote sensing;wavelet transforms","super-resolution remote sensing images;wavelet transform combined;recursive Res-Net;deep learning;single image super-resolution;high-resolution image;low-resolution counterpart;DL-based methods;spatial domain;frequency domain;HR image;LR image;recursive residual blocks;high-frequency components;reconstructed image;SISR scheme;residual learning;deep network;low-frequency wavelet component;NWPU-RESISC45 data set;frequency bands","","1","61","","","","","IEEE","IEEE Journals"
"Remote Sensing Airport Detection Based on End-to-End Deep Transferable Convolutional Neural Networks","S. Li; Y. Xu; M. Zhu; S. Ma; H. Tang","Aeronautics Engineering College, Air Force Engineering University, Xi’an, China; Unmanned System Research Institute, Northwestern Polytechnical University, Xi’an, China; Aeronautics Engineering College, Air Force Engineering University, Xi’an, China; Aeronautics Engineering College, Air Force Engineering University, Xi’an, China; Aeronautics Engineering College, Air Force Engineering University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","10","1640","1644","Rapid intelligent detection of airports from remote sensing images is required to accomplish autonomous intelligent landing of unmanned aerial vehicles (UAVs) and other tasks. To address the insufficiency of traditional models in detecting airports under complicated backgrounds from remote sensing images, we propose an end-to-end remote sensing airport hierarchical expression and detection model based on deep transferable convolutional neural networks. Based on transfer learning, we solve the fundamental problem of overfitting due to the inadequate number of labeled remote sensing images by transferring the network model from natural image source domain to remote sensing image target domain. In addition, we introduce a cascade region proposal network with soft-decision nonmaximal suppression to improve the network structure and the performance of our method under complex backgrounds. Moreover, we use skip-layer feature fusion and hard example mining methods to improve the object expression ability and the training efficiency. Finally, the experimental results demonstrate that the method established in this letter can quickly and effectively detect different types of airports over complex backgrounds and obtain better detection performance than the other detection methods.","","","10.1109/LGRS.2019.2904076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8681121","Airport detection;convolutional neural network;feature fusion;region proposal network (RPN);remote sensing images;transfer learning","Airports;Feature extraction;Remote sensing;Atmospheric modeling;Training;Nonhomogeneous media;Task analysis","airports;convolutional neural nets;feature extraction;learning (artificial intelligence);object detection;remote sensing","transfer learning;natural image source domain;remote sensing image target domain;cascade region proposal network;remote sensing airport detection;end-to-end deep transferable convolutional neural networks;rapid intelligent detection;autonomous intelligent landing;end-to-end remote sensing airport hierarchical expression;skip-layer feature fusion;object expression ability","","","16","","","","","IEEE","IEEE Journals"
"Neural Predictive Coding Using Convolutional Neural Networks Toward Unsupervised Learning of Speaker Characteristics","A. Jati; P. Georgiou","Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA; Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","10","1577","1589","Learning speaker-specific features is vital in many applications like speaker recognition, diarization, and speech recognition. This paper provides a novel approach, we term neural predictive coding (NPC), to learn speaker-specific characteristics in a completely unsupervised manner from large amounts of unlabeled training data that even contain many non-speech events and multi-speaker audio streams. The NPC framework exploits the proposed short-term active-speaker stationarity hypothesis which assumes two temporally close short speech segments belong to the same speaker, and thus a common representation that can encode the commonalities of both the segments, should capture the vocal characteristics of that speaker. We train a convolutional deep siamese network to produce “speaker embeddings” by learning to separate “same” versus “different” speaker pairs which are generated from an unlabeled data of audio streams. Two sets of experiments are done in different scenarios to evaluate the strength of NPC embeddings and compare with state-of-the-art in-domain supervised methods. First, two speaker identification experiments with different context lengths are performed in a scenario with comparatively limited within-speaker channel variability. NPC embeddings are found to perform the best at short duration experiment, and they provide complementary information to i-vectors for full utterance experiments. Second, a large-scale speaker verification task having a wide range of within-speaker channel variability is adopted as an upper-bound experiment where comparisons are drawn with in-domain supervised methods.","","","10.1109/TASLP.2019.2921890","Office of the Assistant Secretary of Defense for Health Affairs; Military Suicide Research Consortium; Psychological Health and Traumatic Brain Injury Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733892","Speaker-specific characteristics;unsupervised learning;Convolutional Neural Networks (CNN);siamese network;speaker recognition","Training;Task analysis;Speech recognition;Predictive coding;Speaker recognition;Neural networks;Speech processing","audio streaming;convolutional neural nets;speaker recognition;speech coding;unsupervised learning","active-speaker stationarity hypothesis;temporally close short speech segments;vocal characteristics;convolutional deep siamese network;speaker embeddings;audio streams;NPC embeddings;speaker identification experiments;within-speaker channel variability;large-scale speaker verification task;neural predictive coding;speaker recognition;speech recognition;speaker-specific characteristics;unlabeled training data;NPC framework;convolutional neural networks;speaker-specific features;unsupervised learning","","","62","Traditional","","","","IEEE","IEEE Journals"
"Deep CM-CNN for Spectrum Sensing in Cognitive Radio","C. Liu; J. Wang; X. Liu; Y. Liang","National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China (UESTC), Chengdu, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; Center for Intelligent Networking and Communications (CINC), University of Electronic Science and Technology of China (UESTC), Chengdu, China","IEEE Journal on Selected Areas in Communications","","2019","37","10","2306","2321","One of the key problems in spectrum sensing is to design the test statistic. Existing methods generally exploit the model-based features as the test statistic, such as energies and eigenvalues. However, these features could not accurately characterize the real environment. Motivated by this, in this paper, we use a deep neural network (DNN) to intelligently explore the data-driven test statistic. Firstly, we introduce a DNN-based detection framework, where a DNN-based likelihood ratio test (DNN-LRT) is derived to guarantee the optimality of the designed test statistic. As a realization of the developed DNN-based framework, we use the sample covariance matrix as the input of a convolutional neural network (CNN), and propose a covariance matrix-aware CNN (CM-CNN)-based spectrum sensing algorithm, which further improves the performance. In addition, we also provide the theoretical analysis of the proposed method. To the best of our knowledge, it's the first time to analyze the theoretical performance of CNN-based methods. Finally, simulation results demonstrate that the performance of the proposed method is close to that of the optimal detector. Particularly, the proposed method could achieve a detection probability of 96.7% with a false alarm probability of 1.9% at SNR = -18dB, which significantly outperforms the conventional methods.","","","10.1109/JSAC.2019.2933892","National Natural Science Foundation of China; Liaoning Revitalization Talents Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792213","Cognitive radio;spectrum sensing;deep learning;convolutional neural network","Sensors;Covariance matrices;Feature extraction;Uncertainty;Eigenvalues and eigenfunctions;Analytical models;Cognitive radio","cognitive radio;convolutional neural nets;covariance matrices;eigenvalues and eigenfunctions;radio spectrum management;signal detection;statistical testing;telecommunication computing","deep CM-CNN;cognitive radio;model-based features;eigenvalues;deep neural network;data-driven test statistic;DNN-based detection framework;DNN-based likelihood ratio test;DNN-LRT;designed test statistic;developed DNN-based framework;sample covariance matrix;convolutional neural network;covariance matrix-aware CNN-based spectrum sensing algorithm;CNN-based methods;noise figure 18.0 dB","","","42","","","","","IEEE","IEEE Journals"
"Contrastive-Regulated CNN in the Complex Domain: A Method to Learn Physical Scattering Signatures From Flexible PolSAR Images","J. Zhao; M. Datcu; Z. Zhang; H. Xiong; W. Yu","Department of Electric Information and Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; German Aerospace Center (DLR), Remote Sensing Technology Institute (IMF), Weßling, Germany; Department of Electric Information and Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electric Information and Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electric Information and Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","12","10116","10135","Single- and dual-polarimetric synthetic aperture radar (SAR) images provide very limited capabilities to interpret physical radar signatures. For generality and simplicity, we call single-polarimetric, dual-polarimetric, and fully polarimetric SAR (PolSAR) images flexible PolSAR images. In order to sufficiently extract physical scattering signatures from this kind of data and explore the potentials of different polarization modes on this task, this paper proposes a contrastive-regulated convolutional neural network (CNN) in the complex domain, attempting to learn a physically interpretable deep learning model directly from the original backscattered data. To achieve a better deep model containing physically interpretable parameters, the objective cost is compared to and selected from several commonly used loss functions in the complex form. The required ground-truth labels are generated automatically according to Cloude and Pottier's H-alpha division plane, which significantly reduces intensive labor cost and transfers this method to an unsupervised learning mechanism. The boundaries between different scattering signatures, however, sometimes show an erroneous separation. With the aim of aggregating intra-class instances and alienating inter-class instances, meanwhile, a complex-valued contrastive regularization term is computed mathematically and is added to the objective cost by a tradeoff factor. Moreover, data augmentation is applied to relieve the side effects caused by data imbalance. Finally, we performed experiments on German Aerospace Center's (DLR)'s L-band, high-resolution (HR), and airborne F-SAR data. Our results demonstrate the possibility of extracting physical scattering signatures from flexible PolSAR images. Physically interpretable potentials of SAR images with different polarization modes are analyzed, and we conclude with physical signature identification.","","","10.1109/TGRS.2019.2931620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809406","H-A-μ target decomposition;complex-valued convolutional neural networks (CNNs);contrastive-regulated objective cost;data augmentation;physical scattering signatures;polarimetric synthetic aperture radar (PolSAR)","Scattering;Radar polarimetry;Matrix decomposition;Radar imaging;Synthetic aperture radar;Task analysis","convolutional neural nets;electromagnetic wave scattering;radar computing;radar imaging;radar polarimetry;synthetic aperture radar;unsupervised learning","physically interpretable deep learning model;physically interpretable parameters;unsupervised learning mechanism;physical scattering signatures;physical signature identification;contrastive-regulated CNN;contrastive-regulated convolutional neural network;flexible PolSAR images;single polarimetric synthetic aperture radar image;dual-polarimetric synthetic aperture radar images;Cloude-Pottier H-alpha division plane;scattering signatures","","","45","IEEE","","","","IEEE","IEEE Journals"
"Transferable Object-Based Framework Based on Deep Convolutional Neural Networks for Building Extraction","R. Davari Majd; M. Momeni; P. Moallem","Department of Geomatics Engineering, Faculty of Civil Engineering and Transportation, University of Isfahan, Isfahan, Iran; Department of Geomatics Engineering, Faculty of Civil Engineering and Transportation, University of Isfahan, Isfahan, Iran; Department of Electrical Engineering, University of Isfahan, Isfahan, Iran","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","8","2627","2635","Automated building extraction from single, very-high-resolution (VHR) images is still one of the most challenging tasks for urban planning, estimating population, understanding urban dynamics, and many other applications. The complexities of building objects have caused the images of buildings to be oversegmented into multiple segments in the object-based image analysis (OBIA) method. Selecting the appropriate segmentation scale parameter is a major challenge in OBIA that influences the discriminative features extraction, especially for building objects. Furthermore, transferability of OBIA method is another challenge. Presently, convolutional neural networks (CNNs) are a well-understood tool for images scene classification. However, scene classification based on CNNs is still difficult due to the scale variation of the objects in VHR images. To meet these challenges, we propose a novel object-based deep CNN (OCNN) framework for VHR images. The datasets used for testing were Vaihingen (Germany) aerial images and a Tunis Worldview-2 (WV2) satellite imagery. Experimental results prove that our framework is extensible to different types of the image with the same sensor or another sensor (for example WV2) with once-fine-tuning. In addition, our framework extracts the different types of building with respect to size, color, material, spectral similarity to roads, and complex backgrounds. Quantitative evaluation at the object level demonstrated that the proposed framework could yield promising results (average precision 0.88, recall 0.92, quality 0.82, F-score 0.90, overall accuracy 0.95, and Kappa coefficient 0.90). Comparative experimental results indicate that our proposed OCNN significantly outperforms the traditional method for building extraction.","","","10.1109/JSTARS.2019.2924582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758317","Building extraction;convolutional neural networks (CNN);deep learning (DL);object-based image analysis (OBIA);patch extraction;semantic labeling;very-high-resolution (VHR) imagery","Buildings;Feature extraction;Image segmentation;Training;Semantics;Remote sensing;Image analysis","feature extraction;geophysical image processing;image classification;image segmentation;neural nets;remote sensing","VHR images;Vaihingen aerial images;Tunis Worldview-2 satellite imagery;transferable object-based framework;deep convolutional neural networks;automated building extraction;very-high-resolution images;urban planning;urban dynamics;object-based image analysis method;discriminative features extraction;OBIA method;images scene classification;object-based deep CNN framework;segmentation scale parameter;Germany aerial images","","","50","Traditional","","","","IEEE","IEEE Journals"
"Learning Two-Branch Neural Networks for Image-Text Matching Tasks","L. Wang; Y. Li; J. Huang; S. Lazebnik","Computer Science Department, University of Illinois at Urbana-Champaign, Urbana, IL; Georgia Institute of Technology, Atlanta, GA; Computer Science Department, University of Illinois at Urbana-Champaign, Urbana, IL; Computer Science Department, University of Illinois at Urbana-Champaign, Urbana, IL","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","2","394","407","Image-language matching tasks have recently attracted a lot of attention in the computer vision field. These tasks include image-sentence matching, i.e., given an image query, retrieving relevant sentences and vice versa, and region-phrase matching or visual grounding, i.e., matching a phrase to relevant regions. This paper investigates two-branch neural networks for learning the similarity between these two data modalities. We propose two network structures that produce different output representations. The first one, referred to as an embedding network, learns an explicit shared latent embedding space with a maximum-margin ranking loss and novel neighborhood constraints. Compared to standard triplet sampling, we perform improved neighborhood sampling that takes neighborhood information into consideration while constructing mini-batches. The second network structure, referred to as a similarity network, fuses the two branches via element-wise product and is trained with regression loss to directly predict a similarity score. Extensive experiments show that our networks achieve high accuracies for phrase localization on the Flickr30K Entities dataset and for bi-directional image-sentence retrieval on Flickr30K and MSCOCO datasets.","","","10.1109/TPAMI.2018.2797921","National Science Foundation; Xerox; Sloan Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8268651","Deep learning;cross-modal retrieval;image-sentence retrieval;phrase localization;visual grounding","Task analysis;Visualization;Bidirectional control;Training;Feature extraction;Grounding;Natural languages","computer vision;image matching;image retrieval;image sampling;learning (artificial intelligence);neural nets;text analysis","network structure;embedding network;latent embedding space;maximum-margin ranking loss;neighborhood constraints;standard triplet sampling;neighborhood sampling;similarity network;phrase localization;bi-directional image-sentence retrieval;image-text matching tasks;image-language matching tasks;image-sentence matching;image query;relevant sentences;region-phrase matching;output representations;computer vision;two-branch neural network learning;data modalities;element-wise product;regression loss;Flickr30K Entities dataset;MSCOCO datasets","","6","67","","","","","IEEE","IEEE Journals"
"Sig-NMS-Based Faster R-CNN Combining Transfer Learning for Small Target Detection in VHR Optical Remote Sensing Imagery","R. Dong; D. Xu; J. Zhao; L. Jiao; J. An","Jiangsu Key Laboratory of Internet of Things and Control Technologies, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Jiangsu Key Laboratory of Internet of Things and Control Technologies, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Information Technology Section of Shanghai Railway Administration, Jinhua, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","8534","8545","Small target detection is a challenging task in veryhigh-resolution (VHR) optical remote sensing imagery, because small targets occupy a minuscule number of pixels and are easily disturbed by backgrounds or occluded by others. Although current convolutional neural network (CNN)-based approaches perform well when detecting normal objects, they are barely suitable for detecting small ones. Two practical problems stand in their way. First, current CNN-based approaches are not specifically designed for the minuscule size of small targets (~15 or ~10 pixels in extent). Second, no well-established data sets include labeled small targets and establishing one from scratch is labor-intensive and time-consuming. To address these two issues, we propose an approach that combines Sig-NMS-based Faster R-CNN with transfer learning. Sig-NMS replaces traditional non-maximum suppression (NMS) in the stage of region proposal network and decreases the possibility of missing small targets. Transfer learning can effectively label remote sensing images by automatically annotating both object classes and object locations. We conduct an experiment on three data sets of VHR optical remote sensing images, RSOD, LEVIR, and NWPU VHR-10, to validate our approach. The results demonstrate that the proposed approach can effectively detect small targets in the VHR optical remote sensing images of about 10 × 10 pixels and automatically label small targets as well. In addition, our method presents better mean average precisions than other state-of-the-art methods: 1.5% higher when performing on the RSOD data set, 17.8% higher on the LEVIR data set, and 3.8% higher on NWPU VHR-10.","","","10.1109/TGRS.2019.2921396","National Natural Science Foundation of China; Natural Science Research of Jiangsu Higher Education Institutions of China; Foundation of Graduate Innovation Center in NUAA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8763909","Deep convolution neural network;Faster R-CNN;non-maximum suppression (NMS);optical remote sensing images;Sig-NMS;small object detection;transfer learning","Optical imaging;Optical sensors;Remote sensing;Object detection;Optical fiber networks;Task analysis","geophysical image processing;image resolution;image sensors;learning (artificial intelligence);object detection;remote sensing","VHR optical remote sensing images;NWPU VHR-10;RSOD data set;LEVIR data set;target detection;VHR optical remote sensing imagery;convolutional neural network-based approaches;CNN-based approaches;sig-NMS-based faster R-CNN combining transfer learning;very high-resolution optical remote sensing imagery","","1","42","","","","","IEEE","IEEE Journals"
"Learning a Probabilistic Model for Diffeomorphic Registration","J. Krebs; H. Delingette; B. Mailhé; N. Ayache; T. Mansi","Université Côte d’Azur, Inria, Epione Team, Sophia Antipolis, France; Université Côte d’Azur, Inria, Epione Team, Sophia Antipolis, France; Siemens Healthineers, Digital Services, Digital Technology and Innovation, Princeton, NJ, USA; Université Côte d’Azur, Inria, Epione Team, Sophia Antipolis, France; Siemens Healthineers, Digital Services, Digital Technology and Innovation, Princeton, NJ, USA","IEEE Transactions on Medical Imaging","","2019","38","9","2165","2176","We propose to learn a low-dimensional probabilistic deformation model from data which can be used for the registration and the analysis of deformations. The latent variable model maps similar deformations close to each other in an encoding space. It enables to compare deformations, to generate normal or pathological deformations for any new image, or to transport deformations from one image pair to any other image. Our unsupervised method is based on the variational inference. In particular, we use a conditional variational autoencoder network and constrain transformations to be symmetric and diffeomorphic by applying a differentiable exponentiation layer with a symmetric loss function. We also present a formulation that includes spatial regularization such as the diffusion-based filters. In addition, our framework provides multi-scale velocity field estimations. We evaluated our method on 3-D intra-subject registration using 334 cardiac cine-MRIs. On this dataset, our method showed the state-of-the-art performance with a mean DICE score of 81.2% and a mean Hausdorff distance of 7.3 mm using 32 latent dimensions compared to three state-of-the-art methods while also demonstrating more regular deformation fields. The average time per registration was 0.32 s. Besides, we visualized the learned latent space and showed that the encoded deformations can be used to transport deformations and to cluster diseases with a classification accuracy of 83% after applying a linear projection.","","","10.1109/TMI.2019.2897112","AAP Santé; INRIA Sophia Antipolis–Méditerranée, “NEF” computation cluster; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633848","Deformable registration;probabilistic encoding;deep learning;conditional variational autoencoder;latent variable model;deformation transport","Strain;Probabilistic logic;Deformable models;Training;Estimation;Image registration;Computational modeling","biomedical MRI;diseases;image registration;learning (artificial intelligence);medical image processing;pattern clustering","constrain transformations;symmetric loss function;learned latent space;encoded deformations;probabilistic model;diffeomorphic registration;low-dimensional probabilistic deformation model;pathological deformations;image pair;unsupervised method;conditional variational autoencoder network;3D intrasubject registration;disease;cardiac cine-MRI","","1","55","","","","","IEEE","IEEE Journals"
"SylNet: An Adaptable End-to-End Syllable Count Estimator for Speech","S. Seshadri; O. Räsänen","Department of Signal Processing and Acoustics, Aalto University, Aalto, Finland; Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland","IEEE Signal Processing Letters","","2019","26","9","1359","1363","Automatic syllable count estimation (SCE) is used in a variety of applications ranging from speaking rate estimation to detecting social activity from wearable microphones or developmental research concerned with quantifying speech heard by language-learning children in different environments. The majority of previously utilized SCE methods have relied on heuristic digital signal processing (DSP) methods, and only a small number of bi-directional long short-term memory (BLSTM) approaches have made use of modern machine learning approaches in the SCE task. This letter presents a novel end-to-end method called SylNet for automatic syllable counting from speech, built on the basis of a recent developments in neural network architectures. We describe how the entire model can be optimized directly to minimize SCE error on the training data without annotations aligned at the syllable level, and how it can be adapted to new languages using limited speech data with known syllable counts. Experiments on several different languages reveal that SylNet generalizes to languages beyond its training data and further improves with adaptation. It also outperforms several previously proposed methods for syllabification, including end-to-end BLSTMs.","","","10.1109/LSP.2019.2929415","Academy of Finland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8769944","syllable count estimation;end-to-end learning;deep learning;speech processing","Estimation;Training;Adaptation models;Speech processing;Signal processing algorithms;Training data;Channel estimation","estimation theory;learning (artificial intelligence);natural language processing;neural net architecture;recurrent neural nets;speech processing","SylNet;adaptable end-to-end syllable count estimator;automatic syllable count estimation;wearable microphones;developmental research;language-learning children;heuristic digital signal processing methods;SCE task;automatic syllable counting;SCE error;training data;syllable level;speech data;end-to-end BLSTMs;machine learning approaches;SCE methods;speaking rate estimation;social activity detection;DSP methods;bi-directional short-term memory approaches;neural network architectures;limited speech data","","","29","Traditional","","","","IEEE","IEEE Journals"
"Real-Time Traffic Speed Estimation With Graph Convolutional Generative Autoencoder","J. J. Q. Yu; J. Gu","Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong","IEEE Transactions on Intelligent Transportation Systems","","2019","20","10","3940","3951","Real-time traffic speed estimation is an essential component of intelligent transportation system (ITS) technologies. It is the foundation of modern transportation control and management applications. However, the existing traffic speed acquisition systems can only provide real-time speed measurements of a small number of roads with stationary speed sensors and crowdsourcing vehicles. How to utilize this information to provide traffic speed maps for transportation networks is becoming a key problem in ITSs. In this paper, we present a novel deep-learning model called graph convolutional generative autoencoder to fully address the real-time traffic speed estimation problem. The proposed model incorporates the recent development in deep-learning techniques to extract the spatial correlation of the transportation network from the input incomplete historical data. To evaluate the proposed speed estimation technique, we conduct comprehensive case studies on a real-world transportation network and vehicular traces. The simulation results demonstrate that the proposed technique can notably outperform existing traffic speed estimation and deep-learning techniques. In addition, the impact of dataset properties and control parameters is investigated.","","","10.1109/TITS.2019.2910560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8697151","Traffic estimation;deep learning;generative adversarial network;graph convolutional network;data-driven model","Estimation;Roads;Real-time systems;Global Positioning System;Sensors;Data models","convolutional neural nets;graph theory;intelligent transportation systems;learning (artificial intelligence);road traffic control;traffic engineering computing","graph convolutional generative autoencoder;intelligent transportation system technologies;modern transportation control;management applications;real-time speed measurements;stationary speed sensors;crowdsourcing vehicles;traffic speed maps;real-time traffic speed estimation problem;speed estimation technique;real-world transportation network;deep-learning techniques;traffic speed acquisition systems","","1","54","","","","","IEEE","IEEE Journals"
"Squeeze and Excitation Rank Faster R-CNN for Ship Detection in SAR Images","Z. Lin; K. Ji; X. Leng; G. Kuang","State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","5","751","755","Synthetic aperture radar (SAR) ship detection is an important part of marine monitoring. With the development in computer vision, deep learning has been used for ship detection in SAR images such as the faster region-based convolutional neural network (R-CNN), single-shot multibox detector, and densely connected network. In SAR ship detection field, deep learning has much better detection performance than traditional methods on nearshore areas. This is because traditional methods need sea-land segmentation before detection, and inaccurate sea-land mask decreases its detection performance. Though current deep learning SAR ship detection methods still have many false detections in land areas, and some ships are missed in sea areas. In this letter, a new network architecture based on the faster R-CNN is proposed to further improve the detection performance by using squeeze and excitation mechanism. In order to improve performance, first, the feature maps are extracted and concatenated to obtain multiscale feature maps with ImageNet pretrained VGG network. After region of interest pooling, an encoding scale vector which has values between 0 and 1 is generated from subfeature maps. The scale vector is ranked, and only top K values will be preserved. Other values will be set to 0. Then, the subfeature maps are recalibrated by this scale vector. The redundant subfeature maps will be suppressed by this operation, and the detection performance of detector can be improved. The experimental results based on Sentinel-1 images show that the detection performance of the proposed method achieves 0.836 which is 9.7% better than the state-of-the-art method when using F1 as matric and executes 14% faster.","","","10.1109/LGRS.2018.2882551","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8570858","Deep learning;faster region-based convolutional neural network (R-CNN);ship detection;synthetic aperture radar (SAR)","Feature extraction;Marine vehicles;Synthetic aperture radar;Proposals;Training;Data mining","feature extraction;geophysical image processing;image segmentation;learning (artificial intelligence);neural nets;object detection;radar imaging;ships;synthetic aperture radar","SAR images;synthetic aperture radar ship detection;SAR ship detection field;inaccurate sea-land mask;false detections;convolutional neural network;deep learning SAR ship detection methods;R-CNN;sea-land segmentation","","3","23","","","","","IEEE","IEEE Journals"
"A Deep Information Sharing Network for Multi-Contrast Compressed Sensing MRI Reconstruction","L. Sun; Z. Fan; X. Fu; Y. Huang; X. Ding; J. Paisley","School of Information Science and Engineering, Xiamen University, Xiamen, China; School of Information Science and Engineering, Xiamen University, Xiamen, China; Department of Automation, University of Science and Technology of China, Hefei, China; School of Information Science and Engineering, Xiamen University, Xiamen, China; School of Information Science and Engineering, Xiamen University, Xiamen, China; Department of Electrical Engineering, Columbia University, New York, NY, USA","IEEE Transactions on Image Processing","","2019","28","12","6141","6153","Compressed sensing (CS) theory can accelerate multi-contrast magnetic resonance imaging (MRI) by sampling fewer measurements within each contrast. However, conventional optimization-based reconstruction models suffer several limitations, including a strict assumption of shared sparse support, time-consuming optimization, and “shallow” models with difficulties in encoding the patterns contained in massive MRI data. In this paper, we propose the first deep learning model for multi-contrast CS-MRI reconstruction. We achieve information sharing through feature sharing units, which significantly reduces the number of model parameters. The feature sharing unit combines with a data fidelity unit to comprise an inference block, which are then cascaded with dense connections, allowing for efficient information transmission across different depths of the network. Experiments on various multi-contrast MRI datasets show that the proposed model outperforms both state-of-the-art single-contrast and multi-contrast MRI methods in accuracy and efficiency. We demonstrate that improved reconstruction quality can bring benefits to subsequent medical image analysis. Furthermore, the robustness of the proposed model to misregistration shows its potential in real MRI applications.","","","10.1109/TIP.2019.2925288","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; CCF-Tencent Open Fund; Natural Science Foundation of Fujian Province; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758456","Compressed sensing;multi-contrast MRI reconstruction;deep neural networks","Transmitters;Receivers;Fading channels;Network coding;Output feedback;Numerical models;Nickel","biomedical MRI;compressed sensing;image reconstruction;medical image processing","MRI data;optimization-based reconstruction models;multicontrast compressed sensing MRI reconstruction;medical image analysis;medical image analysis;multicontrast MRI datasets;multicontrast CS-MRI reconstruction;deep learning model;multicontrast magnetic resonance imaging;compressed sensing theory;deep information sharing network;MRI applications;multicontrast MRI methods","","2","41","","","","","IEEE","IEEE Journals"
"Learning Deep Ship Detector in SAR Images From Scratch","Z. Deng; H. Sun; S. Zhou; J. Zhao","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","6","4021","4039","Recently, deep learning-based methods have brought new ideas for ship detection in synthetic aperture radar (SAR) images. However, several challenges still exist: 1) deep models contain millions of parameters, whereas the available annotated samples are not sufficient in number for training. Therefore, most deep detectors have to fine-tune networks pre-trained on ImageNet, which incurs learning bias due to the huge domain mismatch between SAR images and ImageNet images. Furthermore, it has a little flexibility to redesign the network structure; and 2) ships in SAR images are relatively small in size and densely clustered, whereas most deep detectors have poor performance with small objects due to the rough feature map used for detection and the extreme foreground-background imbalance. To address these problems, this paper proposes an effective approach to learn deep ship detector from scratch. First, we design a condensed backbone network, which consists of several dense blocks. Hence, earlier layers can receive additional supervision from the objective function through the dense connections, which makes it easy to train. In addition, feature reuse strategy is adopted to make it highly parameter efficient. Therefore, the backbone network could be freely designed and effectively trained from scratch without using a large amount of annotated samples. Second, we improve the cross-entropy loss to address the foreground-background imbalance and predict multi-scale ship proposals from several intermediate layers to improve the recall rate. Then, position-sensitive score maps are adopted to encode position information into each ship proposal for discrimination. The comparison results on the Sentinel-1 data set show that: 1) learning ship detector from scratch achieved better performance than ImageNet pre-trained model-based detectors and 2) our method is more effective than existing algorithms for detecting the small and densely clustered ships.","","","10.1109/TGRS.2018.2889353","National Natural Science Foundation of China; Fund of Innovation of NUDT Graduate School; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633866","Convolutional neural networks (CNNs);ship detection;ship discrimination network;ship proposal network;synthetic aperture radar (SAR)","Marine vehicles;Synthetic aperture radar;Detectors;Proposals;Task analysis;Training;Feature extraction","convolutional neural nets;radar imaging;sensors;ships;synthetic aperture radar","deep ship detector;SAR images;synthetic aperture radar images;condensed backbone network;objective function;dense connections;feature reuse strategy;cross-entropy loss;foreground-background imbalance","","3","65","","","","","IEEE","IEEE Journals"
"Secondary Information Aware Facial Expression Recognition","Y. Tian; J. Cheng; Y. Li; S. Wang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Signal Processing Letters","","2019","26","12","1753","1757","Facial expression recognition (FER) is a key factor in human behavior analysis. Most algorithms deal with FER as a pure classification problem, assuming that expressions are exclusive to each other. In this letter, the problem of FER is tackled from a more detailed view: learning to discriminate expressions with consideration of the secondary information. We propose the Secondary Information aware Facial Expression Network (SIFE-Net) to explore the latent components without auxiliary labeling, and we propose a novel dynamic weighting strategy to teach the SIFE-Net. In contrast to traditional classifiers trained with one-hot labels, the proposed SIFE-Net takes advantage of secondary expression information and has more rational feature distributions. We carry out extensive experiments and analysis on three widely-used FER datasets, i.e. the CK+ dataset, the JAFFE dataset, and the RAF dataset. Experimental results show that the SIFE-Net achieves state-of-the-art performance on all three datasets, which demonstrates the effectiveness of our method.","","","10.1109/LSP.2019.2942138","State Key Development Program in 13th Five-Year; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844064","Facial expression recognition;secondary information;deep learning","Training;Face recognition;Signal processing algorithms;Deep learning;Data mining;Heuristic algorithms;Image recognition","emotion recognition;face recognition;image classification;learning (artificial intelligence)","SIFE-Net;FER datasets;secondary expression information;Secondary Information aware Facial Expression Network;pure classification problem;human behavior analysis;Secondary Information aware Facial Expression recognition","","","39","IEEE","","","","IEEE","IEEE Journals"
"SMART: Joint Sampling and Regression for Visual Tracking","J. Gao; T. Zhang; C. Xu","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciencesxs, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciencesxs, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciencesxs, Beijing, China","IEEE Transactions on Image Processing","","2019","28","8","3923","3935","Most existing trackers are either sampling-based or regression-based methods. Sampling-based methods estimate the target state by sampling many target candidates. Although these methods achieve significant performance, they often suffer from a high computational burden. Regression-based methods often learn a computationally efficient regression function to directly predict the geometric distortion between frames. However, most of these methods require large-scale external training videos and are still not very impressive in terms of accuracy. To make both types of methods enhance and complement each other, in this paper, we propose a joint sampling and regression scheme for visual tracking, which leverages the region proposal network by a novel design. Specifically, our method can jointly exploit discriminative target proposal generation and structural target regression to predict target location in a simple feedforward propagation. We evaluate the proposed method on five challenging benchmarks, and extensive experimental results demonstrate that our method performs favorably compared with state-of-the-art trackers with respect to both accuracy and speed.","","","10.1109/TIP.2019.2904434","National Natural Science Foundation of China; Key Research Program of Frontier Sciences, CAS; Natural Science Foundation of Beijing Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666049","Visual tracking;deep learning;sampling and regression","Target tracking;Visualization;Proposals;Training;Videos;Deep learning;Real-time systems","feedforward neural nets;image sampling;learning (artificial intelligence);object tracking;regression analysis;target tracking;video signal processing","joint sampling;visual tracking;regression-based methods;sampling-based methods;regression scheme;discriminative target proposal generation;structural target regression;target location;SMART;target state estimation;geometric distortion prediction;large-scale external training videos;feedforward propagation;region proposal network","","","79","","","","","IEEE","IEEE Journals"
"Toward Affordance Detection and Ranking on Novel Objects for Real-World Robotic Manipulation","F. Chu; R. Xu; L. Seguin; P. A. Vela","Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA; Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA; Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA; Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Robotics and Automation Letters","","2019","4","4","4070","4077","This letter presents a framework to detect and rank affordances of novel objects to assist with robotic manipulation tasks. The framework segments the affordance map of unseen objects using region-based affordance segmentation. Detected affordances define an initial state from which to generate action primitives for manipulation via the planning domain definition language (PDDL). The proposed category-agnostic affordance segmentation approach generalizes learned affordances to unseen objects by utilizing binary classification on proposed instance masks. The predicted pixel-wise level affordances are ranked by KL-divergence, augmenting the available affordance choices for manipulation tasks with non-primary affordances of an object. Experimental results show that the proposed method achieves state-of-the-art performance on affordance segmentation of novel objects, and outperforms baselines on affordance ranking. Actual robotic manipulation scenarios demonstrate the use of affordance detection with PDDL-generated action primitives for task execution. Prediction of ranked affordances on unseen objects provides flexibility to accomplish goal-oriented tasks.","","","10.1109/LRA.2019.2930364","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770077","Perception for grasping and manipulation;deep learning in robotics and automation;RGB-D perception","Planning;Robots;Task analysis;Image segmentation;Deep learning;Semantics;Tools","image segmentation;learning (artificial intelligence);manipulators;object detection;planning (artificial intelligence);robot vision","unseen objects;predicted pixel-wise level affordances;available affordance choices;nonprimary affordances;affordance ranking;actual robotic manipulation scenarios;PDDL-generated action primitives;ranked affordances;real-world robotic manipulation;robotic manipulation tasks;framework segments;affordance map;detected affordances;initial state;planning domain definition language;category-agnostic affordance segmentation approach;affordance detection","","","48","Traditional","","","","IEEE","IEEE Journals"
"Segmenting the Brain Surface From CT Images With Artifacts Using Locally Oriented Appearance and Dictionary Learning","J. A. Onofrey; L. H. Staib; X. Papademetris","Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA; Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA; Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA","IEEE Transactions on Medical Imaging","","2019","38","2","596","607","The accurate segmentation of the brain surface in post-surgical computed tomography (CT) images is critical for image-guided neurosurgical procedures in epilepsy patients. Following surgical implantation of intracranial electrodes, surgeons require accurate registration of the post-implantation CT images to the pre-implantation functional and structural magnetic resonance imaging to guide surgical resection of epileptic tissue. One way to perform the registration is via surface matching. The key challenge in this setup is the CT segmentation, where the extraction of the cortical surface is difficult due to the missing parts of the skull and artifacts introduced from the electrodes. In this paper, we present a dictionary learning-based method to segment the brain surface in post-surgical CT images of epilepsy patients following surgical implantation of electrodes. We propose learning a model of locally oriented appearance that captures both the normal tissue and the artifacts found along this brain surface boundary. Utilizing a database of clinical epilepsy imaging data to train and test our approach, we demonstrate that our method using locally oriented image appearance both more accurately extracts the brain surface and better localizes electrodes on the post-operative brain surface compared to standard, non-oriented appearance modeling. In addition, we compare our method to a standard atlas-based segmentation approach and to a U-Net-based deep convolutional neural network segmentation method.","","","10.1109/TMI.2018.2868045","National Institute of Neurological Disorders and Stroke; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451941","Segmentation;dictionary learning;skull stripping;computed-tomography (CT);image-guided surgery;epilepsy","Computed tomography;Image segmentation;Brain;Electrodes;Magnetic resonance imaging;Surgery","biomedical electrodes;brain;computerised tomography;feature extraction;image registration;image segmentation;learning (artificial intelligence);medical disorders;medical image processing;neural nets;neurophysiology;surgery","nonoriented appearance modeling;standard atlas-based segmentation approach;artifacts;locally oriented appearance;post-surgical computed tomography images;image-guided neurosurgical procedures;epilepsy patients;surgical implantation;post-implantation CT images;structural magnetic resonance imaging;surgical resection;surface matching;CT segmentation;cortical surface;dictionary learning-based method;post-surgical CT images;brain surface boundary;clinical epilepsy imaging data;locally oriented image appearance;post-operative brain surface;electrodes;U-Net-based deep convolutional neural network segmentation method","","","39","","","","","IEEE","IEEE Journals"
"Deep Belief Network for Meteorological Time Series Prediction in the Internet of Things","Y. Cheng; X. zhou; S. Wan; K. R. Choo","Jiangsu Key Laboratory of Agricultural Meteorology, Nanjing University of Information Science and Technology, Nanjing, China; Department of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Information and Safety Engineering, Zhongnan University of Economics and Law, Wuhan, China; Department of Information Systems and Cyber Security, University of Texas at San Antonio, San Antonio, TX, USA","IEEE Internet of Things Journal","","2019","6","3","4369","4376","Seeking to address the challenges associated with high-dimensional complex time series representations of recurrent neural networks, such as low generalization ability and long training time, a hybrid neural network based on a deep belief network (DBN) is proposed in this paper to facilitate time series predictions for the Internet of Things. In our approach, we integrate both a DBN and a recurrent neural network with the gated recurrent unit as the activation unit. First, we implement unsupervised pretraining through the DBN and then supervise the curve fitting using the recurrent neural network. Finally, the hybrid neural network is learned and can make predictions. The experimental results show that the hybrid neural network has a stronger historical learning ability than two other widely used recurrent neural networks and can effectively reduce the number of iterations required by the recurrent neural network, thereby reducing the overall learning time.","","","10.1109/JIOT.2018.2878477","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8513849","Deep belief network (DBN);gated recurrent unit (GRU);recurrent neural network;time series prediction","Mathematical model;Time series analysis;Predictive models;Neural networks;Autoregressive processes;Feature extraction;Internet of Things","backpropagation;belief networks;curve fitting;geographic information systems;Internet of Things;recurrent neural nets;time series;unsupervised learning;weather forecasting","deep belief network;meteorological time series prediction;high-dimensional complex time series representations;recurrent neural network;time series predictions;Internet of things;gated recurrent unit;curve fitting;unsupervised pretraining","","3","54","","","","","IEEE","IEEE Journals"
"Deep Learning for Talker-Dependent Reverberant Speaker Separation: An Empirical Study","M. Delfarah; D. Wang","Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Computer Science and Engineering, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","11","1839","1848","Speaker separation refers to the problem of separating speech signals from a mixture of simultaneous speakers. Previous studies are limited to addressing the speaker separation problem in anechoic conditions. This paper addresses the problem of talker-dependent speaker separation in reverberant conditions, which are characteristic of real-world environments. We employ recurrent neural networks with bidirectional long short-term memory (BLSTM) to separate and dereverberate the target speech signal. We propose two-stage networks to effectively deal with both speaker separation and speech dereverberation. In the two-stage model, the first stage separates and dereverberates two-talker mixtures and the second stage further enhances the separated target signal. We have extensively evaluated the two-stage architecture, and our empirical results demonstrate large improvements over unprocessed mixtures and clear performance gain over single-stage networks in a wide range of target-to-interferer ratios and reverberation times in simulated as well as recorded rooms. Moreover, we show that time-frequency masking yields better performance than spectral mapping for reverberant speaker separation.","","","10.1109/TASLP.2019.2934319","National Institute on Deafness and Other Communication Disorders; Ohio Supercomputer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794611","Cochannel speech separation;two-stage network;deep neural networks;speech dereverberation","Training;Spectrogram;Reverberation;Noise measurement;Speech processing;Recurrent neural networks","learning (artificial intelligence);recurrent neural nets;reverberation;source separation;speaker recognition","recurrent neural networks;spectral mapping;time-frequency masking;target-to-interferer ratios;bidirectional long short-term memory;BLSTM;two-talker mixtures;single-stage networks;two-stage model;speech dereverberation;two-stage networks;anechoic conditions;speech signals;talker-dependent reverberant speaker separation","","","42","Traditional","","","","IEEE","IEEE Journals"
"Registration of Multimodal Remote Sensing Image Based on Deep Fully Convolutional Neural Network","H. Zhang; W. Ni; W. Yan; D. Xiang; J. Wu; X. Yang; H. Bian","Remote Sensing Data Processing Lab, Northwest Institute of Nuclear Technology, Xi'an, China; Northwest Institute of Nuclear Technology, Xi'an, China; Remote Sensing Data Processing Lab, Northwest Institute of Nuclear Technology, Xi'an, China; Division of Geoinformatics, KTH Royal Institute of Technology, Stockholm, Sweden; Remote Sensing Data Processing Lab, Northwest Institute of Nuclear Technology, Xi'an, China; Remote Sensing Data Processing Lab, Northwest Institute of Nuclear Technology, Xi'an, China; Remote Sensing Data Processing Lab, Northwest Institute of Nuclear Technology, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","8","3028","3042","Multimodal image registration is the fundamental technique for scene analysis with series remote sensing images of different spectrum region. Due to the highly nonlinear radiometric relationship, it is quite challenging to find common features between images of different modal types. This paper resorts to the deep neural network, and tries to learn descriptors for multimodal image patch matching, which is the key issue of image registration. A Siamese fully convolutional network is set up and trained with a novel loss function, which adopts the strategy of maximizing the feature distance between positive and hard negative samples. The two branches of the Siamese network are connected by the convolutional operation, resulting in the similarity score between the two input image patches. The similarity score value is used, not only for correspondence point location, but also for outlier identification. A generalized workflow for deep feature based multimodal RS image registration is constructed, including the training data curation, candidate feature point generation, and outlier removal. The proposed network is tested on a variety of optical, near infrared, thermal infrared, SAR, and map images. Experiment results verify the superiority over other state-of-the-art approaches.","","","10.1109/JSTARS.2019.2916560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730416","Fully convolutional;hard negative sample;multimodal image registration;Siamese neural network","Image registration;Measurement;Feature extraction;Remote sensing;Optical imaging;Optical sensors;Task analysis","convolutional neural nets;feature extraction;geophysical image processing;image classification;image matching;image registration;image representation;image segmentation;learning (artificial intelligence);remote sensing","multimodal remote sensing image;deep fully convolutional neural network;multimodal image registration;fundamental technique;scene analysis;series remote sensing images;highly nonlinear radiometric relationship;deep neural network;multimodal image patch matching;Siamese fully convolutional network;feature distance;positive samples;hard negative samples;Siamese network;convolutional operation;input image patches;similarity score value;deep feature;multimodal RS image registration;candidate feature point generation;map images;spectrum region;modal types","","","54","Traditional","","","","IEEE","IEEE Journals"
"A Framework for Learning Depth From a Flexible Subset of Dense and Sparse Light Field Views","J. Shi; X. Jiang; C. Guillemot","INRIA (Institut National de Recherche en Informatique et en Automatique) Rennes Bretagne Atlantique, Rennes, France; INRIA (Institut National de Recherche en Informatique et en Automatique) Rennes Bretagne Atlantique, Rennes, France; INRIA (Institut National de Recherche en Informatique et en Automatique) Rennes Bretagne Atlantique, Rennes, France","IEEE Transactions on Image Processing","","2019","28","12","5867","5880","In this paper, we propose a learning-based depth estimation framework suitable for both densely and sparsely sampled light fields. The proposed framework consists of three processing steps: initial depth estimation, fusion with occlusion handling, and refinement. The estimation can be performed from a flexible subset of input views. The fusion of initial disparity estimates, relying on two warping error measures, allows us to have an accurate estimation in occluded regions and along the contours. In contrast with methods relying on the computation of cost volumes, the proposed approach does not need any prior information on the disparity range. Experimental results show that the proposed method outperforms state-of-the-art light fields depth estimation methods, including prior methods based on deep neural architectures.","","","10.1109/TIP.2019.2923323","H2020 European Research Council; (ERC advanced grant CLIM); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8743559","Light fields;depth estimation;deep neural network;occlusion handling","Estimation;Optical imaging;Computer architecture;Three-dimensional displays;Neural networks;Optical variables measurement;Correlation","estimation theory;image fusion;image resolution;learning (artificial intelligence);neural nets;stereo image processing","dense light field views;sparse light field views;learning-based depth estimation framework;densely sampled light fields;sparsely sampled light fields;initial depth estimation;occlusion handling;input views;initial disparity estimates;warping error measures;occluded regions;deep neural architectures","","","43","","","","","IEEE","IEEE Journals"
"Dynamic TCP Initial Windows and Congestion Control Schemes Through Reinforcement Learning","X. Nie; Y. Zhao; Z. Li; G. Chen; K. Sui; J. Zhang; Z. Ye; D. Pei","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Hunan University, Changsha, China; Microsoft Research Asia, Beijing, China; Baidu, Inc., Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Journal on Selected Areas in Communications","","2019","37","6","1231","1247","Despite many years of improvements to it, TCP still suffers from an unsatisfactory performance. For services dominated by short flows (e.g., web search and e-commerce), TCP suffers from the flow startup problem and cannot fully utilize the available bandwidth in the modern Internet: TCP starts from a conservative and static initial window (IW, 2-4 or 10), while most of the web flows are too short to converge to the best sending rate before the session ends. For services dominated by long flows (e.g., video streaming and file downloading), the congestion control (CC) scheme manually and statically configured might not offer the best performance for the latest network conditions. To address these two challenges, we propose TCP-RL, which uses reinforcement learning (RL) techniques to dynamically configure IW and CC in order to improve the performance of TCP flow transmission. Basing on the latest network conditions observed at the server side of a web service, TCP-RL dynamically configures a suitable IW for short flows through group-based RL, and dynamically configures a suitable CC scheme for long flows through deep RL. Our extensive experiments show that for short flows, TCP-RL can reduce the average transmission time by about 23%; and for long flows, compared with the performance of 14 CC schemes, TCP-RL's performance ranks top 5 for about 85% of the 288 given static network conditions, whereas for about 90% of conditions, its performance drops by less than 12% compared with that of the best-performing CC schemes for the same network conditions.","","","10.1109/JSAC.2019.2904350","National Natural Science Foundation of China; Beijing National Research Center for Information Science and Technology (BNRist) Key Projects; Global Talent Recruitment (Youth) Program; Okawa Foundation Research Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668690","TCP initial window;congestion control;web service;reinforcement learning","Servers;Web services;Reinforcement learning;Microsoft Windows;Bandwidth;Data communication","Internet;learning (artificial intelligence);telecommunication congestion control;transport protocols;video streaming;Web services","dynamic TCP initial windows;congestion control scheme;web search;flow startup problem;conservative window;static initial window;web flows;reinforcement learning techniques;TCP flow transmission;web service;CC scheme;static network conditions;TCP-RL performance","","2","46","","","","","IEEE","IEEE Journals"
"Automatic Audio Chord Recognition With MIDI-Trained Deep Feature and BLSTM-CRF Sequence Decoding Model","Y. Wu; W. Li","School of Computer Science and Technology, Fudan University, Shanghai, China; Shanghai Key Laboratory of Intelligent Information Processing, and the School of Computer Science and Technology, Fudan University, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","2","355","366","With the advances of machine learning technologies, data-driven feature extraction and sequence modeling approaches are being widely explored for automatic chord recognition tasks. Currently, there is a bottleneck in the amount of enough annotated data for training robust acoustic models, as hand-annotating time-synchronized chord labels requires professional musical skills and considerable labor. To cope with this limitation, in this paper, we propose a convolutional neural network (CNN) based deep feature extractor, which is trained on a large set of time, synchronized musical instrument digital interface audio data pairs and can robustly estimate pitch class activations of real-world music audio recordings. The CNN feature extractor plus a bidirectional long short-term memory conditional random field decoding model forms the proposed hybrid system for automatic chord recognition. Experiments show that the proposed model is compatible for both regular major/minor triad chord classification and larger vocabulary chord recognition, and outperforms other state-of-the-art chord recognition systems.","","","10.1109/TASLP.2018.2879399","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8523662","Automatic chord recognition;bidirectional long short-term memory (BLSTM);conditional random fields (CRF)","Feature extraction;Hidden Markov models;Music;Decoding;Training;Harmonic analysis","acoustic signal processing;audio signal processing;convolution;feature extraction;feedforward neural nets;learning (artificial intelligence);music;random processes;recurrent neural nets;signal classification","automatic audio chord recognition;MIDI-trained deep feature;machine learning technologies;data-driven feature extraction;automatic chord recognition tasks;hand-annotating time-synchronized chord labels;convolutional neural network;deep feature extractor;real-world music audio recordings;CNN feature extractor;musical instrument digital interface;chord recognition systems;BLSTM-CRF sequence decoding model;chord classification;bidirectional long short-term memory conditional random field","","1","46","","","","","IEEE","IEEE Journals"
"Deep activity recognition on imaging sensor data","F. Setiawan; B. N. Yahya; S. Lee","Hankuk University of Foreign Studies, Republic of Korea; Hankuk University of Foreign Studies, Republic of Korea; Hankuk University of Foreign Studies, Republic of Korea","Electronics Letters","","2019","55","17","928","931","Inspired by the recent success of deep learning (DL) approaches in computer vision domain, this Letter proposes a framework to encode the sensor data into an image representation for the activity recognition task. The signal from sensors is encoded based on the Gramian Angular Field. The encoding technique increases the dimension of the data, captures a local temporal relationship in terms of temporal correlation between time intervals on the geometric interpretation, and can be easily applied to the pre-trained DL architecture. The proposed framework is examined with respect to six popular sensor-based activity recognition datasets. Using the authors' framework, the results show that their approach outperforms most of the state-of-the-art approaches.","","","10.1049/el.2019.0906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805554","","","computer vision;image motion analysis;image recognition;image representation;image sensors;learning (artificial intelligence)","computer vision domain;deep learning;recent success;deep activity recognition;state-of-the-art approaches;authors;popular sensor-based activity recognition datasets;temporal correlation;local temporal relationship;encoding technique;Gramian Angular Field;activity recognition task;image representation;sensor data","","","18","","","","","IET","IET Journals"
"DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation","G. Wang; M. A. Zuluaga; W. Li; R. Pratt; P. A. Patel; M. Aertsen; T. Doel; A. L. David; J. Deprest; S. Ourselin; T. Vercauteren","Translational Imaging Group, Wellcome EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, United Kingdom; Translational Imaging Group, Wellcome EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, United Kingdom; Translational Imaging Group, Wellcome EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, United Kingdom; Translational Imaging Group, Wellcome EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, United Kingdom; Translational Imaging Group, Wellcome EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, United Kingdom; Department of Radiology, University Hospitals KU Leuven, Leuven, Belgium; Translational Imaging Group, Wellcome EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, United Kingdom; Institute for Women's Health, University College London, London, United Kingdom; Department of Obstetrics, University Hospitals KU Leuven, Leuven, Belgium; Translational Imaging Group, Wellcome EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, United Kingdom; Translational Imaging Group, Wellcome EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, United Kingdom","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","7","1559","1572","Accurate medical image segmentation is essential for diagnosis, surgical planning and many other applications. Convolutional Neural Networks (CNNs) have become the state-of-the-art automatic segmentation methods. However, fully automatic results may still need to be refined to become accurate and robust enough for clinical use. We propose a deep learning-based interactive segmentation method to improve the results obtained by an automatic CNN and to reduce user interactions during refinement for higher accuracy. We use one CNN to obtain an initial automatic segmentation, on which user interactions are added to indicate mis-segmentations. Another CNN takes as input the user interactions with the initial segmentation and gives a refined result. We propose to combine user interactions with CNNs through geodesic distance transforms, and propose a resolution-preserving network that gives a better dense prediction. In addition, we integrate user interactions as hard constraints into a back-propagatable Conditional Random Field. We validated the proposed framework in the context of 2D placenta segmentation from fetal MRI and 3D brain tumor segmentation from FLAIR images. Experimental results show our method achieves a large improvement from automatic CNNs, and obtains comparable and even higher accuracy with fewer user interventions and less time compared with traditional interactive methods.","","","10.1109/TPAMI.2018.2840695","Wellcome Trust; Engineering and Physical Sciences Research Council; Wellcome/EPSRC; National Institute for Health Research; Royal Society; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370732","Interactive image segmentation;convolutional neural network;geodesic distance;conditional random fields","Image segmentation;Biomedical imaging;Three-dimensional displays;Image resolution;Two dimensional displays;Convolution;Feature extraction","biomedical MRI;brain;image segmentation;learning (artificial intelligence);medical image processing;neural nets;tumours","deep interactive geodesic framework;deep learning-based interactive segmentation method;automatic CNN;initial automatic segmentation;2D placenta segmentation;3D brain tumor segmentation;medical image segmentation;automatic segmentation methods;convolutional neural networks;FLAIR images","","6","65","CCBY","","","","IEEE","IEEE Journals"
"Image colourisation using deep feature-guided image retrieval","S. Chakraborty","Stony Brook University, USA","IET Image Processing","","2019","13","7","1130","1137","In this study, the authors aim to colourise a greyscale image using a fully automated framework which retrieves similar images from a reference database and then transfers the colour from the most similar retrieved images to perform colourisation. Inspired by the recent success of deep learning techniques in extracting semantic information from images, they first use fc7 features from AlexNet to retrieve similar images from the reference database. Top-k retrieved images are considered for colour transfer to the target greyscale image, using various pixel level features. The images which result from the previous step are given a colour enhancement with Reinhard stain normalisation. They follow a pixel-wise colour saturation based averaging technique to impart colour at pixel level. The final image is rectified using joint bilateral filtering. The resulting coloured images have a realistic appearance, similar in quality to the original coloured images. The proposed method outperforms several previous colourisation techniques, yielding superior performance both quantitatively and qualitatively. The method also enhances low-contrast images.","","","10.1049/iet-ipr.2018.6169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733317","","","filtering theory;image colour analysis;image enhancement;image resolution;image retrieval;learning (artificial intelligence)","image colourisation;deep feature-guided image retrieval;fully automated framework;reference database;similar retrieved images;deep learning techniques;colour transfer;target greyscale image;pixel level features;colour enhancement;pixel-wise colour saturation;original coloured images;low-contrast images;Reinhard stain normalisation;averaging technique;joint bilateral filtering","","","38","","","","","IET","IET Journals"
"SDCA: a novel stack deep convolutional autoencoder – an application on retinal image denoising","S. K. Ghosh; B. Biswas; A. Ghosh","Department of Computer Science and Engineering, Maulana Abul Kalam Azad University of Technology, Kolkata, India; Department of Computer Science and Engineering, University of Calcutta, Kolkata, India; Department of Computer Science and Engineering, Netaji Subhash Engineering College, Kolkata, India","IET Image Processing","","2019","13","14","2778","2789","Retinal fundus images are used for the diagnosis and treatment of various eye diseases such as diabetic retinopathy, glaucoma, exudates and so on. The retinal vasculature is difficult to investigate retinal conditions due to the presence of various noises in the retinal image during the capture of the image. Removal of noise is an important aspect for better visibility and diagnosis of the noisy fundus in ophthalmology. This study represents a deep learning based approach to denoising images and restoring features using stack denoising convolutional autoencoder. The proposed scheme is implemented to restore the structural details of fundus as well as to decrease the noise level. Furthermore, the proposed model utilises shared layers with the optimal manner to reduce the noise level of the target image with minimal computational cost. To restore an image, the proposed model brings a patched base training on samples to suppress with one to one manner without any loss of information. To access the denoising effect of the proposed scheme, several standard fundus databases such as DRIVE, STARE and DIARETDB1 have been tested in this study. Comparing the efficiency of the suggested model with state-of-art methods, the proposed scheme gives better result in terms of qualitative and quantitative analysis.","","","10.1049/iet-ipr.2018.6582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946942","","","diseases;neural nets;blood vessels;medical image processing;learning (artificial intelligence);biomedical optical imaging;image denoising;convolution;eye;image segmentation;image representation","novel stack deep convolutional autoencoder;retinal image denoising;retinal fundus images;eye diseases;diabetic retinopathy;retinal vasculature;retinal conditions;visibility;noisy fundus;deep learning;denoising images;restoring features;noise level;target image;patched base training;denoising effect;standard fundus databases","","","37","","","","","IET","IET Journals"
"Infrared Aerothermal Nonuniform Correction via Deep Multiscale Residual Network","Y. Chang; L. Yan; L. Liu; H. Fang; S. Zhong","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Space Science and Technology, Xidian University, Xi’an, China; School of the Software, Xidian University, Xi’an, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","7","1120","1124","In the infrared focal plane arrays imaging systems, the temperature-dependent nonuniformity effects severely degrade the image quality. In this letter, we propose a very deep convolutional neural network for unified infrared aerothermal nonuniform correction. Our network is built with the multiscale and residual training. The multiscale subnetworks utilize the multiscale property in the images, and the long-short-term residual learning contributes to the information propagation. Compared with the previous methods, the proposed method is more robust to various nonuniform artifacts and more efficient at processing time. Experimental results validate the superiority of our method for infrared nonuniform correction.","","","10.1109/LGRS.2019.2893519","National Natural Science Foundation of China; Natural Science Foundation of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8637823","Convolutional neural network (CNN);infrared image;nonuniform correction","Feature extraction;Image reconstruction;Degradation;Training;Image edge detection;Convolutional neural networks;Detectors","computer vision;convolutional neural nets;focal planes;learning (artificial intelligence)","deep multiscale residual network;temperature-dependent nonuniformity effects;image quality;deep convolutional neural network;unified infrared aerothermal nonuniform correction;multiscale training;residual training;multiscale subnetworks;multiscale property;nonuniform artifacts;infrared focal plane arrays imaging systems;long-short-term residual learning;information propagation","","2","20","","","","","IEEE","IEEE Journals"
"Accurate single image super-resolution using cascading dense connections","W. Wei; G. Feng; Q. Zhang; D. Cui; M. Zhang; F. Chen","School of Computer Science & Engineering, Northeastern University, People's Republic of China; School of Business Administration, Northeastern University, People's Republic of China; Signal and Communication Research Institute, China Academy of Railway Sciences, People's Republic of China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, People's Republic of China; Signal and Communication Research Institute, China Academy of Railway Sciences, People's Republic of China; Signal and Communication Research Institute, China Academy of Railway Sciences, People's Republic of China","Electronics Letters","","2019","55","13","739","742","In recent years, deep learning methods, especially deep convolutional neural network, have been successfully applied to the single-image super-resolution (SISR) task. In this Letter, the authors propose an accurate SISR method by introducing cascading dense connections in a very deep network. In detail, they construct the cascading dense network (CDN) to fully make use of the hierarchical features from all the convolutional layers, which implements a cascading mechanism upon the dense connected convolutional layers. The cascading dense connection in the CDN enables short and long paths to be built directly from the output to each layer, alleviating the vanishing-gradient problem of very deep networks. Extensive experiments show that CDN achieves state-of-the-art performance on traditional metrics (PSNR and SSIM). Also, they introduce object recognition as the additional evaluation metric for SISR, which further demonstrates the effectiveness of the authors' method.","","","10.1049/el.2019.0392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8745460","","","convolution;image resolution;learning (artificial intelligence);neural nets;object recognition","dense connected convolutional layers;cascading mechanism;cascading dense network;accurate SISR method;single-image super-resolution task;deep convolutional neural network;deep learning methods;accurate single image super-resolution;deep network;CDN;cascading dense connection","","","","","","","","IET","IET Journals"
"Hypergraph-Induced Convolutional Networks for Visual Classification","H. Shi; Y. Zhang; Z. Zhang; N. Ma; X. Zhao; Y. Gao; J. Sun","Key Laboratory for Information System Security, Ministry of Education, Beijing National Research Center for Information Science and Technology, School of Software, Tsinghua University, Beijing, China; Key Laboratory for Information System Security, Ministry of Education, Beijing National Research Center for Information Science and Technology, School of Software, Tsinghua University, Beijing, China; Key Laboratory for Information System Security, Ministry of Education, Beijing National Research Center for Information Science and Technology, School of Software, Tsinghua University, Beijing, China; Robotics Institute, Beijing Union University, Beijing, China; Key Laboratory for Information System Security, Ministry of Education, Beijing National Research Center for Information Science and Technology, School of Software, Tsinghua University, Beijing, China; Key Laboratory for Information System Security, Ministry of Education, Beijing National Research Center for Information Science and Technology, School of Software, Tsinghua University, Beijing, China; Key Laboratory for Information System Security, Ministry of Education, Beijing National Research Center for Information Science and Technology, School of Software, Tsinghua University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","10","2963","2972","At present, convolutional neural networks (CNNs) have become popular in visual classification tasks because of their superior performance. However, CNN-based methods do not consider the correlation of visual data to be classified. Recently, graph convolutional networks (GCNs) have mitigated this problem by modeling the pairwise relationship in visual data. Real-world tasks of visual classification typically must address numerous complex relationships in the data, which are not fit for the modeling of the graph structure using GCNs. Therefore, it is vital to explore the underlying correlation of visual data. Regarding this issue, we propose a framework called the hypergraph-induced convolutional network to explore the high-order correlation in visual data during deep neural networks. First, a hypergraph structure is constructed to formulate the relationship in visual data. Then, the high-order correlation is optimized by a learning process based on the constructed hypergraph. The classification tasks are performed by considering the high-order correlation in the data. Thus, the convolution of the hypergraph-induced convolutional network is based on the corresponding high-order relationship, and the optimization on the network uses each data and considers the high-order correlation of the data. To evaluate the proposed hypergraph-induced convolutional network framework, we have conducted experiments on three visual data sets: the National Taiwan University 3-D model data set, Princeton Shape Benchmark, and multiview RGB-depth object data set. The experimental results and comparison in all data sets demonstrate the effectiveness of our proposed hypergraph-induced convolutional network compared with the state-of-the-art methods.","","","10.1109/TNNLS.2018.2869747","National Key R&D Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478794","Convolutional networks;high-order correlation;hypergraph theory;visual classification","Visualization;Task analysis;Correlation;Convolution;Data models;Neural networks;Learning systems","convolutional neural nets;graph theory;image classification;learning (artificial intelligence);solid modelling","visual data sets;National Taiwan University 3-D model data;convolutional neural networks;visual classification tasks;high-order correlation;deep neural networks;hypergraph-induced convolutional network framework;GCNs;graph convolutional networks;learning process","","1","47","","","","","IEEE","IEEE Journals"
"Towards End-to-End ECG Classification With Raw Signal Extraction and Deep Neural Networks","S. S. Xu; M. Mak; C. Cheung","Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong; Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong; Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong","IEEE Journal of Biomedical and Health Informatics","","2019","23","4","1574","1584","This paper proposes deep learning methods with signal alignment that facilitate the end-to-end classification of raw electrocardiogram (ECG) signals into heartbeat types, i.e., normal beat or different types of arrhythmias. Time-domain sample points are extracted from raw ECG signals, and consecutive vectors are extracted from a sliding time-window covering these sample points. Each of these vectors comprises the consecutive sample points of a complete heartbeat cycle, which includes not only the QRS complex but also the P and T waves. Unlike existing heartbeat classification methods in which medical doctors extract handcrafted features from raw ECG signals, the proposed end-to-end method leverages a deep neural network for both feature extraction and classification based on aligned heartbeats. This strategy not only obviates the need to handcraft the features but also produces optimized ECG representation for heartbeat classification. Evaluations on the MIT-BIH arrhythmia database show that at the same specificity, the proposed patient-independent classifier can detect supraventricular- and ventricular-ectopic beats at a sensitivity that is at least 10% higher than current state-of-the-art methods. More importantly, there is a wide range of operating points in which both the sensitivity and specificity of the proposed classifier are higher than those achieved by state-of-the-art classifiers. The proposed classifier can also perform comparable to patient-specific classifiers, but at the same time enjoys the advantage of patient independence.","","","10.1109/JBHI.2018.2871510","RGC of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8468972","ECG classification;arrhythmia classification;end-to-end;deep neural networks;heartbeat alignment","Electrocardiography;Feature extraction;Heart beat;Heart rate variability;Neural networks;Databases;Standards","electrocardiography;feature extraction;learning (artificial intelligence);medical signal detection;medical signal processing;neural nets;signal classification","complete heartbeat cycle;consecutive sample points;sliding time-window;consecutive vectors;time-domain sample points;heartbeat types;raw electrocardiogram;end-to-end classification;signal alignment;raw signal extraction;towards end-to-end ECG classification;MIT-BIH arrhythmia database show;optimized ECG representation;aligned heartbeats;feature extraction;deep neural network;end-to-end method leverages;raw ECG signals;handcrafted features;heartbeat classification methods;P;BIH","","5","38","","","","","IEEE","IEEE Journals"
"Statistical Parametric Speech Synthesis Using Deep Gaussian Processes","T. Koriyama; T. Kobayashi","School of Engineering, Tokyo Institute of Technology, Yokohama, Japan; School of Engineering, Tokyo Institute of Technology, Yokohama, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","5","948","959","This paper proposes a framework of speech synthesis based on deep Gaussian processes (DGPs), which is a deep architecture model composed of stacked Bayesian kernel regressions. In this method, we train a statistical model of transformation from contextual features to speech parameters in a similar manner to deep neural network (DNN)-based speech synthesis. To apply DGPs to a statistical parametric speech synthesis framework, our framework uses an approximation method, doubly stochastic variational inference, which is suitable for an arbitrary amount of data. Since the training of DGPs is based on the marginal likelihood that takes into account not only data fitting, but also model complexity, DGPs are less vulnerable to overfitting compared with DNNs. In experimental evaluations, we investigated a performance comparison of the proposed DGP-based framework with a feedforward DNN-based one. Subjective and objective evaluation results showed that our DGP framework yielded a higher mean opinion score and lower acoustic feature distortions than the conventional framework.","","","10.1109/TASLP.2019.2905167","JSPS Grants-in-Aid for Scientific Research (KAKENHI); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667345","Statistical parametric speech synthesis;Gaussian process;stochastic variational inference;Bayesian model","Global Positioning System;Kernel;Speech synthesis;Training data;Hidden Markov models;Neural networks;Gaussian processes","approximation theory;Bayes methods;Gaussian processes;learning (artificial intelligence);neural nets;regression analysis;speech synthesis","DGP framework;conventional framework;deep Gaussian processes;deep architecture model;stacked Bayesian kernel regressions;contextual features;speech parameters;deep neural network-based speech synthesis;statistical parametric speech synthesis framework;approximation method;doubly stochastic variational inference;model complexity;feedforward DNN-based method","","","38","","","","","IEEE","IEEE Journals"
"User-Ranking Video Summarization With Multi-Stage Spatio–Temporal Representation","S. Huang; X. Li; Z. Zhang; F. Wu; J. Han","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Automation, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Image Processing","","2019","28","6","2654","2664","Video summarization is a challenging task, mainly due to the difficulties in learning complicated semantic structural relations between videos and summaries. In this paper, we present a novel supervised video summarization scheme based on three-stage deep neural networks. The scheme takes a divide-and-conquer strategy to resolve the complicated task of 3D video summarization into a set of easy and flexible computational subtasks, and then to sequentially perform 2D CNNs, 1D CNNs, and long short-term memory to address the subtasks in an hierarchical fashion. The hierarchical modeling of spatio-temporal structure leads to high performance and efficiency. In addition, we propose a simple but effective user-ranking method to cope with the labeling subjectivity problem of user-created video summarization, leading to the labeling quality refinement for robust supervised learning. Experimental results show that our approach outperforms the state-of-the-art video summarization methods on two benchmark datasets.","","","10.1109/TIP.2018.2889265","National Natural Science Foundation of China; Zhejiang Lab; Fundamental Research Funds for Central Universities in China; Key Program of Zhejiang Province, China; ZJU Converging Media Computing Lab; Natural Science Foundation of Zhejiang Province; National Basic Research Program of China; Zhejiang University; Funding from HIKVision; Artificial Intelligence Research Foundation of Baidu, Inc.; Tencent AI Lab Rhino-Bird Joint Research Program JR201806; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8585041","Video summarization;recurrent neural network;convolutional neural network;multi-user inconsistency;user ranking","Two dimensional displays;Context modeling;Three-dimensional displays;Computational modeling;Supervised learning;Convolution;Task analysis","convolutional neural nets;divide and conquer methods;image representation;learning (artificial intelligence);recurrent neural nets;video signal processing","user-ranking video summarization;multistage spatio-temporal representation;novel supervised video summarization scheme;deep neural networks;flexible computational subtasks;2D CNNs;1D CNNs;spatio-temporal structure;user-created video summarization;robust supervised learning;divide-and-conquer strategy;video summarization methods;complicated semantic structural relation learning;three-stage deep neural networks;long short-term memory;hierarchical modeling;effective user-ranking method;labeling quality refinement;benchmark datasets;time 1.0 d","","1","45","","","","","IEEE","IEEE Journals"
"Visual Attention-Driven Hyperspectral Image Classification","J. M. Haut; M. E. Paoletti; J. Plaza; A. Plaza; J. Li","Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Guangdong Provincial Key Laboratory of Urbanization and Geosimulation, Center of Integrated Geographic Information Analysis, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","10","8065","8080","Deep neural networks (DNNs), including convolutional neural networks (CNNs) and residual networks (ResNets) models, are able to learn abstract representations from the input data by considering a deep hierarchy of layers that perform advanced feature extraction. The combination of these models with visual attention techniques can assist with the identification of the most representative parts of the data from a visual standpoint, obtained through more detailed filtering of the features extracted by the operational layers of the network. This is of significant interest for analyzing remotely sensed hyperspectral images (HSIs), characterized by their very high spectral dimensionality. However, few efforts have been conducted in the literature in order to adapt visual attention methods to remotely sensed HSI data analysis. In this paper, we introduce a new visual attention-driven technique for the HSI classification. Specifically, we incorporate attention mechanisms to a ResNet in order to better characterize the spectral-spatial information contained in the data. Our newly proposed method calculates a mask that is applied to the features obtained by the network in order to identify the most desirable ones for classification purposes. Our experiments, conducted using four widely used HSI data sets, reveal that the proposed deep attention model provides competitive advantages in terms of classification accuracy when compared to other state-of-the-art methods.","","","10.1109/TGRS.2019.2918080","Ministerio de Educación (Resolución de 26 de diciembre de 2014 y de 19 de noviembre de 2015), de la Secretaría de Estado de Educación, Formación Profesional y Universidades, por la que se convocan ayudas para la formación de profesorado universitario, de los subprogramas de Formación y de Movilidad incluidos en el Programa Estatal de Promoción del Talento y su Empleabilidad, en el marco del Plan Estatal de Investigación Científica y Técnica y de Innovación 2013–2016; Consejería de Educación y Empleo, Junta de Extremadura; European Union’s Horizon 2020 Research And Innovation Programme; National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; Natural Science Foundation of Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736024","Deep learning (DL);feature extraction;hyperspectral image (HSI) classification;residual neural networks;visual attention","Visualization;Feature extraction;Data models;Data mining;Remote sensing;Neural networks;Earth","convolutional neural nets;data analysis;feature extraction;hyperspectral imaging;image classification;image representation;learning (artificial intelligence);remote sensing","deep neural networks;convolutional neural networks;residual networks models;ResNet;abstract representations;feature extraction;visual attention techniques;visual standpoint;remotely sensed hyperspectral images;high spectral dimensionality;visual attention methods;HSI data analysis;visual attention-driven technique;HSI classification;spectral-spatial information;deep attention model;hyperspectral image classification;HSI data sets","","1","94","","","","","IEEE","IEEE Journals"
"Multi-Task Regression-Based Learning for Autonomous Unmanned Aerial Vehicle Flight Control Within Unstructured Outdoor Environments","B. G. Maciel-Pearson; S. Akçay; A. Atapour-Abarghouei; C. Holder; T. P. Breckon","Department of Computer Science, Durham University, Durham, U.K.; Department of Computer Science, Durham University, Durham, U.K.; Department of Computer Science, Durham University, Durham, U.K.; Department of Computer Science, Durham University, Durham, U.K.; Department of Computer Science, Durham University, Durham, U.K.","IEEE Robotics and Automation Letters","","2019","4","4","4116","4123","Increased growth in the global unmanned aerial vehicles (UAV) (drone) industry has expanded possibilities for fully autonomous UAV applications. A particular application which has in part motivated this research is the use of UAV in wide area search and surveillance operations in unstructured outdoor environments. The critical issue with such environments is the lack of structured features that could aid in autonomous flight, such as road lines or paths. In this letter, we propose an end-to-end multi-task regression-based learning approach capable of defining flight commands for navigation and exploration under the forest canopy, regardless of the presence of trails or additional sensors (i.e., GPS). Training and testing are performed using a software in the loop pipeline, which allows for a detailed evaluation against state-of-the-art pose estimation techniques. Our extensive experiments demonstrate that our approach excels in performing dense exploration within the required search perimeter, is capable of covering wider search regions, generalises to previously unseen and unexplored environments and outperforms contemporary state-ofthe-art techniques.","","","10.1109/LRA.2019.2930496","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8771214","Aerial systems: perception and autonomy;autonomous vehicle navigation;computer vision for other robotic applications;deep learning in robotics and automation","Global Positioning System;Quaternions;Forestry;Unmanned aerial vehicles;Cameras;Collision avoidance","aerospace control;autonomous aerial vehicles;learning (artificial intelligence);path planning;regression analysis;rescue robots;robot vision","autonomous unmanned aerial vehicle flight control;unstructured outdoor environments;global unmanned aerial vehicles industry;fully autonomous UAV applications;wide area search;surveillance operations;road lines;flight commands;end-to-end multitask regression-based learning;forest canopy;search perimeter","","","38","Traditional","","","","IEEE","IEEE Journals"
"On Low-Resolution Face Recognition in the Wild: Comparisons and New Techniques","P. Li; L. Prieto; D. Mery; P. J. Flynn","Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Computer Science, Pontificia Universidad Católica de Chile, Santiago, Chile; Department of Computer Science, Pontificia Universidad Católica de Chile, Santiago, Chile; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA","IEEE Transactions on Information Forensics and Security","","2019","14","8","2000","2012","Although face recognition systems have achieved impressive performance in recent years, the low-resolution face recognition task remains challenging, especially when the low-resolution faces are captured under non-ideal conditions, which is widely prevalent in surveillance-based applications. Faces captured in such conditions are often contaminated by blur, non-uniform lighting, and non-frontal face pose. In this paper, we analyze the face recognition techniques using data captured under low-quality conditions in the wild. We provide a comprehensive analysis of the experimental results for two of the most important applications in real surveillance applications, and demonstrate practical approaches to handle both cases that show promising performance. The following three contributions are made: (i) we conduct experiments to evaluate super-resolution methods for low-resolution face recognition; (ii) we study face re-identification on various public face datasets, including real surveillance and low-resolution subsets of large-scale datasets, presenting a baseline result for several deep learning-based approaches, and improve them by introducing a generative adversarial network pre-training approach and fully convolutional architecture; and (iii) we explore the low-resolution face identification by employing a state-of-the-art supervised discriminative learning approach. The evaluations are conducted on challenging portions of the SCface and UCCSface datasets.","","","10.1109/TIFS.2018.2890812","grant Chile-CORFO Engineering 2030 under Seed Fund Program between University of Notre Dame and Catholic University of Chile; Fondo Nacional de Desarrollo Científico y Tecnológico; University of Notre Dame; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600370","Face recognition;image resolution;video surveillance","Face;Face recognition;Surveillance;Image resolution;Task analysis;Deep learning;Feature extraction","face recognition;image resolution;learning (artificial intelligence)","face re-identification;public face datasets;low-resolution subsets;low-resolution face identification;face recognition systems;low-resolution face recognition task;nonideal conditions;surveillance-based applications;nonfrontal face;face recognition techniques;low-quality conditions;super-resolution methods;deep learning-based approach;generative adversarial network pretraining approach;SCface datasets;UCCSface datasets","","2","92","","","","","IEEE","IEEE Journals"
"A Novel Octree-Based 3-D Fully Convolutional Neural Network for Point Cloud Classification in Road Environment","B. Xiang; J. Tu; J. Yao; L. Li","Computer Vision and Remote Sensing (CVRS) Laboratory, School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Computer Vision and Remote Sensing (CVRS) Laboratory, School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Computer Vision and Remote Sensing (CVRS) Laboratory, School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Computer Vision and Remote Sensing (CVRS) Laboratory, School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","10","7799","7818","The automatic classification of 3-D point clouds is publicly known as a challenging task in a complex road environment. Specifically, each point is automatically classified into a unique category label, and then, the labels are used as clues for semantic analysis and scene recognition. Instead of heuristically extracting handcrafted features in traditional methods to classify all points, we put forward an end-to-end octree-based fully convolutional network (FCN) to classify 3-D point clouds in an urban road environment. There are four contributions in this paper. The first is that the integration and comprehensive uses of OctNet and FCN greatly decrease the computing time and memory demands compared with a dense 3-D convolutional neural network (CNN). The second is that the octree-based network is strengthened by means of modifying the cross-entropy loss function to solve the problems of an unbalanced category distribution. The third is that an Inception-ResNet block is united with our network, which enables our 3-D CNN to effectively learn how to classify scenes containing objects at multiple scales and improve classification accuracy. The last is that an open source data set (HuangshiRoad data set) with ten different classes is introduced for 3-D point cloud classification. Three representative data sets [Semantic3D, WHU_MLS (blocks I and II), and HuangshiRoad] with different covered areas and numbers of points and classes are selected to evaluate our proposed method. The experimental results show that the overall classification accuracy is appreciable, with 89.4% for Semantic3D, 82.9% for WHU_MLS block I, 91.4% for WHU_MLS block II, and 94% for HuangshiRoad. Our deep learning approach can efficiently classify 3-D dense point clouds in an urban road environment measured by a mobile laser scanning (MLS) system or static LiDAR.","","","10.1109/TGRS.2019.2916625","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733078","Fully convolutional network (FCN);octree-based;point cloud classification;weighted loss","Three-dimensional displays;Feature extraction;Roads;Semantics;Convolution;Deep learning;Memory management","computer graphics;convolutional neural nets;entropy;geophysical image processing;image classification;learning (artificial intelligence);octrees;optical radar;remote sensing","unbalanced category distribution;classification accuracy;open source data set;HuangshiRoad data set;point cloud classification;representative data sets;WHU_MLS block II;3-D dense point clouds;urban road environment;automatic classification;complex road environment;semantic analysis;scene recognition;fully convolutional network;FCN;3-D convolutional neural network;category label;octree-based 3-D fully convolutional neural network;OctNet;cross-entropy loss function;Inception-ResNet block;3-D CNN;deep learning approach;mobile laser scanning;system LiDAR;static LiDAR","","1","76","","","","","IEEE","IEEE Journals"
"DeepDetect: A Cascaded Region-Based Densely Connected Network for Seismic Event Detection","Y. Wu; Y. Lin; Z. Zhou; D. C. Bolton; J. Liu; P. Johnson","Earth and Environmental Sciences Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Earth and Environmental Sciences Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Earth and Environmental Sciences Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Department of Geosciences, Penn State University, University Park, PA, USA; Department of Computer Science, University of Rochester, Rochester, NY, USA; Earth and Environmental Sciences Division, Los Alamos National Laboratory, Los Alamos, NM, USA","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","1","62","75","Automatic event detection from time series signals has broad applications. Traditional detection methods detect events primarily by the use of similarity and correlation in data. Those methods can be inefficient and yield low accuracy. In recent years, machine learning techniques have revolutionized many sciences and engineering domains. In particular, the performance of object detection in a 2-D image data has significantly improved due to deep neural networks. In this paper, we develop a deep-learning-based detection method, called “DeepDetect,” to detect events from seismic signals. We find that the direct adaptation of similar ideas from 2-D object detection to our problem faces two challenges. The first challenge is that the duration of earthquake event varies significantly; the other is that the proposals generated are temporally correlated. To address these challenges, we propose a novel cascaded region-based convolutional neural network to capture earthquake events in different sizes while incorporating contextual information to enrich features for each proposal. To achieve a better generalization performance, we use densely connected blocks as the backbone of our network. Because some positive events are not correctly annotated, we further formulate the detection problem as a learning-from-noise problem. To verify the performance, we employ the seismic data generated from the Pennsylvania State University Rock and Sediment Mechanics Laboratory, and we acquire labels with the help of experts. We show that our techniques yield high accuracy. Therefore, our novel deep-learning-based detection methods can potentially be powerful tools for identifying events from the time series data in various applications.","","","10.1109/TGRS.2018.2852302","Los Alamos National Laboratory; Office of Fossil Energy; Institutional Support (LDRD) at Los Alamos; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424545","Convolutional neural network (CNN);event detection;seismic signals;time series segmentation","Proposals;Time series analysis;Event detection;Object detection;Correlation;Feature extraction;Earthquakes","civil engineering computing;convolution;earthquake engineering;feature extraction;feedforward neural nets;learning (artificial intelligence);object detection;time series","DeepDetect;cascaded region-based densely connected network;seismic event detection;automatic event detection;time series signals;machine learning techniques;engineering domains;object detection;deep neural networks;seismic signals;earthquake event;learning-from-noise problem;seismic data;deep-learning-based detection methods;time series data;2D image data;cascaded region-based convolutional neural network","","7","42","","","","","IEEE","IEEE Journals"
"Adversarial auto-encoder for unsupervised deep domain adaptation","R. Shao; X. Lan","Hong Kong Baptist University, Department of Computer Science, Kowloon 999077, Hong Kong; Hong Kong Baptist University, Department of Computer Science, Kowloon 999077, Hong Kong","IET Image Processing","","2019","13","14","2772","2777","Unsupervised visual domain adaptation aims to train a classifier that works well on a target domain given labelled source samples and unlabelled target samples. The key issue in unsupervised visual domain adaptation is how to do the feature alignment between source and target domains. Inspired by the adversarial learning in generative adversarial networks, this study proposes a novel adversarial auto-encoder for unsupervised deep domain adaptation. This method incorporates the auto-encoder with the adversarial learning so that the domain similarity and reconstruction information from the decoder can be exploited to facilitate the adversarial domain adaptation in the encoder. Extensive experiments on various visual recognition tasks show that the proposed method performs favourably against or better than competitive state-of-the-art methods.","","","10.1049/iet-ipr.2018.6687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946969","","","image classification;pattern classification;unsupervised learning;learning (artificial intelligence)","unsupervised deep domain adaptation;unsupervised visual domain adaptation;target domain;labelled source samples;unlabelled target samples;target domains;generative adversarial networks;novel adversarial auto-encoder;domain similarity;reconstruction information;adversarial domain adaptation","","","24","","","","","IET","IET Journals"
"Synthesis of Multispectral Optical Images From SAR/Optical Multitemporal Data Using Conditional Generative Adversarial Networks","J. D. Bermudez; P. N. Happ; R. Q. Feitosa; D. A. B. Oliveira","Department of Electrical Engineering, Pontifical University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical University of Rio de Janeiro, Rio de Janeiro, Brazil; IBM Research Brazil Lab, São Paulo, Brazil","IEEE Geoscience and Remote Sensing Letters","","2019","16","8","1220","1224","The synthesis of realistic data using deep learning techniques has greatly improved the performance of classifiers in handling incomplete data. Remote sensing applications that have profited from those techniques include translating images of different sensors, improving the image resolution and completing missing temporal or spatial data such as in cloudy optical images. In this context, this letter proposes a new deep-learning-based framework to synthesize missing or corrupted multispectral optical images using multimodal/multitemporal data. Specifically, we use conditional generative adversarial networks (cGANs) to generate the missing optical image by exploiting the correspondent synthetic aperture radar (SAR) data with a SAR-optical data from the same area at a different acquisition date. The proposed framework was evaluated in two land-cover applications over tropical regions, where cloud coverage is a major problem: crop recognition and wildfire detection. In both applications, our proposal was superior to alternative approaches tested in our experiments. In particular, our approach outperformed recent cGAN-based proposals for cloud removal, on average, by 7.7% and 8.6% in terms of overall accuracy and F1-score, respectively.","","","10.1109/LGRS.2019.2894734","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; Conselho Nacional de Desenvolvimento Científico e Tecnológico; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8637007","Conditional generative adversarial networks (cGANs);crop recognition;deep learning;remote sensing;wildfire detection","Optical imaging;Optical sensors;Clouds;Nonlinear optics;Adaptive optics;Gallium nitride;Generators","crops;geophysical image processing;image classification;image resolution;land cover;learning (artificial intelligence);neural nets;optical images;radar imaging;radar resolution;remote sensing by radar;synthetic aperture radar","multispectral optical images;conditional generative adversarial networks;deep learning techniques;remote sensing applications;image resolution;spatial data;cloudy optical images;land-cover applications;SAR-optical multitemporal data;multimodal-multitemporal data;SAR optical data;cGAN-based proposals;synthetic aperture radar data","","","15","","","","","IEEE","IEEE Journals"
"MonoCap: Monocular Human Motion Capture using a CNN Coupled with a Geometric Prior","X. Zhou; M. Zhu; G. Pavlakos; S. Leonardos; K. G. Derpanis; K. Daniilidis","Zhejiang University, Hangzhou, Zhejiang, China; Computer and Information Science Department and GRASP Laboratory, University of Pennsylvania, Philadelphia, PA; Computer and Information Science Department and GRASP Laboratory, University of Pennsylvania, Philadelphia, PA; Computer and Information Science Department and GRASP Laboratory, University of Pennsylvania, Philadelphia, PA; Department of Computer Science, Ryerson University, Canada; Computer and Information Science Department and GRASP Laboratory, University of Pennsylvania, Philadelphia, PA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","4","901","914","Recovering 3D full-body human pose is a challenging problem with many applications. It has been successfully addressed by motion capture systems with body worn markers and multiple cameras. In this paper, we address the more challenging case of not only using a single camera but also not leveraging markers: going directly from 2D appearance to 3D geometry. Deep learning approaches have shown remarkable abilities to discriminatively learn 2D appearance features. The missing piece is how to integrate 2D, 3D, and temporal information to recover 3D geometry and account for the uncertainties arising from the discriminative model. We introduce a novel approach that treats 2D joint locations as latent variables whose uncertainty distributions are given by a deep fully convolutional neural network. The unknown 3D poses are modeled by a sparse representation and the 3D parameter estimates are realized via an Expectation-Maximization algorithm, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Extensive evaluation on benchmark datasets shows that the proposed approach achieves greater accuracy over state-of-the-art baselines. Notably, the proposed approach does not require synchronized 2D-3D data for training and is applicable to “in-the-wild” images, which is demonstrated with the MPII dataset.","","","10.1109/TPAMI.2018.2816031","Canadian NSERC Discovery; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8316924","Motion capture;human pose;deep learning;sparse representation","Three-dimensional displays;Two dimensional displays;Solid modeling;Cameras;Image reconstruction;Uncertainty;Pose estimation","cameras;convolutional neural nets;expectation-maximisation algorithm;image capture;image motion analysis;image representation;learning (artificial intelligence);parameter estimation;pose estimation;stereo image processing","MonoCap;monocular human motion capture;CNN;motion capture systems;body worn markers;2D appearance features;temporal information;discriminative model;2D joint locations;sparse representation;3D parameter estimates;Expectation-Maximization algorithm;convolutional neural network;3D full-body human pose;deep learning;MPII dataset;in-the-wild images;geometric prior","","4","73","","","","","IEEE","IEEE Journals"
"Automatic Modulation Classification Using Contrastive Fully Convolutional Network","S. Huang; Y. Jiang; Y. Gao; Z. Feng; P. Zhang","Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; School of Electrical Engineering and Computer Science, Queen Mary University of London, London, U.K.; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Wireless Communications Letters","","2019","8","4","1044","1047","Automatic modulation classification (AMC) aims at identifying the modulation format of the received signal. In this letter, we propose a novel grid constellation matrix (GCM)-based AMC method using a contrastive fully convolutional network (CFCN). We use GCMs as the input of the network, which are extracted from the received signals using low-complexity preprocessing. Moreover, a loss function with contrastive loss is designed to train the CFCN, which boosts the discrepancies among different modulations and obtains discriminative representations. Extensive simulations demonstrate that CFCN performs superior classification performance and better robustness to model mismatches with low training time comparing with other recent methods.","","","10.1109/LWC.2019.2904956","National Natural Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666725","Automatic modulation classification;contrastive loss;deep learning;fully convolutional network","Modulation;Convolution;Kernel;Feature extraction;Training;Robustness;Deep learning","convolutional neural nets;learning (artificial intelligence);matrix algebra;modulation;signal classification;telecommunication computing","contrastive fully convolutional network;automatic modulation classification;CFCN;low-complexity preprocessing;grid constellation matrix","","2","11","","","","","IEEE","IEEE Journals"
"Power Control in Energy Harvesting Multiple Access System With Reinforcement Learning","M. Chu; X. Liao; H. Li; S. Cui","Department of Information and Communication Engineering, Xi’an Jiaotong University, Xi’an, China; Department of Information and Communication Engineering, Xi’an Jiaotong University, Xi’an, China; Shenzhen Research Institute of Big Data, Shenzhen, China; Shenzhen Research Institute of Big Data and the School of Science and Engineering, Chinese University of Hong Kong at Shenzhen, Shenzhen, China","IEEE Internet of Things Journal","","2019","6","5","9175","9186","The Internet of Things (IoT) application has a crucial need for long-term and self-sustainable operations. Energy harvesting (EH) technique has attracted great attention in IoT as it may significantly increase the network lifetime by using renewable energy sources. In this paper, we study a simple IoT system composed of one base station (BS) and multiple EH user equipments (UEs), where the system control is modeled as a Markov decision process without any prior knowledge assumed on the energy dynamics. The central controller, i.e., the BS, is in charge of scheduling a subset of UEs to access the limited orthogonal channels and regulating transmission power for the scheduled UEs. Applying reinforcement learning (RL) methods in this situation is technically challenging since the state and action spaces are continuous. With a long short-term memory (LSTM)-based algorithm to predict the UEs' battery states, we propose an actor-critic deep Q-network (DQN) RL algorithm to simultaneously deal with the access and continuous power control problem, by considering both the sum rate and prediction loss. The experimental results show that the proposed RL algorithm can achieve better performances when compared with the existing benchmarks.","","","10.1109/JIOT.2019.2928837","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770243","Battery prediction;continuous power control;energy harvesting (EH);Internet of Things (IoT);reinforcement learning (RL)","Power control;Internet of Things;Batteries;Aerospace electronics;Resource management;Uplink;Energy harvesting","control engineering computing;decision theory;energy harvesting;Internet of Things;learning (artificial intelligence);Markov processes;multi-access systems;power control;recurrent neural nets;renewable energy sources;telecommunication control;telecommunication power management;telecommunication scheduling;wireless channels","energy harvesting multiple access system;self-sustainable operations;network lifetime;renewable energy sources;BS;system control;Markov decision process;central controller;reinforcement learning methods;action spaces;continuous power control problem;Internet of Things application;IoT system;base station;multiple EH user equipments;UE subset scheduling;transmission power regulation;limited orthogonal channels;long short-term memory-based algorithm;state space;LSTM-based algorithm;UE battery states;actor-critic deep Q-network RL algorithm;actor-critic DQN RL algorithm;prediction loss;sum rate","","","35","","","","","IEEE","IEEE Journals"
"Human Pose Estimation in Video via Structured Space Learning and Halfway Temporal Evaluation","S. Liu; Y. Li; G. Hua","School of Computer Science and Technology, Tianjin University, Tianjin, China; School of Computer Science and Technology, Tianjin University, Tianjin, China; School of Information and Electrical Engineering, Hebei University of Engineering, Handan, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","7","2029","2038","Human pose estimation from image or video is a basic issue in computer graphics and computer vision. The challenge of human pose estimation in video lies in the temporal coherency issue. The temporal consistency in video is the contents' similarity shown in the video frames. In video, temporal consistency maintenance of human pose estimation is to obtain better long-term consistency. Great major methods for the long-term consistency are using the whole video optimization method, which makes very large computation and the absence of consistency before and after the articulated limbs. In this paper, a novel method for the maintenance of temporal consistency is proposed. We maintain the temporal consistency of the video by the structured space learning and halfway temporal evaluation methods. We adopt a three-stage multi-feature deep convolution network framework to generate the initial posture joints position data, and a long-term temporal coherence is propagated to the overall video at each stage. The long-term consistency is more appealing since it produces stable results over larger periods of time. Our method can achieve good temporal consistency and get accurate and stable human pose estimation results. Various experimental results demonstrated the superiority of our method.","","","10.1109/TCSVT.2018.2858828","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418405","Human pose estimation;temporal coherency;structure learning","Pose estimation;Streaming media;Maintenance engineering;Convolution;Optimization methods","computer vision;convolutional neural nets;image classification;image representation;learning (artificial intelligence);optimisation;pose estimation;video signal processing","structured space learning;computer graphics;computer vision;temporal coherency issue;video frames;temporal consistency maintenance;long-term consistency;video optimization method;halfway temporal evaluation methods;long-term temporal coherence;stable human pose estimation results;three-stage multi-feature deep convolution network framework","","","48","","","","","IEEE","IEEE Journals"
"Traffic and Computation Co-Offloading With Reinforcement Learning in Fog Computing for Industrial Applications","Y. Wang; K. Wang; H. Huang; T. Miyazaki; S. Guo","National Engineering Research Center of Communications and Networking, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Academic Center for Computing and Media Studies, Kyoto University, Kyoto, Japan; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan; Hong Kong Polytechnic University, Shenzhen Research Institute, Shenzhen, China","IEEE Transactions on Industrial Informatics","","2019","15","2","976","986","In the past decade, network data communication has experienced a rapid growth, which has led to explosive congestion in heterogeneous networks. Moreover, the emerging industrial applications, such as automatic driving put forward higher requirements on both networks and devices. On the contrary, running computation-intensive industrial applications locally are constrained by the limited resources of devices. Correspondingly, fog computing has recently emerged to reduce the congestion of content-centric networks. It has proven to be a good way in industry and traffic for reducing network delay and processing time. In addition, device-to-device offloading is viewed as a promising paradigm to transmit network data in mobile environment, especially for autodriving vehicles. In this paper, jointly taking both the network traffic and computation workload of industrial traffic into consideration, we explore a fundamental tradeoff between energy consumption and service delay when provisioning mobile services in vehicular networks. In particular, when the available resource in mobile vehicles becomes a bottleneck, we propose a novel model to depict the users' willingness of contributing their resources to the public. We then formulate a cost minimization problem by exploiting the framework of Markov decision progress (MDP) and propose the dynamic reinforcement learning scheduling algorithm and the deep dynamic scheduling algorithm to solve the offloading decision problem. By adopting different mobile trajectory traces, we conduct extensive simulations to evaluate the performance of the proposed algorithms. The results show that our proposed algorithms outperform other benchmark schemes in the mobile edge networks.","","","10.1109/TII.2018.2883991","National Natural Science Foundation of China; China Postdoctoral Science Foundation; China Postdoctoral Science Special Foundation; Shenzhen Basic Research Funding Scheme; Strategic Information and Communications R&D Promotion Programme; Open Research Fund of the Jiangsu Engineering Research Center of Communication and Network Technology; NJUPT; National Engineering Research Center of Communications and Networking (Nanjing University of Posts and Telecommunications); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8552454","Computation offloading;fog computing;industrial application;reinforcement learning (RL);traffic offloading","Mobile handsets;Task analysis;Delays;Device-to-device communication;Servers;Energy consumption;Edge computing","cloud computing;data communication;learning (artificial intelligence);Markov processes;minimisation;mobile radio;telecommunication computing;telecommunication scheduling;telecommunication traffic","network delay reduction;mobile trajectory;cost minimization problem;Markov decision progress framework;mobile trajectory traces;mobile edge networks;offloading decision problem;deep dynamic scheduling algorithm;dynamic reinforcement learning scheduling algorithm;mobile vehicles;vehicular networks;mobile services;service delay;energy consumption;industrial traffic;computation workload;network traffic;autodriving vehicles;mobile environment;device-to-device offloading;content-centric networks;computation-intensive industrial applications;automatic driving;heterogeneous networks;explosive congestion;network data communication;fog computing","","12","29","","","","","IEEE","IEEE Journals"
"Adaptive Deep Convolutional Neural Networks for Scene-Specific Object Detection","X. Li; M. Ye; Y. Liu; C. Zhu","School of Computer Science and Engineering, Key Laboratory for NeuroInformation, Center for Robotics, Ministry of Education, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, Key Laboratory for NeuroInformation, Center for Robotics, Ministry of Education, University of Electronic Science and Technology of China, Chengdu, China; Vision and Image Processing Laboratory, School of Computer Science, Sichuan University, Chengdu, China; School of Electronic Engineering and the Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","9","2538","2551","A deep convolutional neural network (CNN) becomes a widely used tool for object detection. Many previous works have achieved excellent performance on object detection benchmarks. However, these works present generic detectors whose performance will drop rapidly when they are applied to a surveillance scene. In this paper, we propose an efficient method to construct a scene-specific regression model based on a generic CNN-based classifier. Our regression model is an adaptive deep CNN (ADCNN), which can predict object locations in the surveillance scene. First, we transfer the generic CNN-based classifier to the surveillance scene by selecting useful kernels. Second, we learn the context information of the surveillance scene in our regression model for accurate location prediction. Our main contributions are: 1) a transfer learning method that selects useful kernels in the generic CNN-based classifier; 2) a special architecture that can effectively learn the local and global context information in the surveillance scene; and 3) a new objective function to effectively train parameters in ADCNN. Compared with some state-of-the-art models, ADCNN achieves the best performance on three surveillance data sets for pedestrian detection and one surveillance data set for vehicle detection.","","","10.1109/TCSVT.2017.2749620","National Natural Science Foundation of China; Applied Basic Research Programs of Sichuan Science and Technology Department; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8027120","Convolutional neural network;object detection;surveillance scene","Object detection;Feature extraction;Surveillance;Kernel;Adaptation models;Detectors;Linear programming","convolutional neural nets;image classification;learning (artificial intelligence);object detection;pedestrians;regression analysis;traffic engineering computing;video surveillance","adaptive deep convolutional neural networks;scene-specific object detection;surveillance scene;scene-specific regression model;generic CNN-based classifier;object locations;objective function;ADCNN;transfer learning method;vehicle detection;pedestrian detection","","9","50","","","","","IEEE","IEEE Journals"
"Deep Road Scene Understanding","W. Zhou; S. Lv; Q. Jiang; L. Yu","School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, China; School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, China; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; Institute of Information and Communication Engineering, Zhejiang University, Hangzhou, China","IEEE Signal Processing Letters","","2019","26","4","587","591","Road scene understanding is a difficult task in autonomous driving. In this letter, we propose a novel deep encoder-decoder architecture for road scene understanding in an end-to-end manner. This core trainable understanding engine includes an encoder network, a decoder network with two streams, and a pixel-level fusion network with classification layer. The encoder network is composed of the front-end model of the classical convolution neural network, VGGNet. The decoder network with two streams includes multi-scale skip connection modules to reduce the down-scaling effect. Finally, a fusion network fuses the two-level information from the two streams of the decoder network for precise pixel-level classification. Additionally, the convolution layer is added to each skip connection module to increase the depth of the architecture. Our architecture achieves outstanding performance on the publicly available CamVid dataset and significantly outperforms previous architectures. This deep architecture is ideal for road scene understanding.","","","10.1109/LSP.2019.2896793","National Natural Science Foundation of China; Zhejiang Provincial Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630577","Road scene understanding;encoder–decoder architecture;skip connection;fusion layer","Decoding;Convolution;Roads;Semantics;Computer architecture;Fuses;Feature extraction","feature extraction;image classification;image coding;image fusion;image segmentation;learning (artificial intelligence);neural net architecture;object detection;traffic engineering computing","pixel-level fusion network;encoder network;front-end model;decoder network;multiscale skip connection modules;fusion network fuses;deep architecture;deep road scene understanding;deep encoder-decoder architecture;core trainable understanding engine;pixel-level classification;convolution neural network","","","25","","","","","IEEE","IEEE Journals"
"Recolored Image Detection via a Deep Discriminative Model","Y. Yan; W. Ren; X. Cao","State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","","2019","14","1","5","17","Image recoloring is a technique that can transfer image color or theme and result in an imperceptible change in human eyes. Although image recoloring is one of the most important image manipulation techniques, there is no special method designed for detecting this kind of forgery. In this paper, we propose a trainable end-to-end system for distinguishing recolored images from natural images. The proposed network takes the original image and two derived inputs based on illumination consistency and inter-channel correlation of the original input into consideration and outputs the probability that it is recolored. Our algorithm adopts a convolutional neural network (CNN)- based deep architecture, which consists of three feature extraction blocks and a feature fusion module. To train the deep neural network, we synthesize a data set comprised of recolored images and corresponding ground truth using different recoloring methods. Extensive experimental results on the recolored images generated by various methods show that our proposed network is well generalized and very robust.","","","10.1109/TIFS.2018.2834155","National Key Research and Development Plan; National Natural Science Foundation of China; Natural Science Foundation of Beijing Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8355817","Recoloring detection;convolutional neural network","Image color analysis;Forgery;Feature extraction;Authentication;Neural networks;Correlation;Digital images","convolution;feature extraction;feedforward neural nets;image colour analysis;image fusion;learning (artificial intelligence);object detection","trainable end-to-end system;natural images;deep discriminative model;deep neural network training;image manipulation techniques;image detection recoloring;human eyes;forgery detection;illumination consistency;inter-channel correlation;CNN;convolutional neural network- based deep architecture;feature extraction blocks;feature fusion module","","1","59","","","","","IEEE","IEEE Journals"
"Dynamic Power Control for NOMA Transmissions in Wireless Caching Networks","Y. Fu; W. Wen; Z. Zhao; T. Q. S. Quek; S. Jin; F. Zheng","Department of Information Systems Technology and Design, Singapore University of Technology and Design, Singapore; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Department of Information Systems Technology and Design, Singapore University of Technology and Design, Singapore; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China","IEEE Wireless Communications Letters","","2019","8","5","1485","1488","Non-orthogonal multiple access (NOMA) technique is capable of improving the efficiency of delivering and pushing contents in wireless caching networks. However, due to the differences of the data volume and the channel condition, the static power control schemes cannot fully explore the potential of NOMA. To solve this problem, dynamic power control for NOMA transmissions in wireless caching networks is studied in this letter, which can be adjusted based on the status of content transmissions. In particular, we focus on minimizing the transmission delay with the considerations of each user's transmission deadline and the total power constraint. An iterative algorithm is first proposed to approach the optimal solution of dynamic power control. Then a deep neural network (DNN)-based method is designed to keep a balance between the performance and the computational complexity. Finally, Monte-Carlo simulations are provided for verifications.","","","10.1109/LWC.2019.2923410","Natural Science Foundation of Beijing Municipality; National Science and Technology Major Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8738823","Content caching;deep learning approach;delay minimizing;NOMA;dynamic power allocation","Delays;Power control;NOMA;Heuristic algorithms;Wireless communication;Resource management;Deep learning","5G mobile communication;cache storage;computational complexity;iterative methods;minimisation;Monte Carlo methods;multi-access systems;multiuser channels;neural nets;power control;radio networks;telecommunication control;wireless channels","dynamic power control;NOMA transmissions;wireless caching networks;nonorthogonal multiple access technique;content delivery;static power control schemes;content transmissions;transmission delay;total power constraint;deep neural network-based method;data volume;channel condition;transmission delay minimization;user transmission deadline;iterative algorithm;DNN-based method;computational complexity;Monte-Carlo simulations","","","6","","","","","IEEE","IEEE Journals"
"Denoising Prior Driven Deep Neural Network for Image Restoration","W. Dong; P. Wang; W. Yin; G. Shi; F. Wu; X. Lu","State Key Laboratory on Integrated Services Networks, School of Artificial Intelligence, Xidian University, Xi'an, China; School of Artificial Intelligence, Xidian University, Xi'an, China; Department of Mathematics, University of California, Los Angeles, CA, USA; School of Artificial Intelligence, Xidian University, Xi'an, China; School of Artificial Intelligence, Xidian University, Xi'an, China; School of Artificial Intelligence, Xidian University, Xi'an, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","10","2305","2318","Deep neural networks (DNNs) have shown very promising results for various image restoration (IR) tasks. However, the design of network architectures remains a major challenging for achieving further improvements. While most existing DNN-based methods solve the IR problems by directly mapping low quality images to desirable high-quality images, the observation models characterizing the image degradation processes have been largely ignored. In this paper, we first propose a denoising-based IR algorithm, whose iterative steps can be computed efficiently. Then, the iterative process is unfolded into a deep neural network, which is composed of multiple denoisers modules interleaved with back-projection (BP) modules that ensure the observation consistencies. A convolutional neural network (CNN) based denoiser that can exploit the multi-scale redundancies of natural images is proposed. As such, the proposed network not only exploits the powerful denoising ability of DNNs, but also leverages the prior of the observation model. Through end-to-end training, both the denoisers and the BP modules can be jointly optimized. Experimental results on several IR tasks, e.g., image denoisig, super-resolution and deblurring show that the proposed method can lead to very competitive and often state-of-the-art results on several IR tasks, including image denoising, deblurring, and super-resolution.","","","10.1109/TPAMI.2018.2873610","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8481558","denoising-based image restoration;deep neural network;denoising prior;image restoration","Task analysis;Noise reduction;Image restoration;Optimization;Image resolution;Neural networks;Iterative algorithms","convolutional neural nets;image denoising;image restoration;iterative methods;learning (artificial intelligence);neural net architecture","network architectures;denoising-based IR algorithm;back-projection modules;convolutional neural network based denoiser;natural images;image denoising;deep neural network;image restoration;image degradation;end-to-end training","","15","67","","","","","IEEE","IEEE Journals"
"Deep Convolutional Neural Network for Natural Image Matting Using Initial Alpha Mattes","D. Cho; Y. Tai; I. S. Kweon","Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Youtu Research Lab, Tencent, SNG, Hong Kong; Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Image Processing","","2019","28","3","1054","1067","We propose a deep convolutional neural network (CNN) method for natural image matting. Our method takes multiple initial alpha mattes of the previous methods and normalized RGB color images as inputs, and directly learns an end-to-end mapping between the inputs and reconstructed alpha mattes. Among the various existing methods, we focus on using two simple methods as initial alpha mattes: the closed-form matting and KNN matting. They are complementary to each other in terms of local and nonlocal principles. A major benefit of our method is that it can “recognize” different local image structures and then combine the results of local (closed-form matting) and nonlocal (KNN matting) mattings effectively to achieve higher quality alpha mattes than both of the inputs. Furthermore, we verify extendability of the proposed network to different combinations of initial alpha mattes from more advanced techniques such as KL divergence matting and information-flow matting. On the top of deep CNN matting, we build an RGB guided JPEG artifacts removal network to handle JPEG block artifacts in alpha matting. Extensive experiments demonstrate that our proposed deep CNN matting produces visually and quantitatively high-quality alpha mattes. We perform deeper experiments including studies to evaluate the importance of balancing training data and to measure the effects of initial alpha mattes and also consider results from variant versions of the proposed network to analyze our proposed DCNN matting. In addition, our method achieved high ranking in the public alpha matting evaluation dataset in terms of the sum of absolute differences, mean squared errors, and gradient errors. Also, our RGB guided JPEG artifacts removal network restores the damaged alpha mattes from compressed images in JPEG format.","","","10.1109/TIP.2018.2872925","Ministry of Trade, Industry and Energy; Korea Evaluation Institute of Industrial Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8476594","Matting;CNN;initial alpha mattes","Image color analysis;Transform coding;Image coding;Laplace equations;Image reconstruction;Training data;Mathematical model","data compression;feature extraction;image coding;image colour analysis;image segmentation;learning (artificial intelligence);neural nets","natural image matting;deep convolutional neural network method;multiple initial alpha mattes;normalized RGB color images;reconstructed alpha mattes;closed-form matting;KNN matting;KL divergence matting;information-flow matting;deep CNN matting;JPEG artifacts removal network;high-quality alpha mattes;DCNN matting;public alpha matting evaluation dataset;damaged alpha mattes;local image structures","","4","42","","","","","IEEE","IEEE Journals"
"A  $Q$ -Learning-Based Proactive Caching Strategy for Non-Safety Related Services in Vehicular Networks","L. Hou; L. Lei; K. Zheng; X. Wang","Intelligent Computing and Communication Laboratory, Wireless Signal Processing and Networks Laboratory, Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; College of Science and Engineering, James Cook University, Cairns, Australia; Intelligent Computing and Communication Laboratory, Wireless Signal Processing and Networks Laboratory, Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Department of Electrical and Computer Engineering, University of Western Ontario, London, ON, Canada","IEEE Internet of Things Journal","","2019","6","3","4512","4520","Content caching has brought huge potential for the provisioning of non-safety related infotainment services in future vehicular networks. Assisted by multiaccess edge computing, roadside units (RSUs) could become cache-capable and offer fast caching services to moving vehicles for content providers. On the other hand, deep learning makes it possible to accurately estimate the behavior of vehicles, which enables effective proactive caching strategies. However, caching services considering both the mobility of vehicles and storage could incur increased latency and considerable cost due to the cache size needed in RSUs. In this paper, we model such a problem using Markov decision processes, and propose a heuristic Q-learning solution together with vehicle movement predictions based on a long short-term memory network. The optimal caching strategy which minimizes the latency of caching services can be derived by our heuristic εn-greedy training processes. Numerical results demonstrate that our proposed strategy can achieve better performance compared with several baselines under different prediction accuracies.","","","10.1109/JIOT.2018.2883762","National Natural Science Foundation of China; National Key Technology Research and Development Program of China; BUPT Excellent Ph.D. Students Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550787","Mobility prediction;multiaccess edge computing (MEC);proactive cache;vehicular networks","Quality of service;Numerical models;Servers;Internet of Things;Markov processes;Training","cache storage;distributed processing;greedy algorithms;learning (artificial intelligence);Markov processes;mobile radio;telecommunication computing","content providers;deep learning;heuristic Q-learning solution;vehicle movement predictions;short-term memory network;optimal caching strategy;proactive caching strategy;nonsafety related services;content caching;nonsafety related infotainment services;multiaccess edge computing;caching services;RSU;vehicular networks;roadside units;Markov decision process;heuristic εn-greedy training process","","1","40","","","","","IEEE","IEEE Journals"
"Mutual Component Convolutional Neural Networks for Heterogeneous Face Recognition","Z. Deng; X. Peng; Z. Li; Y. Qiao","Shenzhen College of Advanced Technology, University of Chinese Academy of Sciences, Beijing, China; Guangdong Provincial Key Laboratory Computer Vision and Virtual Reality, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Tencent AI Lab, Shenzhen, China; Shenzhen Key Lab of Computer Vision and Pattern Recognition, Shenzhen Institutes of Advanced Technology, Shenzhen Institutes of Advanced Technology, Shenzhen, China","IEEE Transactions on Image Processing","","2019","28","6","3102","3114","Heterogeneous face recognition (HFR) aims to identify a person from different facial modalities, such as visible and near-infrared images. The main challenges of HFR lie in the large modality discrepancy and insufficient training samples. In this paper, we propose the mutual component convolutional neural network (MC-CNN), a modal-invariant deep learning framework, to tackle these two issues simultaneously. Our MC-CNN incorporates a generative module, i.e., the mutual component analysis (MCA), into modern deep CNNs by viewing MCA as a special fully connected (FC) layer. Based on deep features, this FC layer is designed to extract modal-independent hidden factors and is updated according to maximum likelihood analytic formulation instead of back propagation which prevents overfitting from limited data naturally. In addition, we develop an MCA loss to update the network for modal-invariant feature learning. Extensive experiments show that our MC-CNN outperforms several fine-tuned baseline models significantly. Our methods achieve the state-of-the-art performance on the CASIA NIR-VIS 2.0, CUHK NIR-VIS, and IIIT-D Sketch datasets.","","","10.1109/TIP.2019.2894272","National Natural Science Foundation of China; Shenzhen Research Program; Tencent AI Lab Rhino-Bird Joint Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624555","Heterogeneous face recognition;mutual component analysis;mutual component convolutional neural network","Feature extraction;Face recognition;Analytical models;Convolutional neural networks;Face;Task analysis","convolutional neural nets;face recognition;feature extraction;learning (artificial intelligence);maximum likelihood estimation","mutual component convolutional neural network;heterogeneous face recognition;HFR;modality discrepancy;MC-CNN;modal-invariant deep learning framework;mutual component analysis;MCA;modern deep CNNs;special fully connected layer;deep features;modal-independent hidden factors;modal-invariant feature learning;facial modalities;maximum likelihood analytic formulation;fine-tuned baseline models;CUHK NIR-VIS datasets;CASIA NIR-VIS 2.0 datasets;IIIT-D Sketch datasets","","","57","","","","","IEEE","IEEE Journals"
"Runtime Network Routing for Efficient Image Classification","Y. Rao; J. Lu; J. Lin; J. Zhou","Department of Automation, State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","10","2291","2304","In this paper, we propose a generic Runtime Network Routing (RNR) framework for efficient image classification, which selects an optimal path inside the network. Unlike existing static neural network acceleration methods, our method preserves the full ability of the original large network and conducts dynamic routing at runtime according to the input image and current feature maps. The routing is performed in a bottom-up, layer-by-layer manner, where we model it as a Markov decision process and use reinforcement learning for training. The agent determines the estimated reward of each sub-path and conducts routing conditioned on different samples, where a faster path is taken when the image is easier for the task. Since the ability of network is fully preserved, the balance point is easily adjustable according to the available resources. We test our method on both multi-path residual networks and incremental convolutional channel pruning, and show that RNR consistently outperforms static methods at the same computation complexity on both the CIFAR and ImageNet datasets. Our method can also be applied to off-the-shelf neural network structures and easily extended to other application scenarios.","","","10.1109/TPAMI.2018.2878258","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Shenzhen fundamental research fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8510920","Deep network compression;image classification;efficient inference model;reinforcement learning;deep learning","Routing;Runtime;Neural networks;Acceleration;Training;Computational modeling","image classification;learning (artificial intelligence);neural nets","multipath residual networks;RNR;static neural network acceleration methods;layer-by-layer manner;Markov decision process;neural network structures;image classification;feature maps;runtime network routing framework;CIFAR dataset;ImageNet dataset;incremental convolutional channel pruning","","4","69","","","","","IEEE","IEEE Journals"
"On-Board Deep Q-Network for UAV-Assisted Online Power Transfer and Data Collection","K. Li; W. Ni; E. Tovar; A. Jamalipour","Real-Time and Embedded Computing Systems Research Centre (CISTER), Porto, Portugal; Digital Productivity and Services Flagship, Commonwealth Scientific and Industrial Research Organization (CSIRO), Sydney, Australia; Real-Time and Embedded Computing Systems Research Centre (CISTER), Porto, Portugal; School of Electrical and Information Engineering, The University of Sydney, Australia","IEEE Transactions on Vehicular Technology","","2019","68","12","12215","12226","Unmanned Aerial Vehicles (UAVs) with Microwave Power Transfer (MPT) capability provide a practical means to deploy a large number of wireless powered sensing devices into areas with no access to persistent power supplies. The UAV can charge the sensing devices remotely and harvest their data. A key challenge is online MPT and data collection in the presence of on-board control of a UAV (e.g., patrolling velocity) for preventing battery drainage and data queue overflow of the devices, while up-to-date knowledge on battery level and data queue of the devices is not available at the UAV. In this paper, an on-board deep Q-network is developed to minimize the overall data packet loss of the sensing devices, by optimally deciding the device to be charged and interrogated for data collection, and the instantaneous patrolling velocity of the UAV. Specifically, we formulate a Markov Decision Process (MDP) with the states of battery level and data queue length of devices, channel conditions, and waypoints given the trajectory of the UAV; and solve it optimally with Q-learning. Furthermore, we propose the on-board deep Q-network that enlarges the state space of the MDP, and a deep reinforcement learning based scheduling algorithm that asymptotically derives the optimal solution online, even when the UAV has only outdated knowledge on the MDP states. Numerical results demonstrate that our deep reinforcement learning algorithm reduces the packet loss by at least 69.2%, as compared to existing non-learning greedy algorithms.","","","10.1109/TVT.2019.2945037","National Funds through FCT/MCTES (Portuguese Foundation for Science and Technology); CISTER Research Unit; Operational Competitiveness Programme and Internationalization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854903","Unmanned aerial vehicle;microwave power transfer;online resource allocation;deep reinforcement learning;Markov decision process","Trajectory;Batteries;Data collection;Wireless communication;Wireless sensor networks;Resource management;Sensors","","","","","59","IEEE","","","","IEEE","IEEE Journals"
"A Survey on 3D Object Detection Methods for Autonomous Driving Applications","E. Arnold; O. Y. Al-Jarrah; M. Dianati; S. Fallah; D. Oxtoby; A. Mouzakitis","Warwick Manufacturing Group, The University of Warwick, Coventry, U.K.; Warwick Manufacturing Group, The University of Warwick, Coventry, U.K.; Warwick Manufacturing Group, The University of Warwick, Coventry, U.K.; Centre for Automotive Engineering, University of Surrey, Guildford, U.K.; Jaguar Land Rover, Ltd., Coventry, U.K.; Jaguar Land Rover, Ltd., Coventry, U.K.","IEEE Transactions on Intelligent Transportation Systems","","2019","20","10","3782","3795","An autonomous vehicle (AV) requires an accurate perception of its surrounding environment to operate reliably. The perception system of an AV, which normally employs machine learning (e.g., deep learning), transforms sensory data into semantic information that enables autonomous driving. Object detection is a fundamental function of this perception system, which has been tackled by several works, most of them using 2D detection methods. However, the 2D methods do not provide depth information, which is required for driving tasks, such as path planning, collision avoidance, and so on. Alternatively, the 3D object detection methods introduce a third dimension that reveals more detailed object's size and location information. Nonetheless, the detection accuracy of such methods needs to be improved. To the best of our knowledge, this is the first survey on 3D object detection methods used for autonomous driving applications. This paper presents an overview of 3D object detection methods and prevalently used sensors and datasets in AVs. It then discusses and categorizes the recent works based on sensors modalities into monocular, point cloud-based, and fusion methods. We then summarize the results of the surveyed works and identify the research gaps and future research directions.","","","10.1109/TITS.2019.2892405","Jaguar Land Rover; EPSRC U.K. through the Towards Autonomy: Smart and Connected Control Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8621614","Machine learning;deep learning;computer vision;object detection;autonomous vehicles;intelligent vehicles","Sensors;Three-dimensional displays;Object detection;Cameras;Laser radar;Autonomous vehicles;Two dimensional displays","intelligent transportation systems;learning (artificial intelligence);mobile robots;object detection;robot vision;vehicles","3D object detection;autonomous driving applications;autonomous vehicle;AV;perception system;2D detection methods;detection accuracy;fusion methods;point cloud;machine learning;sensory data","","4","70","","","","","IEEE","IEEE Journals"
"CountNet: Estimating the Number of Concurrent Speakers Using Supervised Learning","F. Stöter; S. Chakrabarty; B. Edler; E. A. P. Habets","International Audio Laboratories Erlangen, Erlangen, Germany; International Audio Laboratories Erlangen, Erlangen, Germany; International Audio Laboratories Erlangen, Erlangen, Germany; International Audio Laboratories Erlangen, Erlangen, Germany","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","2","268","282","Estimating the maximum number of concurrent speakers from single-channel mixtures is a challenging problem and an essential first step to address various audio-based tasks such as blind source separation, speaker diarization, and audio surveillance. We propose a unifying probabilistic paradigm, where deep neural network architectures are used to infer output posterior distributions. These probabilities are in turn processed to yield discrete point estimates. Designing such architectures often involves two important and complementary aspects that we investigate and discuss. First, we study how recent advances in deep architectures may be exploited for the task of speaker count estimation. In particular, we show that convolutional recurrent neural networks outperform recurrent networks used in a previous study when adequate input features are used. Even for short segments of speech mixtures, we can estimate up to five speakers, with a significantly lower error than other methods. Second, through comprehensive evaluation, we compare the best-performing method to several baselines, as well as the influence of gain variations, different data sets, and reverberation. The output of our proposed method is compared to human performance. Finally, we give insights into the strategy used by our proposed method.","","","10.1109/TASLP.2018.2877892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8506601","Speaker count estimation;number of concurrent speakers;overlap detection;cocktail-party","Estimation;Task analysis;Speech processing;Neural networks;Microphones;Surveillance","blind source separation;learning (artificial intelligence);probability;recurrent neural nets;speech processing","deep neural network architectures;output posterior distributions;discrete point estimates;complementary aspects;deep architectures;speaker count estimation;convolutional recurrent neural networks;speech mixtures;concurrent speakers;supervised learning;maximum number;single-channel mixtures;audio-based tasks;blind source separation;speaker diarization;audio surveillance;unifying probabilistic paradigm","","2","88","","","","","IEEE","IEEE Journals"
"Inpainting of Remote Sensing SST Images With Deep Convolutional Generative Adversarial Network","J. Dong; R. Yin; X. Sun; Q. Li; Y. Yang; X. Qin","Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapoils, MN, USA","IEEE Geoscience and Remote Sensing Letters","","2019","16","2","173","177","Cloud occlusion is a common problem in the satellite remote sensing (RS) field and poses great challenges for image processing and object detection. Most existing methods for cloud occlusion recovery extract the surrounding information from the single corrupted image rather than the historical RS image records. Moreover, the existing algorithms can only handle small and regular-shaped obnubilation regions. This letter introduces a deep convolutional generative adversarial network to recover the RS sea surface temperature images with cloud occlusion from the big historical image records. We propose a new loss function for the inpainting network, which adds a supervision term to solve our specific problem. Given a trained generative model, we search for the closest encoding of the corrupted image in the low-dimensional space using our inpainting loss function. This encoding is then passed through the generative model to infer the missing content. We conduct experiments on the RS image data set from the national oceanic and atmospheric administration. Compared with traditional and machine learning methods, both qualitative and quantitative results show that our method has advantages over existing methods.","","","10.1109/LGRS.2018.2870880","National Natural Science Foundation of China; Key Research and Development Program of Shandong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8480867","Cloud occlusion images;deep convolutional generative adversarial network (DCGAN);inpainting;sea surface temperature (SST) images","Generative adversarial networks;Ocean temperature;Gallium nitride;Clouds;Land surface temperature;Image color analysis;Generators","geophysical image processing;geophysical signal processing;image processing;learning (artificial intelligence);object detection;ocean temperature;oceanographic techniques;remote sensing","deep convolutional generative adversarial network;common problem;satellite remote sensing field;image processing;object detection;cloud occlusion recovery;single corrupted image;historical RS image records;existing algorithms;regular-shaped obnubilation regions;RS sea surface temperature images;big historical image records;inpainting network;trained generative model;inpainting loss function;RS image data;remote sensing SST images","","","18","","","","","IEEE","IEEE Journals"
"Phase-Aware Speech Enhancement Based on Deep Neural Networks","N. Zheng; X. Zhang","State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; Center for Intelligent Acoustics and Immersive Communications, School of Marine Science and Technology, Northwestern Polytechnical University, Xi’an, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","1","63","76","Short-time frequency transform (STFT) is fundamental in speech processing. Because of the difficulty of processing highly unstructured STFT phase, most speech-processing algorithms only operate with STFT magnitude, leaving the STFT phase far from explored. However, with the recent development of deep neural network (DNN) based speech processing, e.g., speech enhancement and recognition, phase processing is becoming more important than ever before as a new growing point of DNN-based methods. In this paper, we propose a phase-aware speech enhancement algorithm based on DNN. Specifically, in the training stage, when incorporating phase as a target, our core idea is to transform an unstructured phase spectrogram to its derivative along the time axis, i.e., instantaneous frequency deviation (IFD), which has a similar structure with its corresponding magnitude spectrogram. We further propose to optimize both IFD and magnitude jointly in a multiobjective learning framework. In the test stage, we propose a postprocessing method to recover the phase spectrogram from the estimated IFD. Experimental results demonstrate the effectiveness of the proposed method.","","","10.1109/TASLP.2018.2870742","National Natural Science Foundation of China; Shenzhen Science and Technology Plan; Shaanxi Natural Science Basic Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466892","Deep neural network (DNN);phase estimation;speech enhancement;instantaneous frequency;harmonic model","Spectrogram;Speech enhancement;Noise measurement;Training;Time-frequency analysis;Wrapping","Fourier transforms;learning (artificial intelligence);neural nets;radio applications;speech enhancement;speech recognition;telecommunication;time-frequency analysis","speech-processing algorithms;STFT magnitude;phase processing;DNN-based methods;phase-aware speech enhancement algorithm;unstructured phase spectrogram;instantaneous frequency deviation;corresponding magnitude spectrogram;deep neural networks;short-time frequency;speech recognition;unstructured STFT phase;incorporating phase","","1","48","","","","","IEEE","IEEE Journals"
"Structure-Texture Image Decomposition Using Deep Variational Priors","Y. Kim; B. Ham; M. N. Do; K. Sohn","Agency for Defense Development, Institute of Defense Advanced Technology Research, Daejeon, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; Department of Electrical and Computer Engineering, University of Illinois at Urbana–Champaign, Urbana, IL, USA; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Image Processing","","2019","28","6","2692","2704","Most variational formulations for structure-texture image decomposition force the structure images to have small norm in some functional spaces and to share a common notion of edges, i.e., large-gradients or large-intensity differences. However, such a definition makes it difficult to distinguish structure edges from oscillations that have fine spatial scale but high contrast. In this paper, we introduce a new model by learning deep variational priors for structure images without explicit training data. An alternating direction method of a multiplier algorithm and its modular structure are adopted to plug deep variational priors into an iterative smoothing process. The central observations are that convolution neural networks (CNNs) can replace the total variation prior, and are indeed powerful to capture the natures of structure and texture. We show that our learned priors using CNNs successfully differentiate high-amplitude details from structure edges, and avoid halo artifacts. Different from previous data-driven smoothing schemes, our formulation provides another degree of freedom to produce continuous smoothing effects. Experimental results demonstrate the effectiveness of our approach on various computational photography and image processing applications, including texture removal, detail manipulation, HDR tone-mapping, and non-photorealistic abstraction.","","","10.1109/TIP.2018.2889531","National Research Foundation of Korea; MSIT, MOTIE, and KNPA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8586974","Structure-texture image decomposition;total variation;adaptive neighborhood filtering;alternating direction method of multiplier algorithm;texture filtering","Image decomposition;Smoothing methods;TV;Image edge detection;Kernel;Photography","convolutional neural nets;image texture;iterative methods;learning (artificial intelligence);smoothing methods","nonphotorealistic abstraction;HDR tone-mapping;detail manipulation;texture removal;computational photography;continuous smoothing effects;degree of freedom;halo artifacts;CNNs;convolution neural networks;iterative smoothing process;alternating direction method of multiplier algorithm;deep variational prior learning;high-amplitude details;image processing applications;modular structure;structure edges;structure images;structure-texture image decomposition;variational formulations","","1","56","","","","","IEEE","IEEE Journals"
"Deep Collaborative Learning With Application to the Study of Multimodal Brain Development","W. Hu; B. Cai; A. Zhang; V. D. Calhoun; Y. Wang","Biomedical Engineering DepartmentTulane University; Biomedical Engineering DepartmentTulane University; Biomedical Engineering DepartmentTulane University; Mind Research Network and the Department of Electronics and Communication EngineeringUniversity of New Mexico; Biomedical Engineering Department, Tulane University, New Orleans, LA, USA","IEEE Transactions on Biomedical Engineering","","2019","66","12","3346","3359","Objective: Multi-modal functional magnetic resonance imaging has been widely used for brain research. Conventional data-fusion methods cannot capture complex relationship (e.g., nonlinear predictive relationship) between multiple data. This paper aims to develop a neural network framework to extract phenotype related cross-data relationships and use it to study the brain development. Methods: We propose a novel method, deep collaborative learning (DCL), to address the limitation of existing methods. DCL first uses a deep network to represent original data and then seeks their correlations, while also linking the data representation with phenotypical information. Results: We studied the difference of functional connectivity (FCs) between different age groups and also use FCs as a fingerprint to predict cognitive abilities. Our experiments demonstrated higher accuracy of using DCL over other conventional models when classifying populations of different ages and cognitive scores. Moreover, DCL revealed that brain connections became stronger at adolescence stage. Furthermore, DCL detected strong correlations between default mode network and other networks which were overlooked by linear canonical correlation analysis, demonstrating DCL's ability of detecting nonlinear correlations. Conclusion: The results verified the superiority of DCL over conventional data-fusion methods. In addition, the stronger brain connection demonstrated the importance of adolescence stage for brain development. Significance: DCL can better combine complex correlations between multiple data sets in addition to their fitting to phenotypes, with the potential to overcome the limitations of several current data-fusion models.","","","10.1109/TBME.2019.2904301","National Institutes of Health; NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666981","Canonical correlation;deep network;fMRI;functional connectivity;brain development","Correlation;Data models;Functional magnetic resonance imaging;Brain modeling","","","","","42","Traditional","","","","IEEE","IEEE Journals"
"HyperDense-Net: A Hyper-Densely Connected CNN for Multi-Modal Image Segmentation","J. Dolz; K. Gopinath; J. Yuan; H. Lombaert; C. Desrosiers; I. Ben Ayed","Department of Software and Information Technology Engineering, École de technologie supérieure, Montreal, QC, Canada; Department of Software and Information Technology Engineering, École de technologie supérieure, Montreal, QC, Canada; School of Mathematics and Statistics, Xidian University, Xi’an, China; Department of Software and Information Technology Engineering, École de technologie supérieure, Montreal, QC, Canada; Department of Software and Information Technology Engineering, École de technologie supérieure, Montreal, QC, Canada; Department of Automated Manufacturing Engineering, École de technologie supérieure, Montreal, QC, Canada","IEEE Transactions on Medical Imaging","","2019","38","5","1116","1126","Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training. Particularly, DenseNet that connects each layer to every other layer in a feed-forward fashion and has shown impressive performances in natural image classification tasks. We propose HyperDenseNet, a 3-D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path but also between those across different paths. This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network. Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation. We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on six month infant data and the latter on adult images. HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks. We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning. Our code is publicly available.","","","10.1109/TMI.2018.2878669","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8515234","Deep learning;brain MRI;segmentation;3-D CNN;multi-modal imaging","Image segmentation;Brain;Three-dimensional displays;Training;Magnetic resonance imaging;Task analysis","biomedical MRI;brain;convolutional neural nets;image classification;image segmentation;learning (artificial intelligence);medical image processing","single joint layer;adult images;state-of-the-art segmentation networks;multimodal representation learning;multimodal image segmentation;implicit deep supervision;feed-forward fashion;natural image classification tasks;dense connectivity;imaging modality;3D fully convolutional neural network;hyperdensely connected CNN;hyperdense connection;multimodal CNN approach;HyperDenseNet;multimodal brain tissue segmentation","","3","57","","","","","IEEE","IEEE Journals"
"Deep Endoscopic Visual Measurements","D. K. Iakovidis; G. Dimas; A. Karargyris; F. Bianchi; G. Ciuti; A. Koulaouzidis","Department of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Department of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; IBM Research, San Jose, CA, USA; BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; BioRobotics Institute, Scuola Superiore Sant'Anna, Pisa, Italy; Endoscopy Unit, Royal Infirmary of Edinburgh, Edinburgh, U.K.","IEEE Journal of Biomedical and Health Informatics","","2019","23","6","2211","2219","Robotic endoscopic systems offer a minimally invasive approach to the examination of internal body structures, and their application is rapidly extending to cover the increasing needs for accurate therapeutic interventions. In this context, it is essential for such systems to be able to perform measurements, such as measuring the distance traveled by a wireless capsule endoscope, so as to determine the location of a lesion in the gastrointestinal tract, or to measure the size of lesions for diagnostic purposes. In this paper, we investigate the feasibility of performing contactless measurements using a computer vision approach based on neural networks. The proposed system integrates a deep convolutional image registration approach and a multilayer feed-forward neural network into a novel architecture. The main advantage of this system, with respect to the state-of-the-art ones, is that it is more generic in the sense that it is 1) unconstrained by specific models, 2) more robust to nonrigid deformations, and 3) adaptable to most of the endoscopic systems and environment, while enabling measurements of enhanced accuracy. The performance of this system is evaluated under ex vivo conditions using a phantom experimental model and a robotically assisted test bench. The results obtained promise a wider applicability and impact in endoscopy in the era of big data.","","","10.1109/JBHI.2018.2853987","European Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8408470","Endoscopy;neural networks;deep learning;deep matching;measurements","Biomedical measurement;Cameras;Endoscopes;Size measurement;Visualization;Lesions;Robots","","","","1","47","Traditional","","","","IEEE","IEEE Journals"
"Modeling and Planning Under Uncertainty Using Deep Neural Networks","D. L. Marino; M. Manic","Department of Computer Science, Virginia Commonwealth University, Richmond, VA, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, VA, USA","IEEE Transactions on Industrial Informatics","","2019","15","8","4442","4454","Artificial neural networks (ANNs) have been frequently used in industrial applications to model complex systems. However, using traditional ANNs for long-term planning tasks remains a challenge as they lack the capability to model uncertainty. Process noise and approximation errors cause ANN long-term estimations to deviate from the real behavior of the system. Unlike traditional ANNs, stochastic models provide a natural way to model uncertainty, providing estimations over a range of several possible outcomes. This paper introduces a stochastic modeling and planning approach using deep Bayesian neural networks (DBNNs). We use DBNNs to learn a stochastic model of the system dynamics. Planning is addressed as an open-loop trajectory optimization problem. We present two approaches for learning the dynamics: using single-step predictions and using multistep predictions. The advantages of the proposed methodology are as follows. First, accurate long-term estimations of the system state-trajectory probability distribution without the need for expert knowledge of the dynamics. Second, improved generalization and faster convergence rates in the trajectory optimization task when using multistep predictions to train the model. Third, viable for real-world applications since all expensive optimizations are executed offline while using a reasonable number of data samples. Testing is performed using challenging underactuated benchmark problems: the Cartpole and the Acrobot. The presented methodology successfully learns the swing-up maneuver using a relatively small number of iterations, with less than 125 sampled trajectories, and without any expert knowledge of the dynamics.","","","10.1109/TII.2019.2917520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8717678","Bayesian neural networks;trajectory optimization;uncertainty;variational inference","Uncertainty;Planning;Computational modeling;Stochastic processes;Trajectory;Estimation;Data models","learning (artificial intelligence);neural nets;optimisation;probability","system state-trajectory probability distribution;trajectory optimization task;deep neural networks;artificial neural networks;model complex systems;long-term planning tasks;model uncertainty;approximation errors;long-term estimations;stochastic modeling;deep Bayesian neural networks;DBNNs;open-loop trajectory optimization problem;single-step predictions;ANNs;stochastic planning approach;multistep predictions;swing-up maneuver","","","38","Traditional","","","","IEEE","IEEE Journals"
"Learning Deep Conditional Neural Network for Image Segmentation","Q. Wang; C. Yuan; Y. Liu","Department of Computer Science, Tsinghua University, Beijing, China; Department of Computer Science, Tsinghua University, Beijing, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Multimedia","","2019","21","7","1839","1852","Combining Convolutional Neural Networks (CNNs) with Conditional Random Fields (CRFs) achieves great success among recent object segmentation methods. There are two advantages by such usage. First, CNNs can extract low-level features, which are very similar to the extracted features in primates' primary visual cortex (V1). Second, CRFs can set up the relationship between input features and output labels in a direct way. In this paper, we extend the first advantage by using CNNs for low-level feature extraction and a Structured Random Forest (SRF)-based border ownership detector for high-level feature extraction, which are similar to the outputs of primates secondary visual cortex (V2). Compared to the CRF model, an improved Conditional Boltzmann Machine (CBM), which has a multi-channel visible layer, is proposed to model the relationship between predicted labels, local and global contexts of objects with multi-scale and multilevel features. Besides, our proposed CBM model is extended for object parsing by using multivisible branches instead of a single visible layer of CBM, which cannot only segment the whole body but also the parts of the body under. These visible branches use each branch for the segmentation of the whole body or one of the body parts. All branches share the same hidden layers of CBM and train the branches under an iterative way. By exploiting object parsing, the whole body segmentation performance of object is improved. To refine the segmentation output, two kinds of optimization algorithms are proposed. The superpixel-based algorithm can re-label the overlapped regions of multiple kinds of objects. The other curve correction algorithm corrects the edges of segmented object parts by using smooth edges under a curve similarity criterion. Experiments demonstrate that our models yield competitive results for object segmentation on the PASCAL VOC 2012 dataset and for object parsing on the PennFudan Pedestrian Parsing dataset, Pedestrian Parsing Surveillance Scenes dataset, Horse-Cow parsing dataset, and PASCAL Quadrupeds dataset.","","","10.1109/TMM.2018.2890360","NSFC; Shenzhen Science and Technologies project; Joint Research Center of Tencent and Tsinghua; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598722","Segmentation;object parsing;convolutional neural networks;conditional Boltzmann machines","Feature extraction;Object segmentation;Visualization;Brain modeling;Context modeling;Convolutional neural networks;Proposals","Boltzmann machines;feature extraction;image segmentation;learning (artificial intelligence);object detection;pedestrians;traffic engineering computing","image segmentation;CNNs;CRFs;low-level features;input features;output labels;high-level feature extraction;primates secondary visual cortex;CRF model;multichannel visible layer;predicted labels;multiscale;multilevel features;CBM model;object parsing;multivisible branches;single visible layer;visible branches;body parts;hidden layers;segmentation output;superpixel-based algorithm;curve correction algorithm;segmented object parts;curve similarity criterion;convolutional neural networks;conditional random fields;pedestrian parsing surveillance scenes dataset;horse-cow parsing dataset;improved conditional Boltzmann machine;deep conditional neural network;structured random forest-based border ownership detector","","1","61","","","","","IEEE","IEEE Journals"
"SilhoNet: An RGB Method for 6D Object Pose Estimation","G. Billings; M. Johnson-Roberson","Department of Naval Architecture and Marine Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Naval Architecture and Marine Engineering, University of Michigan, Ann Arbor, MI, USA","IEEE Robotics and Automation Letters","","2019","4","4","3727","3734","Autonomous robot manipulation involves estimating the translation and orientation of the object to be manipulated as a 6-degree-of-freedom (6D) pose. Methods using RGB-D data have shown great success in solving this problem. However, there are situations where cost constraints or the working environment may limit the use of RGB-D sensors. When limited to monocular camera data only, the problem of object pose estimation is very challenging. In this letter, we introduce a novel method called SilhoNet that predicts 6D object pose from monocular images. We use a convolutional neural network pipeline that takes in region of interest proposals to simultaneously predict an intermediate silhouette representation for objects with an associated occlusion mask and a 3D translation vector. The 3D orientation is then regressed from the predicted silhouettes. We show that our method achieves better overall performance on the YCB-Video dataset than two networks for 6D pose estimation from monocular image input.","","","10.1109/LRA.2019.2928776","National Aeronautics and Space Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8763998","Computer vision for automation;computer vision for other robotic applications;deep learning in robotics and automation;recognition;visual learning","Three-dimensional displays;Feature extraction;Pose estimation;Predictive models;Cameras;Robots;Deep learning","cameras;convolutional neural nets;image colour analysis;image representation;image sensors;pose estimation;stereo image processing","SilhoNet;RGB method;autonomous robot manipulation;6-degree-of-freedom pose;RGB-D data;RGB-D sensors;monocular images;convolutional neural network pipeline;3D translation vector;6D pose estimation;monocular camera;intermediate silhouette representation;occlusion mask;3D orientation;YCB-Video dataset","","","25","Traditional","","","","IEEE","IEEE Journals"
"Scalable Learning With a Structural Recurrent Neural Network for Short-Term Traffic Prediction","Y. Kim; P. Wang; L. Mihaylova","Sevendof AS, Trondheim, Norway; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.; Department of Automatic Control and Systems Engineering, The University of Sheffield, Sheffield, U.K.","IEEE Sensors Journal","","2019","19","23","11359","11366","This paper presents a scalable deep learning approach for short-term traffic prediction based on historical traffic data in a vehicular road network. Capturing the spatio-temporal relationship of the big data often requires a significant amount of computational burden or an ad-hoc design aiming for a specific type of road network. To tackle the problem, we combine a road network graph with recurrent neural networks (RNNs) to construct a structural RNN (SRNN). The SRNN employs a spatio-temporal graph to infer the interaction between adjacent road segments as well as the temporal dynamics of the time series data. The model is scalable thanks to two key aspects. First, the proposed SRNN architecture is built by using the semantic similarity of the spatio-temporal dynamic interactions of all segments. Second, we design the architecture to deal with fixed-length tensors regardless of the graph topology. With the real traffic speed data measured in the city of Santander, we demonstrate the proposed SRNN outperforms the image-based approaches using the capsule network (CapsNet) by 14.1% and the convolutional neural network (CNN) by 5.87%, respectively, in terms of root mean squared error (RMSE). Moreover, we show that the proposed model is scalable. The SRNN model trained with data of a road network is able to predict traffic speed of different road networks, with the fixed number of parameters to train.","","","10.1109/JSEN.2019.2933823","Horizon 2020 Framework Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792103","Graph theory;intelligent transportation systems;machine learning;scalability;time series analysis","Roads;Recurrent neural networks;Deep learning;Sensors;Network topology;Computer architecture;Time series analysis","","","","","20","IEEE","","","","IEEE","IEEE Journals"
"Fusing Object Semantics and Deep Appearance Features for Scene Recognition","N. Sun; W. Li; J. Liu; G. Han; C. Wu","Engineering Research Center of Wideband Wireless Communication Technology, Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Communication and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; Engineering Research Center of Wideband Wireless Communication Technology, Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China; Engineering Research Center of Wideband Wireless Communication Technology, Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China; Engineering Research Center of Wideband Wireless Communication Technology, Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","6","1715","1728","Scene images generally show the characteristics of large intra-class variety and high inter-class similarity because of complicated appearances, subtle differences, and ambiguous categorization. Hence, it is difficult to achieve satisfactory accuracy by using a single representation. For solving this issue, we present a comprehensive representation for scene recognition by fusing deep features extracted from three discriminative views, including the information of object semantics, global appearance, and contextual appearance. These views show diversity and complementarity of features. The object semantics representation of the scene image, denoted by spatial-layout-maintained object semantics features, is extracted from the output of a deep-learning-based multi-classes detector by using spatial fisher vectors, which can simultaneously encode the category and layout information of objects. A multi-direction long short-term memory-based model is built to represent contextual information of the scene image, and the activation of the fully connected layer of a convolutional neural network is used to represent the global appearance of scene image. These three kinds of deep features are then fused to draw a final conclusion for scene recognition. Extensive experiments are conducted to evaluate the proposed comprehensive representation on three benchmarks scene image database. The results show that the three deep features complement to each other strongly and are effective in improving recognition performance after fusion. The proposed method can achieve scene recognition accuracy of 89.51% on the MIT67 database, 78.93% on the SUN397 database, and 57.27% on the Places365 databases, respectively, which are better percentages than the accuracies obtained by the latest reported deep-learning-based scene recognition methods.","","","10.1109/TCSVT.2018.2848543","National Natural Science Foundation of China; Science Foundation of Ministry of Education-China Mobile Communications Corporation; Key Research and Development Foundation Project of Jiangsu province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8387777","Comprehensive representation;contextual feature;object semantics;scene recognition","Feature extraction;Semantics;Image recognition;Databases;Visualization;Hidden Markov models;Benchmark testing","convolutional neural nets;feature extraction;image recognition","deep appearance features;global appearance;contextual appearance;object semantics representation;spatial-layout-maintained object semantics features;scene recognition;deep-learning-based multiclasses detector;spatial fisher vectors;multidirection long short-term memory-based model;convolutional neural network","","","60","","","","","IEEE","IEEE Journals"
"Multilabel Annotation of Multispectral Remote Sensing Images using Error-Correcting Output Codes and Most Ambiguous Examples","A. Radoi; M. Datcu","Faculty of Electronics, Telecommunications and Information Technology, University Politehnica of Bucharest, Bucharest, Romania; Remote Sensing Technology Institute, German Aerospace Center, Oberpfaffenhofen, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","7","2121","2134","This paper presents a novel framework for multilabel classification of multispectral remote sensing images using error-correcting output codes. Starting with a set of primary class labels, the proposed framework consists in transforming the multiclass problem into multiple binary learning subtasks. The distributed output representations of these binary learners are then transformed into primary class labels. In order to train robust binary classifiers on a reduced annotated dataset, the learning process is iterative and involves determining most ambiguous examples, which are included in the training set at each iteration. As part of the semantic image recognition process, two categories of high-level image representations are proposed for the feature extraction part. First, deep convolutional neural networks are used to form high-level representations of the images. Second, we test our classification framework with a bag-of-visual words model based on the scale invariant feature transform, used in combination with color descriptors. In the first case, we propose the usage of pretrained state-of-the-art deep learning models that cancel the need to estimate model parameters of complex architectures, whereas, in the second case, a dictionary of visual words must be determined from the training set. Experiments are conducted on GeoEye-1 and Sentinel-2 images and the results show the effectiveness of the proposed approach toward a multilabel classification, when compared to other methods.","","","10.1109/JSTARS.2019.2916838","Deutsches Zentrum für Luft- und Raumfahrt; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8753507","Error-correcting output codes (ECOCs);multilabel image classification;pretrained convolutional neural networks;support vector machines (SVMs)","Training;Remote sensing;Task analysis;Neural networks;Support vector machines;Feature extraction;Semantics","convolutional neural nets;error correction codes;feature extraction;image classification;image recognition;image representation;learning (artificial intelligence);remote sensing","multispectral remote sensing images;error-correcting output codes;multilabel classification;primary class labels;multiple binary learning subtasks;distributed output representations;binary learners;robust binary classifiers;reduced annotated dataset;learning process;training set;semantic image recognition process;high-level image representations;feature extraction part;deep convolutional neural networks;high-level representations;classification framework;bag-of-visual words model;pretrained state-of-the-art deep learning models;Sentinel-2 images","","","30","OAPA","","","","IEEE","IEEE Journals"
"CNN-Based Multilayer Spatial–Spectral Feature Fusion and Sample Augmentation With Local and Nonlocal Constraints for Hyperspectral Image Classification","J. Feng; J. Chen; L. Liu; X. Cao; X. Zhang; L. Jiao; T. Yu","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Chinese Academy of Sciences, Key Laboratory of Spectral Imaging Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","4","1299","1313","The extraction of joint spatial-spectral features has been proved to improve the classification performance of hyperspectral images (HSIs). Recently, utilizing convolutional neural networks (CNNs) to learn joint spatial-spectral features has become of great interest. However, the existing CNN models ignore complementary spatial-spectral information among the shallow and deep layers. Moreover, insufficient training samples in HSIs afflict these CNN models with overfitting problem. In order to address these problems, a novel CNN method for HSI classification is proposed. It considers multilayer spatial-spectral feature fusion and sample augmentation with local and nonlocal constraints, which is abbreviated as MSLN-CNN. In MSLN-CNN, a triple-architecture CNN is constructed to extract spatial-spectral features by cascading spectral features to dual-scale spatial features from shallow to deep layers. Then, multilayer spatial-spectral features are fused to learn complementary information among the shallow layers with detailed information and the deep layers with semantic information. Finally, the multilayer spatial-spectral feature fusion and classification are integrated into a unified network, and MSLN-CNN can be optimized in the end-to-end way. To alleviate the small sample size problem, the unlabeled samples having high confidences on local spatial constraint and nonlocal spectral constraint are selected and prelabeled. The nonlocal spectral constraint considers the structure information with spectrally similar samples in the nonlocal searching, while the local spatial constraint utilizes the contextual information with spatially adjacent samples. Experimental results on several hyperspectral datasets demonstrate that the proposed method achieves more encouraging classification performance than the current state-of-the-art classification methods, especially with the limited training samples.","","","10.1109/JSTARS.2019.2900705","National Natural Science Foundation of China; China Postdoctoral Science Foundation; National Natural Science of China; Open Research Fund of Key Laboratory of Spectral Imaging Technology, Chinese Academy of Sciences; Fundamental Research Funds for the Central Universities; Postdoctoral Research Program in Shaanxi Province of China; Joint Fund of the Equipment Research of Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667086","Convolutional neural networks (CNNs);hyperspectral image (HSI) classification;multilayer feature fusion;nonlocal information;spatial–spectral feature extraction","Feature extraction;Nonhomogeneous media;Training;Deep learning;Data mining;Hyperspectral sensors","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);remote sensing","sample augmentation;MSLN-CNN;triple-architecture CNN;dual-scale spatial features;deep layers;local spatial constraint;nonlocal spectral constraint;spectrally similar samples;spatially adjacent samples;CNN-based multilayer spatial-spectral feature fusion;hyperspectral image classification;complementary spatial-spectral information;HSIs classification;joint spatial-spectral features extraction;convolutional neural networks;shallow layers;semantic information","","2","49","","","","","IEEE","IEEE Journals"
"High Efficient Deep Feature Extraction and Classification of Spectral-Spatial Hyperspectral Image Using Cross Domain Convolutional Neural Networks","Y. Guo; H. Cao; J. Bai; Y. Bai","Department of Computer Science, School of Computer Science, Shaanxi Normal Universtiy, Xi'an, China; Department of Computer Science, School of Computer Science, Shaanxi Normal Universtiy, Xi'an, China; School of Geography and Tourism, Shaanxi Normal University, Xi'an, China; School of Engineering and Computer Science, California State University, Fullerton, CA, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","1","345","356","Recently, numerous remote sensing applications highly depend on the hyperspectral image (HSI). HSI classification, as a fundamental issue, has attracted increasing attention and become a hot topic in the remote sensing community. We implemented a regularized convolutional neural network (CNN), which adopted dropout and regularization strategies to address the overfitting problem of limited training samples. Although many kinds of the literature have confirmed that it is an effective way for HSI classification to integrate spectrum with spatial context, the scaling issue is not fully exploited. In this paper, we propose a high efficient deep feature extraction and the classification method for the spectral-spatial HSI, which can make full use of multiscale spatial feature obtained by guided filter. The proposed approach is the first attempt to lean a CNN for spectral and multiscale spatial features. Compared to its counterparts, experimental results show that the proposed method can achieve 3% improvement in accuracy, according to various datasets such as Indian Pines, Pavia University, and Salinas.","","","10.1109/JSTARS.2018.2888808","Shaanxi Natural Science Foundation; Fundamental Research Funds for the Central Universities; Shandong Province Higher Educational Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8602463","Convolutional neural network (CNN);guided filter;hyperspectral image (HSI) classification;spectral-spatial fusion","Feature extraction;Hyperspectral imaging;Principal component analysis;Computer science;Deep learning","convolutional neural nets;feature extraction;geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;remote sensing","spectral-spatial hyperspectral image classification;remote sensing applications;dropout strategies;Indian Pines;Pavia University;Salinas;multiscale spatial features;spectral features;multiscale spatial feature;spectral-spatial HSI;classification method;regularization strategies;CNN;regularized convolutional neural network;remote sensing community;HSI classification;cross domain convolutional neural networks;high efficient deep feature extraction","","","41","","","","","IEEE","IEEE Journals"
"Impervious Surface Estimation From Optical and Polarimetric SAR Data Using Small-Patched Deep Convolutional Networks: A Comparative Study","H. Zhang; L. Wan; T. Wang; Y. Lin; H. Lin; Z. Zheng","Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong; Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong; Department of Natural and Resources of Hubei Province, Wuhan, China; Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong; School of Geography and Environment, Jiangxi Normal University, Nanchang, China; School of Resources and Environment, University of Electronic Science and Technology of China (UESTC), Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","7","2374","2387","Incorporating optical and polarimetric synthetic-aperture radar (SAR) data to estimate impervious surface is useful but challenging due to their different geometric imaging mechanism and the high diversity of urban land covers. The recent development of deep convolutional networks (DCN) opens a promising opportunity by automatically extracting the deep features from both data sets. In this study, a small-patched DCN (SDCN) was designed to estimate the impervious surface from optical and SAR data. Benchmark methods, e.g., GoogLeNet, VGG16, ResNet50, and the support vector machine were employed for comparison. Two study sites in the most complex metropolitan of China, the Guangdong-Hong Kong-Macau Greater Bay Area, were selected to assess the proposed method. Experimental results indicated the effectiveness of proposed SDCN with a better accuracy outperforming other benchmark methods. Furthermore, we found that 60%-80% of training samples performed comparably with the whole training set, indicating that a large number of training samples may not be necessary in all cases, depending on the settings of some factors (e.g., number of epochs). Generally, SDCN appears more suitable than other methods in terms of combining the optical and SAR data and improved the accuracy of estimating impervious surface.","","","10.1109/JSTARS.2019.2915277","Research Grants Council (RGC) General Research Fund; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8725899","convolutional neural network (CNN);deep learning;impervious surface area (ISA);synthetic-aperture radar (SAR);urban","Optical sensors;Synthetic aperture radar;Optical imaging;Feature extraction;Remote sensing;Land surface;Adaptive optics","convolutional neural nets;feature extraction;geophysical image processing;image sampling;land cover;optical radar;radar imaging;radar polarimetry;support vector machines;synthetic aperture radar","surface estimation;geometric imaging mechanism;optical SAR data;Guangdong-Hong Kong-Macau Greater Bay Area;support vector machine;SDCN;urban land covers;synthetic-aperture radar data;small-patched deep convolutional networks;polarimetric SAR data","","","77","Traditional","","","","IEEE","IEEE Journals"
"Learning Deep Correlated Representations for Nonlinear Process Monitoring","Q. Jiang; X. Yan","Key Laboratory of Advanced Control and Optimization for Chemical Processes of Ministry of Education, East China University of Science and Technology, Shanghai, China; Key Laboratory of Advanced Control and Optimization for Chemical Processes of Ministry of Education, East China University of Science and Technology, Shanghai, China","IEEE Transactions on Industrial Informatics","","2019","15","12","6200","6209","Deep neural network (DNN) extracts hierarchical representations from process data and is promising for nonlinear process monitoring. Obtaining meaningful representations and generating efficient fault detection residual are the main challenges in DNN-based monitoring. This study proposes a regularized deep correlated representation (RDCR) method that incorporates deep belief networks (DBNs) and canonical correlation analysis (CCA) for nonlinear process monitoring. Hierarchical representations are initially extracted using DBN to process input and output variables. Second, hierarchical representations from process input and output are modeled through CCA to characterize the relationship between them. Efficient fault detection residuals are then generated, and monitoring statistics are established. CCA-based monitoring relies on the most correlated representations; thus, a multiobjective evolutionary optimization-based regularization is performed to select the most correlated representations and eliminate the influence of unrelated representations. The advantages of the RDCR monitoring are verified through experimental studies on a numerical example and the Tennessee Eastman process.","","","10.1109/TII.2018.2886048","National Natural Science Foundation of China; Shanghai Pujiang; Fundamental Research Funds for the Central Universities; Introducing Talents of Discipline to Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8571266","Canonical correlation analysis (CCA);deep neural network (DNN);multiobjective evolutionary optimization (MEO);nonlinear process monitoring","Monitoring;Feature extraction;Optimization;Correlation;Fault detection;Data mining;Kernel","","","","2","42","IEEE","","","","IEEE","IEEE Journals"
"LSTM-Based SQL Injection Detection Method for Intelligent Transportation System","Q. Li; F. Wang; J. Wang; W. Li","University of Posts and Telecommunications, Beijing, China; University of Posts and Telecommunications, Beijing, China; Sichuan University, Chengdu, China; University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Vehicular Technology","","2019","68","5","4182","4191","Intelligent transportation is an emerging technology that integrates advanced sensors, network communication, data processing, and automatic control technologies to provide great convenience for our daily lives. With the increasing popularity of intelligent transportation, its security issues have also attracted much attention. SQL injection attack is one of the most common attacks in the intelligent transportation system. It has characteristics of various types, fast mutations, hidden attacks, etc., and leads to great harm. Most of the current SQL detection methods are based on manually defined features. The detection results are heavily dependent on the accuracy of feature extraction, so it cannot cope with the increasingly complex SQL injection attacks in the intelligent transportation system. In order to solve this problem, this paper proposes a long short-term memory based SQL injection attack detection method, which can automatically learn the effective representation of data, and has a strong advantage to confront with complex high-dimensional massive data. In addition, this paper proposes an injection sample generation method based on data transmission channel from the perspective of penetration. This method can formally model SQL injection attack and generate valid positive samples. It can effectively solve the over-fitting problem caused by insufficient positive samples. The experimental results show that the proposed method improves the accuracy of the SQL injection attack detection and reduces the false positive rate, which is better than several related classical machine learning algorithms and commonly used deep learning algorithms.","","","10.1109/TVT.2019.2893675","National Key Research and Development Program; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8616823","SQL injection detection;intelligent transportation system;LSTM;word2vec","SQL injection;Feature extraction;Intelligent transportation systems;Machine learning;Machine learning algorithms;Training","feature extraction;intelligent transportation systems;learning (artificial intelligence);security of data;SQL","SQL injection attack detection method;injection sample generation method;LSTM-based SQL injection detection method;intelligent transportation system;automatic control technologies;common attacks;hidden attacks;detection results;increasingly complex SQL injection attacks;deep learning algorithms;over-fitting problem","","","35","","","","","IEEE","IEEE Journals"
"Combined Static and Motion Features for Deep-Networks-Based Activity Recognition in Videos","S. Ramasinghe; J. Rajasegaran; V. Jayasundara; K. Ranasinghe; R. Rodrigo; A. A. Pasqual","Department of Electronic and Telecommunication Engineering, University of Moratuwa, Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Moratuwa, Sri Lanka","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","9","2693","2707","Activity recognition in videos in a deep-learning setting-or otherwise-uses both static and pre-computed motion components. The method of combining the two components, while keeping the burden on the deep network less, still remains uninvestigated. Moreover, it is not clear what the level of contribution of individual components is, and how to control the contribution. In this paper, we use a combination of convolutional-neural-network-generated static features and motion features in the form of motion tubes. We propose three schemas for combining static and motion components: based on a variance ratio, principal components, and Cholesky decomposition. The Cholesky-decomposition-based method allows the control of contributions. The ratio given by variance analysis of static and motion features matches well with the experimental optimal ratio used in the Cholesky decomposition-based method. The resulting activity recognition system is better or on par with the existing state-of-the-art when tested with three popular data sets. The findings also enable us to characterize a data set with respect to its richness in motion information.","","","10.1109/TCSVT.2017.2760858","National Research Council Sri Lanka; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8060555","Activity recognition;fusing features;convolutional neural networks (CNN);recurrent neural networks (RNN);long short-term memory (LSTM)","Videos;Electron tubes;Feature extraction;Activity recognition;Dynamics;Tracking;Fuses","convolutional neural nets;feature extraction;gesture recognition;image motion analysis;image representation;principal component analysis;video signal processing","motion features;deep-networks-based activity recognition;deep-learning setting;pre-computed motion components;deep network;convolutional-neural-network-generated static features;motion tubes;Cholesky decomposition-based method;motion information;static features;video activity recognition;principal components;variance ratio","","","47","","","","","IEEE","IEEE Journals"
"Hippocampus Segmentation Based on Iterative Local Linear Mapping With Representative and Local Structure-Preserved Feature Embedding","S. Pang; Z. Lu; J. Jiang; L. Zhao; L. Lin; X. Li; T. Lian; M. Huang; W. Yang; Q. Feng","Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Department of Radiotherapy, The First Affiliated Hospital of Zhengzhou University, Zhengzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Guangdong Provincial Key Laboratory of Medical Image Processing, School of Biomedical Engineering, Southern Medical University, Guangzhou, China","IEEE Transactions on Medical Imaging","","2019","38","10","2271","2280","Hippocampus segmentation plays a significant role in mental disease diagnoses, such as Alzheimer's disease, epilepsy, and so on. Patch-based multi-atlas segmentation (PBMAS) approach is a popular method for hippocampus segmentation and has achieved a promising result. However, the PBMAS approach needs high computation cost due to registration and the segmentation accuracy is subject to the registration accuracy. In this paper, we propose a novel method based on iterative local linear mapping (ILLM) with the representative and local structure-preserved feature embedding to achieve accurate and robust hippocampus segmentation with no need for registration. In the proposed approach, semi-supervised deep autoencoder (SSDA) exploits unsupervised deep autoencoder and local structure-preserved manifold regularization to nonlinearly transform the extracted magnetic resonance (MR) patch to embedded feature manifold, whose adjacent relationship is similar to the signed distance map (SDM) patch manifold. Local linear mapping is used to preliminarily predict SDM patch corresponding to the MR patch. Subsequently, threshold segmentation generates a preliminary segmentation. The ILLM refines the segmentation result iteratively by ensuring the local constraints of embedded feature manifold and SDM patch manifold using a space-constrained dictionary update. Thus, a refined segmentation is obtained with no need for registration. The experiments on 135 subjects from ADNI dataset show that the proposed approach is superior to the state-of-the-art PBMAS and classification-based approaches with mean Dice similarity coefficients of 0.8852 ± 0.0203 and 0.8783 ± 0.0251 for bilateral hippocampus segmentation of 1.5T and 3.0T datasets, respectively.","","","10.1109/TMI.2019.2906727","National Natural Science Foundation of China; Science and Technology Project of Guangdong Province; Natural Science Foundation of Guangdong Province; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672587","Deep learning;multi-atlas segmentation;iterative local linear mapping;manifold regularization","Manifolds;Image segmentation;Hippocampus;Dictionaries;Image reconstruction;Diseases;Training","biomedical MRI;diseases;feature extraction;image classification;image registration;image segmentation;iterative methods;learning (artificial intelligence);medical image processing","classification-based approaches;bilateral hippocampus segmentation;iterative local linear mapping;mental disease diagnoses;patch-based multiatlas segmentation approach;PBMAS approach;local structure-preserved feature embedding;hippocampus segmentation;semisupervised deep autoencoder;unsupervised deep autoencoder;local structure-preserved manifold regularization;extracted magnetic resonance patch;embedded feature manifold;signed distance map patch manifold;threshold segmentation;local constraints;SDM patch manifold;Dice similarity coefficients;ADNI dataset","","","27","","","","","IEEE","IEEE Journals"
"Learning Composite Latent Structures for 3D Human Action Representation and Recognition","P. Wei; H. Sun; N. Zheng","Xi’an Jiaotong University, Shaanxi, China; Xi’an Jiaotong University, Shaanxi, China; Xi’an Jiaotong University, Shaanxi, China","IEEE Transactions on Multimedia","","2019","21","9","2195","2208","3D human action representation and recognition are important issues in many multimedia applications. While latent state approaches have been widely used for action modeling, previous works assume the latent states of actions are single attribute. This assumption is inaccurate for representing structures of complex actions. In this paper, we propose that latent states have composite attributes and introduce a novel composite latent structure (CLS) model to represent and recognize 3D human actions with skeleton sequences. A human action is modeled with a hierarchical graph, which represents the action sequence as sequential atomic actions. An atomic action is represented as a composite latent state, which is composed of a latent semantic attribute and a latent geometric attribute. A discriminative EM-like algorithm is proposed to learn the model parameters and the composite latent structures of human actions. Given a 3D skeleton sequence, a composite attribute iterative programming algorithm is proposed to recognize the action and infer the action's latent temporal structure. We evaluate the proposed method on three challenging 3D action datasets-MSR 3D Action Dataset, Multiview 3D Event Dataset, and UTKinect-Action 3D Dataset. Extensive experimental results on these datasets demonstrate the effectiveness and advantage of the proposed method.","","","10.1109/TMM.2019.2897902","National Natural Science Foundation of China; China Postdoctoral Science Foundation; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636161","3D human action;action representation;action recognition;composite latent structure","Three-dimensional displays;Skeleton;Semantics;Hidden Markov models;Deep learning;Sun;Solid modeling","gesture recognition;graph theory;image motion analysis;image recognition;image representation;image sequences;iterative methods;learning (artificial intelligence);multimedia communication;stereo image processing","latent semantic attribute;latent geometric attribute;3D skeleton sequence;UTKinect-Action 3D Dataset;3D human action representation;action sequence;sequential atomic actions;composite latent structure model;iterative programming algorithm","","1","77","OAPA","","","","IEEE","IEEE Journals"
"Analysis of Spatio-Temporal Representations for Robust Footstep Recognition with Deep Residual Neural Networks","O. Costilla-Reyes; R. Vera-Rodriguez; P. Scully; K. B. Ozanyan","The University of Manchester, Manchester, United Kingdom; Universidad Autonoma de Madrid, Madrid, Spain; Faculty of Engineering and Physical Sciences, The University of Manchester, Manchester, United Kingdom; The University of Manchester, Manchester, United Kingdom","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","2","285","296","Human footsteps can provide a unique behavioural pattern for robust biometric systems. We propose spatio-temporal footstep representations from floor-only sensor data in advanced computational models for automatic biometric verification. Our models deliver an artificial intelligence capable of effectively differentiating the fine-grained variability of footsteps between legitimate users (clients) and impostor users of the biometric system. The methodology is validated in the largest to date footstep database, containing nearly 20,000 footstep signals from more than 120 users. The database is organized by considering a large cohort of impostors and a small set of clients to verify the reliability of biometric systems. We provide experimental results in 3 critical data-driven security scenarios, according to the amount of footstep data made available for model training: at airports security checkpoints (smallest training set), workspace environments (medium training set) and home environments (largest training set). We report state-of-the-art footstep recognition rates with an optimal equal false acceptance and false rejection rate (equal error rate) of 0.7 percent an improvement ratio of 371 percent compared to previous state-of-the-art. We perform a feature analysis of deep residual neural networks showing effective clustering of client's footstep data and to provide insights of the feature learning process.","","","10.1109/TPAMI.2018.2799847","Cognimetrics TEC2015-70627-R MINECO/FEDER; EPSRC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8275035","Biometric system;verification system;deep learning;footstep recognition;floor sensor system","Biological system modeling;Hidden Markov models;Biometrics (access control);Databases;Data models;Sensor systems;Security","artificial intelligence;biometrics (access control);feature extraction;neural nets;pattern classification;pattern clustering;sensor fusion","feature analysis;deep residual neural networks;robust footstep recognition;human footsteps;robust biometric systems;floor-only sensor data;automatic biometric verification;airports security checkpoints;home environments;behavioural pattern;artificial intelligence;spatiotemporal footstep representations;workspace environments;footstep data clustering","","2","39","","","","","IEEE","IEEE Journals"
"Online Proactive Caching in Mobile Edge Computing Using Bidirectional Deep Recurrent Neural Network","L. Ale; N. Zhang; H. Wu; D. Chen; T. Han","Department of Computing Sciences, Texas A&M University at Corpus Christi, Corpus Christi, TX, USA; Department of Computing Sciences, Texas A&M University at Corpus Christi, Corpus Christi, TX, USA; National Engineering Laboratory for Mobile Network Technologies, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical and Computer Engineering, University of North Carolina at Charlotte, Charlotte, NC, USA","IEEE Internet of Things Journal","","2019","6","3","5520","5530","With emergence of Internet of Things (IoT), wireless traffic has grown dramatically, posing severe strain on core network and backhaul bandwidth. Proactive caching in mobile edge computing systems can not only efficiently mitigate the traffic congestion and relieve burden of backhaul but also can reduce the service latency for end devices. However, proactive caching heavily relies on the prediction accuracy of content popularity, which is typically unknown and change over time. In this paper, we propose an online proactive caching scheme based on bidirectional deep recurrent neural network (BRNN) model to predict time-series content requests and update edge caching accordingly. Specifically, on the first layer, a 1-D convolution neural network (CNN) is devised to reduce the computational costs. Then, BRNN is employed to predict time-varying requests from users. Afterward, a fully connected neural network (FCNN) is harnessed to learn and sample predicts from the BRNN. Finally, we conduct experiments based on real datasets, which demonstrate that the proposed approach can achieve considerably high prediction accuracy and significantly improve content hit rate of end devices.","","","10.1109/JIOT.2019.2903245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660445","Caching;convolution neural network (CNN);deep learning;mobile edge computing (MEC);recurrent neural network","Predictive models;Computational modeling;Internet of Things;Edge computing;Cloud computing;Streaming media;Training","convolutional neural nets;mobile computing;recurrent neural nets;time series","wireless traffic;backhaul bandwidth;mobile edge computing systems;traffic congestion;online proactive caching scheme;bidirectional deep recurrent neural network model;BRNN;time-varying requests;fully connected neural network;1D convolution neural network;Internet of Things;IoT;time-series content request prediction;edge caching updating;CNN;FCNN","","1","45","","","","","IEEE","IEEE Journals"
"GenSyth: a new way to understand deep learning","A. Wong; M. J. Shafiee; B. Chwyl; F. Li","Waterloo Artificial Intelligence Institute, Canada; Waterloo Artificial Intelligence Institute, Canada; DarwinAI Corp, Canada; DarwinAI Corp, Canada","Electronics Letters","","2019","55","18","970","971","","","","10.1049/el.2019.2376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8822900","","","","","","","11","","","","","IET","IET Journals"
"CNN-Based Adversarial Embedding for Image Steganography","W. Tang; B. Li; S. Tan; M. Barni; J. Huang","School of Electronics and Information Technology, Sun Yat-sen University, Guangdong, China; Guangdong Key Laboratory of Intelligent Information Processing and Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing and Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, China; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy; Guangdong Key Laboratory of Intelligent Information Processing and Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, China","IEEE Transactions on Information Forensics and Security","","2019","14","8","2074","2087","Steganographic schemes are commonly designed in a way to preserve image statistics or steganalytic features. Since most of the state-of-the-art steganalytic methods employ a machine learning (ML)-based classifier, it is reasonable to consider countering steganalysis by trying to fool the ML classifiers. However, simply applying perturbations on stego images as adversarial examples may lead to the failure of data extraction and introduce unexpected artifacts detectable by other classifiers. In this paper, we present a steganographic scheme with a novel operation called adversarial embedding (ADV-EMB), which achieves the goal of hiding a stego message while at the same time fooling a convolutional neural network (CNN)-based steganalyzer. The proposed method works under the conventional framework of distortion minimization. In particular, ADV-EMB adjusts the costs of image elements modifications according to the gradients back propagated from the target CNN steganalyzer. Therefore, modification direction has a higher probability to be the same as the inverse sign of the gradient. In this way, the so-called adversarial stego images are generated. Experiments demonstrate that the proposed steganographic scheme achieves better security performance against the target adversary-unaware steganalyzer by increasing its missed detection rate. In addition, it deteriorates the performance of other adversary-aware steganalyzers, opening the way to a new class of modern steganographic schemes capable of overcoming powerful CNN-based steganalysis.","","","10.1109/TIFS.2019.2891237","National Natural Science Foundation of China; Shenzhen Research Development Program; Alibaba Group through the Alibaba Innovative Research (AIR) Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603808","Steganography;steganalysis;adversarial machine learning","Distortion;Perturbation methods;Deep learning;Feature extraction;Minimization;Security;Image coding","convolutional neural nets;image processing;learning (artificial intelligence);steganography","target adversary-unaware steganalyzer;adversary-aware steganalyzers;modern steganographic schemes;CNN-based adversarial embedding;image steganography;image statistics;machine learning-based classifier;ML classifiers;data extraction;stego message;image elements modifications;target CNN steganalyzer;adversarial stego images;convolutional neural network;ADV-EMB","","3","51","","","","","IEEE","IEEE Journals"
"Edge-Assisted Vehicle Mobility Prediction to Support V2X Communications","W. Liu; Y. Shoji","Open Innovation Promotion Headquarters, National Institute of Information and Communications Technology, Tokyo, JP.; Open Innovation Promotion Headquarters, National Institute of Information and Communications Technology, Tokyo, JP.","IEEE Transactions on Vehicular Technology","","2019","68","10","10227","10238","Vehicle-to-everything (V2X) communications have a great potential of enabling future intelligent vehicle applications, and exploiting vehicle mobility is of great importance in designing efficient V2X protocols and applications. Thus, this paper proposes a novel edge-assisted algorithm that makes use of the resources in both cloud and edge sides of vehicular networks to predict vehicle mobility. The proposed algorithm adopts a hybrid architecture of convolutional and recurrent neural networks, and enables computationally efficient transfer learning in each vehicle to generate its customized mobility prediction model. Extensive evaluations have been conducted by using a real taxi mobility data set that is obtained from a testbed deployed in Tokyo, Japan. The results have validated that, compared with other state-of-art algorithms, our proposal improves the prediction F1 score of vehicle mobility by more than 30%, especially for those vehicles that own a strong individual mobility preference.","","","10.1109/TVT.2019.2937825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818318","Vehicle-to-everything communications;mobility prediction;transfer learning","Prediction algorithms;Vehicle-to-everything;Predictive models;Sensors;Handover;Deep learning","convolutional neural nets;learning (artificial intelligence);mobile computing;mobility management (mobile radio);recurrent neural nets;traffic engineering computing;vehicular ad hoc networks","edge-assisted vehicle mobility prediction;Vehicle-to-everything communications;future intelligent vehicle applications;edge-assisted algorithm;cloud sides;edge sides;convolutional neural networks;recurrent neural networks;customized mobility prediction model;strong individual mobility preference;taxi mobility data set;V2X protocols;V2X communications;vehicular networks;transfer learning","","","29","Traditional","","","","IEEE","IEEE Journals"
"Deep residual refining based pseudo-multi-frame network for effective single image super-resolution","K. Mei; A. Jiang; J. Li; B. Liu; J. Ye; M. Wang","School of Computer and Information Engineering, Jiangxi Normal University, People's Republic of China; School of Computer and Information Engineering, Jiangxi Normal University, People's Republic of China; School of Computer Science and Software Engineering, East China Normal University, People's Republic of China; College of Computer Science and Software Engineering, Auburn University, USA; School of Computer and Information Engineering, Jiangxi Normal University, People's Republic of China; School of Computer and Information Engineering, Jiangxi Normal University, People's Republic of China","IET Image Processing","","2019","13","4","591","599","Single image super-resolution (SISR) has gained great attraction and progress in recent years. Since the SISR is an ill-posed inverse problem, most researchers are concentrated on making efforts to learn effective and reasonable mapping functions from low-resolution observation to its potential high-resolution (HR) counterpart. In this study, the authors have proposed a deep residual refining based pseudo-multi-frame network for efficient SISR. A channel-wise attention mechanism is employed for residual refinement. It can ease residual learning process through explicitly modelling non-linear dependencies between channels by using global information embedding. Multiple potential HRs from different deconvolutional layers are further artificially learned, and then adaptively fused into final desired HR image. The authors call this strategy as pseudo-multi-frame SR. It could make full use of available redundant information possessed in hierarchical layers. They have evaluated the proposed network on several popular benchmark datasets. The experimental results have shown that the two highlights proposed can consistently boost final performance. The proposed network can outperform most of the state-of-the-art methods with acceptable less parameters.","","","10.1049/iet-ipr.2018.6057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695183","","","image resolution;learning (artificial intelligence);inverse problems;convolution","deep residual refining;pseudomultiframe network;channel-wise attention mechanism;residual learning process;global information embedding;inverse problem;SISR;single image super-resolution;deconvolutional layers","","","","","","","","IET","IET Journals"
"Unsupervised Hyperspectral Image Band Selection Based on Deep Subspace Clustering","M. Zeng; Y. Cai; Z. Cai; X. Liu; P. Hu; J. Ku","School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Automation, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; Hainan Institute of Science and Technology, Haikou, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","12","1889","1893","Hyperspectral image (HSI) consists of hundreds of continuous narrow bands with high redundancy, resulting in the curse of dimensionality and an increased computation complexity in HSI classification. Many clustering-based band selection approaches have been proposed to deal with such a problem. However, a few of them consider the spectral and spatial relationship simultaneously. In this letter, we proposed a novel clustering-based band selection approach using deep subspace clustering (DSC). The proposed approach combines the subspace clustering task into a convolutional autoencoder by treating it as a self-expressive layer, enabling it to be trained end to end. The resulting network can fully extract the interaction of spectral bands based on using spatial information and nonlinear feature transformation. We compared the results of the proposed method with existing band selection methods for three widely used HSI data sets, showing that the proposed method is able to accurately select an informative band subset with remarkable classification accuracy.","","","10.1109/LGRS.2019.2912170","National Natural Science Foundation of China; China University of Geosciences (Wuhan) through the Fundamental Research Funds for National University; Natural Science Foundation of Hubei Province; Open Research Project of Hubei Key Laboratory of Intelligent Geo-Information Processing; Key Project of Hainan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715383","Band selection;deep convolutional autoencoder (CAE);hyperspectral image (HSI);subspace clustering","Hyperspectral imaging;Clustering algorithms;Feature extraction;Geology;Sensor phenomena and characterization","computational complexity;feature extraction;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);pattern clustering;remote sensing","unsupervised hyperspectral image band selection;deep subspace clustering;continuous narrow bands;increased computation complexity;HSI classification;clustering-based band selection approaches;spectral relationship;spatial relationship;novel clustering-based band selection approach;subspace clustering task;spectral bands;band selection methods;informative band subset","","","19","IEEE","","","","IEEE","IEEE Journals"
"Automatic Land Cover Reconstruction From Historical Aerial Images: An Evaluation of Features Extraction and Classification Algorithms","R. Ratajczak; C. F. Crispim-Junior; E. Faure; B. Fervers; L. Tougne","Laboratoire d’Informatique en Image et Systèmes d’information, University of Lyon, University of Lyon 2, Lyon, France; Laboratoire d’Informatique en Image et Systèmes d’information, University of Lyon, University of Lyon 2, Lyon, France; Leon Bérard Center, Lyon, France; Leon Bérard Center, Lyon, France; Laboratoire d’Informatique en Image et Systèmes d’information, University of Lyon, University of Lyon 2, Lyon, France","IEEE Transactions on Image Processing","","2019","28","7","3357","3371","The land cover reconstruction from monochromatic historical aerial images is a challenging task that has recently attracted an increasing interest from the scientific community with the proliferation of large-scale epidemiological studies involving retrospective analysis of spatial patterns. However, the efforts made by the computer vision community in remote-sensing applications are mostly focused on prospective approaches through the analysis of high-resolution multi-spectral data acquired by the advanced spatial programs. Hence, four contributions are proposed in this paper. They aim at providing a comparison basis for the future development of computer vision algorithms applied to the automation of the land cover reconstruction from monochromatic historical aerial images. First, a new multi-scale multi-date dataset composed of 4.9 million non-overlapping annotated patches of the France territory between 1970 and 1990 has been created with the help of geography experts. This dataset has been named HistAerial. Second, an extensive comparison study of the state-of-the-art texture features extraction and classification algorithms, including deep convolutional neural networks (DCNNs), has been performed. It is presented in the form of an evaluation. Third, a novel low-dimensional local texture filter named rotated-corner local binary pattern (R-CRLBP) is presented as a simplification of the binary gradient contours filter through the use of an orthogonal combination representation. Finally, a novel combination of low-dimensional texture descriptors, including the R-CRLBP filter, is introduced as a light combination of local binary patterns (LCoLBPs). The LCoLBP filter achieved state-of-the-art results on the HistAerial dataset while conserving a relatively low-dimensional feature vector space compared with the DCNN approaches (17 times shorter).","","","10.1109/TIP.2019.2896492","French Environment and Energy Management Agency (ADEME); Rhône-Alpes Health-Environment Interface Platform (ENVITERA); Labex; Léon Bérard Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630683","Features extraction;texture filters;deep convolutional neural networks;deep learning;machine learning;land cover;historical aerial images","Satellites;Earth;Remote sensing;Image reconstruction;Feature extraction;Image resolution;Image segmentation","computer vision;convolutional neural nets;feature extraction;filtering theory;geophysical image processing;image classification;image reconstruction;image resolution;image texture;land cover;remote sensing","automatic land cover reconstruction;classification algorithms;monochromatic historical aerial images;scientific community;large-scale epidemiological studies;retrospective analysis;spatial patterns;computer vision community;remote-sensing applications;high-resolution multispectral data;advanced spatial programs;computer vision algorithms;multiscale multidate dataset;nonoverlapping annotated patches;novel low-dimensional local texture filter;rotated-corner local binary pattern;low-dimensional texture descriptors;local binary patterns;low-dimensional feature vector space;texture features extraction;LCoLBP filter;HistAerial dataset;deep convolutional neural networks","","1","65","","","","","IEEE","IEEE Journals"
"Importance-Aware Semantic Segmentation for Autonomous Vehicles","B. Chen; C. Gong; J. Yang","Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Intelligent Transportation Systems","","2019","20","1","137","148","Semantic segmentation (SS) partitions an image into several coherent semantically meaningful parts and classifies each part into one of the pre-determined classes. In this paper, we argue that the existing SS methods cannot be reliably applied to autonomous driving system as they ignore the different importance levels of distinct classes for safe driving. For example, pedestrian, car, and bicyclist in the scene are much more important than sky and building when driving a car, so their segmentations should be as accurate as possible. To incorporate the importance information possessed by various object classes, this paper designs an “importance-aware loss” (IAL) that specifically emphasizes the critical objects for autonomous driving. The IAL operates under a hierarchical structure and the classes with different importance are located in different levels so that they are assigned distinct weights. Furthermore, we derive the forward and backward propagation rules for IAL and apply them to four typical deep neural networks for realizing SS in an intelligent driving system. The experiments on CamVid and Cityscapes data sets reveal that, by employing the proposed loss function, the existing deep learning models, including FCN, SegNet, ENet, and ERFNet, are able to consistently obtain the improved segmentation results on the pre-defined important classes for safe driving.","","","10.1109/TITS.2018.2801309","National Natural Science Foundation of China; 973 Program; Program for Changjiang Scholars; Natural Science Foundation of Jiangsu Province; Six Talent Peaks Project in Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8319519","Semantic segmentation;importance-aware loss;deep leaning;autonomous driving","Image segmentation;Autonomous vehicles;Roads;Neural networks;Feature extraction;Semantics;Reliability","automobiles;image classification;image segmentation;intelligent transportation systems;learning (artificial intelligence);mobile robots;neural nets;object recognition;road safety;robot vision","importance-aware semantic segmentation;autonomous driving system;safe driving;importance-aware loss;IAL;intelligent driving system;deep neural networks;deep learning models;FCN;SegNet;ENet;ERFNet;CamVid data set;Cityscapes data set","","1","45","","","","","IEEE","IEEE Journals"
"A Two-Stage Approach for the Remaining Useful Life Prediction of Bearings Using Deep Neural Networks","M. Xia; T. Li; T. Shu; J. Wan; C. W. de Silva; Z. Wang","Department of Mechanical Engineering, University of British Columbia, Vancouver, BC, Canada; Department of Mechanical Engineering, University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; School of Mechanical and Automotive Engineering, South China University of Technology, Guangzhou, China; Department of Mechanical Engineering, University of British Columbia, Vancouver, BC, Canada; School of Mechanical Engineering, Hubei University of Arts and Science, Xiangyang, China","IEEE Transactions on Industrial Informatics","","2019","15","6","3703","3711","The degradation of bearings plays a key role in the failures of industrial machinery. Prognosis of bearings is critical in adopting an optimal maintenance strategy to reduce the overall cost and to avoid unwanted downtime or even casualties by estimating the remaining useful life (RUL) of the bearings. Traditional data-driven approaches of RUL prediction rely heavily on manual feature extraction and selection using human expertise. This paper presents an innovative two-stage automated approach to estimate the RUL of bearings using deep neural networks (DNNs). A denoising autoencoder-based DNN is used to classify the acquired signals of the monitored bearings into different degradation stages. Representative features are extracted directly from the raw signal by training the DNN. Then, regression models based on shallow neural networks are constructed for each health stage. The final RUL result is obtained by smoothing the regression results from different models. The proposed approach has achieved satisfactory prediction performance for a real bearing degradation dataset with different working conditions.","","","10.1109/TII.2018.2868687","Natural Sciences and Engineering Research Council of Canada; Canada Foundation for Innovation; British Columbia Knowledge Development Fund; National Key Research and Development Program of China; Science and Technology Program of Guangzhou, China; Key Program of Natural Science Foundation of Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454498","Bearings;deep neural networks (DNNs);prognosis;remaining useful life (RUL) prediction","Feature extraction;Degradation;Neural networks;Training;Hidden Markov models;Prognostics and health management;Noise reduction","condition monitoring;cost reduction;failure analysis;feature extraction;feature selection;learning (artificial intelligence);machine bearings;machinery;maintenance engineering;neural nets;regression analysis;remaining life assessment;signal classification;signal denoising;vibrational signal processing","remaining useful life prediction;deep neural networks;optimal maintenance strategy;two-stage automated approach;denoising autoencoder-based DNN;data-driven approach;feature extraction;industrial machinery failure;cost reduction;human expertise;regression model;faeture selection;signal classification;vibrational signal processing","","4","25","","","","","IEEE","IEEE Journals"
"Improving Mispronunciation Detection of Mandarin Tones for Non-Native Learners With Soft-Target Tone Labels and BLSTM-Based Deep Tone Models","W. Li; N. F. Chen; S. M. Siniscalchi; C. Lee","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Agency for Science, Technology, and Research (A*STAR), Institute of Infocomm Research (I2R), Singapore; Department of Computer Engineering, Kore University of Enna, Enna, Italy; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","12","2012","2024","We investigate the effectiveness of soft-target tone labels and sequential context information for mispronunciation detection of Mandarin lexical tones pronounced by second language (L2) learners whose first language (L1) is of European origin. In conventional approaches, prosodic information (e.g., F0 and tone posteriors extracted from trained tone models) is used to calculate goodness of pronunciation (GOP) scores or train binary classifiers to verify pronunciation correctness. We propose three techniques to improve detection of mispronunciation of Mandarin tones for non-native learners. First, we extend our tone model from a deep neural network (DNN) to a bidirectional long short-term memory (BLSTM) network in order to more accurately model the high variability of non-native tone productions and the contextual information expressed in tone-level co-articulation. Second, we characterize ambiguous pronunciations where L2 learners' tone realizations are between two canonical tone categories by relaxing hard target labels to soft targets with probabilistic transcriptions. Third, segmental tone features fed into verifiers are extracted by a BLSTM to exploit sequential context information to improve mispronunciation detection. Compared to DNN-GOP trained with hard targets, the proposed BLSTM-GOP framework trained with soft targets reduces the tones' averaged equal error rate (ERR) from 7.58% to 5.83% and the averaged area under ROC curve (AUC) is increased from 97.85% to 98.31%. By utilizing BLSTM-based verifiers the EER further decreases to 5.16%, and the AUC is increased to 98.47%.","","","10.1109/TASLP.2019.2936755","China Scholarship Council; NFR AULUS project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809229","Computer-assisted language learning (CALL);computer-assisted pronunciation training (CAPT);non-native tone modeling and mispronunciation detection;deep learning","Feature extraction;Production;Context modeling;Acoustics;Data mining;Hidden Markov models;Speech processing","natural language processing;neural nets;pattern classification;speech processing;speech recognition","mispronunciation detection;nonnative learners;soft-target tone labels;sequential context information;Mandarin lexical tones;language learners whose first language;tone posteriors;trained tone models;train binary classifiers;short-term memory network;nonnative tone productions;tone-level co-articulation;L2 learners;canonical tone categories;hard target labels;soft targets;segmental tone features;hard targets;BLSTM-based verifiers","","","70","Traditional","","","","IEEE","IEEE Journals"
"Learning Structural Representations via Dynamic Object Landmarks Discovery for Sketch Recognition and Retrieval","H. Zhang; P. She; Y. Liu; J. Gan; X. Cao; H. Foroosh","State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Educational Informatization for Nationalities, Yunnan Normal University, Ministry of Education, Kunming, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, University of Central Florida, Orlando, FL, USA","IEEE Transactions on Image Processing","","2019","28","9","4486","4499","State-of-the-art methods on sketch classification and retrieval are based on deep convolutional neural network to learn representations. Although deep neural networks have the ability to model images with hierarchical representations by convolution kernels, they cannot automatically extract the structural representations of object categories in a human-perceptible way. Furthermore, sketch images usually have large-scale visual variations caused by the styles of drawing or viewpoints, which make it difficult to develop generalized representations using the fixed computational mode of convolutional kernel. In this paper, our aim is to address the problem of fixed computational mode in feature extraction process without extra supervision. We propose a novel architecture to dynamically discover the object landmarks and learn the discriminative structural representations. Our model is composed of two components: a representative landmark discovering module that localizes the key points on the object and a category-aware representation learning module that develops the category-specific features. Specifically, we develop a structure-aware offset layer to dynamically localize the representative landmarks, which is optimized based on the category labels without extra supervision. After that, a diversity branch is introduced to extract the global discriminative features for each category. Finally, we employ a multi-task loss function to develop an end-to-end trainable architecture. At testing time, we fuse all the predictions with different number of landmarks to achieve the final results. Through extensive experiments, we compare our model with several state-of-the-art methods on two challenging datasets, TU-Berlin and Sketchy, for sketch classification and retrieval, and the experimental results demonstrate the effectiveness of our proposed model.","","","10.1109/TIP.2019.2910398","National Key R&D Program of China; National Natural Science Foundation of China; Natural Science Foundation of Beijing Municipality; CCF-Tencent Open Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8694004","Sketch based image classification and retrieval;dynamic landmarks discovery;structural feature representation","Shape;Visualization;Kernel;Feature extraction;Three-dimensional displays;Convolution;Computational modeling","convolutional neural nets;feature extraction;image classification;image recognition;image representation;image retrieval;learning (artificial intelligence)","end-to-end trainable architecture;global discriminative features;structure-aware offset layer;category-aware representation learning module;representative landmark discovering module;discriminative structural representations;feature extraction process;convolutional kernel;large-scale visual variations;hierarchical representations;deep convolutional neural network;sketch classification;sketch recognition;dynamic object landmarks discovery","","","57","","","","","IEEE","IEEE Journals"
"Automated Layer Segmentation of Retinal Optical Coherence Tomography Images Using a Deep Feature Enhanced Structured Random Forests Classifier","X. Liu; T. Fu; Z. Pan; D. Liu; W. Hu; J. Liu; K. Zhang","College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan, China; College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan, China; Hubei Province Key Laboratory of Intelligent Information Processing and Real-Time Industrial System, Wuhan, China; College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan, China; College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan, China; College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan, China; College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan, China","IEEE Journal of Biomedical and Health Informatics","","2019","23","4","1404","1416","Optical coherence tomography (OCT) is a high-resolution and noninvasive imaging modality that has become one of the most prevalent techniques for ophthalmic diagnosis. Retinal layer segmentation is very crucial for doctors to diagnose and study retinal diseases. However, manual segmentation is often a time-consuming and subjective process. In this work, we propose a new method for automatically segmenting retinal OCT images, which integrates deep features and hand-designed features to train a structured random forests classifier. The deep convolutional features are learned from deep residual network. With the trained classifier, we can get the contour probability graph of each layer; finally, the shortest path is employed to achieve the final layer segmentation. The experimental results show that our method achieves good results with the mean layer contour error of 1.215 pixels, whereas that of the state of the art was 1.464 pixels, and achieves an F1-score of 0.885, which is also better than 0.863 that is obtained by the state of the art method.","","","10.1109/JBHI.2018.2856276","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Natural Science Foundation of Hubei Province; Foundation of Wenzhou Science and Technology Bureau; Natural Science Foundation of Zhejiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411332","OCT;image processing;convolutional neural networks;structured random forests;layer segmentation","Forestry;Image segmentation;Retina;Feature extraction;Decision trees;Training;Task analysis","biomedical optical imaging;diseases;eye;feature extraction;image classification;image segmentation;medical image processing;optical tomography;probability","retinal optical coherence tomography images;deep feature enhanced structured random forests classifier;noninvasive imaging modality;prevalent techniques;ophthalmic diagnosis;retinal layer segmentation;retinal diseases;manual segmentation;subjective process;retinal OCT images;deep features;hand-designed features;deep convolutional features;deep residual network;trained classifier;contour probability graph;final layer segmentation;mean layer contour error","","4","62","","","","","IEEE","IEEE Journals"
"Supervised-Learning for Multi-Hop MU-MIMO Communications With One-Bit Transceivers","D. Kim; S. Hong; N. Lee","Department of Electrical Engineering, POSTECH, Pohang, South Korea; Department of Electrical and Computer Engineering, Ajou University, Suwon, South Korea; Department of Electrical Engineering, POSTECH, Pohang, South Korea","IEEE Journal on Selected Areas in Communications","","2019","37","11","2559","2572","This paper considers a nonlinear multi-hop multi-user multiple-input multiple-output (MU-MIMO) relay channel, in which multiple users send information symbols to a multi-antenna base station (BS) with one-bit analog-to-digital converters via intermediate relays, each with one-bit transceiver. To understand the fundamental limit of the detection performance, the optimal maximum-likelihood (ML) detector is proposed with the assumption of perfect and global channel state information (CSI) at the BS. This multi-user detector, however, is not practical due to the unrealistic CSI assumption and the overwhelming detection complexity. These limitations are addressed by presenting a novel detection framework inspired by supervised-learning. The key idea is to model the complicated multi-hop MU-MIMO channel as a simplified channel with much fewer and learnable parameters. One major finding is that, even using the simplified channel model, a near ML detection performance is achievable with a reasonable amount of pilot overheads in a certain condition. In addition, an online supervised-learning detector is proposed, which adaptively tracks channel variations. The idea is to update the model parameters with a reliably detected data symbol by treating it as a new training (labeled) data. Lastly, a multi-user detector using a deep neural network is proposed. Unlike the model-based approaches, this model-free approach enables to remove the errors in the simplified channel model, while increasing the computational complexity for parameter learning. Via simulations, the detection performances of classical, model-based, and model-free detectors are thoroughly compared to demonstrate the effectiveness of the supervised-learning approaches in this channel.","","","10.1109/JSAC.2019.2933965","Samsung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792182","Multi-user multiple-input multiple-output (MU-MIMO);multihop relay networks;machine learning","Relays;Detectors;Computational modeling;MIMO communication;Channel models;Training;Adaptation models","analogue-digital conversion;maximum likelihood detection;MIMO communication;multiuser detection;neural nets;radio transceivers;relay networks (telecommunication);supervised learning;telecommunication computing;telecommunication network reliability;wireless channels","multihop MU-MIMO communications;one-bit transceiver;multiantenna base station;one-bit analog-to-digital converters;maximum-likelihood detector;channel state information;multiuser detector;multihop MU-MIMO channel;supervised-learning detector;data symbol;model-free detectors;nonlinear multihop multi-user multiple-input multiple-output relay channel","","1","33","","","","","IEEE","IEEE Journals"
"Multi-Scale Interpretation Model for Convolutional Neural Networks: Building Trust Based on Hierarchical Interpretation","X. Cui; D. Wang; Z. J. Wang","Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada","IEEE Transactions on Multimedia","","2019","21","9","2263","2276","With the rapid development of deep learning models, their performances in various tasks have improved; meanwhile, their increasingly intricate architectures make them difficult to interpret. To tackle this challenge, model interpretability is essential and has been investigated in a wide range of applications. For end users, model interpretability can be used to build trust in the deployed machine learning models. For practitioners, interpretability plays a critical role in model explanation, model validation, and model improvement to develop a faithful model. In this paper, we propose a novel Multi-scale Interpretation (MINT) model for convolutional neural networks using both the perturbation-based and the gradient-based interpretation approaches. It learns the class-discriminative interpretable knowledge from the multi-scale perturbation of feature information in different layers of deep networks. The proposed MINT model provides the coarse-scale and the fine-scale interpretations for the attention in the deep layer and specific features in the shallow layer, respectively. Experimental results show that the MINT model presents the class-discriminative interpretation of the network decision and explains the significance of the hierarchical network structure.","","","10.1109/TMM.2019.2902099","Natural Sciences and Engineering Research Council of Canada; Four-Year Doctoral Fellowship; University of British Columbia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653995","Model interpretability;multi-scale interpretation;convolutional neural networks;model-agnostic","Visualization;Computational modeling;Analytical models;Feature extraction;Perturbation methods;Image segmentation;Heating systems","convolutional neural nets;formal verification;learning (artificial intelligence);pattern classification","convolutional neural networks;deep learning models;model interpretability;model explanation;model validation;class-discriminative interpretable knowledge;deep networks;machine learning models;gradient-based interpretation;multiscale interpretation model;perturbation-based interpretation","","","21","Traditional","","","","IEEE","IEEE Journals"
"Joint Image Filtering with Deep Convolutional Networks","Y. Li; J. Huang; N. Ahuja; M. Yang","School of Engineering, University of California, Merced, CA, USA; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Champaign, IL, USA; School of Engineering, University of California, Merced, CA, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","8","1909","1923","Joint image filters leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for suppressing noise or enhancing spatial resolution. Existing methods either rely on various explicit filter constructions or hand-designed objective functions, thereby making it difficult to understand, improve, and accelerate these filters in a coherent framework. In this paper, we propose a learning-based approach for constructing joint filters based on Convolutional Neural Networks. In contrast to existing methods that consider only the guidance image, the proposed algorithm can selectively transfer salient structures that are consistent with both guidance and target images. We show that the model trained on a certain type of data, e.g., RGB and depth images, generalizes well to other modalities, e.g., flash/non-Flash and RGB/NIR images. We validate the effectiveness of the proposed joint filter through extensive experimental evaluations with state-of-the-art methods.","","","10.1109/TPAMI.2018.2890623","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598855","Joint filtering;deep convolutional neural networks;depth upsampling","Feature extraction;Linear programming;Task analysis;Visualization;Optimization;Image edge detection;Computational modeling","convolutional neural nets;image colour analysis;image denoising;image filtering;image resolution;learning (artificial intelligence)","joint image filtering;deep Convolutional Networks;joint image filters;guidance image;target image;noise;explicit filter constructions;hand-designed objective functions;Convolutional Neural Networks;target images;depth images;RGB images;NIR images","","","48","","","","","IEEE","IEEE Journals"
"An Embedded Computer-Vision System for Multi-Object Detection in Traffic Surveillance","A. Mhalla; T. Chateau; S. Gazzah; N. E. B. Amara","Institut Pascal, Clermont Auvergne University, Campus Universitaire des Cezeaux, Aubiere, France; Institut Pascal, Clermont Auvergne University, Campus Universitaire des Cezeaux, Aubiere, France; LATIS, ENISo, National Engineering School of Sousse, University of Sousse, Sousse, Tunisia; LATIS, ENISo, National Engineering School of Sousse, University of Sousse, Sousse, Tunisia","IEEE Transactions on Intelligent Transportation Systems","","2019","20","11","4006","4018","Intelligent traffic systems for traffic surveillance and monitoring have become a topic of great interest to some cities in the world. Generally, the existing traffic surveillance systems are made up of costly equipment with complicated operational procedures and have difficulties with congestion, occlusion, and lighting night/day and day/night transitions. In this paper, we propose an embedded system for traffic surveillance that can be utilized under these challenging conditions. This system analyses traffic and particularly focuses on the problem of detecting and categorizing traffic objects in several traffic scenarios. Moreover, it contains a robust detector produced by an original specialization framework. The proposed specialization framework utilizes a generic deep detector so as to improve the detection accuracy in a specific traffic scenario. The experiments demonstrate that the proposed specialization framework presents encouraging results for multi-traffic object detection and outperforms the state-of-the-art specialization frameworks on several public traffic datasets.","","","10.1109/TITS.2018.2876614","Tunisian Ministry of Higher Education and Scientific Research and the French Government Research Program “Investissements d’avenir” through the IMobS3 Laboratory of Excellence; European Union through the Program Regional Competitiveness and Employment 2007–2013 (ERDF–Auvergne region); Auvergne Region; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8546791","Traffic surveillance;embedded system;transfer learning;deep learning;MF R-CNN;likelihood function;traffic object detection","Detectors;Surveillance;Object detection;Training;Embedded systems;Computer vision","computer vision;embedded systems;intelligent transportation systems;learning (artificial intelligence);object detection;road traffic;telecommunication traffic;traffic engineering computing;video surveillance","embedded computer-vision system;intelligent traffic systems;specialization framework;multitraffic object detection;public traffic datasets;traffic surveillance systems;operational procedures;generic deep detector","","1","44","","","","","IEEE","IEEE Journals"
"Multi-scale Cross-path Concatenation Residual Network for Poisson denoising","Y. Su; Q. Lian; X. Zhang; B. Shi; X. Fan","Yanshan University, People's Republic of China; Yanshan University, People's Republic of China; Yanshan University, People's Republic of China; Yanshan University, People's Republic of China; Yanshan University, People's Republic of China","IET Image Processing","","2019","13","8","1295","1303","The signal degradation due to the Poisson noise is a common problem in the low-light imaging field. Recently, deep learning employing the convolution neural network for image denoising has drawn considerable attention owing to its favourable denoising performance. On the basis of the fact that the reconstruction of corrupted pixels can be facilitated by the context information in image denoising, the authors propose a deep multi-scale cross-path concatenation residual network (MC2RNet) which incorporates cross-path concatenation modules for Poisson denoising. Multiple paths are achieved by the cross-path concatenation operation and the skip connection. As a consequence, multi-scale context representations of images under different receptive fields can be learnt by MC2RNet. With the residual learning strategy, MC2RNet learns the residual between the noisy image and the latent clean image rather than the direct mapping to facilitate model training. Specially, unlike existing discriminative Poisson denoising algorithms that train a model only for the specific noise level, they aim to train a single model for handling Poisson noise with different levels, i.e. blind Poisson denoising. Quantitative experiments demonstrate that the proposed model is superior over the state-of-the-art Poisson denoising approaches in terms of peak signal-to-noise ratio and visual effect.","","","10.1049/iet-ipr.2018.5941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8741329","","","convolutional neural nets;Gaussian noise;image denoising;image reconstruction;image resolution;learning (artificial intelligence);stochastic processes","visual effect;multiscale context representations;corrupted pixel reconstruction;MC2RNet;receptive fields;peak signal-to-noise ratio;blind Poisson denoising;discriminative Poisson denoising algorithms;latent clean image;noisy image;residual learning strategy;cross-path concatenation operation;multiple paths;cross-path concatenation modules;deep multiscale cross-path concatenation residual network;image denoising;convolution neural network;deep learning;low-light imaging field;Poisson noise;signal degradation","","1","36","","","","","IET","IET Journals"
"Locality Preserving Joint Transfer for Domain Adaptation","J. Li; M. Jing; K. Lu; L. Zhu; H. T. Shen","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Image Processing","","2019","28","12","6103","6115","Domain adaptation aims to leverage knowledge from a well-labeled source domain to a poorly labeled target domain. A majority of existing works transfer the knowledge at either feature level or sample level. Recent studies reveal that both of the paradigms are essentially important, and optimizing one of them can reinforce the other. Inspired by this, we propose a novel approach to jointly exploit feature adaptation with distribution matching and sample adaptation with landmark selection. During the knowledge transfer, we also take the local consistency between the samples into consideration so that the manifold structures of samples can be preserved. At last, we deploy label propagation to predict the categories of new instances. Notably, our approach is suitable for both homogeneous- and heterogeneous-domain adaptations by learning domain-specific projections. Extensive experiments on five open benchmarks, which consist of both standard and large-scale datasets, verify that our approach can significantly outperform not only conventional approaches but also end-to-end deep models. The experiments also demonstrate that we can leverage handcrafted features to promote the accuracy on deep features by heterogeneous adaptation.","","","10.1109/TIP.2019.2924174","National Natural Science Foundation of China; National Postdoctoral Program for Innovative Talents; China Postdoctoral Science Foundation; Sichuan Department of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746823","Domain adaptation;transfer learning;landmark selection;subspace learning","Knowledge transfer;Adaptation models;Manifolds;Optimization;Task analysis;Feature extraction;Dimensionality reduction","feature selection;learning (artificial intelligence)","feature adaptation;distribution matching;sample adaptation;knowledge transfer;deep features;domain adaptation;domain-specific projection learning;landmark selection;locality preservation joint transfer","","","66","","","","","IEEE","IEEE Journals"
"A Blind Stereoscopic Image Quality Evaluator With Segmented Stacked Autoencoders Considering the Whole Visual Perception Route","J. Yang; K. Sim; X. Gao; W. Lu; Q. Meng; B. Li","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; Department of Computer Science, Loughborough University, Loughborough, U.K.; Department of Computer Science, Loughborough University, Loughborough, U.K.","IEEE Transactions on Image Processing","","2019","28","3","1314","1328","Most of the current blind stereoscopic image quality assessment (SIQA) algorithms cannot show reliable accuracy. One reason is that they do not have the deep architectures and the other reason is that they are designed on the relatively weak biological basis, compared with the findings on the human visual system. In this paper, we propose a Deep Edge and COlor Signal INtegrity Evaluator (DECOSINE) based on the whole visual perception route from eyes to the frontal lobe, and especially focus on the edge and color signal processing in retinal ganglion cells and lateral geniculate nucleus. Furthermore, to model the complex and deep structure of the visual cortex, segmented stacked auto-encoder (S-SAE) is used, which has not utilized for SIQA before. The utilization of the S-SAE complements the weakness of deep learning-based SIQA metrics that require a very long training time. Experiments are conducted on popular SIQA databases, and the superiority of DECOSINE in terms of prediction accuracy and monotonicity is proved. The experimental results show that our model about the whole visual perception route and utilization of S-SAE are effective for SIQA.","","","10.1109/TIP.2018.2878283","National Natural Science Foundation of China; Foundation of Pre-Research on Equipment of China; Joint Foundation of Pre-Research on Equipment from Education Department of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8510911","Stereoscopic image quality assessment;retinal ganglion cell;lateral geniculate nucleus;segmented stacked autoencoder;edge quality;color quality","Image color analysis;Measurement;Visualization;Image edge detection;Retina;Ganglia;Feature extraction","eye;image colour analysis;learning (artificial intelligence);stereo image processing;visual perception","S-SAE;DECOSINE;blind stereoscopic image quality evaluator;blind stereoscopic image quality assessment algorithms;color signal integrity evaluator;deep edge;popular SIQA databases;deep learning-based SIQA metrics;segmented stacked auto-encoder;visual cortex;deep structure;complex structure;color signal processing;human visual system;relatively weak biological basis;deep architectures;reliable accuracy;visual perception route;segmented stacked autoencoders","","5","62","","","","","IEEE","IEEE Journals"
"Deep Integration: A Multi-Label Architecture for Road Scene Recognition","L. Chen; W. Zhan; W. Tian; Y. He; Q. Zou","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Computer Science, Wuhan University, Wuhan, China","IEEE Transactions on Image Processing","","2019","28","10","4883","4898","Deep convolutional neural networks have been applied by automobile industries, Internet giants, and academic institutes to boost autonomous driving technologies; while progress has been witnessed in environmental perception tasks, such as object detection and driver state recognition, the scene-centric understanding and identification still remain a virgin land. This mainly encompasses two key issues: 1) the lack of shared large datasets with comprehensively annotated road scene information and 2) the difficulty to find effective ways to train networks concerning the bias of category samples, image resolutions, scene dynamics, and capturing conditions. In this paper, we make two contributions: 1) we introduce a large-scale dataset with over 110 k images, dubbed DrivingScene, covering traffic scenarios under different weather conditions, road structures, and environmental instances and driving places, which is the first large-scale dataset for multi-class traffic scenes classification and 2) we propose a multi-label neural network for road scene recognition, which incorporates both single- and multi-class classification modes into a multi-level cost function for training with imbalanced categories and utilizes a deep data integration strategy to improve the classification ability on hard samples. The experimental results on DrivingScene and PASCAL VOC demonstrate the effectiveness of the proposed approach in handling the challenge of data imbalance.","","","10.1109/TIP.2019.2913079","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8708965","Large-scale dataset;data imbalance;multi-label classification;road scene recognition","Roads;Training;Image recognition;Image resolution;Task analysis;Meteorology;Semantics","automobile industry;convolutional neural nets;data integration;driver information systems;image classification;image resolution;learning (artificial intelligence);mobile robots;object recognition;road vehicles;traffic engineering computing","multilabel architecture;road scene recognition;Internet giants;autonomous driving technologies;environmental perception tasks;comprehensively annotated road scene information;image resolutions;scene dynamics;large-scale dataset;road structures;environmental instances;driving places;multiclass traffic scenes classification;multilabel neural network;multilevel cost function;deep data integration strategy;deep convolutional neural networks;academic institution;automobile industries;dubbed DrivingScene;weather conditions;PASCAL VOC;single-class classification modes","","","43","","","","","IEEE","IEEE Journals"
"Retinal Image Synthesis and Semi-Supervised Learning for Glaucoma Assessment","A. Diaz-Pinto; A. Colomer; V. Naranjo; S. Morales; Y. Xu; A. F. Frangi","Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Artificial Intelligence Innovation Business, Baidu, Inc., Beijing, China; CISTIB Center for Computational Imaging and Simulation Technologies in Biomedicine, University of Leeds, Leeds, U.K.","IEEE Transactions on Medical Imaging","","2019","38","9","2211","2218","Recent works show that generative adversarial networks (GANs) can be successfully applied to image synthesis and semi-supervised learning, where, given a small labeled database and a large unlabeled database, the goal is to train a powerful classifier. In this paper, we trained a retinal image synthesizer and a semi-supervised learning method for automatic glaucoma assessment using an adversarial model on a small glaucoma-labeled database and a large unlabeled database. Various studies have shown that glaucoma can be monitored by analyzing the optic disc and its surroundings, and for that reason, the images used in this paper were automatically cropped around the optic disc. The novelty of this paper is to propose a new retinal image synthesizer and a semi-supervised learning method for glaucoma assessment based on the deep convolutional GANs. In addition, and to the best of our knowledge, this system is trained on an unprecedented number of publicly available images (86926 images). This system, hence, is not only able to generate images synthetically but to provide labels automatically. Synthetic images were qualitatively evaluated using t-SNE plots of features associated with the images and their anatomical consistency was estimated by measuring the proportion of pixels corresponding to the anatomical structures around the optic disc. The resulting image synthesizer is able to generate realistic (cropped) retinal images, and subsequently, the glaucoma classifier is able to classify them into glaucomatous and normal with high accuracy (AUC = 0.9017). The obtained retinal image synthesizer and the glaucoma classifier could then be used to generate an unlimited number of cropped retinal images with glaucoma labels.","","","10.1109/TMI.2019.2903434","Project GALAHAD; Generalitat Valenciana; Spanish Government through a FPI Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8662628","Glaucoma assessment;retinal image synthesis;fundus images;DCGAN;medical imaging","Optical imaging;Biomedical optical imaging;Retina;Databases;Semisupervised learning;Synthesizers;Generative adversarial networks","biomedical optical imaging;diseases;eye;image classification;learning (artificial intelligence);medical image processing;pattern classification","retinal image synthesis;generative adversarial networks;retinal image synthesizer;semisupervised learning method;automatic glaucoma assessment;optic disc;glaucoma classifier;glaucoma labels;retinal images;small glaucoma-labeled database;t-SNE plots","","","39","","","","","IEEE","IEEE Journals"
"Synthesizing Chest X-Ray Pathology for Training Deep Convolutional Neural Networks","H. Salehinejad; E. Colak; T. Dowdell; J. Barfett; S. Valaee","Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Department of Medical Imaging, St. Michael’s Hospital, Toronto, Canada; Department of Medical Imaging, St. Michael’s Hospital, Toronto, Canada; Department of Medical Imaging, St. Michael’s Hospital, Toronto, Canada; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada","IEEE Transactions on Medical Imaging","","2019","38","5","1197","1206","Medical datasets are often highly imbalanced with over-representation of prevalent conditions and poor representation of rare medical conditions. Due to privacy concerns, it is challenging to aggregate large datasets between health care institutions. We propose synthesizing pathology in medical images as a means to overcome these challenges. We implement a deep convolutional generative adversarial network (DCGAN) to create synthesized chest X-rays based upon a modest sized labeled dataset. We used a combination of real and synthesized images to train deep convolutional neural networks (DCNNs) to detect pathology across five classes of chest X-rays. The comparative study of DCNNs trained with the combination of real and synthesized images showed that these networks can outperform similar networks trained solely with real images in pathology classification. This improved performance is largely attributable to the balancing of the dataset using DCGAN synthesized images, where classes that are lacking in example images are preferentially augmented.","","","10.1109/TMI.2018.2881415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8534421","Chest X-ray;convolutional neural network;generative adversarial network;image augmentation;synthesized images","X-ray imaging;Gallium nitride;Computed tomography;Biomedical imaging;Magnetic resonance imaging;Training;Image segmentation","biomedical MRI;computerised tomography;convolutional neural nets;health care;learning (artificial intelligence);medical image processing;pattern classification","medical datasets;privacy concerns;health care institutions;medical images;deep convolutional generative adversarial network;pathology classification;deep convolutional neural networks training;chest X-ray pathology;DCNNs;DCGAN synthesized images","","","42","","","","","IEEE","IEEE Journals"
"Sequence Labeling With Deep Gated Dual Path CNN","L. Zhao; X. Qiu; Q. Zhang; X. Huang","School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","12","2326","2335","Sequence labeling, such as part-of-speech (POS) tagging, named entity recognition (NER), text chunking, is a classic task in natural language processing. Most existing neural networks models for sequence labeling are based on recurrent neural networks. Recently, convolutional neural networks have been proposed to replace the recurrent components for sequence labeling. However, they are usually shallow compared to deep convolutional networks that achieve start-of-the-art performance in other fields. Due to the vanishing gradient problem, these models usually can not work well when simply increasing the number of layers. In this paper, we propose using deep CNN architecture in sequence labeling, which can capture a large context through stacked convolutions. To reduce the vanishing gradient problem, the proposed method incorporates gated linear units, residual connections, and dense connections. Experimental results on three sequence labeling tasks show that the proposed model can achieve competitive performance to the RNN-based state-of-the-art method while maintaining 2.41 × faster speed, even with up to 10 convolutional layers.","","","10.1109/TASLP.2019.2944563","China National Key R&D Program; National Natural Science Foundation of China; Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8853307","Sequence labeling;recurrent neural networks;convolutional neural networks","Logic gates;Labeling;Task analysis;Hidden Markov models;Context modeling;Natural language processing;Recurrent neural networks","convolutional neural nets;learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis","deep gated dual path CNN;text chunking;recurrent neural networks;convolutional neural networks;deep convolutional networks;vanishing gradient problem;sequence labeling tasks","","","51","IEEE","","","","IEEE","IEEE Journals"
"Short Utterance Based Speech Language Identification in Intelligent Vehicles With Time-Scale Modifications and Deep Bottleneck Features","Z. Ma; H. Yu; W. Chen; J. Guo","Pattern Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Electrical Engineering, Ludong University, Yantai, China; Beijing Sogou Technology Development Company Ltd., Beijing, China; Pattern Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Vehicular Technology","","2019","68","1","121","128","Conversations in the intelligent vehicles are usually short utterance. As the durations of the short utterances are small (e.g., less than 3 s), it is difficult to learn sufficient information to distinguish the type of languages. In this paper, we propose an end-to-end short utterances based speech language identification (SLI) approach, which is especially suitable for the short utterance based language identification. This approach is implemented with a long short-term memory (LSTM) neural network, which is designed for the SLI application in intelligent vehicles. The features used for LSTM learning are generated by a transfer learning method. The bottleneck features of a deep neural network, which are obtained for a mandarin acoustic-phonetic classifier, are used for the LSTM training. In order to improve the SLD accuracy with short utterances, a phase vocoder based time-scale modification method is utilized to reduce/increase the speech rate of the test utterance. By connecting the normal, speech rate reduced, and speech rate increased utterances, we can extend the length of the test utterances such that the performance of the SLI system is improved. The experimental results on the AP17-OLR database demonstrate that the proposed method can improve the performance of SLD, especially on short utterance. The proposed SLI has robust performance under the vehicular noisy environment.","","","10.1109/TVT.2018.2879361","National Key R&D Program of China; National Natural Science Foundation of China; Beijing Nova Program; Beijing Nova Program Interdisciplinary Cooperation Project; Beijing Natural Science Foundation; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520780","Speech language identification;time-scale modification;DNN-BN feature;LSTM","Feature extraction;Task analysis;Neural networks;Hidden Markov models;Intelligent vehicles;Speech recognition;Acoustics","feature extraction;learning (artificial intelligence);natural language processing;recurrent neural nets;signal classification;speech processing;speech recognition;vocoders","long short-term memory neural network;phase vocoder;LSTM learning;transfer learning;Mandarin acoustic-phonetic classifier;time-scale modification;short utterance based speech language identification;intelligent vehicles","","45","57","","","","","IEEE","IEEE Journals"
"Pedestrian Alignment Network for Large-scale Person Re-Identification","Z. Zheng; L. Zheng; Y. Yang","Centre for Artificial Intelligence, University of Technology Sydney, Ultimo, NSW, Australia; Research School of Computer Science, Australian National University (ANU), Canberra, ACT, Australia; Centre for Artificial Intelligence, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","10","3037","3045","Person re-identification (re-ID) is mostly viewed as an image retrieval problem. This task aims to search a query person in a large image pool. In practice, person re-ID usually adopts automatic detectors to obtain cropped pedestrian images. However, this process suffers from two types of detector errors: excessive background and part missing. Both errors deteriorate the quality of pedestrian alignment and may compromise pedestrian matching due to the position and scale variances. To address the misalignment problem, we propose that alignment be learned from an identification procedure. We introduce the pedestrian alignment network (PAN) which allows discriminative embedding learning pedestrian alignment without extra annotations. We observe that when the convolutional neural network learns to discriminate between different identities, the learned feature maps usually exhibit strong activations on the human body rather than the background. The proposed network thus takes advantage of this attention mechanism to adaptively locate and align pedestrians within a bounding box. Visual examples show that pedestrians are better aligned with PAN. Experiments on three large-scale re-ID datasets confirm that PAN improves the discriminative ability of the feature embeddings and yields competitive accuracy with the state-of-the-art methods.","","","10.1109/TCSVT.2018.2873599","Data to Decisions Cooperative Research Centres; Google; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8481710","Person re-identification;person search;person alignment;image retrieval;deep learning","Feature extraction;Task analysis;Detectors;Training;Face;Image retrieval;Cameras","convolutional neural nets;feature extraction;image matching;image retrieval;learning (artificial intelligence);pedestrians","pedestrian alignment network;image retrieval problem;query person;image pool;person re-ID;cropped pedestrian images;pedestrian matching;PAN;convolutional neural network;learned feature maps;large-scale person re-identification;discriminative embedding learning pedestrian alignment","","17","75","","","","","IEEE","IEEE Journals"
"Exploiting Web Images for Weakly Supervised Object Detection","Q. Tao; H. Yang; J. Cai","School of Computer Science and Engineering, Nanyang Technological University, Singapore; Amazon, Seattle, WA, USA; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Multimedia","","2019","21","5","1135","1146","In recent years, the performance of object detection has advanced significantly with the evolution of deep convolutional neural networks. However, the state-of-the-art object detection methods still rely on accurate bounding box annotations that require extensive human labeling. Object detection without bounding box annotations, that is, weakly supervised detection methods, are still lagging far behind. As weakly supervised detection only uses image level labels and does not require the ground truth of bounding box location and label of each object in an image, it is generally very difficult to distill knowledge of the actual appearances of objects. Inspired by curriculum learning, this paper proposes an easy-to-hard knowledge transfer scheme that incorporates easy web images to provide prior knowledge of object appearance as a good starting point. While exploiting large-scale free web imagery, we introduce a sophisticated labor-free method to construct a web dataset with good diversity in object appearance. After that, semantic relevance and distribution relevance are introduced and utilized in the proposed curriculum training scheme. Our end-to-end learning with the constructed web data achieves remarkable improvement across most object classes, especially for the classes that are often considered hard in other works.","","","10.1109/TMM.2018.2875597","MoE Tier-2; Tier-1; NTU CoE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489885","Weakly supervised learning;object detection;curriculum learning","Object detection;Task analysis;Knowledge engineering;Semantics;Training;Detectors;Videos","convolutional neural nets;Internet;learning (artificial intelligence);object detection","easy-to-hard knowledge transfer scheme;large-scale free web imagery;weakly supervised object detection;deep convolutional neural networks;bounding box annotations;web images;curriculum learning;semantic relevance;distribution relevance;curriculum training scheme","","","34","","","","","IEEE","IEEE Journals"
"Short-Term Traffic Prediction Based on DeepCluster in Large-Scale Road Networks","L. Han; K. Zheng; L. Zhao; X. Wang; X. Shen","Intelligent Computing and Communication (IC2) Laboratory, the Wireless Signal Processing and Networks Laboratory (WSPN), and the Key Laboratory of Universal Wireless Communications, Beijing University of Posts and Telecommunications, Ministry of Education, Beijing, China; Intelligent Computing and Communication (IC2) Laboratory, the Wireless Signal Processing and Networks Laboratory (WSPN), and the Key Laboratory of Universal Wireless Communications, Beijing University of Posts and Telecommunications, Ministry of Education, Beijing, China; Intelligent Computing and Communication (IC2) Laboratory, the Wireless Signal Processing and Networks Laboratory (WSPN), and the Key Laboratory of Universal Wireless Communications, Beijing University of Posts and Telecommunications, Ministry of Education, Beijing, China; Department of Electrical and Computer Engineering, University of Western Ontario, London, ON, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Vehicular Technology","","2019","68","12","12301","12313","Short-term traffic prediction (STTP) is one of the most critical capabilities in Intelligent Transportation Systems (ITS), which can be used to support driving decisions, alleviate traffic congestion and improve transportation efficiency. However, STTP of large-scale road networks remains challenging due to the difficulties of effectively modeling the diverse traffic patterns by high-dimensional time series. Therefore, this paper proposes a framework that involves a deep clustering method for STTP in large-scale road networks. The deep clustering method is employed to supervise the representation learning in a visualized way from the large unlabeled dataset. More specifically, to fully exploit the traffic periodicity, the raw series is first divided into a number of sub-series for triplet generation. The convolutional neural networks (CNNs) with triplet loss are utilized to extract the features of shape by transforming the series into visual images. The shape-based representations are then used to cluster road segments into groups. Thereafter, a model sharing strategy is further proposed to build recurrent NNs-based predictions through group-based models (GBMs). GBM is built for a type of traffic patterns, instead of one road segment exclusively or all road segments uniformly. Our framework can not only significantly reduce the number of prediction models, but also improve their generalization by virtue of being trained on more diverse examples. Furthermore, the proposed framework over a selected road network in Beijing is evaluated. Experiment results show that the deep clustering method can effectively cluster the road segments and GBM can achieve comparable prediction accuracy against the IBM with less number of prediction models.","","","10.1109/TVT.2019.2947080","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8867988","Short-term traffic prediction;large-scale road networks;deep representation learning;deep clustering","Roads;Predictive models;Time series analysis;Clustering methods;Image segmentation;Learning systems;Training","","","","","56","IEEE","","","","IEEE","IEEE Journals"
"Learning Driver Braking Behavior Using Smartphones, Neural Networks and the Sliding Correlation Coefficient: Road Anomaly Case Study","S. G. Christopoulos; S. Kanarachos; A. Chroneos","Faculty of Engineering, Environment and Computing, Coventry University, Coventry, U.K.; Faculty of Engineering, Environment and Computing, Coventry University, Coventry, U.K.; Faculty of Engineering, Environment and Computing, Coventry University, Coventry, U.K.","IEEE Transactions on Intelligent Transportation Systems","","2019","20","1","65","74","This paper focuses on the automated learning of driver braking “signature” in the presence of road anomalies. Our motivation is to improve driver experience using preview information from navigation maps. Smartphones facilitate, due to their unprecedented market penetration, the large-scale deployment of advanced driver assistance systems. On the other hand, it is challenging to exploit smartphone sensor data because of the fewer and lower quality signals, compared to the ones on board. Methods for detecting braking behavior using smartphones exist, however, most of them focus only on harsh events. Additionally, only a few studies correlate longitudinal driving behavior with the road condition. In this paper, a new method, based on deep neural networks and the sliding correlation coefficient, is proposed for the spatio-temporal correlation of road anomalies and driver behavior. A unique deep neural network structure, that requires minimum tuning, is proposed. Extensive field trials were conducted and vehicle motion was recorded using smartphones and a data acquisition system, comprising an inertial measurement unit and differential GPS. The proposed method was validated using the probabilistic Receiver Operating Characteristics method. The method proves to be a robust and flexible tool for self-learning driver behavior.","","","10.1109/TITS.2018.2797943","Intelligent Variable Message Systems, through the Government’s Local Growth Fund through Coventry; Warwickshire Local Enterprise Partnership; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8294049","Advanced driver assistance systems;braking behavior;neural networks;smartphones;road condition","Smart phones;Roads;Vehicles;Acceleration;Global Positioning System;Accelerometers;Correlation","behavioural sciences computing;braking;driver information systems;feature extraction;learning (artificial intelligence);mobile computing;neural nets;road vehicles;smart phones;traffic engineering computing","sliding correlation coefficient;road anomalies;advanced driver assistance systems;smartphone sensor data;deep neural networks;data acquisition system;driver braking behavior learning;braking behavior detection;driver braking signature","","1","30","","","","","IEEE","IEEE Journals"
"Unsupervised Identification of Disease Marker Candidates in Retinal OCT Imaging Data","P. Seeböck; S. M. Waldstein; S. Klimscha; H. Bogunovic; T. Schlegl; B. S. Gerendas; R. Donner; U. Schmidt-Erfurth; G. Langs","Department of Biomedical Imaging and Image-Guided Therapy, Computational Imaging Research Laboratory, Medical University Vienna, Vienna, Austria; Department of Ophthalmology and Optometry, Christian Doppler Laboratory for Ophthalmic Image Analysis, Vienna Reading Center, Medical University Vienna, Vienna, Austria; Department of Ophthalmology and Optometry, Christian Doppler Laboratory for Ophthalmic Image Analysis, Vienna Reading Center, Medical University Vienna, Vienna, Austria; Department of Ophthalmology and Optometry, Christian Doppler Laboratory for Ophthalmic Image Analysis, Vienna Reading Center, Medical University Vienna, Vienna, Austria; Department of Biomedical Imaging and Image-Guided Therapy, Computational Imaging Research Laboratory, Medical University Vienna, Vienna, Austria; Department of Ophthalmology and Optometry, Christian Doppler Laboratory for Ophthalmic Image Analysis, Vienna Reading Center, Medical University Vienna, Vienna, Austria; Department of Biomedical Imaging and Image-Guided Therapy, Computational Imaging Research Laboratory, Medical University Vienna, Vienna, Austria; Department of Ophthalmology and Optometry, Christian Doppler Laboratory for Ophthalmic Image Analysis, Vienna Reading Center, Medical University Vienna, Vienna, Austria; Department of Biomedical Imaging and Image-Guided Therapy, Computational Imaging Research Laboratory, Medical University Vienna, Vienna, Austria","IEEE Transactions on Medical Imaging","","2019","38","4","1037","1047","The identification and quantification of markers in medical images is critical for diagnosis, prognosis, and disease management. Supervised machine learning enables the detection and exploitation of findings that are known a priori after annotation of training examples by experts. However, supervision does not scale well, due to the amount of necessary training examples, and the limitation of the marker vocabulary to known entities. In this proof-of-concept study, we propose unsupervised identification of anomalies as candidates for markers in retinal optical coherence tomography (OCT) imaging data without a constraint to a priori definitions. We identify and categorize marker candidates occurring frequently in the data and demonstrate that these markers show a predictive value in the task of detecting disease. A careful qualitative analysis of the identified data driven markers reveals how their quantifiable occurrence aligns with our current understanding of disease course, in early- and late age-related macular degeneration (AMD) patients. A multi-scale deep denoising autoencoder is trained on healthy images, and a one-class support vector machine identifies anomalies in new data. Clustering in the anomalies identifies stable categories. Using these markers to classify healthy-, early AMD- and late AMD cases yields an accuracy of 81.40%. In a second binary classification experiment on a publicly available data set (healthy versus intermediate AMD), the model achieves an area under the ROC curve of 0.944.","","","10.1109/TMI.2018.2877080","Christian Doppler Forschungsgesellschaft; Austrian Federal Ministry for Digital and Economic Affairs; Austrian Science Fund; IBM (2016–2017 IBM Ph.D. Fellowship Award and Faculty Award); Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502086","Unsupervised deep learning;anomaly detection;biomarker identification;optical coherence tomography","Diseases;Retina;Biomedical imaging;Training;Anomaly detection;Task analysis","biomedical optical imaging;diseases;eye;geriatrics;image classification;image denoising;medical image processing;neural nets;optical tomography;support vector machines;unsupervised learning","disease course;retinal OCT imaging data;medical images;disease management;supervised machine learning;retinal optical coherence tomography;late age-related macular degeneration patients;unsupervised identification;disease marker candidates;multiscale deep denoising autoencoder;support vector machine;binary classification","","1","34","","","","","IEEE","IEEE Journals"
"CE-Net: Context Encoder Network for 2D Medical Image Segmentation","Z. Gu; J. Cheng; H. Fu; K. Zhou; H. Hao; Y. Zhao; T. Zhang; S. Gao; J. Liu","School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; Cixi Institute of Biomedical Engineering, Chinese Academy of Sciences, Zhejiang, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Cixi Institute of Biomedical Engineering, Chinese Academy of Sciences, Zhejiang, China; Cixi Institute of Biomedical Engineering, Chinese Academy of Sciences, Zhejiang, China; Cixi Institute of Biomedical Engineering, Chinese Academy of Sciences, Zhejiang, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Cixi Institute of Biomedical Engineering, Chinese Academy of Sciences, Zhejiang, China","IEEE Transactions on Medical Imaging","","2019","38","10","2281","2292","Medical image segmentation is an important step in medical image analysis. With the rapid development of a convolutional neural network in image processing, deep learning has been used for medical image segmentation, such as optic disc segmentation, blood vessel detection, lung segmentation, cell segmentation, and so on. Previously, U-net based approaches have been proposed. However, the consecutive pooling and strided convolutional operations led to the loss of some spatial information. In this paper, we propose a context encoder network (CE-Net) to capture more high-level information and preserve spatial information for 2D medical image segmentation. CE-Net mainly contains three major components: a feature encoder module, a context extractor, and a feature decoder module. We use the pretrained ResNet block as the fixed feature extractor. The context extractor module is formed by a newly proposed dense atrous convolution block and a residual multi-kernel pooling block. We applied the proposed CE-Net to different 2D medical image segmentation tasks. Comprehensive results show that the proposed method outperforms the original U-Net method and other state-of-the-art methods for optic disc segmentation, vessel detection, lung segmentation, cell contour segmentation, and retinal optical coherence tomography layer segmentation.","","","10.1109/TMI.2019.2903562","Chinese Academy of Sciences; Ningbo 3315 Innovation Team Grant, Zhejiang Province; National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8662594","Medical image segmentation;deep learning;context encoder network","Image segmentation;Feature extraction;Convolution;Biomedical optical imaging;Optical imaging;Computed tomography","blood vessels;convolutional neural nets;eye;feature extraction;image segmentation;learning (artificial intelligence);lung;medical image processing","CE-Net;context encoder network;2D medical image segmentation;medical image analysis;optic disc segmentation;lung segmentation;U-Net method;convolutional neural network;deep learning","","5","79","","","","","IEEE","IEEE Journals"
"DenseFuse: A Fusion Approach to Infrared and Visible Images","H. Li; X. Wu","School of Internet of Things Engineering, Jiangnan University, Wuxi, China; School of Internet of Things Engineering, Jiangnan University, Wuxi, China","IEEE Transactions on Image Processing","","2019","28","5","2614","2623","In this paper, we present a novel deep learning architecture for infrared and visible images fusion problems. In contrast to conventional convolutional networks, our encoding network is combined with convolutional layers, a fusion layer, and dense block in which the output of each layer is connected to every other layer. We attempt to use this architecture to get more useful features from source images in the encoding process, and two fusion layers (fusion strategies) are designed to fuse these features. Finally, the fused image is reconstructed by a decoder. Compared with existing fusion methods, the proposed fusion method achieves the state-of-the-art performance in objective and subjective assessment.","","","10.1109/TIP.2018.2887342","National Natural Science Foundation of China; 111 Project of Ministry of Education of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8580578","Image fusion;deep learning;dense block;infrared image;visible image","Feature extraction;Decoding;Image coding;Image reconstruction;Training;Image fusion","convolutional neural nets;image coding;image fusion;image reconstruction;infrared imaging;learning (artificial intelligence)","infrared image fusion problems;visible image fusion problems;decoder;fused image reconstruction;fusion method;encoding process;source images;fusion layer;convolutional layers;encoding network;conventional convolutional networks;deep learning architecture","","4","32","","","","","IEEE","IEEE Journals"
"Fast ScanNet: Fast and Dense Analysis of Multi-Gigapixel Whole-Slide Images for Cancer Metastasis Detection","H. Lin; H. Chen; S. Graham; Q. Dou; N. Rajpoot; P. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Imsight Medical Technology, Co., Ltd., Shenzhen, China; Department of Computer Science, University of Warwick, Coventry, U.K.; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science, University of Warwick, Coventry, U.K.; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Medical Imaging","","2019","38","8","1948","1958","Lymph node metastasis is one of the most important indicators in breast cancer diagnosis, that is traditionally observed under the microscope by pathologists. In recent years, with the dramatic advance of high-throughput scanning and deep learning technology, automatic analysis of histology from whole-slide images has received a wealth of interest in the field of medical image computing, which aims to alleviate pathologists' workload and simultaneously reduce misdiagnosis rate. However, the automatic detection of lymph node metastases from whole-slide images remains a key challenge because such images are typically very large, where they can often be multiple gigabytes in size. Also, the presence of hard mimics may result in a large number of false positives. In this paper, we propose a novel method with anchor layers for model conversion, which not only leverages the efficiency of fully convolutional architectures to meet the speed requirement in clinical practice but also densely scans the whole-slide image to achieve accurate predictions on both micro- and macro-metastases. Incorporating the strategies of asynchronous sample prefetching and hard negative mining, the network can be effectively trained. The efficacy of our method is corroborated on the benchmark dataset of 2016 Camelyon Grand Challenge. Our method achieved significant improvements in comparison with the state-of-the-art methods on tumor localization accuracy with a much faster speed and even surpassed human performance on both challenge tasks.","","","10.1109/TMI.2019.2891305","Innovation and Technology Commission; Hong Kong Research Grants Council; University of Warwick; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8604098","Histopathology image analysis;computational pathology;whole-slide image;deep learning;metastasis detection","Breast cancer;Image segmentation;Metastasis;Tumors;Task analysis;Image analysis","biological tissues;biomedical optical imaging;cancer;convolutional neural nets;learning (artificial intelligence);medical image processing","fully convolutional architectures;misdiagnosis rate;histology;high-throughput scanning;lymph node metastases;automatic detection;pathologists;medical image computing;deep learning technology;breast cancer diagnosis;lymph node metastasis;cancer metastasis detection;multigigapixel whole-slide images;fast ScanNet","","","60","","","","","IEEE","IEEE Journals"
"Real-Time Identification of Power Fluctuations Based on LSTM Recurrent Neural Network: A Case Study on Singapore Power System","S. Wen; Y. Wang; Y. Tang; Y. Xu; P. Li; T. Zhao","Energy Research Institute, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Energy Research Institute, Nanyang Technological University, Singapore","IEEE Transactions on Industrial Informatics","","2019","15","9","5266","5275","Fast and stochastic power fluctuations caused by renewable energy sources and flexible loads have significantly deteriorated the frequency performance of modern power systems. Power system frequency control aims to achieve real-time power balance between generations and loads. In practice, it is much more difficult to exactly acquire the values of unbalance power in both transmission and distribution systems, especially when there is a high penetration level of renewable energies. This paper explores a deep learning approach to identify active power fluctuations in real-time, which is based on a long short-term memory recurrent neural network. The developed method provides a more accurate and faster estimation of the value of power fluctuations from the real-time measured frequency signal. The identified power fluctuations can serve as control reference so that the system frequency can be better maintained by automatic generation control, as well as emerging frequency control elements, such as energy storage system. A detailed model of Singapore power system integrated with distributed energy storage systems is used to verify the proposed method and to compare with various classical methods. The simulation results clearly demonstrate the necessity for power fluctuation identification, and the advantages of the proposed method.","","","10.1109/TII.2019.2910416","Energy Market Authority; National Natural Science Foundation of China; China Postdoctoral Science Foundation; Nanyang Technological University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8686213","Deep learning;long short-term memory (LSTM);power fluctuation identification;recurrent neural network (RNN);Singapore power system","Frequency control;Real-time systems;Power system stability;Recurrent neural networks;Logic gates;Frequency measurement","distributed power generation;energy storage;frequency control;learning (artificial intelligence);power distribution control;power engineering computing;power generation control;power transmission control;recurrent neural nets;renewable energy sources;stochastic processes","power system frequency control;real-time power balance;automatic generation control;Singapore power system;distributed energy storage systems;LSTM recurrent neural network;stochastic power fluctuations;renewable energy sources;power distribution systems;frequency signal;power transmission systems;deep learning approach","","","30","Traditional","","","","IEEE","IEEE Journals"
"GCNv2: Efficient Correspondence Prediction for Real-Time SLAM","J. Tang; L. Ericson; J. Folkesson; P. Jensfelt","Centre for Autonomous Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Centre for Autonomous Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Centre for Autonomous Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Centre for Autonomous Systems, KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Robotics and Automation Letters","","2019","4","4","3505","3512","In this letter, we present a deep learning-based network, GCNv2, for generation of keypoints and descriptors. GCNv2 is built on our previous method, GCN, a network trained for 3D projective geometry. GCNv2 is designed with a binary descriptor vector as the ORB feature so that it can easily replace ORB in systems such as ORB-SLAM2. GCNv2 significantly improves the computational efficiency over GCN that was only able to run on desktop hardware. We show how a modified version of ORBSLAM2 using GCNv2 features runs on a Jetson TX2, an embedded low-power platform. Experimental results show that GCNv2 retains comparable accuracy as GCN and that it is robust enough to use for control of a flying drone. Source code is available at: https://github.com/jiexiong2016/GCNv2_SLAM.","","","10.1109/LRA.2019.2927954","Wallenberg AI; Autonomous Systems and Software Program (WASP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758836","Visual-based navigation;SLAM;deep learning in robotics and automation","Simultaneous localization and mapping;Drones;Feature extraction;Real-time systems;Motion estimation;Training;Image resolution","learning (artificial intelligence);mobile robots;robot vision;SLAM (robots)","ORB-SLAM2;flying drone;Jetson TX2;ORB feature;binary descriptor vector;3D projective geometry;generation of keypoints and descriptors;deep learning-based network;real-time SLAM;GCNv2","","2","45","Traditional","","","","IEEE","IEEE Journals"
"Convolutional Recurrent Neural Networks for Dynamic MR Image Reconstruction","C. Qin; J. Schlemper; J. Caballero; A. N. Price; J. V. Hajnal; D. Rueckert","Department of Computing, Biomedical Image Analysis Group, Imperial College London, London, U.K.; Department of Computing, Biomedical Image Analysis Group, Imperial College London, London, U.K.; Department of Computing, Biomedical Image Analysis Group, Imperial College London, London, U.K.; Biomedical Engineering Department, Division of Imaging Sciences, King’s College London, London, U.K.; Biomedical Engineering Department, Division of Imaging Sciences, King’s College London, London, U.K.; Department of Computing, Biomedical Image Analysis Group, Imperial College London, London, U.K.","IEEE Transactions on Medical Imaging","","2019","38","1","280","290","Accelerating the data acquisition of dynamic magnetic resonance imaging leads to a challenging ill-posed inverse problem, which has received great interest from both the signal processing and machine learning communities over the last decades. The key ingredient to the problem is how to exploit the temporal correlations of the MR sequence to resolve aliasing artifacts. Traditionally, such observation led to a formulation of an optimization problem, which was solved using iterative algorithms. Recently, however, deep learning-based approaches have gained significant popularity due to their ability to solve general inverse problems. In this paper, we propose a unique, novel convolutional recurrent neural network architecture which reconstructs high quality cardiac MR images from highly undersampled k-space data by jointly exploiting the dependencies of the temporal sequences as well as the iterative nature of the traditional optimization algorithms. In particular, the proposed architecture embeds the structure of the traditional iterative algorithms, efficiently modeling the recurrence of the iterative reconstruction stages by using recurrent hidden connections over such iterations. In addition, spatio-temporal dependencies are simultaneously learnt by exploiting bidirectional recurrent hidden connections across time sequences. The proposed method is able to learn both the temporal dependence and the iterative reconstruction process effectively with only a very small number of parameters, while outperforming current MR reconstruction methods in terms of reconstruction accuracy and speed.","","","10.1109/TMI.2018.2863670","Engineering and Physical Sciences Research Council; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8425639","Recurrent neural network;convolutional neural network;dynamic magnetic resonance imaging;cardiac image reconstruction","Image reconstruction;Magnetic resonance imaging;Optimization;Machine learning;Iterative methods;Recurrent neural networks","biomedical MRI;data acquisition;image reconstruction;inverse problems;iterative methods;learning (artificial intelligence);medical image processing;recurrent neural nets","traditional iterative algorithms;iterative reconstruction stages;spatio-temporal dependencies;bidirectional recurrent hidden connections;time sequences;temporal dependence;iterative reconstruction process;reconstruction accuracy;convolutional recurrent neural networks;dynamic MR image reconstruction;data acquisition;dynamic magnetic resonance imaging;signal processing;machine learning communities;key ingredient;temporal correlations;MR sequence;aliasing artifacts;optimization problem;deep learning-based approaches;significant popularity;general inverse problems;unique network architecture;high quality cardiac MR images;highly undersampled k-space data;temporal sequences;iterative nature;traditional optimization algorithms;ill-posed inverse problem;convolutional recurrent neural network architecture","","25","32","","","","","IEEE","IEEE Journals"
"Learning to Find Unpaired Cross-Spectral Correspondences","S. Jeong; S. Kim; K. Park; K. Sohn","School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Computer and Communication Sciences, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Image Processing","","2019","28","11","5394","5406","We present a deep architecture and learning framework for establishing correspondences across cross-spectral visible and infrared images in an unpaired setting. To overcome the unpaired cross-spectral data problem, we design the unified image translation and feature extraction modules to be learned in a joint and boosting manner. Concretely, the image translation module is learned only with the unpaired cross-spectral data, and the feature extraction module is learned with an input image and its translated image. By learning two modules simultaneously, the image translation module generates the translated image that preserves not only the domain-specific attributes with separate latent spaces but also the domain-agnostic contents with feature consistency constraint. In an inference phase, the cross-spectral feature similarity is augmented by intra-spectral similarities between the features extracted from the translated images. Experimental results show that this model outperforms the state-of-the-art unpaired image translation methods and cross-spectral feature descriptors on various visible and infrared benchmarks.","","","10.1109/TIP.2019.2917864","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721725","Image-to-image translation;feature extraction;multi-spectral;unpaired setting;infrared","Feature extraction;Training;Benchmark testing;Computer vision;Lighting;Generative adversarial networks;Task analysis","feature extraction;infrared imaging;learning (artificial intelligence)","unpaired cross-spectral correspondences;deep architecture;learning framework;infrared images;unpaired setting;unpaired cross-spectral data problem;unified image translation;feature extraction modules;image translation module;feature extraction module;input image;translated image;feature consistency constraint;cross-spectral feature similarity;intra-spectral similarities;cross-spectral feature descriptors;cross-spectral visible images","","","45","","","","","IEEE","IEEE Journals"
"Video Foreground Extraction Using Multi-View Receptive Field and Encoder–Decoder DCNN for Traffic and Surveillance Applications","T. Akilan; Q. M. J. Wu; W. Zhang","Department of Computer Science, Lakehead University, Thunder Bay, ON, Canada; Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada; Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada","IEEE Transactions on Vehicular Technology","","2019","68","10","9478","9493","The automatic detection of foreground (FG) objects in videos is a demanding area of computer vision, with essential applications in video-based traffic analysis and surveillance. New solutions have attempted exploiting deep neural network (DNN) for this purpose. In DNN, learning agents, i.e., features for video FG object segmentation is nontrivial, unlike image segmentation. It is a temporally processed decision-making problem, where the agents involved are the spatial and temporal correlations of the FG objects and the background (BG) of the scene. To handle this and to overcome the conventional DL models' poor delineation at the borders of FG regions due to fixed-view receptive filed-based learning, this work introduces a Multi-view Receptive Field EncoderDecoder Convolutional Neural Network called MvRF-CNN. The main contribution of the model is harnessing multiple views of convolutional (conv) kernels with residual feature fusions at early, mid and late stages in an encoder-decoder (EnDec) architecture. It enhances the ability of the model to learn condition-invariant agents resulting in highly delineated FG masks when compared to the existing approaches from heuristicto DL-based techniques. The model is trained with sequence-specific labeled samples to predict scene-specific pixel-level labels of FG objects in near static scenes with a minute dynamism. The experimental study on 37 video sequences from traffic and surveillance scenarios that include complex environments, viz. dynamic background, camera jittery, intermittent object motion, scenes with cast shadows, night videos, and lousy weather proves the effectiveness of the model. The study covers two input configurations: a 3-channel (RGB) single frame and a 3-channel double-frame with a BG such that two consecutive grayscale frames stacked with a prior BG model. The ablation investigations are also conducted to show the importance of transfer learning (TL) and mid-fusion approaches for enhancing the segmentation performance and the model's robustness on failure modes: when there is lack of manually annotated hard ground truths (HGT) and testing the model under non-scenespecific videos. In overall, the model achieves a figure-of-merit of 95% and 42 FPS of mean average performance.","","","10.1109/TVT.2019.2937076","Canada Research Chair Program; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809739","Background subtraction;encoder-decoder network;foreground extraction;transfer learning","Surveillance;Image segmentation;Cameras;Object segmentation;Convolutional neural networks","computer vision;convolutional neural nets;feature extraction;image colour analysis;image motion analysis;image segmentation;image sequences;learning (artificial intelligence);neural net architecture;object detection;traffic engineering computing;video signal processing;video surveillance","video-based traffic analysis;deep neural network;DNN;learning agents;video FG object segmentation;decision-making problem;residual feature fusions;encoder-decoder architecture;video sequences;surveillance scenarios;video foreground extraction;encoder-decoder DCNN;computer vision;multiview receptive field encoder decoder convolutional neural network;MvRF-CNN;convolutional kernels;transfer learning;foreground object detection","","","83","Traditional","","","","IEEE","IEEE Journals"
"Makeup Removal via Bidirectional Tunable De-Makeup Network","C. Cao; F. Lu; C. Li; S. Lin; X. Shen","State Key Laboratory of Virtual Reality Technology and Systems, School of New Media Art and Design, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Tencent Youtu Lab, Shenzhen 518000, China; Microsoft Research, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of New Media Art and Design, Beihang University, Beijing, China","IEEE Transactions on Multimedia","","2019","21","11","2750","2761","We present a deep learning-based method for removing makeup effects (de-makeup) in a face image. This problem poses a major challenge due to obscuring of the underlying facial features by cosmetics, which is very important in multimedia applications in the field of security, entertainment, and social networking. To address this task, we propose the bidirectional tunable de-makeup network (BTD-Net), which jointly learns the makeup process to aid in learning the de-makeup process. For tractable learning of the makeup process, which is a one-to-many mapping determined by the cosmetics that are applied, we introduce a latent variable that reflects the makeup style. This latent variable is extracted in the de-makeup process and used as a condition on the makeup process to constrain the one-to-many mapping to a specific solution. Through extensive experiments, our proposed BTD-Net is found to surpass the state-of-art techniques in estimating realistic non-makeup faces that correspond to the input makeup images. We additionally show that applications such as tuning the amount of makeup can be enhanced through the use of this method.","","","10.1109/TMM.2019.2911457","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693800","Makeup removal;generative adversarial learning;bidirectional network","Face;Task analysis;Training;Image synthesis;Facial features;Tuning;Gallium nitride","cosmetics;face recognition;feature extraction;learning (artificial intelligence)","deep learning-based method;makeup effects;social networking;BTD-Net;de-makeup process;tractable learning;makeup style;input makeup images;makeup removal;Bidirectional Tunable De-Makeup Network;realistic nonmakeup faces;one-to-many mapping;multimedia applications;face image","","","47","Traditional","","","","IEEE","IEEE Journals"
"CNN-Based Methods for Object Recognition With High-Resolution Tactile Sensors","J. M. Gandarias; A. J. García-Cerezo; J. M. Gómez-de-Gabriel","Systems Engineering and Automation Department, Robotics and Mechatronics Group, University of Málaga, Escuela de Ingenierías Industriales, Málaga, Spain; Systems Engineering and Automation Department, Robotics and Mechatronics Group, University of Málaga, Escuela de Ingenierías Industriales, Málaga, Spain; Systems Engineering and Automation Department, Robotics and Mechatronics Group, University of Málaga, Escuela de Ingenierías Industriales, Málaga, Spain","IEEE Sensors Journal","","2019","19","16","6872","6882","Novel high-resolution pressure-sensor arrays allow treating pressure readings as standard images. Computer vision algorithms and methods such as convolutional neural networks (CNN) can be used to identify contact objects. In this paper, a high-resolution tactile sensor has been attached to a robotic end-effector to identify contacted objects. Two CNN-based approaches have been employed to classify pressure images. These methods include a transfer learning approach using a pre-trained CNN on an RGB-images dataset and a custom-made CNN (TactNet) trained from scratch with tactile information. The transfer learning approach can be carried out by retraining the classification layers of the network or replacing these layers with an SVM. Overall, 11 configurations based on these methods have been tested: eight transfer learning-based, and three TactNet-based. Moreover, a study of the performance of the methods and a comparative discussion with the current state-of-the-art on tactile object recognition is presented.","","","10.1109/JSEN.2019.2912968","Ministerio de Economía y Competitividad; European Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695719","Tactile sensors;object recognition;deep learning","Feature extraction;Object recognition;Tactile sensors;Visualization","computer vision;convolutional neural nets;end effectors;haptic interfaces;image classification;learning (artificial intelligence);object recognition;pressure sensors;robot vision;sensor arrays;support vector machines;tactile sensors","pressure readings;computer vision algorithms;convolutional neural networks;contact objects;high-resolution tactile sensor;robotic end-effector;contacted objects;CNN-based approaches;pressure images;transfer learning approach;pre-trained CNN;RGB-images dataset;tactile information;transfer learning-based;TactNet-based;tactile object recognition;CNN-based methods;high-resolution pressure-sensor arrays","","1","59","","","","","IEEE","IEEE Journals"
"Generalizable Data-Free Objective for Crafting Universal Adversarial Perturbations","K. R. Mopuri; A. Ganeshan; R. V. Babu","Video Analytics Lab, Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India; Video Analytics Lab, Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India; Video Analytics Lab, Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","10","2452","2465","Machine learning models are susceptible to adversarial perturbations: small changes to input that can cause large changes in output. It is also demonstrated that there exist input-agnostic perturbations, called universal adversarial perturbations, which can change the inference of target model on most of the data samples. However, existing methods to craft universal perturbations are (i) task specific, (ii) require samples from the training data distribution, and (iii) perform complex optimizations. Additionally, because of the data dependence, fooling ability of the crafted perturbations is proportional to the available training data. In this paper, we present a novel, generalizable and data-free approach for crafting universal adversarial perturbations. Independent of the underlying task, our objective achieves fooling via corrupting the extracted features at multiple layers. Therefore, the proposed objective is generalizable to craft image-agnostic perturbations across multiple vision tasks such as object recognition, semantic segmentation, and depth estimation. In the practical setting of black-box attack scenario (when the attacker does not have access to the target model and it's training data), we show that our objective outperforms the data dependent objectives to fool the learned models. Further, via exploiting simple priors related to the data distribution, our objective remarkably boosts the fooling ability of the crafted perturbations. Significant fooling rates achieved by our objective emphasize that the current deep learning models are now at an increased risk, since our objective generalizes across multiple tasks without the requirement of training data for crafting the perturbations. To encourage reproducible research, we have released the codes for our proposed algorithm1.","","","10.1109/TPAMI.2018.2861800","Ministry of Human Resource and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8423654","Adversarial perturbations;fooling CNNs;stability of neural networks;perturbations;universal;generalizable attacks;attacks on ML systems;data-free objectives;adversarial noise","Perturbation methods;Task analysis;Data models;Training data;Feature extraction;Image segmentation;Machine learning","feature extraction;learning (artificial intelligence);object recognition","generalizable data-free objective;machine learning models;input-agnostic perturbations;data samples;universal perturbations;training data distribution;data dependence;fooling ability;crafted perturbations;available training data;data-free approach;objective achieves;image-agnostic perturbations;multiple vision tasks;object recognition;data dependent objectives;objective emphasize;current deep learning models;universal adversarial perturbations","","","50","","","","","IEEE","IEEE Journals"
"Morphological Convolutional Neural Network Architecture for Digit Recognition","D. Mellouli; T. M. Hamdani; J. J. Sanchez-Medina; M. Ben Ayed; A. M. Alimi","Department of Electrical and Computer Engineering, Research Groups in Intelligent Machines, National Engineering School of Sfax, University of Sfax, Sfax, Tunisia; Department of Electrical and Computer Engineering, Research Groups in Intelligent Machines, National Engineering School of Sfax, University of Sfax, Sfax, Tunisia; Innovation Center for the Information Society, University of Las Palmas de Gran Canaria, Las Palmas, Spain; Department of Electrical and Computer Engineering, Research Groups in Intelligent Machines, National Engineering School of Sfax, University of Sfax, Sfax, Tunisia; Department of Electrical and Computer Engineering, Research Groups in Intelligent Machines, National Engineering School of Sfax, University of Sfax, Sfax, Tunisia","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","9","2876","2885","Deep neural networks have proved promising results in many applications and fields, but they are still assimilated to a black box. Thus, it is very useful to introduce interpretability aspects to prevent the blind application of deep networks. This paper proposed an interpretable morphological convolutional neural network called Morph-CNN for pattern recognition, where morphological operations were incorporated using counter-harmonic mean into the convolutional layer in order to generate enhanced feature maps. Morph-CNN was extensively evaluated on MNIST and SVHN benchmarks for digit recognition. The different tested configurations showed that Morph-CNN outperforms the existing methods.","","","10.1109/TNNLS.2018.2890334","Ministry of Higher Education and Scientific Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624548","Convolutional neural network (CNN);deep neural networks (DNNs);image recognition;interpretability;morphological CNN (Morph-CNN);morphological operators","Computer architecture;Neural networks;Task analysis;Image recognition;Convolution;Neurons;Morphological operations","convolutional neural nets;image recognition","morphological convolutional neural network architecture;digit recognition;deep neural networks;blind application;deep networks;Morph-CNN;pattern recognition;morphological operations","","","29","","","","","IEEE","IEEE Journals"
"Fusing Body Posture With Facial Expressions for Joint Recognition of Affect in Child–Robot Interaction","P. P. Filntisis; N. Efthymiou; P. Koutras; G. Potamianos; P. Maragos","School of Electrical and Computer Engineering, National Technical University of Athens, Zografou, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Zografou, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Zografou, Greece; Department of Electrical and Computer Engineering, University of Thessaly, Volos, Greece; School of Electrical and Computer Engineering, National Technical University of Athens, Zografou, Greece","IEEE Robotics and Automation Letters","","2019","4","4","4011","4018","In this letter, we address the problem of multi-cue affect recognition in challenging scenarios such as child–robot interaction. Toward this goal we propose a method for automatic recognition of affect that leverages body expressions alongside facial ones, as opposed to traditional methods that typically focus only on the latter. Our deep-learning based method uses hierarchical multi-label annotations and multi-stage losses, can be trained both jointly and separately, and offers us computational models for both individual modalities, as well as for the whole body emotion. We evaluate our method on a challenging child–robot interaction database of emotional expressions collected by us, as well as on the GEneva multimodal emotion portrayal public database of acted emotions by adults, and show that the proposed method achieves significantly better results than facial-only expression baselines.","","","10.1109/LRA.2019.2930434","EU-funded Project BabyRobot H2020; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8769871","Gesture;posture and facial expressions;computer vision for other robotic applications;social human-robot interaction;deep learning in robotics and automation","Emotion recognition;Facial features;Deep learning;Human-robot interaction;Computer vision","","","","","51","Traditional","","","","IEEE","IEEE Journals"
"Preheating Quantification for Smart Hybrid Heat Pumps Considering Uncertainty","M. Sun; G. Strbac; P. Djapic; D. Pudjianto","Department of Electrical & Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical & Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical & Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical & Electronic Engineering, Imperial College London, London, U.K.","IEEE Transactions on Industrial Informatics","","2019","15","8","4753","4763","The deployment of smart hybrid heat pumps (SHHPs) can introduce considerable benefits to electricity systems via smart switching between electricity and gas while minimizing the total heating cost for each individual customer. In particular, the fully optimized control technology can provide flexible heat that redistributes the heat demand across time for improving the utilization of low-carbon generation and enhancing the overall energy efficiency of the heating system. To this end, an accurate quantification of the preheating is of great importance to characterize the flexible heat. This paper proposes a novel data-driven preheating quantification method to estimate the capability of the heat pump demand shifting and isolate the effect of interventions. Varieties of fine-grained data from a real-world trial are exploited to estimate the baseline heat demand using Bayesian deep learning while jointly considering epistemic and aleatoric uncertainties. A comprehensive range of case studies are carried out to demonstrate the superior performance of the proposed quantification method, and then, the estimated demand shift is used as an input into the whole-system model to investigate the system implications and quantify the range of benefits of rolling out the SHHPs developed by PassivSystems to the future GB electricity systems.","","","10.1109/TII.2019.2891089","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603813","Baseline estimation;demand response;deep learning;power system economics;smart hybrid heat pumps (SHHPs)","Heat pumps;Resistance heating;Estimation;Uncertainty;Water heating;Deep learning","belief networks;demand side management;energy conservation;heat pumps;optimal control;optimisation;power system control","smart hybrid heat pumps considering uncertainty;smart switching;total heating cost;flexible heat;low-carbon generation;heat pump demand shifting;fine-grained data;epistemic uncertainties;aleatoric uncertainties;data-driven preheating quantification method;GB electricity systems;optimized control technology;heating system energy efficiency;Bayesian;SHHP","","","32","Traditional","","","","IEEE","IEEE Journals"
"Landslide Inventory Mapping From Bitemporal Images Using Deep Convolutional Neural Networks","T. Lei; Y. Zhang; Z. Lv; S. Li; S. Liu; A. K. Nandi","School of Electronical and Information Engineering, Shaanxi University of Science and Technology, Xi’an, China; School of Electronical and Information Engineering, Shaanxi University of Science and Technology, Xi’an, China; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, China; School of Automation, Xi’an University of Posts and Telecommunications, Xi’an, China; School of Computer Science, Shaanxi Normal University, Xi’an, China; Department of Electronic and Computer Engineering, Brunel University London, Uxbridge, U.K.","IEEE Geoscience and Remote Sensing Letters","","2019","16","6","982","986","Most of the approaches used for Landslide inventory mapping (LIM) rely on traditional feature extraction and unsupervised classification algorithms. However, it is difficult to use these approaches to detect landslide areas because of the complexity and spatial uncertainty of landslides. In this letter, we propose a novel approach based on a fully convolutional network within pyramid pooling (FCN-PP) for LIM. The proposed approach has three advantages. First, this approach is automatic and insensitive to noise because multivariate morphological reconstruction is used for image preprocessing. Second, it is able to take into account features from multiple convolutional layers and explore efficiently the context of images, which leads to a good tradeoff between wider receptive field and the use of context. Finally, the selected PP module addresses the drawback of global pooling employed by convolutional neural network, FCN, and U-Net, and, thus, provides better feature maps for landslide areas. Experimental results show that the proposed FCN-PP is effective for LIM, and it outperforms the state-of-the-art approaches in terms of five metrics, $Precision$ , $Recall$ , $Overall~Error$ , $F$ -$score$ , and $Accuracy$ .","","","10.1109/LGRS.2018.2889307","National Natural Science Foundation of China; National Science Foundation of Shanghai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8618401","Change detection;deep convolutional network;landslide inventory mapping (LIM);multivariate morphological reconstruction (MMR)","Terrain factors;Principal component analysis;Feature extraction;Image reconstruction;Image segmentation;Image color analysis;Task analysis","convolutional neural nets;feature extraction;geomorphology;geophysical image processing;image classification;image segmentation;learning (artificial intelligence)","bitemporal images;deep convolutional neural networks;LIM;traditional feature extraction;unsupervised classification algorithms;landslide areas;complexity;fully convolutional network;pyramid pooling;FCN-PP;multivariate morphological reconstruction;image preprocessing;account features;multiple convolutional layers;convolutional neural network;feature maps;state-of-the-art approaches;landslide inventory mapping","","2","15","CCBY","","","","IEEE","IEEE Journals"
"Semisupervised Stacked Autoencoder With Cotraining for Hyperspectral Image Classification","S. Zhou; Z. Xue; P. Du","School of Earth Sciences and Engineering, Hohai University, Nanjing, China; School of Earth Sciences and Engineering, Hohai University, Nanjing, China; Key Laboratory for Satellite Mapping Technology and Applications of National Administration of Surveying, Mapping and Geoinformation of China, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","6","3813","3826","Recently, deep learning (DL) is of great interest in hyperspectral image (HSI) classification. Although many effective frameworks exist in the literature, the generally limited availability of training samples poses great challenges in applying DL to HSI classification. In this paper, we present a novel DL framework, namely, semisupervised stacked autoencoders (Semi-SAEs) with cotraining, for HSI classification. First, two SAEs are pretrained based on the hyperspectral features and the spatial features, respectively. Second, fine-tuning is alternatively conducted for the two SAEs in a semisupervised cotraining fashion, where the initial training set is enlarged by designing an effective region growing method. Finally, the classification probabilities obtained by the two SAEs are fused using a Markov random field model solved by iterated conditional modes. Experimental results based on three popular hyperspectral data sets demonstrate that the proposed method outperforms other state-of-the-art DL methods.","","","10.1109/TGRS.2018.2888485","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; Fundamental Research Funds for the Central Universities; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing; Open Research Fund in 2018 of Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606445","Deep learning (DL);hyperspectral image (HSI) classification;Markov random field (MRF);semisupervised cotraining;stacked autoencoders (SAEs)","Training;Feature extraction;Hyperspectral imaging;Data mining;Deep learning","hyperspectral imaging;image classification;iterative methods;Markov processes","hyperspectral image classification;HSI classification;hyperspectral features;spatial features;semisupervised stacked autoencoder;region growing method;Markov random field model;iterated conditional modes","","2","40","","","","","IEEE","IEEE Journals"
"Deep Spatial–Temporal 3D Convolutional Neural Networks for Traffic Data Forecasting","S. Guo; Y. Lin; S. Li; Z. Chen; H. Wan","Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Intelligent Transportation Systems","","2019","20","10","3913","3926","Reliable traffic prediction is critical to improve safety, stability, and efficiency of intelligent transportation systems. However, traffic prediction is a very challenging problem because traffic data are a typical type of spatio-temporal data, which simultaneously shows correlation and heterogeneity both in space and time. Most existing works can only capture the partial properties of traffic data and even assume that the effect of correlation on traffic prediction is globally invariable, resulting in inadequate modeling and unsatisfactory prediction performance. In this paper, we propose a novel end-to-end deep learning model, called ST-3DNet, for traffic raster data prediction. ST-3DNet introduces 3D convolutions to automatically capture the correlations of traffic data in both spatial and temporal dimensions. A novel recalibration (Rc) block is proposed to explicitly quantify the difference of the contributions of the correlations in space. Considering two kinds of temporal properties of traffic data, i.e., local patterns and long-term patterns, ST-3DNet employs two components consisting of 3D convolutions and Rc blocks to, respectively, model the two kinds of patterns and then aggregates them together in a weighted way for the final prediction. The experiments on several real-world traffic datasets, viz., traffic congestion data and crowd flows data, demonstrate that our ST-3DNet outperforms the state-of-the-art baselines.","","","10.1109/TITS.2019.2906365","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8684259","Traffic prediction;spatial–temporal data;neural networks;3D convolutions;recalibration block","Three-dimensional displays;Correlation;Forecasting;Data models;Feature extraction;Predictive models;Solid modeling","convolutional neural nets;intelligent transportation systems;learning (artificial intelligence);road traffic;traffic engineering computing","end-to-end deep learning model;traffic raster data prediction;real-world traffic datasets;traffic congestion data;spatial-temporal 3D convolutional neural networks;traffic data forecasting;reliable traffic prediction;spatio-temporal data;ST-3DNet;recalibration block;intelligent transportation systems","","","39","","","","","IEEE","IEEE Journals"
"Combination of modified U-Net and domain adaptation for road detection","M. Dong; X. Zhao; X. Fan; C. Shen; Z. Liu","Chang'an University, School of Information Engineering, Xi'an, Shaanxi, People’s Republic of China; Chang'an University, School of Information Engineering, Xi'an, Shaanxi, People’s Republic of China; Chang'an University, School of Information Engineering, Xi'an, Shaanxi, People’s Republic of China; Chang'an University, School of Information Engineering, Xi'an, Shaanxi, People’s Republic of China; Chang'an University, School of Information Engineering, Xi'an, Shaanxi, People’s Republic of China","IET Image Processing","","2019","13","14","2735","2743","Road detection is one of the crucial tasks for scene understanding in autonomous driving. Recently, methods based on deep learning had rapidly grown and addressed this task excellently, because they can extract more abundant features. In this study, the authors consider the visual road detection problem as a classification for each pixel of the given image, which is road or non-road. There is complex illumination encounter in traffic applications, so that the detection model has poor adaptability. They address this problem by proposing a deep network architecture, which combines the network U-Net-prior and domain adaptation model (DAM). U-Net-prior is a modified segmentation network which integrates location prior and shape prior into U-Net. DAM is a model for reducing the gap between training images and test images, which is optimised in adversarial learning to make the features extracted from different datasets close to each other. They validate the effectiveness of each component of the algorithm, and compare the overall architecture with other state-of-the-art methods, and the results show that the architecture achieves top accuracies with the shortest run time in monocular-vision-based methods, simultaneously, compared with the methods based on other sensors, the architecture also achieves a competitive result.","","","10.1049/iet-ipr.2018.6696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946941","","","learning (artificial intelligence);image classification;object detection;roads;traffic engineering computing;feature extraction;road vehicles;image segmentation;computer vision","modified U-Net;scene understanding;autonomous driving;deep learning;visual road detection problem;complex illumination encounter;detection model;deep network architecture;network U-Net-prior;domain adaptation model;DAM;modified segmentation network;training images;test images;adversarial learning;monocular-vision-based methods","","","57","","","","","IET","IET Journals"
"Focal Boundary Guided Salient Object Detection","Y. Wang; X. Zhao; X. Hu; Y. Li; K. Huang","Center for Research on Intelligent System and Engineering, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent System and Engineering, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Biostatistics and Medical Informatics and the Department of Computer Sciences, University of Wisconsin–Madison, Madison, WI, USA; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2019","28","6","2813","2824","The performance of salient object segmentation has been significantly advanced by using the deep convolutional networks. However, these networks often produce blob-like saliency maps without accurate object boundaries. This is caused by the limited spatial resolution of their feature maps after multiple pooling operations and might hinder downstream applications that require precise object shapes. To address this issue, we propose a novel deep model-Focal Boundary Guided (Focal-BG) network. Our model is designed to jointly learn to segment salient object masks and detect salient object boundaries. Our key idea is that additional knowledge about object boundaries can help to precisely identify the shape of the object. Moreover, our model incorporates a refinement pathway to refine the mask prediction and makes use of the focal loss to facilitate the learning of the hard boundary pixels. To evaluate our model, we conduct extensive experiments. Our Focal-BG network consistently outperforms the state-of-the-art methods on five major benchmarks. We provide a detailed analysis of these results and demonstrate that our joint modeling of salient object boundary and mask helps to better capture the shape details, especially in the vicinity of object boundaries.","","","10.1109/TIP.2019.2891055","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Projects of Chinese Academy of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603790","Visual saliency detection;salient object segmentation;boundary detection;deep learning","Object segmentation;Shape;Decoding;Task analysis;Saliency detection;Image segmentation;Feature extraction","feature extraction;image segmentation;learning (artificial intelligence);object detection","focal boundary guided salient object detection;focal boundary guided salient object detection;salient object boundary;Focal-BG network;hard boundary pixels;focal loss;salient object boundaries;segment salient object masks;precise object shapes;multiple pooling operations;feature maps;accurate object boundaries;blob-like saliency maps;salient object segmentation;deep convolutional networks","","1","51","","","","","IEEE","IEEE Journals"
" $t$ -Exponential Memory Networks for Question-Answering Machines","K. Tolias; S. P. Chatzis","Department of Electrical Engineering, Computer Engineering, and Informatics, Cyprus University of Technology, Limassol, Cyprus; Department of Electrical Engineering, Computer Engineering, and Informatics, Cyprus University of Technology, Limassol, Cyprus","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","8","2463","2477","Recent advances in deep learning have brought to the fore models that can make multiple computational steps in the service of completing a task; these are capable of describing long-term dependencies in sequential data. Novel recurrent attention models over possibly large external memory modules constitute the core mechanisms that enable these capabilities. Our work addresses learning subtler and more complex underlying temporal dynamics in language modeling tasks that deal with sparse sequential data. To this end, we improve upon these recent advances by adopting concepts from the field of Bayesian statistics, namely, variational inference. Our proposed approach consists in treating the network parameters as latent variables with a prior distribution imposed over them. Our statistical assumptions go beyond the standard practice of postulating Gaussian priors. Indeed, to allow for handling outliers, which are prevalent in long observed sequences of multivariate data, multivariate t-exponential distributions are imposed. On this basis, we proceed to infer corresponding posteriors; these can be used for inference and prediction at test time, in a way that accounts for the uncertainty in the available sparse training data. Specifically, to allow for our approach to best exploit the merits of the t-exponential family, our method considers a new t-divergence measure, which generalizes the concept of the Kullback-Leibler divergence. We perform an extensive experimental evaluation of our approach, using challenging language modeling benchmarks, and illustrate its superiority over existing state-of-the-art techniques.","","","10.1109/TNNLS.2018.2884540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8589009","Language modeling;memory networks (MEM-NNs);t-exponential family;variational inference","Computational modeling;Hidden Markov models;Data models;Task analysis;Bayes methods;Uncertainty;Training","Bayes methods;exponential distribution;learning (artificial intelligence);natural language processing;question answering (information retrieval);recurrent neural nets;statistical analysis","t-exponential memory networks;question-answering machines;recent advances;deep learning;fore models;multiple computational steps;long-term dependencies;novel recurrent attention models;possibly large external memory modules;core mechanisms;subtler dynamics;language modeling tasks;sparse sequential data;Bayesian statistics;variational inference;network parameters;latent variables;prior distribution;statistical assumptions;standard practice;postulating Gaussian priors;long observed sequences;multivariate data;t-exponential distributions;corresponding posteriors;prediction;available sparse training data;t-exponential family;challenging language modeling benchmarks;temporal dynamics","","","36","","","","","IEEE","IEEE Journals"
"Two-Stream Convolutional Networks for Blind Image Quality Assessment","Q. Yan; D. Gong; Y. Zhang","School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Image Processing","","2019","28","5","2200","2211","Traditional image quality assessment (IQA) methods do not perform robustly due to the shallow hand-designed features. It has been demonstrated that deep neural network can learn more effective features than ever. In this paper, we describe a new deep neural network to predict the image quality accurately without relying on the reference image. To learn more effective feature representations for non-reference IQA, we propose a two-stream convolution network that includes two subcomponents for image and gradient image. The motivation for this design is using a two-stream scheme to capture different-level information of inputs and easing the difficulty of extracting features from one steam. The gradient stream focuses on extracting structure features in details, and the image stream pays more attention to the information in intensity. In addition, to consider the locally non-uniform distribution of distortion in images, we add a region-based fully convolutional layer for using the information around the center of the input image patch. The final score of the overall image is calculated by averaging of the patch scores. The proposed network performs in an end-to-end manner in both the training and testing phases. The experimental results on a series of benchmark datasets, e.g., LIVE, CISQ, IVC, TID2013, and Waterloo Exploration Database, show that the proposed algorithm outperforms the state-of-the-art methods, which verifies the effectiveness of our network architecture.","","","10.1109/TIP.2018.2883741","National Natural Science Foundation of China; Natural Science Basis Research Plan in Shaanxi Province of China; Chang Jiang Scholars Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550783","Image quality assessment (IQA);feature extraction;deep learning;convolutional neural networks;two-stream networks;no-reference (NR) IQA","Feature extraction;Streaming media;Image quality;Distortion;Data mining;Convolutional neural networks","feature extraction;image classification;image representation;learning (artificial intelligence);neural nets","network architecture;input image patch;region-based fully convolutional layer;image stream;structure features;gradient stream;different-level information;two-stream scheme;gradient image;two-stream convolution network;nonreference IQA;reference image;deep neural network;shallow hand-designed features;blind image quality assessment;stream convolutional networks","","3","52","","","","","IEEE","IEEE Journals"
"Improved appearance loss for deep estimation of image depth","E. H. El-Shazly; X. Zhang; J. Jiang","Shenzhen University, People's Republic of China; Shenzhen University, People's Republic of China; Shenzhen University, People's Republic of China","Electronics Letters","","2019","55","5","264","266","Several recent deep network architectures tried to handle the depth estimation process as an image reconstruction problem, in order to overcome the shortfall that the ground truth depth data is not sufficiently available. The authors introduce and validate an efficient deeper network architecture for unsupervised depth estimation with an automated parameter optimisation. In addition, a hybrid appearance loss function is also proposed to improve the depth estimation accuracy and effectiveness. The authors' proposed model achieves the advantage that individual element of the loss function is weighted using normal distribution characteristics of a Gaussian model. The proposed ideas are validated on KITTI dataset achieving best reported results among recent state-of-the-art methods.","","","10.1049/el.2018.7656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8662824","","","estimation theory;image reconstruction;neural net architecture;optimisation;unsupervised learning","improved appearance loss;image depth;image reconstruction problem;ground truth depth data;efficient deeper network architecture;automated parameter optimisation;hybrid appearance loss function;depth estimation accuracy;unsupervised depth estimation process;normal distribution characteristics;Gaussian model;KITTI dataset","","","","","","","","IET","IET Journals"
"MTBI Identification From Diffusion MR Images Using Bag of Adversarial Visual Features","S. Minaee; Y. Wang; A. Aygar; S. Chung; X. Wang; Y. W. Lui; E. Fieremans; S. Flanagan; J. Rath","Electrical and Computer Engineering Department, New York University, New York City, NY, USA; Electrical and Computer Engineering Department, New York University, New York City, NY, USA; Electrical and Computer Engineering Department, New York University, New York City, NY, USA; Department of Radiology, New York University, New York City, NY, USA; Department of Radiology, New York University, New York City, NY, USA; Department of Radiology, New York University, New York City, NY, USA; Department of Radiology, New York University, New York City, NY, USA; Department of Rehabilitation Medicine, New York University, New York City, NY, USA; Department of Rehabilitation Medicine, New York University, New York City, NY, USA","IEEE Transactions on Medical Imaging","","2019","38","11","2545","2555","In this paper, we propose bag of adversarial features (BAFs) for identifying mild traumatic brain injury (MTBI) patients from their diffusion magnetic resonance images (MRIs) (obtained within one month of injury) by incorporating unsupervised feature learning techniques. MTBI is a growing public health problem with an estimated incidence of over 1.7 million people annually in USA. Diagnosis is based on clinical history and symptoms, and accurate, concrete measures of injury are lacking. Unlike most of the previous works, which use hand-crafted features extracted from different parts of brain for MTBI classification, we employ feature learning algorithms to learn more discriminative representation for this task. A major challenge in this field thus far is the relatively small number of subjects available for training. This makes it difficult to use an end-to-end convolutional neural network to directly classify a subject from MRIs. To overcome this challenge, we first apply an adversarial auto-encoder (with convolutional structure) to learn patch-level features, from overlapping image patches extracted from different brain regions. We then aggregate these features through a bag-of-words approach. We perform an extensive experimental study on a dataset of 227 subjects (including 109 MTBI patients, and 118 age and sex-matched healthy controls) and compare the bag-of-deep-features with several previous approaches. Our experimental results show that the BAF significantly outperforms earlier works relying on the mean values of MR metrics in selected brain regions.","","","10.1109/TMI.2019.2905917","National Institutes of Health; National Institute of Neurological Disorders and Stroke; Center for Advanced Imaging Innovation and Research (CAI2R); NIBIB Biomedical Technology Resource Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668778","MTBI identification;diffusion MRI;deep learning;machine learning","Feature extraction;Magnetic resonance imaging;White matter;Machine learning;Measurement;Visualization","","","","","48","","","","","IEEE","IEEE Journals"
"Sound Event Detection in the DCASE 2017 Challenge","A. Mesaros; A. Diment; B. Elizalde; T. Heittola; E. Vincent; B. Raj; T. Virtanen","Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland; Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland; Machine Learning for Signal Processing Group, Carnegie Mellon University, Pittsburgh, PA, USA; Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland; CNRS, Inria, LORIA, Universite de Lorraine, Nancy, France; Machine Learning for Signal Processing Group, Carnegie Mellon University, Pittsburgh, PA, USA; Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","6","992","1006","Each edition of the challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) contained several tasks involving sound event detection in different setups. DCASE 2017 presented participants with three such tasks, each having specific datasets and detection requirements: Task 2, in which target sound events were very rare in both training and testing data, Task 3 having overlapping events annotated in real-life audio, and Task 4, in which only weakly labeled data were available for training. In this paper, we present three tasks, including the datasets and baseline systems, and analyze the challenge entries for each task. We observe the popularity of methods using deep neural networks, and the still widely used mel frequency-based representations, with only few approaches standing out as radically different. Analysis of the systems behavior reveals that task-specific optimization has a big role in producing good performance; however, often this optimization closely follows the ranking metric, and its maximization/minimization does not result in universally good performance. We also introduce the calculation of confidence intervals based on a jackknife resampling procedure to perform statistical analysis of the challenge results. The analysis indicates that while the 95% confidence intervals for many systems overlap, there are significant differences in performance between the top systems and the baseline for all tasks.","","","10.1109/TASLP.2019.2907016","H2020 European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673582","Sound event detection;weak labels;pattern recognition;jackknife estimates;confidence intervals","Event detection;Task analysis;Training;Acoustics;Speech processing;Glass;Hidden Markov models","acoustic signal detection;acoustic signal processing;learning (artificial intelligence);neural nets;signal classification;signal representation;statistical analysis","sound event detection;DCASE 2017 challenge;detection requirements;target sound events;overlapping events;weakly labeled data;task-specific optimization;confidence interval calculation;statistical analysis;jackknife resampling procedure;maximization-minimization;ranking metric;deep neural networks;training data;testing data;Detection and Classification of Acoustic Scenes and Events;mel frequency-based representations","","2","54","","","","","IEEE","IEEE Journals"
"Autonomous 3-D Reconstruction, Mapping, and Exploration of Indoor Environments With a Robotic Arm","Y. Wang; S. James; E. K. Stathopoulou; C. Beltrán-González; Y. Konishi; A. Del Bue","Visual Geometry and Modelling (VGM) Laboratory, Istituto Italiano di Tecnologia (IIT), Genova, Italy; Center for Cultural Heritage Technology (CCHT), Istituto Italiano di Tecnologia (IIT), Venezia, Italy; E. K. Stathopoulou was with Visual Geometry and Modelling Laboratory, Istituto Italiano di Tecnologia, Genova, Italy; Pattern Analysis and Computer Vision (PAVIS) Department, Istituto Italiano di Tecnologia (IIT), Genova, Italy; OMRON Research, Kyoto, Japan; Visual Geometry and Modelling (VGM) Laboratory, Istituto Italiano di Tecnologia (IIT), Genova, Italy","IEEE Robotics and Automation Letters","","2019","4","4","3340","3347","We propose a novel information gain metric that combines hand-crafted and data-driven metrics to address the next best view problem for autonomous 3-D mapping of unknown indoor environments. For the hand-crafted metric, we propose an entropy-based information gain that accounts for the previous view points to avoid the camera to revisit the same location and to promote the motion toward unexplored or occluded areas. However, for the learnt metric, we adopt a convolutional neural network (CNN) architecture and formulate the problem as a classification problem. The CNN takes the current depth image as input and outputs the motion direction that suggests the largest unexplored surface. We train and test the CNN using a new synthetic dataset based on the SUNCG dataset. The learnt motion direction is then combined with the proposed hand-crafted metric to help handle situations where using only the hand-crafted metric tends to face ambiguities. We finally evaluate the autonomous paths over several real and synthetic indoor scenes including complex industrial and domestic settings and prove that our combined metric is able to further improve the exploration coverage compared to using only the proposed hand-crafted metric.","","","10.1109/LRA.2019.2926676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754778","Computer vision for automation;range sensing;deep learning in robotics and automation","Measurement;Three-dimensional displays;Robot sensing systems;Computer vision;Deep learning;Convolutional neural nets","","","","","26","Traditional","","","","IEEE","IEEE Journals"
"Deep Multi-Modality Adversarial Networks for Unsupervised Domain Adaptation","X. Ma; T. Zhang; C. Xu","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Multimedia","","2019","21","9","2419","2431","Unsupervised domain adaptation aims to transfer domain knowledge from existing well-defined tasks to new ones where labels are unavailable. In the real-world applications, domain discrepancy is usually uncontrollable especially for multi-modality data. Therefore, it is significantly motivated to deal with a multi-modality domain adaptation task. As labels are unavailable in a target domain, how to learn semantic multi-modality representations and successfully adapt the classifier from a source to the target domain remain open challenges in a multi-modality domain adaptation task. To deal with these issues, we propose a multi-modality adversarial network (MMAN), which applies stacked attention to learn semantic multi-modality representations and reduces domain discrepancy via adversarial training. Unlike the previous domain adaptation methods, which cannot make full use of source domain categories information, multi-channel constraint is employed to capture fine-grained categories of knowledge that could enhance the discrimination of target samples and boost target performance on single-modality and multi-modality domain adaptation problems. We apply the proposed MMAN to two applications including cross-domain object recognition and cross-domain social event recognition. The extensive experimental evaluations demonstrate the effectiveness of the proposed model for unsupervised domain adaptation.","","","10.1109/TMM.2019.2902100","National Natural Science Foundation of China; Key Research Program of Frontier Sciences; Chinese Academy of Sciences; Beijing Natural Science Foundation; Youth Innovation Promotion Association Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8656504","Unsupervised domain adaptation;triplet loss;stacked attention;multi-modality;social event recognition","Feature extraction;Task analysis;Semantics;Training;Adaptation models;Correlation;Data mining","feature extraction;pattern classification;unsupervised learning","cross-domain object recognition;cross-domain social event recognition;unsupervised domain adaptation;deep multimodality adversarial networks;multimodality domain adaptation;multimodality representations;domain knowledge transfer;feature extractor;transferable multimodality feature learning","","","78","Traditional","","","","IEEE","IEEE Journals"
"Mixture separability loss in a deep convolutional network for image classification","T. D. Do; C. Jin; V. H. Nguyen; H. Kim","Inha University, Republic of Korea; Inha University, Republic of Korea; Ton Duc Thang University, Vietnam; Inha University, Republic of Korea","IET Image Processing","","2019","13","1","135","141","In machine learning, the cost function is crucial because it measures how good or bad a system is. In image classification, well-known networks only consider modifying the network structures and applying cross-entropy loss at the end of the network. However, using only cross-entropy loss causes a network to stop updating weights when all training images are correctly classified. This is the problem of early saturation. This study proposes a novel cost function, called mixture separability loss (MSL), which updates the weights of the network even when most of the training images are accurately predicted. MSL consists of between-class and within-class loss. Between-class loss maximises the differences between inter-class images, whereas within-class loss minimises the similarities between intra-class images. They designed the proposed loss function to attach to different convolutional layers in the network in order to utilise intermediate feature maps. Experiments show that a network with MSL deepens the learning process and obtains promising results with some public datasets, such as Street View House Number, Canadian Institute for Advanced Research, and the authors' self-collected Inha Computer Vision Lab gender dataset.","","","10.1049/iet-ipr.2018.5613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574089","","","computer vision;entropy;feedforward neural nets;image classification;image representation;learning (artificial intelligence)","intra-class images;loss function;MSL;deep convolutional network;image classification;machine learning;well-known networks;network structures;applying cross-entropy loss;training images;novel cost function;between-class loss;inter-class images;convolutional layers;mixture separability loss;within-class loss;street view house number;Canadian institute for advanced research;self-collected Inha computer vision lab gender dataset","","","33","","","","","IET","IET Journals"
"Supervised Mixed Norm Autoencoder for Kinship Verification in Unconstrained Videos","N. Kohli; D. Yadav; M. Vatsa; R. Singh; A. Noore","Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA; Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA; IIIT-Delhi, New Delhi, India; IIIT-Delhi, New Delhi, India; College of Engineering, Texas A&M University–Kingsville, Kingsville, TX, USA","IEEE Transactions on Image Processing","","2019","28","3","1329","1341","Identifying kinship relations has garnered interest due to several applications such as organizing and tagging the enormous amount of videos being uploaded on the Internet. Existing research in kinship verification primarily focuses on kinship prediction with image pairs. In this research, we propose a new deep learning framework for kinship verification in unconstrained videos using a novel Supervised Mixed Norm AutoEncoder (SMNAE). This new autoencoder formulation introduces class-specific sparsity in the weight matrix. The proposed three-stage SMNAE based kinship verification framework utilizes the learned spatio-temporal representation in the video frames for verifying kinship in a pair of videos. A new kinship video (KIVI) database of more than 500 individuals with variations due to illumination, pose, occlusion, ethnicity, and expression is collected for this research. It comprises a total of 355 true kin video pairs with over 250 000 still frames. The effectiveness of the proposed framework is demonstrated on the KIVI database and six existing kinship databases. On the KIVI database, SMNAE yields video-based kinship verification accuracy of 83.18% which is at least 3.2% better than existing algorithms. The algorithm is also evaluated on six publicly available kinship databases and compared with best reported results. It is observed that the proposed SMNAE consistently yields best results on all the databases.","","","10.1109/TIP.2018.2840880","Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8365817","","Videos;Databases;Machine learning;Measurement;Image reconstruction;Feature extraction;Lighting","feature extraction;image recognition;image representation;learning (artificial intelligence);matrix algebra;object detection;video signal processing","autoencoder formulation;three-stage SMNAE based kinship verification framework;learned spatio-temporal representation;video frames;kinship video database;KIVI database;unconstrained videos;kinship prediction;deep learning framework;kinship relation identification;video-based kinship verification accuracy;true kin video pairs;supervised mixed norm autoencoder;Internet;image pairs;class-specific sparsity;weight matrix","","2","52","","","","","IEEE","IEEE Journals"
"The Labeled Multiple Canonical Correlation Analysis for Information Fusion","L. Gao; R. Zhang; L. Qi; E. Chen; L. Guan","Department of Electrical and Computer Engineering, Ryerson University, Toronto, ON, Canada; Epson Canada, Toronto, ON, Canada; School of Information Engineering, Zhengzhou University, Zhengzhou, China; School of Information Engineering, Zhengzhou University, Zhengzhou, China; Department of Electrical and Computer Engineering, Ryerson University, Toronto, ON, Canada","IEEE Transactions on Multimedia","","2019","21","2","375","387","The objective of multimodal information fusion is to mathematically analyze information carried in different sources and create a new representation that will be more effectively utilized in pattern recognition and other multimedia information processing tasks. In this paper, we introduce a new method for multimodal information fusion and representation based on the Labeled Multiple Canonical Correlation Analysis (LMCCA). By incorporating class label information of the training samples, the proposed LMCCA ensures that the fused features carry discriminative characteristics of the multimodal information representations and are capable of providing superior recognition performance. We implement a prototype of LMCCA to demonstrate its effectiveness on handwritten digit recognition, face recognition, and object recognition utilizing multiple features, bimodal human emotion recognition involving information from both audio and visual domains. The generic nature of LMCCA allows it to take as input features extracted by any means, including those by deep learning (DL) methods. Experimental results show that the proposed method enhanced the performance of both statistical machine learning methods, and methods based on DL.","","","10.1109/TMM.2018.2859590","National Natural Science Foundation of China; State Key Program of NSFC; Key International Collaboration Program of NSFC; Discovery Grant of Natural Science and Engineering Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419299","Labeled multiple canonical correlation analysis (LMCCA);information fusion;handwritten digit recognition;face recognition;object recognition;human emotion recognition","Correlation;Feature extraction;Training;Object recognition;Emotion recognition;Face recognition;Machine learning","correlation methods;emotion recognition;face recognition;feature extraction;image fusion;image recognition;learning (artificial intelligence);neural nets;object recognition;speech recognition;statistical analysis","labeled multiple canonical correlation analysis;multimodal information fusion;pattern recognition;LMCCA;multimodal information representations;handwritten digit recognition;face recognition;bimodal human emotion recognition;multimedia information processing;object recognition;feature extraction;deep learning;statistical machine learning","","3","69","","","","","IEEE","IEEE Journals"
"Learning Where to See: A Novel Attention Model for Automated Immunohistochemical Scoring","T. Qaiser; N. M. Rajpoot","Department of Computer Science, University of Warwick, Coventry, U.K.; Department of Computer Science, University of Warwick, Coventry, U.K.","IEEE Transactions on Medical Imaging","","2019","38","11","2620","2631","Estimating over-amplification of human epidermal growth factor receptor 2 (HER2) on invasive breast cancer is regarded as a significant predictive and prognostic marker. We propose a novel deep reinforcement learning (DRL)-based model that treats immunohistochemical (IHC) scoring of HER2 as a sequential learning task. For a given image tile sampled from multi-resolution giga-pixel whole slide image (WSI), the model learns to sequentially identify some of the diagnostically relevant regions of interest (ROIs) by following a parameterized policy. The selected ROIs are processed by recurrent and residual convolution networks to learn the discriminative features for different HER2 scores and predict the next location, without requiring to process all the sub-image patches of a given tile for predicting the HER2 score, mimicking the histopathologist who would not usually analyze every part of the slide at the highest magnification. The proposed model incorporates a task-specific regularization term and inhibition of return mechanism to prevent the model from revisiting the previously attended locations. We evaluated our model on two IHC datasets: a publicly available dataset from the HER2 scoring challenge contest and another dataset consisting of WSIs of gastroenteropancreatic neuroendocrine tumor sections stained with Glo1 marker. We demonstrate that the proposed model outperforms other methods based on state-of-the-art deep convolutional networks. To the best of our knowledge, this is the first study using DRL for IHC scoring and could potentially lead to wider use of DRL in the domain of computational pathology reducing the computational burden of the analysis of large multi-gigapixel histology images.","","","10.1109/TMI.2019.2907049","University of Warwick; University Hospital Coventry Warwickshire (UHCW); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672928","Deep reinforcement learning;computational pathology;immunohistochemical scoring;breast cancer","Predictive models;Task analysis;Tumors;Computational modeling;Pathology;Breast;Training","","","","","46","","","","","IEEE","IEEE Journals"
"Unsupervised Semantic-Based Aggregation of Deep Convolutional Features","J. Xu; C. Wang; C. Qi; C. Shi; B. Xiao","University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2019","28","2","601","611","In this paper, we propose a simple but effective semantic-based aggregation (SBA) method. The proposed SBA utilizes the discriminative filters of deep convolutional layers as semantic detectors. Moreover, we propose the effective unsupervised strategy to select some semantic detectors to generate the “soft region proposals,” which highlight certain discriminative pattern of objects and suppress the noise of background. The final global SBA representation could then be acquired by aggregating the regional representations weighted by the selected “soft region proposals” corresponding to various semantic content. Our unsupervised SBA is easy to generalize and achieves excellent performance on various tasks. We conduct comprehensive experiments and show that our unsupervised SBA outperforms the state-of-the-art unsupervised and supervised aggregation methods on image retrieval, place recognition, and cloud classification.","","","10.1109/TIP.2018.2867104","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445607","Unsupervised;semantic-based aggregation;semantic detectors","Semantics;Proposals;Task analysis;Image retrieval;Detectors;Training;Image representation","feedforward neural nets;image classification;image representation;image retrieval;unsupervised learning","unsupervised semantic-based aggregation;deep convolutional features;aggregation method;discriminative filters;deep convolutional layers;semantic detectors;supervised aggregation methods","","1","72","","","","","IEEE","IEEE Journals"
"Reference-Guided Deep Super-Resolution via Manifold Localized External Compensation","W. Yang; S. Xia; J. Liu; Z. Guo","Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","5","1270","1283","The rapid development of social network and online multimedia technology makes it possible to address traditional image and video enhancement problems, with the aid of online similar reference data. In this paper, we tackle the problem of super-resolution (SR) in this way, specifically aiming to handle the “one-to-many” problem between the image patches of low resolution (LR) and high resolution (HR). We propose a manifold localized deep external compensation (MALDEC) network to additionally utilize reference images, i. e., retrieved similar images in cloud database and reference HR frame in a video, to provide an accurate localization and mapping to the HR manifold, and compensate the lost high-frequency details. The proposed network employs a three-step recovery: 1) internal structure inference, which uses the LR image itself and the internally inferred high frequency information to preserve main structure of the HR image; 2) manifold localization, which localizes the HR manifold and constructs the correspondence between the internal inferred image and the external images; and 3) external compensation, which introduces the external references of retrieved similar patches based on manifold localization information to reconstruct the high-frequency details. The learnable components of MALDEC, internal structure inference, and external compensation, are trained jointly to make a good tradeoff between these two terms for an optimal SR result. Finally, the proposed method is examined under three tasks: cloud-based image SR, multi-pose face reconstruction, and reference frame-guided video SR. Extensive experiments demonstrate the superiority of our method than the state-of-the-art SR methods in both objective and subjective evaluations, and our method offers new state-of-the-art performance.","","","10.1109/TCSVT.2018.2838453","National Natural Science Foundation of China; CCF-Tencent Open Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8361037","Super-resolution;manifold localization;external compensation;internal structure inference","Manifolds;Image resolution;Image reconstruction;Estimation;Databases;Semantics;Face","cloud computing;face recognition;image enhancement;image reconstruction;image resolution;image retrieval;image texture;learning (artificial intelligence);video signal processing","video enhancement problems;manifold localized deep external compensation network;multipose face reconstruction;cloud-based image SR method;social network;reference-guided deep super-resolution;reference frame-guided video SR;internal structure inference;manifold localization information;internal inferred image;HR image;internally inferred high frequency information;LR image;cloud database;high resolution;low resolution;image patches;online similar reference data;online multimedia technology","","4","65","","","","","IEEE","IEEE Journals"
"Ultrasound Image Segmentation: A Deeply Supervised Network With Attention to Boundaries","D. Mishra; S. Chaudhury; M. Sarkar; A. S. Soin","Department of Electrical Engineering, Indian Institute of Technology Delhi, New Delhi, India; Department of Electrical EngineeringIndian Institute of Technology Delhi; Department of Electrical EngineeringIndian Institute of Technology Delhi; Medanta Hospital","IEEE Transactions on Biomedical Engineering","","2019","66","6","1637","1648","Objective: Segmentation of anatomical structures in ultrasound images requires vast radiological knowledge and experience. Moreover, the manual segmentation often results in subjective variations, therefore, an automatic segmentation is desirable. We aim to develop a fully convolutional neural network (FCNN) with attentional deep supervision for the automatic and accurate segmentation of the ultrasound images. Method: FCNN/CNNs are used to infer high-level context using low-level image features. In this paper, a sub-problem specific deep supervision of the FCNN is performed. The attention of fine resolution layers is steered to learn object boundary definitions using auxiliary losses, whereas coarse resolution layers are trained to discriminate object regions from the background. Furthermore, a customized scheme for downweighting the auxiliary losses and a trainable fusion layer are introduced. This produces an accurate segmentation and helps in dealing with the broken boundaries, usually found in the ultrasound images. Results: The proposed network is first tested for blood vessel segmentation in liver images. It results in F1 score, mean intersection over union, and dice index of 0.83, 0.83, and 0.79, respectively. The best values observed among the existing approaches are produced by U-net as 0.74, 0.81, and 0.75, respectively. The proposed network also results in dice index value of 0.91 in the lumen segmentation experiments on MICCAI 2011 IVUS challenge dataset, which is near to the provided reference value of 0.93. Furthermore, the improvements similar to vessel segmentation experiments are also observed in the experiment performed to segment lesions. Conclusion: Deep supervision of the network based on the input-output characteristics of the layers results in improvement in overall segmentation accuracy. Significance: Sub-problem specific deep supervision for ultrasound image segmentation is the main contribution of this paper. Currently the network is trained and tested for fixed size inputs. It requires image resizing and limits the performance in small size images.","","","10.1109/TBME.2018.2877577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502126","Ultrasound image segmentation;convolutional neural network;sub-problem specific deep supervision","Image segmentation;Biomedical imaging;Image resolution;Liver;Blood;Ultrasonic imaging","biomedical ultrasonics;blood vessels;convolutional neural nets;image segmentation;learning (artificial intelligence);liver;medical image processing","ultrasound image segmentation;deeply supervised network;manual segmentation;automatic segmentation;fully convolutional neural network;attentional deep supervision;high-level context;low-level image features;fine resolution layers;object boundary definitions;auxiliary losses;coarse resolution layers;object regions;trainable fusion layer;blood vessel segmentation;liver images;lumen segmentation experiments;vessel segmentation experiments;segment lesions;segmentation accuracy;image resizing;small size images;anatomical structures;FCNN-CNN;subproblem specific deep supervision;F1 score;MICCAI 2011 IVUS challenge dataset;input-output characteristics","","1","55","","","","","IEEE","IEEE Journals"
"Semantic Prior Analysis for Salient Object Detection","T. V. Nguyen; K. Nguyen; T. Do","Department of Computer Science, University of Dayton, Dayton, OH, USA; University of Information Technology, Ho Chi Minh, Vietnam; Department of Computer Science, University of Liverpool, Liverpool, U.K.","IEEE Transactions on Image Processing","","2019","28","6","3130","3141","Salient object detection aims to detect the main objects in the given image. In this paper, we propose an approach that integrates semantic priors into the salient object detection process. The method first obtains an explicit saliency map that is refined by the explicit semantic priors learned from data. Then an implicit saliency map is constructed using a trained model that maps the implicit semantic priors embedded into superpixel features with the saliency values. Next, the fusion saliency map is computed by adaptively fusing both the explicit and implicit semantic maps. The final saliency map is eventually computed via the post-processing refinement step. Experimental results have demonstrated the effectiveness of the proposed method; particularly, it achieves competitive performance with the state-of-the-art baselines on three challenging datasets, namely, ECSSD, HKUIS, and iCoSeg.","","","10.1109/TIP.2019.2894284","Nvidia; Vietnam National University HoChiMinh City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624611","Salient object detection;semantic priors;deep networks","Semantics;Object detection;Image color analysis;Deep learning;Saliency detection;Task analysis;Visualization","computer vision;feature extraction;image colour analysis;image segmentation;learning (artificial intelligence);object detection","salient object detection process;semantic prior analysis;post-processing refinement step;final saliency map;fusion saliency map;saliency values;implicit semantic priors;implicit saliency map;explicit semantic priors;explicit saliency map","","3","61","","","","","IEEE","IEEE Journals"
"Complex-Valued Restricted Boltzmann Machine for Speaker-Dependent Speech Parameterization From Complex Spectra","T. Nakashika; S. Takaki; J. Yamagishi","Graduate School of Informatics and Engineering, The University of Electro-Communications, Tokyo, Japan; National Institute of Informatics, Tokyo, Japan; National Institute of Informatics, Tokyo, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","2","244","254","This paper describes a novel energy-based probabilistic distribution that represents complex-valued data and explains how to apply it to direct feature extraction from complex-valued spectra. The proposed model, the complex-valued restricted Boltzmann machine (CRBM), is designed to deal with complex-valued visible units as an extension of the well-known restricted Boltzmann machine (RBM). Like the RBM, the CRBM learns the relationships between visible and hidden units without having connections between units in the same layer, which dramatically improves training efficiency by using Gibbs sampling or contrastive divergence. Another important characteristic is that the CRBM also has connections between real and imaginary parts of each of the complex-valued visible units that help represent the data distribution in the complex domain. In speech signal processing, classification and generation features are often based on amplitude spectra (e.g., MFCC, cepstra, and mel-cepstra) even if they are calculated from complex spectra, and they ignore phase information. In contrast, the proposed feature extractor using the CRBM directly encodes the complex spectra (or another complex-valued representation of the complex spectra) into binary-valued latent features (hidden units). Since the visible-hidden connections are undirected, we can also recover (decode) the complex spectra from the latent features directly. Our speech representation experiments demonstrated that the CRBM outperformed other speech representation methods, such as methods using a conventional RBM, a mel-log spectrum approximate decoder, etc.","","","10.1109/TASLP.2018.2877465","Japan Science and Technology Agency (JST) Advanced Information and Communication Technology for Innovation (ACT-I); JST Core Research for Evolutionary Science and Technology (CREST); Ministry of Education, Culture, Sports, Science and Technology/Japan Society for the Promotion of Science (MEXT/JSPS) Grants-in-Aid for Scientific Research (KAKENHI); Telecommunications Advancement Foundation Grant; Nakajima Foundation Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502065","Restricted Boltzmann machine;deep learning;complex-valued representation;feature extraction;speech synthesis","Speech processing;Feature extraction;Speech recognition;Task analysis;Mel frequency cepstral coefficient","Boltzmann machines;cepstral analysis;feature extraction;hidden Markov models;learning (artificial intelligence);signal classification;speech processing;speech recognition","CRBM;complex-valued representation;complex-valued data;complex-valued spectra;complex-valued restricted Boltzmann machine;speaker-dependent speech parameterization;deep learning;feature extraction;Gibbs sampling;contrastive divergence;speech signal processing","","","33","","","","","IEEE","IEEE Journals"
"Fast and Continuous Foothold Adaptation for Dynamic Locomotion Through CNNs","O. A. V. Magaña; V. Barasuol; M. Camurri; L. Franceschi; M. Focchi; M. Pontil; D. G. Caldwell; C. Semini","Dynamic Legged Systems Lab, Istituto Italiano di Tecnologia, Genoa, Italy; Dynamic Legged Systems Lab, Istituto Italiano di Tecnologia, Genoa, Italy; Dynamic Legged Systems Lab, Istituto Italiano di Tecnologia, Genoa, Italy; Computational Statistics and Machine Learning, Istituto Italiano di Tecnologia, Genoa, Italy; Dynamic Legged Systems Lab, Istituto Italiano di Tecnologia, Genoa, Italy; Computational Statistics and Machine Learning, Istituto Italiano di Tecnologia, Genoa, Italy; Department of Advanced Robotics, Istituto Italiano di Tecnologia, Genoa, Italy; Dynamic Legged Systems Lab, Istituto Italiano di Tecnologia, Genoa, Italy","IEEE Robotics and Automation Letters","","2019","4","2","2140","2147","Legged robots can outperform wheeled machines for most navigation tasks across unknown and rough terrains. For such tasks, visual feedback is a fundamental asset to provide robots with terrain awareness. However, robust dynamic locomotion on difficult terrains with real-time performance guarantees remains a challenge. We present here a real-time, dynamic foothold adaptation strategy based on visual feedback. Our method adjusts the landing position of the feet in a fully reactive manner, using only on-board computers and sensors. The correction is computed and executed continuously along the swing phase trajectory of each leg. To efficiently adapt the landing position, we implement a self-supervised foothold classifier based on a convolutional neural network. Our method results in an up to 200 times faster computation with respect to the full-blown heuristics. Our goal is to react to visual stimuli from the environment, bridging the gap between blind reactive locomotion and purely vision-based planning strategies. We assess the performance of our method on the dynamic quadruped robot HyQ, executing static and dynamic gaits (at speeds up to 0.5 m/s) in both simulated and real scenarios; the benefit of safe foothold adaptation is clearly demonstrated by the overall robot behavior.","","","10.1109/LRA.2019.2899434","Istituto Italiano di Tecnologia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642374","Legged Robots;Reactive and Sensor-Based Planning;Deep Learning in Robotics and Automation","Legged locomotion;Robot sensing systems;Foot;Visualization;Trajectory","convolutional neural nets;legged locomotion;path planning;robot dynamics","dynamic gaits;safe foothold adaptation;robot behavior;legged robots;navigation tasks;unknown terrains;rough terrains;visual feedback;terrain awareness;robust dynamic locomotion;real-time performance guarantees;dynamic foothold adaptation strategy;landing position;fully reactive manner;on-board computers;swing phase trajectory;self-supervised foothold classifier;convolutional neural network;method results;visual stimuli;blind reactive locomotion;static gaits;CNN;vision-based system;HyQ dynamic quadruped robot","","2","23","","","","","IEEE","IEEE Journals"
"Edge-Enhanced GAN for Remote Sensing Image Superresolution","K. Jiang; Z. Wang; P. Yi; G. Wang; T. Lu; J. Jiang","National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; Hubei Province Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","8","5799","5812","The current superresolution (SR) methods based on deep learning have shown remarkable comparative advantages but remain unsatisfactory in recovering the high-frequency edge details of the images in noise-contaminated imaging conditions, e.g., remote sensing satellite imaging. In this paper, we propose a generative adversarial network (GAN)-based edge-enhancement network (EEGAN) for robust satellite image SR reconstruction along with the adversarial learning strategy that is insensitive to noise. In particular, EEGAN consists of two main subnetworks: an ultradense subnetwork (UDSN) and an edge-enhancement subnetwork (EESN). In UDSN, a group of 2-D dense blocks is assembled for feature extraction and to obtain an intermediate high-resolution result that looks sharp but is eroded with artifacts and noises as previous GAN-based methods do. Then, EESN is constructed to extract and enhance the image contours by purifying the noise-contaminated components with mask processing. The recovered intermediate image and enhanced edges can be combined to generate the result that enjoys high credibility and clear contents. Extensive experiments on Kaggle Open Source Data set, Jilin-1 video satellite images, and Digitalglobe show superior reconstruction performance compared to the state-of-the-art SR approaches.","","","10.1109/TGRS.2019.2902431","National Natural Science Foundation of China; National Key Research and Development Project; Hubei Province Technological Innovation Major Project; Central Government Guided Local Science and Technology Development Projects; Basic Research Program of Shenzhen City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8677274","Adversarial learning;dense connection;edge enhancement;remote sensing imagery;superresolution","Image edge detection;Image reconstruction;Satellites;Feature extraction;Image resolution;Gallium nitride;Generative adversarial networks","edge detection;feature extraction;gallium compounds;geophysical image processing;image enhancement;image reconstruction;image resolution;learning (artificial intelligence);remote sensing","edge-enhanced GAN;remote sensing image superresolution;current superresolution methods;deep learning;remarkable comparative advantages;high-frequency edge details;remote sensing satellite imaging;generative adversarial network-based edge-enhancement network;EEGAN;robust satellite image SR reconstruction;adversarial learning strategy;main subnetworks;ultradense subnetwork;UDSN;edge-enhancement subnetwork;EESN;intermediate high-resolution result;artifacts;image contours;noise-contaminated components;Jilin-1 video satellite images;mask processing","","4","63","","","","","IEEE","IEEE Journals"
"Multimodal Representation Learning for Recommendation in Internet of Things","Z. Huang; X. Xu; J. Ni; H. Zhu; C. Wang","School of Computer Science, South China Normal University, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China; School of Politics and Administration, South China Normal University, Guangzhou, China; Department of Computer Science and Engineering, Bengbu University, Bengbu, China; Department of Computer Science and Engineering, Tongji University, Shanghai, China","IEEE Internet of Things Journal","","2019","6","6","10675","10685","The recommender system has recently drawn a lot of attention to the communities of information services and mobile applications. Many deep learning-based recommendation models have been proposed to learn the feature representations from items. However, in Internet of Things (IoT), items’ description information are typically heterogeneous and multimodal, posing a challenge to items’ representation learning of recommendation models. To address this challenge and to improve the recommendation effectiveness in IoT, a novel multimodal representation learning-based model (MRLM) has been proposed. In MRLM, two closely related modules were trained simultaneously; they are global feature representation learning and multimodal feature representation learning. The former was designed to learn to accurately represent the global features of items and users through simultaneous training on three tasks: 1) triplet metric learning; 2) softmax classification; and 3) microscopic verification. The latter was proposed to refine items’ global features and to generate the final multimodal features by using items’ multimodal description information. After MRLM converged, items’ multimodal features and users’ global features could be used to calculate users’ preferences on items via cosine similarity. Through extensive experiments on two real-world datasets, MRLM remarkably improved the recommendation effectiveness in IoT.","","","10.1109/JIOT.2019.2940709","National Natural Science Foundation of China; Natural Science Foundation of Shanghai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832204","Deep learning;Internet of Things (IoT);multimodal representation;multitask optimization;recommender system","Feature extraction;Internet of Things;Data models;Training;Task analysis;Data mining;Data preprocessing","","","","1","39","IEEE","","","","IEEE","IEEE Journals"
"Learning Coupled Convolutional Networks Fusion for Video Saliency Prediction","Z. Wu; L. Su; Q. Huang","School of Computer and Control Engineering, University of Chinese Academy of Sciences (UCAS), Beijing, China; School of Computer and Control Engineering, University of Chinese Academy of Sciences (UCAS), Beijing, China; School of Computer and Control Engineering, University of Chinese Academy of Sciences (UCAS), Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","10","2960","2971","Visual saliency provides important information for understanding scenes in many computer vision tasks. The existing video saliency algorithms mainly focus on predicting spatial and temporal saliency maps. However, these maps are simply fused without considering the complex dynamic scenes in videos. To overcome this drawback, we propose a deep convolutional fusion framework for video saliency prediction. The proposed model, which is based on coupled fully convolutional networks (FCNs), effectively encodes the spatiotemporal information by integrating spatial and temporal features. We demonstrate that this information is helpful for accurately fusing the spatial and temporal saliency maps according to changes in video scenes. In particular, we gradually design three different deep fusion architectures to investigate how to better utilize the spatiotemporal information. Moreover, we propose a reasonable sampling strategy for selecting suitable training sets for the coupled FCNs. Through extensive experiments, we demonstrate that our model outperforms the state-of-the-art algorithms on four public video saliency data sets.","","","10.1109/TCSVT.2018.2870954","National Natural Science Foundation of China; Key Research Program of Frontier Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8474977","Video saliency;feature integration;fully convolutional network","Feature extraction;Visualization;Computational modeling;Spatiotemporal phenomena;Training;Biological system modeling;Data mining","computer vision;convolutional neural nets;feature extraction;image motion analysis;inference mechanisms;learning (artificial intelligence);object detection;sensor fusion;video signal processing","convolutional networks fusion;video saliency prediction;visual saliency;understanding scenes;computer vision tasks;spatial saliency maps;temporal saliency maps;complex dynamic scenes;deep convolutional fusion framework;coupled fully convolutional networks;spatiotemporal information;spatial features;temporal features;video scenes;coupled FCNs;public video saliency data sets;deep fusion architectures","","","51","","","","","IEEE","IEEE Journals"
"Segmentation of Nuclei in Histopathology Images by Deep Regression of the Distance Map","P. Naylor; M. Laé; F. Reyal; T. Walter","Center for Computational Biology, MINES ParisTech, PSL Research University, Paris, France; Pathology Department, Institut Curie, Paris, France; Translational Research Department, Residual Tumor and Response to Treatment Laboratory, RT2Lab, Institut Curie, Paris, France; Center for Computational Biology, MINES ParisTech, PSL Research University, Paris, France","IEEE Transactions on Medical Imaging","","2019","38","2","448","459","The advent of digital pathology provides us with the challenging opportunity to automatically analyze whole slides of diseased tissue in order to derive quantitative profiles that can be used for diagnosis and prognosis tasks. In particular, for the development of interpretable models, the detection and segmentation of cell nuclei is of the utmost importance. In this paper, we describe a new method to automatically segment nuclei from Haematoxylin and Eosin (H&E) stained histopathology data with fully convolutional networks. In particular, we address the problem of segmenting touching nuclei by formulating the segmentation problem as a regression task of the distance map. We demonstrate superior performance of this approach as compared to other approaches using Convolutional Neural Networks.","","","10.1109/TMI.2018.2865709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8438559","Cancer research;deep learning;digital pathology;histopathology;nuclei segmentation","Image segmentation;Cancer;Pathology;Task analysis;Biology;Tumors;Computer architecture","biological tissues;cancer;cellular biophysics;diseases;image segmentation;medical image processing;neural nets;patient diagnosis","histopathology images;deep regression;distance map;digital pathology;diseased tissue;quantitative profiles;prognosis tasks;interpretable models;cell nuclei;histopathology data;fully convolutional networks;segmentation problem;regression task;Haematoxylin and Eosin stained histopathology data","","3","50","","","","","IEEE","IEEE Journals"
"Visualization Methods for Image Transformation Convolutional Neural Networks","É. Protas; J. D. Bratti; J. F. O. Gaya; P. Drews; S. S. C. Botelho","Universidade Federal do Rio Grande, Rio Grande, NAUTEC, Brazil; Universidade Federal do Rio Grande, Rio Grande, NAUTEC, Brazil; Universidade Federal do Rio Grande, Rio Grande, NAUTEC, Brazil; Universidade Federal do Rio Grande, Rio Grande, NAUTEC, Brazil; Universidade Federal do Rio Grande, Rio Grande, NAUTEC, Brazil","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","7","2231","2243","Convolutional neural networks (CNNs) are powerful machine learning models that have become the state of the art in several problems in the areas of computer vision and image processing. Nevertheless, the knowledge of why and how these models present an impressive performance is still limited. There are visualization techniques that can help us to understand the inner working of neural networks. However, they have mostly been applied to classification models. In this paper, we evaluate the application of visualization methods to networks where the input and output are images of proportional dimensions. The results show that visualization brings visual cues associated with how these systems work, helping in their understanding and improvement. We use the knowledge obtained from the visualization of an image restoration CNN to improve the architecture's efficiency with no significant degradation of its performance.","","","10.1109/TNNLS.2018.2881194","Agência Nacional do Petróleo, Gás Natural e Biocombustíveis; Brazilian National Institute of Science and Technology—INCT-Mar COI through CNPq; CAPES; CNPq; FAPERGS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8573154","Activation maximization;convolutional neural networks (CNNs);deep visualization;dehazing;underwater","Visualization;TV;Neurons;Optimization;Computational modeling;Computer architecture;Convolutional neural networks","computer vision;convolutional neural nets;data visualisation;image restoration;learning (artificial intelligence)","image transformation convolutional neural networks;computer vision;image processing;visualization techniques;inner working;classification models;visual cues;image restoration CNN;machine learning models","","1","46","","","","","IEEE","IEEE Journals"
"Unsupervised Knowledge Transfer Using Similarity Embeddings","N. Passalis; A. Tefas","Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","3","946","950","With the advent of deep neural networks, there is a growing interest in transferring the knowledge from a large and complex model to a smaller and faster one. In this brief, a method for unsupervised knowledge transfer (KT) between neural networks is proposed. To the best of our knowledge, the proposed method is the first method that utilizes similarity-induced embeddings to transfer the knowledge between any two layers of neural networks, regardless of the number of neurons in each of them. By this way, the knowledge is transferred without using any lossy dimensionality reduction transformations or requiring any information about the complex model, except for the activations of the layer used for KT. This is in contrast with most existing approaches that only generate soft-targets for training the smaller neural network or directly use the weights of the larger model. The proposed method is evaluated using six image data sets and it is demonstrated, through extensive experiments, that the knowledge of a neural network can be successfully transferred using different kinds of (synthetic or not) data, ranging from cross-domain data to just randomly generated data.","","","10.1109/TNNLS.2018.2851924","Horizon 2020 Framework Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419072","Knowledge transfer (KT);neural network distillation;similarity embeddings;unsupervised learning","Receivers;Knowledge engineering;Training;Data models;Biological neural networks;Computational modeling","neural nets;unsupervised learning","unsupervised knowledge transfer;similarity embeddings;deep neural networks;complex model;KT;utilizes similarity-induced embeddings;smaller neural network","","1","25","","","","","IEEE","IEEE Journals"
"Hydroelectric Generating Unit Fault Diagnosis Using 1-D Convolutional Neural Network and Gated Recurrent Unit in Small Hydro","G. Liao; W. Gao; G. Yang; M. Guo","Department of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Department of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Department of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Department of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China","IEEE Sensors Journal","","2019","19","20","9352","9363","Machine learning algorithm based on hand-crafted features from the raw vibration signal has shown promising results in the hydroelectric generating unit (HGU) fault diagnosis in recent years. Such methodologies, nevertheless, can lead to important information loss in representing the vibration signal, which intrinsically relies on engineering experience of diagnostic experts and prior knowledge about feature extraction techniques. Therefore, in this paper, an effective and stable HGU fault diagnosis system using one-dimensional convolutional neural network (1-D CNN) and gated recurrent unit (GRU) based on the sequence data structure is proposed. First, the raw vibration data is reconstructed by data segmentation, which can improve training efficiency. Second, the reconstruction data under the influence of different running conditions and various fault factors can be effectively and adaptively learned by 1-D CNN-GRU and then determine information fault categories via network inference. Finally, four machine learning methods are applied to diagnosis the reconstruction data based on the experimental dataset. The performance of the proposed method is verified by comparing with the results of other machine learning techniques. Furthermore, the fault diagnostic model, which is trained by the practical vibration signal, has successfully applied in engineering practice.","","","10.1109/JSEN.2019.2926095","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752440","Hydroelectric generating unit (HGU);fault diagnosis;1-dimension convolutional neural network (1-D CNN);gated recurrent unit (GRU)","Vibrations;Fault diagnosis;Sensors;Feature extraction;Deep learning;Acceleration;Hydroelectric power generation","condition monitoring;convolutional neural nets;fault diagnosis;feature extraction;hydroelectric generators;learning (artificial intelligence);mechanical engineering computing;vibrations","raw vibration signal;hydroelectric generating unit fault diagnosis;important information loss;diagnostic experts;feature extraction techniques;effective HGU fault diagnosis system;stable HGU fault diagnosis system;one-dimensional convolutional neural network;gated recurrent unit;sequence data structure;raw vibration data;data segmentation;reconstruction data;fault factors;1-D CNN-GRU;information fault categories;network inference;machine learning techniques;fault diagnostic model;practical vibration signal;1-D convolutional neural network;hand-crafted features","","","43","","","","","IEEE","IEEE Journals"
"Learning Long-Range Perception Using Self-Supervision From Short-Range Sensors and Odometry","M. Nava; J. Guzzi; R. O. Chavez-Garcia; L. M. Gambardella; A. Giusti","USI-SUPSI, Dalle Molle Institute for Artificial Intelligence (IDSIA), Lugano, Switzerland; USI-SUPSI, Dalle Molle Institute for Artificial Intelligence (IDSIA), Lugano, Switzerland; USI-SUPSI, Dalle Molle Institute for Artificial Intelligence (IDSIA), Lugano, Switzerland; USI-SUPSI, Dalle Molle Institute for Artificial Intelligence (IDSIA), Lugano, Switzerland; USI-SUPSI, Dalle Molle Institute for Artificial Intelligence (IDSIA), Lugano, Switzerland","IEEE Robotics and Automation Letters","","2019","4","2","1279","1286","We introduce a general self-supervised approach to predict the future outputs of a short-range sensor (such as a proximity sensor) given the current outputs of a long-range sensor (such as a camera). We assume that the former is directly related to some piece of information to be perceived (such as the presence of an obstacle in a given position), whereas the latter is information rich but hard to interpret directly. We instantiate and implement the approach on a small mobile robot to detect obstacles at various distances using the video stream of the robot's forward-pointing camera, by training a convolutional neural network on automatically-acquired datasets. We quantitatively evaluate the quality of the predictions on unseen scenarios, qualitatively evaluate robustness to different operating conditions, and demonstrate usage as the sole input of an obstacle-avoidance controller. We additionally instantiate the approach on a different simulated scenario with complementary characteristics, to exemplify the generality of our contribution.","","","10.1109/LRA.2019.2894849","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624299","Range sensing;computer vision for other robotic applications;deep learning in robotics and automation","Cameras;Robot vision systems;Task analysis;Mobile robots","cameras;collision avoidance;convolutional neural nets;distance measurement;learning (artificial intelligence);mobile robots;neurocontrollers;robot vision;video signal processing;video streaming","long-range perception;short-range sensor;self-supervised approach;proximity sensor;long-range sensor;odometry;small mobile robot;obstacle detection;video stream;robot forward-pointing camera;convolutional neural network training;automatically-acquired datasets;obstacle-avoidance controller","","","28","","","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Multiaccess Control and Battery Prediction With Energy Harvesting in IoT Systems","M. Chu; H. Li; X. Liao; S. Cui","Department of Information and Communication Engineering, Xi’an Jiaotong University, Xi’an, China; Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen, China; Department of Information and Communication Engineering, Xi’an Jiaotong University, Xi’an, China; Shenzhen Research Institute of Big Data and the School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China","IEEE Internet of Things Journal","","2019","6","2","2009","2020","Energy harvesting (EH) is a promising technique to fulfill the long-term and self-sustainable operations for Internet of Things (IoT) systems. In this paper, we study the joint access control and battery prediction problems in a small-cell IoT system including multiple EH user equipments (UEs) and one base station (BS) with limited uplink access channels. Each UE has a rechargeable battery with finite capacity. The system control is modeled as a Markov decision process without complete prior knowledge assumed at the BS, which also deals with large sizes in both state and action spaces. First, to handle the access control problem assuming causal battery and channel state information, we propose a scheduling algorithm that maximizes the uplink transmission sum rate based on reinforcement learning (RL) with deep Q -network enhancement. Second, for the battery prediction problem, with a fixed round-robin access control policy adopted, we develop an RL-based algorithm to minimize the prediction loss (error) without any model knowledge about the energy source and energy arrival process. Finally, the joint access control and battery prediction problem is investigated, where we propose a two-layer RL network to simultaneously deal with maximizing the sum rate and minimizing the prediction loss: the first layer is for battery prediction, the second layer generates the access policy based on the output from the first layer. Experiment results show that the three proposed RL algorithms can achieve better performances compared with existing benchmarks.","","","10.1109/JIOT.2018.2872440","National Natural Science Foundation of China; Division of Astronomical Sciences; Shenzhen Fundamental Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8473693","Access control;battery prediction;energy harvesting (EH);Internet of Things (IoT);reinforcement learning (RL)","Batteries;Access control;Uplink;Internet of Things;Wireless communication;Prediction algorithms;Neural networks","energy harvesting;Internet of Things;learning (artificial intelligence);Markov processes;MIMO communication;multi-access systems;optimisation;telecommunication traffic;wireless channels","reinforcement learning-based multiaccess control;energy harvesting;IoT systems;joint access control;battery prediction problem;small-cell IoT system;multiple EH user equipments;UE;base station;BS;uplink access channels;rechargeable battery;system control;causal battery;channel state information;uplink transmission sum rate;fixed round-robin access control policy;prediction loss;energy source;energy arrival process;access policy;Internet of Things systems","","9","37","","","","","IEEE","IEEE Journals"
"Prediction of Sea Ice Motion With Convolutional Long Short-Term Memory Networks","Z. I. Petrou; Y. Tian","Department of Electrical Engineering, The City College, The City University of New York, New York, NY, USA; Department of Electrical Engineering, The City College, The City University of New York, New York, NY, USA","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","9","6865","6876","Prediction of sea ice motion is important for safeguarding human activities in polar regions, such as ship navigation, fisheries, and oil and gas exploration, as well as for climate and ocean-atmosphere interaction models. Numerical prediction models used for sea ice motion prediction often require a large number of data from diverse sources with varying uncertainties. In this paper, a deep learning approach is proposed to predict sea ice motion for several days in the future, given only a series of past motion observations. The proposed approach consists of an encoder-decoder network with convolutional long short-term memory (LSTM) units. Optical flow is calculated from satellite passive microwave and scatterometer daily images covering the entire Arctic and used in the network. The network proves able to learn long-time dependencies within the motion time series, whereas its convolutional structure effectively captures spatial correlations among neighboring motion vectors. The approach is unsupervised and end-to-end trainable, requiring no manual annotation. Experiments demonstrate that the proposed approach is effective in predicting sea ice motion of up to 10 days in the future, outperforming previous deep learning networks and being a promising alternative or complementary approach to resource-demanding numerical prediction methods.","","","10.1109/TGRS.2019.2909057","Office of Naval Research; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8701505","Advanced Microwave Scanning Radiometer–Earth Observing System (AMSR-E);Advanced Scatterometer (ASCAT);AMSR2;Arctic sea ice;convLSTM;deep neural networks;drift prediction;optical flow;recurrent neural networks (RNNs)","Sea ice;Predictive models;Arctic;Data models;Atmospheric modeling;Numerical models;Microwave radiometry","geophysics computing;learning (artificial intelligence);oceanographic regions;recurrent neural nets;sea ice;time series;vectors","sea ice motion prediction;motion observations;motion time series;convolutional long short-term memory networks;human activities;deep learning approach;motion vectors","","","74","","","","","IEEE","IEEE Journals"
"Convolutional neural network and multi-feature fusion for automatic modulation classification","H. Wu; Y. Li; L. Zhou; J. Meng","Naval University of Engineering, People's Republic of China; Naval University of Engineering, People's Republic of China; Naval University of Engineering, People's Republic of China; Naval University of Engineering, People's Republic of China","Electronics Letters","","2019","55","16","895","897","Automatic modulation classification (AMC) lies at the core of cognitive radio and spectrum sensing. In this Letter, the authors propose a novel convolutional neural network (CNN)-based AMC method with multi-feature fusion. First, the modulation signals are transformed into two image representations of cyclic spectra (CS) and constellation diagram (CD), respectively. Then, a two-branch CNN model is developed, a gradient decent strategy is adopted and a multi-feature fusion technique is exploited to integrate the features learned from CS and CD. The proposed method is computationally efficient, benefited from its simple neural network. Experimental results show that the proposed method can achieve identical or better results with much reduced learned parameters and training time, compared with the state-of-the-art deep learning-based methods.","","","10.1049/el.2019.1789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789823","","","cognitive radio;convolutional neural nets;feature extraction;image representation;learning (artificial intelligence);modulation;pattern classification;signal classification;signal detection","automatic modulation classification;cognitive radio;spectrum sensing;modulation signals;CS;constellation diagram;CD;two-branch CNN model;gradient decent strategy;multifeature fusion technique;simple neural network;state-of-the-art deep learning-based methods;convolutional neural network-based AMC method","","","10","","","","","IET","IET Journals"
"Learning Compact and Discriminative Stacked Autoencoder for Hyperspectral Image Classification","P. Zhou; J. Han; G. Cheng; B. Zhang","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","7","4823","4833","As one of the fundamental research topics in remote sensing image analysis, hyperspectral image (HSI) classification has been extensively studied so far. However, how to discriminatively learn a low-dimensional feature space, in which the mapped features have small within-class scatter and big between-class separation, is still a challenging problem. To address this issue, this paper proposes an effective framework, named compact and discriminative stacked autoencoder (CDSAE), for HSI classification. The proposed CDSAE framework comprises two stages with different optimization objectives, which can learn discriminative low-dimensional feature mappings and train an effective classifier progressively. First, we impose a local Fisher discriminant regularization on each hidden layer of stacked autoencoder (SAE) to train discriminative SAE (DSAE) by minimizing reconstruction error. This stage can learn feature mappings, in which the pixels from the same land-cover class are mapped as nearly as possible and the pixels from different land-cover categories are separated by a large margin. Second, we learn an effective classifier and meanwhile update DSAE with a local Fisher discriminant regularization being embedded on the top of feature representations. Moreover, to learn a compact DSAE with as small number of hidden neurons as possible, we impose a diversity regularization on the hidden neurons of DSAE to balance the feature dimensionality and the feature representation capability. The experimental results on three widely-used HSI data sets and comprehensive comparisons with existing methods demonstrate that our proposed method is effective.","","","10.1109/TGRS.2019.2893180","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Young Star of Science and Technology in Shaanxi Province; Northwestern Polytechnical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641458","Discriminative stacked autoencoder (DSAE);diversity regularization;hyperspectral image (HSI) classification;local Fisher discriminative regularization","Feature extraction;Training;Hyperspectral imaging;Neurons;Kernel;Deep learning","feature extraction;hyperspectral imaging;image classification;image reconstruction;image representation;neural nets;optimisation","hyperspectral image classification;remote sensing image analysis;HSI classification;local Fisher discriminant regularization;feature dimensionality;compact and discriminative stacked autoencoder;CDSAE;discriminative low-dimensional feature mappings;diversity regularization;feature representation","","3","61","","","","","IEEE","IEEE Journals"
"Learning to Predict Ego-Vehicle Poses for Sampling-Based Nonholonomic Motion Planning","H. Banzhaf; P. Sanzenbacher; U. Baumann; J. M. Zöllner","Corporate Research, Automated Driving, Robert Bosch GmbH, Renningen, Germany; Corporate Research, Automated Driving, Robert Bosch GmbH, Renningen, Germany; Corporate Research, Automated Driving, Robert Bosch GmbH, Renningen, Germany; FZI Research Center for Information Technology, Karlsruhe, Germany","IEEE Robotics and Automation Letters","","2019","4","2","1053","1060","Pathological hand tremor (PHT) is among the most common movement symptoms of several neurological disorders including Parkinson's disease and essential tremor. Extracting PHT is of paramount importance in several engineering and clinical applications such as assistive and robotic rehabilitation technologies. In such systems, PHT is modeled as the input noise to the system and thus there is a surge of interest in estimation an compensation of the noise. Although various works in the literature have attempted to estimate and extract the PHT, in this letter, first, we argue that the ground truth signal used in existing works to optimize the performance of tremor extraction techniques is not accurate enough, and thus the performance measures for the prior techniques are not perfectly reliable. In addition, most of the existing tremor extraction techniques impose unrealistic assumptions, which are, typically, violated in practical settings. This letter proposes a novel technique that for the first time incorporates deep bidirectional recurrent neural networks as a processing tool for PHT extraction. Moreover, we devise an intuitively pleasing training strategy that enables the network to perform not only online estimation but also online prediction of the voluntary hand motion in a myopic fashion, which is currently a significantly important unmet need for rehabilitative and assistive robotic technologies designed for patients with pathological tremor.","","","10.1109/LRA.2019.2893975","Robert Bosch; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8618353","Deep learning in robotics and automation;nonholonomic motion planning;motion and path planning","Planning;Trajectory;Convergence;Task analysis;Robot sensing systems;Computer architecture","biomechanics;diseases;filtering theory;medical disorders;medical robotics;medical signal processing;neurophysiology;patient rehabilitation;patient treatment;recurrent neural nets","PHT extraction;online estimation;online prediction;voluntary hand motion;rehabilitative technologies;assistive robotic technologies;pathological tremor;HMFP-DBRNN;real-time hand motion filtering;pathological hand tremor;common movement symptoms;neurological disorders;essential tremor;clinical applications;assistive rehabilitation technologies;robotic rehabilitation technologies;ground truth signal;performance measures;deep bidirectional recurrent neural networks;tremor extraction techniques;Parkinsons disease","","","30","","","","","IEEE","IEEE Journals"
"A Gated Peripheral-Foveal Convolutional Neural Network for Unified Image Aesthetic Prediction","X. Zhang; X. Gao; W. Lu; L. He","Video and Image Processing System Laboratory, School of Electronic Engineering, Xidian University, Xian, China; State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xian, China; Video and Image Processing System Laboratory, School of Electronic Engineering, Xidian University, Xian, China; Video and Image Processing System Laboratory, School of Electronic Engineering, Xidian University, Xian, China","IEEE Transactions on Multimedia","","2019","21","11","2815","2826","Learning fine-grained details is a key issue in image aesthetic assessment. Most of the previous methods extract the fine-grained details via random cropping strategy, which may undermine the integrity of semantic information. Extensive studies show that humans perceive fine-grained details with a mixture of foveal vision and peripheral vision. Fovea has the highest possible visual acuity and is responsible for seeing the details. The peripheral vision is used for perceiving the broad spatial scene and selecting the attended regions for the fovea. Inspired by these observations, we propose a gated peripheral-foveal convolutional neural network. It is a dedicated double-subnet neural network (i.e., a peripheral subnet and a foveal subnet). The former aims to mimic the functions of peripheral vision to encode the holistic information and provide the attended regions. The latter aims to extract fine-grained features on these key regions. Considering that the peripheral vision and foveal vision play different roles in processing different visual stimuli, we further employ a gated information fusion network to weigh their contributions. The weights are determined through the fully connected layers followed by a sigmoid function. We conduct comprehensive experiments on the standard Aesthetic Visual Analysis (AVA) dataset and Photo.net dataset for unified aesthetic prediction tasks: 1) aesthetic quality classification; 2) aesthetic score regression; and 3) aesthetic score distribution prediction. The experimental results demonstrate the effectiveness of the proposed method.","","","10.1109/TMM.2019.2911428","National Natural Science Foundation of China; National Key Research and Development Program of China; National High-Level Talents Special Support Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691784","Visual aesthetic quality assessment;image aesthetics;deep learning","Feature extraction;Logic gates;Visualization;Convolutional neural networks;Task analysis;Deep learning;Image resolution","computer vision;convolutional neural nets;feature extraction;image classification;regression analysis;visual perception","peripheral vision;gated peripheral-foveal convolutional neural network;double-subnet neural network;peripheral subnet;foveal subnet;foveal vision;gated information fusion network;unified aesthetic prediction tasks;fine-grained details;image aesthetic assessment;visual acuity;unified image aesthetic prediction;standard aesthetic visual analysis dataset;broad spatial scene;fine-grained feature extraction;visual stimuli;sigmoid function;aesthetic quality classification;aesthetic score regression;aesthetic score distribution prediction","","1","44","Traditional","","","","IEEE","IEEE Journals"
"3DFR: A Swift 3D Feature Reductionist Framework for Scene Independent Change Detection","M. Mandal; V. Dhar; A. Mishra; S. K. Vipparthi","Vision Intelligence Lab, Department of Computer Science and Engineering, Malaviya National Institute of Technology, Jaipur, India; School of Computing and Information Technology, Manipal University, Jaipur, India; School of Computing and Information Technology, Manipal University, Jaipur, India; Vision Intelligence Lab, Department of Computer Science and Engineering, Malaviya National Institute of Technology, Jaipur, India","IEEE Signal Processing Letters","","2019","26","12","1882","1886","In this paper we propose an end-to-end swift 3D feature reductionist framework (3DFR) for scene independent change detection. The 3DFR framework consists of three feature streams: a swift 3D feature reductionist stream (AvFeat), a contemporary feature stream (ConFeat) and a temporal median feature map. These multilateral foreground/background features are further refined through an encoder-decoder network. As a result, the proposed framework not only detects temporal changes but also learns high-level appearance features. Thus, it incorporates the object semantics for effective change detection. Furthermore, the proposed framework is validated through a scene independent evaluation scheme in order to demonstrate the robustness and generalization capability of the network. The performance of the proposed method is evaluated on the benchmark CDnet 2014 dataset. The experimental results show that the proposed 3DFR network outperforms the state-of-the-art approaches.","","","10.1109/LSP.2019.2952253","Science and Engineering Research Board; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894435","Change detection;scene independent;segmentation;deep learning;spatiotemporal;reductionist","Feature extraction;Videos;Spatiotemporal phenomena;History;Three-dimensional displays;Deep learning;Kernel","","","","","29","IEEE","","","","IEEE","IEEE Journals"
"PET Image Reconstruction Using Deep Image Prior","K. Gong; C. Catana; J. Qi; Q. Li","Gordon Center for Medical Imaging, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA; Martinos Center for Biomedical Imaging, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA; Department of Biomedical Engineering, University of California at Davis, Davis, CA, USA; Gordon Center for Medical Imaging, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA","IEEE Transactions on Medical Imaging","","2019","38","7","1655","1665","Recently, deep neural networks have been widely and successfully applied in computer vision tasks and have attracted growing interest in medical imaging. One barrier for the application of deep neural networks to medical imaging is the need for large amounts of prior training pairs, which is not always feasible in clinical practice. This is especially true for medical image reconstruction problems, where raw data are needed. Inspired by the deep image prior framework, in this paper, we proposed a personalized network training method where no prior training pairs are needed, but only the patient’s own prior information. The network is updated during the iterative reconstruction process using the patient-specific prior information and measured data. We formulated the maximum-likelihood estimation as a constrained optimization problem and solved it using the alternating direction method of multipliers algorithm. Magnetic resonance imaging guided positron emission tomography reconstruction was employed as an example to demonstrate the effectiveness of the proposed framework. Quantification results based on simulation and real data show that the proposed reconstruction framework can outperform Gaussian post-smoothing and anatomically guided reconstructions using the kernel method or the neural-network penalty.","","","10.1109/TMI.2018.2888491","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8581448","Medical image reconstruction;deep neural network;unsupervised learning;positron emission tomography","Image reconstruction;Training;Neural networks;Biomedical imaging;Kernel;Positron emission tomography","","","","3","53","","","","","IEEE","IEEE Journals"
"Multisource Region Attention Network for Fine-Grained Object Recognition in Remote Sensing Imagery","G. Sumbul; R. G. Cinbis; S. Aksoy","Department of Computer Engineering, Bilkent University, Ankara, Turkey; Department of Computer Engineering, METU, Ankara, Turkey; Department of Computer Engineering, Bilkent University, Ankara, Turkey","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","7","4929","4937","Fine-grained object recognition concerns the identification of the type of an object among a large number of closely related subcategories. Multisource data analysis that aims to leverage the complementary spectral, spatial, and structural information embedded in different sources is a promising direction toward solving the fine-grained recognition problem that involves low between-class variance, small training set sizes for rare classes, and class imbalance. However, the common assumption of coregistered sources may not hold at the pixel level for small objects of interest. We present a novel methodology that aims to simultaneously learn the alignment of multisource data and the classification model in a unified framework. The proposed method involves a multisource region attention network that computes per-source feature representations, assigns attention scores to candidate regions sampled around the expected object locations by using these representations, and classifies the objects by using an attention-driven multisource representation that combines the feature representations and the attention scores from all sources. All components of the model are realized using deep neural networks and are learned in an end-to-end fashion. Experiments using RGB, multispectral, and LiDAR elevation data for classification of street trees showed that our approach achieved 64.2% and 47.3% accuracies for the 18-class and 40-class settings, respectively, which correspond to 13% and 14.3% improvement relative to the commonly used feature concatenation approach from multiple sources.","","","10.1109/TGRS.2019.2894425","Türkiye Bilimsel ve Teknolojik Araştirma Kurumu; Bilim Akademisi; Orta Doğu Teknik Üniversitesi; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648481","Deep learning;fine-grained classification;image alignment;multisource classification;object recognition","Object recognition;Vegetation;Laser radar;Remote sensing;Spatial resolution;Sensors;Feature extraction","data analysis;feature extraction;image classification;image recognition;learning (artificial intelligence);neural nets;object recognition;remote sensing","multisource region attention network;remote sensing imagery;multisource data analysis;structural information;fine-grained recognition problem;between-class variance;class imbalance;coregistered sources;candidate regions;expected object locations;attention-driven multisource representation;deep neural networks;LiDAR elevation data;fine-grained object recognition;attention scores;per-source feature representations;feature concatenation approach","","1","32","","","","","IEEE","IEEE Journals"
"Cascaded Recurrent Neural Networks for Hyperspectral Image Classification","R. Hang; Q. Liu; D. Hong; P. Ghamisi","Jiangsu Key Laboratory of Big Data Analysis Technology, School of Automation, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, School of Automation, Nanjing University of Information Science and Technology, Nanjing, China; Remote Sensing Technology Institute, German Aerospace Center, Wessling, Germany; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","8","5384","5394","By considering the spectral signature as a sequence, recurrent neural networks (RNNs) have been successfully used to learn discriminative features from hyperspectral images (HSIs) recently. However, most of these models only input the whole spectral bands into RNNs directly, which may not fully explore the specific properties of HSIs. In this paper, we propose a cascaded RNN model using gated recurrent units to explore the redundant and complementary information of HSIs. It mainly consists of two RNN layers. The first RNN layer is used to eliminate redundant information between adjacent spectral bands, while the second RNN layer aims to learn the complementary information from nonadjacent spectral bands. To improve the discriminative ability of the learned features, we design two strategies for the proposed model. Besides, considering the rich spatial information contained in HSIs, we further extend the proposed model to its spectral-spatial counterpart by incorporating some convolutional layers. To test the effectiveness of our proposed models, we conduct experiments on two widely used HSIs. The experimental results show that our proposed models can achieve better results than the compared models.","","","10.1109/TGRS.2019.2899129","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8662780","Gated recurrent unit (GRU);hyperspectral image (HSI) classification;recurrent neural network (RNN);spectral feature;spectral–spatial feature","Recurrent neural networks;Training;Feature extraction;Hyperspectral imaging;Deep learning;Logic gates","hyperspectral imaging;image classification;learning (artificial intelligence);recurrent neural nets","recurrent neural networks;hyperspectral image classification;spectral signature;discriminative features;hyperspectral images;cascaded RNN model;recurrent units;complementary information;RNN layer;redundant information;adjacent spectral bands;nonadjacent spectral bands;discriminative ability;learned features;spectral-spatial counterpart;convolutional layers;HSI;spatial information","","7","50","","","","","IEEE","IEEE Journals"
"Weakly Supervised Fruit Counting for Yield Estimation Using Spatial Consistency","E. Bellocchio; T. A. Ciarfuglia; G. Costante; P. Valigi","Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy","IEEE Robotics and Automation Letters","","2019","4","3","2348","2355","Fruit counting is a fundamental component for yield estimation applications. Most of the existing approaches address this problem by relying on fruit models (i.e., by using object detectors) or by explicitly learning to count. Despite the impressive results achieved by these approaches, all of them need strong supervision information during the training phase. In agricultural applications, manual labeling may require a huge effort or, in some cases, it could be impossible to acquire fine-grained ground truth labels. In this letter, we tackle this problem by proposing a weakly supervised framework that learns to count fruits without the need for task-specific supervision labels. In particular, we devise a novel convolutional neural network architecture that requires only a simple image level binary classifier to detect whether the image contains instances of the fruits or not and combines this information with image spatial consistency constraints. The result is an architecture that learns to count without task-specific labels (e.g., object bounding boxes or the multiplicity of fruit instances in the image). The experiments on three different varieties of fruits (i.e., olives, almonds, and apples) show that our approach reaches performances that are comparable with SotA approaches based on the supervised paradigm.","","","10.1109/LRA.2019.2903260","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661632","Agricultural automation;computer vision for other robotic applications;deep learning in robotics and automation;robotics in agriculture and forestry;visual learning","Yield estimation;Detectors;Task analysis;Computer architecture;Robots;Training;Labeling","image classification;image segmentation;learning (artificial intelligence);neural nets;object detection","fundamental component;yield estimation applications;fruit models;object detectors;impressive results;strong supervision information;training phase;agricultural applications;manual labeling;fine-grained ground truth labels;weakly supervised framework;task-specific supervision labels;novel convolutional neural network architecture;simple image level;image spatial consistency constraints;task-specific labels;object bounding boxes;fruit instances;SotA approaches;supervised paradigm;weakly supervised fruit counting","","","47","","","","","IEEE","IEEE Journals"
"Cardiac Phase Detection in Echocardiograms With Densely Gated Recurrent Neural Networks and Global Extrema Loss","F. Taheri Dezaki; Z. Liao; C. Luong; H. Girgis; N. Dhungel; A. H. Abdi; D. Behnami; K. Gin; R. Rohling; P. Abolmaesumi; T. Tsang","Department of Electrical and Computer Engineering, The University ofBritish Columbia, Vancouver, Canada; Department of Electrical and Computer Engineering, The University ofBritish Columbia, Vancouver, Canada; Department of Medicine, Vancouver General Hospital, Echocardiography Laboratory, Division of Cardiology, The University of British Columbia, Vancouver, BC, Canada; Department of Medicine, Vancouver General Hospital, Echocardiography Laboratory, Division of Cardiology, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University ofBritish Columbia, Vancouver, Canada; Department of Electrical and Computer Engineering, The University ofBritish Columbia, Vancouver, Canada; Department of Electrical and Computer Engineering, The University ofBritish Columbia, Vancouver, Canada; Department of Medicine, Vancouver General Hospital, Echocardiography Laboratory, Division of Cardiology, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University ofBritish Columbia, Vancouver, Canada; Department of Electrical and Computer Engineering, The University ofBritish Columbia, Vancouver, Canada; Department of Medicine, Vancouver General Hospital, Echocardiography Laboratory, Division of Cardiology, The University of British Columbia, Vancouver, BC, Canada","IEEE Transactions on Medical Imaging","","2019","38","8","1821","1832","Accurate detection of end-systolic (ES) and end-diastolic (ED) frames in an echocardiographic cine series can be difficult but necessary pre-processing step for the development of automatic systems to measure cardiac parameters. The detection task is challenging due to variations in cardiac anatomy and heart rate often associated with pathological conditions. We formulate this problem as a regression problem and propose several deep learning-based architectures that minimize a novel global extrema structured loss function to localize the ED and ES frames. The proposed architectures integrate convolution neural networks (CNNs)-based image feature extraction model and recurrent neural networks (RNNs) to model temporal dependencies between each frame in a sequence. We explore two CNN architectures: DenseNet and ResNet, and four RNN architectures: long short-term memory, bi-directional LSTM, gated recurrent unit (GRU), and Bi-GRU, and compare the performance of these models. The optimal deep learning model consists of a DenseNet and GRU trained with the proposed loss function. On average, we achieved 0.20 and 1.43 frame mismatch for the ED and ES frames, respectively, which are within reported inter-observer variability for the manual detection of these frames.","","","10.1109/TMI.2018.2888807","Natural Sciences and Engineering Research Council of Canada; Canadian Institutes of Health Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8586941","Deep residual neural networks;densely-connected networks;recurrent neural networks;long short term memory;gated recurrent unit;bi-directional RNN;echocardiography;cardiac cycle phase detection","Recurrent neural networks;Echocardiography;Magnetic resonance imaging;Electrocardiography;Phase detection;Logic gates","convolutional neural nets;echocardiography;feature extraction;image sequences;learning (artificial intelligence);medical image processing;neural net architecture;recurrent neural nets;regression analysis","cardiac anatomy;heart rate;regression problem;CNN architectures;RNN architectures;bi-directional LSTM;gated recurrent unit;Bi-GRU;cardiac phase detection;echocardiograms;densely gated recurrent neural networks;global extrema loss;echocardiographic cine series;deep learning;long short-term memory;image feature extraction model;end-diastolic frame detection;end-systolic frame detection;convolution neural network","","","37","","","","","IEEE","IEEE Journals"
"Robust Single-Shot T2 Mapping via Multiple Overlapping-Echo Acquisition and Deep Neural Network","J. Zhang; J. Wu; S. Chen; Z. Zhang; S. Cai; C. Cai; Z. Chen","Department of Electronic Science, Fujian Provincial Key Laboratory of Plasma and Magnetic Resonance, Xiamen University, Xiamen, China; Department of Electronic Science, Fujian Provincial Key Laboratory of Plasma and Magnetic Resonance, Xiamen University, Xiamen, China; Department of Electronic Science, Fujian Provincial Key Laboratory of Plasma and Magnetic Resonance, Xiamen University, Xiamen, China; Department of Chemical and Biological Physics, Weizmann Institute of Science, Rehovot, Israel; Department of Electronic Science, Fujian Provincial Key Laboratory of Plasma and Magnetic Resonance, Xiamen University, Xiamen, China; Department of Electronic Science, Fujian Provincial Key Laboratory of Plasma and Magnetic Resonance, Xiamen University, Xiamen, China; Department of Electronic Science, Fujian Provincial Key Laboratory of Plasma and Magnetic Resonance, Xiamen University, Xiamen, China","IEEE Transactions on Medical Imaging","","2019","38","8","1801","1811","Quantitative magnetic resonance imaging (MRI) is of great value to both clinical diagnosis and scientific research. However, most MRI experiments remain qualitative, especially dynamic MRI, because repeated sampling with variable weighting parameter makes quantitative imaging time-consuming and sensitive to motion artifacts. A single-shot quantitative T2 mapping method based on multiple overlapping-echo acquisition (dubbed MOLED-4) was proposed to obtain reliable T2 mapping in milliseconds. Different from traditional MRI acceleration methods, such as compressed sensing and parallel imaging, MOLED-4 accelerates quantitative T2 mapping via synchronized multisampling and then deep learning to map the complex nonlinear relationship that is difficult to solve by traditional optimization-based methods. The results of simulation, phantom, and in vivo human brain experiments show the great performance of the proposed method. The principle of MOLED-4 may be extended to other ultrafast quantitative parameter mappings and potentially lead to new dynamic MRI with high efficiency to catch quantitative variation of tissue properties.","","","10.1109/TMI.2019.2896085","National Natural Science Foundation of China; Science and Technology Project of Fujian Province of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630865","Quantitative magnetic resonance imaging;T₂ mapping;multiple overlapping-echo detachment;neural network;single shot","Magnetic resonance imaging;Image reconstruction;Acceleration;Organic light emitting diodes;Weight measurement;Convolution","biomedical MRI;brain;medical image processing;neural nets;phantoms","multiple overlapping-echo acquisition;deep neural network;quantitative magnetic resonance imaging;clinical diagnosis;MRI experiments;dynamic MRI;motion artifacts;dubbed MOLED-4;traditional MRI acceleration methods;parallel imaging;deep learning;traditional optimization-based methods;ultrafast quantitative parameter mappings;mapping method;in vivo human brain experiments","","","51","","","","","IEEE","IEEE Journals"
"Three-Dimensional Continuous Movement Control of Drone Cells for Energy-Efficient Communication Coverage","P. Yang; X. Cao; X. Xi; W. Du; Z. Xiao; D. Wu","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA","IEEE Transactions on Vehicular Technology","","2019","68","7","6535","6546","This paper is concerned with the efficient movement control of multiple drone cells for communication coverage. Although many works have been developed to cope with this problem, but only few of them have investigated the three-dimensional (3-D) continuous movement control of multiple drone cells. In this paper, a problem of 3-D continuous movement control of multiple drone cells is formulated with an objective of maximizing the energy-efficient communication coverage of drone-cell networks while preserving the network connectivity. To mitigate this challenging problem, an energy-efficiency and continuous-movement-control algorithm (E2CMC), which is based on an emerging deep reinforcement learning method, is proposed. In E2CMC, an energy-efficiency reward function considering the energy consumption, the sum of quality-of-service (QoS) requirements of users as well as the coverage fairness is first designed. Next, E2CMC learns to adjust drone cells' locations continuously by interacting with an environment in a sequence of observations, actions, and rewards. Furthermore, E2CMC will reduce the reward drastically as long as the networks are disconnected. Simulation results verify the superiority of the proposed learning algorithm on deploying multiple drone cells to provide the communication coverage.","","","10.1109/TVT.2019.2913988","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8701666","Multiple drone-cells;continuous movement control;communication coverage;deep reinforcement learning","Quality of service;Drones;Energy consumption;Task analysis;Optimization;Atmospheric modeling;Simulation","autonomous aerial vehicles;cellular radio;learning (artificial intelligence);multi-robot systems;quality of service","efficient movement control;multiple drone cells;energy-efficient communication coverage;drone-cell networks;continuous-movement-control algorithm;energy-efficiency reward function;E2CMC;three-dimensional continuous movement control;learning algorithm;QoS;quality-of-service requirements","","1","36","","","","","IEEE","IEEE Journals"
"Hybrid Cross Deep Network for Domain Adaptation and Energy Saving in Visual Internet of Things","Z. Zhang; D. Li","Tianjin Key Laboratory of Wireless Mobile Communications and Power Transmission, College of Electronic and Communication Engineering, Tianjin Normal University, Tianjin, China; Tianjin Key Laboratory of Wireless Mobile Communications and Power Transmission, College of Electronic and Communication Engineering, Tianjin Normal University, Tianjin, China","IEEE Internet of Things Journal","","2019","6","4","6026","6033","Recently, Visual Internet of Things (VIoT) has become a fast-growing field based on various applications. In this paper, we focus on two critical challenges for applications in VIoT, i.e., domain adaptation and energy saving. The images captured by various visual sensors in VIoT appear quite different due to changes in visual sensor locations, visual sensor settings, image resolutions, and illuminations. Meanwhile, VIoT generates a number of images, and transmitting original images would take up much bandwidth. In order to effectively classify such images and save energy, we propose a novel deep model named hybrid cross deep network (HCDN), which could learn domain-invariant and discriminative features for images in VIoT. The proposed HCDN is designed to contain the cross regularization loss and the classification loss. Moreover, it is also trained with images from different visual sensors. Specifically, the cross regularization loss selects the triplet samples from the source domain and the target domain, and adopts the calibration parameter to align the difference between two domains. We employ the vector extracted from the proposed HCDN to represent each image, which requires a smaller storage capacity than the original images. Energy consumption will be reduced when we transmit such vectors to the intelligent visual label system for image classification in VIoT. The proposed HCDN is verified on two domain adaptation datasets, and the experimental results prove its effectiveness.","","","10.1109/JIOT.2018.2867083","National Natural Science Foundation of China; Natural Science Foundation of Tianjin City; Tianjin Normal University; Open Projects Program of National Laboratory of Pattern Recognition; China Scholarship Council; Tianjin Higher Education Creative Team Funds Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445552","Deep convolutional neural networks (CNNs);domain adaptation;energy saving;Visual Internet of Things (VIoT)","Visualization;Image sensors;Internet of Things;Intelligent sensors;Task analysis;Feature extraction","convolutional neural nets;image classification;image representation;image resolution;Internet of Things;power aware computing","hybrid cross deep network;VIoT;visual sensor locations;image resolutions;HCDN;cross regularization loss;energy consumption;intelligent visual label system;image classification;domain adaptation datasets;Visual Internet of Things","","1","38","","","","","IEEE","IEEE Journals"
"License Plate Localization in Unconstrained Scenes Using a Two-Stage CNN-RNN","J. Zhang; Y. Li; T. Li; L. Xun; C. Shan","College of Electrical Engineering and Automation, Anhui University, Hefei, China; College of Electrical Engineering and Automation, Anhui University, Hefei, China; College of Electrical Engineering and Automation, Anhui University, Hefei, China; College of Electrical Engineering and Automation, Anhui University, Hefei, China; Philips Research, High-Tech Campus, Eindhoven, AE, The Netherlands","IEEE Sensors Journal","","2019","19","13","5256","5265","Recent deep object detection methods neglect the intrinsic properties of the license plate, which limits the detection performance in unconstrained scenes. In this paper, we propose a two-stage deep learning-based method to locate license plates in unconstrained scenes, especially for special license plates such as fouling, occlusion, and so on. A deep network consisting of convolutional neural network (CNN) and recurrent neural network is designed. In the first stage, fine-scale proposals are detected according to the characteristics of the license plate characters, and CNN is used to extract the local features of characters. A vertical anchor mechanism is designed to jointly predict the position and confidence of each fix-width character. Furthermore, the sequential contexts of characters are modeled with the bi-directional long short-term memory, which greatly improves the locating rate of license plates in complex scenes. In the second stage, the whole license plate is obtained by connecting the fine-scale proposals. The experimental results show that the proposed method not only locates license plates of different countries accurately but also be robust to scenes of illumination variation, noise distortion, and blurry effects. The average precision reaches 97.11% on multi-country license plates, and the precision and recall reaches 99.10% and 98.68%, respectively, on Chinese license plate images.","","","10.1109/JSEN.2019.2900257","Natural Science Foundation of Anhui Province; National Natural Science Foundation of China; Anhui University; Hefei University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643978","License plate localization;convolutional neural networks;recurrent neural networks;BLSTM","Licenses;Proposals;Convolution;Feature extraction;Sensors;Object detection;Lighting","character recognition;convolutional neural nets;feature extraction;image recognition;image segmentation;learning (artificial intelligence);object detection;recurrent neural nets;traffic engineering computing","license plate localization;unconstrained scenes;two-stage CNN-RNN;two-stage deep learning-based method;special license plates;deep network;convolutional neural network;recurrent neural network;fine-scale proposals;license plate characters;multicountry license plates;Chinese license plate images;deep object detection methods","","1","33","","","","","IEEE","IEEE Journals"
"LTNN: A Layerwise Tensorized Compression of Multilayer Neural Network","H. Huang; H. Yu","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","5","1497","1511","An efficient deep learning requires a memory-efficient construction of a neural network. This paper introduces a layerwise tensorized formulation of a multilayer neural network, called LTNN, such that the weight matrix can be significantly compressed during training. By reshaping the multilayer neural network weight matrix into a high-dimensional tensor with a low-rank approximation, significant network compression can be achieved with maintained accuracy. An according layerwise training is developed by a modified alternating least-squares method with backward propagation for fine-tuning only. LTNN can provide the state-of-the-art results on various benchmarks with significant compression. For MNIST benchmark, LTNN shows 64× compression rate without accuracy drop. For Imagenet12 benchmark, our proposed LTNN achieves 35.84× compression of the neural network with around 2% accuracy drop. We have also shown 1.615× faster on inference speed than the existing works due to the smaller tensor core ranks.","","","10.1109/TNNLS.2018.2869974","MediaTek; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8480873","Layer-wise Training;multilayer nerual network;neural network compression;tensorized neural network;tensor-train","Neural networks;Training;Tensile stress;Nonhomogeneous media;Image coding;Benchmark testing;Machine learning","approximation theory;backpropagation;least squares approximations;matrix algebra;neural nets;tensors","memory-efficient construction;layerwise tensorized formulation;deep learning;LTNN;multilayer neural network;network compression;tensor core ranks;low-rank approximation;least-squares method;backward propagation;weight matrix","","","52","","","","","IEEE","IEEE Journals"
"Crosstalk Removal in Forward Scan Sonar Image Using Deep Learning for Object Detection","M. Sung; H. Cho; T. Kim; H. Joe; S. Yu","Department of Creative IT Engineering, Pohang University of Science and Technology, Pohang, South Korea; Department of Creative IT Engineering, Pohang University of Science and Technology, Pohang, South Korea; Department of Creative IT Engineering, Pohang University of Science and Technology, Pohang, South Korea; Department of Creative IT Engineering, Pohang University of Science and Technology, Pohang, South Korea; Department of Creative IT Engineering, Pohang University of Science and Technology, Pohang, South Korea","IEEE Sensors Journal","","2019","19","21","9929","9944","This paper proposes the detection and removal of crosstalk noise using a convolutional neural network in the images of forward scan sonar. Because crosstalk noise occurs near an underwater object and distorts the shape of the object, underwater object detection is limited. The proposed method can detect crosstalk noise using the neural network and remove crosstalk noise based on the detection result. Thus, the proposed method can be applied to other sonar-image-based algorithms and enhance the reliability of those algorithms. We applied the proposed method to a three-dimensional point cloud generation and generated a more accurate point cloud. We verified the performance of the proposed method by performing multiple indoor and field experiments.","","","10.1109/JSEN.2019.2925830","National Research Foundation of Korea; Korean Government (MSIT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752004","Crosstalk detection;sonar image crosstalk;underwater sonar crosstalk;underwater object detection","Crosstalk;Frequency selective surfaces;Three-dimensional displays;Object detection;Sonar detection;Sensors","convolutional neural nets;crosstalk;image denoising;learning (artificial intelligence);object detection;sonar imaging","crosstalk removal;forward scan sonar image;crosstalk noise;convolutional neural network;underwater object detection;sonar-image-based algorithms","","","45","","","","","IEEE","IEEE Journals"
"A Gated Recurrent Convolutional Neural Network for Robust Spoofing Detection","A. Gomez-Alanis; A. M. Peinado; J. A. Gonzalez; A. M. Gomez","Department of Signal Processing, Telematics and Communications, University of Granada, Granada, Spain; Department of Signal Processing, Telematics and Communications, University of Granada, Granada, Spain; Department of Signal Processing, Telematics and Communications, University of Granada, Granada, Spain; Department of Signal Processing, Telematics and Communications, University of Granada, Granada, Spain","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","12","1985","1999","Automatic speaker verification (ASV) systems are exposed to spoofing attacks which may compromise their security. While anti-spoofing techniques have been mainly studied for clean scenarios, it has also been shown that they perform poorly in noisy environments. In this work, we aim at improving the performance of spoofing detection for ASV in clean and noisy scenarios. To achieve this, we first propose the use of Gated Recurrent Convolutional Neural Networks (GRCNNs) as a deep feature extractor to robustly represent speech signals as utterance-level embeddings, which are later used by a back-end recognizer for the final genuine/spoofed classification. Then, to enhance the robustness of the system in noisy conditions, we propose the use of signal-to-noise masks (SNMs) as new input features to inform the anti-spoofing system about the time-frequency regions of the input spectral features that are mostly affected by noise and, hence, should be neglected when computing the embeddings. To evaluate our proposals, experiments were carried out on the clean and noisy versions of the ASVspoof 2015 corpus for detecting logical access attacks, as well as on the ASVspoof 2017 database to detect replay attacks. Additional results are provided for the ASVspoof 2019 corpus, including both logical and physical scenarios. The experimental results show that our proposal clearly outperforms some well-known methods based on classical features and other similar deep feature based systems for both clean and noisy conditions.","","","10.1109/TASLP.2019.2937413","Spanish MINECO/FEDER Project; Spanish Ministry of Education through the National Program FPU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812621","Spoofing detection;noise robustness;speaker verification;deep learning;signal-to-noise masks;deep features","Feature extraction;Noise measurement;Logic gates;Convolutional neural networks;Speech processing;Robustness;Training","feature extraction;recurrent neural nets;speaker recognition","signal-to-noise masks;input spectral features;logical access attacks;logical scenarios;physical scenarios;noisy conditions;robust spoofing detection;automatic speaker verification systems;ASV;spoofing attacks;anti-spoofing techniques;clean scenarios;noisy environments;deep feature extractor;speech signals;utterance-level embeddings;gated recurrent convolutional neural networks;antispoofing system","","","67","Traditional","","","","IEEE","IEEE Journals"
"Sample Fusion Network: An End-to-End Data Augmentation Network for Skeleton-Based Human Action Recognition","F. Meng; H. Liu; Y. Liang; J. Tu; M. Liu","Key Laboratory of Machine Perception, Peking University Shenzhen Graduate School, Shenzhen, China; Key Laboratory of Machine Perception, Peking University Shenzhen Graduate School, Shenzhen, China; School of Electronic and Information Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Key Laboratory of Machine Perception, Peking University Shenzhen Graduate School, Shenzhen, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Image Processing","","2019","28","11","5281","5295","Data augmentation is a widely used technique for enhancing the generalization ability of deep neural networks for skeleton-based human action recognition (HAR) tasks. Most existing data augmentation methods generate new samples by means of handcrafted transforms. However, these methods often cannot be trained and then are discarded during testing because of the lack of learnable parameters. To solve those problems, a novel type of data augmentation network called a sample fusion network (SFN) is proposed. Instead of using handcrafted transforms, an SFN generates new samples via a long short-term memory (LSTM) autoencoder (AE) network. Therefore, an SFN and HAR network can be cascaded together to form a combined network that can be trained in an end-to-end manner. Moreover, an adaptive weighting strategy is employed to improve the complementarity between a sample and the new sample generated from it by an SFN, thus allowing the SFN to more efficiently improve the performance of the HAR network during testing. The experimental results on various datasets verify that the proposed method outperforms state-of-the-art data augmentation methods. More importantly, the proposed SFN architecture is a general framework that can be integrated with various types of networks for HAR. For example, when a baseline HAR model with three LSTM layers and one fully connected (FC) layer was used, the classification accuracy was increased from 79.53% to 90.75% on the NTU RGB+D dataset using a cross-view protocol, thus outperforming most other methods.","","","10.1109/TIP.2019.2913544","National Natural Science Foundation of China; Scientific Research Project of Shenzhen City; Shenzhen Key Laboratory for Intelligent Multimedia and Virtual Reality; Shenzhen Science and Technology Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704987","Human action recognition;data augmentation;autoencoder;LSTM","Skeleton;Training;Testing;Deep learning;Transforms;Neural networks;Task analysis","image recognition;learning (artificial intelligence);recurrent neural nets;sampling methods","sample fusion network;end-to-end data augmentation network;deep neural networks;skeleton-based human action recognition tasks;handcrafted transforms;HAR network;SFN architecture;data augmentation methods;long short-term memory autoencoder network;cross-view protocol;LSTM layers","","1","59","","","","","IEEE","IEEE Journals"
"Face alignment: improving the accuracy of fast models using domain-specific unlabelled data and a teacher–student scheme","C. Álvarez Casado; M. Bordallo López","Visidon, Finland; University of Oulu, Finland","Electronics Letters","","2019","55","11","646","648","Face alignment is a crucial step in multiple face analysis and recognition tasks. The current state-of-the-art is comprised by very slow methods based on deep learning that require computationally heavy inference and very fast methods based on cascades of regressors that lack the ability to cope with complicated cases or extreme poses. The authors show how collecting a small subset of unlabelled domain-specific data can improve the accuracy of fast-inference models utilising data annotated by a slower one and a teacher–student architecture. In the proposed solution, they annotate a small subset of facial images belonging to two challenging domains using a slow but more accurate model, and this data is used to incrementally train a fast one. Their results show that by adding as little as a 5% of challenging data, they can reduce the error rate in a specific domain up to 30% without losing any generalisation abilities. This training scheme has applicability in numerous computer vision and engineering problems where computational power and model size are constrained by the application and platform or real-time operation is a requirement.","","","10.1049/el.2019.0319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730899","","","computer vision;learning (artificial intelligence);face recognition","engineering problems;computational power;model size;numerous computer vision;training scheme;generalisation abilities;specific domain;challenging data;slow but more accurate model;challenging domains;teacher–student architecture;fast-inference models;unlabelled domain-specific data;extreme poses;complicated cases;fast methods;computationally heavy inference;deep learning;slow methods;current state-of-the-art;recognition tasks;multiple face analysis;crucial step;teacher–student scheme;domain-specific unlabelled data;fast models;face alignment","","","16","","","","","IET","IET Journals"
"Tropical cyclone intensity prediction based on recurrent neural networks","B. Pan; X. Xu; Z. Shi","Shandong University of Science and Technology, People's Republic of China; Shandong University of Science and Technology, People's Republic of China; Beihang University, People's Republic of China","Electronics Letters","","2019","55","7","413","415","The accurate prediction for the tropical cyclone (TC) intensity is a recognised challenge. Researchers usually develop dynamical models to address this task. However, since the TC intensity is highly influenced by various factors such as ocean and atmosphere conditions, it is difficult to build the very model which can explicitly describe the mechanism of TC. A new idea is developed, utilising the massive historical observation data by a deep learning approach, to conduct a completely data-driven TC intensity prediction model. All the TC intensity and track data which have been observed in Western North Pacific since 1949 are collected, and recurrent neural network for TC intensity prediction is constructed. In general, their motivation as well as novelty is to develop a data-driven approach instead of empirical models. There are very few researches similar to their exploratory work. The proposed method has presented 5.1 m$\cdot $⋅s$^{-1}$−1 error in 24 h prediction, which is better than some widely used dynamical models and is close to subjective prediction.","","","10.1049/el.2018.8178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8685073","","","storms;weather forecasting;recurrent neural nets;learning (artificial intelligence)","recurrent neural network;recognised challenge;dynamical models;ocean;atmosphere conditions;massive historical observation data;deep learning approach;completely data-driven TC intensity prediction model;track data;data-driven approach;empirical models;subjective prediction;tropical cyclone intensity prediction;size 5.1 m;time 24.0 hour","","","8","","","","","IET","IET Journals"
"Convolutional neural network in network (CNNiN): hyperspectral image classification and dimensionality reduction","P. Shamsolmoali; M. Zareapoor; J. Yang","Shanghai Jiao Tong University, People's Republic of China; Shanghai Jiao Tong University, People's Republic of China; Shanghai Jiao Tong University, People's Republic of China","IET Image Processing","","2019","13","2","246","253","Classification is a principle technique in hyperspectral images (HSIs), where a label is assigned to each pixel based on its characteristics. However, due to lack of labelled training instances in HSIs and also its ultra-high dimensionality, deep learning approaches need a special consideration for HSI classification. As one of the first works in the HSI classification, this study proposes a novel network pipeline called convolutional neural network in network (which is deeper than the existing approaches) by jointly utilising the spatial and spectral information and produces high-level features from the original HSI. This can occur by using spatial-spectral relationships of individual pixel vector at the initial component of the proposed pipeline; the extracted features are then combined to form a joint spatial-spectral feature map. Finally, a recurrent neural network is trained on the extracted features which contain wealthy spectral and spatial properties of the HSI to predict the corresponding label of each vector. The model has been tested on two large scale hyperspectral datasets in terms of classification accuracy, training error, and computational time.","","","10.1049/iet-ipr.2017.1375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8650050","","","feature extraction;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);recurrent neural nets","recurrent neural network;feature extraction;spatial-spectral relationship;network pipeline;HSI classification;deep learning approach;dimensionality reduction;hyperspectral image classification;CNNiN;convolutional neural network in network","","1","47","","","","","IET","IET Journals"
"Thinning of convolutional neural network with mixed pruning","W. Yang; L. Jin; S. Wang; Z. Cu; X. Chen; L. Chen","School of Cyber Security and Computer, Hebei University, People's Republic of China; School of Cyber Security and Computer, Hebei University, People's Republic of China; School of Cyber Security and Computer, Hebei University, People's Republic of China; School of Cyber Security and Computer, Hebei University, People's Republic of China; School of Cyber Security and Computer, Hebei University, People's Republic of China; School of Cyber Security and Computer, Hebei University, People's Republic of China","IET Image Processing","","2019","13","5","779","784","Deep learning has achieved state-of-the-art performance in accuracy of many computer vision tasks. However, convolutional neural network is difficult to deploy on resource constrained devices due to their limited computation power and memory space. Thus, it is necessary to prune the redundant weights and filters rationally and effectively. Considering that the pruned model still exists, redundancy after weight pruning or filter pruning alone, a method of combining weight pruning and filter pruning is proposed. First, filter pruning is performed, which is to remove filters with least importance and using fine-tuning to recover the model's accuracy. Then, all connection weights below a threshold are set to zero. Finally, the pruned model obtained by the first two steps is fine-tuned to recover its predictive accuracy. Experiments on MNIST and CIFAR-10 datasets demonstrate that the proposed approach is effective and feasible. Compared with only weight pruning or filter pruning, the mixed pruning can achieve higher compression ratio of the model parameters. For LeNet-5, the proposed approach can achieve a compression rate of 13.01×, with 1% drop in accuracy. For VGG-16, it can achieve a compression rate of 19.20×, incurring 1.56% accuracy loss.","","","10.1049/iet-ipr.2018.6191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8689160","","","computer vision;convolutional neural nets;image filtering;learning (artificial intelligence)","filter pruning;weight pruning;pruned model;mixed pruning;convolutional neural network;memory space;redundant weights;fine-tuning;deep learning;computer vision tasks","","","26","","","","","IET","IET Journals"
"Deep Convolutional Neural Networks for Feature-Less Automatic Classification of Independent Components in Multi-Channel Electrophysiological Brain Recordings","P. Croce; F. Zappasodi; L. Marzetti; A. Merla; V. Pizzella; A. M. Chiarelli","Department of Neuroscience, Imaging and Clinical Sciences and the Institute for Advanced Biomedical TechnologiesUniversità G. d'Annunzio; Department of Neuroscience, Imaging and Clinical Sciences and the Institute for Advanced Biomedical TechnologiesUniversità G. d'Annunzio; Department of Neuroscience, Imaging and Clinical Sciences and the Institute for Advanced Biomedical TechnologiesUniversità G. d'Annunzio; Department of Neuroscience, Imaging and Clinical Sciences and the Institute for Advanced Biomedical TechnologiesUniversità G. d'Annunzio; Department of Neuroscience, Imaging and Clinical Sciences and the Institute for Advanced Biomedical TechnologiesUniversità G. d'Annunzio; Department of Neuroscience, Imaging and Clinical Sciences and the Institute for Advanced Biomedical Technologies, Università G. d'Annunzio, Chieti, Italy","IEEE Transactions on Biomedical Engineering","","2019","66","8","2372","2380","Objective: Interpretation of the electroencephalographic (EEG) and magnetoencephalographic (MEG) signals requires off-line artifacts removal. Since artifacts share frequencies with brain activity, filtering is insufficient. Blind source separation, mainly through independent component analysis (ICA), is the gold-standard procedure for the identification of artifacts in multi-dimensional recordings. However, a classification of brain and artifactual independent components (ICs) is still required. Since ICs exhibit recognizable patterns, classification is usually performed by experts' visual inspection. This procedure is time consuming and prone to errors. Automatic ICs classification has been explored, often through complex ICs features extraction prior to classification. Relying on deep-learning ability of self-extracting the features of interest, we investigated the capabilities of convolutional neural networks (CNNs) for off-line, automatic artifact identification through ICs without feature selection. Methods: A CNN was applied to spectrum and topography of a large dataset of few thousand samples of ICs obtained from multi-channel EEG and MEG recordings acquired during heterogeneous experimental settings and on different subjects. CNN performances, when applied to EEG, MEG, and combined EEG and MEG ICs, were explored and compared with state-of-the-art feature-based automatic classification. Results: Beyond state-of-the-art automatic classification accuracies were demonstrated through cross validation (92.4% EEG, 95.4% MEG, 95.6% EEG+MEG). Conclusion: High CNN classification performances were achieved through heuristical selection of machinery hyperparameters and through the CNN self-selection of the features of interest. Significance: Considering the large data availability of multi-channel EEG and MEG recordings, CNNs may be suited for classification of ICs of multi-channel brain electrophysiological recordings.","","","10.1109/TBME.2018.2889512","Advancing Smart Optical Imaging and Sensing for Health; BIAL Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8587223","Deep learning;electroencephalography;magnetoencephalography;independent component analysis;artifact identification","Integrated circuits;Electroencephalography;Feature extraction;Neurons;Brain;Filtering;Sensors","blind source separation;convolutional neural nets;electroencephalography;feature extraction;independent component analysis;magnetoencephalography;medical signal processing;signal classification","EEG-MEG;MEG recordings;multichannel brain electrophysiological recordings;state-of-the-art automatic classification accuracies;state-of-the-art feature-based automatic classification;MEG ICs;CNN performances;feature selection;automatic artifact identification;complex ICs features extraction;automatic ICs classification;artifactual independent components;multidimensional recordings;independent component analysis;blind source separation;brain activity;artifacts share frequencies;off-line artifacts removal;multichannel electrophysiological brain recordings;feature-less automatic classification;convolutional neural networks","","","56","","","","","IEEE","IEEE Journals"
"Identifying Brain Networks at Multiple Time Scales via Deep Recurrent Neural Network","Y. Cui; S. Zhao; H. Wang; L. Xie; Y. Chen; J. Han; L. Guo; F. Zhou; T. Liu","College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou, China; College of Biomedical Engineering and Instrument Science and the State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China; College of Biomedical Engineering and Instrument Science, and Zhejiang Provincial Key Laboratory for Network Multimedia Technologies, Zhejiang University, Hangzhou, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou, China; Cortical Architecture Imaging and Discovery Lab, Department of Computer Science and Bioimaging Research Center, The University of Georgia, Athens, GA, USA","IEEE Journal of Biomedical and Health Informatics","","2019","23","6","2515","2525","For decades, task functional magnetic resonance imaging has been a powerful noninvasive tool to explore the organizational architecture of human brain function. Researchers have developed a variety of brain network analysis methods for task fMRI data, including the general linear model, independent component analysis, and sparse representation methods. However, these shallow models are limited in faithful reconstruction and modeling of the hierarchical and temporal structures of brain networks, as demonstrated in more and more studies. Recently, recurrent neural networks (RNNs) exhibit great ability of modeling hierarchical and temporal dependence features in the machine learning field, which might be suitable for task fMRI data modeling. To explore such possible advantages of RNNs for task fMRI data, we propose a novel framework of a deep recurrent neural network (DRNN) to model the functional brain networks from task fMRI data. Experimental results on the motor task fMRI data of Human Connectome Project 900 subjects release demonstrated that the proposed DRNN can not only faithfully reconstruct functional brain networks, but also identify more meaningful brain networks with multiple time scales which are overlooked by traditional shallow models. In general, this work provides an effective and powerful approach to identifying functional brain networks at multiple time scales from task fMRI data.","","","10.1109/JBHI.2018.2882885","National Key R&D Program of China; NSF of China; Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; China Postdoctoral Science Foundation; Zhejiang Province Science and Technology Planning; National Natural Science Foundation of China; NIH; NSF CAREER; NSF; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543169","Task fMRI;brain network;RNN;deep learning","Task analysis;Brain modeling;Recurrent neural networks;Functional magnetic resonance imaging;Data models","","","","","80","Traditional","","","","IEEE","IEEE Journals"
"Understanding Natural Language Instructions for Fetching Daily Objects Using GAN-Based Multimodal Target–Source Classification","A. Magassouba; K. Sugiura; A. T. Quoc; H. Kawai","National Institute of Information and Communication Technology, Kyoto, Japan; National Institute of Information and Communication Technology, Kyoto, Japan; National Institute of Information and Communication Technology, Kyoto, Japan; National Institute of Information and Communication Technology, Kyoto, Japan","IEEE Robotics and Automation Letters","","2019","4","4","3884","3891","In this letter, we address multimodal language understanding with unconstrained fetching instruction for domestic service robots. A typical fetching instruction such as “Bring me the yellow toy from the white shelf” requires to infer the user intention, i.e., what object (target) to fetch and from where (source). To solve the task, we propose a multimodal target-source classifier model (MTCM), which predicts the region-wise likelihood of target and source candidates in the scene. Unlike other methods, MTCM can handle region-wise classification based on linguistic and visual features. We evaluated our approach that outperformed the state-of-the-art method on a standard dataset. We also extended MTCM with generative adversarial nets, and enabled simultaneous data augmentation and classification.","","","10.1109/LRA.2019.2926223","Japan Science and Technology Agency; SCOPE and NEDO; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752407","Deep learning in robotics and automation;domestic robots","Generative adversarial networks;Deep learning;Natural language processing;Service robots","","","","","22","Traditional","","","","IEEE","IEEE Journals"
"Spatio-Temporal Vegetation Pixel Classification by Using Convolutional Networks","K. Nogueira; J. A. dos Santos; N. Menini; T. S. F. Silva; L. P. C. Morellato; R. d. S. Torres","Department of Computer Science, Universidade Federal de Minas Gerais, Belo Horizonte, Brazil; Department of Computer Science, Universidade Federal de Minas Gerais, Belo Horizonte, Brazil; Institute of Computing, University of Campinas, Campinas, Brazil; IGCE, Universidade Estadual Paulista, São Paulo, Brazil; Instituto de Biociências, Universidade Estadual Paulista, São Paulo, Brazil; Institute of Computing, University of Campinas, Campinas, Brazil","IEEE Geoscience and Remote Sensing Letters","","2019","16","10","1665","1669","Plant phenology studies rely on long-term monitoring of life cycles of plants. High-resolution unmanned aerial vehicles (UAVs) and near-surface technologies have been used for plant monitoring, demanding the creation of methods capable of locating, and identifying plant species through time and space. However, this is a challenging task given the high volume of data, the constant data missing from temporal dataset, the heterogeneity of temporal profiles, the variety of plant visual patterns, and the unclear definition of individuals' boundaries in plant communities. In this letter, we propose a novel method, suitable for phenological monitoring, based on convolutional networks (ConvNets) to perform spatio-temporal vegetation pixel classification on high-resolution images. We conducted a systematic evaluation using high-resolution vegetation image datasets associated with the Brazilian Cerrado biome. Experimental results show that the proposed approach is effective, overcoming other spatio-temporal pixel-classification strategies.","","","10.1109/LGRS.2019.2903194","Pró-Reitoria de Pesquisa da UFMG; Fundação de Amparo à Pesquisa do Estado de Minas Gerais; Fundação de Amparo à Pesquisa do Estado de São Paulo; Conselho Nacional de Desenvolvimento Científico e Tecnológico; Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq) Research Fellowship to JAS, LPCM, RST, and TSFS; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; Cedro Têxtil, Reserva Vellozia, Parque Nacional da Serra do Cipó; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8681390","Deep learning;near surface;phenology;pixel classification;unmanned aerial vehicles","Vegetation mapping;Neurons;Convolution;Kernel;Deep learning;Data mining;Cameras","autonomous aerial vehicles;botany;condition monitoring;convolutional neural nets;geophysical image processing;image classification;image resolution;phenology;robot vision;vegetation;vegetation mapping","spatio-temporal vegetation pixel classification;convolutional networks;long-term monitoring;plant visual patterns;phenological monitoring;high-resolution vegetation image datasets;plant phenology;plant species;plant life cycle monitoring;high-resolution unmanned aerial vehicles;Brazilian Cerrado biome","","","16","","","","","IEEE","IEEE Journals"
"Direct Automatic Coronary Calcium Scoring in Cardiac and Chest CT","B. D. de Vos; J. M. Wolterink; T. Leiner; P. A. de Jong; N. Lessmann; I. Išgum","University Medical Center Utrecht, Image Sciences Institute, Utrecht University, Utrecht, CX, The Netherlands; University Medical Center Utrecht, Image Sciences Institute, Utrecht University, Utrecht, CX, The Netherlands; Department of Radiology, University Medical Center Utrecht, Utrecht University, CX, The Netherlands; Department of Radiology, University Medical Center Utrecht, Utrecht University, CX, The Netherlands; University Medical Center Utrecht, Image Sciences Institute, Utrecht University, Utrecht, CX, The Netherlands; University Medical Center Utrecht, Image Sciences Institute, Utrecht University, Utrecht, CX, The Netherlands","IEEE Transactions on Medical Imaging","","2019","38","9","2127","2138","Cardiovascular disease (CVD) is the global leading cause of death. A strong risk factor for CVD events is the amount of coronary artery calcium (CAC). To meet the demands of the increasing interest in quantification of CAC, i.e., coronary calcium scoring, especially as an unrequested finding for screening and research, automatic methods have been proposed. The current automatic calcium scoring methods are relatively computationally expensive and only provide scores for one type of CT. To address this, we propose a computationally efficient method that employs two convolutional neural networks: the first performs registration to align the fields of view of input CTs and the second performs direct regression of the calcium score, thereby circumventing time-consuming intermediate CAC segmentation. Optional decision feedback provides insight into the regions that are contributed to the calcium score. Experiments were performed using 903 cardiac CT and 1687 chest CT scans. The method predicted calcium scores in less than 0.3 s. The intra-class correlation coefficient between predicted and manual calcium scores was 0.98 for both cardiac and chest CT. The method showed almost perfect agreement between automatic and manual CVD risk categorization in both the datasets, with a linearly weighted Cohen's kappa of 0.95 in cardiac CT and 0.93 in chest CT. Performance is similar to that of the state-of-the-art methods, but the proposed method is hundreds of times faster. By providing visual feedback, insight is given in the decision process, making it readily implementable in clinical and research settings.","","","10.1109/TMI.2019.2899534","Stichting voor de Technische Wetenschappen; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643342","Calcium scoring;cardiac CT;chest CT;deep learning;convolutional neural network;atlas-registration;regression","Calcium;Computed tomography;Lesions;Feature extraction;Arteries;Deep learning;Lung","blood vessels;calcium;cardiovascular system;computerised tomography;diseases;image segmentation;medical image processing;neural nets","direct automatic coronary calcium scoring;CVD events;risk factor;cardiac CT;chest CT scans;manual CVD risk categorization;automatic CVD risk categorization;manual calcium scores;time-consuming intermediate CAC segmentation;calcium score;performs direct regression;computationally efficient method;current automatic calcium scoring methods;coronary artery calcium;time 0.3 s","","","48","","","","","IEEE","IEEE Journals"
"BranchGAN: Unsupervised Mutual Image-to-Image Transfer With A Single Encoder and Dual Decoders","Y. Zhou; R. Jiang; X. Wu; J. He; S. Weng; Q. Peng","School of Information Science and Technology, Xipu Campus, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Xipu Campus, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Xipu Campus, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Xipu Campus, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Xipu Campus, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Xipu Campus, Southwest Jiaotong University, Chengdu, China","IEEE Transactions on Multimedia","","2019","21","12","3136","3149","Image-to-image translation is a fundamental task for a wide range of applications, such as image style transfer, video effect generation, cross-domain retrieval, etc. Due to the limited number of labeled data, complex scenes, abstract semantics and various involved domains, image translation remains a challenging task. Compared to the supervised approaches for image translation that need a large collection of paired images for training, the unsupervised methods can significantly reduce the training cost. In this paper, an unsupervised end-to-end generative adversarial network is proposed, named BranchGAN, for mutual image-to-image transfer between two domains. A structure with one single encoder and dual decoders is novelly proposed to capture the cross-domain distributions and generate the images in both domains. Three factors, that is, pixel-level overall style, region semantics, and domain distinguishability are comprehensively considered to constrain the training process of the proposed model, corresponding to reconstruction loss, encoding loss, and adversarial loss, respectively. Experiments conducted on three benchmark datasets demonstrate the effectiveness of the proposed method that outperforms the unsupervised state-of-the-art approaches and has the competitive performance as the supervised method.","","","10.1109/TMM.2019.2920613","National Natural Science Foundation of China; Sichuan Science and Technology Innovation Seedling Fund; Foundation for Department of Transportation of Henan Province; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8727976","Image-to-image transfer;deep learning, generative adversarial network","Generative adversarial networks;Image reconstruction;Deep learning;Network architecture","","","","","44","IEEE","","","","IEEE","IEEE Journals"
"Classification and Quantification of Emphysema Using a Multi-Scale Residual Network","L. Peng; L. Lin; H. Hu; H. Li; Q. Chen; X. Ling; D. Wang; X. Han; Y. Iwamoto; Y. Chen","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Department of Radiology, Sir Run Run Shaw Hospital, Hangzhou, China; Department of Radiology, Sir Run Run Shaw Hospital, Hangzhou, China; Department of Radiology, Sir Run Run Shaw Hospital, Hangzhou, China; Department of Radiology, The Affiliated Hospital of Hangzhou Normal University, Hangzhou, China; Department of Radiology, Sir Run Run Shaw Hospital, Hangzhou, China; College of Information Science and Engineering, Ritsumeikan University, Kusatsu, Japan; College of Information Science and Engineering, Ritsumeikan University, Kusatsu, Japan; College of Information Science and Engineering, Ritsumeikan University, Kusatsu, Japan","IEEE Journal of Biomedical and Health Informatics","","2019","23","6","2526","2536","Automated tissue classification is an essential step for quantitative analysis and treatment of emphysema. Although many studies have been conducted in this area, there still remain two major challenges. First, different emphysematous tissue appears in different scales, which we call “inter-class variations.” Second, the intensities of CT images acquired from different patients, scanners or scanning protocols may vary, which we call “intra-class variations”. In this paper, we present a novel multi-scale residual network with two channels of raw CT image and its differential excitation component. We incorporate multi-scale information into our networks to address the challenge of inter-class variations. In addition to the conventional raw CT image, we use its differential excitation component as a pair of inputs to handle intra-class variations. Experimental results show that our approach has superior performance over the state-of-the- art methods, achieving a classification accuracy of 93.74% on our original emphysema database. Based on the classification results, we also perform the quantitative analysis of emphysema in 50 subjects by correlating the quantitative results (the area percentage of each class) with pulmonary functions. We show that centrilobular emphysema (CLE) and panlobular emphysema (PLE) have strong correlation with the pulmonary functions and the sum of CLE and PLE can be used as a new and accurate measure of emphysema severity instead of the conventional measure (sum of all subtypes of emphysema). The correlations between the new measure and various pulmonary functions are up to |r| = 0.922 (r is correlation coefficient).","","","10.1109/JBHI.2018.2890045","Major Scientific Research Project of Zhejiang Lab; Key Science and Technology Innovation Support Program of Hangzhou; Grant-in Aid for Scientific Research from the Japanese Ministry for Education, Science, Culture and Sports (MEXT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598730","Emphysema classification;deep learning;quantification analysis;multi-scale;differential excitation component","Computed tomography;Lung;Feature extraction;Diseases;Correlation;Deep learning;Biomedical imaging","","","","","54","Traditional","","","","IEEE","IEEE Journals"
"Multi-Modal Reflection Removal Using Convolutional Neural Networks","J. Sun; Y. Chang; C. Jung; J. Feng","School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Signal Processing Letters","","2019","26","7","1011","1015","Although color images are easily interfered by glass, depth images captured by infrared sensors are robust to reflection. In this letter, we propose multi-modal reflection removal using convolutional neural networks (CNNs). We build a multi-modal CNN for reflection removal to separate transmission from reflection using depth information. The proposed network consists of two sub-networks: image restoration and depth adaptation. Image restoration sub-network (IRN) recovers transmission layer from the input image with reflection, whereas depth adaptation sub-network (DAN) guides reflection removal of the IRN. Moreover, to extract image details for reflection removal, we present a multi-scale loss function that penalizes non-similarity for multi-scale outputs. Experimental results demonstrate that the proposed method is robust to dominant reflections and outperforms state-of-the-art methods in terms of both peak signal-to-noise ratio (PSNR) and structural similarity.","","","10.1109/LSP.2019.2915560","National Natural Science Foundation of China; International S&T Cooperation Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8710318","Deep learning;convolutional neural networks;depth;multi-modal;reflection removal","Glass;Convolution;Color;Image restoration;Sun;Deep learning;Image reconstruction","convolutional neural nets;feature extraction;image colour analysis;image denoising;image enhancement;image restoration","peak signal-to-noise ratio;PSNR;structural similarity;depth adaptation subnetwork;image restoration subnetwork;dominant reflections;multiscale outputs;multiscale loss function;image details;depth information;multimodal CNN;depth images;color images;convolutional neural networks;multimodal reflection removal","","","19","","","","","IEEE","IEEE Journals"
"Adaptive Convolution for Object Detection","C. Chen; Q. Ling","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China","IEEE Transactions on Multimedia","","2019","21","12","3205","3217","It is quite challenging to detect objects, especially, small objects, in complex scenes. To solve this problem, we propose a novel module named as adaptive convolution block (ACB), which adaptively adjusts the parameters of convolutional filters according to the current feature maps, and then, filter these feature maps with the obtained adaptive convolutional filters to generate enhanced features. Due to such adaptive convolution, the enhanced features can pay more attention to the concerned objects, suppress the interference information caused by irrelevant surroundings, and efficiently improve the detection accuracy. The proposed ACB is light weight and fast. By directly embedding the ACB into the single shot detection framework, we construct a novel real-time adaptive convolutional detector (ACD). Experiments on PASCAL VOC and MS COCO benchmarks confirm that our ACD outperforms the existing state-of-the-art single-stage detection models, and achieves a better tradeoff between accuracy and speed.","","","10.1109/TMM.2019.2916104","Intelligent Networked Electric Vehicle Key System Integration Development and Industrialization Project; National Basic Research Program of China (973 Program); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8712433","object detection;adaptive convolution;real time detector;deep learning","Feature extraction;Detectors;Convolution;Object detection;Adaptive systems;Deep learning;Real-time systems","","","","","42","IEEE","","","","IEEE","IEEE Journals"
"Toward Ergonomic Risk Prediction via Segmentation of Indoor Object Manipulation Actions Using Spatiotemporal Convolutional Networks","B. Parsa; E. U. Samani; R. Hendrix; C. Devine; S. M. Singh; S. Devasia; A. G. Banerjee","Department of Mechanical Engineering, University of Washington, Seattle, WA, USA; Department of Mechanical Engineering, University of Washington, Seattle, WA, USA; Department of Mechanical Engineering, University of Washington, Seattle, WA, USA; Department of Mechanical Engineering, University of Washington, Seattle, WA, USA; Department of Mechanical Engineering, Indian Institute of Technology Gandhinagar, Gandhinagar, India; Department of Mechanical Engineering, University of Washington, Seattle, WA, USA; Department of Industrial and Systems Engineering and the Department of Mechanical Engineering, University of Washington, Seattle, WA, USA","IEEE Robotics and Automation Letters","","2019","4","4","3153","3160","Automated real-time prediction of the ergonomic risks of manipulating objects is a key unsolved challenge in developing effective human-robot collaboration systems for logistics and manufacturing applications. We present a foundational paradigm to address this challenge by formulating the problem as one of action segmentation from RGB-D camera videos. Spatial features are first learned using a deep convolutional model from the video frames, which are then fed sequentially to temporal convolutional networks to semantically segment the frames into a hierarchy of actions, which are either ergonomically safe, require monitoring, or need immediate attention. For performance evaluation, in addition to an open-source kitchen dataset, we collected a new dataset comprising 20 individuals picking up and placing objects of varying weights to and from cabinet and table locations at various heights. Results show very high (87%-94%) F1 overlap scores among the ground truth and predicted frame labels for videos lasting over 2 min and consisting of a large number of actions.","","","10.1109/LRA.2019.2925305","Amazon Robotics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746140","Deep learning in robotics and automation;human-centered automation;computer vision for automation;action segmentation;ergonomic safety","Videos;Ergonomics;Cameras;Feature extraction;Safety;Service robots","ergonomics;human-robot interaction;image colour analysis;image motion analysis;image segmentation;learning (artificial intelligence);manipulators;robot vision;video signal processing","manufacturing applications;logistics;effective human-robot collaboration systems;key unsolved challenge;manipulating objects;ergonomic risks;real-time prediction;spatiotemporal convolutional networks;indoor object manipulation actions;toward ergonomic risk prediction;frame labels;open-source kitchen dataset;ergonomically safe require monitoring;temporal convolutional networks;video frames;deep convolutional model;spatial features;RGB-D camera videos;action segmentation;foundational paradigm","","","43","Traditional","","","","IEEE","IEEE Journals"
"Real-Time 3D Hand Pose Estimation with 3D Convolutional Neural Networks","L. Ge; H. Liang; J. Yuan; D. Thalmann","Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Department of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY; École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","4","956","970","In this paper, we present a novel method for real-time 3D hand pose estimation from single depth images using 3D Convolutional Neural Networks (CNNs). Image-based features extracted by 2D CNNs are not directly suitable for 3D hand pose estimation due to the lack of 3D spatial information. Our proposed 3D CNN-based method, taking a 3D volumetric representation of the hand depth image as input and extracting 3D features from the volumetric input, can capture the 3D spatial structure of the hand and accurately regress full 3D hand pose in a single pass. In order to make the 3D CNN robust to variations in hand sizes and global orientations, we perform 3D data augmentation on the training data. To further improve the estimation accuracy, we propose applying the 3D deep network architectures and leveraging the complete hand surface as intermediate supervision for learning 3D hand pose from depth images. Extensive experiments on three challenging datasets demonstrate that our proposed approach outperforms baselines and state-of-the-art methods. A cross-dataset experiment also shows that our method has good generalization ability. Furthermore, our method is fast as our implementation runs at over 91 frames per second on a standard computer with a single GPU.","","","10.1109/TPAMI.2018.2827052","BeingTogether Centre; National Research Foundation Singapore; Singapore Ministry of Education Academic Research Fund; University at Buffalo; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8338122","3D hand pose estimation;3D convolutional neural networks;deep learning","Three-dimensional displays;Pose estimation;Two dimensional displays;Feature extraction;Solid modeling;Heating systems;Real-time systems","feature extraction;image classification;image representation;learning (artificial intelligence);neural nets;pose estimation;stereo image processing","single depth images;image-based features;3D spatial information;3D CNN-based method;3D volumetric representation;hand depth image;3D spatial structure;3D data augmentation;estimation accuracy;3D deep network architectures;complete hand surface;real-time 3D hand pose estimation;3D convolutional neural networks;volumetric input","","4","74","","","","","IEEE","IEEE Journals"
"Unsupervised Feature Extraction in Hyperspectral Images Based on Wasserstein Generative Adversarial Network","M. Zhang; M. Gong; Y. Mao; J. Li; Y. Wu","School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Xi’an Key Laboratory of Big Data and Intelligent Vision, School of Computer Science and Technology, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","5","2669","2688","Feature extraction (FE) is a crucial research area in hyperspectral image (HSI) processing. Recently, due to the powerful ability of deep learning (DL) to extract spatial and spectral features, DL-based FE methods have shown great potentials for HSI processing. However, most of the DL-based FE methods are supervised, and the training of them suffers from the absence of labeled samples in HSIs severely. The training issue of supervised DL-based FE methods limits their application on HSI processing. To address this issue, in this paper, a novel modified generative adversarial network (GAN) is proposed to train a DL-based feature extractor without supervision. The designed GAN consists of two components, which are a generator and a discriminator. The generator can focus on the learning of real probability distributions of data sets and the discriminator can extract spatial-spectral features with superior invariance effectively. In order to learn upsampling and downsampling strategies adaptively during FE, the proposed generator and discriminator are designed based on a fully deconvolutional subnetwork and a fully convolutional subnetwork, respectively. Moreover, a novel min-max cost function is designed for training the proposed GAN in an end-to-end fashion without supervision, by utilizing the zero-sum game relationship between the generator and discriminator. Besides, the proposed modified GAN replaces the original Jensen-Shannon divergence with the Wasserstein distance, aiming to mitigate the unstability and difficulty of the training of GAN frameworks. Experimental results on three real data sets validate the effectiveness of the proposed method.","","","10.1109/TGRS.2018.2876123","National Natural Science Foundation of China; National Key Research and Development Program of China; Key Research and Development Program of Shaanxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8527649","Convolutional neural network (CNN);feature extraction (FE);generative adversarial network (GAN);hyperspectral images (HSIs)","Feature extraction;Iron;Training;Gallium nitride;Generative adversarial networks;Generators;Probability distribution","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;minimax techniques;probability;supervised learning;unsupervised learning","HSI processing;fully deconvolutional subnetwork;fully convolutional subnetwork;GAN frameworks;unsupervised feature extraction;Wasserstein generative adversarial network;hyperspectral image processing;deep learning;supervised DL-based FE methods;probability distributions;min-max cost function;Jensen-Shannon divergence","","3","91","","","","","IEEE","IEEE Journals"
"DNN-Aided Block Sparse Bayesian Learning for User Activity Detection and Channel Estimation in Grant-Free Non-Orthogonal Random Access","Z. Zhang; Y. Li; C. Huang; Q. Guo; C. Yuen; Y. L. Guan","Xidian University, Xi’an, China; Xidian University, Xi’an, China; Singapore University of Technology and Design, Singapore; School of Electrical, Computer and Telecommunications Engineering, University of Wollongong, NSW, Australia; Singapore University of Technology and Design, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Vehicular Technology","","2019","68","12","12000","12012","In the upcoming Internet-of-Things (IoT) era, the communication is often featured by massive connection, sporadic transmission, and small-sized data packets, which poses new requirements on the delay expectation and resource allocation efficiency of the Random Access (RA) mechanisms of the IoT communication stack. A grant-free non-orthogonal random access (NORA) system is considered in this paper, which could simultaneously reduce the access delay and support more Machine Type Communication (MTC) devices with limited resources. In order to address the joint user activity detection (UAD) and channel estimation (CE) problem in the grant-free NORA system, we propose a deep neural network-aided message passing-based block sparse Bayesian learning (DNN-MP-BSBL) algorithm. In the DNN-MP-BSBL algorithm, the iterative message passing process is transferred from a factor graph to a deep neural network (DNN). Weights are imposed on the messages in the DNN and trained to minimize the estimation error. It is shown that the trained weights could alleviate the convergence problem of the MP-BSBL algorithm, especially on crowded RA scenarios. Simulation results show that the proposed DNN-MP-BSBL algorithm could improve the UAD and CE accuracy with a smaller number of iterations, indicating its advantages for low-latency grant-free NORA systems.","","","10.1109/TVT.2019.2947214","National Natural Science Foundation of China; A*STAR AME IAF PP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8868222","Deep neural network;sparse Bayesian learning;grant-free;user activity detection;channel estimation","Bayes methods;Message passing;Channel estimation;Protocols;Neural networks;Internet of Things;Uplink","","","","","38","IEEE","","","","IEEE","IEEE Journals"
"A Sequential Data Analysis Approach to Detect Emergent Leaders in Small Groups","C. Beyan; V. Katsageorgiou; V. Murino","Department of Pattern Analysis and Computer Vision, Istituto Italiano di Tecnologia, Genoa, Italy; Department of Advanced Robotics, Istituto Italiano di Tecnologia, Genoa, Italy; Department of Pattern Analysis and Computer Vision, Istituto Italiano di Tecnologia, Genoa, Italy","IEEE Transactions on Multimedia","","2019","21","8","2107","2116","This paper addresses the problem of predicting emergent leaders (ELs) in small groups, that is, meetings. This is a long-lasting research problem for social and organizational psychology and a relevant problem that recently gained momentum in social computing. Toward this goal, we propose a novel method, which analyzes the temporal dependencies of the audio-visual data by applying unsupervised deep learning generative models (feature learning). To the best of our knowledge, this is the first attempt that sequential data processing is performed for EL detection. Feature learning results in a single feature vector per a given time interval and all feature vectors representing a participant are aggregated using novel fusion techniques. Finally, the EL detection is performed using the state-of-the-art single and multiple kernel learning algorithms. The proposed method shows (significantly) improved results compared to the state-of-the-art methods and it can be adapted to analyze various small group interactions given that it is a general approach.","","","10.1109/TMM.2019.2895505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8626131","Emergent leader;social signal processing;nonverbal features;small group interactions;sequential data analysis;temporal data;restricted Boltzmann machines","Feature extraction;Data analysis;Support vector machines;Hidden Markov models;Data mining;Psychology;Data models","data analysis;industrial psychology;organisational aspects;unsupervised learning","EL detection;single feature vector;feature vectors;multiple kernel learning algorithms;group interactions;sequential data analysis approach;social psychology;organizational psychology;social computing;temporal dependencies;audio-visual data;unsupervised deep learning generative models;feature learning;sequential data processing;emergent leaders","","","54","Traditional","","","","IEEE","IEEE Journals"
"Quality-Aware Unpaired Image-to-Image Translation","L. Chen; L. Wu; Z. Hu; M. Wang","School of Computer and Information, Hefei University of Technology, Hefei, China; School of Computer and Information, Hefei University of Technology, Hefei, China; School of Computer and Information, Hefei University of Technology, Hefei, China; School of Computer and Information, Hefei University of Technology, Hefei, China","IEEE Transactions on Multimedia","","2019","21","10","2664","2674","Generative adversarial networks (GANs) have been widely used for the image-to-image translation task. While these models rely heavily on the labeled image pairs, recently some GAN variants have been proposed to tackle the unpaired image translation task. These models exploited supervision at the domain level with a reconstruction process for unpaired image translation. On the other hand, parallel works have shown that leveraging perceptual loss functions based on high-level deep features could enhance the generated image quality. Nevertheless, as these GAN-based models either depended on the pretrained deep network structure or relied on the labeled image pairs, they could not be directly applied to the unpaired image translation task. Moreover, despite the improvement of the introduced perceptual losses from deep neural networks, few researchers have explored the possibility of improving the generated image quality from classical image quality measures. To tackle the above two challenges, in this paper, we propose a unified quality-aware GAN-based framework for unpaired image-to-image translation, where a quality-aware loss is explicitly incorporated by comparing each source image and the reconstructed image at the domain level. Specifically, we design two detailed implementations of the quality loss. The first method is based on a classical image quality assessment measure by defining a classical quality-aware loss to ensure similar quality score between an original image and the reconstructed image at the domain level. The second method proposes an adaptive deep network based loss that compares the high level content structure between each original image and its reconstructed image from the generator. Finally, extensive experimental results on many real-world datasets clearly show the quality improvement of our proposed framework, and the superiority of leveraging classical image quality measures for unpaired image translation compared to the deep network based model.","","","10.1109/TMM.2019.2907052","National Natural Science Foundation for Distinguished Young Scholars of China; National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673660","Image generation;image processing;image quality;neural networks","Task analysis;Gallium nitride;Image quality;Adaptation models;Image reconstruction;Loss measurement;Generators","image classification;image reconstruction;learning (artificial intelligence);neural nets","image quality assessment measure;deep network based model;quality improvement;adaptive deep network based loss;quality loss;reconstructed image;source image;unified quality-aware GAN;pretrained deep network structure;GAN-based models;generated image quality;high-level deep features;domain level;unpaired image translation task;labeled image pairs;image-to-image translation task","","","49","Traditional","","","","IEEE","IEEE Journals"
"Hierarchical and Robust Convolutional Neural Network for Very High-Resolution Remote Sensing Object Detection","Y. Zhang; Y. Yuan; Y. Feng; X. Lu","Key Laboratory of Spectral Imaging Technology, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China; Center for Optical Imagery Analysis and Learning, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Key Laboratory of Spectral Imaging Technology, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China; Key Laboratory of Spectral Imaging Technology, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","8","5535","5548","Object detection is a basic issue of very high-resolution remote sensing images (RSIs) for automatically labeling objects. At present, deep learning has gradually gained the competitive advantage for remote sensing object detection, especially based on convolutional neural networks (CNNs). Most of the existing methods use the global information in the fully connected feature vector and ignore the local information in the convolutional feature cubes. However, the local information can provide spatial information, which is helpful for accurate localization. In addition, there are variable factors, such as rotation and scaling, which affect the object detection accuracy in RSIs. In order to solve these problems, this paper presents a hierarchical robust CNN. First, multiscale convolutional features are extracted to represent the hierarchical spatial semantic information. Second, multiple fully connected layer features are stacked together so as to improve the rotation and scaling robustness. Experiments on two data sets have shown the effectiveness of our method. In addition, a large-scale high-resolution remote sensing object detection data set is established to make up for the current situation that the existing data set is insufficient or too small. The data set is available at https://github.com/CrazyStoneonRoad/TGRS-HRRSD-Dataset.","","","10.1109/TGRS.2019.2900302","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Chinese Academy of Sciences; Key Research Program of Frontier Sciences, CAS; Young Top-Notch Talent Program of CAS; State Key Program of National Natural Science of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676107","Convolutional neural networks (CNNs);hierarchical robust CNN (HRCNN);hierarchical spatial semantic (HSS);object detection;remote sensing images (RSIs);rotation and scaling robust enhancement (RSRE)","Feature extraction;Object detection;Remote sensing;Semantics;Task analysis;Computational modeling;Convolutional neural networks","convolutional neural nets;feature extraction;learning (artificial intelligence);object detection;remote sensing","high-resolution remote sensing object detection;high-resolution remote sensing images;convolutional neural networks;global information;fully connected feature vector;local information;convolutional feature cubes;spatial information;object detection accuracy;hierarchical robust CNN;multiscale convolutional features;hierarchical spatial semantic information;multiple fully connected layer features;object labeling;hierarchical convolutional neural network;robust convolutional neural network;deep learning;https://github.com/CrazyStoneonRoad/TGRS-HRRSD-Dataset","","3","61","","","","","IEEE","IEEE Journals"
"Toward Deep Universal Sketch Perceptual Grouper","K. Li; K. Pang; Y. Song; T. Xiang; T. M. Hospedales; H. Zhang","Pattern Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China; SketchX Research Lab, Queen Mary University of London, London, U.K.; SketchX Research Lab, Queen Mary University of London, London, U.K.; SketchX Research Lab, Queen Mary University of London, London, U.K.; SketchX Research Lab, Queen Mary University of London, London, U.K.; Pattern Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Image Processing","","2019","28","7","3219","3231","Human free-hand sketches provide the useful data for studying human perceptual grouping, where the grouping principles such as the Gestalt laws of grouping are naturally in play during both the perception and sketching stages. In this paper, we make the first attempt to develop a universal sketch perceptual grouper. That is, a grouper that can be applied to sketches of any category created with any drawing style and ability, to group constituent strokes/segments into semantically meaningful object parts. The first obstacle to achieving this goal is the lack of large-scale datasets with grouping annotation. To overcome this, we contribute the largest sketch perceptual grouping dataset to date, consisting of 20 000 unique sketches evenly distributed over 25 object categories. Furthermore, we propose a novel deep perceptual grouping model learned with both generative and discriminative losses. The generative loss improves the generalization ability of the model, while the discriminative loss guarantees both local and global grouping consistency. Extensive experiments demonstrate that the proposed grouper significantly outperforms the state-of-the-art competitors. In addition, we show that our grouper is useful for a number of sketch analysis tasks, including sketch semantic segmentation, synthesis, and fine-grained sketch-based image retrieval.","","","10.1109/TIP.2019.2895155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8626530","Sketch perceptual grouping;universal grouper;deep grouping model;dataset","Semantics;Image segmentation;Task analysis;Visualization;Training;Data models;Analytical models","feature extraction;group theory;image retrieval;image segmentation;object detection","free-hand sketches;human perceptual grouping;large-scale datasets;grouping annotation;generative loss;sketch analysis tasks;sketch semantic segmentation;sketch-based image retrieval;deep universal sketch perceptual grouper","","","39","","","","","IEEE","IEEE Journals"
"Deep Convolutional Neural Networks for Link Adaptations in MIMO-OFDM Wireless Systems","M. Elwekeil; S. Jiang; T. Wang; S. Zhang","College of Information Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China","IEEE Wireless Communications Letters","","2019","8","3","665","668","This letter proposes a deep convolutional neural network (DCNN) approach for adaptive modulation and coding in practical multiple-input, multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) systems. Our target is to maximize the throughput and fulfill a packet error rate constraint. We consider practical impairments of MIMO-OFDM receiver, such as imperfect timing synchronization, carrier frequency offset correction, and channel estimation. We treat the estimated channel state information and the noise standard deviation as input features to the DCNN. The main advantages of the proposed approach are: 1) it learns the characteristics of the MIMO-OFDM channel properly and predicts the suitable modulation and coding scheme and 2) it does not need complex features selection.","","","10.1109/LWC.2018.2881978","National Natural Science Foundation of China; Guangdong NSF Projects; Shenzhen NSF Projects; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540019","Multiple-input;multiple-output (MIMO);orthogonal frequency division multiplexing (OFDM);adaptive modulation and coding (AMC);deep convolutional neural networks","OFDM;Channel estimation;Training;Receivers;MIMO communication;Encoding;Quadrature amplitude modulation","adaptive modulation;channel coding;channel estimation;convolutional neural nets;MIMO communication;modulation coding;OFDM modulation;radio receivers;telecommunication computing","input features;DCNN;MIMO-OFDM channel;coding scheme;link adaptations;MIMO-OFDM wireless systems;deep convolutional neural network approach;packet error rate constraint;MIMO-OFDM receiver;imperfect timing synchronization;channel estimation;estimated channel state information;multiple-input multiple-output systems;orthogonal frequency division multiplexing systems;adaptive modulation-and-coding;carrier frequency offset correction;noise standard deviation","","1","9","","","","","IEEE","IEEE Journals"
"Online Kernel-Based Structured Output SVM for Early Expression Detection","L. Xie; J. Zhao; H. Wei; K. Zhang; G. Pang","Key Laboratory of Measurement and Control of CSE, Ministry of Education, School of Automation, Southeast University, Nanjing, China; School of Mathematical Science, Liaocheng University, Liaocheng, China; School of Automation, Southeast University, Nanjing, China; School of Automation, Southeast University, Nanjing, China; School of Automation and Electrical Engineering, Linyi University, Linyi, China","IEEE Signal Processing Letters","","2019","26","9","1305","1309","As a key component of human-computer intelligent interaction and many real-world applications, the real-time property of facial expression recognition is especially important. However, the recognition result of conventional video-based approaches can not be given until the entire video is finished. In this letter, we deal with early expression detection, which aims to identify the expression as early as possible before its ending. This is a relatively new and challenging problem. Max-margin early event detector (MMED) is a well-known framework, which can make early detection. However, the linearity restricts its applications. We thus introduce kernel learning to model the nonlinear structure of complex data distribution. Moreover, the model is further reformulated in an online setting to address the streaming videos. The high retraining cost and large memory requirement of MMED are thus significantly reduced. In addition, we employ AlexNet architecture to make further comparison with mid-level features. Experiments on two popular video-based expression datasets demonstrate both the effectiveness and efficiency of the proposed method.","","","10.1109/LSP.2019.2929713","National Key Research and Development Program of Chin; National NSF of China; NSF of Jiangsu; Innovation Fund of Key Laboratory of Measurement and Control of CSE through Southeast University; Innovation Fund of Key Lab of Digital Signal and Image Processing of Guangdong Province; Innovation Fund of Jiangsu Key Laboratory of Image and Video Understanding for Social Safety through Nanjing University of Science and Technology; Open Project Program of the State Key Lab of CAD&CG; Zhejiang University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765792","Online learning;kernel learning;early expression detection;deep features","Silicon;Training;Kernel;Data models;Feature extraction;Streaming media;Memory management","face recognition;feature extraction;learning (artificial intelligence);support vector machines;video signal processing","online kernel-based structured output SVM;early expression detection;human-computer intelligent interaction;facial expression recognition;max-margin early event detector;streaming videos;video-based expression datasets","","","28","Traditional","","","","IEEE","IEEE Journals"
"Chest X-ray image denoising method based on deep convolution neural network","Y. Jin; X. Jiang; Z. Wei; Y. Li","College of Information Engineering, Zhejiang University of Technology, People's Republic of China; College of Information Engineering, Zhejiang University of Technology, People's Republic of China; College of Information Engineering, Zhejiang University of Technology, People's Republic of China; College of Information Engineering, Zhejiang University of Technology, People's Republic of China","IET Image Processing","","2019","13","11","1970","1978","To improve the visual effect of chest X-ray images and reduce the noise interference in disease diagnosis based on the chest X-ray images, the authors proposed an image denoising model based on deep convolution neural network. They utilise batch normalisation to solve the problem of performance degradation due to the increase of neural network layers, and use residual learning of the distribution of noise in noisy X-ray images. Specifically, the depthwise separable convolution is used to accelerate the convergence speed of network model, shorten the training time, and improve accuracy of the model. Compared to the several popular or the state-of-the-art denoising algorithms, their extensive experiments demonstrate that their method can not only achieve better denoising effects, but also significantly reduce the complexity of the network and shorten the computation time.","","","10.1049/iet-ipr.2019.0241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8853443","","","convolution;image denoising;neural nets;diseases;learning (artificial intelligence)","chest X-ray image denoising method;deep convolution neural network;chest X-ray images;noise interference;image denoising model;neural network layers;noisy X-ray images;depthwise separable convolution;network model;state-of-the-art denoising algorithms;denoising effects;shorten","","","33","","","","","IET","IET Journals"
"Parallel Tracking and Verifying","H. Fan; H. Ling","Department of Computer and Information Science, Temple University, Philadelphia, PA, USA; Department of Computer and Information Science, Temple University, Philadelphia, PA, USA","IEEE Transactions on Image Processing","","2019","28","8","4130","4144","Visual object tracking has played a crucial role in computer vision with many applications. Being intensively studied in recent decades, visual tracking has witnessed great advances in either speed (e.g., with correlation filters) or accuracy (e.g., with deep features). Real-time and high accuracy tracking algorithms, nevertheless, remain scarce. In this paper, we study the problem from a new perspective and present a novel parallel tracking and verifying (PTAV) framework, by taking advantage of the ubiquity of multi-thread techniques and borrowing ideas from the success of parallel tracking and mapping in visual SLAM. The proposed PTAV framework typically consists of two components, a (base) tracker T and a verifier V, working in parallel on two separate threads. The tracker T aims at providing a super real-time tracking inference and is expected to perform well most of the time; by contrast, the verifier V validates the tracking results and corrects T when needed. The key innovation is that V does not work on every frame but only upon the requests from T ; on the other end, T may adjust the tracking according to the feedback from V. With such collaboration, PTAV enjoys both high efficiencies provided by T and strong discriminative power by V. Meanwhile, in order to adapt V to object appearance changes, we maintain a dynamic target template pool for adaptive verification, resulting in further improvement. In our extensive experiments on OTB2015, TC128, UAV20L, and VOT2016, PTAV achieves top tracking accuracy among all real-time trackers, and in fact, even outperforms many deep learning-based algorithms. Moreover, as a general framework, PTAV is very flexible with great potentials for future improvement and generalization.","","","10.1109/TIP.2019.2904789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667882","Visual tracking;deep learning;correlation filter;verification;multi-thread;parallel tracking and verifying","Target tracking;Visualization;Correlation;Real-time systems;Object tracking;Feature extraction","inference mechanisms;object tracking","parallel tracking;visual object tracking;PTAV framework;deep learning-based algorithms;OTB2015 framework;TC128 framework;UAV20L framework;VOT2016 framework;dynamic target template pool;multithread techniques;parallel tracking and verifying framework;correlation filters;computer vision;real-time tracking inference algorithms","","","64","","","","","IEEE","IEEE Journals"
"HMFP-DBRNN: Real-Time Hand Motion Filtering and Prediction via Deep Bidirectional RNN","S. Shahtalebi; S. F. Atashzar; R. V. Patel; A. Mohammadi","Concordia Institute for Information System Engineering, Concordia University, Montreal, QC, Canada; Department of Bioengineering, Imperial College London, London, U.K.; Department of Electrical and Computer Engineering, University of Western Ontario, London, ON, Canada; Concordia Institute for Information System Engineering, Concordia University, Montreal, QC, Canada","IEEE Robotics and Automation Letters","","2019","4","2","1061","1068","Pathological hand tremor (PHT) is among the most common movement symptoms of several neurological disorders including Parkinson's disease and essential tremor. Extracting PHT is of paramount importance in several engineering and clinical applications such as assistive and robotic rehabilitation technologies. In such systems, PHT is modeled as the input noise to the system and thus there is a surge of interest in estimation an compensation of the noise. Although various works in the literature have attempted to estimate and extract the PHT, in this letter, first, we argue that the ground truth signal used in existing works to optimize the performance of tremor extraction techniques is not accurate enough, and thus the performance measures for the prior techniques are not perfectly reliable. In addition, most of the existing tremor extraction techniques impose unrealistic assumptions, which are, typically, violated in practical settings. This letter proposes a novel technique that for the first time incorporates deep bidirectional recurrent neural networks as a processing tool for PHT extraction. Moreover, we devise an intuitively pleasing training strategy that enables the network to perform not only online estimation but also online prediction of the voluntary hand motion in a myopic fashion, which is currently a significantly important unmet need for rehabilitative and assistive robotic technologies designed for patients with pathological tremor.","","","10.1109/LRA.2019.2894005","Fonds de Recherche du Québec Nature et Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8618351","Deep learning in robotics and automation;medical robots and systems;rehabilitation robotics","Training;Estimation;Recurrent neural networks;Real-time systems;Filtering;Robot kinematics","","","","","43","","","","","IEEE","IEEE Journals"
"Triplanar Imaging of 3-D GPR Data for Deep-Learning-Based Underground Object Detection","N. Kim; S. Kim; Y. An; J. Lee","Department of Civil and Environmental Engineering, Sejong University, Seoul, South Korea; Korea Infrastructure Safety Corporation, Jinju-si, South Korea; Department of Architectural Engineering, Sejong University, Seoul, South Korea; Department of Civil and Environmental Engineering, Sejong University, Seoul, South Korea","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","11","4446","4456","This article proposes a deep-learning-based underground object classification technique that uses triplanar ground-penetrating radar images consisting of B-, C-, and D-scan images. Although multichannel ground-penetrating radar (GPR) systems provide three-dimensional (3-D) information about underground objects, there is currently no suitable technique available for processing 3-D data as opposed to 2-D images. In this article, a triplanar deep convolutional neural network technique is proposed for use in processing 3-D GPR data for use in automatized underground object classification. The proposed method was validated experimentally using 3-D GPR road scanning data obtained from urban roads in Seoul, South Korea. In addition, the classification performance of the method was compared to that of a conventional method that uses only B-scan-images. The results of the validation and comparison tests reveal that the classification performance of the proposed technique is notably better than that of the conventional B-scan-image-based method and that its use results in decrease misclassification ratios.","","","10.1109/JSTARS.2019.2953505","Transportation Technology Promotion Research Program; Ministry of Land, Infrastructure and Transport; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935527","D-scan image;ground-penetrating radar (GPR);triplanar images;underground object classification","","","","","","46","IEEE","","","","IEEE","IEEE Journals"
"Bi-Directional Spatial-Semantic Attention Networks for Image-Text Matching","F. Huang; X. Zhang; Z. Zhao; Z. Li","Beijing Key Laboratory of Network Technology, Beihang University, Beijing, China; School of Cyber Science and Technology, Beihang University, Beijing, China; National Computer Emergency Technical Team/Coordination Center of China, Beijing, China; State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China","IEEE Transactions on Image Processing","","2019","28","4","2008","2020","Image-text matching by deep models has recently made remarkable achievements in many tasks, such as image caption and image search. A major challenge of matching the image and text lies in that they usually have complicated underlying relations between them and simply modeling the relations may lead to suboptimal performance. In this paper, we develop a novel approach bi-directional spatial-semantic attention network, which leverages both the word to regions (W2R) relation and visual object to words (O2W) relation in a holistic deep framework for more effectively matching. Specifically, to effectively encode the W2R relation, we adopt LSTM with bilinear attention function to infer the image regions which are more related to the particular words, which is referred as the W2R attention networks. On the other side, the O2W attention networks are proposed to discover the semantically close words for each visual object in the image, i.e., the visual O2W relation. Then, a deep model unifying both of the two directional attention networks into a holistic learning framework is proposed to learn the matching scores of image and text pairs. Compared to the existing image-text matching methods, our approach achieves state-of-the-art performance on the datasets of Flickr30K and MSCOCO.","","","10.1109/TIP.2018.2882225","Natural Science Foundation of Beijing Municipality; National Natural Science Foundation of China; State Key Laboratory of Software Development Environment; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540429","Image-text matching;attention networks;deep learning;spatial-semantic","Visualization;Correlation;Bidirectional control;Semantics;Pins;Task analysis;Natural language processing","image matching;image retrieval;learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis;text detection","spatial-semantic attention networks;bilinear attention function;image regions;W2R attention networks;O2W attention networks;visual object to words relation;word to regions relation;image-text matching;bi-directional spatial-semantic attention network;LSTM","","","53","","","","","IEEE","IEEE Journals"
"Deep Sequential Models for Suicidal Ideation From Multiple Source Data","I. Peis; P. M. Olmos; C. Vera-Varela; M. L. Barrigón; P. Courtet; E. Baca-García; A. Artés-Rodríguez","Department of Signal Theory and Communications, Universidad Carlos III de Madrid, Madrid, Spain; Department of Signal Theory and Communications, Universidad Carlos III de Madrid, Madrid, Spain; Department of Psychiatry, IIS-Jiménez Díaz Foundation, Madrid, Spain; Department of Psychiatry, IIS-Jiménez Díaz Foundation, Madrid, Spain; Department of Psychiatric Emergency and Acute Care, Lapeyronie Hospital, University of Montpellier, Montpellier, France; Department of Psychiatry, IIS-Jiménez Díaz Foundation, Madrid, Spain; Department of Signal Theory and Communications, Universidad Carlos III de Madrid, Madrid, Spain","IEEE Journal of Biomedical and Health Informatics","","2019","23","6","2286","2293","This paper presents a novel method for predicting suicidal ideation from electronic health records (EHR) and ecological momentary assessment (EMA) data using deep sequential models. Both EHR longitudinal data and EMA question forms are defined by asynchronous, variable length, randomly sampled data sequences. In our method, we model each of them with a recurrent neural network, and both sequences are aligned by concatenating the hidden state of each of them using temporal marks. Furthermore, we incorporate attention schemes to improve performance in long sequences and time-independent pre-trained schemes to cope with very short sequences. Using a database of 1023 patients, our experimental results show that the addition of EMA records boosts the system recall to predict the suicidal ideation diagnosis from 48.13% obtained exclusively from EHR-based state-of-the-art methods to 67.78%. Additionally, our method provides interpretability through the t-distributed stochastic neighbor embedding (t-SNE) representation of the latent space. Furthermore, the most relevant input features are identified and interpreted medically.","","","10.1109/JBHI.2019.2919270","Spanish MINECO; Spanish MICINN; Comunidad de Madrid; BBVA Foundation; ISCIII; AFSP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8723181","Deep learning;RNN;attention;EMA;suicide","Predictive models;Data models;Recurrent neural networks;Psychiatry;Informatics;Biological system modeling;Databases","","","","","32","Traditional","","","","IEEE","IEEE Journals"
"A Novel Feature Extraction Method an Electronic Nose for Aroma Classification","G. Jong; Hendrick; Z. Wang; K. Hsieh; G. Horng","Department of Electronic Engineering, National Kaohsiung University of Science and Technology, Kaohsiung, Taiwan; Department of Electronic Engineering, National Kaohsiung University of Science and Technology, Kaohsiung, Taiwan; Department of Electronic Engineering, National Kaohsiung University of Science and Technology, Kaohsiung, Taiwan; Department of Pediatrics, Taipei Medical University, Taipei, Taiwan; Department of Computer Science and Information Engineering, Southern Taiwan University of Science and Technology, Tainan, Taiwan","IEEE Sensors Journal","","2019","19","22","10796","10803","In this paper, we describe an electronic nose (e-Nose) capable of classifying the aroma of alcoholic beverages. The novelty of this research is using signal processing for initial feature extraction from a sensor and then the use of deep learning to identify patterns of alcoholic beverage aromas. The sensor array was formed by nine types of metal oxide semiconductor sensors. The dataset was formed by images of standard deviations and correlation coefficients for processed signals from the e-Nose sensors. Thus, two patterns were generated. The first pattern came from a polar-chart image of the processed signals' standard deviations. The second pattern was produced by correlation coefficient converted into 3D heat-map images. The image size is 256 × 256 pixels. The convolutional architecture for fast feature embedding framework then trained GoogLeNet network using the dataset images. The training process was configured for 300 epochs and 0.0001 learning rate. The GoogLeNet network model from deep learning was compared with the AlexNet network model. The final classification was based on two patterns of prediction. The true label is used if the prediction accuracy value exceeds 70 %. The result is true only if both 3D heatmap and polar-chart prediction have true labels. The aroma detection accuracies of the GoogLeNet model are 85.0% for polar-chart and 85.416% accuracy for 3D heat-map. The aroma identification accuracy of AlexNet model are 85.0% for polarchart and 85.416% accuracy for 3D heat-map.","","","10.1109/JSEN.2019.2929239","Allied Advanced Intelligent Biomedical Research Center, STUST, thorough the Higher Education Sprout Project, Ministry of Education, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770251","Caffee framework;alcoholic beverage;electronic nose;GoogLeNet","Sensor arrays;Standards;Heating systems;Correlation coefficient;Three-dimensional displays;Gas detectors","computerised instrumentation;electronic noses;feature extraction;image classification;learning (artificial intelligence);sensor arrays","learning rate;GoogLeNet network model;deep learning;AlexNet network model;prediction accuracy value;aroma detection accuracies;GoogLeNet model;aroma identification accuracy;feature extraction method;electronic nose sensors;alcoholic beverages;signal processing;feature extraction;metal oxide semiconductor sensor array;correlation coefficient;e-nose sensors;3D heat-map imaging;polar-chart imaging;alcoholic beverage aroma classification;fast feature embedding framework","","","36","","","","","IEEE","IEEE Journals"
"Adversarial MACE Prediction After Acute Coronary Syndrome Using Electronic Health Records","Z. Huang; W. Dong","College of Biomedical Engineering and Instrument Science, Zhejiang University, HangzhouChina; Department of Cardiology, Chinese PLA General Hospital, Beijing, China","IEEE Journal of Biomedical and Health Informatics","","2019","23","5","2117","2126","Acute coronary syndrome (ACS), as an emergent and severe syndrome due to decreased blood flow in the coronary arteries, is a leading cause of death and serious long-term disability globally. ACS is usually caused by one of three problems: ST elevation myocardial infarction, non-ST elevation myocardial infarction, or unstable angina. Major adverse cardiac event (MACE) prediction, as a critical tool to estimate the likelihood an individual is at risk of ACS, has been widely adopted in the early prevention and intervention of ACS. Although valuable, existing MACE prediction models are designed to predict the overall probability of MACE occurrence for ACS patients, and lack the ability to look for insight into the disease to distinguish the different subtypes of ACS in a fine-grained manner. It is interesting to exploit the different subtypes of ACS and mine their private and shared underlying knowledge to improve the performance of MACE prediction. In this study, we propose utilizing a large volume of heterogeneous electronic health records for the application of MACE prediction. In detail, we address the multi-subtype-oriented MACE prediction for ACS as a multi-task learning (MTL) problem, present a MTL-based model to predict MACE of ACS patients with the different subtypes, and incorporate adversarial learning into the model to alleviate both the shared and private latent feature spaces of each subtype of ACS from interfering with each other. A real clinical dataset containing 2,863 ACS patient samples is collected from a Chinese hospital to validate the proposed model. Experimental results demonstrate that the prediction performance of our proposed model obtains a significant improvement, compared to single-subtype-oriented MACE prediction models.","","","10.1109/JBHI.2018.2882518","National Natural Science Foundation of China; the PLAGH Medical Bigdata R&D; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540947","MACE Prediction;acute coronary syndrome;deep learning;adversarial network;electronic health records","Predictive models;Myocardium;Task analysis;Data models;Feature extraction;Noise reduction;Electronic medical records","blood vessels;cardiovascular system;diseases;electronic health records;haemodynamics;learning (artificial intelligence);medical computing;medical disorders","multisubtype-oriented MACE prediction;prediction performance;MACE prediction models;adversarial MACE prediction;acute coronary syndrome;severe syndrome;ST elevation myocardial infarction;nonST elevation myocardial infarction;adverse cardiac event prediction;MACE occurrence;ACS patient samples;blood flow;coronary arteries;unstable angina;heterogeneous electronic health records;multitask learning problem;clinical dataset","","","51","Traditional","","","","IEEE","IEEE Journals"
"Unobtrusive Activity Recognition of Elderly People Living Alone Using Anonymous Binary Sensors and DCNN","M. Gochoo; T. Tan; S. Liu; F. Jean; F. S. Alnajjar; S. Huang","Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Information Engineering, Chaoyang University of Technology, Taichung, Taiwan; Department of Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan; College of Information Technology, United Arab Emirates University, Al-Ain, United Arab Emirates; Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan","IEEE Journal of Biomedical and Health Informatics","","2019","23","2","693","702","Elderly population (over the age of 60) is predicted to be 1.2 billion by 2025. Most of the elderly people would like to stay alone in their own house due to the high eldercare cost and privacy invasion. Unobtrusive activity recognition is the most preferred solution for monitoring daily activities of the elderly people living alone rather than the camera and wearable devices based systems. Thus, we propose an unobtrusive activity recognition classifier using deep convolutional neural network (DCNN) and anonymous binary sensors that are passive infrared motion sensors and door sensors. We employed Aruba annotated open data set that was acquired from a smart home where a voluntary single elderly woman was living inside for eight months. First, ten basic daily activities, namely, Eating, Bed_to_Toilet, Relax, Meal_Preparation, Sleeping, Work, Housekeeping, Wash_Dishes, Enter_Home, and Leave_Home are segmented with different sliding window sizes, and then converted into binary activity images. Next, the activity images are employed as the ground truth for the proposed DCNN model. The 10-fold cross-validation evaluation results indicated that our proposed DCNN model outperforms the existing models with F1-score of 0.79 and 0.951 for all ten activities and eight activities (excluding Leave_Home and Wash_Dishes), respectively.","","","10.1109/JBHI.2018.2833618","Ministry of Science and Technology of the Republic of China; International Conference on Electrical and Computing Technologies and Applications; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8355255","Unobtrusive;device-free;deep learning;activity recognition;elder care","Activity recognition;Senior citizens;Magnetic sensors;Hidden Markov models;Monitoring;Feature extraction","assisted living;convolutional neural nets;geriatrics;health care;home computing;learning (artificial intelligence);patient monitoring;pattern classification","elderly people;camera;wearable devices;unobtrusive activity recognition classifier;deep convolutional neural network;anonymous binary sensors;passive infrared motion sensors;door sensors;voluntary single elderly woman;basic daily activities;binary activity images;DCNN model;privacy invasion;daily activity monitoring;Aruba annotated open data set;10-fold cross-validation evaluation;eldercare cost","","6","55","","","","","IEEE","IEEE Journals"
"Front-End Smart Visual Sensing and Back-End Intelligent Analysis: A Unified Infrastructure for Economizing the Visual System of City Brain","Y. Lou; L. Duan; S. Wang; Z. Chen; Y. Bai; C. Chen; W. Gao","Institute of Digital Media, Peking University, Beijing, China; Institute of Digital Media, Peking University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Institute of Digital Media, Peking University, Beijing, China; Institute of Digital Media, Peking University, Beijing, China; Peng Cheng Laboratory, Shenzhen, China; Institute of Digital Media, Peking University, Beijing, China","IEEE Journal on Selected Areas in Communications","","2019","37","7","1489","1503","The visual data, which are acquired from the ubiquitous visual sensors deployed in metropolitans, are of great value and paramount significance to enhance the effectiveness and pursue the future development of smart cities. In this paper, the essential building blocks of the unified visual data management and analysis infrastructure that serve as the foundation for the economical visual system in the city brain, are introduced to facilitate the utilization of the visual signal in the artificial intelligence era. In particular, we start by the discussion of the front-end smart visual sensing in the context of economical communication and service with the heterogeneous network, and the functionalities and necessities of compact visual feature and deep learning model representations are detailed. Subsequently, the utilities of the infrastructure are demonstrated through two intelligent applications at the back-end, including vehicle re-identification and person re-identification. The standardizations regarding compact feature and deep neural network representations, which are regarded as the key ingredients in this infrastructure and greatly facilitate the construction of the visual system in the city brain, are also discussed. Finally, we envision how the potential issues regarding the economical visual communications for future smart cities might be pragmatically approached within this unified infrastructure.","","","10.1109/JSAC.2019.2916488","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Shenzhen Municipal Science and Technology Program; Hong Kong RGC Early Career Scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8713488","Smart city;visual analysis;compact feature representation;vehicle re-identification;person re-identification;standardization","Visualization;Feature extraction;Smart cities;Sensors;Deep learning;Cameras","data visualisation;feature extraction;learning (artificial intelligence);neural nets;smart cities;ubiquitous computing","front-end smart visual sensing;back-end intelligent analysis;unified infrastructure;city brain;ubiquitous visual sensors;unified visual data management;analysis infrastructure;economical visual system;visual signal;artificial intelligence era;economical communication;compact visual feature;economical visual communications;future smart cities","","","100","","","","","IEEE","IEEE Journals"
"Synthetic Data Generation for End-to-End Thermal Infrared Tracking","L. Zhang; A. Gonzalez-Garcia; J. van de Weijer; M. Danelljan; F. S. Khan","Computer Vision Center, Universitat Autònoma de Barcelona, Barcelona, Spain; Computer Vision Center, Universitat Autònoma de Barcelona, Barcelona, Spain; Computer Vision Center, Universitat Autònoma de Barcelona, Barcelona, Spain; Linköping University, Linköping, Sweden; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE","IEEE Transactions on Image Processing","","2019","28","4","1837","1850","The usage of both off-the-shelf and end-to-end trained deep networks have significantly improved the performance of visual tracking on RGB videos. However, the lack of large labeled datasets hampers the usage of convolutional neural networks for tracking in thermal infrared (TIR) images. Therefore, most state-of-the-art methods on tracking for TIR data are still based on handcrafted features. To address this problem, we propose to use image-to-image translation models. These models allow us to translate the abundantly available labeled RGB data to synthetic TIR data. We explore both the usage of paired and unpaired image translation models for this purpose. These methods provide us with a large labeled dataset of synthetic TIR sequences, on which we can train end-to-end optimal features for tracking. To the best of our knowledge, we are the first to train end-to-end features for TIR tracking. We perform extensive experiments on the VOT-TIR2017 dataset. We show that a network trained on a large dataset of synthetic TIR data obtains better performance than one trained on the available real TIR data. Combining both data sources leads to further improvement. In addition, when we combine the network with motion features, we outperform the state of the art with a relative gain of over 10%, clearly showing the efficiency of using synthetic data to train end-to-end TIR trackers.","","","10.1109/TIP.2018.2879249","CHISTERA Project M2CR of the Spanish Ministry; Generalitat de Catalunya; CENIIT; VR Starting Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520752","Visual tracking;thermal infrared;deep learning;generative networks","Target tracking;Correlation;Videos;Visualization;Feature extraction","convolution;feature extraction;feedforward neural nets;image colour analysis;image motion analysis;image sequences;infrared imaging;learning (artificial intelligence);object tracking;video signal processing","VOT-TIR2017 dataset;end-to-end TIR trackers;synthetic data generation;end-to-end thermal infrared tracking;deep networks;visual tracking;RGB videos;convolutional neural networks;thermal infrared images;image-to-image translation models;paired image translation models;unpaired image translation models;synthetic TIR sequences;end-to-end optimal features;TIR tracking;motion features;labeled RGB data","","1","70","","","","","IEEE","IEEE Journals"
"Normalization in Training U-Net for 2-D Biomedical Semantic Segmentation","X. Zhou; G. Yang","Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.; Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.","IEEE Robotics and Automation Letters","","2019","4","2","1792","1799","Two-dimensional (2-D) biomedical semantic segmentation is important for robotic vision in surgery. Segmentation methods based on deep convolutional neural network (DCNN) can out-perform conventional methods in terms of both accuracy and levels of automation. One common issue in training a DCNN for biomedical semantic segmentation is the internal covariate shift where the training of convolutional kernels is encumbered by the distribution change of input features, hence both the training speed and performance are decreased. Batch normalization (BN) is the first proposed method for addressing internal covariate shift and is widely used. Instance normalization (IN) and layer normalization (LN) have also been proposed. Group normalization (GN) is proposed more recently and has not yet been applied to 2-D biomedical semantic segmentation (GN was used in 3-D biomedical semantic segmentation in [P.-Y. Kao, T. Ngo, A. Zhang, J. Chen, and B. Manjunath, Brain tumor segmentation and tractographic feature extraction from structural MR images for overall survival prediction 2018, arXiv:1807.07716], however, no specific validations on GN were given). Most DCNNs for biomedical semantic segmentation adopt BN as the normalization method by default, without reviewing its performance. In this letter, four normalization methods-BN, IN, LN, and GN are compared in details, specifically for 2-D biomedical semantic segmentation. U-Net is adopted as the basic DCNN structure. Three datasets regarding the right ventricle, aorta, and left ventricle are used for the validation. The results show that detailed subdivision of the feature map, i.e., GN with a large group number or IN, achieves higher accuracy. This accuracy improvement mainly comes from better model generalization. Codes are uploaded and maintained at Xiao-Yun Zhou's Github.","","","10.1109/LRA.2019.2896518","EP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630007","Computer vision for medical robotics;deep learning in robotics and automation;surgical robotics: planning;visual-based navigation","Semantics;Image segmentation;Feature extraction;Training;Two dimensional displays;Robots;Three-dimensional displays","biomedical MRI;brain;convolutional neural nets;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;tumours","two-dimensional biomedical semantic segmentation;deep convolutional neural network;batch normalization;internal covariate shift;instance normalization;layer normalization;group normalization;brain tumor segmentation;tractographic feature extraction;structural MR images;U-Net training;right ventricle;left ventricle;aorta;feature map","","2","29","","","","","IEEE","IEEE Journals"
"Hyperspectral Image Classification Method Based on CNN Architecture Embedding With Hashing Semantic Feature","C. Yu; M. Zhao; M. Song; Y. Wang; F. Li; R. Han; C. Chang","Center of Hyperspectral Imaging in Remote Sensing, Information and Technology College, Dalian Maritime University, Dalian, China; Center of Hyperspectral Imaging in Remote Sensing, Information and Technology College, Dalian Maritime University, Dalian, China; Center of Hyperspectral Imaging in Remote Sensing, Information and Technology College, Dalian Maritime University, Dalian, China; Center of Hyperspectral Imaging in Remote Sensing, Information and Technology College, Dalian Maritime University, Dalian, China; Center of Hyperspectral Imaging in Remote Sensing, Information and Technology College, Dalian Maritime University, Dalian, China; Center of Hyperspectral Imaging in Remote Sensing, Information and Technology College, Dalian Maritime University, Dalian, China; Center of Hyperspectral Imaging in Remote Sensing, Information and Technology College, Dalian Maritime University, Dalian, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","6","1866","1881","Deep convolutional neural networks (CNN) have led to a successful breakthrough for hyperspectral image (HSI) classification. In this paper, a CNN system embedded with an extracted hashing feature is proposed for HSI classification that utilizes the semantic information of the HSI. First, a series of hash functions are constructed to enhance the presentation of the locality and discriminability of classes. Then, the sparse binary hash codes calculated by the discriminative learning algorithm are combined into the original HSI. Next, we design a CNN framework with seven hidden layers to obtain the hierarchical feature maps with both spectral and spatial information for classification. A deconvolution layer aims to improve the robustness of the proposed CNN network and is used to enhance the expression of deep features. The proposed CNN classification architecture achieves powerful distinguishing ability from different classes. The extensive experiments on real hyperspectral images results demonstrate that the proposed CNN network can effectively improve the classification accuracy after the embedding of the extracted semantic features.","","","10.1109/JSTARS.2019.2911987","Natural Science Foundation of Liaoning Province; Fundamental Research Funds for Central Universities; Recruitment Program of Global Experts for National Science and Technology Major Project; State Administration of Foreign Experts Affairs; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8720040","Convolutional neural networks (CNN);hashing learning;hyperspectral image classification (HSIC);semantic feature extraction (SFE)","Feature extraction;Hyperspectral imaging;Semantics;Hash functions;Data mining","convolutional neural nets;deconvolution;feature extraction;file organisation;hyperspectral imaging;image classification;learning (artificial intelligence)","hyperspectral image classification method;CNN architecture embedding;HSI classification;semantic information;sparse binary hash codes;discriminative learning algorithm;hierarchical feature maps;spectral information;spatial information;deconvolution layer;hashing semantic feature;deep convolutional neural networks;hashing feature","","","37","Traditional","","","","IEEE","IEEE Journals"
"Visual Tracking Via Multi-Layer Factorized Correlation Filter","B. Kang; G. Chen; Q. Zhou; J. Yan; M. Lin","Department of Internet of Things and the Jiangsu Engineering Research Center of Communication and Network Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Communication and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Communication and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Communication and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Signal Processing Letters","","2019","26","12","1763","1767","Pruning the parameters of basis filters can effectively eliminate the negative effect of redundant deep features in discriminative correlation filter based trackers. However, traditional methods often treat feature maps in Convolutional Neural Networks (CNN) as isolate observations, ignore the intrinsic correlation between partially attentional feature maps in multiple convolutional layers, when basis filter pruning is pursued. In this letter, we propose a multi-layer factorized discriminant correlation filter (MLF-DCF) for visual tracking. By integrating the multi-view discriminant learning and the discriminative correlation filter into a unified optimization problem, we can explore the correlation between different target sub-regions from multi-layer viewpoint, thus can effectively prune multi-layer basis filters. To enhance the efficiency of MLF-DCF in terms of speed and accuracy, we not only adopt alternating direction method of multipliers (ADMM) to solve the unified optimization, but also employ a mask estimation strategy to eliminate the background noise in deep features. A large number of experiments on challenging video sequences are given to illustrate the superiority of our tracking method.","","","10.1109/LSP.2019.2947185","National Natural Science Foundation of China; NSF of Jiangsu Province; Key International Cooperation Research Project; NUPT program; Open Research Fund of Jiangsu Engineering Research Center of Communication and Network Technology, NUPT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8867976","Visual tracking;factorized correlation filter;multi-view discriminant learning;ADMM","Correlation;Target tracking;Visualization;Training;Convolution;Noise measurement","convolutional neural nets;filtering theory;image representation;image sequences;learning (artificial intelligence);object tracking;optimisation","visual tracking;multilayer factorized correlation filter;redundant deep features;discriminative correlation filter based trackers;Convolutional Neural Networks;intrinsic correlation;partially attentional feature maps;multiple convolutional layers;basis filter pruning;multilayer factorized discriminant correlation filter;MLF-DCF;multiview discriminant learning;multilayer viewpoint;multilayer basis filters;tracking method","","","29","IEEE","","","","IEEE","IEEE Journals"
"GLAD: Global–Local-Alignment Descriptor for Scalable Person Re-Identification","L. Wei; S. Zhang; H. Yao; W. Gao; Q. Tian","School of Electronic Engineering and Computer Science, Peking University, Beijing, China; School of Electronic Engineering and Computer Science, Peking University, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, China; School of Electronic Engineering and Computer Science, Peking University, Beijing, China; Noah's Ark Lab, Huawei, Shenzhen, China","IEEE Transactions on Multimedia","","2019","21","4","986","999","The huge variance of human pose and the misalign-ment of detected human images significantly increase the difficulty of pedestrian image matching in person Re-Identification (Re-ID). Moreover, the massive visual data being produced by surveillance video cameras requires highly efficient person Re-ID systems. Targeting to solve the first problem, this work proposes a robust and discriminative pedestrian image descriptor, namely, the Global-Local-Alignment Descriptor (GLAD). For the second problem, this work treats person Re-ID as image retrieval and proposes an efficient indexing and retrieval framework. GLAD explicitly leverages the local and global cues in the human body to generate a discriminative and robust representation. It consists of part extraction and descriptor learning modules, where several part regions are first detected and then deep neural networks are designed for representation learning on both the local and global regions. A hierarchical indexing and retrieval framework is designed to perform offline relevance mining to eliminate the huge person ID redundancy in the gallery set, and accelerate the online Re-ID procedure. Extensive experimental results on widely used public benchmark datasets show GLAD achieves competitive accuracy compared to the state-of-the-art methods. On a large-scale person, with the Re-ID dataset containing more than 520 K images, our retrieval framework significantly accelerates the online Re-ID procedure while also improving Re-ID accuracy. Therefore, this work has the potential to work better on person Re-ID tasks in real scenarios.","","","10.1109/TMM.2018.2870522","National Natural Science Foundation of China; Beijing Major Science and Technology Project; ARO; Faculty Research Gift Awards by NEC Laboratories of America and Blippar; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466034","Person re-identification (Re-ID);global-local-alignment descriptor;retrieval framework","Robustness;Feature extraction;Cameras;Measurement;Indexing;Task analysis;Machine learning","data mining;feature extraction;image matching;image representation;image retrieval;learning (artificial intelligence);object detection;pose estimation;video surveillance","GLAD;pedestrian image matching;massive visual data;surveillance video cameras;robust pedestrian image descriptor;discriminative pedestrian image descriptor;image retrieval;retrieval framework;local cues;global cues;discriminative representation;robust representation;part extraction;descriptor learning modules;online Re-ID procedure;Re-ID dataset;person Re-ID tasks;deep neural networks;person ID redundancy;scalable person re-identification;global-local-alignment descriptor","","4","78","","","","","IEEE","IEEE Journals"
"CarNet: A Dual Correlation Method for Health Perception of Rotating Machinery","W. Zhang; D. Yang; H. Wang; X. Huang; M. Gidlund","School of Electronics and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronics and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronics and Information Engineering, Beijing Jiaotong University, Beijing, China; Beijing Sheenline Technology, Beijing, China; Information and Communication Systems, Mid Sweden University, Sundsvall, Sweden","IEEE Sensors Journal","","2019","19","16","7095","7106","As a key component of rotating machinery, the health perception of bearings is essential to ensure the safe and reliable operation of industrial equipment. In recent years, research on equipment health perception based on data-driven methods has received extensive attention. Overall, most studies focus on several public datasets to verify the effectiveness of their algorithms. However, the scale of these datasets cannot completely satisfy the representation learning of deep models. Therefore, this paper proposes a novel method, called CarNet, to obtain a more robust model and ensure that the model is sufficiently trained on a limited dataset. Specifically, it is composed of a data augmentation method named equitable sliding stride segmentation (ESSS) and a hybrid-stacked deep model (HSDM). The ESSS not only amplifies the scale of the original dataset but also enables newly generated data with both spatial and temporal correlations. The HSDM can, therefore, extract shallow spatial features and deep temporal information from the strongly correlated 2-dimensional (2-D) sensor array using a CNN and a bi-GRU, respectively. Moreover, the integrated attention mechanism contributes to focusing limited resources on informative areas. The effectiveness of CarNet is evaluated on the CWRU dataset, and an optimal diagnostic accuracy of 99.92% is achieved.","","","10.1109/JSEN.2019.2912934","Fundamental Research Funds for the Central Universities; National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695784","Health perception;convolutional neural network;gated recurrent unit;attention mechanism;temporal and spatial correlation","Vibrations;Feature extraction;Data models;Correlation;Sensor arrays;Artificial intelligence","condition monitoring;correlation methods;electric machines;feature extraction;learning (artificial intelligence);machine bearings;mechanical engineering computing","equipment health perception;data-driven methods;representation learning;robust model;data augmentation method;equitable sliding stride segmentation;ESSS;hybrid-stacked deep model;HSDM;spatial correlations;temporal correlations;deep temporal information;CWRU dataset;dual correlation method;rotating machinery;safe operation;reliable operation;industrial equipment;CarNet;2-dimensional sensor array;bi-GRU","","","39","","","","","IEEE","IEEE Journals"
"Adversarially Approximated Autoencoder for Image Generation and Manipulation","W. Xu; S. Keshmiri; G. Wang","Department of Aerospace Engineering, University of Kansas, Lawrence, KS, USA; Department of Aerospace Engineering, University of Kansas, Lawrence, KS, USA; Department of Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS, USA","IEEE Transactions on Multimedia","","2019","21","9","2387","2396","Regularized autoencoders learn the latent codes, a structure with the regularization under the distribution, which enables them the capability to infer the latent codes given observations and generate new samples given the codes. However, they are sometimes ambiguous as they tend to produce reconstructions that are not necessarily a faithful reproduction of the inputs. The main reason is to enforce the learned latent code distribution to match a prior distribution while the true distribution remains unknown. To improve the reconstruction quality and learn the latent space a manifold structure, this paper presents a novel approach using the adversarially approximated autoencoder (AAAE) to investigate the latent codes with adversarial approximation. Instead of regularizing the latent codes by penalizing on the distance between the distributions of the model and the target, AAAE learns the autoencoder flexibly and approximates the latent space with a simpler generator. The ratio is estimated using a generative adversarial network to enforce the similarity of the distributions. In addition, the image space is regularized with an additional adversarial regularizer. The proposed approach unifies two deep generative models for both latent space inference and diverse generation. The learning scheme is realized without regularization on the latent codes, which also encourages faithful reconstruction. Extensive validation experiments on four real-world datasets demonstrate the superior performance of AAAE. In comparison to the state-of-the-art approaches, AAAE generates samples with better quality and shares the properties of a regularized autoencoder with a nice latent manifold structure.","","","10.1109/TMM.2019.2898777","NSF NRI; USDA NIFA; NSFC; Nvidia GPU Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8638964","Autoencoder;generative adversarial network;adversarial approximation;faithful reconstruction;latent manifold structure","Gallium nitride;Training;Neural networks;Data models;Image reconstruction;Image generation;Generative adversarial networks","image reconstruction;inference mechanisms;learning (artificial intelligence);neural nets","adversarially approximated autoencoder;regularized autoencoder;latent codes;learned latent code distribution;adversarial approximation;generative adversarial network;latent space inference;diverse generation;adversarial regularizer;latent manifold structure;image generation;image manipulation;reconstruction quality;image space;deep generative models;learning scheme","","4","63","Traditional","","","","IEEE","IEEE Journals"
"DeepCorrect: Correcting DNN Models Against Image Distortions","T. S. Borkar; L. J. Karam","Department of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; Department of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA","IEEE Transactions on Image Processing","","2019","28","12","6022","6034","In recent years, the widespread use of deep neural networks (DNNs) has facilitated great improvements in performance for computer vision tasks like image classification and object recognition. In most realistic computer vision applications, an input image undergoes some form of image distortion such as blur and additive noise during image acquisition or transmission. Deep networks trained on pristine images perform poorly when tested on such distortions. In this paper, we evaluate the effect of image distortions like Gaussian blur and additive noise on the activations of pre-trained convolutional filters. We propose a metric to identify the most noise susceptible convolutional filters and rank them in order of the highest gain in classification accuracy upon correction. In our proposed approach called DeepCorrect, we apply small stacks of convolutional layers with residual connections at the output of these ranked filters and train them to correct the worst distortion affected filter activations, while leaving the rest of the pre-trained filter outputs in the network unchanged. Performance results show that applying DeepCorrect models for common vision tasks like image classification (ImageNet), object recognition (Caltech-101, Caltech-256), and scene classification (SUN-397), significantly improves the robustness of DNNs against distorted images and outperforms other alternative approaches.","","","10.1109/TIP.2019.2924172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746775","Deep neural networks;image distortion;image classification;residual learning;image denoising;image deblurring","Distortion;AWGN;Task analysis;Training;Robustness;Perturbation methods;Computer vision","computer vision;convolutional neural nets;image classification;image filtering","input image;realistic computer vision applications;image classification;computer vision tasks;deep neural networks;distorted images;object recognition;DeepCorrect models;pre-trained filter outputs;filter activations;ranked filters;noise susceptible convolutional filters;pre-trained convolutional filters;deep networks;image acquisition;additive noise;image distortion","","","45","","","","","IEEE","IEEE Journals"
"Enhanced Non-Local Total Variation Model and Multi-Directional Feature Prediction Prior for Single Image Super Resolution","C. Ren; X. He; Y. Pu; T. Q. Nguyen","College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; Department of Electrical and Computer Engineering, University of California at San Diego, La Jolla, CA, USA","IEEE Transactions on Image Processing","","2019","28","8","3778","3793","It is widely acknowledged that single image super-resolution (SISR) methods play a critical role in recovering the missing high-frequencies in an input low-resolution image. As SISR is severely ill-conditioned, image priors are necessary to regularize the solution spaces and generate the corresponding high-resolution image. In this paper, we propose an effective SISR framework based on the enhanced non-local similarity modeling and learning-based multi-directional feature prediction (ENLTV-MDFP). Since both the modeled and learned priors are exploited, the proposed ENLTV-MDFP method benefits from the complementary properties of the reconstruction-based and learning-based SISR approaches. Specifically, for the non-local similarity-based modeled prior [enhanced non-local total variation, (ENLTV)], it is characterized via the decaying kernel and stable group similarity reliability schemes. For the learned prior [multi-directional feature prediction prior, (MDFP)], it is learned via the deep convolutional neural network. The modeled prior performs well in enhancing edges and suppressing visual artifacts, while the learned prior is effective in hallucinating details from external images. Combining these two complementary priors in the MAP framework, a combined SR cost function is proposed. Finally, the combined SR problem is solved via the split Bregman iteration algorithm. Based on the extensive experiments, the proposed ENLTV-MDFP method outperforms many state-of-the-art algorithms visually and quantitatively.","","","10.1109/TIP.2019.2902794","National Natural Science Foundation of China; National Postdoctoral Program for Innovative Talents; Sichuan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658150","Super resolution;decaying kernel;stable group similarity reliability;enhanced non-local total variation;multi-directional feature prediction","Image reconstruction;Training;Image resolution;Reliability;Image edge detection;Prediction algorithms;Kernel","convolutional neural nets;image enhancement;image reconstruction;image resolution;iterative methods;learning (artificial intelligence)","high-resolution image;ENLTV-MDFP method;learning-based multi-directional feature prediction;reconstruction-based SISR approaches;learning-based SISR approaches;deep convolutional neural network;visual artifacts;split Bregman iteration algorithm;stable group similarity reliability schemes;decaying kernel;nonlocal similarity modeling;image priors;input low-resolution image;single image super-resolution methods;enhanced nonlocal total variation model","","","71","","","","","IEEE","IEEE Journals"
"Short-Term Prediction of Electricity Outages Caused by Convective Storms","R. Tervo; J. Karjalainen; A. Jung","Observing and Information Systems Center, Finnish Meteorological Institute, Helsinki, Finland; Observing and Information Systems Center, Finnish Meteorological Institute, Helsinki, Finland; Department of Computer Science, Aalto University, Aalto, Finland","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","8618","8626","Prediction of power outages caused by convective storms, which are highly localized in space and time, is of crucial importance to power grid operators. We propose a new machine learning approach to predict the damage caused by storms. This approach hinges identifying and tracking of storm cells using weather radar images on the application of machine learning techniques. Overall prediction process consists of identifying storm cells from CAPPI weather radar images by contouring them with a solid 35-dBZ threshold, predicting a track of storm cells, and classifying them based on their damage potential to power grid operators. Tracked storm cells are then classified by combining data obtained from weather radar, ground weather observations, and lightning detectors. We compare random forest classifiers and deep neural networks as alternative methods to classify storm cells. The main challenge is that the training data are heavily imbalanced, as extreme weather events are rare.","","","10.1109/TGRS.2019.2921809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8751131","Machine learning;multilayer perceptrons (MLPs);power distribution faults;radar tracking","Storms;Power grids;Radar tracking;Radar imaging;Meteorological radar","atmospheric techniques;learning (artificial intelligence);lightning;meteorological radar;neural nets;power grids;power system reliability;radar imaging;remote sensing by radar;storms;weather forecasting","short-term prediction;electricity outages;convective storms;power outages;power grid operators;machine learning techniques;CAPPI weather radar images;tracked storm cells;ground weather observations;lightning detectors;random forest classifiers;deep neural networks","","","46","","","","","IEEE","IEEE Journals"
"An Entropy and MRF Model-Based CNN for Large-Scale Landsat Image Classification","X. Zhao; L. Gao; Z. Chen; B. Zhang; W. Liao; X. Yang","Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Airborne Remote Sensing Center, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Department of Telecommunications and Information Processing, IMEC-Ghent University, Ghent, Belgium; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","7","1145","1149","Large-scale Landsat image classification is essential for the production of land cover maps. The rise of convolutional neural networks (CNNs) provides a new idea for the implementation of Landsat image classification. However, pixels in Landsat images have higher uncertainty compared with high-resolution images due to its 30-m spatial resolution. In addition, the current deep learning methods tend to lose detailed information such as boundaries along with the stacking of convolutional and pooling layers. To solve these problems, we propose a new method called entropy and MRF model (EMM)-CNN based on Pyramid Scene Parsing Network. The EMM-CNN uses entropy to decrease the uncertainty of pixels. Then, the Markov random filed (MRF) model is employed to construct the connections between neighboring pixels and defined a prior distribution to prevent the cross entropy from sacrificing detailed information for the overall accuracy. Finally, transfer learning based on the pretrained ImageNet is introduced to overcome the shortage of training samples and boost the speed of the training process. Experimental results demonstrate that the proposed EMM-CNN is able to obtain classification results with fine structure by decreasing the uncertainty and retaining detailed information of the detected image.","","","10.1109/LGRS.2019.2890996","Chinese Academy of Sciences; National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625709","Convolutional neural network (CNN);entropy;Landsat image classification;Markov random field (MRF) model;transfer learning","Remote sensing;Earth;Artificial satellites;Entropy;Uncertainty;Training;Forestry","convolutional neural nets;entropy;geophysical image processing;image classification;image resolution;learning (artificial intelligence);Markov processes","detected image;large-scale Landsat image classification;land cover maps;convolutional neural networks;30-m spatial resolution;convolutional layers;pooling layers;EMM-CNN;cross entropy;neighboring pixels;deep learning methods;pyramid scene parsing network;Markov random filed model;entropy-and-MRF model-based CNN;pretrained ImageNet","","1","15","","","","","IEEE","IEEE Journals"
"sEMG-Based Tremor Severity Evaluation for Parkinson's Disease Using a Light-Weight CNN","Z. Qin; Z. Jiang; J. Chen; C. Hu; Y. Ma","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Man-Machine-Environment Engineering Institute, School of Aerospace Engineering, Tsinghua University, Beijing, China; Tsinghua University Yuquan Hospital, Beijing, China","IEEE Signal Processing Letters","","2019","26","4","637","641","We propose a deep learning based approach for quantifying the tremor severity of Parkinson's disease (PD) based on surface electromyography (sEMG). We design the S-Net, a light weight and computational efficient convolutional neural network that learns the similarity between sEMG signals in terms of the tremor severity. Labeled sEMG samples are used for jointly voting for the final results. Experiments on 147 PD patients demonstrate that our approach outperforms traditional methods by a significant margin. In addition, our approach is simple and has potentials in real applications.","","","10.1109/LSP.2019.2903334","National Natural Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661631","Parkinson's Disease;tremor severity level;sEMG;UPDRS;CNN;similarity learning","Training;Testing;Parkinson's disease;Task analysis;Feature extraction;Hospitals;Muscles","convolutional neural nets;diseases;electromyography;learning (artificial intelligence);medical signal processing","PD patients;light-weight CNN;sEMG-based tremor severity evaluation;S-Net;labeled sEMG samples;sEMG signals;computational efficient convolutional neural network;surface electromyography;deep learning based approach;Parkinson's disease","","","21","","","","","IEEE","IEEE Journals"
"Joint Intensity Transformer Network for Gait Recognition Robust Against Clothing and Carrying Status","X. Li; Y. Makihara; C. Xu; Y. Yagi; M. Ren","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Institute of Scientific and Industrial Research, Osaka University, Osaka, Japan; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Institute of Scientific and Industrial Research, Osaka University, Osaka, Japan; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Information Forensics and Security","","2019","14","12","3102","3115","Clothing and carrying status variations are the two key factors that affect the performance of gait recognition because people usually wear various clothes and carry all kinds of objects, while walking in their daily life. These covariates substantially affect the intensities within conventional gait representations such as gait energy images. Hence, to properly compare a pair of input gait features, an appropriate metric for joint intensity is needed in addition to the conventional spatial metric. We therefore propose a unified joint intensity transformer network for gait recognition that is robust against various clothing and carrying statuses. Specifically, the joint intensity transformer network is a unified deep learning-based architecture containing three parts: a joint intensity metric estimation net, a joint intensity transformer, and a discrimination network. First, the joint intensity metric estimation net uses a well-designed encoder-decoder network to estimate a sample-dependent joint intensity metric for a pair of input gait energy images. Subsequently, a joint intensity transformer module outputs the spatial dissimilarity of two gait energy images using the metric learned by the joint intensity metric estimation net. Third, the discrimination network is a generic convolution neural network for gait recognition. In addition, the joint intensity transformer network is designed with different loss functions depending on the gait recognition task (i.e., a contrastive loss function for the verification task and a triplet loss function for the identification task). The experiments on the world's largest datasets containing various clothing and carrying statuses demonstrate the state-of-the-art performance of the proposed method.","","","10.1109/TIFS.2019.2912577","Japan Society for the Promotion of Science; National R&D Program for Major Research Instruments; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695052","Joint intensity transformer network;joint intensity metric learning;gait recognition","Measurement;Clothing;Gait recognition;Databases;Estimation;Robustness;Dynamics","convolutional neural nets;decoding;estimation theory;gait analysis;image coding;image recognition;image sampling;learning (artificial intelligence)","convolution neural network;encoder-decoder network;unified deep learning-based architecture;joint intensity transformer network module;input gait energy imaging;gait recognition;gait representations;sample-dependent joint intensity metric;discrimination network;joint intensity metric estimation net","","","66","","","","","IEEE","IEEE Journals"
"Cross-Modality Bridging and Knowledge Transferring for Image Understanding","C. Yan; L. Li; C. Zhang; B. Liu; Y. Zhang; Q. Dai","Institute of Information and Control, Hangzhou Dianzi University, Hangzhou, China; Chinese Academy of Sciences, Key Lab of Intelligent Information Processing of Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Institute of Information and Control, Hangzhou Dianzi University, Hangzhou, China; Chinese Academy of Sciences, Advanced Computing Research Laboratory, Institute of Computing Technology, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Multimedia","","2019","21","10","2675","2685","The understanding of web images has been a hot research topic in both artificial intelligence and multimedia content analysis domains. The web images are composed of various complex foregrounds and backgrounds, which makes the design of an accurate and robust learning algorithm a challenging task. To solve the above significant problem, first, we learn a cross-modality bridging dictionary for the deep and complete understanding of a vast quantity of web images. The proposed algorithm leverages the visual features into the semantic concept probability distribution, which can construct a global semantic description for images while preserving the local geometric structure. To discover and model the occurrence patterns between intra- and inter-categories, multi-task learning is introduced for formulating the objective formulation with Capped-ℓ1 penalty, which can obtain the optimal solution with a higher probability and outperform the traditional convex function-based methods. Second, we propose a knowledge-based concept transferring algorithm to discover the underlying relations of different categories. This distribution probability transferring among categories can bring the more robust global feature representation, and enable the image semantic representation to generalize better as the scenario becomes larger. Experimental comparisons and performance discussion with classical methods on the ImageNet, Caltech-256, SUN397, and Scene15 datasets show the effectiveness of our proposed method at three traditional image understanding tasks.","","","10.1109/TMM.2019.2903448","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Key Research Program of Frontier Sciences, CAS; Zhejiang Province Nature Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8662712","Object and scene recognition;image semantic search;cross-modality bridging;multi-task learning;knowledge transferring","Semantics;Visualization;Dictionaries;Task analysis;Probability distribution;Convolutional neural networks;Bridges","feature extraction;image classification;image representation;learning (artificial intelligence);multimedia systems;optimisation;probability","multitask learning;distribution probability transferring;robust global feature representation;image semantic representation;knowledge transferring;web images;robust learning algorithm;cross-modality bridging dictionary;probability distribution;convex function;artificial intelligence;multimedia content analysis;visual features;geometric structure;optimal solution","","1","73","Traditional","","","","IEEE","IEEE Journals"
"Intelligent Latency-Aware Virtual Network Embedding for Industrial Wireless Networks","M. Li; C. Chen; C. Hua; X. Guan","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","IEEE Internet of Things Journal","","2019","6","5","7484","7496","The growing popularity of industrial wireless networks (IWNs) is driven by various applications with stringent timeliness requests. However, the ossification, deep-rooted in the one-application one-network architecture of traditional IWNs, impedes the evolution of IWNs toward smart factory. As a solution, the slice-based network virtualization (NV) breaks the tight coupling between applications and network infrastructure, and thus provides a more flexible and scalable IWN architecture. The application of NV relies on the algorithms that instantiate multiple virtual networks (VNs) on a substrate infrastructure, known as VN embedding (VNE). However, existing VNE algorithms are not necessarily optimal for IWNs due to the absence of QoS-compliant capacity. To this end, so called iVNE, an intelligent latency-aware VNE scheme, is proposed to provide deadline guarantee for various industrial VNs (IVNs), which involves both static embedding and dynamic forwarding. In the static stage, an anypath embedding algorithm is introduced for the new arrival of IVNs so that their resource demands and deadlines can be satisfied with coarse grain. Then, a dynamic anypath forwarding method is incorporated into iVNE to offer intelligent latency sensing via deep Q-learning, and thus forwarding adjustments can be made timely to address the dynamic changes of link quality and network workload. The simulation results are provided to demonstrate the learning efficiency as well as the ability of load-balancing through responsive forwarding under dynamic environment.","","","10.1109/JIOT.2019.2900855","National Natural Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648178","Deep Q-learning;industrial wireless networks (IWNs);low-latency;network virtualization (NV);smart factory","Virtualization;Wireless networks;Quality of service;Monitoring;Substrates;Virtual machine monitors;Internet of Things","embedded systems;learning (artificial intelligence);quality of service;radio networks;resource allocation;telecommunication computing;virtualisation","intelligent latency-aware virtual network;stringent timeliness requests;one-network architecture;slice-based network virtualization;scalable IWN architecture;instantiate multiple virtual networks;intelligent latency-aware VNE scheme;dynamic anypath forwarding method;link quality;network workload;IWN;static anypath embedding algorithm;IVN algorithms;iVNE algorithms;industrial VN algorithms;industrial wireless network infrastructure","","","41","","","","","IEEE","IEEE Journals"
"Knowledge-Aided Convolutional Neural Network for Small Organ Segmentation","Y. Zhao; H. Li; S. Wan; A. Sekuboyina; X. Hu; G. Tetteh; M. Piraud; B. Menze","Department of Computer Science, Technische Universität München, Munich, Germany; Department of Computer Science, Technische Universität München, Munich, Germany; School of Information and Safety Engineering, Zhongnan University of Economics and Law, Wuhan, China; Department of Computer Science, Technische Universität München, Munich, Germany; Department of Computer Science, Technische Universität München, Munich, Germany; Department of Computer Science, Technische Universität München, Munich, Germany; Department of Computer Science, Technische Universität München, Munich, Germany; Department of Computer Science, Technische Universität München, Munich, Germany","IEEE Journal of Biomedical and Health Informatics","","2019","23","4","1363","1373","Accurate and automatic organ segmentation is critical for computer-aided analysis towards clinical decision support and treatment planning. State-of-the-art approaches have achieved remarkable segmentation accuracy on large organs, such as the liver and kidneys. However, most of these methods do not perform well on small organs, such as the pancreas, gallbladder, and adrenal glands, especially when lacking sufficient training data. This paper presents an automatic approach for small organ segmentation with limited training data using two cascaded steps- localization and segmentation. The localization stage involves the extraction of the region of interest after the registration of images to a common template and during the segmentation stage, a voxel-wise label map of the extracted region of interest is obtained and then transformed back to the original space. In the localization step, we propose to utilize a graph-based groupwise image registration method to build the template for registration so as to minimize the potential bias and avoid getting a fuzzy template. More importantly, a novel knowledge-aided convolutional neural network is proposed to improve segmentation accuracy in the second stage. This proposed network is flexible and can combine the effort of both deep learning and traditional methods, consequently achieving better segmentation relative to either of individual methods. The ISBI 2015 VISCERAL challenge dataset is used to evaluate the presented approach. Experimental results demonstrate that the proposed method outperforms cutting-edge deep learning approaches, traditional forest-based approaches, and multiatlas approaches in the segmentation of small organs.","","","10.1109/JBHI.2019.2891526","Technische Universität München–Institute for Advanced Study; German Excellence Initiative; European Union Seventh Framework Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606255","Medical image segmentation;convolutional neural networks;knowledge-aided;deep learning","Image segmentation;Convolutional neural networks;Biological systems;Forestry;Training;Training data;Informatics","biological organs;image matching;image registration;image segmentation;kidney;learning (artificial intelligence);liver;medical image processing;neural nets","knowledge-aided convolutional neural network;traditional forest-based approaches;graph-based groupwise image registration method;localization step;voxel-wise label map;segmentation stage;common template;localization stage;cascaded steps- localization;small organ segmentation;remarkable segmentation accuracy;clinical decision support;computer-aided analysis;automatic organ segmentation","","4","50","","","","","IEEE","IEEE Journals"
"Foreground Detection for Infrared Videos With Multiscale 3-D Fully Convolutional Network","Y. Wang; L. Zhu; Z. Yu","School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University, Beijing, China; School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University, Beijing, China; School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","5","712","716","Foreground detection for infrared (IR) videos is an important and fundamental problem in many applications, e.g., IR surveillance, IR object tracking, and so on. Conventional foreground detection algorithms developed for visible videos do not focus on the problems for IR videos, e.g., low contrast, coarse texture, lack of color information, and so on. Recent foreground detection methods based on deep neural network (DNN) demonstrated significant improvement, but most of them still use only spatial features, which is less obvious in IR images. In this letter, we add deeply learned multiscale temporal features to improve the performance of background subtraction for IR videos. We propose a novel multiscale 3-D fully convolutional network (MFC3-D) to establish a mapping from image sequences to pixelwise classification results and to learn deep and hierarchical multiscale spatial-temporal features of the input images sequence. The experimental results show that the MFC3-D can learn spatial-temporal features effectively and achieved state-of-the-art results on the test data set, comparing to other DNN-based methods and traditional background subtraction methods.","","","10.1109/LGRS.2018.2881053","National Basic Research Program of China (973 Program); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8556042","3-D convolutional (C3-D) networks;background modeling;deep neural networks (DNNs);fully convolutional networks (FCNs);infrared (IR) foreground detection;spatial–temporal features","Feature extraction;Videos;Decoding;Training;Convolution;Image sequences","convolutional neural nets;feature extraction;image classification;image segmentation;image sequences;infrared imaging;learning (artificial intelligence);object detection;video signal processing","IR images;multiscale temporal features;IR videos;MFC3-D;hierarchical multiscale spatial-temporal features;input images sequence;DNN-based methods;infrared videos;visible videos;deep neural network;background subtraction methods;foreground detection algorithms;multiscale 3-D fully convolutional network;pixelwise classification","","","24","","","","","IEEE","IEEE Journals"
"Salient Object Detection Using Cascaded Convolutional Neural Networks and Adversarial Learning","Y. Tang; X. Wu","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Multimedia","","2019","21","9","2237","2247","Salient object detection has received much attention and achieved great success in last several years. It is still challenging to get clear boundaries and consistent saliencies, which can be considered as the structural information of salient objects. A popular solution is to conduct some post-processes (e.g., conditional random field (CRF)) to refine these structural information. In this paper, a novel cascaded convolutional neural networks (CNNs) based method is proposed to implicitly learn these structural information via adversarial learning for salient object detection (we termed the proposed method as CCAL). A cascaded CNNs model is first designed as a generator G, which consists of an encoder-decoder network for global saliency estimation and a deep residual network for local saliency refinement. It is hard to explicitly learn such structural information due to the limitation of frequently-used pixel-wise loss functions. Instead, a discriminator D is then designed to distinguish the real salient maps (i.e., ground truths) from the fake ones produced by G, based on which an adversarial loss is introduced to optimize G. G and D are trained in a fully end-to-end fashion by following the strategy of conditional generative adversarial networks to make G well learn the structural information. At last, G is able to produce high quality salient maps without requiring any post-process to fool D. Experimental results on eight benchmark datasets demonstrate the effectiveness and efficiency (about 17 fps on graphics processing unit (GPU)) of the proposed method for salient object detection.","","","10.1109/TMM.2019.2900908","National Natural Science Foundation of China; National Key R&D Program of China; Distinguished Youth Science Foundation of Heilongjiang Province of China; Shandong Provincial Natural Science Foundation, China; Humanity and Social Science Youth Foundation of the Ministry of Education of China; State Key Laboratory of Robotics and System; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8649755","Salient object detection;cascaded convolutional neural networks;conditional generative adversarial networks;adversarial learning","Object detection;Feature extraction;Generators;Estimation;Convolutional neural networks;Decoding;Kernel","convolutional neural nets;object detection","salient object detection;adversarial learning;structural information;salient objects;convolutional neural networks based method;cascaded CNNs model;encoder-decoder network;conditional generative adversarial networks;high quality salient maps;cascaded convolutional neural networks","","1","70","Traditional","","","","IEEE","IEEE Journals"
"Data Augmentation for Hyperspectral Image Classification With Deep CNN","W. Li; C. Chen; M. Zhang; H. Li; Q. Du","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Geoscience and Remote Sensing Letters","","2019","16","4","593","597","Convolutional neural network (CNN) has been widely used in hyperspectral imagery (HSI) classification. Data augmentation is proven to be quite effective when training data size is relatively small. In this letter, extensive comparison experiments are conducted with common data augmentation methods, which draw an observation that common methods can produce a limited and up-bounded performance. To address this problem, a new data augmentation method, named as pixel-block pair (PBP), is proposed to greatly increase the number of training samples. The proposed method takes advantage of deep CNN to extract PBP features, and decision fusion is utilized for final label assignment. Experimental results demonstrate that the proposed method can outperform the existing ones.","","","10.1109/LGRS.2018.2878773","National Natural Science Foundation of China; Natural Science Foundation of Beijing Municipality; Beijing Nova Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8542643","Convolutional neural network (CNN);data augmentation;hyperspectral imagery (HSI);pattern classification","Training;Testing;Convolution;Hyperspectral imaging;Feature extraction;Kernel","convolutional neural nets;feature extraction;geophysical image processing;image classification;learning (artificial intelligence);object detection","training data size;extensive comparison experiments;common data;data augmentation method;training samples;deep CNN;hyperspectral image classification;convolutional neural network;hyperspectral imagery classification","","","21","","","","","IEEE","IEEE Journals"
"Deep neural network with FGL for small dataset classification","C. Guo; R. Li; M. Yang; X. Tang","College of Communication Engineering, Hangzhou Dianzi University, People's Republic of China; College of Communication Engineering, Hangzhou Dianzi University, People's Republic of China; College of Communication Engineering, Hangzhou Dianzi University, People's Republic of China; College of Communication Engineering, Hangzhou Dianzi University, People's Republic of China","IET Image Processing","","2019","13","3","491","497","In certain applications, classification models have to be trained with small datasets. This study proposes a new deep neural network with a feature generalisation layer (FGL). First, instead of using a generative network for data augmentation, the FGL is modelled using a latent variable model to diversify features directly by sharing other layers. Then, dual-objective functions are defined to optimise the parameters of the network: one minimises the generation error and the other minimises the classification error. Finally, a parallel multibranch structure is used in the FGL to improve the convergence of model training. The classification accuracy obtained using various quantities of training samples increased up to 4.63% on the MNIST dataset, up to 3.00% on the CIFAR10 nature image dataset, over the reference model. These experimental results illustrate the effectiveness of the authors' method for training classification models with small datasets.","","","10.1049/iet-ipr.2018.5616","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653230","","","learning (artificial intelligence);neural nets;pattern classification","dataset classification;reference model;CIFAR10 nature image dataset;MNIST dataset;training samples;classification accuracy;classification error;dual-objective functions;latent variable model;FGL;data augmentation;generative network;feature generalisation layer;deep neural network;classification models","","","21","","","","","IET","IET Journals"
"Toward End-to-End Car License Plate Detection and Recognition With Deep Neural Networks","H. Li; P. Wang; C. Shen","School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Intelligent Transportation Systems","","2019","20","3","1126","1136","In this paper, we tackle the problem of car license plate detection and recognition in natural scene images. We propose a unified deep neural network, which can localize license plates and recognize the letters simultaneously in a single forward pass. The whole network can be trained end-to-end. In contrast to existing approaches which take license plate detection and recognition as two separate tasks and settle them step by step, our method jointly solves these two tasks by a single network. It not only avoids intermediate error accumulation but also accelerates the processing speed. For performance evaluation, four data sets including images captured from various scenes under different conditions are tested. Extensive experiments show the effectiveness and the efficiency of our proposed approach.","","","10.1109/TITS.2018.2847291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424450","Car plate detection and recognition;convolutional neural networks;recurrent neural networks","Licenses;Text recognition;Character recognition;Image edge detection;Feature extraction;Task analysis","character recognition;feature extraction;image capture;image recognition;learning (artificial intelligence);natural scenes;neural nets;object detection;object recognition;traffic engineering computing","deep neural networks;car license plate detection;natural scene images;end-to-end training;car license plate recognition","","5","43","","","","","IEEE","IEEE Journals"
"Natural Language Statistical Features of LSTM-Generated Texts","M. Lippi; M. A. Montemurro; M. Degli Esposti; G. Cristadoro","Department of Sciences and Methods for Engineering, University of Modena and Reggio Emilia, Modena, Italy; Division of Neuroscience and Experimental Psychology, The University of Manchester, Manchester, U.K.; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Mathematics and Applications, University of Milano - Bicocca, Milan, Italy","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","11","3326","3337","Long short-term memory (LSTM) networks have recently shown remarkable performance in several tasks that are dealing with natural language generation, such as image captioning or poetry composition. Yet, only few works have analyzed text generated by LSTMs in order to quantitatively evaluate to which extent such artificial texts resemble those generated by humans. We compared the statistical structure of LSTM-generated language to that of written natural language, and to those produced by Markov models of various orders. In particular, we characterized the statistical structure of language by assessing word-frequency statistics, long-range correlations, and entropy measures. Our main finding is that while both LSTM- and Markov-generated texts can exhibit features similar to real ones in their word-frequency statistics and entropy measures, LSTM-texts are shown to reproduce long-range correlations at scales comparable to those found in natural language. Moreover, for LSTM networks, a temperature-like parameter controlling the generation process shows an optimal value-for which the produced texts are closest to real language-consistent across different statistical features investigated.","","","10.1109/TNNLS.2019.2890970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8681285","Authorship attribution;entropy;long short-term memory networks;long-range correlations;natural language generation (NLG)","Natural languages;Computer architecture;Correlation;Training;Microprocessors;Entropy;Deep learning","entropy;Markov processes;natural language processing;recurrent neural nets;statistical analysis;text analysis","natural language statistical features;LSTM-generated texts;short-term memory networks;natural language generation;image captioning;statistical structure;LSTM-generated language;Markov models;word-frequency statistics;long-range correlations;entropy measures;Markov-generated texts;LSTM-texts;LSTM networks","","2","50","","","","","IEEE","IEEE Journals"
"Recognizing Brain States Using Deep Sparse Recurrent Neural Network","H. Wang; S. Zhao; Q. Dong; Y. Cui; Y. Chen; J. Han; L. Xie; T. Liu","College of Biomedical Engineering & Instrument Science, Zhejiang University, Hangzhou, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; Department of Computer Science, Cortical Architecture Imaging and Discovery Lab, Bioimaging Research Center, University of Georgia, Athens, GA, USA; College of Biomedical Engineering & Instrument Science, Zhejiang University, Hangzhou, China; College of Biomedical Engineering & Instrument Science, Zhejiang University, Hangzhou, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; College of Biomedical Engineering & Instrument Science, Zhejiang University, Hangzhou, China; Department of Computer Science, Cortical Architecture Imaging and Discovery Lab, Bioimaging Research Center, University of Georgia, Athens, GA, USA","IEEE Transactions on Medical Imaging","","2019","38","4","1058","1068","Brain activity is a dynamic combination of different sensory responses and thus brain activity/state is continuously changing over time. However, the brain's dynamical functional states recognition at fast time-scales in task fMRI data have been rarely explored. In this paper, we propose a novel 5-layer deep sparse recurrent neural network (DSRNN) model to accurately recognize the brain states across the whole scan session. Specifically, the DSRNN model includes an input layer, one fully-connected layer, two recurrent layers, and a softmax output layer. The proposed framework has been tested on seven task fMRI data sets of Human Connectome Project. Extensive experiment results demonstrate that the proposed DSRNN model can accurately identify the brain's state in different task fMRI data sets and significantly outperforms other auto-correlation methods or non-temporal approaches in the dynamic brain state recognition accuracy. In general, the proposed DSRNN offers a new methodology for basic neuroscience and clinical research.","","","10.1109/TMI.2018.2877576","National Key R&D Program of China; Fundamental Research Funds for the Central Universities; Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China; National Science Foundation of China; Fundamental Research Funds for the Central Universities; China Postdoctoral Science Foundation; Zhejiang Province Science and Technology Planning; National Institutes of Health; National Institutes of Health; NSF CAREER; National Science Foundation; National Science Foundation; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502825","Dynamic brain state;recurrent neural network;fMRI;brain networks","Task analysis;Brain modeling;Recurrent neural networks;Functional magnetic resonance imaging;Logic gates;Data models","biomedical MRI;brain;feedforward neural nets;learning (artificial intelligence);neural nets;neurophysiology;recurrent neural nets;signal classification","dynamic combination;brain activity/state;DSRNN model;softmax output layer;data sets;dynamic brain state recognition accuracy;deep sparse recurrent neural network model","","","71","","","","","IEEE","IEEE Journals"
"Hierarchical Decision and Control for Continuous Multitarget Problem: Policy Evaluation With Action Delay","J. Zhu; J. Zhu; Z. Wang; S. Guo; C. Xu","State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; School of Mathematical Sciences, Zhejiang University, Hangzhou, China; State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","2","464","473","This paper proposes a hierarchical decision-making and control algorithm for the shepherd game, the seventh mission in the International Aerial Robotics Competition (IARC). In this game, the agent (a multirotor aerial robot) is required to contact targets (ground vehicles) sequentially and drive them to a certain boundary to earn score. During the game of 10 min, the agent should be fully autonomous without any human interference. Regarding the lower-level controller and dynamics of the agent, each action takes a duration of time to accomplish. Denoted as an action delay, in this paper, this action duration is nonconstant and is related to the final reward. Therefore, the challenging point is making the agent “aware of time” when applying a certain action. We solve this problem by two approaches: deep Q-networks and lookup table. The action delay predictor in the decision-level is fitted by a lower-level controller. Through simulations by the example of the shepherd game, the effectiveness and efficiency of this approach are validated. This paper helps our team winning the first prize in IARC 2017, and keeps the best record of this mission since it was released in 2013.","","","10.1109/TNNLS.2018.2844466","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities of China; National Natural Science Foundation of China through Foundation for Innovative Research Groups; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8401524","Deep reinforcement learning (RL);Monte Carlo sampling;policy evaluation","Games;Mobile robots;Delays;Drones;Decision making;Collision avoidance","aircraft control;autonomous aerial vehicles;decision making;delays;mobile robots;remotely operated vehicles;target tracking","contact targets;ground vehicles;human interference;lower-level controller;action duration;action delay predictor;decision-level;shepherd game;IARC 2017;continuous multitarget problem;policy evaluation;hierarchical decision-making;control algorithm;seventh mission;multirotor aerial robot;international aerial robotics competition","","","30","","","","","IEEE","IEEE Journals"
"Turning video into traffic data – an application to urban intersection analysis using transfer learning","B. Dey; M. K. Kundu","Center for Soft Computing Research, Indian Statistical Institute, India; , Indian Statistical Unit, India","IET Image Processing","","2019","13","4","673","679","With modern socio-economic development, the number of vehicles in metropolitan cities is growing rapidly. Therefore, obtaining real-time traffic volume estimates has a very important significance in using the limited road space and traffic infrastructure. In this study, the authors present a video-based traffic volume and direction estimation at road intersections. To discriminate the vehicles from the remaining foreground objects, vehicle recognition is performed by training a deep-learning architecture from a pre-trained model. This method, called transfer learning, primarily circumvents the requirement of huge labelled datasets and the time for training the network. The video sequence is first detected for moving foreground regions or patches. The trained model is subsequently used to classify the vehicles. The vehicles are tracked, and trajectory patterns are clustered using standard techniques. The number and direction of vehicles are noted, which are later compared with the manually observed values. All experiments were performed on real-life surveillance sequences recorded at four different traffic intersections in the city of Kolkata.","","","10.1049/iet-ipr.2018.5985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695180","","","image sequences;video signal processing;traffic engineering computing;learning (artificial intelligence);road traffic;object detection;image motion analysis;video surveillance;road vehicles","turning video;traffic data;urban intersection analysis;modern socio-economic development;metropolitan cities;real-time traffic volume estimates;road space;traffic infrastructure;video-based traffic volume;direction estimation;road intersections;vehicle recognition;pre-trained model;huge labelled datasets;video sequence;foreground regions;patches;foreground objects;transfer learning;traffic intersections","","","","","","","","IET","IET Journals"
"A Deep Coupled Network for Health State Assessment of Cutting Tools Based on Fusion of Multisensory Signals","M. Ma; C. Sun; X. Chen; X. Zhang; R. Yan","Department of Mechanical Engineering, University of Massachusetts Lowell, Lowell, MA, USA; School of Mechanical Engineering, Xi’an Jiaotong University, Xi’an, China; School of Mechanical Engineering, Xi’an Jiaotong University, Xi’an, China; School of Mechanical Engineering, Xi’an Jiaotong University, Xi’an, China; School of Mechanical Engineering, Xi’an Jiaotong University, Xi’an, China","IEEE Transactions on Industrial Informatics","","2019","15","12","6415","6424","The cutting tool is a key part of a machine system, which plays an important role in modern manufacturing systems. To avoid an unexpected tool failure, it is necessary to carry out health condition assessment of cutting tools. In this paper, a deep coupled restricted Boltzmann machine (DCRBM) is proposed for health state assessment of cutting tools based on fusion of vibration signals and acoustic emission (AE) signals. Because of the complementary of multisensory signals, it is necessary to develop a fusion strategy for the fusion of multisource signals. The proposed DCRBM is symmetric with each side consisting of several hidden layers and one coupled layer, which is constructed by two basic restricted Boltzmann machines with similarity constraints. Vibration signals and AE signals, which are connected with the two sides of DCRBM, respectively, are mapped into a feature space, where similar representations are learned. The parameters of the deep architecture are learned by optimizing the new objective function. Experimental results on fusion of vibration signals and AE signals demonstrate the promising performance of DCRBM for health state assessment of cutting tools compared with other fusion strategies.","","","10.1109/TII.2019.2912428","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Shaanxi Province Postdoctoral Science Foundation; Fundamental Research Funds for the Central Universities; Science and Technology Department in Shaanxi Province of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695114","Cutting tool;deep coupled network;health state assessment;multisensory signal fusion","Cutting tools;Tools;Sensors;Feature extraction;Vibrations;Data models;Monitoring","","","","","34","IEEE","","","","IEEE","IEEE Journals"
"PC2A: Predicting Collective Contextual Anomalies via LSTM With Deep Generative Model","S. Dou; K. Yang; H. V. Poor","Department of Computer Science, Tongji University, Shanghai, China; Department of Computer Science, Tongji University, Shanghai, China; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA","IEEE Internet of Things Journal","","2019","6","6","9645","9655","Proactive anomaly detection and diagnosis play an essential role in ensuring the security and stability of a large-scale information technology (IT) system with thousands or even millions of components that are interacting with each other. Collective contextual anomalies (CCAs) carry the characteristics of both collective and contextual anomalies. This type of anomalies is common in IT system monitoring, often manifested as security risks to be ameliorated, service outages to be eliminated, or stragglers to be mitigated. However, most existing studies emphasize primarily on the detection of point anomalies while the prediction or early detection of CCA has been an underexplored topic. In this paper, we propose a framework for discovering and studying CCAs in multiple time series based on a combination of semi-supervised deep learning, time series modeling, and graph analysis. A primary advantage of the proposed framework is that it can effectively predict CCAs with no human intervention. In addition, the performance of the proposed method can be further enhanced via learning from a small amount of anomalous training data, if it is available. Finally, the proposed framework is of low computational complexity and is thus particularly suitable for large-scale data streams. Simulation studies are carried out to reveal the superior performance of the proposed method and underscore the significant benefits of combining deep neural networks with time series analysis and graph models for the prediction and analysis of CCAs.","","","10.1109/JIOT.2019.2930202","National Natural Science Foundation of China; 2017 Key Joint Research Program of China Mobile and MOE of China; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794857","Anomaly prediction;collective anomaly;cyber security;deep generative model;high-dimensional time series;LSTM neural-network;root cause analysis","","","","","","41","IEEE","","","","IEEE","IEEE Journals"
"Application of Neural Network to GNSS-R Wind Speed Retrieval","Y. Liu; I. Collett; Y. J. Morton","Smead Aerospace Engineering Sciences Department, University of Colorado Boulder, Boulder, CO, USA; Smead Aerospace Engineering Sciences Department, University of Colorado Boulder, Boulder, CO, USA; Smead Aerospace Engineering Sciences Department, University of Colorado Boulder, Boulder, CO, USA","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","12","9756","9766","This paper applies a machine learning (ML) algorithm based on the multi-hidden layer neural network (MHL-NN) for ocean surface wind speed estimation using global navigation satellite system (GNSS) reflection measurements. Unlike conventional wind speed retrieval methods that often depend on limited scalar delay-Doppler map (DDM) observables, the proposed MHL-NN makes use of information captured by the entire DDM. Both simulated and real data sets are used to train and evaluate the performance of the MHL-NN and compare it to a conventional wind speed retrieval method and other prevailing ML algorithms. The results show that the MHL-NN algorithm outperforms the other methods in terms of the root mean square error (RMSE) and mean absolute percentage error (MAPE) of the wind speed estimation.","","","10.1109/TGRS.2019.2929002","University of Colorado Boulder; National Aeronautics and Space Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802279","Advanced Scatterometer (ASCAT);cyclone global navigation satellite system (CYGNSS);deep learning;delay-Doppler map (DDM);GNSS-reflectometry (GNSS-R);multi-hidden layer neural network (MHL-NN);spaceborne remote sensing;wind speed retrieval","Wind speed;Sea surface;Sea measurements;Global navigation satellite system;Neurons;Satellites","atmospheric techniques;geophysics computing;learning (artificial intelligence);mean square error methods;multilayer perceptrons;oceanographic techniques;satellite navigation;wind","GNSS-r;machine learning algorithm;multihidden layer neural network;ocean surface wind;global navigation satellite system reflection measurements;retrieval methods;scalar delay-Doppler map;entire DDM;simulated data sets;real data sets;conventional wind speed retrieval method;prevailing ML algorithms;MHL-NN algorithm","","1","36","IEEE","","","","IEEE","IEEE Journals"
"CNN-Based Real-Time Dense Face Reconstruction with Inverse-Rendered Photo-Realistic Face Images","Y. Guo; j. zhang; J. Cai; B. Jiang; J. Zheng","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; Nanyang Technological University, Singapore; University of Science and Technology of China, Hefei, China; Nanyang Technological University, Singapore","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","6","1294","1307","With the powerfulness of convolution neural networks (CNN), CNN based face reconstruction has recently shown promising performance in reconstructing detailed face shape from 2D face images. The success of CNN-based methods relies on a large number of labeled data. The state-of-the-art synthesizes such data using a coarse morphable face model, which however has difficulty to generate detailed photo-realistic images of faces (with wrinkles). This paper presents a novel face data generation method. Specifically, we render a large number of photo-realistic face images with different attributes based on inverse rendering. Furthermore, we construct a fine-detailed face image dataset by transferring different scales of details from one image to another. We also construct a large number of video-type adjacent frame pairs by simulating the distribution of real video data.11.All these coarse-scale and fine-scale photo-realistic face image datasets can be downloaded from https://github.com/Juyong/3DFace. With these nicely constructed datasets, we propose a coarse-to-fine learning framework consisting of three convolutional networks. The networks are trained for real-time detailed 3D face reconstruction from monocular video as well as from a single image. Extensive experimental results demonstrate that our framework can produce high-quality reconstruction but with much less computation time compared to the state-of-the-art. Moreover, our method is robust to pose, expression and lighting due to the diversity of data.","","","10.1109/TPAMI.2018.2837742","National Key R&D Program of China; National Natural Science Foundation of China; Youth Innovation Promotion Association of the Chinese Academy of Sciences; WASP/NTU; MOE Tier-2; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360505","3D face reconstruction;face tracking;face performance capturing;3D face dataset;image synthesis;deep learning","Face;Image reconstruction;Three-dimensional displays;Rendering (computer graphics);Geometry;Solid modeling;Real-time systems","convolutional neural nets;face recognition;image reconstruction;learning (artificial intelligence);realistic images;rendering (computer graphics);stereo image processing;video signal processing","dense face reconstruction;convolution neural networks;2D face images;coarse morphable face model;face data generation method;inverse rendering;fine-detailed face image dataset;video-type adjacent frame pairs;fine-scale photo-realistic face image datasets;coarse-to-fine learning framework;real-time detailed 3D face reconstruction;high-quality reconstruction;CNN;photo-realistic face images;face reconstruction;face shape","","4","61","","","","","IEEE","IEEE Journals"
"Spectral-Spatial Feature Extraction and Classification by ANN Supervised With Center Loss in Hyperspectral Imagery","A. J. X. Guo; F. Zhu","Center for Applied Mathematics, Tianjin University, Tianjin, China; Center for Applied Mathematics, Tianjin University, Tianjin, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","3","1755","1767","In this paper, we propose a spectral-spatial feature extraction and classification framework based on an artificial neuron network in the context of hyperspectral imagery. With limited labeled samples, only spectral information is exploited for training and spatial context is integrated posteriorly at the testing stage. Taking advantage of recent advances in face recognition, a joint supervision symbol that combines softmax loss and center loss is adopted to train the proposed network, by which intraclass features are gathered while interclass variations are enlarged. Based on the learned architecture, the extracted spectrum-based features are classified by a center classifier. Moreover, to fuse the spectral and spatial information, an adaptive spectral-spatial center classifier is developed, where multiscale neighborhoods are considered simultaneously, and the final label is determined using an adaptive voting strategy. Finally, experimental results on three well-known data sets validate the effectiveness of the proposed methods compared with the state-of-the-art approaches.","","","10.1109/TGRS.2018.2869004","National Natural Science Foundation of China; Natural Science Foundation of Tianjin City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8475019","Artificial neural networks (ANN);deep learning;feature extraction;hyperspectral image classification","Feature extraction;Training;Neurons;Hyperspectral imaging;Testing;Iron","face recognition;feature extraction;hyperspectral imaging;image classification;learning (artificial intelligence);neural nets","spectral-spatial feature extraction;center loss;hyperspectral imagery;classification framework;artificial neuron network;spectral information;spatial context;joint supervision symbol;softmax loss;intraclass features;extracted spectrum-based features;spatial information;ANN;adaptive spectral-spatial center classifier;learned architecture;interclass variations","","2","43","","","","","IEEE","IEEE Journals"
"Convolutional Autoencoder for Feature Extraction in Tactile Sensing","M. Polic; I. Krajacic; N. Lepora; M. Orsag","Laboratory for Robotics and Intelligent Control Systems, Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia; Laboratory for Robotics and Intelligent Control Systems, Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia; Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol, U.K.; Laboratory for Robotics and Intelligent Control Systems, Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia","IEEE Robotics and Automation Letters","","2019","4","4","3671","3678","A common approach in the field of tactile robotics is the development of a new perception algorithm for each new application of existing hardware solutions. In this letter, we present a method of dimensionality reduction of an optical-based tactile sensor image output using a convolutional neural network encoder structure. Instead of using various complex perception algorithms, and/or manually choosing task-specific data features, this unsupervised feature extraction method allows simultaneous online deployment of multiple simple perception algorithms on a common set of black-box features. The method is validated on a set of benchmarking use cases. Contact object shape, edge position, orientation, and indentation depth are estimated using shallow neural networks and machine learning models. Furthermore, a contact force estimator is trained, affirming that the extracted features contain sufficient information on both spatial and mechanical characteristics of the manipulated object.","","","10.1109/LRA.2019.2927950","Hrvatska Zaklada za Znanost; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758942","Force and tactile sensing;deep learning in robotics and automation;soft sensors and actuators;perception for grasping and manipulation","Feature extraction;Pins;Tactile sensors;Training","convolutional neural nets;feature extraction;learning (artificial intelligence);tactile sensors","tactile robotics;perception algorithm;dimensionality reduction;convolutional neural network encoder structure;complex perception algorithms;task-specific data features;unsupervised feature extraction method;simultaneous online deployment;multiple simple perception algorithms;black-box features;contact object shape;shallow neural networks;machine learning models;contact force estimator;convolutional autoencoder;optical-based tactile sensor image output;edge position;spatial characteristic;mechanical characteristic","","","27","Traditional","","","","IEEE","IEEE Journals"
"Shallowing Deep Networks: Layer-Wise Pruning Based on Feature Representations","S. Chen; Q. Zhao","Department of Computer Science and Engineering, University of Minnesota System, Minneapolis, MN, USA; Department of Computer Science and Engineering, University of Minnesota System, Minneapolis, MN, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","12","3048","3056","Recent surge of Convolutional Neural Networks (CNNs) has brought successes among various applications. However, these successes are accompanied by a significant increase in computational cost and the demand for computational resources, which critically hampers the utilization of complex CNNs on devices with limited computational power. In this work, we propose a feature representation based layer-wise pruning method that aims at reducing complex CNNs to more compact ones with equivalent performance. Different from previous parameter pruning methods that conduct connection-wise or filter-wise pruning based on weight information, our method determines redundant parameters by investigating the features learned in the convolutional layers and the pruning process is operated at a layer level. Experiments demonstrate that the proposed method is able to significantly reduce computational cost and the pruned models achieve equivalent or even better performance compared to the original models on various datasets.","","","10.1109/TPAMI.2018.2874634","NSF; University of Minnesota Department of Computer Science and Engineering Start-up Fund (QZ); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8485719","Model pruning;compact design;convolutional neural networks","Computational modeling;Computational efficiency;Feature extraction;Task analysis;Convolutional neural networks;Acceleration","convolutional neural nets;learning (artificial intelligence)","computational resources;complex CNNs;computational power;feature representation based layer-wise pruning method;filter-wise pruning;convolutional layers;pruning process;computational cost;deep Networks;feature representations;convolutional neural networks;connection-wise pruning;parameter pruning methods","","1","36","","","","","IEEE","IEEE Journals"
"Joint 2-D–3-D Traffic Sign Landmark Data Set for Geo-Localization Using Mobile Laser Scanning Data","C. You; C. Wen; C. Wang; J. Li; A. Habib","Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University, Xiamen, FJ, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University, Xiamen, FJ, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University, Xiamen, FJ, China; MoE Key Laboratory of Underwater Acoustic Communication and Marine Information Technology, School of Information Science and Engineering, Xiamen University, Xiamen, FJ, China; Lyles School of Civil Engineering, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Intelligent Transportation Systems","","2019","20","7","2550","2565","This paper presents a framework to build a joint 2-D-3-D traffic sign landmark data set for geo-localization using mobile laser scanning (MLS) data. The MLS data include 3-D point clouds and corresponding multi-view images. First, an integrated method, based on a deep learning network and the retro-reflective properties of traffic signs, is developed to accurately extract traffic signs from MLS point clouds. Next, the semantic and spatial properties of the traffic signs (type, location, position, and geometric characteristics) are obtained. Then, a joint 2-D-3-D traffic sign landmark data set is built, and a semantic-spatial organization graph is used to organize the traffic sign data set. Last, based on the traffic sign landmark data set, a geo-localization method for a driving car is proposed to estimate the driving trajectory. It can be used for auxiliary positioning of autonomous vehicles. Experimental results demonstrate the reliability of our proposed method for traffic sign detection and the potential of building 2-D-3-D traffic sign landmark data set for driving trajectory estimation from MLS data.","","","10.1109/TITS.2018.2868168","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478211","Point cloud;multi-view images;mobile laser scanning (MLS);traffic sign;joint 2-D-3-D;geo-localization","Three-dimensional displays;Semantics;Trajectory;Machine learning;Shape;Autonomous vehicles;Estimation","feature extraction;geographic information systems;graph theory;learning (artificial intelligence);mobile computing;mobile robots;object detection;road traffic;traffic engineering computing","traffic sign detection;MLS data;mobile laser scanning data;joint 2D-3D traffic sign landmark data set;3D point clouds;multiview images;deep learning network;traffic sign extraction;MLS point clouds;semantic properties;spatial properties;semantic-spatial organization graph;geo-localization method;driving trajectory;auxiliary f autonomous vehicle positioning","","","46","","","","","IEEE","IEEE Journals"
"Remote Sensing Image Scene Classification Using Rearranged Local Features","Y. Yuan; J. Fang; X. Lu; Y. Feng","Chinese Academy of Sciences, Xi’an Institute of Optics and Precision Mechanics, Xi’an, China; Chinese Academy of Sciences, Xi’an Institute of Optics and Precision Mechanics, Xi’an, China; Chinese Academy of Sciences, Xi’an Institute of Optics and Precision Mechanics, Xi’an, China; Chinese Academy of Sciences, Xi’an Institute of Optics and Precision Mechanics, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","3","1779","1792","Remote sensing image scene classification is a fundamental problem, which aims to label an image with a specific semantic category automatically. Recently, deep learning methods have achieved competitive performance for remote sensing image scene classification, especially the methods based on a convolutional neural network (CNN). However, most of the existing CNN methods only use feature vectors of the last fully connected layer. They give more importance to global information and ignore local information of images. It is common that some images belong to different categories, although they own similar global features. The reason is that the category of an image may be highly related to local features, other than the global feature. To address this problem, a method based on rearranged local features is proposed in this paper. First, outputs of the last convolutional layer and the last fully connected layer are employed to depict the local and global information, respectively. After that, the remote sensing images are clustered to several collections using their global features. For each collection, local features of an image are rearranged according to their similarities with local features of the cluster center. In addition, a fusion strategy is proposed to combine global and local features for enhancing the image representation. The proposed method surpasses the state of the arts on four public and challenging data sets: UC-Merced, WHU-RS19, Sydney, and AID.","","","10.1109/TGRS.2018.2869101","National Natural Science Foundation of China; Chinese Academy of Sciences; National Key R&D Program of China; CAS Light of West China Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8485313","Feature fusion;rearranged local features;remote sensing image;representation scene classification","Remote sensing;Feature extraction;Visualization;Task analysis;Semantics;Image color analysis;Machine learning","convolutional neural nets;feature extraction;geophysical image processing;image classification;image representation;learning (artificial intelligence);remote sensing","similar global features;global feature;rearranged local features;fully connected layer;local information;global information;image representation;remote sensing image scene classification;CNN;convolutional neural network;deep learning methods;UC-Merced dataset;WHU-RS19 dataset;Sydney dataset;AID dataset","","2","52","","","","","IEEE","IEEE Journals"
"Increasing Compactness of Deep Learning Based Speech Enhancement Models With Parameter Pruning and Quantization Techniques","J. Wu; C. Yu; S. Fu; C. Liu; S. Chien; Y. Tsao","Graduate Institute of Environmental Engineering, National Taiwan University, Taipei City, Taiwan; Graduate Institute of Environmental Engineering, National Taiwan University, Taipei City, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Graduate Institute of Environmental Engineering, National Taiwan University, Taipei City, Taiwan; Graduate Institute of Environmental Engineering, National Taiwan University, Taipei City, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","IEEE Signal Processing Letters","","2019","26","12","1887","1891","The most recent studies on deep learning based speech enhancement (SE) are focused on improving denoising performance. However, successful SE applications require striking a desirable balance between the denoising performance and computational cost in real scenarios. In this study, we propose a novel parameter pruning (PP) technique, which removes redundant channels in a neural network. In addition, parameter quantization (PQ) and feature-map quantization (FQ) techniques were also integrated to generate even more compact SE models. The experimental results show that the integration of PP, PQ, and FQ can produce a compacted SE model with a size of only 9.76 ${\%}$ compared to that of the original model, resulting in minor performance losses of 0.01 (from 0.85 to 0.84) and 0.03 (from 2.55 to 2.52) for STOI and PESQ scores, respectively. These promising results confirm that the PP, PQ, and FQ techniques can be used to effectively reduce the storage of an SE system on edge devices.","","","10.1109/LSP.2019.2951950","Ministry of Science and Technology, Taiwan; National Taiwan University; Intel Corporation; Delta Electronics; Compal Electronics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8892545","Compactness;Parameter Pruning;Parameter Quantization;Low Computational Cost","Quantization (signal);Speech enhancement;Signal to noise ratio;Training;Neural networks;Noise measurement;Computational modeling","","","","","25","IEEE","","","","IEEE","IEEE Journals"
"Improved Low-Bitrate HEVC Video Coding Using Deep Learning Based Super-Resolution and Adaptive Block Patching","H. Lin; X. He; L. Qing; Q. Teng; S. Yang","College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; Artificial Intelligence Laboratory, TAL Education Group, Beijing, China","IEEE Transactions on Multimedia","","2019","21","12","3010","3023","Good-quality video coding for low-bitrate applications is essential for narrow bandwidth transmission and limited capacity storage. In this paper, we propose an adaptive downsampling-based coding model to improve the low-bitrate compression efficiency of high-efficiency video coding (HEVC). At the encoder, the video sequence is adaptively divided into key frames (KFs) and nonkey frames (NKFs), which are encoded at the original resolution and at a reduced resolution, respectively. At the decoder, a super-resolution method based on deep learning and gradient transformation is used to upscale the NKFs. To improve the quality of NKFs without additional information during decoding, we use motion estimation to find the most similar blocks between the upscaled NKFs and the associated high-resolution KFs. Then, an adaptive patching-based method is used to warp the low-quality NKF blocks with the high-quality KF blocks. Experimental results indicate that for standard high-definition test video sequences, the maximum improvement in the peak signal-to-noise ratio can reach 3.54 dB, and the critical bitrate can reach 9.89 Mb/s at a low bitrate when compared to HEVC. These results demonstrate significant improvements compared to existing methods.","","","10.1109/TMM.2019.2919433","National Natural Science Foundation of China; Fundamental Research for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8723517","High-efficiency video coding;key frames;low bitrate;motion estimation;patching;super-resolution;video compression","Motion estimation;Bit rate;Image reconstruction;High efficiency video coding;Video compression;Superresolution","","","","","57","IEEE","","","","IEEE","IEEE Journals"
"Reinforcement Learning-Based Microgrid Energy Trading With a Reduced Power Plant Schedule","X. Lu; X. Xiao; L. Xiao; C. Dai; M. Peng; H. V. Poor","Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Key Laboratory of Universal Wireless Communication (Ministry of Education), Beijing University of Posts and Telecommunications, Beijing, China; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA","IEEE Internet of Things Journal","","2019","6","6","10728","10737","With dynamic renewable energy generation and power demand, microgrids (MGs) exchange energy with each other to reduce their dependence on power plants. In this article, we present a reinforcement learning (RL)-based MG energy trading scheme to choose the electric energy trading policy according to the predicted future renewable energy generation, the estimated future power demand, and the MG battery level. This scheme designs a deep RL-based energy trading algorithm to address the supply–demand mismatch problem for a smart grid with a large number of MGs without relying on the renewable energy generation and power demand models of other MGs. A performance bound on the MG utility and dependence on the power plant is provided. Simulation results based on a smart grid with three MGs using wind speed data from Hong Kong Observation and electricity prices from ISO New England show that this scheme significantly reduces the average power plant schedule and thus increases the MG utility in comparison with a benchmark methodology.","","","10.1109/JIOT.2019.2941498","National Natural Science Foundation of China; State Major Science and Technology Special Project; Fundamental Research Funds for the Central Universities; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839066","Energy trading;power plant schedule;reinforcement learning (RL);smart grids","Power generation;Power demand;Renewable energy sources;Batteries;Smart grids;Reinforcement learning;Schedules","","","","","32","IEEE","","","","IEEE","IEEE Journals"
"A Learnable Distortion Correction Module for Modulation Recognition","K. Yashashwi; A. Sethi; P. Chaporkar","Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India; Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India; Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India","IEEE Wireless Communications Letters","","2019","8","1","77","80","Modulation recognition is a challenging task while performing spectrum sensing in cognitive radio. Recently, deep learning techniques, such as convolutional neural networks (CNNs) have been shown to achieve state-of-the-art accuracy for modulation recognition. However, CNNs are not explicitly designed to undo distortions caused by wireless channels. To improve the accuracy of CNN-based modulation recognition schemes, we propose a signal distortion correction module (CM). The proposed CM is also based on a neural network that can be thought of as an estimator of carrier frequency and phase offset introduced by the channel. The CM output is used to shift the signal frequency and phase before modulation recognition and is differentiable with respect to its weights. This allows the CM to be co-trained end-to-end in tandem with the CNN used for modulation recognition. For supervision, only the modulation scheme label is used and the knowledge of true frequency or phase offset is not required for co-training the combined network (CM+CNN).","","","10.1109/LWC.2018.2855749","Bharti Centre for Communication; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8410801","Cognitive radio;deep learning;modulation recognition;signal distortion","Frequency modulation;Convolution;Signal to noise ratio;Frequency estimation;Time-frequency analysis;Distortion","cognitive radio;convolutional neural nets;modulation;radio spectrum management;signal detection;telecommunication computing;wireless channels","learnable distortion correction module;CNN-based modulation recognition schemes;signal distortion correction module;modulation scheme label;spectrum sensing;cognitive radio;deep learning techniques;convolutional neural networks;wireless channels;carrier frequency estimator","","","16","","","","","IEEE","IEEE Journals"
"Deep3DSaliency: Deep Stereoscopic Video Saliency Detection Model by 3D Convolutional Networks","Y. Fang; G. Ding; J. Li; Z. Fang","School of Information Management, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Management, Jiangxi University of Finance and Economics, Nanchang, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; School of Electronic and Electrical Engineering, Shanghai University of Engineering Science, Shanghai, China","IEEE Transactions on Image Processing","","2019","28","5","2305","2318","Stereoscopic saliency detection plays an important role in various stereoscopic video processing applications. However, conventional stereoscopic video saliency detection methods mainly use independent low-level features instead of extracting them automatically, and thus, they ignore the intrinsic relationship between the spatial and temporal information. In this paper, we propose a novel stereoscopic video saliency detection method based on 3D convolutional neural networks, namely, deep 3D video saliency (Deep3DSaliency). The proposed network consists of two sub-models: spatiotemporal saliency model (STSM) and stereoscopic saliency aware model (SSAM). STSM directly takes three consecutive video frames as the input to extract visual spatiotemporal features, while SSAM attempts to further infer the depth and semantic features from the left and right video frames by shared parameters from STSM. The visual spatiotemporal features from STSM and the depth and semantic features from SSAM are learned by an alternating optimization scheme. Finally, all these saliency-related features are combined together for the final stereoscopic saliency detection via 3D deconvolution. Experimental results show the superior performance of the proposed model over other existing ones in saliency estimation for 3D video sequences.","","","10.1109/TIP.2018.2885229","National Natural Science Foundation of China; Natural Science Foundation of Jiangxi Province; Henry Fok Education Foundation; Beijing Nova Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8561195","Visual attention;stereoscopic video;spatiotemporal saliency;3D convolutional neural networks","Feature extraction;Saliency detection;Three-dimensional displays;Stereo image processing;Visualization;Spatiotemporal phenomena;Video sequences","convolutional neural nets;deconvolution;feature extraction;image sequences;object detection;optimisation;stereo image processing;video signal processing","Deep3DSaliency;3D convolutional networks;3D convolutional neural networks;deep 3D video saliency;STSM;stereoscopic saliency aware model;SSAM;saliency estimation;3D video sequences;stereoscopic video processing;deep stereoscopic video saliency detection model;stereoscopic video saliency detection;visual spatiotemporal feature extraction;spatiotemporal saliency model;3D deconvolution","","3","63","","","","","IEEE","IEEE Journals"
"Toward a Comprehensive Face Detector in the Wild","J. Li; L. Liu; J. Li; J. Feng; S. Yan; T. Sim","School of Computing, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Optical Engineering, Beijing Institute of Technology University, Beijing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","1","104","114","In this paper, we aim to build a comprehensive face detection system which provides a one-stop solution to various practical challenges for face detection in realistic scenarios, e.g., detecting faces from multiple-views, faces with occlusions, exaggerated expressions or blurred faces. Moreover, we introduce an automatic data harvest algorithm to effectively improve the generalization performance of the system even when collecting training faces containing various challenging patterns is difficult. In particular, we introduce three critical components to build the system, i.e., a recently widely used deep convolutional neural network (CNN), a novel blur-aware bi-channel network architecture, and a new self-learning mechanism capable of exploiting video contexts continuously. The aforementioned challenges except for detecting blurred faces can potentially be addressed by the CNN component owing its robustness to local deformation of target faces. The more challenging problem of detecting blurred faces is addressed by the bi-channel architecture component which processes blurred and clear faces adaptively. In addition, to address the difficulties in improving the generalization performance of the learning-based face detection system, we introduce a video-context-based self-learning mechanism into the system, which enables the system to continuously enhance its performance by harvesting faces with challenging training patterns automatically. To exploit video context, the detector is applied to massive unlabeled videos, and challenging faces are captured based on temporal inference. These recaptured faces, generally corresponding to one or multiple challenges mentioned above, are fed into the detection system to further improve its performance. Extensive experiments with the proposed detection system provide new state-of-the-art performance on FDDB data set, PASCAL face data set, AFW data set, and WIDER Face data set.","","","10.1109/TCSVT.2017.2778227","National Research Foundation Singapore; NUS startup; MOE Tier-I; IDS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8122006","Bi-channel architecture;blur-aware;face detection;video context","Face;Face detection;Detectors;Training;Feature extraction;Neural networks;Proposals","face recognition;feedforward neural nets;learning (artificial intelligence);object detection","blurred faces;automatic data harvest algorithm;generalization performance;training faces;self-learning mechanism;video context;target faces;bi-channel architecture component;learning-based face detection system;recaptured faces;PASCAL face data;WIDER Face data;comprehensive face detection system;deep convolutional neural network;training patterns;blur-aware bi-channel network architecture;CNN component","","2","41","","","","","IEEE","IEEE Journals"
"OrthoMaps: an efficient convolutional neural network with orthogonal feature maps for tiny image classification","R. Moradi; R. Berangi; B. Minaei","School of Computer Engineering, Iran University of Science and Technology, Iran; School of Computer Engineering, Iran University of Science and Technology, Iran; School of Computer Engineering, Iran University of Science and Technology, Iran","IET Image Processing","","2019","13","12","2067","2076","In image processing domain of deep learning, the big size and complexity of the visual data require a large number of learnable variables. Subsequently, the training process consumes enormous computation and memory resources. Based on residual modules, the authors developed a new model architecture that has a minimal number of parameters and layers that enabled us to classify tiny images using much less computation and memory costs. Also, the summation of correlations between pairs of feature maps as an additive penalty in the objective function was used. This technique encourages the kernels to be learned in a way that elicit uncorrelated representations from the input images. Also, employing Fractional pooling helped to have deeper networks that consequently resulted in more informative representation. Moreover, employing periodic learning rate curves, multiple machines are trained with a less total cost. In the training phase, a random augmentation to the input data that prevent the model from being overfitted was applied. Applying MNIST and CIFAR-10 datasets to the proposed model resulted in the classification accuracy of 99.72 and 93.98, respectively.","","","10.1049/iet-ipr.2018.6620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870589","","","convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);visual databases","fractional pooling;convolutional neural network;input data;total cost;periodic learning rate curves;informative representation;deeper networks;input images;elicit uncorrelated representations;objective function;additive penalty;memory costs;model architecture;residual modules;memory resources;training process;learnable variables;visual data;deep learning;image processing domain;tiny image classification;orthogonal feature maps;OrthoMaps","","","45","","","","","IET","IET Journals"
"Learning Pose-Aware Models for Pose-Invariant Face Recognition in the Wild","I. Masi; F. Chang; J. Choi; S. Harel; J. Kim; K. Kim; J. Leksut; S. Rawls; Y. Wu; T. Hassner; W. AbdAlmageed; G. Medioni; L. Morency; P. Natarajan; R. Nevatia","University of Southern California, Los Angeles, CA; University of Southern California, Los Angeles, CA; University of Southern California, Los Angeles, CA; Open University of Israel, Ra’anana, Israel; University of Southern California, Los Angeles, CA; University of Southern California, Los Angeles, CA; University of Southern California, Los Angeles, CA; University of Southern California, Los Angeles, CA; University of Southern California, Los Angeles, CA; University of Southern California, Los Angeles, CA; University of Southern California, Los Angeles, CA; University of Southern California, Los Angeles, CA; Carnegie Mellon University, Pittsburgh, PA; University of Southern California, Los Angeles, CA; University of Southern California, Los Angeles, CA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","2","379","393","We propose a method designed to push the frontiers of unconstrained face recognition in the wild with an emphasis on extreme out-of-plane pose variations. Existing methods either expect a single model to learn pose invariance by training on massive amounts of data or else normalize images by aligning faces to a single frontal pose. Contrary to these, our method is designed to explicitly tackle pose variations. Our proposed Pose-Aware Models (PAM) process a face image using several pose-specific, deep convolutional neural networks (CNN). 3D rendering is used to synthesize multiple face poses from input images to both train these models and to provide additional robustness to pose variations at test time. Our paper presents an extensive analysis of the IARPA Janus Benchmark A (IJB-A), evaluating the effects that landmark detection accuracy, CNN layer selection, and pose model selection all have on the performance of the recognition pipeline. It further provides comparative evaluations on IJB-A and the PIPA dataset. These tests show that our approach outperforms existing methods, even surprisingly matching the accuracy of methods that were specifically fine-tuned to the target dataset. Parts of this work previously appeared in [1] and [2] .","","","10.1109/TPAMI.2018.2792452","Office of the Director of National Intelligence; Intelligence Advanced Research Projects Activity; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8255649","Face recognition;CNN;pose-aware","Face;Three-dimensional displays;Benchmark testing;Face recognition;Training;Solid modeling;Rendering (computer graphics)","face recognition;feedforward neural nets;learning (artificial intelligence);pipeline processing;pose estimation;visual databases","unconstrained face recognition;extreme out-of-plane pose variations;normalize images;aligning faces;face image;deep convolutional neural networks;recognition pipeline;CNN layer selection;IJB-A;IARPA Janus Benchmark;pose-invariant face recognition;pose-aware model process","","18","66","","","","","IEEE","IEEE Journals"
"3D-Aided Dual-Agent GANs for Unconstrained Face Recognition","J. Zhao; L. Xiong; J. Li; J. Xing; S. Yan; J. Feng","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Core Technology Group, Learning & Vision, Panasonic R&D Center Singapore, Singapore; School of Computing, National University of Singapore, Singapore; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","10","2380","2394","Synthesizing realistic profile faces is beneficial for more efficiently training deep pose-invariant models for large-scale unconstrained face recognition, by augmenting the number of samples with extreme poses and avoiding costly annotation work. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy betwedistributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces while preserving the identity information during the realism refinement. The dual agents are specially designed for distinguishing real versus fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose, texture as well as identity, and stabilize the training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only achieves outstanding perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A and CFP unconstrained face recognition benchmarks. In addition, the proposed DA-GAN is also a promising new approach for solving generic transfer learning problems more effectively. DA-GAN is the foundation of our winning entry to the NIST IJB-A face recognition competition in which we secured the 1st places on the tracks of verification and identification.","","","10.1109/TPAMI.2018.2858819","China Scholarship Council; National Natural Science Foundation of China; National University of Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417439","Face synthesis;unconstrained face recognition;3D face model;generative adversarial networks","Face;Face recognition;Gallium nitride;Training;Three-dimensional displays;Generators;Solid modeling","face recognition;learning (artificial intelligence);neural nets;pose estimation","real face images;Dual-Agent Generative Adversarial Network model;DA-GAN;face simulator;unlabeled real faces;identity information;realism refinement;dual agents;off-the-shelf 3D face model;profile face images;fully convolutional network;high-resolution images;training process;identity perception loss;adversarial loss;NIST IJB-A face recognition competition;realistic profile faces;large-scale unconstrained face recognition;extreme poses;synthetic faces;deep pose-invariant models;3D-aided dual-agent GANs;boundary equilibrium regularization term;pose perception loss;auto-encoder","","1","57","","","","","IEEE","IEEE Journals"
"Triplet-Based Semantic Relation Learning for Aerial Remote Sensing Image Change Detection","M. Zhang; G. Xu; K. Chen; M. Yan; X. Sun","Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","2","266","270","This letter presents a novel supervised change detection method based on a deep siamese semantic network framework, which is trained by using improved triplet loss function for optical aerial images. The proposed framework can not only extract features directly from image pairs which include multiscale information and are more abstract as well as robust, but also enhance the interclass separability and the intraclass inseparability by learning semantic relation. The feature vectors of the pixels pair with the same label are closer, and at the same time, the feature vectors of the pixels with different labels are farther from each other. Moreover, we use the distance of the feature map to detect the changes on the difference map between the image pair. Binarized change map can be obtained by a simple threshold. Experiments on optical aerial image data set validate that the proposed approach produces comparable, even better results, favorably to the state-of-the-art methods in terms of F-measure.","","","10.1109/LGRS.2018.2869608","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8488487","Change detection;optical aerial images;semantic relation;siamese semantic network;triplet loss function","Feature extraction;Semantics;Tensile stress;Remote sensing;Training;Optical imaging;Optical sensors","feature extraction;geophysical image processing;image classification;image segmentation;learning (artificial intelligence);object detection;remote sensing;semantic networks","image pair;binarized change map;optical aerial image data;triplet-based semantic relation;aerial remote sensing image change detection;supervised change detection method;deep siamese semantic network framework;improved triplet loss function;optical aerial images;multiscale information;interclass separability;feature vectors;pixels pair","","1","15","","","","","IEEE","IEEE Journals"
"Predicting Athlete Ground Reaction Forces and Moments From Spatio-Temporal Driven CNN Models","W. R. Johnson; J. Alderson; D. Lloyd; A. Mian","School of Human Sciences (Exercise and Sport Science), The University of Western Australia, Perth, WA, Australia; School of Human Sciences (Exercise and Sport Science)The University of Western Australia; Menzies Health Institute Queensland; School of Computer Science and Software EngineeringThe University of Western Australia","IEEE Transactions on Biomedical Engineering","","2019","66","3","689","694","The accurate prediction of three-dimensional (3-D) ground reaction forces and moments (GRF/Ms) outside the laboratory setting would represent a watershed for on-field biomechanical analysis. To extricate the biomechanist's reliance on ground embedded force plates, this study sought to improve on an earlier partial least squares (PLS) approach by using deep learning to predict 3-D GRF/Ms from legacy marker based motion capture sidestepping trials, ranking multivariate regression of GRF/Ms from five convolutional neural network (CNN) models. In a possible first for biomechanics, tactical feature engineering techniques were used to compress space-time and facilitate fine-tuning from three pretrained CNNs, from which a model derivative of ImageNet called “CaffeNet” achieved the strongest average correlation to ground truth GRF/Ms r(Fmean) 0.9881 and r(Mmean) 0.9715 (rRMSE 4.31 and 7.04%). These results demonstrate the power of CNN models to facilitate real-world multivariate regression with practical application for spatio-temporal sports analytics.","","","10.1109/TBME.2018.2854632","ARC Discovery; Australian Government Research Training Program Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8408711","Biomechanics;Supervised learning;Image motion analysis;Pattern analysis","Force;Trajectory;Biomechanics;Australia;Biological system modeling;Training","biomechanics;biomedical measurement;convolutional neural nets;feature extraction;gait analysis;image motion analysis;learning (artificial intelligence);medical image processing;physiological models;regression analysis;sport","biomechanist;ground embedded force plates;legacy marker based motion capture sidestepping trials;convolutional neural network models;tactical feature engineering techniques;spatio-temporal sports analytics;athlete ground reaction forces;spatio-temporal driven CNN models;partial least squares approach;ground truth GRF/Ms;PLS approach;multivariate regression;deep learning","","1","49","","","","","IEEE","IEEE Journals"
"Gated Residual Networks With Dilated Convolutions for Monaural Speech Enhancement","K. Tan; J. Chen; D. Wang","Department of Computer Science and Engineering, Ohio State University, Columbus, USA; Department of Computer Science and Engineering, Ohio State University, Columbus, USA; Department of Computer Science and Engineering and the Center for Cognitive and Brain Sciences, Ohio State University, Columbus, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","1","189","198","For supervised speech enhancement, contextual information is important for accurate mask estimation or spectral mapping. However, commonly used deep neural networks (DNNs) are limited in capturing temporal contexts. To leverage long-term contexts for tracking a target speaker, we treat speech enhancement as a sequence-to-sequence mapping, and present a novel convolutional neural network (CNN) architecture for monaural speech enhancement. The key idea is to systematically aggregate contexts through dilated convolutions, which significantly expand receptive fields. The CNN model additionally incorporates gating mechanisms and residual learning. Our experimental results suggest that the proposed model generalizes well to untrained noises and untrained speakers. It consistently outperforms a DNN, a unidirectional long short-term memory (LSTM) model, and a bidirectional LSTM model in terms of objective speech intelligibility and quality metrics. Moreover, the proposed model has far fewer parameters than DNN and LSTM models.","","","10.1109/TASLP.2018.2876171","NIDCD; Ohio Supercomputer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8492428","Dilated convolutions;residual learning;gated linear units;sequence-to-sequence mapping;speech enhancement","Speech enhancement;Noise measurement;Training;Convolution;Signal to noise ratio;Logic gates","feedforward neural nets;generalisation (artificial intelligence);learning (artificial intelligence);recurrent neural nets;speech enhancement;speech intelligibility","quality metrics;bidirectional LSTM model;unidirectional LSTM model;unidirectional long short-term memory;gating mechanisms;systematic context aggregation;target speaker tracking;long-term contexts;deep neural networks;mask estimation;objective speech intelligibility;residual learning;CNN model;convolutional neural network architecture;sequence-to-sequence mapping;temporal contexts;spectral mapping;contextual information;supervised speech enhancement;monaural speech enhancement;dilated convolutions;gated residual networks","","3","50","","","","","IEEE","IEEE Journals"
"Enhancing the Robustness of Neural Collaborative Filtering Systems Under Malicious Attacks","Y. Du; M. Fang; J. Yi; C. Xu; J. Cheng; D. Tao","Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; Tencent AI Lab, Shenzhen, China; JD AI Research, Beijing, China; UBTECH Sydney Artificial Intelligence Centre and the School of Information Technologies, Faculty of Engineering and Information Technologies, University of Sydney, Darlington, NSW, Australia; Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; UBTECH Sydney Artificial Intelligence Centre and the School of Information Technologies, Faculty of Engineering and Information Technologies, University of Sydney, Darlington, NSW, Australia","IEEE Transactions on Multimedia","","2019","21","3","555","565","Recommendation systems have become ubiquitous in online shopping in recent decades due to their power in reducing excessive choices of customers and industries. Recent collaborative filtering methods based on the deep neural network are studied and introduce promising results due to their power in learning hidden representations for users and items. However, it has revealed its vulnerabilities under malicious user attacks. With the knowledge of a collaborative filtering algorithm and its parameters, the performance of this recommendation system can be easily downgraded. Unfortunately, this problem is not addressed well, and the study on defending recommendation systems is insufficient. In this paper, we aim to improve the robustness of recommendation systems based on two concepts - stage-wise hints training and randomness. To protect a target model, we introduce noise layers in the training of a target model to increase its resistance to adversarial perturbations. To reduce the noise layers' influence on model performance, we introduce intermediate layer outputs as hints from a teacher model to regularize the intermediate layers of a student target model. We consider white box attacks under which attackers have the knowledge of the target model. The generalizability and robustness properties of our method have been analytically inspected in experiments and discussions, and the computational cost is comparable to training a standard neural network-based collaborative filtering model. Through our investigation, the proposed defensive method can reduce the success rate of malicious user attacks and keep the prediction accuracy comparable to standard neural recommendation systems.","","","10.1109/TMM.2018.2887018","Australian Research Council; Shenzhen Key Laboratory; National Natural Science Foundation of China; Guangdong Technology; Shenzhen Technology Project; CAS Key Technology Talent Program, Shenzhen Engineering Laboratory for 3D Content Generating Technologies; Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576563","Recommendation systems;adversarial learning;collaborative filtering;malicious attacks","Collaboration;Training;Neural networks;Perturbation methods;Robustness;Standards;Information technology","collaborative filtering;learning (artificial intelligence);neural nets;recommender systems;security of data","neural collaborative filtering systems;deep neural network;malicious user attacks;teacher model;student target model;white box attacks;standard neural recommendation systems;learning hidden representations","","","59","","","","","IEEE","IEEE Journals"
"Exploiting Mid-Level Semantics for Large-Scale Complex Video Classification","J. Zhang; K. Mei; Y. Zheng; J. Fan","Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; School of Cyber Engineering, Xidian University, Xi’an, China; University of North Carolina at Charlotte, Charlotte, NC, USA","IEEE Transactions on Multimedia","","2019","21","10","2518","2530","As the amount of available video data has grown substantially, automatic video classification has become an urgent yet challenging task. Most video classification methods focus on acquiring discriminative spacial visual features and motion patterns for video representation, especially deep learning methods, which have achieved very good results on action recognition problems. However, the performance of most of these methods drastically degenerates for more generic video classification tasks where the video contents are much more complex. Thus, in this paper, the mid-level semantics of videos are exploited to bridge the semantic gap between low-level features and high-level video semantics. Inspired by the term ``frequency-inverse document frequency'', a word weighting method for the problem of text classification is introduced to the video domain. The visual objects in videos are regarded as the words in texts, and two new weighting methods are proposed to encode videos by weighting visual objects according to the characteristics of videos. In addition, the semantic similarities between video categories and visual objects are introduced from the text domain as privileged information to facilitate classifier training on the obtained semantic representations of videos. The proposed semantic encoding method (semantic stream) is then fused with the popular two-stream CNN model for the final classification results. Experiments are conducted on two large-scale complex video datasets, CCV and ActivityNet. The experimental results validate the effectiveness of the proposed methods.","","","10.1109/TMM.2019.2907453","National Key Research and Development Plan; Guangdong Science and Technology Project; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673926","Mid-level semantics;word weighting methods;semantic similarities;learning using privileged information;large-scale video classification","Semantics;Visualization;Task analysis;Streaming media;Training;Frequency-domain analysis;Object detection","computer vision;feature extraction;image classification;image motion analysis;image representation;learning (artificial intelligence);object detection;text detection;video coding","mid-level semantics;large-scale complex video classification;available video data;automatic video classification;video classification methods focus;discriminative spacial visual features;motion patterns;video representation;deep learning methods;generic video classification tasks;video contents;low-level features;high-level video semantics;word weighting method;text classification;video domain;visual objects;weighting methods;video categories;semantic representations;semantic encoding method;final classification results;large-scale complex video datasets","","","64","Traditional","","","","IEEE","IEEE Journals"
"Weakly Labelled AudioSet Tagging With Attention Neural Networks","Q. Kong; C. Yu; Y. Xu; T. Iqbal; W. Wang; M. D. Plumbley","Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; University of Stuttgart, 70174 Stuttgart, Germany; Tencent AI Lab, Bellevue, WA, USA; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","11","1791","1802","Audio tagging is the task of predicting the presence or absence of sound classes within an audio clip. Previous work in audio tagging focused on relatively small datasets limited to recognizing a small number of sound classes. We investigate audio tagging on AudioSet, which is a dataset consisting of over 2 million audio clips and 527 classes. AudioSet is weakly labelled, in that only the presence or absence of sound classes is known for each clip, whereas the onset and offset times are unknown. To address the weakly labelled audio tagging problem, we propose attention neural networks as a way to attend the most salient parts of an audio clip. We bridge the connection between attention neural networks and multiple instance learning (MIL) methods, and propose decision-level and feature-level attention neural networks for audio tagging. We investigate attention neural networks modeled by different functions, depths, and widths. Experiments on AudioSet show that the feature-level attention neural network achieves a state-of-the-art mean average precision of 0.369, outperforming the best MIL method of 0.317 and Google's deep neural network baseline of 0.314. In addition, we discover that the audio tagging performance on AudioSet-embedding features has a weak correlation with the number of training samples and the quality of labels of each sound class.","","","10.1109/TASLP.2019.2930913","Engineering and Physical Sciences Research Council; Research Scholarship from the China Scholarship Council; Studentship; EPSRC Doctoral Training Partnership; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8777125","Audio tagging;AudioSet;attention neural network;weakly labelled data;multiple instance learning","Tagging;Neural networks;Task analysis;Training;Feature extraction;Speech processing;Spectrogram","audio signal processing;learning (artificial intelligence);neural nets;signal classification","sound class;weakly labelled audio tagging problem;attention neural networks;audio clip;feature-level attention neural network;Google's deep neural network baseline;audio tagging performance;weakly labelled audioset tagging;sound classes;multiple instance learning;audioset-embedding features","","","59","Traditional","","","","IEEE","IEEE Journals"
"I Can See Your Brain: Investigating Home-Use Electroencephalography System Security","Y. Xiao; Y. Jia; X. Cheng; J. Yu; Z. Liang; Z. Tian","Department of Computer Science, George Washington University, Washington, DC, USA; Department of Computer Science, George Washington University, Washington, DC, USA; Department of Computer Science, George Washington University, Washington, DC, USA; School of Computer Science and Technology, Qilu University of Technology (Shandong Academy of Sciences), Jinan, Shandong, China; School of Computing, National University of Singapore, Singapore; Department of Computer Science, George Mason University, Fairfax, VA, USA","IEEE Internet of Things Journal","","2019","6","4","6681","6691","Health-related Internet of Things (IoT) devices are becoming more popular in recent years. On the one hand, users can access information of their health conditions more conveniently; on the other hand, they are exposed to new security risks. In this paper, we presented, to the best of our knowledge, the first in-depth security analysis on home-use electroencephalography (EEG) IoT devices. Our key contributions are twofold. First, we reverse-engineered the home-use EEG system framework via which we identified the design and implementation flaws. By exploiting these flaws, we developed two sets of novel easy-to-exploit PoC attacks, which consist of four remote attacks and one proximate attack. In a remote attack, an attacker can steal a user's brain wave data through a carefully crafted program while in the proximate attack, the attacker can steal a victim's brain wave data over-the-air without accessing the victim's device on any sense when he is close to the victim. As a result, all the 156 brain-computer interface (BCI) apps in the NeuroSky App store are vulnerable to the proximate attack. We also discovered that all the 31 free apps in the NeuroSky App store are vulnerable to at least one remote attack. Second, we proposed a novel deep learning model of a joint recurrent convolutional neural network (RCNN) to infer a user's activities based on the reduced-featured EEG data stolen from the home-use EEG IoT devices, and our evaluation over the real-world EEG data indicates that the inference accuracy of the proposed RCNN is can reach 70.55%.","","","10.1109/JIOT.2019.2910115","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8685117","Electroencephalography;inference algorithms;Internet of Things (IoT);machine learning;security","Electroencephalography;Security;Frequency measurement;Internet of Things;Brain modeling;Distance measurement","brain-computer interfaces;convolutional neural nets;electroencephalography;Internet of Things;learning (artificial intelligence);medical information systems;medical signal processing;recurrent neural nets;security of data;signal classification","remote attack;home-use EEG IoT devices;health conditions;security risks;in-depth security analysis;home-use electroencephalography IoT devices;home-use EEG system framework;PoC attacks;home-use electroencephalography system security;health-related Internet of Things;user brain wave data;brain-computer interface apps;NeuroSky app store;deep learning model;recurrent convolutional neural network;reduced-featured EEG data","","1","49","","","","","IEEE","IEEE Journals"
"An Accurate and Robust Approach of Device-Free Localization With Convolutional Autoencoder","L. Zhao; H. Huang; X. Li; S. Ding; H. Zhao; Z. Han","School of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan; School of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan; School of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan; School of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan; School of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan; School of Computer Science and Engineering, University of Aizu, Aizuwakamatsu, Japan","IEEE Internet of Things Journal","","2019","6","3","5825","5840","Device-free localization (DFL), as an emerging technology that locates targets without any attached devices via wireless sensor networks, has spawned extensive applications in the Internet of Things (IoT) field. For DFL, a key problem is how to extract significant features to characterize raw signals with different patterns associated with different locations. To address this problem, in this paper, the DFL problem is formulated as an image classification problem. Moreover, we design a three-layer convolutional autoencoder (CAE) neural network to perform unsupervised feature extraction from raw signals followed by supervised fine-tuning for classification. The CAE combines the advantages of a convolutional neural network (CNN) and a deep autoencoder (AE) in the feature learning and signals reconstruction, which is expected to achieve good performance for DFL. The experimental results show that the proposed approach can achieve a high localization accuracy rate of 100% for a reasonable grid size on the raw real-world data, i.e., the collected raw data without added Gaussian noise, and is robust to noisy data with a signal-to-noise ratio greater than -5 dB. Additionally, its time cost for the classification of a single activity is 4 ms, which is fast enough for the IoT applications. The proposed approach outperforms the deep CNN and AE in terms of localization accuracy and robust ability against noise.","","","10.1109/JIOT.2019.2907580","Ministry of Education, Culture, Sports, Science and Technology; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8674463","Classification;convolution autoencoder (CAE);deep learning;device-free localization (DFL);image;wireless sensor network (WSN)","Wireless sensor networks;Wireless communication;Communication system security;Internet of Things;Convolution;Feature extraction;Neural networks","convolutional neural nets;feature extraction;image classification;image coding;image reconstruction;Internet of Things;learning (artificial intelligence)","device-free localization;wireless sensor networks;DFL problem;image classification problem;three-layer convolutional autoencoder neural network;unsupervised feature extraction;supervised fine-tuning;convolutional neural network;signal-to-noise ratio;IoT applications;Internet of Things;CAE neural network;signal reconstruction;CNN;Gaussian noise;time 4.0 ms","","1","67","","","","","IEEE","IEEE Journals"
"Attitude Jitter Compensation for Remote Sensing Images Using Convolutional Neural Network","Z. Zhaoxiang; A. Iwasaki; G. Xu","Department of Aerospace Engineering, Harbin Institute of Technology, Harbin, China; Department of Aerospace Engineering, The University of Tokyo, Tokyo, Japan; Department of Aerospace Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","9","1358","1362","Attitude jitter of satellites and unmanned aerial vehicle (UAV) platforms is a problem that degenerates the imaging quality in high-resolution remote sensing. This letter proposes a deep learning architecture that automatically learns essential scene features from a single image to estimate the attitude jitter, which is used to compensate deformed images. The proposed methodology consists of a convolutional neural network and a jitter compensation model. The neural network analyzes the deformed images and generates the attitude jitter vectors in two directions, which are utilized to correct the images through interpolation and resampling. The PatternNet and the small UAV data sets are introduced to train the neural network and to validate its effectiveness and accuracy. The compensation results on distorted remote sensing images obtained by satellites and UAVs reveal that the image distortion due to attitude jitter is clearly reduced and that the geometric quality is effectively improved. Compared to the existing methods that primarily rely on sensor data or parallax observation, no auxiliary information is required in our framework.","","","10.1109/LGRS.2019.2897710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8657348","Convolutional neural network (CNN);high-resolution (HR) remote sensing;jitter compensation","Jitter;Remote sensing;Cameras;Training;Satellites;Image restoration","autonomous aerial vehicles;backpropagation;convolutional neural nets;geophysical image processing;image matching;jitter;learning (artificial intelligence);object detection;remote sensing;remotely operated vehicles;vectors","deformed images;convolutional neural network;jitter compensation model;neural network analyzes;attitude jitter vectors;UAV data sets;compensation results;distorted remote sensing images;satellites;image distortion;attitude jitter compensation;unmanned aerial vehicle;imaging quality;high-resolution remote sensing;deep learning architecture;automatically learns essential scene features;single image","","","19","","","","","IEEE","IEEE Journals"
"Emotion-Aware Multimedia Systems Security","Y. Zhang; Y. Qian; D. Wu; M. S. Hossain; A. Ghoneim; M. Chen","School of Information and Safety Engineering, Zhongnan University of Economics and Law, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China; Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Mathematics and Computer Science, Faculty of Science, Menoufia University, Shebin, El-Koom, Egypt; Wuhan National Laboratory for Optoelectronics, Wuhan, China","IEEE Transactions on Multimedia","","2019","21","3","617","624","The interactive robot is expected to support emotion analysis and utilize the deep learning and machine learning to provide users with continuous emotional care. However, it is a great challenge to securely acquire sufficient data for emotion analysis such that the privacy of emotional data is adequately protected. To address the security issue, this paper proposes a security policy based on identity authentication and access control to ensure the security certificate through an interactive robot or edge devices while the access control of private data stored in the edge cloud is adequately protected. Specifically, this paper adopts a polynomial-based access control policy and designs a secure and effective access control scheme. At the same time, this paper puts forward an identity authentication mechanism in view of edge cloud systems, which can reduce the computational overhead and authentication delay in a collaborative authentication of multiple edge clouds. The effectiveness of the proposed access control policy and identity authentication mechanism is verified by an actual testbed platform.","","","10.1109/TMM.2018.2882744","Deanship of Scientific Research at King Saud University, Riyadh, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8542709","Security analysis;Access control;emotion interaction;identity authentication;social robot","Access control;Authentication;Robot sensing systems;Cloud computing;Multimedia systems","authorisation;cloud computing;data privacy;emotion recognition;learning (artificial intelligence);multimedia systems","emotion-aware multimedia systems security;interactive robot;emotion analysis;deep learning;machine learning;continuous emotional care;emotional data;security policy;security certificate;edge devices;private data;polynomial-based access control policy;secure access control scheme;identity authentication mechanism;edge cloud systems;computational overhead;authentication delay;collaborative authentication;multiple edge clouds","","3","29","","","","","IEEE","IEEE Journals"
"MR Image Super-Resolution via Wide Residual Networks With Fixed Skip Connection","J. Shi; Z. Li; S. Ying; C. Wang; Q. Liu; Q. Zhang; P. Yan","Shanghai University, Shanghai, China; Shanghai University, Shanghai, China; Department of Mathematics, School of Science, Shanghai University, Shanghai, China; Shanghai University, Shanghai, China; Shanghai University, Shanghai, China; Shanghai University, Shanghai, China; Biomedical Engineering Department at Rensselaer Polytechnic Institute, Troy, NY, USA","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","1129","1140","Spatial resolution is a critical imaging parameter in magnetic resonance imaging. The image super-resolution (SR) is an effective and cost efficient alternative technique to improve the spatial resolution of MR images. Over the past several years, the convolutional neural networks (CNN)-based SR methods have achieved state-of-the-art performance. However, CNNs with very deep network structures usually suffer from the problems of degradation and diminishing feature reuse, which add difficulty to network training and degenerate the transmission capability of details for SR. To address these problems, in this work, a progressive wide residual network with a fixed skip connection (named FSCWRN) based SR algorithm is proposed to reconstruct MR images, which combines the global residual learning and the shallow network based local residual learning. The strategy of progressive wide networks is adopted to replace deeper networks, which can partially relax the above-mentioned problems, while a fixed skip connection helps provide rich local details at high frequencies from a fixed shallow layer network to subsequent networks. The experimental results on one simulated MR image database and three real MR image databases show the effectiveness of the proposed FSCWRN SR algorithm, which achieves improved reconstruction performance compared with other algorithms.","","","10.1109/JBHI.2018.2843819","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8371605","Magnetic resonance imaging;Super-resolution;Wide residual networks;Fixed skip connection","Image reconstruction;Spatial resolution;Magnetic resonance imaging;Feature extraction;Training;Power capacitors","biomedical MRI;convolutional neural nets;image reconstruction;image resolution;learning (artificial intelligence);medical image processing","spatial resolution;critical imaging parameter;magnetic resonance imaging;convolutional neural networks-based SR methods;deep network structures;network training;progressive wide residual network;global residual learning;fixed shallow layer network;subsequent networks;simulated MR image database;FSCWRN SR algorithm;MR image superresolution;transmission capability;fixed skip connection based SR algorithm;shallow network based local residual learning;real MR image databases;reconstruction performance","","4","48","","","","","IEEE","IEEE Journals"
"Context-Aware Depth and Pose Estimation for Bronchoscopic Navigation","M. Shen; Y. Gu; N. Liu; G. Yang","Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.; Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.; Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.; Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.","IEEE Robotics and Automation Letters","","2019","4","2","732","739","Endobronchial intervention is increasingly used as a minimally invasive means of lung intervention. Vision-based localization approaches are often sensitive to image artifacts in bronchoscopic videos. In this letter, a robust navigation system based on a context-aware depth recovery approach for monocular video images is presented. To handle the artifacts, a conditional generative adversarial learning framework is proposed for reliable depth recovery. The accuracy of depth estimation and camera localization is validated on an in vivo dataset. Both quantitative and qualitative results demonstrate that the depth recovered with the proposed method preserves better structural information of airway lumens in the presence of image artifacts, and the improved camera localization accuracy demonstrates its clinical potential for bronchoscopic navigation.","","","10.1109/LRA.2019.2893419","Engineering and Physical Sciences Research Council; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613897","Visual learning;visual-based navigation;computer vision for medical robotics;deep learning in robotics and automation","Videos;Cameras;Navigation;Estimation;Computed tomography;Atmospheric modeling;In vivo","cameras;endoscopes;lung;medical image processing;pose estimation;video signal processing","monocular video images;conditional generative adversarial learning framework;reliable depth recovery;depth estimation;context-aware depth recovery approach;robust navigation system;bronchoscopic videos;image artifacts;vision-based localization approaches;lung intervention;minimally invasive means;endobronchial intervention;bronchoscopic navigation;pose estimation;airway lumens;structural information;in vivo dataset;camera localization accuracy","","1","31","","","","","IEEE","IEEE Journals"
"Deep Progressive Hashing for Image Retrieval","J. Bai; B. Ni; M. Wang; Z. Li; S. Cheng; X. Yang; C. Hu; W. Gao","Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, University of California, San Diego, La Jolla, CA, USA; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Technology, Peking University, Beijing, China","IEEE Transactions on Multimedia","","2019","21","12","3178","3193","Hashing is a widely adopted method based on an approximate nearest neighbor search and is used in large-scale image retrieval tasks. Conventional learning-based hashing algorithms employ end-to-end representation learning, which is a one-off technique. Because of the tradeoff between efficiency and performance, conventional learning-based hashing methods must sacrifice code length to improve performance, which increases their computational complexity. To improve the efficiency of binary codes, motivated by the “nonsalient-to-salient” attention scheme of humans, we propose a recursive hashing mechanism that maps progressively expanded salient regions to a series of binary codes. These salient regions are generated by a conventional saliency model based on bottom-up saliency-driven attention and a semantic-guided saliency model based on top-down task-driven attention. After obtaining a series of salient regions, we perform long-range temporal modeling of salient regions using a graph-based recurrent deep network to obtain more refined representative features. The later output nodes inherit aggregated information from all previous nodes and extract discriminative features from more salient regions. Therefore, this network possesses more significant information and satisfactory scalability. The proposed recursive hashing neural network, optimized by a triplet ranking loss, is end-to-end trainable. Extensive experimental results from several image retrieval benchmarks show the scalability of our method and demonstrate its strong performance compared with state-of-the-art methods.","","","10.1109/TMM.2019.2920601","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8728056","Saliency;image retrieval;deep hashing;graph-based recurrent neural networks","Binary codes;Semantics;Image retrieval;Saliency detection;Feature extraction;Recurrent neural networks","","","","","80","IEEE","","","","IEEE","IEEE Journals"
"GANobfuscator: Mitigating Information Leakage Under GAN via Differential Privacy","C. Xu; J. Ren; D. Zhang; Y. Zhang; Z. Qin; K. Ren","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; Institute of Cyberspace Research, Zhejiang University, Hangzhou, China; Institute of Cyberspace Research, Zhejiang University, Hangzhou, China","IEEE Transactions on Information Forensics and Security","","2019","14","9","2358","2371","By learning generative models of semantic-rich data distributions from samples, generative adversarial network (GAN) has recently attracted intensive research interests due to its excellent empirical performance as a generative model. The model is used to estimate the underlying distribution of a dataset and randomly generate realistic samples according to their estimated distribution. However, GANs can easily remember training samples due to the high model complexity of deep networks. When GANs are applied to private or sensitive data, the concentration of distribution may divulge some critical information. It consequently requires new technological advances to mitigate the information leakage under GANs. To address this issue, we propose GANobfuscator, a differentially private GAN, which can achieve differential privacy under GANs by adding carefully designed noise to gradients during the learning procedure. With GANobfuscator, analysts are able to generate an unlimited amount of synthetic data for arbitrary analysis tasks without disclosing the privacy of training data. Moreover, we theoretically prove that GANobfuscator can provide strict privacy guarantee with differential privacy. In addition, we develop a gradient-pruning strategy for GANobfuscator to improve the scalability and stability of data training. Through extensive experimental evaluation on benchmark datasets, we demonstrate that GANobfuscator can produce high-quality generated data and retain desirable utility under practical privacy budgets.","","","10.1109/TIFS.2019.2897874","Young and Middle-Aged Scientific Research Project in the Department of Education of Fujian; Scientific Research Fund of the Hunan Education Department; National Natural Science Foundation of China; 111 Project; Central South University; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636556","Information leakage;generative adversarial network;deep learning;differential privacy","Differential privacy;Gallium nitride;Privacy;Training;Generative adversarial networks;Data models","data privacy;learning (artificial intelligence);neural nets","GANobfuscator;information leakage;differential privacy;semantic-rich data distributions;generative adversarial network;realistic samples;private data;sensitive data;differentially private GAN;data training;high-quality generated data;model complexity","","5","44","","","","","IEEE","IEEE Journals"
"3-D Convolutional Neural Networks for Automatic Detection of Pulmonary Nodules in Chest CT","A. Pezeshk; S. Hamidian; N. Petrick; B. Sahiner","Division of Imaging, Diagnostics, and Software Reliability, Office of Science and Engineering Laboratories, Center for Devices and Radiological Health, U.S. Food and Drug Administration, Silver Spring, MD, USA; George Washington University, Washington, DC, USA; Division of Imaging, Diagnostics, and Software Reliability, Office of Science and Engineering Laboratories, Center for Devices and Radiological Health, U.S. Food and Drug Administration, Silver Spring, MD, USA; Division of Imaging, Diagnostics, and Software Reliability, Office of Science and Engineering Laboratories, Center for Devices and Radiological Health, U.S. Food and Drug Administration, Silver Spring, MD, USA","IEEE Journal of Biomedical and Health Informatics","","2019","23","5","2080","2090","Deep two-dimensional (2-D) convolutional neural networks (CNNs) have been remarkably successful in producing record-breaking results in a variety of computer vision tasks. It is possible to extend CNNs to three dimensions using 3-D kernels to make them suitable for volumetric medical imaging data such as CT or MRI, but this increases the processing time as well as the required number of training samples (due to the higher number of parameters that need to be learned). In this paper, we address both of these issues for a 3-D CNN implementation through the development of a two-stage computer-aided detection system for automatic detection of pulmonary nodules. The first stage consists of a 3-D fully convolutional network for fast screening and generation of candidate suspicious regions. The second stage consists of an ensemble of 3-D CNNs trained using extensive transformations applied to both the positive and negative patches to augment the training set. To enable the second stage classifiers to learn differently, they are trained on false positive patches obtained from the screening model using different thresholds on their associated scores as well as different augmentation types. The networks in the second stage are averaged together to produce the final classification score for each candidate patch. Using this procedure, our overall nodule detection system called DeepMed is fast and can achieve 91% sensitivity at 2 false positives per scan on cases from the LIDC dataset.","","","10.1109/JBHI.2018.2879449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8528826","Deep learning;fully convolutional networks;computer-aided diagnosis;chest CT","Three-dimensional displays;Training;Computed tomography;Two dimensional displays;Lung;Detection algorithms;Informatics","computer vision;computerised tomography;convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);lung;medical image processing","3D convolutional neural networks;automatic pulmonary nodule detection;computer vision;3D CNN training;computer-aided detection;chest CT;DeepMed nodule detection system;volumetric medical imaging data","","1","41","Traditional","","","","IEEE","IEEE Journals"
"On the Duality Between Belief Networks and Feed-Forward Neural Networks","P. M. Baggenstoss","Kommunikationssysteme, Fraunhofer FKIE, Wachtberg, Germany","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","1","190","200","This paper addresses the duality between the deterministic feed-forward neural networks (FF-NNs) and linear Bayesian networks (LBNs), which are the generative stochastic models representing probability distributions over the visible data based on a linear function of a set of latent (hidden) variables. The maximum entropy principle is used to define a unique generative model corresponding to each FF-NN, called projected belief network (PBN). The FF-NN exactly recovers the hidden variables of the dual PBN. The large-$N$ asymptotic approximation to the PBN has the familiar structure of an LBN, with the addition of an invertible nonlinear transformation operating on the latent variables. It is shown that the exact nature of the PBN depends on the range of the input (visible) data details for the three cases of input data range are provided. The likelihood function of the PBN is straightforward to calculate, allowing it to be used as a generative classifier. An example is provided in which a generative classifier based on the PBN has comparable performance to a deep belief network in classifying handwritten characters. In addition, several examples are provided that demonstrate the duality relationship, for example, by training networks from either side of the duality.","","","10.1109/TNNLS.2018.2836662","Fraunhofer FKIE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8374069","Information entropy;multilayer perceptron;neural networks","Bayes methods;Entropy;Neural networks;Probability density function;Training;Data models;Manifolds","Bayes methods;belief networks;entropy;feedforward neural nets;handwritten character recognition;learning (artificial intelligence);maximum entropy methods;pattern classification;stochastic processes","feed-forward neural networks;FF-NN;linear Bayesian networks;generative stochastic models;probability distributions;visible data;linear function;latent variables;maximum entropy principle;unique generative model;hidden variables;dual PBN;invertible nonlinear transformation;exact nature;input data details;input data range;likelihood function;generative classifier;deep belief network;duality relationship;training networks;LBN","","2","18","","","","","IEEE","IEEE Journals"
"Semantically Modeling of Object and Context for Categorization","C. Zhang; J. Cheng; Q. Tian","Research Center for Brain-Inspired Intelligence, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Department of Computer Sciences, The University of Texas at San Antonio, San Antonio, TX, USA","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","4","1013","1024","Object-centric-based categorization methods have been proven more effective than hard partitions of images (e.g., spatial pyramid matching). However, how to determine the locations of objects is still an open problem. Besides, modeling of context areas is often mixed with the background. Moreover, the semantic information is often ignored by these methods that only use visual representations for classification. In this paper, we propose an object categorization method by semantically modeling the object and context information (SOC). We first select a number of candidate regions with high confidence scores and semantically represent these regions by measuring correlations of each region with prelearned classifiers (e.g., local feature-based classifiers and deep convolutional-neural-network-based classifiers). These regions are clustered for object selections. The other selected areas are then viewed as context areas. We treat other areas beyond the object and context areas within one image as the background. The visually and semantically represented objects and contexts are then used along with the background area for object representations and categorizations. Experimental results on several public data sets well demonstrate the effectiveness of the proposed object categorization method by semantically modeling the object and context information.","","","10.1109/TNNLS.2018.2856096","Beijing Municipal Commission of Education; National Natural Science Foundation of China; ARO; Faculty Research Gift Awards through the NEC Laboratories of America and Blippar; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8428657","Context modeling;object categorization;object modeling;semantic representation","Semantics;Context modeling;Visualization;Proposals;Correlation;Training;Neural networks","convolutional neural nets;feature extraction;image classification;image matching;image representation;learning (artificial intelligence)","object-centric-based categorization methods;spatial pyramid matching;context areas;semantic information;object categorization method;context information;candidate regions;local feature-based classifiers;object selections;semantically represented objects;background area;object representations;deep convolutional-neural-network-based classifiers","","2","76","","","","","IEEE","IEEE Journals"
"Unified Confidence Estimation Networks for Robust Stereo Matching","S. Kim; D. Min; S. Kim; K. Sohn","School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; Department of Computer Science and Engineering, Ewha Womans University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Image Processing","","2019","28","3","1299","1313","We present a deep architecture that estimates a stereo confidence, which is essential for improving the accuracy of stereo matching algorithms. In contrast to existing methods based on deep convolutional neural networks (CNNs) that rely on only one of the matching cost volume or estimated disparity map, our network estimates the stereo confidence by using the two heterogeneous inputs simultaneously. Specifically, the matching probability volume is first computed from the matching cost volume with residual networks and a pooling module in a manner that yields greater robustness. The confidence is then estimated through a unified deep network that combines confidence features extracted both from the matching probability volume and its corresponding disparity. In addition, our method extracts the confidence features of the disparity map by applying multiple convolutional filters with varying sizes to an input disparity map. To learn our networks in a semi-supervised manner, we propose a novel loss function that use confident points to compute the image reconstruction loss. To validate the effectiveness of our method in a disparity post-processing step, we employ three post-processing approaches; cost modulation, ground control points-based propagation, and aggregated ground control points-based propagation. Experimental results demonstrate that our method outperforms state-of-the-art confidence estimation methods on various benchmarks.","","","10.1109/TIP.2018.2878325","Next-Generation Information Computing Development Program; National Research Foundation of Korea; Ministry of Science and ICT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8510870","Stereo confidence;confidence learning;matching probability volume;confidence estimation network","Estimation;Feature extraction;Reliability;Benchmark testing;Task analysis;Volume measurement;Training","feature extraction;image matching;image reconstruction;neural nets;probability;stereo image processing","unified deep network;confidence features;matching probability volume;multiple convolutional filters;input disparity map;confident points;disparity post-processing step;cost modulation;aggregated ground control points-based propagation;unified confidence estimation networks;deep architecture;stereo confidence;stereo matching algorithms;deep convolutional neural networks;matching cost volume;estimated disparity map;heterogeneous inputs;residual networks;greater robustness;robust stereo matching;image reconstruction loss","","","50","","","","","IEEE","IEEE Journals"
"Deep Universal Generative Adversarial Compression Artifact Removal","L. Galteri; L. Seidenari; M. Bertini; A. D. Bimbo","Media Integration and Communication Center (MICC), Università degli Studi di Firenze, Firenze, Italy; Media Integration and Communication Center (MICC), Università degli Studi di Firenze, Firenze, Italy; Media Integration and Communication Center (MICC), Università degli Studi di Firenze, Firenze, Italy; Media Integration and Communication Center (MICC), Università degli Studi di Firenze, Firenze, Italy","IEEE Transactions on Multimedia","","2019","21","8","2131","2145","Image compression is a need that arises in many circumstances. Unfortunately, whenever a lossy compression algorithm is used, artifacts will manifest. Image artifacts, caused by compression tend to eliminate higher frequency details and, in certain cases, may add noise or small image structures. There are two main drawbacks of this phenomenon. First, images appear much less pleasant to the human eye. Second, computer vision algorithms, such as object detectors, may be hindered and their performance reduced. Removing such artifacts means recovering the original image from a perturbed version of it. This means that one ideally should invert the compression process through a complicated nonlinear image transformation. We propose an image transformation approach based on a feedforward fully convolutional residual network model. We show that this model can be optimized either traditionally, directly optimizing an image similarity loss (SSIM), or using a generative adversarial approach (GAN). Our GAN is able to produce images with more photorealistic details than SSIM-based networks. We describe a novel training procedure based on subpatches and devise a novel testing protocol to evaluate restored images quantitatively. We show that our approach can be used as a preprocessing step for different computer vision tasks in case images are degraded by compression to a point that state-of-the art algorithms fail. In this case, our GAN-based approach obtains better performance than MSE or SSIM trained networks. Different from previously proposed approaches, we are able to remove artifacts generated at any QF by inferring the image quality directly from data.","","","10.1109/TMM.2019.2895280","Nvidia; Titan X Pascal GPUs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625533","Image compression;image restoration;object detection","Image coding;Image restoration;Image reconstruction;Task analysis;Transform coding;Image resolution;Discrete cosine transforms","computer vision;convolutional neural nets;data compression;image coding;image restoration;learning (artificial intelligence);optimisation;recurrent neural nets","feedforward fully convolutional residual network model;image similarity loss;photorealistic details;SSIM-based networks;restored images;GAN-based approach;image quality;deep universal generative adversarial compression artifact removal;image compression;lossy compression algorithm;image artifacts;image structures;human eye;computer vision algorithms;nonlinear image transformation","","","54","Traditional","","","","IEEE","IEEE Journals"
"Long-Short-Term Features for Dynamic Scene Classification","Y. Huang; X. Cao; Q. Wang; B. Zhang; X. Zhen; X. Li","School of Electronics and Information Engineering, Beihang University, Beijing, China; School of Electronics and Information Engineering, Beihang University, Beijing, China; School of Computer Science and Center for OPTical IMagery Analysis and Learning, Northwestern Polytechnical University, Xi’an, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Electronics and Information Engineering, Beihang University, Beijing, China; Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","4","1038","1047","Dynamic scene classification has been extensively studied in computer vision due to its widespread applications. The key to dynamic scene classification lies in jointly characterizing spatial appearance and temporal dynamics to achieve informative representation, which remains an outstanding task in the literature. In this paper, we propose a unified framework to extract spatial and temporal features for dynamic scene representation. More specifically, we deploy two variants of deep convolutional neural networks to encode spatial appearance and short-term dynamics into short-term deep features (STDF). Based on STDF, we propose using the autoregressive moving average model to extract long-term frequency features (LTFF). By combining STDF and LTFF, we establish the long-short-term feature (LSTF) representations of dynamic scenes. The LSTF characterizes both spatial and temporal patterns of dynamic scenes for comprehensive and information representation that enables more accurate classification. Extensive experiments on three-dynamic scene classification benchmarks have shown that the proposed LSTF achieves high performance and substantially surpasses the state-of-the-art methods.","","","10.1109/TCSVT.2018.2823360","National Basic Research Program of China (973 Program); National Science Fund for Distinguished Young Scholars; National Natural Science Foundation of China; National Natural Science Foundation of China; Key Research Program of Frontier Sciences, CAS; National Key R&D Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8331876","Dynamic scene classification;long-short term feature;long term frequency feature","Feature extraction;Dynamics;Vehicle dynamics;Task analysis;Three-dimensional displays;Convolutional neural networks;Integrated optics","autoregressive moving average processes;computer vision;convolutional neural nets;feature extraction;image classification;image representation;learning (artificial intelligence)","spatial appearance;temporal dynamics;spatial features;temporal features;dynamic scene representation;short-term dynamics;short-term deep features;STDF;long-term frequency features;long-short-term feature representations;dynamic scenes;spatial patterns;temporal patterns;three-dynamic scene classification benchmarks;LTFF;LSTF","","2","50","","","","","IEEE","IEEE Journals"
"Heterogeneous Face Recognition Using Domain Specific Units","T. de Freitas Pereira; A. Anjos; S. Marcel","Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland","IEEE Transactions on Information Forensics and Security","","2019","14","7","1803","1816","The task of Heterogeneous Face Recognition consists in matching face images that are sensed in different domains, such as sketches to photographs (visual spectra images), and thermal images to photographs or near-infrared images to photographs. In this paper, we suggest that the high-level features of Deep Convolutional Neural Networks trained in visual spectra images are potentially domain independent and can be used to encode faces sensed in different image domains. A generic framework for Heterogeneous Face Recognition is proposed by adapting Deep Convolutional Neural Networks low-level features in, so-called, Domain Specific Units. The adaptation using the Domain Specific Units allows the learning of shallow feature detectors specific for each new image domain. Furthermore, it handles its transformation to a generic face space shared between all image domains. Experiments carried out with four different face databases covering three different image domains show substantial improvements, in terms of recognition rate, surpassing the state-of-the-art for most of them. This work is made reproducible: all the source code, scores, and trained models of this approach are made publicly available.","","","10.1109/TIFS.2018.2885284","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8565895","Face recognition;heterogeneous face recognition;reproducible research;domain adaptation;deep neural networks","Face;Databases;Face recognition;Feature extraction;Protocols;Visualization;Image recognition","convolutional neural nets;face recognition;image matching;learning (artificial intelligence)","visual spectra images;face images;thermal images;domain specific units;deep convolutional neural networks;heterogeneous face recognition","","2","56","","","","","IEEE","IEEE Journals"
"A Novel Equivalent Model of Active Distribution Networks Based on LSTM","C. Zheng; S. Wang; Y. Liu; C. Liu; W. Xie; C. Fang; S. Liu","State Key Laboratory of Advanced Electromagnetic Engineering and Technology, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Advanced Electromagnetic Engineering and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN, USA; Department of Energy Technology, Aalborg University, Aalborg, Denmark; State Grid Shanghai Municipal Electric Power Company, Shanghai, China; State Grid Shanghai Municipal Electric Power Company, Shanghai, China; State Grid Shanghai Municipal Electric Power Company, Shanghai, China","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","9","2611","2624","Dynamic behaviors of distribution networks are of great importance for the power system analysis. Nowadays, due to the integration of the renewable energy generation, energy storage, plug-in electric vehicles, and distribution networks turn from passive systems to active ones. Hence, the dynamic behaviors of active distribution networks (ADNs) are much more complex than the traditional ones. The research interests how to establish an accurate model of ADNs in modern power systems are drawing a great deal of attention. In this paper, motivated by the similarities between power system differential algebraic equations and the forward calculation flows of recurrent neural networks (RNNs), a long short-term memory (LSTM) RNN-based equivalent model is proposed to accurately represent the ADNs. First, the adoption reasons of the proposed LSTM RNN-based equivalent model are explained, and its advantages are analyzed from the mathematical point of view. Then, the accuracy and generalization performance of the proposed model is evaluated using the IEEE 39-Bus New England system integrated with ADNs in the study cases. It reveals that the proposed LSTM RNN-based equivalent model has a generalization capability to capture the dynamic behaviors of ADNs with high accuracy.","","","10.1109/TNNLS.2018.2885219","National Basic Research Program of China (973 Program); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598989","Deep learning;dynamic behaviors;load modeling;long short-term memory (LSTM);measurement-based approach;recurrent neural network (RNN)","Load modeling;Power system dynamics;Artificial neural networks;Mathematical model;Power system stability;Analytical models;Neurons","differential algebraic equations;distribution networks;power engineering computing;power system stability;recurrent neural nets","novel equivalent model;active distribution networks;dynamic behaviors;power system analysis;renewable energy generation;energy storage;plug-in electric vehicles;passive systems;ADNs;modern power systems;power system differential algebraic equations;recurrent neural networks;LSTM RNN-based equivalent model;IEEE 39-bus New England system;long short-term memory RNN-based equivalent model","","3","62","","","","","IEEE","IEEE Journals"
"Automatic classification of skin burn colour images using texture-based feature extraction","U. Şevik; E. Karakullukçu; T. Berber; Y. Akbaş; S. Türkyılmaz","Karadeniz Technical University, Turkey; Karadeniz Technical University, Turkey; Karadeniz Technical University, Turkey; Karadeniz Technical University, Turkey; Karadeniz Technical University, Turkey","IET Image Processing","","2019","13","11","2018","2028","Current standard of burn wound evaluation is based on digital photography of wounds examined by a burn specialist. Due to subjectivity of this approach, automated burn wound analysis systems are being developed by researchers. Those systems should contain three major components: segmentation of burn images, feature extraction, and classification of segmented regions into healthy skin, burned skin, and background. The first purpose of this study is to examine various methods in each of these steps and to achieve the best combination. Comparing the performance of segmentation-based classification approach versus deep learning is the second goal of the study. SegNet-based semantic segmentation was implemented as a deep learning approach. The best combination to successfully classify the images into skin, burn, and background regions was found to be the fuzzy c-means algorithm for the segmentation part, and a multilayer feed-forward artificial neural network trained by the back-propagation algorithm for the classification part. Having an F-score of 74.28% in the classification of images captured without a protocol, the proposed scheme managed to achieve similar results with deep learning, which had an F-score of 80.50%.","","","10.1049/iet-ipr.2018.5899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8853464","","","image classification;image colour analysis;wounds;feature extraction;image segmentation;medical image processing;feedforward neural nets;learning (artificial intelligence);image texture;skin","texture-based feature extraction;burn wound evaluation;burn specialist;automated burn wound analysis systems;burn images;segmented regions;healthy skin;burned skin;segmentation-based classification approach;SegNet-based semantic segmentation;deep learning approach;background regions;segmentation part;multilayer feed-forward artificial neural network;classification part;automatic classification;colour images;digital photography","","","39","","","","","IET","IET Journals"
"An Evaluation of HTM and LSTM for Short-Term Arterial Traffic Flow Prediction","J. Mackenzie; J. F. Roddick; R. Zito","College of Science and Engineering, Flinders University at Tonsley, Clovelly Park, SA, Australia; College of Science and Engineering, Flinders University at Tonsley, Clovelly Park, SA, Australia; College of Science and Engineering, Flinders University at Tonsley, Clovelly Park, SA, Australia","IEEE Transactions on Intelligent Transportation Systems","","2019","20","5","1847","1857","Recent years have seen the emergence of two significant technologies: big data systems capable of storing, retrieving, and processing large amounts of data, and machine learning algorithms capable of learning and predicting complex sequences. In combination, these technologies present new opportunities to leverage the increasingly large amounts of traffic volume data to improve traffic flow prediction and the detection of anomalous traffic flows. In this paper, we investigate and evaluate the use of hierarchical temporal memory (HTM) for short-term prediction of traffic flows over real-world Sydney Coordinated Adaptive Traffic System data on arterial roads in the Adelaide metropolitan area in South Australia. Results are compared with those from long-short-term memory (LSTM). Extended experimentation with LSTM network configurations in both batch learning and online learning modes provide results with superior predictive performance over previous usage of LSTM and other deep learning techniques for short-term traffic flow prediction. In addition, we argue that HTM has potential as an effective tool for short term traffic flow prediction with results on par with LSTM and improvements when traffic flow distributions change.","","","10.1109/TITS.2018.2843349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424074","Arterial road networks;hierarchical temporal memory (HTM);intelligent transportation systems;long short-term memory (LSTM);traffic-flow prediction","Roads;Neural networks;Prediction algorithms;Traffic control;Predictive models;Timing","Big Data;learning (artificial intelligence);recurrent neural nets;road traffic;traffic engineering computing","HTM;LSTM;short-term arterial Traffic flow prediction;machine learning algorithms;short-term prediction;long-short-term memory;short term traffic flow prediction;Big Data systems;traffic flow distributions;anomalous traffic flows;Sydney coordinated adaptive traffic system data;South Australia;batch learning;online learning","","1","37","","","","","IEEE","IEEE Journals"
"Intelligent Traffic Control System Based on Cloud Computing and Big Data Mining","M. Shengdong; X. Zhengxian; T. Yixiang","Collaborative Innovation Center for Green Development in the Wuling Shan Region, Yangtze Normal University, Chongqing, China; Collaborative Innovation Center for Green Development in the Wuling Shan Region, Yangtze Normal University, Chongqing, China; School of Management and Economics, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Industrial Informatics","","2019","15","12","6583","6592","This article aims at discussing problems such as complex object types, large amount of data collection, high demand for transmission and calculation, and weak real-time scheduling and control ability in the construction of modern intelligent traffic information physical fusion network, cloud-based control system theory, modern intelligent traffic control network as the research object, and the physical design of the intelligent transportation information fusion cloud control system scheme. The scheme includes intelligent transportation edge control technology and intelligent transportation network virtualization technology. Based on intelligent traffic flow data, in the center of the cloud control management server using deep learning and overrun learning machine intelligence study methods, such as the forecast of traffic flow data for training, to predict urban road short-term traffic flow and congestion. Further up in the air by using intelligent optimization scheduling algorithm for real-time traffic flow control strategy, the simulation results show the effectiveness of the proposed method.","","","10.1109/TII.2019.2929060","Chongqing Municipal Education Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765413","Cloud control systems;cyber-physical;deep learning;extreme learning machine (ELM);intelligent transportation","Cloud computing;Control systems;Real-time systems;Predictive models;Transportation;Big Data;Job shop scheduling","","","","","29","IEEE","","","","IEEE","IEEE Journals"
"Which Has Better Visual Quality: The Clear Blue Sky or a Blurry Animal?","D. Li; T. Jiang; W. Lin; M. Jiang","Key Laboratory of Mathematics and Its Applications (LMAM), School of Mathematical Sciences, Beijing International Center for Mathematical Research, Cooperative Medianet Innovation Center, Peking University, Beijing, China; National Engineering Laboratory for Video Technology, School of Electrical Engineering and Computer Science, Cooperative Medianet Innovation Center, Peking University, Beijing, China; School of Computer Engineering, Nanyang Technological University, Singapore; Key Laboratory of Mathematics and Its Applications (LMAM), School of Mathematical Sciences, Beijing International Center for Mathematical Research, Cooperative Medianet Innovation Center, Peking University, Beijing, China","IEEE Transactions on Multimedia","","2019","21","5","1221","1234","Image content variation is a typical and challenging problem in no-reference image-quality assessment (NR-IQA). This work pays special attention to the impact of image content variation on NR-IQA methods. To better analyze this impact, we focus on blur-dominated distortions to exclude the impacts of distortion-type variations. We empirically show that current NR-IQA methods are inconsistent with human visual perception when predicting the relative quality of image pairs with different image contents. In view of deep semantic features of pretrained image classification neural networks always containing discriminative image content information, we put forward a new NR-IQA method based on semantic feature aggregation (SFA) to alleviate the impact of image content variation. Specifically, instead of resizing the image, we first crop multiple overlapping patches over the entire distorted image to avoid introducing geometric deformations. Then, according to an adaptive layer selection procedure, we extract deep semantic features by leveraging the power of a pretrained image classification model for its inherent content-aware property. After that, the local patch features are aggregated using several statistical structures. Finally, a linear regression model is trained for mapping the aggregated global features to image-quality scores. The proposed method, SFA, is compared with nine representative blur-specific NR-IQA methods, two general-purpose NR-IQA methods, and two extra full-reference IQA methods on Gaussian blur images (with and without Gaussian noise/JPEG compression) and realistic blur images from multiple databases, including LIVE, TID2008, TID2013, MLIVE1, MLIVE2, BID, and CLIVE. Experimental results show that SFA is superior to the state-of-the-art NR methods on all seven databases. It is also verified that deep semantic features play a crucial role in addressing image content variation, and this provides a new perspective for NR-IQA.","","","10.1109/TMM.2018.2875354","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Tier 2 Fund of Ministry of Education, Singapore; Sino-German Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489929","Deep semantic features;image content variation;no-reference image quality assessment;realistic blur;statistical aggregation","Feature extraction;Image quality;Semantics;Image edge detection;Distortion;Adaptation models;Image coding","data compression;feature extraction;Gaussian noise;Gaussian processes;image classification;image coding;image colour analysis;image denoising;learning (artificial intelligence);neural nets;regression analysis;visual databases;visual perception","no-reference image-quality assessment;image content variation;NR-IQA method;image pairs;deep semantic features;pretrained image classification neural networks;discriminative image content information;pretrained image classification model;image-quality scores;representative blur-specific NR-IQA methods;Gaussian blur images;realistic blur images;distorted image;NR-IQA methods;image contents;semantic feature aggregation;JPEG compression","","","61","","","","","IEEE","IEEE Journals"
"Sample generation of semi-automatic pavement crack labelling and robustness in detection of pavement diseases","G. Jia; W. Song; D. Jia; H. Zhu","School of Geomatics, Liaoning Technical University, Fuxin 123000, People's Republic of China; School of Geomatics, Liaoning Technical University, Fuxin 123000, People's Republic of China; School of Electronic and Information Engineering, Liaoning Technical University, Huludao 125105, People's Republic of China; College of Ecology and Environment, Institute of Disaster Prevention, Beijing 101601, People's Republic of China","Electronics Letters","","2019","55","23","1235","1238","Recent convolutional neural networks have made significant advancements in the detection of road cracks. However, the lack of accurate crack training data reduces the generalisation ability of the deep model. In this Letter, a semi-automatic pavement crack labelling algorithm is proposed to solve the problem of insufficient training data. First, the modified C-V model is used to obtain the preliminary segmentation results. Second, the direction of the initial segmentation area is calculated by the ellipse fitting method, and the preliminary segmentation results are used as samples for accurate labelling. Finally, a multi-scale feature extraction module is proposed for learning rich deep convolutional features, which allows the acquired crack features under a complex background to be more discriminant. The experimental results were compared with the manual marking method, and this method can achieve accurate marking of crack images with a low amount of interaction, thereby significantly reducing the cost of ground-truth making. The results of the validation and comparison experiments on test data sets indicate that the proposed method can not only effectively identify cracks, but also overcome the interference of many factors in the environment.","","","10.1049/el.2019.2692","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8896712","","","convolutional neural nets;crack detection;feature extraction;image segmentation;learning (artificial intelligence);road building;structural engineering computing","multiscale feature extraction module;rich deep convolutional features;manual marking method;crack images;sample generation;pavement diseases;road cracks;generalisation ability;semiautomatic pavement crack labelling algorithm;modified C-V model;ellipse fitting method;crack features;convolutional neural networks","","","8","","","","","IET","IET Journals"
"Sparse Bayesian Learning-Based Seismic High-Resolution Time-Frequency Analysis","S. Yuan; Y. Ji; P. Shi; J. Zeng; J. Gao; S. Wang","State Key Laboratory of Petroleum Resources and Prospecting, China University of Petroleum, Beijing, China; State Key Laboratory of Petroleum Resources and Prospecting, China University of Petroleum, Beijing, China; School of Earth and Environment, University of Leeds, Leeds, U.K.; State Key Laboratory of Petroleum Resources and Prospecting, China University of Petroleum, Beijing, China; Northwest Branch, Research Institute of Petroleum Exploration and Development, Petrochina, Lanzhou, China; State Key Laboratory of Petroleum Resources and Prospecting, China University of Petroleum, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","4","623","627","Time-frequency (TF) analysis is a useful tool for seismic data processing and interpretation. We introduce sparse Bayesian learning (SBL) to TF analysis and propose a new SBL-based high-resolution TF method. The method decomposes the seismic trace into a series of Ricker wavelets using SBL-based sparse representations and subsequently implements Wigner-Ville distribution (WVD) on the decomposed wavelets to produce TF spectra. By iteratively solving a Bayesian maximum posterior and a type-II maximum likelihood, SBL-based decomposition can sequentially obtain an optimal number of Ricker wavelets with different peak frequencies or phases from a preset wavelet dictionary, and can simultaneously invert for the associated sparse TF pseudoreflectivity with the prediction uncertainty. The WVD of SBL-based decomposed wavelets can assemble TF distribution of the reconstructed signals to approximately characterize WVD of the original data. Therefore, the linear stack of WVD of all decomposed independent wavelets is immune from both the notorious cross-term interferences of the traditional WVD and random noise. Synthetic data example involving thin beds and laboratorial physical modeling data example involving several known multicave combinations are used to demonstrate the effectiveness of the proposed SBL-based TF analysis method and illustrate its advantages over WVD and the orthogonal matching pursuit-based TF analysis method. The 3-D real seismic data example is adopted to test its application potential for interpreting deep channels and the karst slope fracture zone. The results show that the proposed SBL-based TF method is a potentially effective, stable and high-resolution seismic TF analysis tool even in the presence of thin beds.","","","10.1109/LGRS.2018.2883496","National Natural Science Foundation of China; Major Scientific Research Program of Petrochina Science and Technology Management Department; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8579223","Seismic data interpretation;sparse Bayesian learning (SBL);sparse representations;time-frequency (TF) analysis","Wavelet analysis;Wavelet transforms;Tools;Bayes methods;Time-frequency analysis;Sparse matrices;Time-domain analysis","approximation theory;Bayes methods;geophysical signal processing;interference (signal);iterative methods;maximum likelihood estimation;seismology;signal reconstruction;time-frequency analysis","associated sparse TF pseudoreflectivity;SBL-based decomposed wavelets;TF distribution;decomposed independent wavelets;synthetic data example;laboratorial physical modeling data example;SBL-based TF analysis method;orthogonal matching pursuit-based TF analysis method;seismic data example;high-resolution seismic TF analysis tool;seismic data processing;SBL-based high-resolution TF method;seismic trace;Ricker wavelets;SBL-based sparse representations;TF spectra;Bayesian maximum posterior;type-II maximum likelihood;SBL-based decomposition;preset wavelet dictionary;sparse Bayesian learning;seismic data interpretation;Wigner-Ville distribution;peak frequencies;seismic high-resolution time-frequency analysis;cross-term interferences;karst slope fracture zone","","6","26","","","","","IEEE","IEEE Journals"
"Computer-Aided Diagnostic System for Early Detection of Acute Renal Transplant Rejection Using Diffusion-Weighted MRI","M. Shehata; F. Khalifa; A. Soliman; M. Ghazal; F. Taher; M. A. El-Ghar; A. C. Dwyer; G. Gimel’farb; R. S. Keynton; A. El-Baz","Department of Bioengineering, University of Louisville, Louisville, KY, USA; Department of Bioengineering, University of Louisville, Louisville, KY, USA; Department of Bioengineering, University of Louisville, Louisville, KY, USA; Department of Electrical and Computer EngineeringAbu Dhabi University; College of Technological InnovationZayed University; Department of Radiology, Urology, and Nephrology CenterUniversity of Mansoura; Kidney Transplantation–Kidney Disease CenterUniversity of Louisville; Department of Computer ScienceUniversity of Auckland; Department of Bioengineering, University of Louisville, Louisville, KY, USA; Department of Bioengineering, University of Louisville, Louisville, KY, USA","IEEE Transactions on Biomedical Engineering","","2019","66","2","539","552","Objective: Early diagnosis of acute renal transplant rejection (ARTR) is critical for accurate treatment. Although the current gold standard, diagnostic technique is renal biopsy, it is not preferred due to its invasiveness, long recovery time (1-2 weeks), and potential for complications, e.g., bleeding and/or infection. Methods: This paper presents a computer-aided diagnostic (CAD) system for early ARTR detection using (3D + b-value) diffusion-weighted (DW) magnetic resonance imaging (MRI) data. The CAD process starts from kidney tissue segmentation with an evolving geometric (level-set-based) deformable model. The evolution is guided by a voxel-wise stochastic speed function, which follows from a joint kidney-background Markov-Gibbs random field model accounting for an adaptive kidney shape prior and on-going kidney-background visual appearances. A B-spline-based three-dimensional data alignment is employed to handle local deviations due to breathing and heart beating. Then, empirical cumulative distribution functions of apparent diffusion coefficients of the segmented DW-MRI at different b-values are collected as discriminatory transplant status features. Finally, a deep-learning-based classifier with stacked nonnegative constrained autoencoders is employed to distinguish between rejected and nonrejected renal transplants. Results: In our initial “leave-one-subject-out” experiment on 100 subjects, 97.0% of the subjects were correctly classified. The subsequent four-fold and ten-fold cross-validations gave the average accuracy of 96.0% and 94.0%, respectively. Conclusion: These results demonstrate the promise of this new CAD system to reliably diagnose renal transplant rejection. Significance: The technology presented here can significantly impact the quality of care of renal transplant patients since it has the potential to replace the gold standard in kidney diagnosis, biopsy.","","","10.1109/TBME.2018.2849987","American people through the United States Agency for International Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8395032","Renal rejection;CAD system;Deep learning;DW-MRI;ADC","Kidney;Shape;Image segmentation;Magnetic resonance imaging;Biopsy;Three-dimensional displays;Deformable models","biological tissues;biomedical MRI;cardiology;diseases;image segmentation;kidney;learning (artificial intelligence);Markov processes;medical image processing;patient treatment","computer-aided diagnostic system;acute renal transplant rejection;diagnostic technique;renal biopsy;CAD process;kidney tissue segmentation;voxel-wise stochastic speed function;joint kidney-background Markov-Gibbs random field model accounting;adaptive kidney shape;kidney-background visual appearances;B-spline-based three-dimensional data alignment;empirical cumulative distribution functions;segmented DW-MRI;discriminatory transplant status features;CAD system;renal transplant patients;kidney diagnosis;diffusion coefficients","","4","56","","","","","IEEE","IEEE Journals"
"VRSA Net: VR Sickness Assessment Considering Exceptional Motion for 360° VR Video","H. G. Kim; H. Lim; S. Lee; Y. M. Ro","Image and Video Systems Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Lab, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Image Processing","","2019","28","4","1646","1660","The viewing safety is one of the main issues in viewing virtual reality (VR) content. In particular, VR sickness could occur when watching immersive VR content. To deal with the viewing safety for VR content, objective assessment of VR sickness is of great importance. In this paper, we propose a novel objective VR sickness assessment (VRSA) network based on deep generative model for automatically predicting the VR sickness score. The proposed method takes into account motion patterns of VR videos in which an exceptional motion is a critical factor inducing excessive VR sickness in human motion perception. The proposed VRSA network consists of two parts, which are VR video generator and VR sickness score predictor. By training the VR video generator with common videos with non-exceptional motion, the generator learns the tolerance of VR sickness in human motion perception. As a result, the difference between the original and the generated videos by the VR video generator could represent exceptional motion of VR video causing VR sickness. In the VR sickness score predictor, the VR sickness score is predicted by projecting the difference between the original and the generated videos onto the subjective score space. For the evaluation of VR sickness assessment, we built a new dataset which consists of 360° videos (stimuli), corresponding physiological signals, and subjective questionnaires from subjective assessment experiments. Experimental results demonstrated that the proposed VRSA network achieved a high correlation with human perceptual score for VR sickness.","","","10.1109/TIP.2018.2880509","Institute for Information and communications Technology Promotion; Korea government (MSIT) (Development of VR sickness reduction technique for enhanced sensitivity broadcasting); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8531715","VR sickness;deep learning;virtual reality;objective assessment;motion mismatch","Physiology;Generators;Visualization;Image quality;Safety;Training;Heart rate","human factors;learning (artificial intelligence);video signal processing;virtual reality;visual perception","virtual reality content;immersive VR content;viewing safety;objective VR sickness assessment;excessive VR sickness;human motion perception;VR video generator;VR sickness score predictor;generated videos;VR sickness assessment considering exceptional motion","","2","62","","","","","IEEE","IEEE Journals"
"Attended Relation Feature Representation of Facial Dynamics for Facial Authentication","S. T. Kim; Y. M. Ro","Image and Video Systems Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Image and Video Systems Laboratory, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Information Forensics and Security","","2019","14","7","1768","1778","In psychology, it is known that facial dynamics benefit the perception of identity. This paper proposes a novel deep network framework to capture identity information from facial dynamics and their relations. In the proposed method, facial dynamics occurred from a smile expression are analyzed and utilized for facial authentication. Detailed changes in the local regions of a face such as wrinkles and dimples are encoded in the facial dynamic feature representation. The latent relationships of the facial dynamic features are learned by the facial dynamic relational network. In the facial dynamic relational network, the relation features of the facial dynamic are encoded and the relational importance is encoded based on the relation features. As a result, the proposed method has more attention on the important relation features in facial authentication. Through comprehensive and comparative experiments, the effectiveness of the proposed method has been verified in facial authentication.","","","10.1109/TIFS.2018.2885276","Institute for Information and communications Technology Promotion; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8565878","Facial dynamics;deep learning;relations;facial authentication","Authentication;Dynamics;Encoding;Three-dimensional displays;Face;Feature extraction;Mouth","face recognition;feature extraction;image representation;learning (artificial intelligence);psychology","facial authentication;facial dynamic feature representation;facial dynamic relational network;relation feature representation;psychology;identity information","","","50","","","","","IEEE","IEEE Journals"
"Shadow Detection in Single RGB Images Using a Context Preserver Convolutional Neural Network Trained by Multiple Adversarial Examples","S. Mohajerani; P. Saeedi","School of Engineering Science, Simon Fraser University, Burnaby, BC, Canada; School of Engineering Science, Simon Fraser University, Burnaby, BC, Canada","IEEE Transactions on Image Processing","","2019","28","8","4117","4129","Automatic identification of shadow regions in an image is a basic and yet very important task in many computer vision applications, such as object detection, target tracking, and visual data analysis. Although shadow detection is a well-studied topic, current methods for identification of shadow are not as accurate as required. In this paper, we propose a deep-learning method for shadow detection at a pixel-level that is suitable for single RGB images. The proposed CNN-based method benefits from a novel architecture through which global and local shadow attributes are identified using a new and efficient mapping scheme in the skip connection. It extracts and preserves shadow context in multiple layers and utilizes them gradually in multiple blocks to generate final shadow masks. The training phase of the network is simple and can be directly and easily adapted for other image segmentation tasks. The performance of the proposed system is evaluated on three publicly available datasets, where it outperforms the state-of-the-art balanced error rates (BER) by 3%, 6.2%, and 11.4%.","","","10.1109/TIP.2019.2904267","Government of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664462","Shadow detection;image segmentation;deep-learning;pixel-level labeling;adversarial examples;CNN;U-Net","Training;Feature extraction;Task analysis;Lighting;Image segmentation;Object detection","convolutional neural nets;feature extraction;image segmentation;learning by example;object detection","multiple adversarial examples training;shadow regions automatic identification;computer vision applications;target tracking;visual data analysis;global-local shadow attributes identified;mapping scheme;skip connection;final shadow masks generation;simple network training phase;context preserver convolutional neural network;single RGB images shadow detection","","1","55","","","","","IEEE","IEEE Journals"
"Twin-Timescale Artificial Intelligence Aided Mobility-Aware Edge Caching and Computing in Vehicular Networks","L. T. Tan; R. Q. Hu; L. Hanzo","Department of Electrical and Computer Engineering, Utah State University, Logan, UT, USA; Department of Electrical and Computer Engineering, Utah State University, Logan, UT, USA; School of Electronics and Computer Science, University of Southampton, Southampton, U.K.","IEEE Transactions on Vehicular Technology","","2019","68","4","3086","3099","In this paper, we propose a joint communication, caching and computing strategy for achieving cost efficiency in vehicular networks. In particular, the resource allocation policy is specifically designed by considering the vehicle's mobility and the hard service deadline constraint. An artificial intelligence-based multi-timescale framework is proposed for tackling these challenges. To mitigate the complexity associated with the large action and search space in the sophisticated multi-timescale framework considered, we propose to maximize a carefully constructed mobility-aware reward function using the classic particle swarm optimization scheme at the associated large timescale level, while we employ deep reinforcement learning at the small timescale level of our sophisticated twin-timescale solution. Numerical results are presented to illustrate the theoretical findings and to quantify the performance gains attained.","","","10.1109/TVT.2019.2893898","National Science Foundation; Natural Science Foundation of China; Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8618350","Vehicular networks;vehicular mobility;edge caching and computing;artificial intelligence;deep reinforcement learning;particle swarm optimization","Computer architecture;Wireless communication;Delays;Task analysis;Relays;Computational modeling;Device-to-device communication","cache storage;cooperative communication;learning (artificial intelligence);particle swarm optimisation;resource allocation;road traffic;traffic engineering computing;vehicular ad hoc networks","twin-timescale artificial intelligence aided mobility-aware edge caching;vehicular networks;cost efficiency;resource allocation policy;hard service deadline constraint;artificial intelligence-based multitimescale framework;classic particle swarm optimization scheme;vehicles mobility;mobility-aware reward function","","3","47","","","","","IEEE","IEEE Journals"
"A CNN Model for Semantic Person Part Segmentation With Capacity Optimization","Y. Jiang; Z. Chi","Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong; Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Image Processing","","2019","28","5","2465","2478","In this paper, a deep learning model with an optimal capacity is proposed to improve the performance of person part segmentation. Previous efforts in optimizing the capacity of a convolutional neural network (CNN) model suffer from a lack of large datasets as well as the over-dependence on a single-modality CNN, which is not effective in learning. We make several efforts in addressing these problems. First, other datasets are utilized to train a CNN module for pre-processing image data and a segmentation performance improvement is achieved without a time-consuming annotation process. Second, we propose a novel way of integrating two complementary modules to enrich the feature representations for more reliable inferences. Third, the factors to determine the capacity of a CNN model are studied and two novel methods are proposed to adjust (optimize) the capacity of a CNN to match it to the complexity of a task. The over-fitting and under-fitting problems are eased by using our methods. Experimental results show that our model outperforms the state-of-the-art deep learning models with a better generalization ability and a lower computational complexity.","","","10.1109/TIP.2018.2886785","Hong Kong Polytechnic University; National Natural Science Foundation of China; Hong Kong Polytechnic University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576539","Person part segmentation;convolutional neural network;complementary modules;simplification of CNNs;capacity optimization","Image segmentation;Task analysis;Computational modeling;Semantics;Complexity theory;Training data;Skeleton","computational complexity;convolutional neural nets;image segmentation;learning (artificial intelligence);object detection;optimisation","CNN model;semantic person part segmentation;capacity optimization;deep learning model;optimal capacity;convolutional neural network model;single-modality CNN;CNN module;segmentation performance improvement;time-consuming annotation process;complementary modules;computational complexity;generalization ability;image data preprocessing","","2","57","","","","","IEEE","IEEE Journals"
"Deep Alignment Network Based Multi-Person Tracking With Occlusion and Motion Reasoning","Q. Zhou; B. Zhong; Y. Zhang; J. Li; Y. Fu","College of Computer Science and Technology, Huaqiao University, Xiamen, China; College of Computer Science and Technology, Huaqiao University, Xiamen, China; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA","IEEE Transactions on Multimedia","","2019","21","5","1183","1194","Tracking-by-detection is one of the typical paradigms for multi-person tracking, due to the availability of automatic pedestrian detectors. However, existing multi-person trackers are greatly challenged by misalignment in the pedestrian detectors (i.e., excessive background and part missing) and occlusion. To effectively handle these problems, we propose a deep alignment network-based multi-person tracking method with occlusion and motion reasoning. Specifically, the inaccurate detections are first corrected via a deep alignment network, in which an alignment estimation module is used to automatically learn the spatial transformation of these detections. As a result, the deep features from our alignment network will have better representation power and, thus, lead to more consistent tracks. Then, a coarse-to-fine schema is designed for construing a discriminative association cost matrix with spatial, motion, and appearance information. Meanwhile, a principled approach is developed to allow our method to handle occlusion with motion reasoning and the reidentification ability of the pedestrian alignment network. Finally, the association problem is solved via a simple yet real-time Hungarian algorithm. Comprehensive experiments on MOT16, ISSIA soccer, PETS09, and TUD datasets validate the effectiveness and robustness of our proposed tracker.","","","10.1109/TMM.2018.2875360","National Natural Science Foundation of China; Natural Science Foundation of Fujian Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8488599","Multi-person tracking;alignment network;occlusion reasoning;motion reasoning","Target tracking;Detectors;Cognition;Estimation;Cameras;Feature extraction","image motion analysis;object detection;object tracking;pedestrians","spatial motion;motion reasoning;pedestrian alignment network;tracking-by-detection;automatic pedestrian detectors;deep alignment network-based multiperson tracking method;alignment estimation module;multiperson trackers","","3","64","","","","","IEEE","IEEE Journals"
"Convolutional Neural Network Based Text Steganalysis","J. Wen; X. Zhou; P. Zhong; Y. Xue","College of Information and Electrical Engineering, China Agricultural University, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China; College of Science, China Agricultural University, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China","IEEE Signal Processing Letters","","2019","26","3","460","464","The prevailing text steganalysis methods detect steganographic communication by extracting hand-crafted features and classifying them using SVM. However, these features are designed based on the statistical changes caused by steganography, thus they are difficult to adapt to different kinds of embedding algorithms and the detection performance is heavily dependent on the text size. In this letter, we propose a novel text steganalysis model based on convolutional neural network, which is able to capture complex dependencies and learn feature representations automatically from the texts. First, we use a word embedding layer to extract the semantic and syntax feature of words. Second, the rectangular convolution kernels with different sizes are used to learn the sentence features. To further improve the performance, we present a decision strategy for detecting the long texts. Experimental results show that the proposed method can effectively detect different kinds of text steganographic algorithms and achieve comparable or superior performance for a wide variety of text sizes compared with the previous methods.","","","10.1109/LSP.2019.2895286","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625512","Text steganalysis;convolutional neural network;deep learning;decision strategy","Feature extraction;Kernel;Convolution;Semantics;Convolutional neural networks;Adaptation models;Syntactics","convolutional neural nets;data encapsulation;feature extraction;learning (artificial intelligence);statistical analysis;steganography;support vector machines;text analysis","convolutional neural network;steganographic communication;hand-crafted features;statistical changes;detection performance;text size;text steganalysis model;complex dependencies;feature representations;word embedding layer;rectangular convolution kernels;sentence features;text steganographic algorithms;SVM","","2","18","","","","","IEEE","IEEE Journals"
"RACE-Net: A Recurrent Neural Network for Biomedical Image Segmentation","A. Chakravarty; J. Sivaswamy","Center for Visual Information Technology, International Institute of Information Technology, Hyderabad, India; Center for Visual Information Technology, International Institute of Information Technology, Hyderabad, India","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","1151","1162","The level set based deformable models (LDM) are commonly used for medical image segmentation. However, they rely on a handcrafted curve evolution velocity that needs to be adapted for each segmentation task. The Convolutional Neural Networks (CNN) address this issue by learning robust features in a supervised end-to-end manner. However, CNNs employ millions of network parameters, which require a large amount of data during training to prevent over-fitting and increases the memory requirement and computation time during testing. Moreover, since CNNs pose segmentation as a region-based pixel labeling, they cannot explicitly model the high-level dependencies between the points on the object boundary to preserve its overall shape, smoothness or the regional homogeneity within and outside the boundary. We present a Recurrent Neural Network based solution called the RACE-net to address the above issues. RACE-net models a generalized LDM evolving under a constant and mean curvature velocity. At each time-step, the curve evolution velocities are approximated using a feed-forward architecture inspired by the multiscale image pyramid. RACE-net allows the curve evolution velocities to be learned in an end-to-end manner while minimizing the number of network parameters, computation time, and memory requirements. The RACE-net was validated on three different segmentation tasks: optic disc and cup in color fundus images, cell nuclei in histopathological images, and the left atrium in cardiac MRI volumes. Assessment on public datasets was seen to yield high Dice values between 0.87 and 0.97, which illustrates its utility as a generic, off-the-shelf architecture for biomedical segmentation.","","","10.1109/JBHI.2018.2852635","Tata Consultancy Services; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8402041","Biomedical segmentation;deep learning;deformable model;RNN","Image segmentation;Computer architecture;Biomedical imaging;Task analysis;Level set;Training;Computational modeling","biomedical MRI;biomedical optical imaging;cardiology;cellular biophysics;convolutional neural nets;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;recurrent neural nets","biomedical image segmentation;handcrafted curve evolution velocity;segmentation task;network parameters;memory requirement;region-based pixel labeling;high-level dependencies;Recurrent Neural Network based solution;RACE-net models;curve evolution velocities;multiscale image pyramid;color fundus images;histopathological images;convolutional neural networks;cardiac MRI volume;cell nuclei;left atrium","","2","44","","","","","IEEE","IEEE Journals"
"Contextualized Spatial–Temporal Network for Taxi Origin-Destination Demand Prediction","L. Liu; Z. Qiu; G. Li; Q. Wang; W. Ouyang; L. Lin","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Electrical and Information Engineering, The University of Sydney, Camperdown, NSW, Australia; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Intelligent Transportation Systems","","2019","20","10","3875","3887","Taxi demand prediction has recently attracted increasing research interest due to its huge potential application in large-scale intelligent transportation systems. However, most of the previous methods only considered the taxi demand prediction in origin regions, but neglected the modeling of the specific situation of the destination passengers. We believe it is suboptimal to preallocate the taxi into each region-based solely on the taxi origin demand. In this paper, we present a challenging and worth-exploring task, called taxi origin-destination demand prediction, which aims at predicting the taxi demand between all-region pairs in a future time interval. Its main challenges come from how to effectively capture the diverse contextual information to learn the demand patterns. We address this problem with a novel contextualized spatial-temporal network (CSTN), which consists of three components for the modeling of local spatial context (LSC), temporal evolution context (TEC), and global correlation context (GCC), respectively. First, an LSC module utilizes two convolution neural networks to learn the local spatial dependencies of taxi, demand respectively, from the origin view and the destination view. Second, a TEC module incorporates the local spatial features of taxi demand and the meteorological information to a Convolutional Long Short-term Memory Network (ConvLSTM) for the analysis of taxi demand evolution. Finally, a GCC module is applied to model the correlation between all regions by computing a global correlation feature as a weighted sum of all regional features, with the weights being calculated as the similarity between the corresponding region pairs. The extensive experiments and evaluations on a large-scale dataset well demonstrate the superiority of our CSTN over other compared methods for the taxi origin-destination demand prediction.","","","10.1109/TITS.2019.2915525","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Science and Technology Planning Project of Guangdong Province; SenseTime Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8720246","Taxi demand prediction;origin-destination;context;deep learning;spatial-temporal modeling","Public transportation;Task analysis;Correlation;Neural networks;Urban areas;Context modeling;Predictive models","convolutional neural nets;intelligent transportation systems;learning (artificial intelligence);recurrent neural nets","spatial-temporal network;taxi demand prediction;taxi origin demand;taxi origin-destination demand prediction;temporal evolution;taxi demand evolution;contextualized spatial-temporal network;large-scale intelligent transportation systems;local spatial context;temporal evolution context;global correlation context;convolution neural networks;taxi local spatial dependencies;TEC module;LSC module;meteorological information;convolutional long short-term memory network;ConvLSTM;GCC module","","1","54","","","","","IEEE","IEEE Journals"
"Learning Gestures From WiFi: A Siamese Recurrent Convolutional Architecture","J. Yang; H. Zou; Y. Zhou; L. Xie","School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, Berkeley, CA, USA; School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore","IEEE Internet of Things Journal","","2019","6","6","10763","10772","We propose a gesture recognition system that leverages existing WiFi infrastructures and learns gestures from channel state information (CSI) measurements. Having developed an innovative OpenWrt-based platform for commercial WiFi devices to extract CSI data, we propose a novel deep Siamese representation learning architecture for one-shot gesture recognition. Technically, our model extends the capacity of spatio-temporal patterns learning for the standard Siamese structure by incorporating convolutional and bidirectional recurrent neural networks. More importantly, the representation learning is ameliorated by our Siamese framework and transferable pairwise loss which helps to remove structured noise, such as individual heterogeneity and various measurement conditions during domain-different training. Meanwhile, our Siamese model also enables one-shot learning for higher availability in reality. We prototype our system on commercial WiFi routers. The experiments demonstrate that our model outperforms state-of-the-art solutions for temporal–spatial representation learning and achieves satisfactory results under one-shot conditions.","","","10.1109/JIOT.2019.2941527","Ministry of Education of Singapore MoE Tier 1; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839094","Channel state information (CSI);gesture recognition;Siamese network;transfer learning","Wireless fidelity;Gesture recognition;Sensors;Internet of Things;Feature extraction;Training;OFDM","","","","","56","IEEE","","","","IEEE","IEEE Journals"
"A Probabilistic Approach to Cross-Region Matching-Based Image Retrieval","Z. Gao; L. Wang; L. Zhou","School of Information Engineering, Zhengzhou University, Zhengzhou, China; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Image Processing","","2019","28","3","1191","1204","With deep convolutional features, cross-region matching (CRM) has recently shown superior performance on image retrieval. It evaluates image similarity by comparing image regions at different locations and scales, and is, therefore, more robust to geometric variance of objects. This paper first scrutinizes CRM-based image retrieval to provide a rigorous probabilistic interpretation by following the probability ranking principle. In addition to manifesting the assumptions implicitly taken by CRM, our interpretation highlights a fundamental issue hindering the performance of CRM-when comparing two image regions, CRM ignores modeling the distribution of the visual concept class associated with an image region, making the similarity comparison less precise. Taking advantage of the unprecedented representation capability of deep convolutional features, this paper proposes one approach to tackle that issue. It treats locally clustered image regions as a pseudo-labeled class sharing the same visual concept and utilizes them to model the distribution of the visual concept class associated with an image region. Both non-parametric and parametric methods are developed for this purpose, with careful probabilistic justification. Extensive experimental study on multiple benchmark data sets demonstrates the superior performance of the proposed pseudo-label approach to CRM and other comparable methods, with the maximum improvement of more than 10 percentage points over CRM.","","","10.1109/TIP.2018.2872831","National Computational Infrastructure; Australian Government; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478170","Cross-region matching;deep convolutional features;probability ranking principle;image retrieval","Customer relationship management;Image retrieval;Probabilistic logic;Visualization;Convolutional codes;Task analysis;Australia","feature extraction;image matching;image representation;image retrieval;learning (artificial intelligence);pattern clustering","deep convolutional features;image similarity;visual concept class;cross-region matching;object geometric variance;CRM-based image retrieval;probability ranking principle;unprecedented representation capability;clustered image regions;pseudolabeled class sharing;probabilistic justification;multiple benchmark data sets","","","54","","","","","IEEE","IEEE Journals"
"Boosting Image Steganalysis Under Universal Deep Learning Architecture Incorporating Ensemble Classification Strategy","A. Su; X. Zhao","State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Signal Processing Letters","","2019","26","12","1852","1856","Image steganalysis based on convolutional neural networks (CNNs) has achieved remarkable performance. However, all existing CNN-based steganalysis methods form an ensemble by merging the outputs of independently trained networks with the same architecture. In this letter, we propose a universal CNN architecture incorporating ensemble classification strategy. Any CNN-based steganalysis with the proposed architecture needs to train only one model to form an ensemble and can boost detection accuracy in both spatial and JPEG steganography. In particular, we propose a new method to construct subspaces for training well-designed base learners. In addition, a novel voting fusion structure automatically optimized with the training process is proposed. Experimental results on the public dataset demonstrate that the proposed architecture can further improve the performance of CNN-based steganalysis. Source code is available via GitHub (https://github.com/Ante-Su/CAECS).","","","10.1109/LSP.2019.2950081","National Natural Science Foundation of China; National Key Technology R&D Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8884722","Convolutional neural networks;universal architecture;steganalysis;steganography;ensemble","Training;Transform coding;Convolution;Testing;Merging;Neural networks;Distance measurement","","","","","24","IEEE","","","","IEEE","IEEE Journals"
"Mixed-Bandwidth Cross-Channel Speech Recognition via Joint Optimization of DNN-Based Bandwidth Expansion and Acoustic Modeling","J. Gao; J. Du; E. Chen","National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; Hefei, China; Hefei, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","3","559","571","Automatic speech recognition (ASR) systems are often built using scene related speech data due to large variations of transmission channels and sampling rates in different scenarios. In this study, we propose a general framework that establishes a unified model for diversified speech data with different sampling rates and channels. The framework is a joint optimization of deep neural network (DNN)-based bandwidth expansion and acoustic modeling to exploit a large amount of diversified training data. First, we design two novel DNN architectures to map the acoustic features from narrowband to wideband speech through direct mapping and progressive mapping. The learning targets of the direct mapping DNN (DNN-DM) are the acoustic features extracted from speech with the largest bandwidth, while the acoustic features from speech with all the other bandwidths are used as input. A progressive stacking network (PSN) gradually maps the features from the low sampling rates to the highest sampling rate through the design of intermediate target layers via multitask training. Then, in addition to these bandwidth expansion networks, we investigate several joint training strategies for DNN-based acoustic models. Our experiments conducted on three diversified large-scale Mandarin speech datasets with different recording channels and sampling rates (6, 8, and 16 kHz) show that the proposed unified model using PSN for bandwidth expansion not only is a more flexible and compact design than conventional multiple acoustic models with each bandwidth for a specific sampling rate, but also yields consistent and significant improvements over bandwidth-dependent models with an average relative word error rate reduction of 6.2%, indicating that the proposed model can fully utilize the diversified cross-channel speech data with multiple bandwidths. Moreover, the proposed methods are verified to be robust on different realistic scenes and can be effectively extended to a long short-term memory framework.","","","10.1109/TASLP.2018.2886739","National Key R&D Program of China; National Natural Science Foundation of China; Key Science and Technology Project of Anhui Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574905","Deep neural network;bandwidth expansion;acoustic modeling;joint optimization;mixed-bandwidth speech recognition","Acoustics;Hidden Markov models;Speech recognition;Wideband;Data models;Narrowband","feature extraction;learning (artificial intelligence);neural nets;speech recognition","DNN-based acoustic models;large-scale Mandarin speech datasets;different recording channels;conventional multiple acoustic models;specific sampling rate;bandwidth-dependent models;average relative word error rate reduction;diversified cross-channel speech data;multiple bandwidths;joint training strategies;bandwidth expansion networks;multitask training;intermediate target layers;highest sampling rate;low sampling rates;progressive stacking network;DNN-DM;direct mapping DNN;progressive mapping;acoustic features;novel DNN architectures;diversified training data;deep neural network-based bandwidth expansion;diversified speech data;unified model;general framework;transmission channels;scene related speech data;automatic speech recognition systems;acoustic modeling;DNN-based bandwidth expansion;joint optimization;mixed-bandwidth cross-channel speech recognition","","","41","","","","","IEEE","IEEE Journals"
"Spectral–Spatial Feature Extraction for Hyperspectral Anomaly Detection","J. Lei; W. Xie; J. Yang; Y. Li; C. Chang","State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; Center of Hyperspectral Imaging in Remote Sensing, Information and Technology College, Dalian Maritime University, Dalian, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","10","8131","8143","Hyperspectral anomaly detection faces various levels of difficulty due to the high dimensionality of hyperspectral images (HSIs), redundant information, noisy bands, and the limited capability of utilizing spectral-spatial information. In this paper, we address these problems and propose a novel approach, called spectral-spatial feature extraction (SSFE), which is based on two main aspects. In the spectral domain, we assume that the anomalous pixels are rarely present and all (or most) of the samples around the anomalies belong to background (BKG). Using this fact, we introduce a suppression function to construct a discriminative feature space and utilize a deep brief network to learn spectral representation and abstraction automatically that are used as inputs to the Mahalanobis distance (MD)-based detector. In the spatial domain, the anomalies appear as a small area grouped by pixels with high correlation among them compared to BKG. Therefore, the objects appearing as a small area are extracted based on attribute filtering, and a guided filter is further employed for local smoothness. More specifically, we extract spatial features of anomalies only from one single band obtained by fusing all bands in the visible wavelength range. Finally, we detect anomalies by jointly considering the spectral and spatial detection results. Several experiments are performed, which show that our proposed method outperforms the state-of-the-art methods.","","","10.1109/TGRS.2019.2918387","National Natural Science Foundation of China; 111 Project; Fundamental Research Funds for the Central Universities; Yangtse Rive Scholar Bonus Schemes; Ten Thousand Talent Program; Natural Science Basic Research Plan in Shaanxi Province of China; China Postdoctoral Science Foundation; Science and Technology on Electro-Optic Control Laboratory and Aeronautical Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736048","Anomaly detection;deep brief network;feature extraction;hyperspectral image (HSI);interference suppression","Feature extraction;Anomaly detection;Hyperspectral imaging;Detectors;Spectral analysis","feature extraction;hyperspectral imaging;image filtering;image representation;learning (artificial intelligence);neural nets;object detection;spectral analysis","hyperspectral anomaly detection;hyperspectral images;spectral-spatial information;spectral representation;Mahalanobis distance-based detector;spatial domain;spectral-spatial feature extraction;spectral-spatial feature extraction;suppression function;spectral abstraction;attribute filtering;deep brief network","","","43","","","","","IEEE","IEEE Journals"
"SVM-CNN-Based Fusion Algorithm for Vehicle Navigation Considering Atypical Observations","J. Sun; Z. Wu; Z. Yin; Z. Yang","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Signal Processing Letters","","2019","26","2","212","216","Modern intelligent transport systems focus on the integration of multiple sensors to obtain hybrid navigation schemes. A key issue of a hybrid scheme is distribution of the information sharing coefficients (ISCs) of subsystems and the fusion of parallel multiple observations of navigation sensors. Recently, deep learning methods, particularly convolutional neural networks (CNNs), have achieved great success in image processing tasks. However, there has been limited work in using deep learning for multisensor-based integrated navigation solutions. In this letter, we propose an ensemble learner-based classification and information fusion method, in which estimation error covariance matrices provided by local adaptive filters are used as input for the classifier, and the triple numbers of ISCs are determined by the proposed scheme. The results validate the effectiveness of the proposed scheme, in which the adequately trained ensemble learner can detect the degradation of a subsystem that may suffer atypical observations or faults and consequently can adjust the corresponding ISC in real time.","","","10.1109/LSP.2018.2885511","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8570762","Hybrid navigation;convolutional neural networks (CNN);information fusion;ensemble learner","Sensors;Covariance matrices;Global navigation satellite system;Estimation;Support vector machines","adaptive filters;convolution;covariance matrices;feedforward neural nets;image classification;learning (artificial intelligence);navigation;sensor fusion;support vector machines;traffic engineering computing","image processing tasks;multisensor-based integrated navigation solutions;ensemble learner-based classification;information fusion method;estimation error covariance matrices;local adaptive filters;faults;SVM-CNN-based fusion;modern intelligent transport systems focus;multiple sensors;hybrid navigation schemes;parallel multiple observations;navigation sensors;deep learning methods;convolutional neural networks;ISC;vehicle navigation","","","43","","","","","IEEE","IEEE Journals"
"A Multi-Stage Algorithm for Acoustic Physical Model Parameters Estimation","L. Gabrielli; S. Tomassetti; S. Squartini; C. Zinato; S. Guaiana","Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy; Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy; Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy; Viscount International SpA, Mondaino, Italy; Viscount International SpA, Mondaino, Italy","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","8","1229","1240","One of the challenges in computational acoustics is the identification of models that can simulate and predict the physical behavior of a system generating an acoustic signal. Whenever such models are used for commercial applications, an additional constraint is the time to market, making automation of the sound design process desirable. In previous works, a computational sound design approach has been proposed for the parameter estimation problem involving timbre matching by deep learning, which was applied to the synthesis of pipe organ tones. In this paper, we refine previous results by introducing the former approach in a multi-stage algorithm that also adds heuristics and a stochastic optimization method operating on perceptually motivated objective cost functions. The optimization method shows to be able to refine the first estimate given by the deep learning approach and substantially improve the objective metrics, with the additional benefit of reducing the sound design process time. Subjective listening tests are also conducted to gather additional insights on the results.","","","10.1109/TASLP.2019.2914530","Viscount International; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8706579","Physics-based acoustic modeling;neural networks;computational sound design;iterative optimization","Computational modeling;Measurement;Neural networks;Timbre;Stochastic processes;Parameter estimation","acoustic signal processing;learning (artificial intelligence);musical acoustics;musical instruments;optimisation;parameter estimation;stochastic processes","acoustic signal;computational sound design approach;timbre matching;pipe organ tones;multistage algorithm;stochastic optimization method;perceptually motivated objective cost functions;deep learning approach;sound design process time;computational acoustics;acoustic physical model parameter estimation;time to market","","","47","","","","","IEEE","IEEE Journals"
"PixTextGAN: structure aware text image synthesis for license plate recognition","S. Wu; W. Zhai; Y. Cao","School of Information Science and Technology, University of Science and Technology of China, Department of Automation, Hefei, People's Republic of China; School of Information Science and Technology, University of Science and Technology of China, Department of Automation, Hefei, People's Republic of China; School of Information Science and Technology, University of Science and Technology of China, Department of Automation, Hefei, People's Republic of China","IET Image Processing","","2019","13","14","2744","2752","Rapid progress on text image recognition has been achieved with the development of deep-learning techniques. However, it is still a great challenge to achieve a comprehensive license plate recognition in the real scenes, since there are no publicly available large diverse datasets for the training of deep learning models. This paper aims at synthesising of license plate images with generative adversarial networks (GAN), refraining from collecting a vast amount of labelled data. The authors thus propose a novel PixTextGAN that leverages a controllable architecture that generates specific character structures for different text regions to generate synthetic license plate images with reasonable text details. Specifically, a comprehensive structure-aware loss function is presented to preserve the key characteristic of each character region and thus to achieve appearance adaption for better recognition. Qualitative and quantitative experiments demonstrate the superiority of authors’ proposed method in text image synthetisation over state-of-the-art GANs. Further experimental results of license plate recognition on ReId and CCPD dataset demonstrate that using the synthesised images by PixTextGAN can greatly improve the recognition accuracy.","","","10.1049/iet-ipr.2018.6588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946907","","","learning (artificial intelligence);feature extraction;text analysis;image segmentation;traffic engineering computing;object recognition","labelled data;text details;text regions;recognition accuracy;synthesised images;text image synthetisation;comprehensive structure-aware loss function;synthetic license plate images;specific character structures;generative adversarial networks;deep learning models;comprehensive license plate recognition;text image recognition;structure aware text image synthesis;PixTextGAN","","","29","","","","","IET","IET Journals"
"Effective 3-D Shape Retrieval by Integrating Traditional Descriptors and Pointwise Convolution","Z. Kuang; J. Yu; S. Zhu; Z. Li; J. Fan","Key Laboratory of Complex Systems Modeling and Simulation, School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Key Laboratory of Complex Systems Modeling and Simulation, School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Key Laboratory of Complex Systems Modeling and Simulation, School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; College of Computer and Communication Engineering, China University of Petroleum, Qingdao, China; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA","IEEE Transactions on Multimedia","","2019","21","12","3164","3177","The applications of isometric 3-D objects have recently received sufficient attention and, thus, it is very attractive to retrieve such isometric 3-D objects from large-scale collections. Although existing approaches have presented some interesting ideas, their performance is limited to their ability on feature representation. To improve the performance of 3-D object (shape) recognition, some recent algorithms prefer using complicated deep neural networks to learn discriminative features, but they consume huge amounts of computing resources. Instead, this paper presents a more effective solution by seamlessly integrating the traditional local descriptor with a deep pointwise convolutional network to extract 1-D features for shape recognition and retrieval. To reduce the costs of designing a complicated deep network, the first step of our algorithm is to describe the shape deformation by sampling a set of intrinsic point descriptors. Then, we introduce a simple yet effective pointwise convolutional network to integrate these descriptors as a global feature and the learning process can be significantly accelerated with the help of downsampling. Furthermore, a knowledge transfer strategy is used to upgrade our feature by compensating for information loss. Finally, we carry out experimental evaluations over popular shape benchmarks, and the results suggest that our approach exhibits superior accuracy rates and robustness on shape recognition and retrieval.","","","10.1109/TMM.2019.2918729","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721114","3-D Shape retrieval and recognition;isometric variation;intrinsic point descriptor;pointwise convolution;parallel knowledge transfer","Shape;Three-dimensional displays;Deep learning;Convolution;Feature extraction;Knowledge transfer","","","","1","60","IEEE","","","","IEEE","IEEE Journals"
"Learning Object Detectors With Semi-Annotated Weak Labels","D. Zhang; J. Han; G. Guo; L. Zhao","School of Mechano-Electronic Engineering, Xidian University, Xi’an, China; School of Mechano-Electronic Engineering, Xidian University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","12","3622","3635","For alleviating the human labor associated with annotating the training data for learning object detectors, recent research has focused on semi-supervised object detection (SSOD) and weakly supervised object detection (WSOD) approaches. In SSOD, instead of annotating all the instances in the whole training set, people only need to annotate the part of the training instances using bounding boxes. In WSOD, people need to annotate the image-level tags on all training images to indicate the object categories contained by the corresponding images since more detailed bounding box annotations are no longer needed. Along this line of research, this paper makes a further step to alleviate the human labor in annotating training data, leading to the problem of object detection with semi-annotated weak labels (ODSAWLs). Instead of labeling image-level tags on all training images, ODSAWL only needs the image-level tags for a small portion of the training images, and then, the object detectors can be learned from a small portion of the weakly-labeled training images and from the remaining unlabeled training images. To address such a challenging problem, this paper proposes a cross model co-training framework that collaborates an object localizer and a tag generator in an alternative optimization procedure. Specifically, during the learning procedure, these two (deep) models can transfer the needed knowledge (including labels and visual patterns) between each other. The whole learning procedure is accomplished in a few stages under the guidance of a progressive learning curriculum. To demonstrate the effectiveness of the proposed approach, we implement the comprehensive experiments on three benchmark datasets, where the obtained experimental results are quite encouraging. Notably, by using only about 15% weakly labeled training images, the proposed approach can effectively approach, or even outperform, the state-of-the-art WSOD methods.","","","10.1109/TCSVT.2018.2884173","National Key R&D Program of China; National Natural Science Foundation of China; China Postdoctoral Support Scheme for Innovative Talents; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554285","Computer vision;image processing;object detection;learning (artificial intelligence)","Training;Object detection;Detectors;Training data;Generators;Visualization;Semantics","","","","","51","IEEE","","","","IEEE","IEEE Journals"
"Multi-View Incremental Segmentation of 3-D Point Clouds for Mobile Robots","J. Chen; Y. K. Cho; Z. Kira","Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA; School of Civil and Environmental Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Robotics and Automation Letters","","2019","4","2","1240","1246","Mobile robots need to create high-definition three-dimensional (3-D) maps of the environment for applications such as remote surveillance and infrastructure mapping. Accurate semantic processing of the acquired 3-D point cloud is critical for allowing the robot to obtain a high-level understanding of the surrounding objects and perform context-aware decision making. Existing techniques for point cloud semantic segmentation are mostly applied on a single frame or offline basis, with no way to integrate the segmentation results over time. This letter proposes an online method for mobile robots to incrementally build a semantically rich 3-D point cloud of the environment. The proposed deep neural network, MCPNet, is trained to predict class labels and object instance labels for each point in the scanned point cloud in an incremental fashion. A multi-view context pooling (MCP) operator is used to combine point features obtained from multiple viewpoints to improve the classification accuracy. The proposed architecture was trained and evaluated on ray-traced scans derived from the Stanford 3-D Indoor Spaces dataset. Results show that the proposed approach led to 15% improvement in pointwise accuracy and 7% improvement in normalized mutual information compared to the next best online method, with only a 6% drop in accuracy compared to the PointNet-based offline approach.","","","10.1109/LRA.2019.2894915","Air Force Office of Scientific Research; Ministry of Land, Infrastructure, and Transport of Korea Agency for Infrastructure Technology Advancement; National Science Foundation and National Robotics Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624392","Deep learning in robotics and automation;object detection;segmentation and categorization;semantic scene understanding","Three-dimensional displays;Robot kinematics;Semantics;Ray tracing;Mobile robots;Lasers","decision making;image segmentation;learning (artificial intelligence);mobile robots;neural nets;robot vision","point features;online method;multiview incremental segmentation;mobile robots;remote surveillance;infrastructure mapping;accurate semantic processing;high-level understanding;context-aware decision making;point cloud semantic segmentation;single frame;segmentation results;scanned point cloud;multiview context pooling operator;Stanford 3-D indoor spaces dataset;PointNet;MCPNet","","3","24","","","","","IEEE","IEEE Journals"
"A Novel Patch Variance Biased Convolutional Neural Network for No-Reference Image Quality Assessment","L. Po; M. Liu; W. Y. F. Yuen; Y. Li; X. Xu; C. Zhou; P. H. W. Wong; K. W. Lau; H. Luk","Department of Electronic Engineering, City University of Hong Kong, Hong Kong; Department of Electronic Engineering, City University of Hong Kong, Hong Kong; TFI Digital Media Ltd., Hong Kong; Minieye Company, Shenzhen, China; Tencent Video, Tencent Holdings Ltd., Shenzhen, China; Department of Electronic Engineering, City University of Hong Kong, Hong Kong; TFI Digital Media Ltd., Hong Kong; TFI Digital Media Ltd., Hong Kong; TFI Digital Media Ltd., Hong Kong","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","4","1223","1229","Deep convolutional neural networks (CNNs) have been successfully applied on no-reference image quality assessment (NR-IQA) with respect to human perception. Most of these methods deal with small image patches and use the average score of the test patches for predicting the whole image quality. We discovered that image patches from homogenous regions are unreliable for both neural network training and final image quality score estimation. In addition, image patches with complex structures have much higher chances of achieving better image quality prediction. Based on these findings, we enhanced the conventional CNN-based NR-IQA algorithm to avoid homogenous patches for the network training and quality score estimation. Moreover, we also use a variance-based weighting average to bias the final image quality score to the patches with complex structure. The experimental results show that this simple approach can achieve state-of-the-art performance compared with well-known NR-IQA algorithms.","","","10.1109/TCSVT.2019.2891159","City University of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603835","Deep learning;convolution neural network;no-reference image quality assessment","Training;Image quality;Estimation;Image color analysis;Convolution;Convolutional neural networks","convolutional neural nets;feature extraction;image classification;image resolution;learning (artificial intelligence)","no-reference image quality assessment;image patches;neural network training;image quality prediction;patch variance biased convolutional neural network;human perception;image quality score estimation;CNN-based NR-IQA algorithm;variance-based weighting average","","","22","","","","","IEEE","IEEE Journals"
"Automatic Needle Segmentation and Localization in MRI With 3-D Convolutional Neural Networks: Application to MRI-Targeted Prostate Biopsy","A. Mehrtash; M. Ghafoorian; G. Pernelle; A. Ziaei; F. G. Heslinga; K. Tuncali; A. Fedorov; R. Kikinis; C. M. Tempany; W. M. Wells; P. Abolmaesumi; T. Kapur","Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada; TomTom, Amsterdam, The Netherlands; Department of Bioengineering, Imperial College London, London, U.K.; Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA; Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA; Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA; Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA; Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA; Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA; Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA; Department of Electrical and Computer Engineering, The University of British Columbia Vancouver, Canada; Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA","IEEE Transactions on Medical Imaging","","2019","38","4","1026","1036","Image guidance improves tissue sampling during biopsy by allowing the physician to visualize the tip and trajectory of the biopsy needle relative to the target in MRI, CT, ultrasound, or other relevant imagery. This paper reports a system for fast automatic needle tip and trajectory localization and visualization in MRI that has been developed and tested in the context of an active clinical research program in prostate biopsy. To the best of our knowledge, this is the first reported system for this clinical application and also the first reported system that leverages deep neural networks for segmentation and localization of needles in MRI across biomedical applications. Needle tip and trajectory were annotated on 583 T2-weighted intra-procedural MRI scans acquired after needle insertion for 71 patients who underwent transperineal MRI-targeted biopsy procedure at our institution. The images were divided into two independent training-validation and test sets at the patient level. A deep 3-D fully convolutional neural network model was developed, trained, and deployed on these samples. The accuracy of the proposed method, as tested on previously unseen data, was 2.80-mm average in needle tip detection and 0.98° in needle trajectory angle. An observer study was designed in which independent annotations by a second observer, blinded to the original observer, were compared with the output of the proposed method. The resultant error was comparable to the measured inter-observer concordance, reinforcing the clinical acceptability of the proposed method. The proposed system has the potential for deployment in clinical routine.","","","10.1109/TMI.2018.2876796","National Institutes of Health; Natural Sciences and Engineering Research Council of Canada; Canadian Institutes of Health Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8496860","Convolutional neural networks;deep learning;segmentation;localization;magnetic resonance imaging;prostate;needle;biopsy","Needles;Magnetic resonance imaging;Biopsy;Image segmentation;Trajectory;Cancer;Observers","biological organs;biological tissues;biomedical MRI;biomedical ultrasonics;cancer;computerised tomography;image segmentation;medical image processing;needles;neural nets;patient diagnosis","automatic needle segmentation;MRI-targeted prostate biopsy;image guidance;tissue sampling;biopsy needle;trajectory localization;active clinical research program;deep neural networks;biomedical applications;transperineal MRI-targeted biopsy procedure;3D convolutional neural networks;3D fully convolutional neural network model;intraprocedural MRI scans","","","61","","","","","IEEE","IEEE Journals"
"Deep Architectures and Ensembles for Semantic Video Classification","E. Ong; S. S. Husain; M. Bober-Irizar; M. Bober","University of Surrey, Guildford, U.K.; University of Surrey, Guildford, U.K.; Visual Atoms Ltd., Guildford, U.K.; University of Surrey, Guildford, U.K.","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","12","3568","3582","This paper addresses the problem of accurate semantic labeling of short videos. To this end, a multitude of three different deep nets, ranging from traditional recurrent neural 4 networks (LSTM, GRU), temporal agnostic networks (FV, VLAD, BoW), fully connected neural networks mid-stage AV fusion, and others were considered. Additionally, we also propose a residual architecture-based deep neural network (DNN) for video classification, with state-of-the-art classification performance at significantly reduced complexity. Furthermore, we propose four new approaches to diversity-driven multi-net ensembling, one based on fast correlation measure and three incorporating a DNN-based combiner. We show that significant performance gains can be achieved by ensembling diverse nets and we investigate factors contributing to high diversity. Based on the extensive YouTube8M dataset, we provide an in-depth evaluation and analysis of their behavior. We show that the performance of the ensemble is state-of-the-art achieving the highest accuracy on the YouTube8M Kaggle test data. The performance of the ensemble of classifiers was also evaluated on the HMDB51 and UCF101 datasets, and show that the resulting method achieves comparable accuracy with the state-of-the-art methods using similar input features.","","","10.1109/TCSVT.2018.2881842","Innovate UK; U.K. Defence Science and Technology Laboratory (Dstl) and Engineering and Physical Research Council; U.S DOD, U.K. MOD; Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8537942","Computer vision;artificial neural networks;machine learning algorithms","Logic gates;Semantics;Complexity theory;Neural networks;Correlation;Training;Computer architecture","","","","","39","IEEE","","","","IEEE","IEEE Journals"
"Change Detection by Training a Triplet Network for Motion Feature Extraction","T. P. Nguyen; C. C. Pham; S. V. Ha; J. W. Jeon","College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; Smart Car Division, Funzin, Seoul, South Korea; School of Computer Science and Engineering, International University—Vietnam National University, Ho Chi Minh City, Vietnam; College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","2","433","446","Change/motion detection is a challenging problem in video analysis and surveillance system. Recently, the state-of-the-art methods using the sample-based background model have demonstrated astonishing results with this problem. However, they are ineffective in the dynamic scenes that contain complex motion patterns. In this paper, we introduce a novel data-driven approach that combines the sample-based background model with a feature extractor obtained by training a triplet network. We construct the network by three identical convolutional neural networks, each of which is called a motion feature network. Our network can automatically learn motion patterns from small image patches and transform input images of any size into feature embeddings for high-level representations. The sample-based background model of each pixel is then employed by using the color information and the extracted feature embeddings. We also propose an approach to generate triplet examples from CDNet 2014 for training our network model from scratch. The offline trained network can be used on the fly without re-training on any video sequence before each execution. Therefore, it is feasible for real-time surveillance systems. In this paper, we show that our method outperforms the other state-of-the-art methods on CDNet 2014 and other benchmarks (BMC and Wallflower).","","","10.1109/TCSVT.2018.2795657","Ministry of Trade, Industry and Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8263554","Change detection;background subtraction;convolutional neural network;deep learning;video analysis","Feature extraction;Adaptation models;Computational modeling;Training;Image color analysis;Dynamics;Image segmentation","feature extraction;Gaussian processes;image colour analysis;image motion analysis;image segmentation;image sequences;learning (artificial intelligence);neural nets;object detection;surveillance;video signal processing;video surveillance","change detection;triplet network;motion feature extraction;video analysis;surveillance system;sample-based background model;complex motion patterns;feature extractor;identical convolutional neural networks;motion feature network;extracted feature embeddings;network model;offline trained network;motion detection;data-driven approach","","1","55","","","","","IEEE","IEEE Journals"
"Advanced Multi-Sensor Optical Remote Sensing for Urban Land Use and Land Cover Classification: Outcome of the 2018 IEEE GRSS Data Fusion Contest","Y. Xu; B. Du; L. Zhang; D. Cerra; M. Pato; E. Carmona; S. Prasad; N. Yokoya; R. Hänsch; B. Le Saux","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; German Aerospace Center (DLR), Remote Sensing Technology Institute (MF-PBA), Weßling, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute (MF-PBA), Weßling, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute (MF-PBA), Weßling, Germany; Electrical and Computer Engineering Department, University of Houston, Houston, TX, USA; RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan; Computer Vision and Remote Sensing Department, Technical University of Berlin, Berlin, Germany; DTIS, ONERA, University Paris Saclay, Palaiseau, France","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","6","1709","1724","This paper presents the scientific outcomes of the 2018 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2018 Contest addressed the problem of urban observation and monitoring with advanced multi-source optical remote sensing (multispectral LiDAR, hyperspectral imaging, and very high-resolution imagery). The competition was based on urban land use and land cover classification, aiming to distinguish between very diverse and detailed classes of urban objects, materials, and vegetation. Besides data fusion, it also quantified the respective assets of the novel sensors used to collect the data. Participants proposed elaborate approaches rooted in remote-sensing, and also in machine learning and computer vision, to make the most of the available data. Winning approaches combine convolutional neural networks with subtle earth-observation data scientist expertise.","","","10.1109/JSTARS.2019.2911113","National Natural Science Foundation of China; National Key R & D Program of China; Natural Science Foundation of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8727489","Convolutional neural networks (CNN);deep learning;hyperspectral (HS) imaging (HSI);image analysis and data fusion;multimodal;multiresolution;multisource;multispectral light detection and ranging (LiDAR)","Remote sensing;Laser radar;Data integration;Optical sensors;Optical imaging;Training","geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image fusion;land cover;learning (artificial intelligence);neural nets;terrain mapping","urban observation;advanced multisource optical remote sensing;multispectral LiDAR;hyperspectral imaging;urban land use;land cover classification;urban objects;urban monitoring;Image Analysis and Data Fusion Technical Committee;IEEE GRSS Data Fusion Contest;IEEE GRSS Data Fusion;IEEE Geoscience and Remote Sensing Society;advanced multisensor optical remote sensing;subtle Earth-observation data scientist expertise;convolutional neural networks","","","46","OAPA","","","","IEEE","IEEE Journals"
"Heterogeneous Hashing Network for Face Retrieval Across Image and Video Domains","C. Jing; Z. Dong; M. Pei; Y. Jia","Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Multimedia","","2019","21","3","782","794","In this paper, we present a heterogeneous hashing network to generate effective and compact hash representations of both face images and face videos for face retrieval across image and video domains. The network contains an image branch and a video branch to project face images and videos into a common space, respectively. Then, the non-linear hash functions are learned in the common space to obtain the corresponding binary hash representations. The network is trained with three loss functions: 1) the Fisher loss; 2) the softmax loss; and 3) the triplet ranking loss. The Fisher loss uses the difference form of within-class and between-class scatter and is appropriate for the mini-batch-based optimization method. The Fisher loss together with the softmax loss is exploited to enhance the discriminative power of the common space. The triplet ranking loss is enforced on the final binary hash representations to improve retrieval performance. Experiments on a large-scale face video dataset and two challenging TV-series datasets demonstrate the effectiveness of the proposed method.","","","10.1109/TMM.2018.2866222","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8440769","Face retrieval;image and video domains;deep CNN;hash learning","Face;Feature extraction;Task analysis;Semantics;Image retrieval;Correlation;Neural networks","cryptography;face recognition;feature extraction;file organisation;image representation;image retrieval;learning (artificial intelligence);optimisation;video signal processing","heterogeneous hashing network;face retrieval;face images;face videos;image branch;video branch;nonlinear hash functions;loss functions;Fisher loss;softmax loss;triplet ranking loss;retrieval performance;large-scale face video dataset;image domain;video domain;binary hash representations;TV-series datasets;within-class scatter;between-class scatter;mini-batch-based optimization method","","1","60","","","","","IEEE","IEEE Journals"
"Environment-Aware Multi-Target Tracking of Pedestrians","J. Doellinger; V. S. Prabhakaran; L. Fu; M. Spies","Bosch Center for Artificial Intelligence, Renningen, Germany; Bosch Center for Artificial Intelligence, Renningen, Germany; Meituan-Dianping Group, Beijing, China; Bosch Center for Artificial Intelligence, Renningen, Germany","IEEE Robotics and Automation Letters","","2019","4","2","1831","1837","When navigating mobile robotic systems in dynamic environments, the ability to predict where pedestrians will move in the next few seconds is crucial. To tackle this problem, many solutions have been developed which take the environment's influence on human navigation behavior into account. However, few do this in a way which seamlessly generalizes to new environments where no prior observations of pedestrians are available. In this letter, we propose a novel method that uses convolutional neural networks (CNNs) to predict local statistics about the direction of likely human motion. Specifically, the network takes crops of a floor plan as an input and computes a cell-wise probability distributions over directions conditioned on the direction in the last time step. These environment-aware motion models can be used to improve the performance of existing tracking algorithms. We evaluated our approach with two different existing trackers, a discrete, grid-based filter as well as a particle filter and show that using our proposed method as motion model considerably improves prediction quality in previously unseen test environments. Even though we train the CNN entirely in simulation, our experiments suggest that the learned models generalize to real pedestrian data.","","","10.1109/LRA.2019.2898039","Bosch Center for Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636974","Deep Learning in Robotics and Automation;Semantic Scene Understanding;Social Human-Robot Interaction","Tracking;Hidden Markov models;Microprocessors;Computer architecture;Navigation;Predictive models;Dynamics","convolutional neural nets;human-robot interaction;image motion analysis;learning (artificial intelligence);mobile robots;navigation;particle filtering (numerical methods);pedestrians;robot vision;statistical distributions;target tracking","floor plan;cell-wise probability distributions;environment-aware motion models;grid-based filter;unseen test environments;environment-aware multitarget tracking;dynamic environments;pedestrians;human navigation behavior;convolutional neural networks;local statistics;human motion;mobile robotic system navigation;CNNs;particle filter;discrete filter","","","24","","","","","IEEE","IEEE Journals"
"Attention-Based 3D-CNNs for Large-Vocabulary Sign Language Recognition","J. Huang; W. Zhou; H. Li; W. Li","Department of Electrical Engineering and Information Science, University of Science and Technology of China, Hefei, China; Department of Electrical Engineering and Information Science, University of Science and Technology of China, Hefei, China; Department of Electrical Engineering and Information Science, University of Science and Technology of China, Hefei, China; Department of Electrical Engineering and Information Science, University of Science and Technology of China, Hefei, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","9","2822","2832","Sign language recognition (SLR) is an important and challenging research topic in the multimedia field. Conventional techniques for SLR rely on hand-crafted features, which achieve limited success. In this paper, we present attention-based 3D-convolutional neural networks (3D-CNNs) for SLR. The framework has two advantages: 3D-CNNs learn spatio-temporal features from raw video without prior knowledge and the attention mechanism helps to select the clue. When training 3D-CNN for capturing spatio-temporal features, spatial attention is incorporated into the network to focus on the areas of interest. After feature extraction, temporal attention is utilized to select the significant motions for classification. The proposed method is evaluated on two large scale sign language data sets. The first one, collected by ourselves, is a Chinese sign language data set that consists of 500 categories. The other is the ChaLearn14 benchmark. The experiment results demonstrate the effectiveness of our approach compared with state-of-the-art algorithms.","","","10.1109/TCSVT.2018.2870740","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; NSFC; Fundamental Research Funds for the Central Universities; Youth Innovation Promotion Association CAS; Intel ICRI-MNC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466903","Sign language recognition;3D convolutional neural networks;attention mechanism;deep learning","Gesture recognition;Feature extraction;Hidden Markov models;Assistive technology;Three-dimensional displays;Convolution;Neural networks","computer vision;convolutional neural nets;feature extraction;image classification;image motion analysis;learning (artificial intelligence);object recognition;sign language recognition;video signal processing;vocabulary","large-vocabulary sign language recognition;SLR;multimedia field;hand-crafted features;attention-based 3D-convolutional neural networks;spatial attention;feature extraction;spatio-temporal features;sign language data sets;Chinese sign language data set","","3","50","","","","","IEEE","IEEE Journals"
"Bio-LSTM: A Biomechanically Inspired Recurrent Neural Network for 3-D Pedestrian Pose and Gait Prediction","X. Du; R. Vasudevan; M. Johnson-Roberson","Department of Naval Architecture and Marine Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Naval Architecture and Marine Engineering, University of Michigan, Ann Arbor, MI, USA","IEEE Robotics and Automation Letters","","2019","4","2","1501","1508","In applications, such as autonomous driving, it is important to understand, infer, and anticipate the intention and future behavior of pedestrians. This ability allows vehicles to avoid collisions and improve ride safety and quality. This letter proposes a biomechanically inspired recurrent neural network that can predict the location and three-dimensional (3-D) articulated body pose of pedestrians in a global coordinate frame, given 3-D poses and locations estimated in prior frames with inaccuracy. The proposed network is able to predict poses and global locations for multiple pedestrians simultaneously, for pedestrians up to 45 m from the cameras (urban intersection scale). The outputs of the proposed network are full-body 3-D meshes represented in skinned multiperson linear model parameters. The proposed approach relies on a novel objective function that incorporates the periodicity of human walking (gait), the mirror symmetry of the human body, and the change of ground reaction forces in a human gait cycle. This letter presents prediction results on the PedX dataset, a large-scale, in-the-wild data set collected at real urban intersections with heavy pedestrian traffic. Results show that the proposed network can successfully learn the characteristics of pedestrian gait and produce accurate and consistent 3-D pose predictions.","","","10.1109/LRA.2019.2895266","Ford-UM Alliance; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8626436","Deep learning in robotics and automation;gesture;posture and facial expressions;kinematics;long short-term memory (LSTM);pedestrian gait prediction","Three-dimensional displays;Legged locomotion;Biomechanics;Skeleton;Solid modeling;Biological system modeling;Shape","collision avoidance;gait analysis;image motion analysis;learning (artificial intelligence);pedestrians;pose estimation;recurrent neural nets","urban intersections;PedX dataset;ride safety;gait prediction;3-D pedestrian pose;biomechanically inspired recurrent neural network;bio-LSTM;3-D pose predictions;pedestrian gait;heavy pedestrian traffic;human gait cycle;human body;human walking;skinned multiperson linear model parameters;full-body 3-D meshes;cameras;multiple pedestrians;global locations;global coordinate frame;articulated body","","","56","","","","","IEEE","IEEE Journals"
"A New Framework for CNN-Based Speech Enhancement in the Time Domain","A. Pandey; D. Wang","Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering and the Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","7","1179","1188","This paper proposes a new learning mechanism for a fully convolutional neural network (CNN) to address speech enhancement in the time domain. The CNN takes as input the time frames of noisy utterance and outputs the time frames of the enhanced utterance. At the training time, we add an extra operation that converts the time domain to the frequency domain. This conversion corresponds to simple matrix multiplication, and is hence differentiable implying that a frequency domain loss can be used for training in the time domain. We use mean absolute error loss between the enhanced short-time Fourier transform (STFT) magnitude and the clean STFT magnitude to train the CNN. This way, the model can exploit the domain knowledge of converting a signal to the frequency domain for analysis. Moreover, this approach avoids the well-known invalid STFT problem since the proposed CNN operates in the time domain. Experimental results demonstrate that the proposed method substantially outperforms the other methods of speech enhancement. The proposed method is easy to implement and applicable to related speech processing tasks that require time-frequency masking or spectral mapping.","","","10.1109/TASLP.2019.2913512","National Institute on Deafness and Other Communication Disorders; Ohio Supercomputer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8701652","Speech enhancement;fully convolutional neural network;time domain enhancement;deep learning;mean absolute error","Time-domain analysis;Speech enhancement;Frequency-domain analysis;Convolution;Noise measurement;Convolutional neural networks;Task analysis","convolutional neural nets;Fourier transforms;learning (artificial intelligence);speech enhancement","time domain;short-time Fourier transform magnitude;convolutional neural network;noisy utterance;mean absolute error loss;CNN-based speech enhancement;time-frequency masking;frequency domain loss","","3","43","","","","","IEEE","IEEE Journals"
"Improved ArtGAN for Conditional Synthesis of Natural Image and Artwork","W. R. Tan; C. S. Chan; H. E. Aguirre; K. Tanaka","Shinshu University, Nagano, Japan; Center of Image and Signal Processing, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Shinshu University, Nagano, Japan; Shinshu University, Nagano, Japan","IEEE Transactions on Image Processing","","2019","28","1","394","409","This paper proposes a series of new approaches to improve generative adversarial network (GAN) for conditional image synthesis and we name the proposed model as “ArtGAN. ” One of the key innovation of ArtGAN is that, the gradient of the loss function w.r.t. the label (randomly assigned to each generated image) is back-propagated from the categorical discriminator to the generator. With the feedback from the label information, the generator is able to learn more efficiently and generate image with better quality. Inspired by recent works, an autoencoder is incorporated into the categorical discriminator for additional complementary information. Last but not least, we introduce a novel strategy to improve the image quality. In the experiments, we evaluate ArtGAN on CIFAR-10 and STL-10 via ablation studies. The empirical results showed that our proposed model outperforms the state-of-the-art results on CIFAR-10 in terms of Inception score. Qualitatively, we demonstrate that ArtGAN is able to generate plausible-looking images on Oxford-102 and CUB-200, as well as able to draw realistic artworks based on style, artist, and genre. The source code and models are available at: https://github.com/cs-chan/ArtGAN.","","","10.1109/TIP.2018.2866698","Fundamental Research Grant Scheme (FRGS) MoHE from the Ministry of Education Malaysia; UM Frontier Research from University of Malaya; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444471","Generative adversarial networks;deep learning;image synthesis;artwork synthesis;ArtGAN","Gallium nitride;Generators;Image generation;Image quality;Generative adversarial networks;Training;Image resolution","art;image processing;learning (artificial intelligence)","ArtGAN;CIFAR-10;conditional synthesis;natural image;generative adversarial network;image quality;image synthesis;STL-10","","1","65","","","","","IEEE","IEEE Journals"
"Artificial Intelligence Inspired Transmission Scheduling in Cognitive Vehicular Communications and Networks","K. Zhang; S. Leng; X. Peng; L. Pan; S. Maharjan; Y. Zhang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information Science and Engineering, Hunan Institute of Science and Technology, Yueyang, China; School of Information Science and Engineering, Hunan Institute of Science and Technology, Yueyang, China; Simula Metropolitan Center for Digital Engineering, Fornebu, Norway; Department of Informatics, University of Oslo, Oslo, Norway","IEEE Internet of Things Journal","","2019","6","2","1987","1997","The Internet of Things (IoT) platform has played a significant role in improving road transport safety and efficiency by ubiquitously connecting intelligent vehicles through wireless communications. Such an IoT paradigm however, brings in considerable strain on limited spectrum resources due to the need of continuous communication and monitoring. Cognitive radio (CR) is a potential approach to alleviate the spectrum scarcity problem through opportunistic exploitation of the underutilized spectrum. However, highly dynamic topology and time-varying spectrum states in CR-based vehicular networks introduce quite a few challenges to be addressed. Moreover, a variety of vehicular communication modes, such as vehicle-to-infrastructure and vehicle-to-vehicle, as well as data QoS requirements pose critical issues on efficient transmission scheduling. Based on this motivation, in this paper, we adopt a deep Q -learning approach for designing an optimal data transmission scheduling scheme in cognitive vehicular networks to minimize transmission costs while also fully utilizing various communication modes and resources. Furthermore, we investigate the characteristics of communication modes and spectrum resources chosen by vehicles in different network states, and propose an efficient learning algorithm for obtaining the optimal scheduling strategies. Numerical results are presented to illustrate the performance of the proposed scheduling schemes.","","","10.1109/JIOT.2018.2872013","Fundamental Research Funds for the Central Universities, China; National Natural Science Foundation of China; Joint Fund of the Ministry of Education of China and China Mobile; Natural Science Foundation of Hunan Province; National Natural Science Foundation of China; 13th Five-Years Plan of Education Science Program of Hunan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8471165","Cognitive radio (CR);Q-learning;transmission scheduling;vehicular communication","Optimal scheduling;Roads;Data communication;Wireless communication;Vehicle dynamics;Delays;Scheduling","artificial intelligence;cognitive radio;Internet of Things;learning (artificial intelligence);quality of service;road safety;vehicular ad hoc networks","vehicular communication modes;vehicle-to-infrastructure;vehicle-to-vehicle;efficient transmission scheduling;optimal data transmission scheduling scheme;cognitive vehicular networks;transmission costs;spectrum resources;efficient learning algorithm;optimal scheduling strategies;artificial intelligence inspired transmission scheduling;road transport safety;intelligent vehicles;wireless communications;IoT paradigm;monitoring;cognitive radio;spectrum scarcity problem;opportunistic exploitation;underutilized spectrum;highly dynamic topology;time-varying spectrum states;CR-based vehicular networks;network states;Internet of Things platform","","6","29","","","","","IEEE","IEEE Journals"
"Multi-View Linear Discriminant Analysis Network","P. Hu; D. Peng; Y. Sang; Y. Xiang","Machine Intelligence Laboratory, College of Computer Science, Sichuan University, Chengdu, China; Machine Intelligence Laboratory, College of Computer Science, Sichuan University, Chengdu, China; School of Information Technology, Deakin University, VIC, Australia; Machine Intelligence Laboratory, College of Computer Science, Sichuan University, Chengdu, China","IEEE Transactions on Image Processing","","2019","28","11","5352","5365","In many real-world applications, an object can be described from multiple views or styles, leading to the emerging multi-view analysis. To eliminate the complicated (usually highly nonlinear) view discrepancy for favorable cross-view recognition and retrieval, we propose a Multi-view Linear Discriminant Analysis Network (MvLDAN) by seeking a nonlinear discriminant and view-invariant representation shared among multiple views. Unlike existing multi-view methods which directly learn a common space to reduce the view gap, our MvLDAN employs multiple feedforward neural networks (one for each view) and a novel eigenvalue-based multi-view objective function to encapsulate as much discriminative variance as possible into all the available common feature dimensions. With the proposed objective function, the MvLDAN could produce representations possessing: 1) low variance within the same class regardless of view discrepancy, 2) high variance between different classes regardless of view discrepancy, and 3) high covariance between any two views. In brief, in the learned multi-view space, the obtained deep features can be projected into a latent common space in which the samples from the same class are as close to each other as possible (even though they are from different views), and the samples from different classes are as far from each other as possible (even though they are from the same view). The effectiveness of the proposed method is verified by extensive experiments carried out on five databases, in comparison with the 19 state-of-the-art approaches.","","","10.1109/TIP.2019.2913511","National Key Research and Development Project of China; National Natural Science Foundation of China; Sichuan Science and Technology Planning Projects; Sichuan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704986","Heterogeneous recognition;cross-view retrieval;latent common space;multi-view linear discriminant analysis network;multi-view representation learning","Correlation;Linear programming;Feature extraction;Kernel;Neural networks;Image reconstruction;Linear discriminant analysis","eigenvalues and eigenfunctions;feedforward neural nets;image recognition;image representation;image retrieval;learning (artificial intelligence)","MvLDAN;view-invariant representation;multiview methods;view gap;multiple feedforward neural networks;learned multiview space;multiview analysis;view discrepancy;eigenvalue-based multiview objective function;multiview linear discriminant analysis network;cross-view recognition;nonlinear discriminant representation;cross-view retrieval","","1","69","","","","","IEEE","IEEE Journals"
"Self-Supervised Optical Flow Estimation by Projective Bootstrap","S. Alletto; D. Abati; S. Calderara; R. Cucchiara; L. Rigazio","Enzo Ferrari Department of Engineering, University of Modena and Reggio Emilia, Modena, Italy; Enzo Ferrari Department of Engineering, University of Modena and Reggio Emilia, Modena, Italy; Enzo Ferrari Department of Engineering, University of Modena and Reggio Emilia, Modena, Italy; Enzo Ferrari Department of Engineering, University of Modena and Reggio Emilia, Modena, Italy; Panasonic Beta, Mountain View, CA, USA","IEEE Transactions on Intelligent Transportation Systems","","2019","20","9","3294","3302","Dense optical flow estimation is complex and time consuming, with state-of-the-art methods relying either on large synthetic data sets or on pipelines requiring up to a few minutes per frame pair. In this paper, we address the problem of optical flow estimation in the automotive scenario in a self-supervised manner. We argue that optical flow can be cast as a geometrical warping between two successive video frames and devise a deep architecture to estimate such transformation in two stages. First, a dense pixel-level flow is computed with a projective bootstrap on rigid surfaces. We show how such global transformation can be approximated with a homography and extend spatial transformer layers so that they can be employed to compute the flow field implied by such transformation. Subsequently, we refine the prediction by feeding a second, deeper network that accounts for moving objects. A final reconstruction loss compares the warping of frame Xt with the subsequent frame Xt+1 and guides both estimates. The model has the speed advantages of end-to-end deep architectures while achieving competitive performances, both outperforming recent unsupervised methods and showing good generalization capabilities on new automotive data sets.","","","10.1109/TITS.2018.2873980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8506466","Computer vision;image motion analysis;unsupervised learning","Optical imaging;Estimation;Adaptive optics;Optical sensors;Automotive engineering;Cameras;Computer architecture","image motion analysis;image reconstruction;image sequences;unsupervised learning;video signal processing","self-supervised optical flow estimation;projective bootstrap;geometrical warping;deep architecture;dense pixel-level flow;unsupervised methods;video frames","","","35","","","","","IEEE","IEEE Journals"
"Understanding Patients’ Behavior: Vision-Based Analysis of Seizure Disorders","D. Ahmedt-Aristizabal; S. Denman; K. Nguyen; S. Sridharan; S. Dionisio; C. Fookes","Image and Video Research Laboratory, SAIVT, Queensland University of Technology, Brisbane City, QLD, Australia; Image and Video Research Laboratory, SAIVT, Queensland University of Technology, Brisbane City, QLD, Australia; Image and Video Research Laboratory, SAIVT, Queensland University of Technology, Brisbane City, QLD, Australia; Image and Video Research Laboratory, SAIVT, Queensland University of Technology, Brisbane City, QLD, Australia; Department of Mater Advanced Epilepsy Unit, Mater Centre for Neurosciences, South Brisbane, QLD, Australia; Image and Video Research Laboratory, SAIVT, Queensland University of Technology, Brisbane City, QLD, Australia","IEEE Journal of Biomedical and Health Informatics","","2019","23","6","2583","2591","A substantial proportion of patients with functional neurological disorders (FND) are being incorrectly diagnosed with epilepsy because their semiology resembles that of epileptic seizures (ES). Misdiagnosis may lead to unnecessary treatment and its associated complications. Diagnostic errors often result from an overreliance on specific clinical features. Furthermore, the lack of electrophysiological changes in patients with FND can also be seen in some forms of epilepsy, making diagnosis extremely challenging. Therefore, understanding semiology is an essential step for differentiating between ES and FND. Existing sensor-based and marker-based systems require physical contact with the body and are vulnerable to clinical situations such as patient positions, illumination changes, and motion discontinuities. Computer vision and deep learning are advancing to overcome these limitations encountered in the assessment of diseases and patient monitoring; however, they have not been investigated for seizure disorder scenarios. Here, we propose and compare two marker-free deep learning models, a landmark-based and a region-based model, both of which are capable of distinguishing between seizures from video recordings. We quantify semiology by using either a fusion of reference points and flow fields, or through the complete analysis of the body. Average leave-one-subject-out cross-validation accuracies for the landmark-based and region-based approaches of 68.1% and 79.6% in our dataset collected from 35 patients, reveal the benefit of video analytics to support automated identification of semiology in the challenging conditions of a hospital setting.","","","10.1109/JBHI.2019.2895855","Mater Hospital Brisbane and Mater Centre for Neuroscience; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629065","Epileptic seizures;functional neurological disorder;quantitative movement analysis","Feature extraction;Epilepsy;Semiotics;Cameras;Deep learning;Computer architecture;Monitoring","","","","","52","Traditional","","","","IEEE","IEEE Journals"
"HSI-DeNet: Hyperspectral Image Restoration via Convolutional Neural Network","Y. Chang; L. Yan; H. Fang; S. Zhong; W. Liao","National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; School of Software, Xidian University, Xi’an, China; National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","2","667","682","The spectral and the spatial information in hyperspectral images (HSIs) are the two sides of the same coin. How to jointly model them is the key issue for HSIs' noise removal, including random noise, structural stripe noise, and dead pixels/lines. In this paper, we introduce the deep convolutional neural network (CNN) to achieve this goal. The learned filters can well extract the spatial information within their local receptive filed. Meanwhile, the spectral correlation can be depicted by the multiple channels of the learned 2-D filters, namely, the number of filters in each layer. The consequent advantages of our CNN-based HSI denoising method (HSI-DeNet) over previous methods are threefold. First, the proposed HSI-DeNet can be regarded as a tensor-based method by directly learning the filters in each layer without damaging the spectral-spatial structures. Second, the HSI-DeNet can simultaneously accommodate various kinds of noise in HSIs. Moreover, our method is flexible for both single image and multiple images by slightly modifying the channels of the filters in the first and last layers. Last but not least, our method is extremely fast in the testing phase, which makes it more practical for real application. The proposed HSI-DeNet is extensively evaluated on several HSIs, and outperforms the state-of-the-art HSI-DeNets in terms of both speed and performance.","","","10.1109/TGRS.2018.2859203","National Natural Science Foundation of China; Natural Science Foundation of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8435923","Convolutional neural network (CNN);denoising;destriping;hyperspectral image (HSI) restoration","Image restoration;Correlation;Data models;Noise reduction;Tensile stress;Convolution;Task analysis","convolutional neural nets;hyperspectral imaging;image denoising;image filtering;learning (artificial intelligence);random noise;tensors","HSI noise removal;spectral information;dead pixels-lines;spectral correlation;learned 2D filters;tensor-based method;CNN-based HSI denoising method;deep convolutional neural network;structural stripe noise;random noise;spatial information;hyperspectral image restoration;HSI-DeNet;spectral-spatial structures","","4","78","","","","","IEEE","IEEE Journals"
"Content-Aware Convolutional Neural Network for In-Loop Filtering in High Efficiency Video Coding","C. Jia; S. Wang; X. Zhang; S. Wang; J. Liu; S. Pu; S. Ma","Institute of Digital Media, Peking University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong; School of Computer and Control Engineering, University of the Chinese Academy of Sciences, Beijing, China; Institute of Digital Media, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China; Hikvision Research Institute, Hangzhou, China; Institute of Digital Media, Peking University, Beijing, China","IEEE Transactions on Image Processing","","2019","28","7","3343","3356","Recently, convolutional neural network (CNN) has attracted tremendous attention and has achieved great success in many image processing tasks. In this paper, we focus on CNN technology combined with image restoration to facilitate video coding performance and propose the content-aware CNN based in-loop filtering for high-efficiency video coding (HEVC). In particular, we quantitatively analyze the structure of the proposed CNN model from multiple dimensions to make the model interpretable and optimal for CNN-based loop filtering. More specifically, each coding tree unit (CTU) is treated as an independent region for processing, such that the proposed content-aware multimodel filtering mechanism is realized by the restoration of different regions with different CNN models under the guidance of the discriminative network. To adapt the image content, the discriminative neural network is learned to analyze the content characteristics of each region for the adaptive selection of the deep learning model. The CTU level control is also enabled in the sense of rate-distortion optimization. To learn the CNN model, an iterative training method is proposed by simultaneously labeling filter categories at the CTU level and fine-tuning the CNN model parameters. The CNN based in-loop filter is implemented after sample adaptive offset in HEVC, and extensive experiments show that the proposed approach significantly improves the coding performance and achieves up to 10.0% bit-rate reduction. On average, 4.1%, 6.0%, 4.7%, and 6.0% bit-rate reduction can be obtained under all intra, low delay, low delay P, and random access configurations, respectively.","","","10.1109/TIP.2019.2896489","National Natural Science Foundation of China; National Basic Research Program of China (973 Program); Peking University; City University of Hong Kong; National Natural Science Foundation of China; Natural Science Foundation of Beijing Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630681","High-efficiency video coding (HEVC);in-loop filter;convolutional neural network","Video coding;Image coding;Image restoration;Adaptation models;Image reconstruction;Analytical models;Encoding","convolutional neural nets;image filtering;image restoration;learning (artificial intelligence);optimisation;ubiquitous computing;video coding","coding tree unit;independent region;content-aware multimodel filtering mechanism;discriminative network;image content;discriminative neural network;content characteristics;deep learning model;CTU level control;filter categories;CNN model parameters;in-loop filter;HEVC;content-aware convolutional neural network;high efficiency video coding;image processing tasks;CNN technology;image restoration;video coding performance;content-aware CNN;in-loop filtering;CNN-based loop filtering;bit-rate reduction;CNN models;rate-distortion optimization","","1","60","","","","","IEEE","IEEE Journals"
"Discarding jagged artefacts in image upscaling with total variation regularisation","J. Xu; M. Li; J. Fan; W. Xie","Xi'an University of Posts and Telecommunications, The School of Communications and Information Engineering, Western Changan Road, Changan Region, Xi'an 710121, People's Republic of China; Xi'an University of Posts and Telecommunications, The School of Communications and Information Engineering, Western Changan Road, Changan Region, Xi'an 710121, People's Republic of China; Xi'an University of Posts and Telecommunications, The School of Communications and Information Engineering, Western Changan Road, Changan Region, Xi'an 710121, People's Republic of China; Xi'an University of Posts and Telecommunications, The School of Communications and Information Engineering, Western Changan Road, Changan Region, Xi'an 710121, People's Republic of China","IET Image Processing","","2019","13","13","2495","2506","Image upscaling is needed in many areas. There are two types of methods: methods based on a simple hypothesis and methods based on machine learning. Most of the machine learning-based methods have disadvantages: no support is provided for a variety of upscaling factors, a training process with a high time cost is required, and a large amount of storage space and high-end equipment are required. To avoid the disadvantages of machine learning, upscaling images with a simple hypothesis is a promising strategy but simple hypothesis always produces jaggy artifacts. The authors propose a new method to remove these jagged artifacts. They consider an edge in an image as a deformed curve. Removing jagged artefacts is considered equivalent to shortening the full arc length of a curve. By optimising the regularization model, the severity of the artifacts decreases as the number of iterations increases. They compare nine existing methods on the Set5, Set14, and Urban100 datasets. Without using any external data, the proposed algorithm has high visual quality, has few jagged artefacts and performs similarly to very recent state-of-the-art deep convolutional network-based approaches. Compared to other methods without external data, the proposed algorithm balances the quality and time cost well.","","","10.1049/iet-ipr.2018.6494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911570","","","image reconstruction;image enhancement;image sequences;image resolution;neural nets;iterative methods;learning (artificial intelligence)","jagged artefacts;recent state-of-the-art deep convolutional network-based approaches;image upscaling;total variation regularisation;image-upscaling methods;simple hypothesis;machine learning;upscaling factors;high time cost;high-end equipment;image enhancement","","","53","","","","","IET","IET Journals"
"Divide and Count: Generic Object Counting by Image Divisions","T. Stahl; S. L. Pintea; J. C. van Gemert","University of Amsterdam, Amsterdam, The Netherlands; Computer Vision Lab, Delft University of Technology, Delft, The Netherlands; Computer Vision Lab, Delft University of Technology, Delft, The Netherlands","IEEE Transactions on Image Processing","","2019","28","2","1035","1044","We propose a general object counting method that does not use any prior category information. We learn from local image divisions to predict global image-level counts without using any form of local annotations. Our method separates the input image into a set of image divisions-each fully covering the image. Each image division is composed of a set of region proposals or uniform grid cells. Our approach learns in an end-to-end deep learning architecture to predict global image-level counts from local image divisions. The method incorporates a counting layer which predicts object counts in the complete image, by enforcing consistency in counts when dealing with overlapping image regions. Our counting layer is based on the inclusion-exclusion principle from set theory. We analyze the individual building blocks of our proposed approach on Pascal-VOC2007 and evaluate our method on the MS-COCO large scale generic object data set as well as on three class-specific counting data sets: UCSD pedestrian data set, and CARPK, and PUCPR+ car data sets.","","","10.1109/TIP.2018.2875353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8488575","Generic-class object counting;inclusion-exclusion principle;regression;fully convolutional networks;counting with region proposals","Proposals;Computer architecture;Task analysis;Automobiles;Animals;Object detection","image annotation;image segmentation;learning (artificial intelligence);object detection;pedestrians;regression analysis;set theory","end-to-end deep learning architecture;global image-level counts;local image divisions;counting layer;overlapping image regions;class-specific counting data sets;generic object counting;region proposals;PUCPR car data set;CARPK;UCSD pedestrian data set;MS-COCO large scale generic object data set;Pascal-VOC2007;set theory;inclusion-exclusion principle;uniform grid cells;local annotations","","","52","","","","","IEEE","IEEE Journals"
"Domain Adaptation With Discriminative Distribution and Manifold Embedding for Hyperspectral Image Classification","Z. Wang; B. Du; Q. Shi; W. Tu","School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; National Engineering Research Center for Multimedia Software and School of Computer Science, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","7","1155","1159","Hyperspectral remote sensing image classification has drawn a great attention in recent years due to the development of remote sensing technology. To build a high confident classifier, the large number of labeled data is very important, e.g., the success of deep learning technique. Indeed, the acquisition of labeled data is usually very expensive, especially for the remote sensing images, which usually needs to survey outside. To address this problem, in this letter, we propose a domain adaptation method by learning the manifold embedding and matching the discriminative distribution in source domain with neural networks for hyperspectral image classification. Specifically, we use the discriminative information of source image to train the classifier for the source and target images. To make the classifier can work well on both domains, we minimize the distribution shift between the two domains in an embedding space with prior class distribution in the source domain. Meanwhile, to avoid the distortion mapping of the target domain in the embedding space, we try to keep the manifold relation of the samples in the embedding space. Then, we learn the embedding on source domain and target domain by minimizing the three criteria simultaneously based on a neural network. The experimental results on two hyperspectral remote sensing images have shown that our proposed method can outperform several baseline methods.","","","10.1109/LGRS.2018.2889967","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8610281","Domain adaptation;hyperspectral image classification;manifold embedding;maximum mean discrepancy (MMD);neural network;remote sensing","Manifolds;Hyperspectral imaging;Neural networks;Distortion;Kernel","geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;learning (artificial intelligence);remote sensing","discriminative distribution;manifold embedding;hyperspectral remote sensing image classification;remote sensing technology;high confident classifier;deep learning technique;domain adaptation method;source domain;neural network;discriminative information;source image;target images;distribution shift;embedding space;prior class distribution;target domain;manifold relation;hyperspectral remote sensing images;data acquisition;distortion mapping","","","26","","","","","IEEE","IEEE Journals"
"Estimating Sea Ice Concentration From SAR: Training Convolutional Neural Networks With Passive Microwave Data","C. L. V. Cooke; K. A. Scott","Department of Mechanical and Mechatronics Engineering, University of Waterloo, Waterloo, Canada; Department of Systems Design Engineering, University of Waterloo, Waterloo, Canada","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","7","4735","4747","Historically, sea ice concentration (SIC) has been measured through the use of passive microwave sensors, as well as human interpretation of synthetic aperture radar (SAR). Although passive microwave data are processed automatically, it suffers from poor spatial resolution and the higher frequency channels are sensitive to weather conditions. Deep learning has demonstrated its ability to perform complex and accurate analysis of images; here, we apply deep learning to estimate ice concentration from SAR scenes. We developed a deep convolutional neural network (CNN) that predicts SIC from SAR, trained upon passive microwave data. The model achieves a 5.24%/7.87% error on its train and test set, respectively. To assess the real-world applicability, we performed an independent validation on 18 SAR scenes (from two distinct geographical regions), not previously seen during training or test. Comparing against human-generated ice analysis charts, we achieved an L1 error of 0.2059, competitive with passive microwave (EL1 = 0.1863) for the Canadian Arctic Archipelago. For the Gulf of Saint Lawrence region, we achieved an L1 error of 0.2653, significantly better than the passive microwave result (EL1 = 0.3593). By using novel techniques for model training, as well as training entirely upon passive microwave data, we present an accessible and robust method of developing similar systems for processing SAR.1 Our results suggest that with further postprocessing, CNNs are accurate and robust enough to be used for operational tasks.","","","10.1109/TGRS.2019.2892723","Marine Environmental Observation Prediction and Response Network; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636521","Image processing;neural networks;remote sensing;sea ice;synthetic aperture radar (SAR)","Synthetic aperture radar;Microwave FET integrated circuits;Microwave imaging;Microwave integrated circuits;Silicon carbide;Sea ice","geophysical image processing;image classification;neural nets;oceanographic techniques;radar imaging;remote sensing by radar;sea ice;synthetic aperture radar","sea ice concentration;training convolutional neural networks;passive microwave data;passive microwave sensors;deep learning;deep convolutional neural network;human-generated ice analysis charts;processing SAR;SAR scenes;weather conditions;Canadian Arctic Archipelago;Gulf of Saint Lawrence region","","1","37","","","","","IEEE","IEEE Journals"
"BLTRCNN-Based 3-D Articulatory Movement Prediction: Learning Articulatory Synchronicity From Both Text and Audio Inputs","L. Yu; J. Yu; Q. Ling","Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China","IEEE Transactions on Multimedia","","2019","21","7","1621","1632","Predicting articulatory movements from audio or text has diverse applications, such as speech visualization. Various approaches have been proposed to solve the acoustic-articulatory mapping problem. However, their precision is not high enough with only acoustic features available. Recently, deep neural network (DNN) has brought tremendous success in various fields, like speech recognition and image processing. To increase the accuracy, we propose a new network architecture for articulatory movement prediction with both text and audio inputs, called a bottleneck long-term recurrent convolutional neural network (BLTRCNN). To the best of our knowledge, it is the first time to predict articulatory movements based on DNN by fusing text and audio inputs. Our BLTRCNN consists of two networks. The first is the bottleneck network, generating a compact bottleneck features of text information for each frame independently. The second, including convolutional neural network, long short-term memory and skip connection, is called the long-term recurrent convolutional neural network (LTRCNN). LTRCNN is used for articulatory movement prediction when bottleneck features, acoustic features, and text features are integrated as inputs together. Experiments show that the proposed BLTRCNN achieves the state-of-the-art root-mean-square error (RMSE) 0.528 mm and the correlation coefficient 0.961. Moreover, we also demonstrate how text information complements acoustic features in this prediction task.","","","10.1109/TMM.2018.2887027","National Natural Science Foundation of China; Anhui Provincial Natural Science Foundation; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8579230","Convolutional neural network;long short-term memory;bottleneck network;skip connection;articulatory movement prediction","Acoustics;Linguistics;Feature extraction;Three-dimensional displays;Animation;Convolutional neural networks;Visualization","convolutional neural nets;mean square error methods;recurrent neural nets;speech processing","articulatory movement prediction;articulatory synchronicity;acoustic-articulatory mapping problem;deep neural network;bottleneck long-term recurrent convolutional neural network;compact bottleneck features;text information complements acoustic features;3D articulatory movement prediction;BLTRCNN;learning articulatory synchronicity;speech visualization;DNN;speech recognition;image processing;LTRCNN;long-term recurrent convolutional neural network;root-mean-square error;RMSE","","1","49","","","","","IEEE","IEEE Journals"
"Character Recognition in Air-Writing Based on Network of Radars for Human-Machine Interface","M. Arsalan; A. Santra","Infineon Technologies AG, Neubiberg, Germany; Infineon Technologies AG, Neubiberg, Germany","IEEE Sensors Journal","","2019","19","19","8855","8864","Radar technology plays a vital role in contact-less detection of hand gestures or motions, which forms an alternate and intuitive form of human-computer interface. Air-writing refers to the writing of linguistic characters or words in free space by hand gesture movements. In this paper, we propose an air-writing system based on a network of millimeter wave radars. We propose a two-stage approach for extraction and recognition of handwriting gestures. The extraction processing stage uses a fine range estimate combined with the trilateration technique to detect and localize the hand marker, followed by a smoothening filter to create a trajectory of the character through the hand movement. For the recognition stage, we explore two approaches: one extracts the time-series trajectory data and recognizes the drawn character using long short term memory (LSTM), bi-directional LSTM (BLSTM), and convolutional LSTM (ConvLSTM) with connectionist temporal classification (CTC) loss function, and the other approach reconstructs a 2D image from the trajectory of drawn character and uses deep convolutional neural network (DCNN) to classify the alphabets drawn by the user. ConvLSTM-CTC performs best among LSTM variants on time-series trajectory data achieving 98.33% classification accuracy similar to DCNN over the chosen character set. This paper employs real data using a network of three 60-GHz millimeter wave radar sensor to demonstrate the success of the proposed setup and associated algorithm with design consideration.","","","10.1109/JSEN.2019.2922395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735896","60 GHz mm-wave radar;human-machine interface;gesture recognition;sensing;network of radars;long short term memory;deep convolutional neural network;convolutional LSTM;connectionist temporal classification","Sensors;Trajectory;Radar antennas;Radar cross-sections;Writing;Chirp","convolutional neural nets;feature extraction;gesture recognition;handwritten character recognition;human computer interaction;image classification;image filtering;learning (artificial intelligence);millimetre wave radar;object detection;radar detection;radar receivers;smoothing methods;time series","character recognition;human-machine interface;radar technology;contact-less detection;linguistic characters;hand gesture movements;air-writing system;millimeter wave radars;handwriting gestures;extraction processing stage;trilateration technique;hand marker;time-series trajectory data;drawn character;convolutional LSTM;connectionist temporal classification loss function;deep convolutional neural network;human-computer interface;millimeter wave radar sensor;ConvLSTM-CTC;bidirectional LSTM;BLSTM;long short term memory;DCNN;2D image;smoothening filter;frequency 60 GHz","","","20","","","","","IEEE","IEEE Journals"
"An Effective Hybrid Model for EEG-Based Drowsiness Detection","U. Budak; V. Bajaj; Y. Akbulut; O. Atila; A. Sengur","Electrical and Electronics Engineering Department, Engineering Faculty, Bitlis Eren University, Bitlis, Turkey; Electronics and Communication Discipline, Indian Institute of Information Technology Design and Manufacturing at Jabalpur, Jabalpur, India; Informatics Department, Firat University, Elaziǧ, Turkey; Electrical and Electronics Engineering Department, Technology Faculty, Firat University, Elaziǧ, Turkey; Electrical and Electronics Engineering Department, Technology Faculty, Firat University, Elaziǧ, Turkey","IEEE Sensors Journal","","2019","19","17","7624","7631","Early detection of driver drowsiness and the development of a functioning driver alertness system may support the prevention of numerous vehicular accidents worldwide. Wearable sensors and camera-based systems are generally employed in the driver drowsiness detection. Electroencephalogram (or EEG) is considered another effective option for the driver drowsiness detection. Various EEG-based drowsiness detection systems have been proposed to date. In this paper, EEG signals are also used for the detection of drowsiness, with the proposed method being composed of three main building blocks. Both raw EEG signals and their corresponding spectrograms are used in the proposed building blocks. In the first building block, while energy distribution and zero-crossing distribution features are calculated from the raw EEG signals, spectral entropy and instantaneous frequency features are extracted from the EEG spectrogram images. In the second building block, deep feature extraction is employed directly on the EEG spectrogram images using pre-trained AlexNet and VGGNet. In the third building block, the tunable Q-factor wavelet transform (TQWT) is used to decompose the EEG signals into related sub-bands. The spectrogram images of the obtained sub-bands and statistical features, such as mean and standard deviation of the sub-bands' instantaneous frequencies, are then calculated. Each feature group from each building block is fed to a long-short term memory (LSTM) network for the purposes of classification. The obtained results from the LSTM networks are then fused with a majority voting layer. The MIT-BIH Polysomnographic database was used in the experimental works. The evaluation of the proposed method was carried out with ten-fold cross validation test and the average accuracy represented accordingly. The obtained average accuracy score was 94.31%. The obtained result was also compared with other results to be found in the literature. The comparison shows that the proposed method's achievement was found to be better than the compared results.","","","10.1109/JSEN.2019.2917850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718312","Drowsiness detection;EEG signals;signal processing;deep feature extraction;LSTM network","Electroencephalography;Feature extraction;Vehicles;Spectrogram;Wavelet transforms;Databases","driver information systems;electroencephalography;entropy;feature extraction;image fusion;learning (artificial intelligence);medical image processing;neural nets;Q-factor;road accidents;sleep;statistical analysis;wavelet transforms","EEG-based drowsiness detection;camera-based systems;driver drowsiness detection;drowsiness detection systems;raw EEG signals;zero-crossing distribution features;EEG spectrogram images;deep feature extraction;building blocks;energy distribution;driver alertness system;tunable Q-factor wavelet transform;TQWT;statistical features;sub-bands;pretrained AlexNet;vehicular accidents;pretrained VGGNet;MIT-BIH Polysomnographic database;long-short term memory network","","2","36","","","","","IEEE","IEEE Journals"
"Single-Image Reflection Removal via a Two-Stage Background Recovery Process","T. Li; D. P. K. Lun","Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong; Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong","IEEE Signal Processing Letters","","2019","26","8","1237","1241","The reflection problem often occurs when imaging through a semitransparent material such as glass. It degrades the image quality and affects the subsequent analyses on the image. Traditional single-image based reflection removal methods assume the reflection is blurry. Deep neural networks (DNNs) are, then, used to identify the blurry reflection and remove it. However, it is often that the blurry reflection still contains strong edges. They will be treated as the background and kept in the image. In this letter, we propose a novel two-stage DNN based reflection removal algorithm. In the first stage, we include a new feature reduction term in the loss function when training the network. Due to its strong reflection suppression ability, the reflection components in the image can be more effectively suppressed. However, it will also attenuate the gradient values of the background image. For recovering the background, in the second stage, we first estimate a reflection gradient confidence map based on the initial estimation result and use it to identify the strong background gradients. Then, we use a generative adversarial network to reconstruct the background image from its gradients. Experimental results show that the proposed two-stage approach can give a superior performance compared with the state-of-the-art DNN based methods.","","","10.1109/LSP.2019.2926828","The Hong Kong Polytechnic University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8755471","Image reflection removal;blind image separation;deep neural network","Estimation;Neural networks;Training;Generative adversarial networks;Image reconstruction;Erbium;Image edge detection","edge detection;gradient methods;image restoration;learning (artificial intelligence);neural nets","two-stage background recovery process;image quality;deep neural networks;blurry reflection;reflection gradient confidence map;single-image reflection removal;background image reconstruction;two-stage DNN;feature reduction;generative adversarial network","","","20","Traditional","","","","IEEE","IEEE Journals"
"Exploring Joint AB-LSTM With Embedded Lemmas for Adverse Drug Reaction Discovery","S. Santiso; A. Pérez; A. Casillas","IXA Group, University of the Basque Country (UPV-EHU), Donostia, Spain; IXA Group, University of the Basque Country (UPV-EHU), Donostia, Spain; IXA Group, University of the Basque Country (UPV-EHU), Donostia, Spain","IEEE Journal of Biomedical and Health Informatics","","2019","23","5","2148","2155","This work focuses on the detection of adverse drug reactions (ADRs) in electronic health records (EHRs) written in Spanish. The World Health Organization underlines the importance of reporting ADRs for patients' safety. The fact is that ADRs tend to be under-reported in daily hospital praxis. In this context, automatic solutions based on text mining can help to alleviate the workload of experts. Nevertheless, these solutions pose two challenges: 1) EHRs show high lexical variability, the characterization of the events must be able to deal with unseen words or contexts and 2) ADRs are rare events, hence, the system should be robust against skewed class distribution. To tackle these challenges, deep neural networks seem appropriate because they allow a high-level representation. Specifically, we opted for a joint AB-LSTM network, a sub-class of the bidirectional long short-term memory network. Besides, in an attempt to reinforce lexical variability, we proposed the use of embeddings created using lemmas. We compared this approach with supervised event extraction approaches based on either symbolic or dense representations. Experimental results showed that the joint AB-LSTM approach outperformed previous approaches, achieving an f-measure of 73.3.","","","10.1109/JBHI.2018.2879744","Nvidia; PROSAMED; DETEAMI; BERBAOLA; Predoctoral; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8523679","Adverse drug reactions;electronic health records;deep neural networks","Drugs;Diseases;Task analysis;Feature extraction;Informatics;Neural networks;Text mining","data mining;drug delivery systems;drugs;information retrieval;medical information systems;natural language processing;neural nets;supervised learning;text analysis","embedded lemmas;adverse drug reaction discovery;electronic health records;World Health Organization;daily hospital praxis;text mining;skewed class distribution;deep neural networks;joint AB-LSTM network;short-term memory network;supervised event extraction;patient safety;ADR;EHR","","","62","Traditional","","","","IEEE","IEEE Journals"
"A Lightweight Multi-Section CNN for Lung Nodule Classification and Malignancy Estimation","P. Sahu; D. Yu; M. Dasari; F. Hou; H. Qin","Department of Computer Science, Stony Brook University, Stony Brook, NY, USA; Martin Tuchman School of Management, New Jersey Institute of Technology, Newark, NJ, USA; Department of Computer Science, Stony Brook University, Stony Brook, NY, USA; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Science, Beijing, China; Department of Computer Science, Stony Brook University, Stony Brook, NY, USA","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","960","968","The size and shape of a nodule are the essential indicators of malignancy in lung cancer diagnosis. However, effectively capturing the nodule's structural information from CT scans in a computer-aided system is a challenging task. Unlike previous models that proposed computationally intensive deep ensemble models or three-dimensional CNN models, we propose a lightweight, multiple view sampling based multi-section CNN architecture. The model obtains a nodule's cross sections from multiple view angles and encodes the nodule's volumetric information into a compact representation by aggregating information from its different cross sections via a view pooling layer. The compact feature is subsequently used for the task of nodule classification. The method does not require the nodule's spatial annotation and works directly on the cross sections generated from volume enclosing the nodule. We evaluated the proposed method on lung image database consortium (LIDC) and image database resource initiative (IDRI) dataset. It achieved the state-of-the-art performance with a mean 93.18% classification accuracy. The architecture could also be used to select the representative cross sections determining the nodule's malignancy that facilitates in the interpretation of results. Because of being lightweight, the model could be ported to mobile devices, which brings the power of artificial intelligence (AI) driven application directly into the practitioner's hand.","","","10.1109/JBHI.2018.2879834","Brookhaven National Laboratory and National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8525322","Lung cancer;deep learning;nodule classification;transfer Learning;spherical sampling","Lung;Cancer;Three-dimensional displays;Training;Task analysis;Computational modeling;Feature extraction","cancer;computerised tomography;image classification;lung;medical image processing;visual databases","nodule's malignancy;image database resource initiative dataset;lung image database consortium;pooling layer;classification accuracy;compact feature;compact representation;multiple view angles;multisection CNN architecture;multiple view sampling;lightweight view sampling;three-dimensional CNN models;computer-aided system;CT scans;lung cancer diagnosis;malignancy estimation;lung nodule classification;lightweight multisection CNN","","2","31","","","","","IEEE","IEEE Journals"
"State-of-art analysis of image denoising methods using convolutional neural networks","R. S. Thakur; R. N. Yadav; L. Gupta","Maulana Azad National Institute of Technology, Department of Electronics and Communication Engineering, Bhopal, MP, India; Maulana Azad National Institute of Technology, Department of Electronics and Communication Engineering, Bhopal, MP, India; Maulana Azad National Institute of Technology, Department of Electronics and Communication Engineering, Bhopal, MP, India","IET Image Processing","","2019","13","13","2367","2380","Convolutional neural networks (CNNs) are deep neural networks that can be trained on large databases and show outstanding performance on object classification, segmentation, image denoising etc. In the past few years, several image denoising techniques have been developed to improve the quality of an image. The CNN based image denoising models have shown improvement in denoising performance as compared to non-CNN methods like block-matching and three-dimensional (3D) filtering, contemporary wavelet and Markov random field approaches etc. which had remained state-of-the-art for years. This study provides a comprehensive study of state-of-the-art image denoising methods using CNN. The literature associated with different CNNs used for image restoration like residual learning based models (DnCNN-S, DnCNN-B, IDCNN), non-locality reinforced (NN3D), fast and flexible network (FFDNet), deep shrinkage CNN (SCNN), a model for mixed noise reduction, denoising prior driven network (PDNN) are reviewed. DnCNN-S and PDNN remove Gaussian noise of fixed level, whereas DnCNN-B, IDCNN, NN3D and SCNN are used for blind Gaussian denoising. FFDNet is used for spatially variant Gaussian noise. The performance of these CNN models is analysed on BSD-68 and Set-12 datasets. PDNN shows the best result in terms of PSNR for both BSD-68 and Set-12 datasets.","","","10.1049/iet-ipr.2019.0157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911561","","","image denoising;learning (artificial intelligence);Gaussian noise;image segmentation;image restoration;convolutional neural nets","convolutional neural networks;deep neural networks;object classification;CNN based image denoising models;denoising performance;nonCNN methods;image restoration;residual learning based models;DnCNN-S;DnCNN-B;NN3D;prior driven network;PDNN;blind Gaussian denoising;CNN models;Markov random field approaches;block-matching;three-dimensional filtering;wavelet random field;object segmentation","","","72","","","","","IET","IET Journals"
"LDS-Inspired Residual Networks","A. Dimou; D. Ataloglou; K. Dimitropoulos; F. Alvarez; P. Daras","Señales, Sistemas y Radiocomunicaciones, Universidad Politécnica de Madrid, Madrid, Spain; Centre for Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece; Señales, Sistemas y Radiocomunicaciones, Universidad Politécnica de Madrid, Madrid, Spain; Centre for Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","8","2363","2375","Residual networks (ResNets) have introduced a milestone for the deep learning community due to their outstanding performance in diverse applications. They enable efficient training of increasingly deep networks, reducing the training difficulty and error. The main intuition behind them is that, instead of mapping the input information, they are mapping a residual part of it. Since the original work, a lot of extensions have been proposed to improve information mapping. In this paper, a novel extension of the residual block is proposed inspired by linear dynamical systems (LDSs), called LDS-ResNet. Specifically, a new module is presented that improves mapping of residual information by transforming it in a hidden state and then mapping it back to the desired feature space using convolutional layers. The proposed module is utilized to construct multi-branch residual blocks for convolutional neural networks. An exploration of possible architectural choices is presented and evaluated. Experimental results show that LDS-ResNet outperforms the original ResNet in image classification and object detection tasks on public datasets such as CIFAR-10/100, ImageNet, VOC, and MOT2017. Moreover, its performance boost is complementary to other extensions of the original network such as pre-activation and bottleneck, as well as stochastic training and Squeeze-Excitation.","","","10.1109/TCSVT.2018.2869680","H2020 Fast Track to Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463635","ResNet;linear dynamical systems;convolutional neural networks;image classification;object detection","Training;Task analysis;Stochastic processes;Object detection;Data models;Neural networks;Integrated circuit modeling","convolutional neural nets;image classification;learning (artificial intelligence);object detection","information mapping;residual block;linear dynamical systems;residual information;convolutional layers;multibranch residual blocks;convolutional neural networks;stochastic training;LDS-inspired residual networks;deep learning community;feature space;LDS-ResNet;CIFAR-10/100 dataset;ImageNet dataset;VOC dataset;MOT2017 dataset","","","30","","","","","IEEE","IEEE Journals"
"Pelvic Organ Segmentation Using Distinctive Curve Guided Fully Convolutional Networks","K. He; X. Cao; Y. Shi; D. Nie; Y. Gao; D. Shen","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Biomedical Research Imaging Center, University of North Carolina, Chapel Hill, NC, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Biomedical Research Imaging Center, University of North Carolina, Chapel Hill, NC, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Biomedical Research Imaging Center, University of North Carolina, Chapel Hill, NC, USA","IEEE Transactions on Medical Imaging","","2019","38","2","585","595","Accurate segmentation of pelvic organs (i.e., prostate, bladder, and rectum) from CT image is crucial for effective prostate cancer radiotherapy. However, it is a challenging task due to: 1) low soft tissue contrast in CT images and 2) large shape and appearance variations of pelvic organs. In this paper, we employ a two-stage deep learning-based method, with a novel distinctive curve-guided fully convolutional network (FCN), to solve the aforementioned challenges. Specifically, the first stage is for fast and robust organ detection in the raw CT images. It is designed as a coarse segmentation network to provide region proposals for three pelvic organs. The second stage is for fine segmentation of each organ, based on the region proposal results. To better identify those indistinguishable pelvic organ boundaries, a novel morphological representation, namely, distinctive curve, is also introduced to help better conduct the precise segmentation. To implement this, in this second stage, a multi-task FCN is initially utilized to learn the distinctive curve and the segmentation map separately and then combine these two tasks to produce accurate segmentation map. The final segmentation results of all three pelvic organs are generated by a weighted max-voting strategy. We have conducted exhaustive experiments on a large and diverse pelvic CT data set for evaluating our proposed method. The experimental results demonstrate that our proposed method is accurate and robust for this challenging segmentation task, by also outperforming the state-of-the-art segmentation methods.","","","10.1109/TMI.2018.2867837","National Natural Science Foundation of China; Young Elite Scientists Sponsorship Program through CAST; NIH Clinical Center; Collaborative Innovation Center of Novel Software Technology and Industrialization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451958","Image segmentation;neural networks;multitasking;computed tomography;pelvic organ;prostate cancer","Image segmentation;Computed tomography;Shape;Bladder;Task analysis;Robustness","biological organs;biological tissues;cancer;computerised tomography;convolutional neural nets;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;radiation therapy","prostate cancer radiotherapy;CT images;distinctive curve-guided fully convolutional network;pelvic organ boundaries;coarse segmentation network;robust organ detection;two-stage deep learning-based method;pelvic organ segmentation","","5","39","","","","","IEEE","IEEE Journals"
"Speedy and accurate image super-resolution via deeply recursive CNN with skip connection and network in network","D. Guo; Y. Niu; P. Xie",", Beihang University, People's Republic of China; , Beihang University, People's Republic of China; , Beihang University, People's Republic of China","IET Image Processing","","2019","13","7","1201","1209","The single image super-resolution (SISR) methods based on the deep convolutional neural network (CNN) have recently achieved significant improvements in accuracy, advancing the state of the art. However, these deeper models are computationally expensive and require a large number of parameters. Accordingly, they demand more memory and are unsuitable for on-chip devices. In this study, a novel SISR method using a deeply recursive CNN with skip connections and a network in network structure is proposed. The deeply recursive CNN with skip connections is adopted for the image feature extraction at both local and global levels. Parallelised 1 × 1 CNNs, usually called a network in network structure, are adopted for image reconstruction. Specifically, recursive learning is utilised to control the number of model parameters needed and residual learning is used to ease the difficulty of training. The proposed method performs favourably against the state-of-the-art methods in terms of computational speed and accuracy. It significantly outperforms the previous methods by a large margin, while demanding far fewer parameters. This model requires less memory and is friendly to on-chip devices.","","","10.1049/iet-ipr.2018.5907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733286","","","convolution;feature extraction;image reconstruction;image resolution;learning (artificial intelligence);neural nets","accurate image super-resolution;deeply recursive CNN;skip connection;single image super-resolution methods;deep convolutional neural network;on-chip devices;novel SISR method;network structure;image feature extraction;parallelised 1 × 1 CNNs;image reconstruction;recursive learning;state-of-the-art methods","","","27","","","","","IET","IET Journals"
"A Novel Weakly-Supervised Approach for RGB-D-Based Nuclear Waste Object Detection","L. Sun; C. Zhao; Z. Yan; P. Liu; T. Duckett; R. Stolkin","Oxford Robotics Institute, University of Oxford, Oxford, U.K.; Extreme Robotics Lab, University of Birmingham, Birmingham, U.K.; EPAN Research Group, University of Technology of Belfort-Montbéliard, Sevenans, France; Lincoln Centre for Autonomous Systems, University of Lincoln, Lincoln, U.K.; Lincoln Centre for Autonomous Systems, University of Lincoln, Lincoln, U.K.; Extreme Robotics Lab, University of Birmingham, Birmingham, U.K.","IEEE Sensors Journal","","2019","19","9","3487","3500","This paper addresses the problem of RGBD-based detection and categorization of waste objects for nuclear decommissioning. To enable autonomous robotic manipulation for nuclear decommissioning, nuclear waste objects must be detected and categorized. However, as a novel industrial application, large amounts of annotated waste object data are currently unavailable. To overcome this problem, we propose a weakly supervised learning approach which is able to learn a deep convolutional neural network from unlabeled RGBD videos while requiring very few annotations. The proposed method also has the potential to be applied to other household or industrial applications. We evaluate our approach on the Washington RGB-D object recognition benchmark, achieving the state-of-the-art performance among semi-supervised methods. More importantly, we introduce a novel dataset, i.e., Birmingham nuclear waste simulants dataset, and evaluate our proposed approach on this novel industrial object recognition challenge. We further propose a complete real-time pipeline for RGBD-based detection and categorization of nuclear waste simulants. Our weakly supervised approach has demonstrated to be highly effective in solving a novel RGB-D object detection and recognition application with limited human annotations.","","","10.1109/JSEN.2018.2888815","EU Projects; Engineering and Physical Sciences Research Council; DISTINCTIVE; Research Councils UK; Royal Society; EU; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8581517","Nuclear waste detection and categorization;nuclear waste decommissioning;autonomous waste sorting and segregation","Radioactive pollution;Three-dimensional displays;Proposals;Object detection;Training;Real-time systems;Object recognition","convolutional neural nets;environmental science computing;learning (artificial intelligence);object detection;object recognition;waste handling","Washington RGB-D object recognition benchmark;semisupervised methods;Birmingham nuclear waste simulants dataset;recognition application;RGB-D-based nuclear waste object detection;nuclear decommissioning;autonomous robotic manipulation;nuclear waste objects;annotated waste object data;weakly supervised learning approach;deep convolutional neural network;unlabeled RGBD videos;industrial applications;industrial application;weakly-supervised approach;industrial object recognition challenge","","","50","","","","","IEEE","IEEE Journals"
"SS-HCNN: Semi-Supervised Hierarchical Convolutional Neural Network for Image Classification","T. Chen; S. Lu; J. Fan","School of Information Science and Technology, Fudan University, Shanghai, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Satellite Department, Institute for Infocomm Research, Singapore","IEEE Transactions on Image Processing","","2019","28","5","2389","2398","The availability of large-scale annotated data and the uneven separability of different data categories have become two major impediments of deep learning for image classification. In this paper, we present a semi-supervised hierarchical convolutional neural network (SS-HCNN) to address these two challenges. A large-scale unsupervised maximum margin clustering technique is designed, which splits images into a number of hierarchical clusters iteratively to learn cluster-level CNNs at parent nodes and category-level CNNs at leaf nodes. The splitting uses the similarity of CNN features to group visually similar images into the same cluster, which relieves the uneven data separability constraint. With the hierarchical cluster-level CNNs capturing certain high-level image category information, the category-level CNNs can be trained with a small amount of labeled images, and this relieves the data annotation constraint. A novel cluster splitting criterion is also designed, which automatically terminates the image clustering in the tree hierarchy. The proposed SS-HCNN has been evaluated on the CIFAR-100 and ImageNet classification datasets. The experiments show that the SS-HCNN trained using a portion of labeled training images can achieve comparable performance with other fully trained CNNs using all labeled images. Additionally, the SS-HCNN trained using all labeled images clearly outperforms other fully trained CNNs.","","","10.1109/TIP.2018.2886758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576611","SS-HCNN;semi-supervised;hierarchical;unsupervised;image classification","Training;Feature extraction;Convolutional neural networks;Visualization;Image annotation;Testing;Fans","convolutional neural nets;feature extraction;image annotation;image classification;learning (artificial intelligence);pattern clustering","hierarchical cluster-level CNNs;high-level image category information;data annotation constraint;SS-HCNN;semisupervised hierarchical convolutional neural network;image classification;unsupervised maximum margin clustering technique;cluster splitting criterion;deep learning","","2","29","","","","","IEEE","IEEE Journals"
"VUNet: Dynamic Scene View Synthesis for Traversability Estimation Using an RGB Camera","N. Hirose; A. Sadeghian; F. Xia; R. Martín-Martín; S. Savarese","Computer Science Department, Stanford University, Stanford, CA, USA; Computer Science Department, Stanford University, Stanford, CA, USA; Computer Science Department, Stanford University, Stanford, CA, USA; Computer Science Department, Stanford University, Stanford, CA, USA; Computer Science Department, Stanford University, Stanford, CA, USA","IEEE Robotics and Automation Letters","","2019","4","2","2062","2069","We present VUNet, a novel view(VU) synthesis method for mobile robots in dynamic environments, and its application to the estimation of future traversability. Our method predicts future images for given virtual robot velocity commands using only RGB images at previous and current time steps. The future images result from applying two types of image changes to the previous and current images: first, changes caused by different camera pose. Second, changes due to the motion of the dynamic obstacles. We learn to predict these two types of changes disjointly using two novel network architectures, SNet and DNet. We combine SNet and DNet to synthesize future images that we pass to our previously presented method GONet [N. Hirose, A. Sadeghian, M. Vazquez, P. Goebel, and S. Savarese, “Gonet: A semi-supervised deep learning approach for traversability estimation,” in Proc. IEEE International Conference on Intelligent Robots and Systems, 2018, pp. 3044-3051] to estimate the traversable areas around the robot. Our quantitative and qualitative evaluation indicate that our approach for view synthesis predicts accurate future images in both static and dynamic environments. We also show that these virtual images can be used to estimate future traversability correctly. We apply our view synthesis-based traversability estimation method to two applications for assisted teleoperation.","","","10.1109/LRA.2019.2894869","Toyota Research Institute; The TOYOTA Central R&D Labs., INC. at Stanford University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624332","Robot safety;computer vision for other robotic applications;collision avoidance","Cameras;Dynamics;Robot vision systems;Estimation;Navigation","cameras;image colour analysis;intelligent robots;learning (artificial intelligence);mobile robots;path planning;robot vision;telerobotics","VUNet;dynamic scene view synthesis;RGB camera;mobile robots;dynamic environments;future traversability;RGB images;previous time steps;current time steps;image changes;dynamic obstacles;presented method GONet;semisupervised deep learning approach;traversable areas;accurate future images;virtual images;view synthesis-based traversability estimation method;virtual robot velocity;novel view synthesis method;GONet","","1","36","","","","","IEEE","IEEE Journals"
"A Novel Weakly Supervised Multitask Architecture for Retinal Lesions Segmentation on Fundus Images","C. Playout; R. Duval; F. Cheriet","Department of Informatics, École Polytechnique de Montréal, Montréal, QC, Canada; CUO-Hôpital Maisonneuve Rosemont and Département d’ophtalmologie, Université de Montréal, Montréal, QC, Canada; Department of Informatics, École Polytechnique de Montréal, Montréal, QC, Canada","IEEE Transactions on Medical Imaging","","2019","38","10","2434","2444","Obtaining the complete segmentation map of retinal lesions is the first step toward an automated diagnosis tool for retinopathy that is interpretable in its decision-making. However, the limited availability of ground truth lesion detection maps at a pixel level restricts the ability of deep segmentation neural networks to generalize over large databases. In this paper, we propose a novel approach for training a convolutional multi-task architecture with supervised learning and reinforcing it with weakly supervised learning. The architecture is simultaneously trained for three tasks: segmentation of red lesions and of bright lesions, those two tasks done concurrently with lesion detection. In addition, we propose and discuss the advantages of a new preprocessing method that guarantees the color consistency between the raw image and its enhanced version. Our complete system produces segmentations of both red and bright lesions. The method is validated at the pixel level and per-image using four databases and a cross-validation strategy. When evaluated on the task of screening for the presence or absence of lesions on the Messidor image set, the proposed method achieves an area under the ROC curve of 0.839, comparable with the state-of-the-art.","","","10.1109/TMI.2019.2906319","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672120","Computer-aided diagnostic;fundus imaging;lesions segmentations;retina;screening","Lesions;Retina;Image segmentation;Diseases;Training;Task analysis;Feature extraction","biomedical optical imaging;convolutional neural nets;diseases;eye;image classification;image segmentation;medical image processing;supervised learning","complete segmentation map;automated diagnosis tool;decision-making;ground truth lesion detection maps;pixel level;deep segmentation neural networks;convolutional multitask architecture;weakly supervised learning;red lesions;bright lesions;raw image;Messidor image set;weakly supervised multitask architecture;retinal lesion segmentation","","","43","","","","","IEEE","IEEE Journals"
"HyperReconNet: Joint Coded Aperture Optimization and Image Reconstruction for Compressive Hyperspectral Imaging","L. Wang; T. Zhang; Y. Fu; H. Huang","Beijing Laboratory of Intelligent Information Technology, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Image Processing","","2019","28","5","2257","2270","Coded aperture snapshot spectral imaging (CASSI) system encodes the 3D hyperspectral image (HSI) within a single 2D compressive image and then reconstructs the underlying HSI by employing an inverse optimization algorithm, which equips with the distinct advantage of snapshot but usually results in low reconstruction accuracy. To improve the accuracy, existing methods attempt to design either alternative coded apertures or advanced reconstruction methods, but cannot connect these two aspects via a unified framework, which limits the accuracy improvement. In this paper, we propose a convolution neural network-based end-to-end method to boost the accuracy by jointly optimizing the coded aperture and the reconstruction method. On the one hand, based on the nature of CASSI forward model, we design a repeated pattern for the coded aperture, whose entities are learned by acting as the network weights. On the other hand, we conduct the reconstruction through simultaneously exploiting intrinsic properties within HSI-the extensive correlations across the spatial and spectral dimensions. By leveraging the power of deep learning, the coded aperture design and the image reconstruction are connected and optimized via a unified framework. Experimental results show that our method outperforms the state-of-the-art methods under both comprehensive quantitative metrics and perceptive quality.","","","10.1109/TIP.2018.2884076","National Natural Science Foundation of China; Beijing Municipal Science and Technology Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8552450","Compressive hyperspectral imaging;convolution neural network;coded aperture optimization;hyperspectral image greconstruction","Image reconstruction;Apertures;Image coding;Hyperspectral imaging;Optimization;Three-dimensional displays;Spatial resolution","convolutional neural nets;data compression;hyperspectral imaging;image coding;image reconstruction;image resolution;learning (artificial intelligence);optimisation","joint coded aperture optimization;image reconstruction;3D hyperspectral image;single 2D compressive image;inverse optimization algorithm;coded aperture design;state-of-the-art methods;compressive hyperspectral imaging;HSI;convolution neural network;coded aperture snapshot spectral imaging;CASSI forward model;deep learning","","","68","","","","","IEEE","IEEE Journals"
"Fast and efficient contrast-enhanced super-resolution without real-world data using concatenated recursive compressor–decompressor network","J. Choi; D. Kang","Pusan National University, Republic of Korea; Pusan National University, Republic of Korea","IET Image Processing","","2019","13","5","817","824","The authors propose a novel model called concatenated recursive compressor-decompressor network (CRCDNet) for contrast-enhanced super-resolution. The characteristics of authors' model can be summarised as follows. First, a compression-decompression process reduces the computational complexity compared with the general fully convolutional model. Second, an internal/external skip-connection is used to preserve information of the preceding layers. Finally, by employing a recursive module, authors' model has a small number of parameters, yet is a deep and robust network. The authors apply authors' proposed network to license plate images. As a real application, license plates can provide important evidence for investigation of crimes and for security, but it is very difficult to collect the vast amounts of license plates required for analysis based on a data-driven approach. To solve this problem, the authors generated virtual datasets to train authors' model, while analysing the performance with real license plate datasets. Authors' method achieves better performance than the state-of-the-art models on license plate images.","","","10.1049/iet-ipr.2018.5751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8689156","","","computational complexity;data compression;image enhancement;image resolution;image sensors;learning (artificial intelligence);object recognition;parallel algorithms;traffic engineering computing","CRCDNet;internal-external skip-connection;license plates;robust network;deep network;recursive module;general fully convolutional model;compression-decompression process;concatenated recursive compressor-decompressor network;real-world data;efficient contrast-enhanced super-resolution;fast contrast-enhanced super-resolution;license plate images;state-of-the-art models;license plate datasets","","","40","","","","","IET","IET Journals"
"Speech Enhancement Using a Two-Stage Network for an Efficient Boosting Strategy","J. Kim; M. Hahn","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Signal Processing Letters","","2019","26","5","770","774","A novel neural network architecture, called two-stage network (TSN), with a multi-objective learning (MOL) method for an efficient boosting strategy (BS) is proposed for speech enhancement. BS is an ensemble method using multiple base predictions (MBPs) for better final prediction. Because of the necessity for MBPs, the computational cost and model size of BS-based methods are greater than those of a single model. In overcoming this, TSN first obtains MBPs from a single deep neural network. Then, to obtain better final prediction, the convolution layers of TSN aggregate not only MBPs but also some auxiliary information such as contextual information, while adaptively filtering out some unnecessary information, e.g., poor base predictions. At the training phase, the MOL enables all stages of TSN to learn jointly, whereas allowing the TSN framework to embed a BS. Our experimental results confirm that the embedded BS leads TSN to outperform other baseline methods with a reasonably low computational cost and model size.","","","10.1109/LSP.2019.2905660","Basic Science Research Program through the National Research Foundation of Korea; Ministry of Science; H2020 LEIT Information and Communication Technologies; Future Planning; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668449","Speech enhancement;two-stage network","Convolution;Feature extraction;Training;Artificial neural networks;Noise measurement;Speech enhancement;Computational efficiency","adaptive filters;computational complexity;learning (artificial intelligence);neural net architecture;set theory;speech enhancement","MBPs;single deep neural network;contextual information;MOL;TSN framework;embedded BS;speech enhancement;multiobjective learning method;ensemble method;multiple base predictions;BS-based methods;low computational cost;boosting strategy;neural network architecture;two-stage network;convolution layers","","","33","","","","","IEEE","IEEE Journals"
"Audio-Noise Power Spectral Density Estimation Using Long Short-Term Memory","X. Li; S. Leglaive; L. Girin; R. Horaud","Inria Grenoble Rhône-Alpes, Montbonnot Saint-Martin, France; Inria Grenoble Rhône-Alpes, Montbonnot Saint-Martin, France; Inria Grenoble Rhône-Alpes, Montbonnot Saint-Martin, France; Inria Grenoble Rhône-Alpes, Montbonnot Saint-Martin, France","IEEE Signal Processing Letters","","2019","26","6","918","922","We propose a method using a long short-term memory (LSTM) network to estimate the noise power spectral density (PSD) of single-channel audio signals represented in the short-time Fourier transform (STFT) domain. An LSTM network common to all frequency bands is trained, which processes each frequency band individually by mapping the noisy STFT magnitude sequence to its corresponding noise PSD sequence. Unlike deep-learning-based speech-enhancement methods, which learn the full-band spectral structure of speech segments, the proposed method exploits the sub-band STFT magnitude evolution of noise with long time dependence, in the spirit of the unsupervised noise estimators described in the literature. Speaker- and speech-independent experiments with different types of noise show that the proposed method outperforms the unsupervised estimators, and it generalizes well to noise types that are not present in the training set.","","","10.1109/LSP.2019.2911879","ERC Advanced; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693515","LSTM;noise PSD;speech enhancement","Noise measurement;Training;Estimation;Speech enhancement;Signal to noise ratio;Indexes;Frequency estimation","audio signal processing;Fourier transforms;learning (artificial intelligence);speech enhancement","noise PSD sequence;short-time Fourier transform domain;noise types;unsupervised estimators;speech-independent experiments;unsupervised noise estimators;long time dependence;speech segments;full-band spectral structure;deep-learning-based speech-enhancement methods;noisy STFT magnitude sequence;frequency band;LSTM network;single-channel audio signals;short-term memory network;long short-term memory;audio-noise power spectral density estimation","","1","31","","","","","IEEE","IEEE Journals"
"Aircraft tracking based on fully conventional network and Kalman filter","J. Yang; W. Zhao; Y. Han; C. Ji; B. Jiang; Z. Zheng; H. Song","School of Electrical and Information Engineering, Tianjin University, People's Republic of China; School of Electrical and Information Engineering, Tianjin University, People's Republic of China; School of Electrical and Information Engineering, Tianjin University, People's Republic of China; School of Electrical and Information Engineering, Tianjin University, People's Republic of China; School of Electrical and Information Engineering, Tianjin University, People's Republic of China; Beijing Aerospace Automatic Control Institute, People's Republic of China; Embry-Riddle Aeronautical University, USA","IET Image Processing","","2019","13","8","1259","1265","Aircraft tracking is a significant technology for military reconnaissance, but there is no efficient algorithm to solve this particular problem. Recently, research based on deep learning for object tracking has developed rapidly, and the performance is greatly improved compared to the traditional methods, so the authors refer to relevant work and make an improvement on the previous research to improve the performance on aircraft tracking. They first learn the idea from region-based fully convolutional networks to perform detection on each frame of video. To avoid the target drift due to the failure of object detection on a certain frame, then they employ Kalman filter (KF) and extended KF together to predict the moving trajectory of the target. Beyond that, this method can confine the valid range based on the size of a target object, which increases the speed of detection. This approach can also correct the bounding box on adjacent frames. The steps are not complicated but have an excellent performance. Through the experiment, it is clear that the proposed method is reasonable and more precise.","","","10.1049/iet-ipr.2018.5022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8741338","","","aircraft;convolutional neural nets;image filtering;Kalman filters;learning (artificial intelligence);military computing;nonlinear filters;object detection;object tracking;target tracking;video signal processing","aircraft tracking;military reconnaissance;deep learning;object tracking;region-based fully convolutional networks;object detection;Kalman filter;target object;extended KF","","","48","","","","","IET","IET Journals"
"COMIC: Toward A Compact Image Captioning Model With Attention","J. H. Tan; C. S. Chan; J. H. Chuah","Center of Image and Signal Processing, Department of Artificial Intelligence, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Center of Image and Signal Processing, Department of Artificial Intelligence, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Department of Electrical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, Malaysia","IEEE Transactions on Multimedia","","2019","21","10","2686","2696","Recent works in image captioning have shown very promising raw performance. However, we realize that most of these encoder-decoder style networks with attention do not scale naturally to large vocabulary size, making them difficult to deploy on embedded systems with limited hardware resources. This is because the size of word and output embedding matrices grow proportionally with the size of vocabulary, adversely affecting the compactness of these networks. To address this limitation, this paper introduces a brand new idea in the domain of image captioning. That is, we tackle the problem of compactness of image captioning models which is hitherto unexplored. We showed that our proposed model, named COMIC for compact image captioning, achieves comparable results in five common evaluation metrics with state-of-the-art approaches on both MS-COCO and InstaPIC-1.1M datasets despite having an embedded vocabulary size that is 39×-99× smaller.","","","10.1109/TMM.2019.2904878","UM Frontier Research; University of Malaya; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666805","Image captioning;deep compression network;deep learning","Vocabulary;Computational modeling;Complexity theory;Encoding;Task analysis;Image coding;Context modeling","computer vision;embedded systems;feature extraction;image retrieval;matrix algebra","embedded vocabulary size;raw performance;encoder-decoder style networks;embedded systems;hardware resources;output embedding matrices;compactness;COMIC;compact image captioning model;MS-COCO dataset;InstaPIC-1.1M dataset","","2","52","Traditional","","","","IEEE","IEEE Journals"
"Long Activity Video Understanding Using Functional Object-Oriented Network","A. B. Jelodar; D. Paulius; Y. Sun","Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA; Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA; Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA","IEEE Transactions on Multimedia","","2019","21","7","1813","1824","Video understanding is one of the most challenging topics in computer vision. In this paper, a four-stage video understanding pipeline is presented to simultaneously recognize all atomic actions and the single ongoing activity in a video. This pipeline uses objects and motions from the video and a graph-based knowledge representation network as prior reference. Two deep networks are trained to identify objects and motions in each video sequence associated with an action and low level image features are used to identify objects of interest in the video sequence. Confidence scores are assigned to objects of interest to represent their involvement in the action and to motion classes based on results from a deep neural network that classifies an ongoing action in video into motion classes. Confidence scores are computed for each candidate functional unit to associate them with an action using a knowledge representation network, object confidences, and motion confidences. Each action, therefore, is associated with a functional unit, and the sequence of actions is evaluated to identify the sole activity occurring in the video. The knowledge representation used in the pipeline is called the functional object-oriented network, which is a graph-based network useful for encoding knowledge about manipulation tasks. Experiments are performed on a dataset of cooking videos to test the proposed algorithm with action inference and activity classification. Experiments show that using a functional object-oriented network improves video understanding significantly.","","","10.1109/TMM.2018.2885228","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8563111","Video understanding;activity understanding;video knowledge representation","Knowledge representation;Knowledge based systems;Object oriented modeling;Activity recognition;Pipelines;Object recognition;Task analysis","computer vision;feature extraction;graph theory;image sequences;knowledge representation;learning (artificial intelligence);neural nets;video signal processing","activity video understanding;functional object-oriented network;four-stage video understanding pipeline;atomic actions;single ongoing activity;graph-based knowledge representation network;deep networks;video sequence;confidence scores;motion classes;deep neural network;object confidences;motion confidences;graph-based network;action inference;activity classification;cooking video dataset","","","63","","","","","IEEE","IEEE Journals"
"The Multi-Lane Capsule Network","V. M. d. Rosario; E. Borin; M. Breternitz","Institute of Computing, Unicamp, Campinas, Brazil; Institute of Computing, Unicamp, Campinas, Brazil; Lisbon University Institute ISCTE-IUL, ISTAR-IUL Lab","IEEE Signal Processing Letters","","2019","26","7","1006","1010","We introduce multi-lane capsule networks (MLCN), which are a separable and resource efficient organization of capsule networks (CapsNet) that allows parallel processing while achieving high accuracy at reduced cost. A MLCN is composed of a number of (distinct) parallel lanes, each contributing to a dimension of the result, trained using the routing-by-agreement organization of CapsNet. Our results indicate similar accuracy with a much-reduced cost in number of parameters for the Fashion-MNIST and Cifar10 datasets. They also indicate that the MLCN outperforms the original CapsNet when using a proposed novel configuration for the lanes. MLCN also has faster training and inference times, being more than two-fold faster than the original CapsNet in a same accelerator.","","","10.1109/LSP.2019.2915661","CAPES/Brasil; Conselho Nacional de Desenvolvimento Científico e Tecnológico; Fundação de Amparo à Pesquisa do Estado de São Paulo; National funds through Fundação para a Ciência e Tecnologia (FCT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8709729","Capsule network;multi-lane;deep learning;CNN","Convolution;Heuristic algorithms;Training;Kernel;Routing;Image reconstruction;Organizations","convolutional neural nets;learning (artificial intelligence);parallel processing","multilane capsule network;MLCN;capsule networks;parallel processing;parallel lanes;routing-by-agreement organization;CapsNet;resource efficient organization","","1","14","","","","","IEEE","IEEE Journals"
"Low-Rank and Locality Constrained Self-Attention for Sequence Modeling","Q. Guo; X. Qiu; X. Xue; Z. Zhang","School of Computer Science and the Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; School of Computer Science and the Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; School of Computer Science and the Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; NYU Shanghai, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","12","2213","2222","Self-attention mechanism becomes more and more popular in natural language processing (NLP) applications. Recent studies show the Transformer architecture which relies mainly on the attention mechanism achieves much success on large datasets. But a raised problem is its generalization ability is weaker than CNN and RNN on many moderate-sized datasets. We think the reason can be attributed to its unsuitable inductive bias of the self-attention structure. In this paper, we regard the self-attention as matrix decomposition problem and propose an improved self-attention module by introducing two linguistic constraints: low-rank and locality. We further develop the low-rank attention and band attention to parameterize the self-attention mechanism under the low-rank and locality constraints. Experiments on several real NLP tasks show our model outperforms the vanilla Transformer and other self-attention models on moderate size datasets. Additionally, evaluation on a synthetic task gives us a more detailed understanding of working mechanisms of different architectures.","","","10.1109/TASLP.2019.2944078","National Natural Science Foundation of China; Science and Technology Commission of Shanghai Municipality; Shanghai Municipal Science and Technology Major Project; ZJLab; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894858","Sequence modeling;self-attention;transformer;deep learning","Sparse matrices;Bit error rate;Matrix decomposition;Linguistics;Task analysis;Natural language processing;Data models","computational linguistics;learning (artificial intelligence);matrix decomposition;natural language processing","moderate size datasets;self-attention models;locality constraints;band attention;low-rank attention;matrix decomposition problem;self-attention structure;moderate-sized datasets;transformer architecture;natural language processing applications;sequence modeling","","","43","IEEE","","","","IEEE","IEEE Journals"
"Fast Spectrogram Inversion Using Multi-Head Convolutional Neural Networks","S. Ö. Arık; H. Jun; G. Diamos","Baidu Silicon Valley Artificial Intelligence Lab, Sunnyvale, CA, USA; Baidu Silicon Valley Artificial Intelligence Lab, Sunnyvale, CA, USA; Baidu Silicon Valley Artificial Intelligence Lab, Sunnyvale, CA, USA","IEEE Signal Processing Letters","","2019","26","1","94","98","We propose the multi-head convolutional neural network (MCNN) for waveform synthesis from spectrograms. Nonlinear interpolation in MCNN is employed with transposed convolution layers in parallel heads. MCNN enables significantly better utilization of modern multi-core processors than commonly used iterative algorithms like Griffin-Lim, and yields very fast (more than 300 × real time) runtime. For training of MCNN, we use a large-scale speech recognition dataset and losses defined on waveforms that are related to perceptual audio quality. We demonstrate that MCNN constitutes a very promising approach for high-quality speech synthesis, without any iterative algorithms or autoregression in computations.","","","10.1109/LSP.2018.2880284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8528831","Phase reconstruction;deep learning;convolutional neural networks;short-time Fourier transform;spectrogram;time-frequency signal processing;speech synthesis","Spectrogram;Convolution;Time-frequency analysis;Training;Convolutional neural networks;Adaptation models;Signal processing algorithms","audio signal processing;feedforward neural nets;interpolation;iterative methods;learning (artificial intelligence);speech recognition;speech synthesis","multicore processors;high-quality speech synthesis;multihead convolutional neural network;fast spectrogram inversion;MCNN;parallel heads;transposed convolution layers","","","25","","","","","IEEE","IEEE Journals"
"A CNN-Based Spatial Feature Fusion Algorithm for Hyperspectral Imagery Classification","A. J. X. Guo; F. Zhu","Center for Applied Mathematics, Tianjin University, Tianjin, China; Center for Applied Mathematics, Tianjin University, Tianjin, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","9","7170","7181","The shortage of training samples remains one of the main obstacles in applying the neural networks to the hyperspectral images classification. To fuse the spatial and spectral information, pixel patches are often utilized to train a model, which may further aggregate this problem. In the existing works, an artificial neural network (ANN) model supervised by centerloss (ANNC) was introduced. Training merely with spectral information, the ANNC yields discriminative spectral features suitable for the subsequent classification tasks. In this paper, we propose a novel convolutional neural network (CNN)-based spatial feature fusion (CSFF) algorithm, which allows a smart integration of spatial information to the spectral features extracted by ANNC. As a critical part of CSFF, a CNN-based discriminant model is introduced to estimate whether two pixels belong to the same class. At the testing stage, by applying the discriminant model to the pixel pairs generated by a test pixel and each of its neighbors, the local structure is estimated and represented as a customized convolutional kernel. The spectral-spatial feature is generated by a convolutional operation between the estimated kernel and the corresponding spectral features within a local region. The final label is determined by classifying the resulting spectral-spatial feature. Without increasing the number of training samples or involving pixel patches at the training stage, the CSFF framework achieves the state of the art by declining 20%-50% classification failures in experiments on three well-known hyperspectral images.","","","10.1109/TGRS.2019.2911993","National Natural Science Foundation of China; Natural Science Foundation of Tianjin City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8709967","Convolutional neural networks;deep learning;feature extraction;hyperspectral image classification","Feature extraction;Training;Hyperspectral imaging;Testing;Training data;Adaptation models","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);pattern classification","hyperspectral imagery classification;artificial neural network model;CNN-based discriminant model;ANNC;CSFF;CNN-based spatial feature fusion;convolutional neural network-based spatial feature fusion;feature extraction","","","50","","","","","IEEE","IEEE Journals"
"PointAtrousNet: Point Atrous Convolution for Point Cloud Analysis","L. Pan; P. Wang; C. Chew","Department of Mechanical Engineering, National University of Singapore, Singapore; Temasek Laboratories, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore","IEEE Robotics and Automation Letters","","2019","4","4","4035","4041","In this letter, we propose a permutation-invariant architecture-PointAtrousNet (PAN), which focuses on exploiting multi-scale local geometric details for point cloud analysis. Inspired by atrous convolution in image domains, we propose the point atrous convolution (PAC) operation. Our PAC can effectively enlarge the receptive field of filters without introducing more parameters or increasing computation amount. In particular, we propose a novel point atrous spatial pyramid pooling module to explicitly exploit neighboring contextual information at multiple scales. Moreover, local geometric details are captured by constructing neighborhood graphs in metric and feature spaces. Experimental results show that our PAN achieves state-of-the-art performance on various point cloud inference applications.","","","10.1109/LRA.2019.2927948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758990","Deep learning in robotics and automation;computer vision for other robotic applications","Convolution;Three-dimensional displays;Picture archiving and communication systems;Feature extraction;Extraterrestrial measurements;Task analysis","feature extraction;graph theory;image classification;image representation;learning (artificial intelligence);object detection;object recognition","point cloud analysis;exploiting multiscale local geometric details;point atrous convolution operation;PAC;spatial pyramid pooling module;point cloud inference applications;PointAtrousNet;permutation-invariant architecture","","","28","Traditional","","","","IEEE","IEEE Journals"
"Decoupled Spatial Neural Attention for Weakly Supervised Semantic Segmentation","T. Zhang; G. Lin; J. Cai; T. Shen; C. Shen; A. C. Kot","Interdisciplinary Graduate School and the School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; JD AI Research, Beijing 100101, China; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Multimedia","","2019","21","11","2930","2941","Weakly supervised semantic segmentation receives much research attention since it alleviates the need to obtain a large amount of dense pixel-wise ground-truth annotations for the training images. Compared with other forms of weak supervision, image labels are quite efficient to obtain. In this paper, we focus on the weakly supervised semantic segmentation with image label annotations. Recent progress for this task has been largely dependent on the quality of generated pseudo-annotations. In this paper, inspired by spatial neural-attention for image captioning, we propose a decoupled spatial neural attention network for generating pseudo-annotations. Our decoupled attention structure could simultaneously identify the object regions and localize the discriminative parts, which generates high-quality pseudo-annotations in one forward path. The generated pseudo-annotations lead to the segmentation results that achieve the state of the art in weakly supervised semantic segmentation.","","","10.1109/TMM.2019.2914870","National Research Foundation Singapore; Infocomm Media Development Authority, Singapore; National Research Foundation Singapore; AI Singapore Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8705324","Semantic segmentation;deep convolutional neural network (DCNN);weakly-supervised learning","Image segmentation;Semantics;Detectors;Training;Task analysis;Pipelines;Object recognition","image segmentation;neural nets;supervised learning","supervised semantic segmentation;image label annotations;pseudoannotations;decoupled spatial neural attention network;object regions","","","55","Traditional","","","","IEEE","IEEE Journals"
"Emotion-Semantic-Enhanced Neural Network","G. Yang; H. He; Q. Chen","School of Information and Safety Engineering, Zhongnan University of Economics and Law, Wuhan, China; Department of Electrical, Computer and Biomedical Engineering, University of Rhode Island, Kingston, USA; Department of Information Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","3","531","543","Although sentiment analysis on microblog posts has been studied in depth, sentiment analysis of posts is still challenging because of the limited contextual information that they normally contain. In microblog environments, emoticons are frequently used and they have clear emotional meanings. They are important emotional signals for microblog sentimental analysis. Existing studies typically use emoticons as noisy sentiment labels or similar sentiment indicators to effectively train classifier but overlook their emotional potentiality. We address this issue by constructing an emotional space as a feature representation matrix and projecting emoticons and words into the emotional space based on the semantic composition. To improve the performance of sentimental analysis, we propose a new emotion-semantic-enhanced convolutional neural network (ECNN) model. ECNN can use emoticon embedding as an emotional space projection operator. By projecting emoticons and words into an emoticon space, it can help identify subjectivity, polarity, and emotion in microblog environments. It is more capable of capturing emotion semantic than other models, so it can improve the sentiment analysis performance. The experimental results show that this model consistently outperforms other models on the dataset of several sentiment tasks. This paper provides insights on the design of ECNN for sentimental analysis in other natural language processing tasks.","","","10.1109/TASLP.2018.2885775","State of Scholarship Fund of the China Scholarship Council; Basic Research Fund of China's Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8573804","Natural language processing;sentiment analysis;deep learning;convolution neural network;emoticons","Semantics;Sentiment analysis;Task analysis;Computational modeling;Feature extraction","convolutional neural nets;emotion recognition;learning (artificial intelligence);matrix algebra;pattern classification;sentiment analysis;social networking (online)","microblog sentimental analysis;feature representation matrix;emotion-semantic-enhanced convolutional neural network model;emotional space projection operator;emotional signals;ECNN;natural language processing tasks;emoticons projections","","2","54","","","","","IEEE","IEEE Journals"
"Hybrid Noise Removal in Hyperspectral Imagery With a Spatial–Spectral Gradient Network","Q. Zhang; Q. Yuan; J. Li; X. Liu; H. Shen; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","10","7317","7329","The existence of hybrid noise in hyperspectral images (HSIs) severely degrades the data quality, reduces the interpretation accuracy of HSIs, and restricts the subsequent HSI applications. In this paper, the spatial-spectral gradient network (SSGN) is presented for mixed noise removal in HSIs. The proposed method employs a spatial-spectral gradient learning strategy, in consideration of the unique spatial structure directionality of sparse noise and spectral differences with additional complementary information for effectively extracting intrinsic and deep features of HSIs. Based on a fully cascaded multiscale convolutional network, SSGN can simultaneously deal with different types of noise in different HSIs or spectra by the use of the same model. The simulated and real-data experiments undertaken in this study confirmed that the proposed SSGN outperforms at mixed noise removal compared with the other state-of-the-art HSI denoising algorithms, in evaluation indices, visual assessments, and time consumption.","","","10.1109/TGRS.2019.2912909","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Natural Science Foundation of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8734833","Gradient learning;hybrid noise;hyperspectral;multiscale convolutional network;spatial–spectral","Noise reduction;Data models;Noise measurement;Hyperspectral imaging","convolutional neural nets;feature extraction;geophysical image processing;gradient methods;hyperspectral imaging;image denoising;learning (artificial intelligence)","spatial-spectral gradient network;SSGN;fully cascaded multiscale convolutional network;hybrid noise removal;spatial-spectral gradient learning;hyperspectral image noise;data quality;feature extraction;Indian Pines image;AVIRIS urban image","","1","46","","","","","IEEE","IEEE Journals"
"Resolution-Aware Network for Image Super-Resolution","Y. Wang; L. Wang; H. Wang; P. Li","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","5","1259","1269","In existing deep network-based image super-resolution (SR) methods, each network is only trained for a fixed upscaling factor and can hardly generalize to unseen factors at test time, which is non-scalable in real applications. To mitigate this issue, this paper proposes a resolution-aware network (RAN) for simultaneous SR of multiple factors. The key insight is that SR of multiple factors is essentially different but also shares common operations. To attain stronger generalization across factors, we design an upsampling network (U-Net) consisting of several sub-modules, in which each sub-module implements an intermediate step of the overall image SR and can be shared by SR of different factors. A decision network (D-Net) is further adopted to identify the quality of the input low-resolution image and adaptively select suitable sub-modules to perform SR. U-Net and D-Net together constitute the proposed RAN model, and are jointly trained using a new hierarchical loss function on SR tasks of multiple factors. Experimental evaluations demonstrate that the proposed RAN compares favorably against the state-of-the-art methods and its performance can well generalize across different upscaling factors.","","","10.1109/TCSVT.2018.2839879","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8362960","Super-resolution;resolution-aware;cascade","Task analysis;Image resolution;Training;Correlation;Image reconstruction;Image restoration;Learning systems","image resolution;image sampling;learning (artificial intelligence)","multiple factors;stronger generalization;upsampling network;image SR;decision network;input low-resolution image;SR tasks;RAN;resolution-aware network;deep network-based image super-resolution methods;fixed upscaling factor;U-Net;D-Net","","","43","","","","","IEEE","IEEE Journals"
"Richer Convolutional Features for Edge Detection","Y. Liu; M. Cheng; X. Hu; J. Bian; L. Zhang; X. Bai; J. Tang","College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Huazhong University of Science and Technology, Wuhan, Hubei, China; School of Computer Sience and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","8","1939","1946","Edge detection is a fundamental problem in computer vision. Recently, convolutional neural networks (CNNs) have pushed forward this field significantly. Existing methods which adopt specific layers of deep CNNs may fail to capture complex data structures caused by variations of scales and aspect ratios. In this paper, we propose an accurate edge detector using richer convolutional features (RCF). RCF encapsulates all convolutional features into more discriminative representation, which makes good usage of rich feature hierarchies, and is amenable to training via backpropagation. RCF fully exploits multiscale and multilevel information of objects to perform the image-to-image prediction holistically. Using VGG16 network, we achieve state-of-the-art performance on several available datasets. When evaluating on the well-known BSDS500 benchmark, we achieve ODS F-measure of 0.811 while retaining a fast speed (8 FPS). Besides, our fast version of RCF achieves ODS F-measure of 0.806 with 30 FPS. We also demonstrate the versatility of the proposed method by applying RCF edges for classical image segmentation.","","","10.1109/TPAMI.2018.2878849","National Natural Science Foundation of China; national youth talent support program; Natural Science Foundation of Tianjin City; Huawei Innovation Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516362","Edge detection;deep learning;richer convolutional features","Image edge detection;Feature extraction;Computer vision;Detectors;Training;Computer architecture;Task analysis","backpropagation;computer vision;convolutional neural nets;data structures;edge detection;feature extraction;image representation;image segmentation","image segmentation;BSDS500 benchmark;multilevel information;multiscale information;backpropagation;aspect ratio;scale variations;deep CNN;accurate edge detector;complex data structures;convolutional neural networks;computer vision;edge detection;RCF edges;VGG16 network;image-to-image prediction;rich feature hierarchies;richer convolutional features","","7","46","","","","","IEEE","IEEE Journals"
"DNN-Based Cepstral Excitation Manipulation for Speech Enhancement","S. Elshamy; T. Fingscheidt","Institute for Communications Technology, Technische Universität Braunschweig, Braunschweig, Germany; Institute for Communications Technology, Technische Universität Braunschweig, Braunschweig, Germany","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","11","1803","1814","This contribution aims at speech model-based speech enhancement by exploiting the source-filter model of human speech production. The proposed method enhances the excitation signal in the cepstral domain by making use of a deep neural network (DNN). We investigate two types of target representations along with the significant effects of their normalization. The new approach exceeds the performance of a formerly introduced classical signal processing-based cepstral excitation manipulation (CEM) method in terms of noise attenuation by about 1.5 dB. We show that this gain also holds true when comparing serial combinations of envelope and excitation enhancement. In the important low-SNR conditions, no significant trade-off for speech component quality or speech intelligibility is induced, while allowing for substantially higher noise attenuation. In total, a traditional purely statistical state-of-the-art speech enhancement system is outperformed by more than 3 dB noise attenuation.","","","10.1109/TASLP.2019.2933698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792089","Speech enhancement;deep learning;cepstrum; ${a priori}$  SNR","Speech enhancement;Signal to noise ratio;Cepstral analysis;Estimation;Noise reduction;Noise measurement","cepstral analysis;neural nets;speech enhancement;speech intelligibility","statistical state-of-the-art speech enhancement;speech intelligibility;low-SNR conditions;noise attenuation;target representation normalization;excitation signal enhancement;human speech production source-filter model;deep neural network based cepstral excitation manipulation;speech enhancement;DNN-based cepstral excitation manipulation","","1","49","CCBY","","","","IEEE","IEEE Journals"
"ThiNet: Pruning CNN Filters for a Thinner Net","J. Luo; H. Zhang; H. Zhou; C. Xie; J. Wu; W. Lin","National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","10","2525","2538","This paper aims at accelerating and compressing deep neural networks to deploy CNN models into small devices like mobile phones or embedded gadgets. We focus on filter level pruning, i.e., the whole filter will be discarded if it is less important. An effective and unified framework, ThiNet (stands for “Thin Net”), is proposed in this paper. We formally establish filter pruning as an optimization problem, and reveal that we need to prune filters based on statistics computed from its next layer, not the current layer, which differentiates ThiNet from existing methods. We also propose “gcos” (Group COnvolution with Shuffling), a more accurate group convolution scheme, to further reduce the pruned model size. Experimental results demonstrate the effectiveness of our method, which has advanced the state-of-the-art. Moreover, we show that the original VGG-16 model can be compressed into a very small model (ThiNet-Tiny) with only 2.66 MB model size, but still preserve AlexNet level accuracy. This small model is evaluated on several benchmarks with different vision tasks (e.g., classification, detection, segmentation), and shows excellent generalization ability.","","","10.1109/TPAMI.2018.2858232","National Natural Science Foundation of China; Young Scholar Exchange; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416559","Convolutional neural networks;filter pruning;deep learning;model compression","Convolution;Computational modeling;Task analysis;Acceleration;Training;Neural networks;Image coding","convolutional neural nets;mobile computing;optimisation","embedded gadgets;filter level pruning;effective framework;optimization problem;accurate group convolution scheme;pruned model size;original VGG-16 model;ThiNet-Tiny;AlexNet level accuracy;pruning CNN filters;thinner Net;CNN models;mobile phones;model size;deep neural networks;generalization ability;gcos;group convolution with shuffling","","2","54","","","","","IEEE","IEEE Journals"
"MVPointNet: Multi-View Network for 3D Object Based on Point Cloud","W. Zhou; X. Jiang; Y. Liu","Department of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; Department of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China; T Stone Robotics Institute, The Chinese University of Hong Kong, Hong Kong","IEEE Sensors Journal","","2019","19","24","12145","12152","The research of hand pose estimation is a hot topic in computer vision, robotics and virtual reality. Compared with using data glove, vision based methods show great advantage for its contactless property, low-cost and convenience. With the commercial depth cameras became widely available and the great success of Convolution Neural Network (CNN) on images, various works focused on hand pose estimation have achieved promising performance. This research is inspired by the recent work that directly perform 3D classification and segmentation tasks on point cloud. In this paper, Multi-View PointNet (MVPointNet) is proposed which takes several views of point cloud as input source. Then, they fed into the well-performing point cloud-based architecture. In addition, to better capture the hand context structure and improve the performance, more features between centroid and local neighborhood points (norm, edge, angle) are extracted and fed into a deep CNN architecture. To our knowledge, our proposed method achieved good performance on the ModelNet40 dataset for 3D shape classification. Besides, it achieved superior performance over other deep learning methods for 3D hand pose estimation based on point cloud, which is evaluated on MSRA dataset.","","","10.1109/JSEN.2019.2937089","Shenzhen Pea-cock Plan Team; Shenzhen and Hong Kong Joint Innovation Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811582","Multi-view;point cloud;hand pose estimation;MVPointNet","Three-dimensional displays;Pose estimation;Deep learning;Feature extraction;Task analysis;Two dimensional displays;Sensors","","","","","42","IEEE","","","","IEEE","IEEE Journals"
"Seven-Point Checklist and Skin Lesion Classification Using Multitask Multimodal Neural Nets","J. Kawahara; S. Daneshvar; G. Argenziano; G. Hamarneh","School of Computing Science, Simon Fraser University, Burnaby, Canada; School of Computing Science, Simon Fraser University, Burnaby, Canada; Dermatology Unit, University of Campania, Naples, Italy; School of Computing Science, Simon Fraser University, Burnaby, Canada","IEEE Journal of Biomedical and Health Informatics","","2019","23","2","538","546","We propose a multitask deep convolutional neural network, trained on multimodal data (clinical and dermoscopic images, and patient metadata), to classify the 7-point melanoma checklist criteria and perform skin lesion diagnosis. Our neural network is trained using several multitask loss functions, where each loss considers different combinations of the input modalities, which allows our model to be robust to missing data at inference time. Our final model classifies the 7-point checklist and skin condition diagnosis, produces multimodal feature vectors suitable for image retrieval, and localizes clinically discriminant regions. We benchmark our approach using 1011 lesion cases, and report comprehensive results over all 7-point criteria and diagnosis. We also make our dataset (images and metadata) publicly available online at http://derm.cs.sfu.ca.","","","10.1109/JBHI.2018.2824327","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8333693","Classification;convolutional neural networks;deep learning;dermatology;melanoma;skin;7-point checklist","Lesions;Skin;Malignant tumors;Feature extraction;Pattern analysis;Convolutional neural networks","cancer;convolutional neural nets;feature extraction;image classification;image retrieval;medical image processing;patient diagnosis;skin","skin lesion classification;multitask multimodal neural nets;multitask deep convolutional neural network;multimodal data;clinical images;dermoscopic images;patient metadata;7-point melanoma checklist criteria;skin lesion diagnosis;skin condition diagnosis;multimodal feature vectors;image retrieval;seven-point checklist","","1","38","","","","","IEEE","IEEE Journals"
"Bayesian DeNet: Monocular Depth Prediction and Frame-Wise Fusion With Synchronized Uncertainty","X. Yang; Y. Gao; H. Luo; C. Liao; K. Cheng","School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; HiScene Information Technology Company Ltd., Shanghai, China; School of Engineering, Hong Kong University of Science and Technology, Kowloon, Hong Kong","IEEE Transactions on Multimedia","","2019","21","11","2701","2713","Using deep convolutional neural networks (CNN) to predict the depth from a single image has received considerable attention in recent years due to its impressive performance. However, existing methods process each single image independently without leveraging the multiview information of video sequences in practical scenarios. Properly taking into account multiview information in video sequences beyond individual frames could offer considerable benefits in terms of depth prediction accuracy and robustness. In addition, a meaningful measure of prediction uncertainty is essential for decision making, which is not provided in existing methods. This paper presents a novel video-based depth prediction system based on a monocular camera, named Bayesian DeNet. Specifically, Bayesian DeNet consists of a 59-layer CNN that can concurrently output a depth map and an uncertainty map for each video frame. Each pixel in an uncertainty map indicates the error variance of the corresponding depth estimate. Depth estimates and uncertainties of previous frames are propagated to the current frame based on the tracked camera pose, yielding multiple depth/uncertainty hypotheses for the current frame which are then fused in a Bayesian inference framework for greater accuracy and robustness. Extensive exper-iments on three public datasets demonstrate that our Bayesian DeNet outperforms the state-of-the-art methods for monocular depth prediction. A demo video and code are publicly available.1","","","10.1109/TMM.2019.2912121","National Natural Science Foundation of China; Wuhan Science and Technology Bureau; Program for HUST Acadamic Frontier Youth Team; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693882","Depth estimation;deep learning;convolutional neural network","Uncertainty;Cameras;Bayes methods;Simultaneous localization and mapping;Training;Video sequences;Estimation","Bayes methods;convolutional neural nets;image fusion;image sequences;pose estimation;video cameras;video signal processing","monocular depth prediction;frame-wise fusion;synchronized uncertainty;deep convolutional neural networks;video sequences;multiview information;depth prediction accuracy;prediction uncertainty;video-based depth prediction system;monocular camera;59-layer CNN;depth map;uncertainty map;video frame;depth estimates;Bayesian inference framework;demo video;Bayesian DeNet","","","39","Traditional","","","","IEEE","IEEE Journals"
"A Robust Residual Dense Neural Network For Countering Antiforensic Attack on Median Filtered Images","D. B. Tariang; R. S. Chakraborty; R. Naskar","Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India; Department of Information Technology, Indian Institute of Engineering Science and Technology Shibpur, Howrah, India","IEEE Signal Processing Letters","","2019","26","8","1132","1136","Recently, antiforensic methods have been proposed that invalidate most of the state-of-the-art median filter digital image forensic techniques. Also, the existing counter antiforensic methods decline noticeably when evaluated on small-sized patches in JPEG compressed images. In this letter, we have developed a robust residual dense (Neural) network-based counter antiforensic median filter detection technique that exploits local dense connection and residual learning of features for improved classification of images. Experimental results demonstrate that the proposed approach achieves superior performance to state-of-the-art techniques in detecting forgeries, even in small patches, in JPEG compressed images, for both median filtered and antiforensic median filtered images.","","","10.1109/LSP.2019.2922498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735937","Antiforensic attack;deep neural network (DNN);digital image forensics;median filter forensics;residual dense network (RDN)","Feature extraction;Forensics;Transform coding;Image coding;Neural networks;Detectors;Training","data compression;image classification;image coding;image filtering;image forensics;learning (artificial intelligence);median filters;neural nets","small-sized patches;local dense connection;antiforensic median filtered images;image classification;residual learning;robust residual dense network-based counter antiforensic median filter detection technique;robust residual dense neural network;antiforensic attack;median filter digital image forensic techniques;JPEG compressed images","","","40","","","","","IEEE","IEEE Journals"
"EMBDN: An Efficient Multiclass Barcode Detection Network for Complicated Environments","J. Jia; G. Zhai; J. Zhang; Z. Gao; Z. Zhu; X. Min; X. Yang; G. Guo","Institute of Image Communication and Network Engineering, Shanghai Key Laboratory of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Key Laboratory of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Key Laboratory of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Key Laboratory of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Key Laboratory of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Key Laboratory of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Key Laboratory of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, China; Institute of Deep Learning, Baidu Research, Beijing, China","IEEE Internet of Things Journal","","2019","6","6","9919","9933","This article presents a novel method for efficient barcodes detection in real and complicated environments using a convolutional neural network (CNN)-based model. The method is developed as a preprocess-module of existing decoders to enhance decoding rates. Our method is trained as an end-to-end model to determine accurate locations of four barcode vertexes. Our method consists of four modules: 1) base net module; 2) region proposals generator; 3) classification and regression module; and 4) distortion removal module. The feature of barcodes extracted from the base net is fed to the next module. Region proposals are generated and selected as region of interest (ROI). Then the ROI are forward propagated to the classification and regression module to determine the positions and shapes of the barcodes. Finally, the distortion removal module is used to remove the geometric distortion according to regression parameters acquired from the previous step. The accurate position and distorted barcodes shape can be determined and corrected by our method. We validate our method on a challenging large-scale dataset in experiments. Compared with the previous methods, our method provides an end-to-end solution to determine accurate locations of barcode vertexes, which shows an excellent performance on detection accuracy. In addition, our method can enhance decoding rate through distortion removal.","","","10.1109/JIOT.2019.2933254","National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789638","Barcode detection;complicated environment;convolutional neural network (CNN);end-to-end model","Distortion;Decoding;Feature extraction;Cameras;Internet of Things;Proposals;Two dimensional displays","","","","","51","IEEE","","","","IEEE","IEEE Journals"
"PsyPhy: A Psychophysics Driven Evaluation Framework for Visual Recognition","B. Richard Webster; S. E. Anthony; W. J. Scheirer","Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Psychology, Harvard University, Cambridge, MA, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","9","2280","2286","By providing substantial amounts of data and standardized evaluation protocols, datasets in computer vision have helped fuel advances across all areas of visual recognition. But even in light of breakthrough results on recent benchmarks, it is still fair to ask if our recognition algorithms are doing as well as we think they are. The vision sciences at large make use of a very different evaluation regime known as Visual Psychophysics to study visual perception. Psychophysics is the quantitative examination of the relationships between controlled stimuli and the behavioral responses they elicit in experimental test subjects. Instead of using summary statistics to gauge performance, psychophysics directs us to construct item-response curves made up of individual stimulus responses to find perceptual thresholds, thus allowing one to identify the exact point at which a subject can no longer reliably recognize the stimulus class. In this article, we introduce a comprehensive evaluation framework for visual recognition models that is underpinned by this methodology. Over millions of procedurally rendered 3D scenes and 2D images, we compare the performance of well-known convolutional neural networks. Our results bring into question recent claims of human-like performance, and provide a path forward for correcting newly surfaced algorithmic deficiencies.","","","10.1109/TPAMI.2018.2849989","Intelligence Advanced Research Projects Activity; NSF; NSF SBIR; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8395028","Object recognition;visual psychophysics;neuroscience;psychology;evaluation;deep learning","Visualization;Computer vision;Computational modeling;Psychology;Task analysis;Machine learning;Observers","computer vision;feature extraction;neurophysiology;object recognition;protocols;rendering (computer graphics);visual perception","vision sciences;Visual Psychophysics;visual perception;behavioral responses;experimental test subjects;item-response curves;individual stimulus responses;comprehensive evaluation framework;visual recognition models;standardized evaluation protocols;computer vision","","1","46","","","","","IEEE","IEEE Journals"
"Common Semantic Representation Method Based on Object Attention and Adversarial Learning for Cross-Modal Data in IoV","F. Kou; J. Du; W. Cui; L. Shi; P. Cheng; J. Chen; J. Li","Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Vehicular Technology","","2019","68","12","11588","11598","With the significant development of the Internet of Vehicles (IoV), various modal data, such as image and text, are emerging, which provide data support for good vehicle networking services. In order to make full use of the cross-modal data, we need to establish a common semantic representation to achieve effective measurement and comparison of different modal data. However, due to the heterogeneous distributions of cross-modal data, there exists a semantic gap between them. Although some deep neural network (DNN) based methods have been proposed to deal with this problem, there still exist several challenges: the qualities of the modality-specific features, the structure of the DNN, and the components of the loss function. In this paper, for representing cross-modal data in IoV, we propose a common semantic representation method based on object attention and adversarial learning (OAAL). To acquire high-quality modality-specific feature, in OAAL, we design an object attention mechanism, which links the cross-modal features effectively. To further alleviate the heterogeneous semantic gap, we construct a cross-modal generative adversarial network, which contains two parts: a generative model and a discriminative model. Besides, we also design a comprehensive loss function for the generative model to produce high-quality features. With a minimax game between the two models, we can construct a shared semantic space and generate the unified representations for cross-modal data. Finally, we apply our OAAL on retrieval task, and the results of the experiments have verified its effectiveness.","","","10.1109/TVT.2018.2890405","National Key R&D Program of China; National Natural Science Foundation of China; Science and Technology Major Project of Guangxi; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598987","Cross-modal data;GAN;attention model;Internet of Vehicles","Semantics;Feature extraction;Task analysis;Neural networks;Generative adversarial networks;Data models;Learning systems","","","","","38","IEEE","","","","IEEE","IEEE Journals"
"Fully Automatic Left Atrium Segmentation From Late Gadolinium Enhanced Magnetic Resonance Imaging Using a Dual Fully Convolutional Neural Network","Z. Xiong; V. V. Fedorov; X. Fu; E. Cheng; R. Macleod; J. Zhao","Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Department of Physiology and Cell Biology, The Ohio State University Wexner Medical Center, Columbus, OH, USA; Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand; Department of Bioengineering, The University of Utah, Salt Lake City, UT, USA; Auckland Bioengineering Institute, The University of Auckland, Auckland, New Zealand","IEEE Transactions on Medical Imaging","","2019","38","2","515","524","Atrial fibrillation (AF) is the most prevalent form of cardiac arrhythmia. Current treatments for AF remain suboptimal due to a lack of understanding of the underlying atrial structures that directly sustain AF. Existing approaches for analyzing atrial structures in 3-D, especially from late gadolinium-enhanced (LGE) magnetic resonance imaging, rely heavily on manual segmentation methods that are extremely labor-intensive and prone to errors. As a result, a robust and automated method for analyzing atrial structures in 3-D is of high interest. We have, therefore, developed AtriaNet, a 16-layer convolutional neural network (CNN), on 154 3-D LGE-MRIs with a spatial resolution of 0.625 mm × 0.625 mm × 1.25 mm from patients with AF, to automatically segment the left atrial (LA) epicardium and endocardium. AtriaNet consists of a multi-scaled, dualpathway architecture that captures both the local atrial tissue geometry and the global positional information of LA using 13 successive convolutions and three further convolutions for merging. By utilizing computationally efficient batch prediction, AtriaNet was able to successfully process each 3-D LGE-MRI within 1 min. Furthermore, benchmarking experiments have shown that AtriaNet has outperformed the state-of-the-art CNNs, with a DICE score of 0.940 and 0.942 for the LA epicardium and endocardium, respectively, and an inter-patient variance of <;0.001. The estimated LA diameter and volume computed from the automatic segmentations were accurate to within 1.59 mm and 4.01 cm3 of the ground truths. Our proposed CNN was tested on the largest known data set for LA segmentation, and to the best of our knowledge, it is the most robust approach that has ever been developed for segmenting LGE-MRIs. The increased accuracy of atrial reconstruction and analysis could potentially improve the understanding and treatment of AF.","","","10.1109/TMI.2018.2866845","National Institutes of Health; Health Research Council of New Zealand; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8447517","Atrial fibrillation;convolutional neural network;deep learning;MRIs;segmentation;structural analysis","Three-dimensional displays;Training;Machine learning algorithms;Image segmentation;Magnetic resonance imaging;Convolutional neural networks;Classification algorithms","biomedical MRI;cardiology;convolutional neural nets;diseases;image enhancement;image segmentation;medical image processing","endocardium;local atrial tissue geometry;LGE-MRI;automatic segmentations;atrial reconstruction;atrium segmentation;dual fully convolutional neural network;atrial fibrillation;cardiac arrhythmia;manual segmentation methods;left atrial segmentation;atrial structure;convolutional neural network;AtriaNet;late gadolinium-enhanced magnetic resonance imaging","","1","60","","","","","IEEE","IEEE Journals"
"A 64-mW DNN-Based Visual Navigation Engine for Autonomous Nano-Drones","D. Palossi; A. Loquercio; F. Conti; E. Flamand; D. Scaramuzza; L. Benini","Integrated Systems Laboratory, ETH Zürich, Zürich, Switzerland; Department of Informatics, Robotic and Perception Group, University of Zürich, Zürich, Switzerland; Integrated Systems Laboratory, ETH Zürich, Zürich, Switzerland; Integrated Systems Laboratory, ETH Zürich, Zürich, Switzerland; Department of Informatics, Robotic and Perception Group, University of Zürich, Zürich, Switzerland; Integrated Systems Laboratory, ETH Zürich, Zürich, Switzerland","IEEE Internet of Things Journal","","2019","6","5","8357","8371","Fully miniaturized robots (e.g., drones), with artificial intelligence (AI)-based visual navigation capabilities, are extremely challenging drivers of Internet-of-Things edge intelligence capabilities. Visual navigation based on AI approaches, such as deep neural networks (DNNs) are becoming pervasive for standard-size drones, but are considered out of reach for nano-drones with a size of a few cm2. In this paper, we present the first (to the best of our knowledge) demonstration of a navigation engine for autonomous nano-drones capable of closed-loop end-to-end DNN-based visual navigation. To achieve this goal we developed a complete methodology for parallel execution of complex DNNs directly on board resource-constrained milliwatt-scale nodes. Our system is based on GAP8, a novel parallel ultralow-power computing platform, and a 27-g commercial, open-source Crazyflie 2.0 nano-quadrotor. As part of our general methodology, we discuss the software mapping techniques that enable the DroNet state-of-the-art deep convolutional neural network to be fully executed aboard within a strict 6 frame-per-second real-time constraint with no compromise in terms of flight results, while all processing is done with only 64 mW on average. Our navigation engine is flexible and can be used to span a wide performance range: at its peak performance corner, it achieves 18 frames/s while still consuming on average just 3.5% of the power envelope of the deployed nano-aircraft. To share our key findings with the embedded and robotics communities and foster further developments in autonomous nano-unmanned aerial vehicles (UAVs), we publicly release all our code, datasets, and trained networks.","","","10.1109/JIOT.2019.2917066","EC H2020 OPRECOMP Project; ALOHA Project; National Centre of Competence in Research Robotics; SNSF-ERC Starting Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715489","Autonomous UAV;CNNs;end-to-end learning;nano-UAV;ultralow-power","Navigation;Internet of Things;Visualization;Engines;Drones;Robot sensing systems","aircraft control;artificial intelligence;attitude control;autonomous aerial vehicles;convolutional neural nets;helicopters;microrobots;mobile robots;multi-robot systems;neurocontrollers;path planning;remotely operated vehicles;robot vision;trajectory control","64-mW DNN-based visual navigation engine;autonomous nanodrones;Internet-of-Things edge intelligence capabilities;AI approaches;deep neural networks;standard-size drones;closed-loop end-to-end DNN-based visual navigation;board resource-constrained milliwatt-scale nodes;ultralow-power computing platform;DroNet state-of-the-art deep convolutional neural network;deployed nanoaircraft;artificial intelligence;open-source Crazyflie 2.0 nanoquadrotor;autonomous nanounmanned aerial vehicles;UAVs","","1","46","","","","","IEEE","IEEE Journals"
"Unsupervised Domain Adaptation for Micro-Doppler Human Motion Classification via Feature Fusion","Y. Lang; Q. Wang; Y. Yang; C. Hou; D. Huang; W. Xiang","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; College of Science and Engineering, James Cook University, Cairns, QLD, Australia","IEEE Geoscience and Remote Sensing Letters","","2019","16","3","392","396","Micro-Doppler-based human motion classification has become a topical area of research recently. However, the current research is limited by the lack of labeled training data. Domain adaptation, namely, the ability to take advantage of knowledge from an available source data set and apply it to an unlabeled target data set, is useful in this situation. A typical strategy for this transfer learning technique is to extract domain-invariant feature representations. In this letter, an unsupervised domain adaptation method for micro-Doppler classification is proposed. Given no available measurement training samples, we creatively utilize the motion capture database as an auxiliary and adapt its interior knowledge to the measurement data set. To achieve domain-invariant features, three types of features are extracted and fused including low-level deep features from the convolutional neural network, empirical features, and statistical features. After feature fusion, a k-nearest neighbor classifier is applied to the measurement data to classify seven human activities. Experimental results show that our approach outperforms several state-of-the-art unsupervised domain adaptation methods. The impact of the output from different convolution layers is further investigated, and ablation studies of the efficacy of each feature are also carried out in this letter.","","","10.1109/LGRS.2018.2873776","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8497048","Domain adaptation;human motion classification;micro-Doppler","Feature extraction;Spectrogram;Radar;Data models;Motion measurement;Convolution;Doppler effect","convolutional neural nets;Doppler radar;feature extraction;image classification;image fusion;image motion analysis;learning (artificial intelligence);pattern classification;radar imaging","feature fusion;labeled training data;transfer learning technique;domain-invariant feature representations;unsupervised domain adaptation method;motion capture database;measurement data;domain-invariant features;empirical features;statistical features;unsupervised domain adaptation methods;low-level deep features;microdoppler-based human motion classification;convolutional neural network;feature extraction;k-nearest neighbor classifier","","1","21","","","","","IEEE","IEEE Journals"
"Fusion of Multiple Edge-Preserving Operations for Hyperspectral Image Classification","P. Duan; X. Kang; S. Li; P. Ghamisi; J. A. Benediktsson","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","12","10336","10349","In this article, a novel hyperspectral image (HSI) classification method based on fusing multiple edge-preserving operations (EPOs) is proposed, which consists of the following steps. First, the edge-preserving features are obtained by performing different types of EPOs, i.e., local edge-preserving filtering and global edge-preserving smoothing on the dimension-reduced HSI. Then, with the assistance of a superpixel segmentation method, the edge-preserving features are further improved by considering the inter and intra spectral properties of superpixels. Finally, the spectral and edge-preserving features are fused to form one composite kernel, which is fed into the support vector machine (SVM) followed by a majority voting fusion scheme. Experimental results on three data sets demonstrate the superiority of the proposed method over several state-of-the-art classification approaches, especially when the training sample size is limited. Furthermore, 21 well-known methods, including mathematical morphology-based approaches, sparse representation models, and deep learning-based classifiers, are adopted to be compared with the proposed method on Houston data set with standard sets of training and test samples released during 2013 Data Fusion Contest, which also shows the effectiveness of the proposed method.","","","10.1109/TGRS.2019.2933588","National Natural Science Foundation of China; National Natural Science Foundation of China; National Natural Science Foundation of China; Fund of the Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province; Fund of Hunan Province for the Science and Technology Plan Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8821552","Decision fusion;edge-preserving operation (EPO);feature extraction;hyperspectral image (HSI);image classification","Image edge detection;Smoothing methods;Feature extraction;Support vector machines;Transforms;Hyperspectral imaging","edge detection;feature extraction;hyperspectral imaging;image classification;image fusion;image representation;image segmentation;learning (artificial intelligence);mathematical morphology;support vector machines","EPOs;local edge-preserving filtering;global edge-preserving smoothing;dimension-reduced HSI;superpixel segmentation method;edge-preserving features;multiple edge-preserving operations;hyperspectral image classification method;support vector machine;SVM;voting fusion scheme;mathematical morphology-based approaches;sparse representation models;deep learning-based classifiers;Houston data set;data fusion contest;spectral-preserving features","","","62","IEEE","","","","IEEE","IEEE Journals"
"Cooperative Sensing and Wearable Computing for Sequential Hand Gesture Recognition","X. Zhang; Z. Yang; T. Chen; D. Chen; M. Huang","Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, OH, USA; School of Electrical and Electronic Engineering, Huazhong University of Science and Technology, Wuhan, China; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, OH, USA; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, OH, USA; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, OH, USA","IEEE Sensors Journal","","2019","19","14","5775","5783","Hand gestures recognition (HGR) has been considered as one of the crucial research fields of human-computer interaction (HCI). Computer vision is a very active research field in the HGR, traditional vision-based methods, which used camera and ultrasonic/optical sensor to collect the videos or images of the hand gestures shown by participants, have some limitations, such as fixed in-lab location, complex lighting conditions, and cluttered backgrounds. In order to provide new approaches, we described the development of a novel hand gesture recognition system that combined wearable armband and smart glove made by customizable pressure sensor arrays to detect sequential hand gestures. A deep learning technique long short-term memory (LSTM) algorithm had been computed to build an effective model to classify hand gestures by training and testing the collected inertial measurement unit (IMU), electromyographic (EMG), and finger and palm's pressure data. Furthermore, we built a relatively large database of ten sequential hand gestures consisted by five dynamic gestures and five air gestures collected from ten participants. Our experimental results showed an outstanding classification performance of the proposed LSTM algorithm. These findings have promising implications for sequential hand gesture recognition and the HCI research status.","","","10.1109/JSEN.2019.2904595","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8665985","Sequential hand gesture recognition;LSTM;wearable sensor devices;MYO armband;customizable pressure sensor arrays","Pressure sensors;Gesture recognition;Sensor arrays;Array signal processing;Human computer interaction","body sensor networks;electromyography;gesture recognition;human computer interaction;learning (artificial intelligence);medical signal processing;pressure sensors;sensor arrays","computer vision;traditional vision-based methods;hand gesture recognition system;dynamic gestures;air gestures;sequential hand gesture recognition;human-computer interaction;pressure sensor arrays;ultrasonic-optical sensor;wearable computing;wearable armband;smart glove;deep learning technique long short-term memory algorithm;LSTM algorithm;inertial measurement unit;electromyographic data;EMG;finger pressure data;palm's pressure data","","1","33","","","","","IEEE","IEEE Journals"
"Automatic Calibration of a Six-Degrees-of-Freedom Pose Estimation System","W. Jansen; D. Laurijssen; W. Daems; J. Steckel","CoSys Lab, Faculty of Applied Engineering, University of Antwerp, Antwerp, Belgium; CoSys Lab, Faculty of Applied Engineering, University of Antwerp, Antwerp, Belgium; CoSys Lab, Faculty of Applied Engineering, University of Antwerp, Antwerp, Belgium; CoSys Lab, Faculty of Applied Engineering, University of Antwerp, Antwerp, Belgium","IEEE Sensors Journal","","2019","19","19","8824","8831","Systems for estimating the six-degrees-of-freedom human body pose have been improving for over two decades. Technologies such as motion capture cameras, advanced gaming peripherals and more recently both deep learning techniques and virtual reality systems have shown impressive results. However, most systems that provide high accuracy and high precision are expensive and not easy to operate. Recently, research has been carried out to estimate the human body pose using the HTC Vive virtual reality system. This system shows accurate results while keeping the cost under a 1000 USD. This system uses an optical approach. Two transmitter devices emit infrared pulses and laser planes are tracked by use of photo diodes on receiver hardware. A system using these transmitter devices combined with low-cost custom-made receiver hardware was developed previously but requires manual measurement of the position and orientation of the transmitter devices. These manual measurements can be time consuming, prone to error and not possible in particular setups. We propose an algorithm to automatically calibrate the poses of the transmitter devices in any chosen environment with custom receiver/calibration hardware. Results show that the calibration works in a variety of setups while being more accurate than what manual measurements would allow. Furthermore, the calibration movement and speed has no noticeable influence on the precision of the results.","","","10.1109/JSEN.2019.2921644","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733070","Pose estimation;motion capture;sensor arrays;distributed embedded systems;calibration","Calibration;Hardware;Pose estimation;Receivers;Measurement by laser beam;Interpolation;Time measurement","biomedical optical imaging;cameras;learning (artificial intelligence);medical image processing;photodiodes;pose estimation;virtual reality","low-cost custom-made receiver hardware;calibration movement;photodiodes;manual measurement;transmitter devices;HTC Vive virtual reality system;virtual reality systems;deep learning techniques;advanced gaming peripherals;motion capture cameras;six-degrees-of-freedom human body;six-degrees-of-freedom pose estimation system;automatic calibration","","","27","","","","","IEEE","IEEE Journals"
"A 3-D Atrous Convolution Neural Network for Hyperspectral Image Denoising","W. Liu; J. Lee","Department of Computer Engineering, Artificial Intelligence Laboratory, Chonbuk National University, Jeonju, South Korea; Department of Computer Engineering, Artificial Intelligence Laboratory, Chonbuk National University, Jeonju, South Korea","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","8","5701","5715","Deep learning, especially a discriminative model for image denoising, has shown great potential in removing complex spectral-spatial noise in hyperspectral images (HSI). For HSI denoising, it is crucial to extract more context information around each pixel and to predict each pixel according to the surrounding context. Therefore, the effective receptive field plays an important role when performing denoising task. Generally, an HSI denoising model can achieve better performance by reserving the correlation of adjacent spectral bands and extracting more pixel features in the spatial domain. In this paper, 3-D atrous denoising convolution neural network (3DADCNN) is proposed for HSI. The model extracts feature maps along both spatial and spectral dimensions and enlarges the receptive field without significantly increasing the number of network parameters. Simultaneously, the multibranch and multiscale structure is utilized to reduce training difficulty, lessen overfitting risk, and preserve details in texture. The proposed model can be applied to the corrupted image with a mixed type of photon and thermal noise. Experimental results of the quantitative and qualitative evaluation show that 3DADCNN outperforms state-of-the-art HSI denoising methods.","","","10.1109/TGRS.2019.2901737","The Korea Energy Technology Evaluation and Planning (KETEP); Energy Efficiency and Resources Core Technology Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676115","3-D convolution neural network (CNN);atrous convolution;hyperspectral images (HSI) denoising;mixed noise;multiscale structure","Noise reduction;Convolution;Feature extraction;Neural networks;Data mining;Task analysis;Hyperspectral sensors","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image denoising;image representation;learning (artificial intelligence)","spatial dimensions;spectral dimensions;network parameters;corrupted image;3DADCNN;3-D atrous convolution neural network;hyperspectral image denoising;deep learning;discriminative model;hyperspectral images;context information;effective receptive field;HSI denoising model;adjacent spectral bands;pixel features;spatial domain;3-D atrous denoising convolution neural network;complex spectral-spatial noise","","","66","","","","","IEEE","IEEE Journals"
"RETOUCH: The Retinal OCT Fluid Detection and Segmentation Benchmark and Challenge","H. Bogunović; F. Venhuizen; S. Klimscha; S. Apostolopoulos; A. Bab-Hadiashar; U. Bagci; M. F. Beg; L. Bekalo; Q. Chen; C. Ciller; K. Gopinath; A. K. Gostar; K. Jeon; Z. Ji; S. H. Kang; D. D. Koozekanani; D. Lu; D. Morley; K. K. Parhi; H. S. Park; A. Rashno; M. Sarunic; S. Shaikh; J. Sivaswamy; R. Tennakoon; S. Yadav; S. De Zanet; S. M. Waldstein; B. S. Gerendas; C. Klaver; C. I. Sánchez; U. Schmidt-Erfurth","Department of Ophthalmology, Christian Doppler Laboratory for Ophthalmic Image Analysis, Medical University of Vienna, Vienna, Austria; Department of Radiology and Nuclear Medicine, Diagnostic Image Analysis Group, Radboud University Medical Center, Nijmegen, GA, The Netherlands; Department of Ophthalmology, Christian Doppler Laboratory for Ophthalmic Image Analysis, Medical University of Vienna, Vienna, Austria; RetinAI Medical GmbH, Bern, Switzerland; School of Engineering, RMIT University, Melbourne, VIC, Australia; Center for Research in Computer Vision, University of Central Florida, Orlando, FL, USA; School of Engineering Science, Simon Fraser University, Burnaby, BC, Canada; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; RetinAI Medical GmbH, Bern, Switzerland; IIIT Hyderabad, Hyderabad, India; School of Engineering, RMIT University, Melbourne, VIC, Australia; National Institute for Mathematical Sciences, Daejeon, South Korea; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; National Institute for Mathematical Sciences, Daejeon, South Korea; Department of Ophthalmology and Visual Neuroscience, University of Minnesota, Minneapolis, MN, USA; School of Engineering Science, Simon Fraser University, Burnaby, BC, Canada; Center for Research in Computer Vision, University of Central Florida, Orlando, FL, USA; Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN, USA; National Institute for Mathematical Sciences, Daejeon, South Korea; Department of Computer Engineering, Engineering Faculty, Lorestan University, Khorramabad, Iran; School of Engineering Science, Simon Fraser University, Burnaby, BC, Canada; Center for Research in Computer Vision, University of Central Florida, Orlando, FL, USA; IIIT Hyderabad, Hyderabad, India; School of Engineering, RMIT University, Melbourne, VIC, Australia; IIIT Hyderabad, Hyderabad, India; RetinAI Medical GmbH, Bern, Switzerland; Department of Ophthalmology, Christian Doppler Laboratory for Ophthalmic Image Analysis, Medical University of Vienna, Vienna, Austria; Department of Ophthalmology, Christian Doppler Laboratory for Ophthalmic Image Analysis, Medical University of Vienna, Vienna, Austria; Department of Ophthalmology, Erasmus University Medical Center, Rotterdam, DR, The Netherlands; Department of Radiology and Nuclear Medicine, Diagnostic Image Analysis Group, Radboud University Medical Center, Nijmegen, GA, The Netherlands; Department of Ophthalmology, Christian Doppler Laboratory for Ophthalmic Image Analysis, Medical University of Vienna, Vienna, Austria","IEEE Transactions on Medical Imaging","","2019","38","8","1858","1874","Retinal swelling due to the accumulation of fluid is associated with the most vision-threatening retinal diseases. Optical coherence tomography (OCT) is the current standard of care in assessing the presence and quantity of retinal fluid and image-guided treatment management. Deep learning methods have made their impact across medical imaging, and many retinal OCT analysis methods have been proposed. However, it is currently not clear how successful they are in interpreting the retinal fluid on OCT, which is due to the lack of standardized benchmarks. To address this, we organized a challenge RETOUCH in conjunction with MICCAI 2017, with eight teams participating. The challenge consisted of two tasks: fluid detection and fluid segmentation. It featured for the first time: all three retinal fluid types, with annotated images provided by two clinical centers, which were acquired with the three most common OCT device vendors from patients with two different retinal diseases. The analysis revealed that in the detection task, the performance on the automated fluid detection was within the inter-grader variability. However, in the segmentation task, fusing the automated methods produced segmentations that were superior to all individual methods, indicating the need for further improvements in the segmentation performance.","","","10.1109/TMI.2019.2901398","Christian Doppler Forschungsgesellschaft; Austrian Federal Ministry for Digital and Economic Affairs; National Foundation for Research, Technology and Development; MD fonds; LSBS fonds; OOG fonds through UitZicht; Institute of Mathematical Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653407","Evaluation;image segmentation;image classification;optical coherence tomography;retina","Retina;Image segmentation;Diseases;Biomedical imaging;Image analysis;Fluids;Benchmark testing","biomedical optical imaging;diseases;eye;image segmentation;learning (artificial intelligence);medical image processing;optical tomography","retinal OCT fluid detection;optical coherence tomography;image-guided treatment management;medical imaging;retinal OCT analysis methods;fluid segmentation;retinal fluid types;automated fluid detection;segmentation task;retinal diseases;OCT device vendors;RETOUCH;retinal swelling;deep learning","","","78","","","","","IEEE","IEEE Journals"
"One-for-All: Grouped Variation Network-Based Fractional Interpolation in Video Coding","J. Liu; S. Xia; W. Yang; M. Li; D. Liu","Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China; CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application Systems, University of Science and Technology of China, Hefei, China","IEEE Transactions on Image Processing","","2019","28","5","2140","2151","Fractional interpolation is used to provide sub-pixel level references for motion compensation in the interprediction of video coding, which attempts to remove temporal redundancy in video sequences. Traditional handcrafted fractional interpolation filters face the challenge of modeling discontinuous regions in videos, while existing deep learning-based methods are either designed for a single quantization parameter (QP), only generating half-pixel samples, or need to train a model for each sub-pixel position. In this paper, we present a one-for-all fractional interpolation method based on a grouped variation convolutional neural network (GVCNN). Our method can deal with video frames coded using different QPs and is capable of generating all sub-pixel positions at one sub-pixel level. Also, by predicting variations between integer-position pixels and sub-pixels, our network offers more expressive power. Moreover, we perform specific measurements in training data generation to simulate practical situations in video coding, including blurring the down-sampled sub-pixel samples to avoid aliasing effects and coding integer pixels to simulate reconstruction errors. In addition, we analyze the impact of the size of blur kernels theoretically. Experimental results verify the efficiency of GVCNN. Compared with HEVC, our method achieves 2.2% in bit saving on average and up to 5.2% under low-delay P configuration.","","","10.1109/TIP.2018.2882923","National Natural Science Foundation of China; Peking University; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543165","High efficient video coding (HEVC);fractional interpolation;convolutional neural network (CNN);grouped variation network","Interpolation;Video coding;Training data;Standards;Motion compensation;Redundancy","convolutional neural nets;image filtering;image restoration;image sequences;interpolation;learning (artificial intelligence);motion compensation;video coding","GVCNN;motion compensation;fractional interpolation;grouped variation network;deep learning;grouped variation convolutional neural network;fractional interpolation filters;video sequences;video coding","","3","35","","","","","IEEE","IEEE Journals"
"Analyzing and Detecting Emerging Internet of Things Malware: A Graph-Based Approach","H. Alasmary; A. Khormali; A. Anwar; J. Park; J. Choi; A. Abusnaina; A. Awad; D. Nyang; A. Mohaisen","Department of Computer Science, King Khalid University, Abha, Saudi Arabia; Department of Computer Science, University of Central Florida, Orlando, FL, USA; Department of Computer Science, University of Central Florida, Orlando, FL, USA; Department of Computer Science, University of Central Florida, Orlando, FL, USA; Department of Computer Science, University of Central Florida, Orlando, FL, USA; Department of Computer Science, University of Central Florida, Orlando, FL, USA; Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA; Department of Computer Engineering, Inha University, Incheon, South Korea; Department of Computer Science, University of Central Florida, Orlando, FL, USA","IEEE Internet of Things Journal","","2019","6","5","8977","8988","The steady growth in the number of deployed Internet of Things (IoT) devices has been paralleled with an equal growth in the number of malicious software (malware) targeting those devices. In this paper, we build a detection mechanism of IoT malware utilizing control flow graphs (CFGs). To motivate for our detection mechanism, we contrast the underlying characteristics of IoT malware to other types of malware-Android malware, which are also Linux-based-across multiple features. The preliminary analyses reveal that the Android malware have high density, strong closeness and betweenness, and a larger number of nodes. We show that IoT malware samples have a large number of edges despite a smaller number of nodes, which demonstrate a richer flow structure and higher complexity. We utilize those various characterizing features as a modality to build a highly effective deep learning-based detection model to detect IoT malware. To test our model, we use CFGs of about 6000 malware and benign IoT disassembled samples, and show a detection accuracy of ≈99.66 %.","","","10.1109/JIOT.2019.2925929","Air Force Research Laboratory; National Science Foundation; National Research Foundation of Korea; Cyber Florida Seed Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752028","Android;graph analysis;Internet of Things (IoT);IoT detection;malware","Malware;Internet of Things;Feature extraction;Tools;Computer science;Security","Android (operating system);flow graphs;Internet of Things;invasive software;learning (artificial intelligence)","graph-based approach;malicious software;IoT malware samples;Internet of Things devices;control flow graphs;Android malware;deep learning-based detection model;CFGs;Linux;flow structure","","1","52","","","","","IEEE","IEEE Journals"
"An Estimation Method of Defect Size From MFL Image Using Visual Transformation Convolutional Neural Network","S. Lu; J. Feng; H. Zhang; J. Liu; Z. Wu","School of Information Science and Eng-ineering, Northeastern University, Shenyang, China; School of Information Science and Eng-ineering, Northeastern University, Shenyang, China; School of Information Science and Eng-ineering, Northeastern University, Shenyang, China; School of Information Science and Eng-ineering, Northeastern University, Shenyang, China; School of Information Science and Eng-ineering, Northeastern University, Shenyang, China","IEEE Transactions on Industrial Informatics","","2019","15","1","213","224","In most current nondestructive testing systems, a magnetic flux leakage (MFL) method is widely used in various industry fields, where the structural integrity of specimens is of vital importance. The estimation of defect size in specimen from the MFL measurements is a key and difficult problem. The traditional methods have low precision because feature extraction procedure relies on prior knowledge and the ability of designer. Inspired by the idea of convolutional neural network (CNN), a novel visual transformation CNN (VT-CNN) is proposed in this paper to overcome the limitation of traditional method in a feature extraction procedure. By adding a visual transformation layer according to the characteristics of the MFL measurements, the VT-CNN can distinguish the defect feature with different sizes more accurately. Moreover, since the VT-CNN method is designed based on the deep learning theory, more industrial big data with accurate label should be used to train the network. Due to the difficulty of making real industrial big data, a novel mesher magnetic dipole model is designed to simulate this industrial process. A large simulated MFL measurement of irregular defects produced by this model can increase the number of training samples and improve the robustness of the network. Experiments to estimate natural corrosion defects on real industrial pipelines are performed to validate the proposed framework. The experimental results are illustrated in detail, which highlights the superiority of the proposed method in industrial applications.","","","10.1109/TII.2018.2828811","National Key R&D Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8344425","Convolutional neural network (CNN);fault detection;intelligence fault diagnosis;magnetic flux leakage (MFL);mesher magnetic dipole;visual transformation","Feature extraction;Estimation;Visualization;Magnetic resonance imaging;Size measurement;Training;Numerical models","Big Data;convolutional neural nets;corrosion;fault diagnosis;feature extraction;learning (artificial intelligence);magnetic flux;magnetic leakage;magnetic moments;nondestructive testing;pipelines;production engineering computing","MFL image;visual transformation convolutional neural network;magnetic flux leakage method;MFL measurements;feature extraction procedure;VT-CNN method;deep learning theory;industrial big data;simulated MFL measurement;irregular defects;natural corrosion defects;nondestructive testing systems;defect size estimation method;visual transformation CNN;mesher magnetic dipole model","","3","39","","","","","IEEE","IEEE Journals"
"Saliency Integration: An Arbitrator Model","Y. Xu; X. Hong; F. Porikli; X. Liu; J. Chen; G. Zhao","Center for Machine Vision and Signal Analysis, University of Oulu, Oulu, Finland; Center for Machine Vision and Signal Analysis, University of Oulu, Oulu, Finland; Research School of Engineering, Australian National University, Canberra, ACT, Australia; Center for Machine Vision and Signal Analysis, University of Oulu, Oulu, Finland; Center for Machine Vision and Signal Analysis, University of Oulu, Oulu, Finland; Center for Machine Vision and Signal Analysis, University of Oulu, Oulu, Finland","IEEE Transactions on Multimedia","","2019","21","1","98","113","Saliency integration has attracted much attention on unifying saliency maps from multiple saliency models. Previous offline integration methods usually face two challenges: 1) if most of the candidate saliency models misjudge the saliency on an image, the integration result will lean heavily on those inferior candidate models; and 2) an unawareness of the ground truth saliency labels brings difficulty in estimating the expertise of each candidate model. To address these problems, in this paper, we propose an arbitrator model (AM) for saliency integration. First, we incorporate the consensus of multiple saliency models and the external knowledge into a reference map to effectively rectify the misleading by candidate models. Second, our quest for ways of estimating the expertise of the saliency models without ground truth labels gives rise to two distinct online model-expertise estimation methods. Finally, we derive a Bayesian integration framework to reconcile the saliency models of varying expertise and the reference map. To extensively evaluate the proposed AM model, we test 27 state-of-the-art saliency models, covering both traditional and deep learning ones, on various combinations over four datasets. The evaluation results show that the AM model improves the performance substantially compared to the existing state-of-the-art integration methods, regardless of the chosen candidate saliency models.","","","10.1109/TMM.2018.2856126","Tekes Fidipro Program; Tekes project; Academy of Finland; Infotech; Natural Science Foundation of China; CSC-IT Center for Science, Finland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411135","Saliency integration;saliency aggregation;online model;arbitrator model","Computational modeling;Bayes methods;Estimation;Adaptation models;Predictive models;Biological system modeling;Heating systems","Bayes methods;learning (artificial intelligence);object detection;visual perception","AM model;saliency integration;arbitrator model;Bayesian integration framework;truth labels;deep learning;estimation methods;online model","","1","80","","","","","IEEE","IEEE Journals"
"Codebook-Free Compact Descriptor for Scalable Visual Search","Y. Wu; F. Gao; Y. Huang; J. Lin; V. Chandrasekhar; J. Yuan; L. Duan","Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Future Lab, Tsinghua University, Beijing, China; Institute of Digital Media, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; Institute for Infocomm Research, Singapore; Institute for Infocomm Research, Singapore; Department of Computer Science and Engineering, State University of New York, Buffalo, NY, USA; Institute of Digital Media, School of Electronics Engineering and Computer Science, Peking University, Beijing, China","IEEE Transactions on Multimedia","","2019","21","2","388","401","The MPEG compact descriptors for visual search (CDVS) is a standard toward image matching and retrieval. To achieve high retrieval accuracy over a large scale image/video dataset, recent research efforts have demonstrated that employing extremely high-dimensional descriptors such as the Fisher vector (FV) and the vector of locally aggregated descriptors (VLAD) can yield good performance. Since the FV (or VLAD) possesses high discriminability but small visual vocabulary, it has been adopted by CDVS to construct a global compact descriptor. In this paper, we study the development of global compact descriptors in the completed CDVS standard and the emerging compact descriptors for video analysis (CDVA) standard, in which we formulate the FV (or VLAD) compression as a resource-constrained optimization problem. Accordingly, we propose a codebook-free aggregation method via dual selection to generate a global compact visual descriptor, which supports fast and accurate feature matching free of large visual codebooks, fulfilling the low memory requirement of mobile visual search at significantly reduced latency. Specifically, we investigate both sample-specific Gaussian component redundancy and bit dependency within a binary aggregated descriptor to produce compact binary codes. Our technique contributes to the scalable compressed Fisher vector (SCFV) adopted by the CDVS standard. Moreover, the SCFV descriptor is currently serving as the frame-level hand-crafted video feature, which inspires the inheritance of CDVS descriptors for the emerging CDVA standard. Furthermore, we investigate the positive complementary effect of our standard compliant compact descriptor and deep learning based features extracted from convolutional neural networks with significant mean average precision gains. Extensive evaluation over benchmark databases shows the significant merits of the codebook-free binary codes for scalable visual search.","","","10.1109/TMM.2018.2856628","National Natural Science Foundation of China; National Key Research and Development Program of China; Key Research and Development Program of Beijing Municipal Science & Technology Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8412594","Visual Search;Compact Descriptor;CDVS;CDVA;Codebook free;Feature Descriptor Aggregation","Visualization;Standards;Feature extraction;Binary codes;Transform coding;Redundancy;Complexity theory","binary codes;convolutional neural nets;data compression;feature extraction;Gaussian processes;image matching;image representation;image retrieval;learning (artificial intelligence);optimisation;video signal processing","vector of locally aggregated descriptors;VLAD;CDVA standard;scale image-video dataset;image retrieval;compact descriptor for video analysis standard;resource-constrained optimization problem;dual selection;feature matching;sample-specific Gaussian component redundancy;positive complementary effect;deep learning based feature extraction;convolutional neural networks;mean average precision gains;small visual vocabulary;codebook-free binary codes;standard compliant compact descriptor;CDVS descriptors;frame-level hand-crafted video feature;SCFV descriptor;compact binary codes;binary aggregated descriptor;bit dependency;mobile visual search;visual codebooks;global compact visual descriptor;codebook-free aggregation method;video analysis standard;completed CDVS standard;global compact descriptor;visual vocabulary;FV;Fisher vector;high-dimensional descriptors;high retrieval accuracy;image matching;MPEG compact descriptors;scalable visual search;codebook-free compact descriptor","","","65","","","","","IEEE","IEEE Journals"
"Robust Depth Estimation Using Auto-Exposure Bracketing","S. Im; H. Jeon; I. S. Kweon","School of Electrical Engineering, KAIST, Daejeon, South Korea; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; School of Electrical Engineering, KAIST, Daejeon, South Korea","IEEE Transactions on Image Processing","","2019","28","5","2451","2464","As the computing power of handheld devices grows, there has been increasing interest in the capture of depth information to enable a variety of photographic applications. However, under low-light conditions, most devices still suffer from low imaging quality and inaccurate depth acquisition. To address the problem, we present a robust depth estimation method from a short burst shot with varied intensity (i.e., auto-exposure bracketing) and/or strong noise (i.e., high ISO). Our key idea synergistically combines deep convolutional neural networks with a geometric understanding of the scene. We introduce a geometric transformation between optical flow and depth tailored for burst images, enabling our learning-based multi-view stereo matching to be performed effectively. We then describe our depth estimation pipeline that incorporates this geometric transformation into our residual-flow network. It allows our framework to produce an accurate depth map even with a bracketed image sequence. We demonstrate that our method outperforms the state-of-the-art methods for various datasets captured by a smartphone and a DSLR camera. Moreover, we show that the estimated depth is applicable for image quality enhancement and photographic editing.","","","10.1109/TIP.2018.2886777","Technology Innovation Program; Korea Evaluation Institute of Industrial Technology; National Research Foundation of Korea; Ministry of Education; Basic Science Research Program through the NRF of Korea; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576538","Depth estimation;exposure fusion;image denoising;3D reconstruction;geometry;convolutional neural network","Cameras;Estimation;Optical imaging;Three-dimensional displays;Noise reduction;Photography","convolutional neural nets;image enhancement;image fusion;image matching;image reconstruction;image sensors;image sequences;learning (artificial intelligence);stereo image processing","auto-exposure bracketing;computing power;handheld devices;depth information;photographic applications;low-light conditions;low imaging quality;inaccurate depth acquisition;robust depth estimation method;high ISO;deep convolutional neural networks;geometric understanding;geometric transformation;optical flow;burst images;learning-based multiview stereo matching;depth estimation pipeline;residual-flow network;accurate depth map;bracketed image sequence;image quality enhancement","","","54","","","","","IEEE","IEEE Journals"
"Context-Aware Mouse Behavior Recognition Using Hidden Markov Models","Z. Jiang; D. Crookes; B. D. Green; Y. Zhao; H. Ma; L. Li; S. Zhang; D. Tao; H. Zhou","Department of Informatics, University of Leicester, Leicester, U.K; School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K; School of Biological Sciences, Queen’s University Belfast, Belfast, U.K; School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K; Department of Electrical Engineering, Shaoxing University, Shaoxing, China; School of Computing, University of Kent, Canterbury, U.K.; School of Computer Science and Technology, Harbin Institute of Technology, Weihai, China; UBTECH Sydney Artificial Intelligence Centre, Faculty of Engineering and Information Technologies, School of Information Technologies, The University of Sydney, Darlington, NSW, Australia; Department of Informatics, University of Leicester, Leicester, U.K","IEEE Transactions on Image Processing","","2019","28","3","1133","1148","Automated recognition of mouse behaviors is crucial in studying psychiatric and neurologic diseases. To achieve this objective, it is very important to analyze the temporal dynamics of mouse behaviors. In particular, the change between mouse neighboring actions is swift in a short period. In this paper, we develop and implement a novel hidden Markov model (HMM) algorithm to describe the temporal characteristics of mouse behaviors. In particular, we here propose a hybrid deep learning architecture, where the first unsupervised layer relies on an advanced spatial-temporal segment Fisher vector encoding both visual and contextual features. Subsequent supervised layers based on our segment aggregate network are trained to estimate the state-dependent observation probabilities of the HMM. The proposed architecture shows the ability to discriminate between visually similar behaviors and results in high recognition rates with the strength of processing imbalanced mouse behavior datasets. Finally, we evaluate our approach using JHuang's and our own datasets, and the results show that our method outperforms other state-of-the-art approaches.","","","10.1109/TIP.2018.2875335","Engineering and Physical Sciences Research Council; Natural Science Foundation of Zhejiang Province; National Natural Science Foundation of China; Australian Research Council; Royal Society-Newton Advanced Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8488486","Mouse behaviors;hidden Markov model;spatial-temporal segment;Fisher vector;segment aggregate network","Hidden Markov models;Mice;Feature extraction;Diseases;Aggregates;Visualization;Rats","diseases;feature extraction;hidden Markov models;image segmentation;learning (artificial intelligence);zoology","hybrid deep learning architecture;spatial-temporal segment Fisher vector;automated recognition;psychiatric diseases;neurologic diseases;hidden Markov model algorithm;context-aware mouse behavior recognition;mouse behaviors temporal dynamics;supervised layers;segment aggregate network;state-dependent observation probabilities","","","56","","","","","IEEE","IEEE Journals"
"Two-order graph convolutional networks for semi-supervised classification","F. Sichao; L. Weifeng; L. Shuying; Z. Yicong","China University of Petroleum (East China), College of Information and Control Engineering, Qingdao 266580, People's Republic of China; China University of Petroleum (East China), College of Information and Control Engineering, Qingdao 266580, People's Republic of China; Xi'an University of Posts & Telecommunications, School of Automation, Xi'an 710121, People's Republic of China; University of Macau, Department of Computer and Information Science, Macau 999078, People's Republic of China","IET Image Processing","","2019","13","14","2763","2771","Currently, deep learning (DL) algorithms have achieved great success in many applications including computer vision and natural language processing. Many different kinds of DL models have been reported, such as DeepWalk, LINE, diffusionconvolutional neural networks, graph convolutional networks (GCN), and so on. The GCN algorithm is a variant of convolutional neural network and achieves significant superiority by using a one-order localised spectral graph filter. However, only a one-order polynomial in the Laplacian of GCN has been approximated and implemented, which ignores undirect neighbour structure information. The lack of rich structure information reduces the performance of the neural networks in the graph structure data. In this study, the authors deduce and simplify the formula of two-order spectral graph convolutions to preserve rich local information. Furthermore, they build a layerwise GCN based on this two-order approximation, i.e. two-order GCN (TGCN) for semi-supervised classification. With the two-order polynomial in the Laplacian, the proposed TGCN model can assimilate abundant localised structure information of graph data and then boosts the classification significantly. To evaluate the proposed solution, extensive experiments are conducted on several popular datasets including the Citeseer, Cora, and PubMed dataset. Experimental results demonstrate that the proposed TGCN outperforms the state-of-art methods.","","","10.1049/iet-ipr.2018.6224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946955","","","approximation theory;learning (artificial intelligence);pattern classification;graph theory;convolutional neural nets","semisupervised classification;deep learning algorithms;natural language processing;diffusion-convolutional neural networks;GCN algorithm;one-order localised spectral graph filter;one-order polynomial;Laplacian;undirect neighbour structure information;graph structure data;two-order spectral graph convolutions;two-order approximation;two-order polynomial;abundant localised structure information;graph data;computer vision;two-order GCN;layerwise GCN;two-order graph convolutional networks;semi-supervised classification","","","36","","","","","IET","IET Journals"
"Stereo-Matching Network for Structured Light","Q. Du; R. Liu; B. Guan; Y. Pan; S. Sun","Department of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Electronic and Information Engineering, Beihang University, Beijing, China","IEEE Signal Processing Letters","","2019","26","1","164","168","Recently, deep learning has been widely applied in binocular stereo matching for depth acquisition, which has led to an immense increase of accuracy. However, little attention has been paid to the structured light field. In this letter, a network for structured light is proposed to extract effective matching features for depth acquisition. The proposed network promotes the Siamese network by considering receptive fields of different scales and assigning proper weights to the corresponding features, which is achieved by combining pyramid-pooling structure with the squeeze-and-excitation network into the Siamese network for feature extraction and weight calculations, respectively. For network training and testing, a structured-light dataset with amended ground truths is generated by projecting a random pattern into the existing binocular stereo dataset. Experiments demonstrate that the proposed network is capable of real-time depth acquisition, and it provides superior depth maps using structured light.","","","10.1109/LSP.2018.2883865","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550745","Structured light;stereo matching;siamese network;SLNet","Feature extraction;Cameras;Three-dimensional displays;Training;Convolution;Kernel;Testing","feature extraction;image matching;image representation;learning (artificial intelligence);stereo image processing","binocular stereo matching;structured light field;Siamese network;assigning proper weights;corresponding features;pyramid-pooling structure;feature extraction;weight calculations;network training;structured-light dataset;real-time depth acquisition;stereo-matching network;deep learning;squeeze-and-excitation network;binocular stereo dataset","","","23","","","","","IEEE","IEEE Journals"
"A Tensor Factorization Method for 3-D Super Resolution With Application to Dental CT","J. Hatvani; A. Basarab; J. Tourneret; M. Gyöngy; D. Kouamé","CNRS UMR 5505, IRIT, Université Paul Sabatier Toulouse 3, University of Toulouse, Toulouse, France; CNRS UMR 5505, IRIT, Université Paul Sabatier Toulouse 3, University of Toulouse, Toulouse, France; IRIT/INP-ENSEEIHT/Tésa, University of Toulouse, Toulouse, France; Faculty of Information Technology and Bionics, Pazmany Peter Catholic University, Budapest, Hungary; CNRS UMR 5505, IRIT, Université Paul Sabatier Toulouse 3, University of Toulouse, Toulouse, France","IEEE Transactions on Medical Imaging","","2019","38","6","1524","1531","Available super-resolution techniques for 3-D images are either computationally inefficient priorknowledge-based iterative techniques or deep learning methods which require a large database of known lowresolution and high-resolution image pairs. A recently introduced tensor-factorization-based approach offers a fast solution without the use of known image pairs or strict prior assumptions. In this paper, this factorization framework is investigated for single image resolution enhancement with an offline estimate of the system point spread function. The technique is applied to 3-D cone beam computed tomography for dental image resolution enhancement. To demonstrate the efficiency of our method, it is compared to a recent state-of-the-art iterative technique using low-rank and total variation regularizations. In contrast to this comparative technique, the proposed reconstruction technique gives a 2-order-of-magnitude improvement in running time-2 min compared to 2 h for a dental volume of 282 × 266 × 392 voxels. Furthermore, it also offers slightly improved quantitative results (peak signal-to-noise ratio and segmentation quality). Another advantage of the presented technique is the low number of hyperparameters. As demonstrated in this paper, the framework is not sensitive to small changes in its parameters, proposing an ease of use.","","","10.1109/TMI.2018.2883517","European Commission; European Social Fund; Pázmány University; Agence Nationale de la Recherche; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8548574","3D super-resolution;single image super-resolution;tensor factorization;cone beam computed tomography;dental application","Tensile stress;Three-dimensional displays;Dentistry;Spatial resolution;Computed tomography;Two dimensional displays","computerised tomography;image enhancement;image reconstruction;image registration;image resolution;image segmentation;iterative methods;learning (artificial intelligence);matrix decomposition;medical computing;medical image processing;optical transfer function;tensors","3-D cone beam computed tomography;dental image resolution enhancement;total variation regularizations;tensor factorization method;deep learning methods;high-resolution image pairs;single image resolution enhancement;point spread function;3-D super resolution techniques;dental CT;knowledge-based iterative techniques;low resolution image pairs;2-order-of-magnitude","","2","27","","","","","IEEE","IEEE Journals"
"A New Multi-Atlas Registration Framework for Multimodal Pathological Images Using Conventional Monomodal Normal Atlases","Z. Tang; P. Yap; D. Shen","Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Image Processing","","2019","28","5","2293","2304","Using multi-atlas registration (MAR), information carried by atlases can be transferred onto a new input image for the tasks of region-of-interest (ROI) segmentation, anatomical landmark detection, and so on. Conventional atlases used in MAR methods are monomodal and contain only normal anatomical structures. Therefore, the majority of MAR methods cannot handle input multimodal pathological images, which are often collected in routine image-based diagnosis. This is because registering monomodal atlases with normal appearances to multimodal pathological images involves two major problems: 1) missing imaging modalities in the monomodal atlases and 2) influence from pathological regions. In this paper, we propose a new MAR framework to tackle these problems. In this framework, deep learning-based image synthesizers are applied for synthesizing multimodal normal atlases from conventional monomodal normal atlases. To reduce the influence from pathological regions, we further propose a multimodal low-rank approach to recover multimodal normal-looking images from multimodal pathological images. Finally, the multimodal normal atlases can be registered to the recovered multimodal images in a multi-channel way. We evaluate our MAR framework via brain ROI segmentation of multimodal tumor brain images. Due to the utilization of multimodal information and the reduced influence from pathological regions, experimental results show that registration based on our method is more accurate and robust, leading to significantly improved brain ROI segmentation compared with the state-of-the-art methods.","","","10.1109/TIP.2018.2884563","National Institutes of Health; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8579568","Image registration;multimodal image;pathological brain image;image synthesis;low-rank image recovery","Brain;Tumors;Pathology;Synthesizers;Gallium nitride;Image segmentation;Measurement","biomedical MRI;brain;image registration;image segmentation;learning (artificial intelligence);medical image processing;tumours","multimodal information;multimodal tumor brain images;recovered multimodal images;multimodal normal atlases;deep learning-based image synthesizers;MAR framework;pathological regions;normal appearances;monomodal atlases;routine image-based diagnosis;input multimodal pathological images;normal anatomical structures;MAR methods;conventional atlases;region-of-interest segmentation;input image;conventional monomodal normal atlases;new multiatlas registration framework","","","42","","","","","IEEE","IEEE Journals"
"MCFNet: Multi-Layer Concatenation Fusion Network for Medical Images Fusion","X. Liang; P. Hu; L. Zhang; J. Sun; G. Yin","College of Computer Science and Technology, Harbin Engineering University, Harbin, China; College of Computer Science and Technology, Harbin Engineering University, Harbin, China; College of Computer Science and Technology, Harbin Engineering University, Harbin, China; College of Computer Science and Technology, Harbin Engineering University, Harbin, China; College of Computer Science and Technology, Harbin Engineering University, Harbin, China","IEEE Sensors Journal","","2019","19","16","7107","7119","Medical image fusion technique can help the physician to execute combined diagnosis, preoperative planning, intraoperative guidance, and interventional treatment in many clinical applications by deriving the complementary information from medical images with different modalities. In this paper, we propose an end-to-end deep learning network, multi-layer concatenation fusion network (MCFNet), for feature extraction, feature fusion, and image reconstruction without manually designing complex feature extraction and fusion rules. MCFNet connects the feature map of each layer in the image reconstruction network with the source map, which has the same resolution due to down-sampling. And the spatial information lost in the fused medical image during two down-sampling processes is supplemented. The up-sampling operation at the image reconstruction stage uses fast up-convolution to reduce the checkerboard effect of traditional deconvolution. The proposed fusion method is tested on many clinical medical images and compared with several commonly used image fusion methods. Experimental results show that the proposed fusion method can provide better fusion results in terms of visual quality and objective evaluation.","","","10.1109/JSEN.2019.2913281","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Heilongjiang Postdoctoral Scientific Research Developmental Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698840","Medical image;images fusion;CNN;MCFNet","Image fusion;Feature extraction;Convolution;Transforms;Medical diagnostic imaging;Spatial resolution","deconvolution;feature extraction;image fusion;image reconstruction;learning (artificial intelligence);medical image processing;neural nets","medical images fusion;end-to-end deep learning network;multilayer concatenation fusion network;MCFNet;image reconstruction network;image reconstruction stage;clinical medical images;feature extraction;deconvolution;feature fusion","","","44","","","","","IEEE","IEEE Journals"
"Polar Transformation on Image Features for Orientation-Invariant Representations","J. Chen; Z. Luo; Z. Zhang; F. Huang; Z. Ye; T. Takiguchi; E. R. Hancock","Center for Computational Social Science, Kobe University, Kobe, Japan; Graduate School of System Informatics, Kobe University, Kobe, Japan; School of Software, Xiamen University, Xiamen, China; School of Mathematical Sciences, Xiamen University, Xiamen, China; College of Mathematics and Informatics, Fujian Normal University, Fuzhou 350108, China; Department of Computer Science, The University of York, York, U.K.; School of Mathematical Sciences, Xiamen University, Xiamen, China","IEEE Transactions on Multimedia","","2019","21","2","300","313","The choice of image feature representation plays a crucial role in the analysis of visual information. Although vast numbers of alternative robust feature representation models have been proposed to improve the performance of different visual tasks, most existing feature representations [e.g., handcrafted features or convolutional neural networks (CNNs)] have a relatively limited capacity to capture the highly orientation-invariant (rotation/reversal) features. The net consequence is suboptimal visual performance. To address these problems, this study adopts a novel transformational approach, which investigates the potential of using polar feature representations. Our low level consists of a histogram of oriented gradient, which is then binned using annular spatial bin-type cells applied to the polar gradient. This gives gradient binning invariance for feature extraction. In this way, the descriptors have significantly enhanced orientation-invariant capabilities. The proposed feature representation, called orientation-invariant histograms of oriented gradients, is capable of accurately processing visual tasks (e.g., facial expression recognition). In the context of the CNN architecture, we propose two polar convolution operations, referred to as full polar convolution and local polar convolution, and use these to develop polar architectures for the CNN orientation-invariant representation. Experimental results show that the proposed orientation-invariant image representation, based on polar models for both handcrafted features and deep learning features, is both competitive with state-of-the-art methods and maintains compact representation on a set of challenging benchmark image datasets.","","","10.1109/TMM.2018.2856121","PRESTO, JST; Japan Society for the Promotion of Science; National Natural Science Foundation of China; Natural Science Foundation of Fujian Province; Fundamental Research Funds for the Central Universities in China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411338","Rotation-invariant and reversal-invariant repr-esentation;HOG;CNN","Task analysis;Visualization;Feature extraction;Robustness;Convolution;Histograms;Image representation","convolutional neural nets;face recognition;feature extraction;image representation;learning (artificial intelligence);neural net architecture","visual tasks;feature representations;convolutional neural networks;robust feature representation models;visual information analysis;net consequence;orientation-invariant histograms of oriented gradients;CNN architecture;full polar convolution;visual information;image feature representation;polar transformation;benchmark image datasets;compact representation;deep learning features;polar models;orientation-invariant image representation;CNN orientation-invariant representation;polar architectures;local polar convolution;polar convolution operations;orientation-invariant capabilities;feature extraction;gradient binning invariance;polar gradient;annular spatial bin-type cells;polar feature representations;novel transformational approach;suboptimal visual performance;orientation-invariant features;handcrafted features","","1","73","","","","","IEEE","IEEE Journals"
"Automatic and optimal segmentation of the left ventricle in cardiac magnetic resonance images independent of the training sets","Z. Wang","College of Electrical and Electronic Engineering, Shandong University of Technology, People's Republic of China","IET Image Processing","","2019","13","10","1725","1735","In cardiac imaging, the boundary of the left ventricle (LV) could be used to measure the dyssynchrony of the heart. Hence, automatic and optimal segmentation of the LV is important. Although deep learning-based methods have achieved significant break-throughs in the accuracy of segmenting LV, it relies on a great number of training sets and the reproduction quality of the tested cases. Due to the variety of patients, it is difficult or impossible to collect the complete training sets that cover all patients with different genders, races, and ages. Therefore, methods independent of the training sets are more reliable and efficient for clinical applications. In this study, the authors propose a training sets-independent method to segment LV optimally and it outperforms all available state-of-the-art training-sets-independent image segmentation methods. In addition, they propose a framework to identify the boundary of the LV automatically. They tested these segmentation methods with both good quality and poor quality images in the proposed framework and verified that the proposed segmentation method yields the optimal solution compared to other state-of-the-art training-sets-independent segmentation methods. Based on their previous research work, the identified boundaries by the proposed approach are accurate enough for calculating the dyssynchrony of the LV.","","","10.1049/iet-ipr.2018.5878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8826726","","","biomedical MRI;learning (artificial intelligence);image segmentation;medical image processing;cardiology","deep learning-based methods;significant break-throughs;complete training sets;training sets-independent method;segment LV optimally;available state-of-the-art training-sets-independent image segmentation methods;segmentation method;poor quality images;optimal solution;state-of-the-art training-sets-independent segmentation methods;optimal segmentation;left ventricle;cardiac magnetic resonance images;cardiac imaging","","","26","","","","","IET","IET Journals"
"A Novel Scheme Based on the Diffusion to Edge Detection","Y. He; L. M. Ni","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Computer and Information Science, University of Macau, Macau, China","IEEE Transactions on Image Processing","","2019","28","4","1613","1624","A novel scheme of edge detection based on the physical law of diffusion is presented in this paper. Though the most current studies are using data based methods such as deep neural networks, these methods on machine learning need big data of labeled ground truth as well as a large amount of resources for training. On the other hand, the widely used traditional methods are based on the gradient of the grayscale or color of images with using different sorts of mathematical tools to accomplish the mission. Instead of treating the outline of an object in an image as a kind of gradient of grayscale or color, our scheme deals with the edge detection as a character of an energy diffusing in the space of media such as charge-coupled device. By using the characteristic function of diffusion, the information of the energy will be extracted. The scheme preserves the structural information of images very well. Because it comes from the inhere law of images' physical property, it has a unified mathematical framework for images' edge detection under different conditions, for example, multiscales, diferent light conditions, and so on. Moreover, it has low computational complexity.","","","10.1109/TIP.2018.2880568","Universidade de Macau; Macau FDCT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8531780","Image processing;diffusion;edge detection;Sobolev space;Bessel potential","Image edge detection;Mathematical model;Feature extraction;Media;Image color analysis;Charge coupled devices;Smoothing methods","computational complexity;edge detection;learning (artificial intelligence);neural nets","edge detection;physical law;deep neural networks;machine learning;big data;labeled ground truth;grayscale;image color;image edge detection","","2","47","","","","","IEEE","IEEE Journals"
"CNN-Based Polarimetric Decomposition Feature Selection for PolSAR Image Classification","C. Yang; B. Hou; B. Ren; Y. Hu; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","8796","8812","In order to better interpret polarimetric synthetic aperture radar (PolSAR) images, many scholars tend to do target decomposition for PolSAR images and utilize the obtained features to perform subsequent classification. These target decomposition features play an important role in terrain classification but completely utilizing them produces a high computational complexity. Furthermore, some features have a negative impact on the classification task. Therefore, selecting the appropriate amount of high-quality features is of great significance to the classification task. In this paper, we propose a convolutional neural network (CNN)-based feature selection algorithm for PolSAR image classification. First, we design a 1-D CNN for feature selection, then train the designed network with all the decomposition features to obtain a trained model. Second, the Kullback-Leibler distance (KLD) between different features is utilized as a standard to select feature subsets. Third, feature subsets with excellent performance form the final results. Due to the special structure of the 1-D CNN, repetitively training model is avoided when the input changes. Different from traditional feature selection methods, our method considers the performance of features combination rather than single feature contribution. To this end, the feature subsets selected by the proposed method are more useful to the classification task. Innovatively introducing KLD in the selection stage avoids random selection and improves the selection efficiency. Finally, we validate the performance of selected feature subsets in traditional and deep learning classification frameworks. Experiments demonstrate that features selected by the proposed method have a good performance comparing with others on three real PolSAR data sets.","","","10.1109/TGRS.2019.2922978","National Natural Science Foundation of China; National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8771134","Convolutional neural network (CNN);feature selection;image classification;Kullback–Leibler distance (KLD);polarimetric target decomposition","Feature extraction;Scattering;Matrix decomposition;Covariance matrices;Task analysis;Indexes;Data mining","computational complexity;convolutional neural nets;feature selection;geophysical image processing;image classification;learning (artificial intelligence);radar imaging;radar polarimetry;synthetic aperture radar","PolSAR image classification;polarimetric synthetic aperture radar images;subsequent classification;terrain classification;high computational complexity;convolutional neural network;1-D CNN;single feature contribution;random selection;deep learning classification;polarimetric decomposition feature selection;feature subset selection;Kullback-Leibler distance;KLD","","","48","","","","","IEEE","IEEE Journals"
"Improving Shadow Suppression for Illumination Robust Face Recognition","W. Zhang; X. Zhao; J. Morvan; L. Chen","Department of Mathematics and Computer Science, Ecole Centrale de Lyon, University of Lyon, Ecully, France; Xi'an Jiaotong University, Xi'an, China; Université Lyon 1, Villeurbanne-Cedex, France; Department of Mathematics and Computer Science, Ecole Centrale de Lyon, University of Lyon, Ecully, France","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","3","611","624","2D face analysis techniques, such as face landmarking, face recognition and face verification, are reasonably dependent on illumination conditions which are usually uncontrolled and unpredictable in the real world. The current massive data-driven approach, e.g., deep learning-based face recognition, requires a huge amount of labeled training face data that hardly cover the infinite lighting variations that can be encountered in real-life applications. An illumination robust preprocessing method thus remains a very interesting but also a significant challenge in reliable face analysis. In this paper we propose a novel model driven approach to improve lighting normalization of face images. Specifically, we propose to build the underlying reflectance model which characterizes interactions between skin surface, lighting source and camera sensor, and elaborate the formation of face color appearance. The proposed illumination processing pipeline enables generation of the Chromaticity Intrinsic Image (CII) in a log chromaticity space which is robust to illumination variations. Moreover, as an advantage over most prevailing methods, a photo-realistic color face image is subsequently reconstructed, which eliminates a wide variety of shadows whilst retaining the color information and identity details. Experimental results under different scenarios and using various face databases show the effectiveness of the proposed approach in dealing with lighting variations, including both soft and hard shadows, in face recognition.","","","10.1109/TPAMI.2018.2803179","French Research Agency, Agence Nationale de Recherche; Jemime Project; Biofence project; National Nature Science Foundation of China; Partner University Fund; The 4D Vision project; Beijing Advanced Innovation Center for Big Data and Brain Computing; Beihang University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8283763","Face recognition;lighting normalization;illumination and texture analysis","Lighting;Face;Image color analysis;Face recognition;Skin;Three-dimensional displays;Solid modeling","face recognition;image colour analysis;learning (artificial intelligence);lighting","face analysis;face recognition;shadow suppression;face databases;photo-realistic color face image;face color appearance;face images;lighting normalization;illumination robust preprocessing method;deep learning-based face recognition;face verification;face landmarking;2D face analysis techniques","","2","44","","","","","IEEE","IEEE Journals"
"Generative Adversarial Networks and Perceptual Losses for Video Super-Resolution","A. Lucas; S. López-Tapia; R. Molina; A. K. Katsaggelos","Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL, USA; Computer Science and Artificial Intelligence Department, Universidad de Granada, Granada, Spain; Computer Science and Artificial Intelligence Department, Universidad de Granada, Granada, Spain; Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL, USA","IEEE Transactions on Image Processing","","2019","28","7","3312","3327","Video super-resolution (VSR) has become one of the most critical problems in video processing. In the deep learning literature, recent works have shown the benefits of using adversarial-based and perceptual losses to improve the performance on various image restoration tasks; however, these have yet to be applied for video super-resolution. In this paper, we propose a generative adversarial network (GAN)-based formulation for VSR. We introduce a new generator network optimized for the VSR problem, named VSRResNet, along with new discriminator architecture to properly guide VSRResNet during the GAN training. We further enhance our VSR GAN formulation with two regularizers, a distance loss in feature-space and pixel-space, to obtain our final VSRResFeatGAN model. We show that pre-training our generator with the mean-squared-error loss only quantitatively surpasses the current state-of-the-art VSR models. Finally, we employ the PercepDist metric to compare the state-of-the-art VSR models. We show that this metric more accurately evaluates the perceptual quality of SR solutions obtained from neural networks, compared with the commonly used PSNR/SSIM metrics. Finally, we show that our proposed model, the VSRResFeatGAN model, outperforms the current state-of-the-art SR models, both quantitatively and qualitatively.","","","10.1109/TIP.2019.2895768","Sony 2016 Research Award Program Research Project; National Science Foundation; Ministerio de Economía y Competitividad; Visiting Scholar Program at the University of Granada; Spanish FPU Program; Ministerio de Economía y Competitividad; Visiting Scholar Program at the University of Granada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629024","Artificial neural networks;video signal processing;image resolution;image generation","Neural networks;Training;Spatial resolution;Generators;Gallium nitride;Task analysis","image resolution;image restoration;learning (artificial intelligence);mean square error methods;neural nets;video signal processing","generative adversarial networks;perceptual losses;video super-resolution;video processing;generative adversarial network-based formulation;VSR GAN formulation;distance loss;mean-squared-error loss;deep learning literature;image restoration;VSRResNet;feature-space;pixel-space;VSRResFeatGAN model;neural networks;PSNR-SSIM metrics","","10","30","","","","","IEEE","IEEE Journals"
"A Super Descriptor Tensor Decomposition for Dynamic Scene Recognition","M. R. Khokher; A. Bouzerdoum; S. L. Phung","School of Electrical, Computer, and Telecommunications Engineering, University of Wollongong, Wollongong, NSW, Australia; School of Electrical, Computer, and Telecommunications Engineering, University of Wollongong, Wollongong, NSW, Australia; School of Electrical, Computer, and Telecommunications Engineering, University of Wollongong, Wollongong, NSW, Australia","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","4","1063","1076","This paper presents a new approach for dynamic scene recognition based on a super descriptor tensor decomposition. Recently, local feature extraction based on dense trajectories has been used for modeling motion. However, dense trajectories usually include a large number of unnecessary trajectories, which increase noise, add complexity, and limit the recognition accuracy. Another problem is that the traditional bag-of-words techniques encode and concatenate the local features extracted from multiple descriptors to form a single large vector for classification. This concatenation not only destroys the spatio-temporal structure among the features but also yields high dimensionality. To address these problems, first, we propose to refine the dense trajectories by selecting only salient trajectories in a region of interest containing motion. Visual descriptors consisting of oriented gradient and motion boundary histograms are then computed along the refined dense trajectories. In case of camera motion, a short-window video stabilization is integrated to compensate for global motion. Second, the extracted features from multiple descriptors are encoded using a super descriptor tensor model. To this end, the TUCKER-3 tensor decomposition is employed to obtain a compact set of salient features, followed by feature selection via Fisher ranking. Experiments are conducted using two benchmark dynamic scene recognition datasets: Maryland “in-the-wild” and YUPPEN dynamic scenes. Experimental results show that the proposed approach outperforms several existing methods in terms of recognition accuracy and achieves a performance comparable with the state-of-the-art deep learning methods. The proposed approach achieves classification rates of 89.2% for Maryland and 98.1% for YUPPEN datasets.","","","10.1109/TCSVT.2018.2825784","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8335306","Dynamic scene recognition;tensor decomposition;super descriptor vector;refined dense trajectories","Dynamics;Trajectory;Feature extraction;Tensile stress;Cameras;Adaptive optics;Optical imaging","feature extraction;image classification;image motion analysis;learning (artificial intelligence);tensors;video signal processing","super descriptor tensor decomposition;modeling motion;recognition accuracy;bag-of-words techniques;multiple descriptors;salient trajectories;visual descriptors;motion boundary histograms;camera motion;super descriptor tensor model;TUCKER-3 tensor decomposition;salient features;feature selection;benchmark dynamic scene recognition datasets;YUPPEN dynamic scenes;feature extraction;dense trajectories;oriented gradient histograms;fisher ranking;deep learning;spatio-temporal structure","","1","44","","","","","IEEE","IEEE Journals"
"Contour Refinement and EG-GHT-Based Inshore Ship Detection in Optical Remote Sensing Image","H. Chen; T. Gao; W. Chen; Y. Zhang; J. Zhao","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","8458","8478","Inshore ship detection becomes challenging in high-resolution optical remote sensing image (RSI) because inshore ships are often incomplete and deformed due to the poor imaging condition and shadow of ship superstructure, and there are various interferences in harbor. A contour refinement and the improved generalized Hough transform (GHT)-based inshore ship detection scheme is proposed for RSI with complex harbor scenes. First, the suspected region of ships (SRS) is located in the entire RSI according to the line segments of ship body and docks. The contours in each SRS are then refined to repair the damaged ship head contour (SHC) using the convex set characteristics of ship head and subsequently reduce non-SHC by curvature filtering. In each refined SRS, equal frequency quantification instead of equal width quantification for R-Table construction and Gini coefficient-based decision criterion combining the number and distribution of votes are proposed to improve GHT (i.e., EG-GHT) and to extract SHCs as candidate targets. The false candidates are removed according to pixel proportion described by the structured binarization feature. Applying the border scoring strategy, the best candidates with the largest score among all the overlapped bounding boxes are selected as the final detection targets. Using the public RSIs with various cases, including turbid water, cloud occlusion, ships moored together, and ships with the different sizes, experimental results demonstrate the proposed scheme outperforms state-of-the-art contour-based methods and deep learning-based methods in terms of precision-recall rate and average precision, respectively.","","","10.1109/TGRS.2019.2921242","Defense Industrial Technology Development Program; National High-tech Research and Development Program; National Natural Science Foundation of China; National Science Foundation of Heilongjiang; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746827","Border scoring;curvature filtering;equal frequency quantification;generalized Hough transform (GHT);Gini coefficient;inshore ship detection;structured binarization feature (SBF)","Marine vehicles;Feature extraction;Head;Shape;Indexes;Training;Strain","geophysical techniques;Hough transforms;image classification;image filtering;image segmentation;learning (artificial intelligence);object detection;optical information processing;remote sensing;ships","complex harbor scenes;damaged ship head contour;equal frequency quantification;equal width quantification;deep learning;contour refinement;high-resolution optical remote sensing image;ship superstructure;generalized Hough transform;contour-based methods;EG-GHT-based inshore ship detection;convex set characteristics;curvature filtering;R-Table construction;Gini coefficient-based decision criterion;structured binarization feature;border scoring strategy;turbid water;cloud occlusion","","","37","","","","","IEEE","IEEE Journals"
"Estimating Missing Data in Temporal Data Streams Using Multi-Directional Recurrent Neural Networks","J. Yoon; W. R. Zame; M. van der Schaar","Department of Electrical and Computer Engineering, University of California, Los Angeles, CA, USA; Department of Economics and MathematicsUniversity of California; Department of Engineering ScienceUniversity of Oxford","IEEE Transactions on Biomedical Engineering","","2019","66","5","1477","1490","Missing data is a ubiquitous problem. It is especially challenging in medical settings because many streams of measurements are collected at different-and often irregular-times. Accurate estimation of the missing measurements is critical for many reasons, including diagnosis, prognosis, and treatment. Existing methods address this estimation problem by interpolating within data streams or imputing across data streams (both of which ignore important information) or ignoring the temporal aspect of the data and imposing strong assumptions about the nature of the data-generating process and/or the pattern of missing data (both of which are especially problematic for medical data). We propose a new approach, based on a novel deep learning architecture that we call a Multi-directional Recurrent Neural Network that interpolates within data streams and imputes across data streams. We demonstrate the power of our approach by applying it to five real-world medical datasets. We show that it provides dramatically improved estimation of missing measurements in comparison to 11 state-of-the-art benchmarks (including Spline and Cubic Interpolations, MICE, MissForest, matrix completion, and several RNN methods); typical improvements in Root Mean Squared Error are between 35%-50%. Additional experiments based on the same five datasets demonstrate that the improvements provided by our method are extremely robust.","","","10.1109/TBME.2018.2874712","Office of Naval Research; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8485748","Missing data;temporal data streams;imputation;recurrent neural nets","Time measurement;Interpolation;Estimation;Medical diagnostic imaging;Correlation;Recurrent neural networks;Biomedical measurement","data analysis;interpolation;learning (artificial intelligence);mean square error methods;medical information systems;recurrent neural nets","temporal data streams;missing measurements;data-generating process;medical data;missing data estimation;multidirectional recurrent neural networks;diagnosis;prognosis;treatment;deep learning architecture;interpolations;root mean squared error","","1","35","","","","","IEEE","IEEE Journals"
"Seismic Fault Detection Using Convolutional Neural Networks Trained on Synthetic Poststacked Amplitude Maps","A. Pochet; P. H. B. Diniz; H. Lopes; M. Gattass","Department of Informatics, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Informatics, Pontificia Universidade Catolica do Rio de Janeiro, Rio de Janeiro, Brazil; Department of Informatics, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Informatics, Pontificia Universidade Catolica do Rio de Janeiro, Rio de Janeiro, Brazil","IEEE Geoscience and Remote Sensing Letters","","2019","16","3","352","356","Fault detection is a crucial step in reservoir characterization. Despite the many tools developed in the past decades, automation of this task remains a challenge. We investigate the application of convolutional neural networks (CNNs) to seismic fault detection. CNN is a deep learning method growing in interest in the computer vision community, due to its high performances in a great variety of object detection tasks. One of the constraints of this method is the need to provide a massive number of interpreted data, a requirement particularly difficult to attend in the seismic area. To this end, we built a synthetic data set with simple fault geometries. The input of our network is the seismic amplitude only; the method does not require computing any seismic attribute. We apply a strategy of patch classification along the images, which requires a simple postprocess to extract the exact fault location. Our network shows good results on synthetic data and encouraging results when tested on regions of a real section of The Netherland offshore F3 block in the North Sea.","","","10.1109/LGRS.2018.2875836","Shell Brazil through the “Coupled Geomechanics” project at Tecgraf Institute (PUC-Rio); ANP “Compromisso com Investimentos em Pesquisa e Desenvolvimento” through the Research and Development Levy Regulation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8527536","Convolutional neural networks (CNNs);Hough transform;seismic fault","Training;Feature extraction;Fault detection;Fault location;Task analysis;Convolution;Convolutional neural networks","computer vision;faulting;geophysical techniques;image classification;learning (artificial intelligence);neural nets;object detection;seismology","seismic fault detection;synthetic poststacked amplitude;deep learning method;computer vision community;object detection tasks;seismic area;synthetic data;simple fault geometries;seismic amplitude;seismic attribute;exact fault location;convolutional neural networks;reservoir characterization;The Netherland offshore F3 block;North Sea","","1","31","","","","","IEEE","IEEE Journals"
"Small Object Sensitive Segmentation of Urban Street Scene With Spatial Adjacency Between Object Classes","D. Guo; L. Zhu; Y. Lu; H. Yu; S. Wang","Department of Computer Science and Engineering, University of South Carolina, Columbia, SC, USA; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Computer Science and Engineering, University of South Carolina, Columbia, SC, USA; Department of Computer Science, University of Texas–Rio Grande Valley, Edinburg, TX, USA; Department of Computer Science and Engineering, University of South Carolina, Columbia, SC, USA","IEEE Transactions on Image Processing","","2019","28","6","2643","2653","Recent advancements in deep learning have shown an exciting promise in the urban street scene segmentation. However, many objects, such as poles and sign symbols, are relatively small, and they usually cannot be accurately segmented, since the larger objects usually contribute more to the segmentation loss. In this paper, we propose a new boundary-based metric that measures the level of spatial adjacency between each pair of object classes and find that this metric is robust against object size-induced biases. We develop a new method to enforce this metric into the segmentation loss. We propose a network, which starts with a segmentation network, followed by a new encoder to compute the proposed boundary-based metric, and then trains this network in an end-to-end fashion. In deployment, we only use the trained segmentation network, without the encoder, to segment new unseen images. Experimentally, we evaluate the proposed method using CamVid and CityScapes data sets and achieve a favorable overall performance improvement and a substantial improvement in segmenting small objects.","","","10.1109/TIP.2018.2888701","National Science Foundation; NSFC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8581453","Small objects segmentation;spatial adjacency;semantic segmentation;urban street scene","Image segmentation;Measurement;Training;Object segmentation;Semantics;Task analysis;Computer science","image classification;image segmentation;learning (artificial intelligence);object detection;object recognition","segmentation loss;spatial adjacency;object classes;object size-induced biases;end-to-end fashion;trained segmentation network;object sensitive segmentation;deep learning;urban street scene segmentation;sign symbols;unseen image segmentation;boundary-based metric;CityScapes data sets;CamVid data sets","","2","46","","","","","IEEE","IEEE Journals"
"Joint Feature and Texture Coding: Toward Smart Video Representation via Front-End Intelligence","S. Ma; X. Zhang; S. Wang; X. Zhang; C. Jia; S. Wang","Institute of Digital Media, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; Institute of Digital Media, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Viterbi School of Engineering, University of Southern California, Los Angeles, CA, USA; Institute of Digital Media, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; Institute of Digital Media, School of Electronics Engineering and Computer Science, Peking University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","10","3095","3105","In this paper, we provide a systematical overview and analysis on the joint feature and texture representation framework, which aims to smartly and coherently represent the visual information with the front-end intelligence in the scenario of video big data applications. In particular, we first demonstrate the advantages of the joint compression framework in terms of both reconstruction quality and analysis accuracy. Subsequently, the interactions between visual feature and texture in the compression process are further illustrated. Finally, the future joint coding scheme by incorporating the deep learning features is envisioned, and future challenges toward seamless and unified joint compression are discussed. The joint compression framework, which bridges the gap between visual analysis and signal-level representation, is expected to contribute to a series of applications, such as video surveillance and autonomous driving.","","","10.1109/TCSVT.2018.2873102","National Natural Science Foundation of China; National Basic Research Program of China (973 Program); Peking University; City University of Hong Kong; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478338","Video compression;feature compression;front-end intelligence","Feature extraction;Visualization;Image coding;Task analysis;Face;Encoding;Streaming media","data compression;feature extraction;image coding;image representation;image texture;learning (artificial intelligence);neural nets;video signal processing","visual information;front-end intelligence;joint compression framework;reconstruction quality;visual feature;deep learning features;unified joint compression;visual analysis;signal-level representation;smart video representation;joint feature texture coding;joint feature texture representation framework","","1","77","","","","","IEEE","IEEE Journals"
"Authentically Distorted Image Quality Assessment by Learning From Empirical Score Distributions","Q. Jiang; Z. Peng; S. Yang; F. Shao","School of Information Science and Engineering, Ningbo University, Ningbo, China; School of Information Science and Engineering, Ningbo University, Ningbo, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Information Science and Engineering, Ningbo University, Ningbo, China","IEEE Signal Processing Letters","","2019","26","12","1867","1871","Most existing works on image quality assessment (IQA) focus on predicting a scalar quality score (SQS) based on the assumption that people can reach a consensus on the judgment of image quality. However, assigning a single scalar fails to reveal the subjective diversity that an image will probably receive divergent opinion scores from different subjects. This is particularly true for real-world authentically distorted images which usually involve composite mixtures of multiple distortions. To characterize such an property, this letter proposes to use a more informative vectorized label called empirical score distribution (ESD) to build an ESD-aided deep neural network (DNN) for authentically distorted image quality prediction. Our proposed network contains two streams: ESD prediction stream and SQS prediction stream. The whole DNN is optimized end-to-end with a combined loss so that both of the supervision information from ESD and SQS can be fully utilized in the training process. Experiments on two public authentically distorted image databases verify the superiority of our method.","","","10.1109/LSP.2019.2951533","National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; K.C. Wong Magna Fund in Ningbo University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8890868","Authentically distorted;deep neural network;empirical score distribution;image quality assessment","Electrostatic discharges;Streaming media;Feature extraction;Distortion;Image quality;Training;Neural networks","","","","","31","IEEE","","","","IEEE","IEEE Journals"
"A CNN-Based Framework for Comparison of Contactless to Contact-Based Fingerprints","C. Lin; A. Kumar","Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong","IEEE Transactions on Information Forensics and Security","","2019","14","3","662","676","Accurate comparison of contactless 2-D fingerprint images with contact-based fingerprints is critical for the success of emerging contactless 2-D fingerprint technologies, which offer more hygienic and deformation-free acquisition of fingerprint features. Convolutional neural networks (CNNs) have shown remarkable capabilities in biometrics recognition. However, there has been almost nil attempt to match fingerprint images using CNN-based approaches. This paper develops a CNN-based framework to accurately match contactless and contact-based fingerprint images. Our framework first trains a multi-Siamese CNN using fingerprint minutiae, respective ridge map and specific region of ridge map. This network is used to generate deep fingerprint representation using a distance-aware loss function. Deep fingerprint representations generated in such multi-Siamese network are concatenated for more accurate cross comparison. The proposed approach for cross-fingerprint comparison is evaluated on two publicly available databases containing contactless 2-D fingerprints and respective contact-based fingerprints. Our experiments presented in this paper consistently achieve outperforming results over several popular deep learning architectures and over contactless to contact-based fingerprints comparison methods in the literature.","","","10.1109/TIFS.2018.2854765","General Research Fund from Hong Kong Research Grant Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8409476","Contactless and contact-based fingerprint;sensor interoperability;multi-Siamese CNN","Sensors;Image sensors;Image recognition;Interoperability;Fingerprint recognition;Databases;Machine learning","image matching;image representation;learning (artificial intelligence);neural nets","CNN-based framework;hygienic deformation-free acquisition;convolutional neural networks;cross-fingerprint comparison;fingerprints comparison methods;contactless 2D fingerprint imaging technologies;contact-based fingerprint imaging technologies;multiSiamese CNN network;biometrics recognition;image matching;distance-aware loss function","","4","57","","","","","IEEE","IEEE Journals"
"Learning Multi-View Representation With LSTM for 3-D Shape Recognition and Retrieval","C. Ma; Y. Guo; J. Yang; W. An","College of Electronic Science, National University of Defense Technology, Changsha, China; College of Electronic Science, National University of Defense Technology, Changsha, China; College of Electronic Science, National University of Defense Technology, Changsha, China; College of Electronic Science, National University of Defense Technology, Changsha, China","IEEE Transactions on Multimedia","","2019","21","5","1169","1182","Shape representation for 3-D models is an important topic in computer vision, multimedia analysis, and computer graphics. Recent multiview-based methods demonstrate promising performance for 3-D shape recognition and retrieval. However, most multiview-based methods ignore the correlations of multiple views or suffer from high computional cost. In this paper, we propose a novel multiview-based network architecture for 3-D shape recognition and retrieval. Our network combines convolutional neural networks (CNNs) with long short-term memory (LSTM) to exploit the correlative information from multiple views. Well-pretrained CNNs with residual connections are first used to extract a low-level feature of each view image rendered from a 3-D shape. Then, a LSTM and a sequence voting layer are employed to aggregate these features into a shape descriptor. The highway network and a three-step training strategy are also adopted to boost the optimization of the deep network. Experimental results on two public datasets demonstrate that the proposed method achieves promising performance for 3-D shape recognition and the state-of-the-art performance for the 3-D shape retrieval.","","","10.1109/TMM.2018.2875512","National Natural Science Foundation of China; Hunan Provincial Natural Science Foundation; National Postdoctoral Program for Innovative Talents; China Postdoctoral Science Foundation; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8490588","3-D shape;multi-view;object recognition;object retrieval;CNN;LSTM","Shape;Three-dimensional displays;Feature extraction;Computational modeling;Solid modeling;Training;Network architecture","computer vision;convolutional neural nets;feature extraction;image classification;image representation;image retrieval;learning (artificial intelligence);object detection;object recognition;recurrent neural nets;rendering (computer graphics);shape recognition","novel multiview-based network architecture;LSTM;shape descriptor;multiview representation;shape representation;3D shape recognition;3D shape recognition;convolutional neural networks;long short-term memory;correlative information;highway network;sequence voting layer;three-step training strategy","","3","54","","","","","IEEE","IEEE Journals"
"Low-Light Image Enhancement via a Deep Hybrid Network","W. Ren; S. Liu; L. Ma; Q. Xu; X. Xu; X. Cao; J. Du; M. Yang","State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; NVIDIA Research, Santa Clara, CA, USA; Tencent AI Lab, Shenzhen, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SenseTime, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; School of Engineering, University of California at Merced, Merced, CA, USA","IEEE Transactions on Image Processing","","2019","28","9","4364","4375","Camera sensors often fail to capture clear images or videos in a poorly lit environment. In this paper, we propose a trainable hybrid network to enhance the visibility of such degraded images. The proposed network consists of two distinct streams to simultaneously learn the global content and the salient structures of the clear image in a unified network. More specifically, the content stream estimates the global content of the low-light input through an encoder-decoder network. However, the encoder in the content stream tends to lose some structure details. To remedy this, we propose a novel spatially variant recurrent neural network (RNN) as an edge stream to model edge details, with the guidance of another auto-encoder. The experimental results show that the proposed network favorably performs against the state-of-the-art low-light image enhancement algorithms.","","","10.1109/TIP.2019.2910412","National Natural Science Foundation of China; National Key R&D Program of China; Natural Science Foundation of Beijing Municipality; CCF-Tencent Open Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8692732","Low-light image enhancement;convolutional neural network;recurrent neural network","Streaming media;Image edge detection;Image enhancement;Lighting;Task analysis;Cameras;Image color analysis","image enhancement;recurrent neural nets","content stream;encoder-decoder network;edge stream;edge details;auto-encoder;deep hybrid network;camera sensors;clear images;videos;trainable hybrid network;degraded images;salient structures;low-light image enhancement algorithms;spatially variant recurrent neural network","","2","59","","","","","IEEE","IEEE Journals"
"Residual Dense Network for Pan-Sharpening Satellite Data","D. S. Vinothini; B. S. Bama","Department of Electronics and Communication Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Electronics and Communication Engineering, Thiagarajar College of Engineering, Madurai, India","IEEE Sensors Journal","","2019","19","24","12279","12285","Pan-sharpening is a multi-sensor fusion task that aims to enhance the spatial resolution of spectral data using panchromatic data of the same scene. This work proposes a deep Residual Dense Model (RDM) for Pan-Sharpening (PS) of satellite data which learns hierarchical features that can efficiently represent the local complex structures from panchromatic data. This work addresses the two general problems emphasized in pan-sharpening application viz., spectral and spatial preservation. The proposed Residual Dense Model for Pan-Sharpening network (RDMPSnet), preserves the spectral information by spectral mapping of Low-Resolution Multi-Spectral data (LRMS) while the spatial preservation is achieved by learning the hierarchical structural features from High-Resolution Panchromatic data (HRP). To extract this structural feature RDMPSnet is trained end to end with Low Resolution (LR) panchromatic patches and High Resolution (HR) residue patches to learn a non-linear mapping. The trained non-linear mapping network is capable to generate structural feature for any LRMS data which is injected into the mapped spectral data. The network is experimentally evaluated with Worldview2 and IKONOS2 satellite data and shows that the proposed RDMPS achieves favorable performance both visually and quantitatively against state-of-the-art methods.","","","10.1109/JSEN.2019.2939844","University Grants Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8826277","Satellite data;pan-sharpening;multi-spectral image;panchromatic image;deep learning","","","","","","20","IEEE","","","","IEEE","IEEE Journals"
"Real-Time Fine-Grained Air Quality Sensing Networks in Smart City: Design, Implementation, and Optimization","Z. Hu; Z. Bai; K. Bian; T. Wang; L. Song","National Engineering Laboratory for Big Data Analysis and Applications, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; National Engineering Laboratory for Big Data Analysis and Applications, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; National Engineering Laboratory for Big Data Analysis and Applications, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; National Engineering Laboratory for Big Data Analysis and Applications, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; State Key Laboratory of Advanced Optical Communication Systems and Networks, School of Electronics Engineering and Computer Science, Peking University, Beijing, China","IEEE Internet of Things Journal","","2019","6","5","7526","7542","Driven by the increasingly serious air pollution problem, the monitoring of air quality has gained much attention in both theoretical studies and practical implementations. In this paper, we present the architecture, implementation, and optimization of our own air quality sensing system, which provides real-time and fine-grained air quality map of the monitored area. As the major component, the optimization problem of our system is studied in detail. Our objective is to minimize the average joint error of the established real-time air quality map, which involves data inference for the unmeasured data values. A deep Q -learning solution has been proposed for the power control problem to reasonably plan the sensing tasks of the power-limited sensing devices online. A genetic algorithm has been designed for the location selection problem to efficiently find the suitable locations to deploy limited number of sensing devices. The performance of the proposed solutions are evaluated by simulations, showing a significant performance gain when adopting both strategies.","","","10.1109/JIOT.2019.2900751","National Natural Science Foundation of China; CERNET Innovation Project NGII; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648185","Air quality;genetic algorithm;power efficiency;reinforcement learning","Sensors;Optimization;Power control;Real-time systems;Air pollution;Servers","air pollution;air pollution measurement;air quality;environmental monitoring (geophysics);environmental science computing;genetic algorithms","data inference;power control problem;sensing tasks;sensing devices;location selection problem;smart city;air quality sensing system;fine-grained air quality map;air pollution problem;air quality map;deep Q -learning solution","","1","29","","","","","IEEE","IEEE Journals"
"Single Image Haze Removal via Region Detection Network","X. Yang; H. Li; Y. Fan; R. Chen","College of Information Science and Technology, Dalian Maritime University, Dalian, China; College of Information Science and Technology, Dalian Maritime University, Dalian, China; College of Information Science and Technology, Dalian Maritime University, Dalian, China; College of Information Science and Technology, Dalian Maritime University, Dalian, China","IEEE Transactions on Multimedia","","2019","21","10","2545","2560","Haze removal typically works on a physical model to estimate how light is transmitted and lost due to absorption and scattering through the atmosphere. In this paper, a region detection network is proposed to learn the relationship between the hazy image and the medium transmission map in a patchwise manner; the transmission map is then used to remove haze via an atmospheric scattering model and enhance the detail of de-hazed images. To this end, we design a simple yet powerful deep convolutional neural network, which mainly consists of two types of network units and can be trained in an end-to-end manner. One network unit is a module with the residual structure that facilitates the learning process of the deep network. The other is a novel module with a cascaded cross channel pool, which fuses multi-level haze-relevant features and boosts the abstraction ability of the model on a nonlinear manifold. Moreover, an evolutionary-based enhancement method is developed to improve the level of detail of over-smoothed results. Several comparative experiments have been conducted on synthetic and real images, through which we conclude that the proposed method achieves state-of-the-art haze removal results, qualitatively and quantitatively. Supplementary experiments further indicate that our method works better against other adverse effects on vision quality (e.g., the mist formed by heavy rain and the veil met underwater). Moreover, we present a lightweight version of the proposed network, which achieves an impressive haze removal performance even on low-power devices.","","","10.1109/TMM.2019.2908375","National Natural Science Foundation of China; Natural Science Foundation of Liaoning Province of China; Fundamental Research Funds for the Central Universities; Next-Generation Internet Innovation Project of CERNET; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676326","Haze removal;convolutional neural network;regional detection network;detail enhancement","Atmospheric modeling;Image color analysis;Image restoration;Scattering;Task analysis;Estimation","convolutional neural nets;feature extraction;image colour analysis;image denoising;image enhancement;image restoration;learning (artificial intelligence)","de-hazed images;network unit;cascaded cross channel pool;single image haze removal;region detection network;medium transmission map;atmospheric scattering model;deep convolutional neural network;multilevel haze-relevant features;evolutionary-based enhancement method","","","60","Traditional","","","","IEEE","IEEE Journals"
"Efficient Brain Tumor Segmentation With Multiscale Two-Pathway-Group Conventional Neural Networks","M. I. Razzak; M. Imran; G. Xu","Advanced Analytics Institute, University of Technology Sydney, Ultimo, Australia; King Saud University, Riyadh, Saudi Arabia; Advanced Analytics Institute, University of Technology Sydney, Ultimo, Australia","IEEE Journal of Biomedical and Health Informatics","","2019","23","5","1911","1919","Manual segmentation of the brain tumors for cancer diagnosis from MRI images is a difficult, tedious, and time-consuming task. The accuracy and the robustness of brain tumor segmentation, therefore, are crucial for the diagnosis, treatment planning, and treatment outcome evaluation. Mostly, the automatic brain tumor segmentation methods use hand designed features. Similarly, traditional methods of deep learning such as convolutional neural networks require a large amount of annotated data to learn from, which is often difficult to obtain in the medical domain. Here, we describe a new model two-pathway-group CNN architecture for brain tumor segmentation, which exploits local features and global contextual features simultaneously. This model enforces equivariance in the two-pathway CNN model to reduce instabilities and overfitting parameter sharing. Finally, we embed the cascade architecture into two-pathway-group CNN in which the output of a basic CNN is treated as an additional source and concatenated at the last layer. Validation of the model on BRATS2013 and BRATS2015 data sets revealed that embedding of a group CNN into a two pathway architecture improved the overall performance over the currently published state-of-the-art while computational complexity remains attractive.","","","10.1109/JBHI.2018.2874033","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8481481","Brain tumor;group CNN;CNN;deep neural network;group convolutional neural networks;cascade CNN;two-pathway CNN;2PG-CNN","Tumors;Image segmentation;Brain modeling;Machine learning;Biomedical imaging;Task analysis;Magnetic resonance imaging","biomedical MRI;brain;cancer;convolutional neural nets;feature extraction;image segmentation;medical image processing;tumours","cancer diagnosis;hand designed features;two-pathway-group CNN architecture;two-pathway CNN model;brain tumor segmentation;contextual features;two-pathway-group conventional neural networks;MRI images","","","45","Traditional","","","","IEEE","IEEE Journals"
"Emergency Drug Procurement Planning Based on Big-Data Driven Morbidity Prediction","Q. Song; Y. Zheng; Y. Huang; Z. Xu; W. Sheng; J. Yang","Scientific Research Institute, Hangzhou Normal University, Hangzhou, China; Hangzhou Institute of Service Engineering, Hangzhou Normal University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Hangzhou Institute of Service Engineering, Hangzhou Normal University, Hangzhou, China; Medical College, Hangzhou Normal University, Hangzhou, China","IEEE Transactions on Industrial Informatics","","2019","15","12","6379","6388","Due to the uncertainty of diseases, traditional approaches of drug procurement planning in hospitals often cause drug overstocking or understocking, which can have strong negative effects on healthcare services. This paper proposes a big-data driven approach, which uses a deep neural network to predict morbidities of acute gastrointestinal infections based on a huge amount of environmental data, and then constructs an optimization problem of drug procurement planning for maximizing the expected therapeutic effect on the predicted cases. The problem is solved by an efficient heuristic optimization algorithm. Computational experiments demonstrate the performance advantages of both the deep learning model and the heuristic algorithm over existing ones, and two real case studies in Central China show that the average prediction error of our approach is only 8% and the estimated recovery rate reaches 99%, much better than the currently used method. Our approach can also be extended for many other medical resource planning problems.","","","10.1109/TII.2018.2870879","National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466910","Big-data;drug procurement planning;deep learning;heuristic optimization;morbidity prediction;water wave optimization (WWO)","Drugs;Diseases;Procurement;Planning;Gastrointestinal tract;Predictive models;Optimization","","","","","48","OAPA","","","","IEEE","IEEE Journals"
"Adaptive Augmentation of Medical Data Using Independently Conditional Variational Auto-Encoders","M. Pesteie; P. Abolmaesumi; R. N. Rohling","Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, Canada","IEEE Transactions on Medical Imaging","","2019","38","12","2807","2820","Current deep supervised learning methods typically require large amounts of labeled data for training. Since there is a significant cost associated with clinical data acquisition and labeling, medical datasets used for training these models are relatively small in size. In this paper, we aim to alleviate this limitation by proposing a variational generative model along with an effective data augmentation approach that utilizes the generative model to synthesize data. In our approach, the model learns the probability distribution of image data conditioned on a latent variable and the corresponding labels. The trained model can then be used to synthesize new images for data augmentation. We demonstrate the effectiveness of the approach on two independent clinical datasets consisting of ultrasound images of the spine and magnetic resonance images of the brain. For the spine dataset, a baseline and a residual model achieve an accuracy of 85% and 92%, respectively, using our method compared to 78% and 83% using a conventional training approach for image classification task. For the brain dataset, a baseline and a U-net network achieve an accuracy of 84% and 88%, respectively, in Dice coefficient in tumor segmentation compared to 80% and 83% for the convention training approach.","","","10.1109/TMI.2019.2914656","Natural Sciences and Engineering Research Council of Canada; Canadian Institutes of Health Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8706960","Data augmentation;deep learning;conditional variational auto-encoder;magnetic resonance;ultrasound;center-line identification;tumor segmentation","Training;Data models;Biomedical imaging;Adaptation models;Image segmentation;Feature extraction;Computational modeling","","","","","80","IEEE","","","","IEEE","IEEE Journals"
"REMAP: Multi-Layer Entropy-Guided Pooling of Dense CNN Features for Image Retrieval","S. S. Husain; M. Bober","Centre for Vision Speech and Signal Processing, University of Surrey, Surrey, U.K.; Centre for Vision Speech and Signal Processing, University of Surrey, Surrey, U.K.","IEEE Transactions on Image Processing","","2019","28","10","5201","5213","This paper addresses the problem of very large-scale image retrieval, focusing on improving its accuracy and robustness. We target enhanced robustness of search to factors, such as variations in illumination, object appearance and scale, partial occlusions, and cluttered backgrounds-particularly important when a search is performed across very large datasets with significant variability. We propose a novel CNN-based global descriptor, called REMAP, which learns and aggregates a hierarchy of deep features from multiple CNN layers, and is trained end-to-end with a triplet loss. REMAP explicitly learns discriminative features which are mutually supportive and complementary at various semantic levels of visual abstraction. These dense local features are max-pooled spatially at each layer, within multi-scale overlapping regions, before aggregation into a single image-level descriptor. To identify the semantically useful regions and layers for retrieval, we propose to measure the information gain of each region and layer using KL-divergence. Our system effectively learns during training how useful various regions and layers are and weights them accordingly. We show that such relative entropy-guided aggregation outperforms classical CNN-based aggregation controlled by SGD. The entire framework is trained in an end-to-end fashion, outperforming the latest state-of-the-art results. On image retrieval datasets Holidays, Oxford, and MPEG, the REMAP descriptor achieves mAP of 95.5%, 91.5%, and 80.1%, respectively, outperforming any results published to date. REMAP also formed the core of the winning submission to the Google Landmark Retrieval Challenge on Kaggle.","","","10.1109/TIP.2019.2917234","Engineering and Physical Sciences Research Council; Defence Science and Technology Laboratory; Multidisciplinary University Research Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8720226","Global image descriptor;object recognition;instance retrieval;CNN;deep features;KL-divergence","Feature extraction;Computer architecture;Training;Entropy;Image retrieval;Visualization;Aggregates","convolutional neural nets;entropy;feature extraction;image retrieval","relative entropy-guided aggregation;image retrieval datasets;REMAP descriptor;Google Landmark Retrieval Challenge;multilayer entropy-guided pooling;dense CNN features;large-scale image retrieval;partial occlusions;novel CNN-based global descriptor;deep features;multiple CNN layers;triplet loss;discriminative features;semantic levels;visual abstraction;dense local features;multiscale overlapping regions;single image-level descriptor;KL-divergence","","","39","","","","","IEEE","IEEE Journals"
"Learning to Deblur Images with Exemplars","J. Pan; W. Ren; Z. Hu; M. Yang","Nanjing University of Science and Technology, Nanjing, Jiangsu, China; State Key Laboratory of Information Security, Chinese Academy of Sciences, Beijing, China; Hikvision Research America, Santa Clara, CA; University of California, Merced, CA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","6","1412","1425","Human faces are one interesting object class with numerous applications. While significant progress has been made in the generic deblurring problem, existing methods are less effective for blurry face images. The success of the state-of-the-art image deblurring algorithms stems mainly from implicit or explicit restoration of salient edges for kernel estimation. However, existing methods are less effective as only few edges can be restored from blurry face images for kernel estimation. In this paper, we address the problem of deblurring face images by exploiting facial structures. We propose a deblurring algorithm based on an exemplar dataset without using coarse-to-fine strategies or heuristic edge selections. In addition, we develop a convolutional neural network to restore sharp edges from blurry images for deblurring. Extensive experiments against the state-of-the-art methods demonstrate the effectiveness of the proposed algorithm for deblurring face images. In addition, we show that the proposed algorithms can be applied to image deblurring for other object classes.","","","10.1109/TPAMI.2018.2832125","National Science Foundation; National Key Research and Development Program; NSFC; National Ten Thousand Talent Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8353166","Image deblurring;face image;exemplar-based;edge prediction;deep edge","Image edge detection;Kernel;Image restoration;Estimation;Prediction algorithms;Convolutional neural networks;Visualization","convolutional neural nets;edge detection;image restoration","generic deblurring problem;blurry face images;salient edges;kernel estimation;deblurring face images;deblurring algorithm;heuristic edge selections;blurry images;image deblurring algorithms;coarse-to-fine strategies;convolutional neural network","","3","44","","","","","IEEE","IEEE Journals"
"Discerning Feature Supported Encoder for Image Representation","S. Wang; Z. Ding; Y. Fu","Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Department of Computer, Information and Technology, Indiana University–Purdue University Indianapolis, Indianapolis, IN, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA","IEEE Transactions on Image Processing","","2019","28","8","3728","3738","Inspired by the recent successes of deep architecture, the auto-encoder and its variants have been intensively explored on image clustering and classification tasks by learning effective feature representations. Conventional auto-encoder attempts to uncover the data's intrinsic structure, by constraining the output to be as much identical to the input as possible, which denotes that the hidden representation could faithfully reconstruct the input data. One issue that arises, however, is that such representations might not be optimized for specific tasks, e.g., image classification and clustering, since it compresses not only the discriminative information but also a lot of redundant or even noise within data. In other words, not all hidden units would benefit the specific tasks, while partial units are mainly used to represent the task-irrelevant patterns. In this paper, a general framework named discerning feature supported encoder (DFSE) is proposed, which integrates the auto-encoder and feature selection together into a unified model. Specifically, the feature selection is adapted to learned hidden-layer features to capture the task-relevant ones from the task-irrelevant ones. Meanwhile, the selected hidden units could in turn encode more discriminability only on the selected task-relevant units. To this end, our proposed algorithm can generate more effective image representation by distinguishing the task-relevant features from the task-irrelevant ones. Two scenarios of the experiments on image classification and clustering are conducted to evaluate our algorithm. The experiments on several benchmarks demonstrate that our method can achieve better performance over the state-of-the-art approaches in two scenarios.","","","10.1109/TIP.2019.2900646","National Science Foundation; Army Research Office; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648349","Image representation;feature extraction;learning systems","Feature extraction;Task analysis;Image reconstruction;Image coding;Image representation;Clustering algorithms;Dimensionality reduction","feature selection;image classification;image coding;image representation;learning (artificial intelligence);pattern clustering","feature supported encoder;image clustering;image classification;discriminative information;task-irrelevant patterns;feature selection;hidden-layer features;task-relevant features;feature representations;image representation","","","45","","","","","IEEE","IEEE Journals"
"Robust Pol-ISAR Target Recognition Based on ST-MC-DCNN","X. Bai; X. Zhou; F. Zhang; L. Wang; R. Xue; F. Zhou","National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology of Ministry of Education, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology of Ministry of Education, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","12","9912","9927","Although the deep convolutional neural network (DCNN) has been successfully applied to automatic target recognition (ATR) of ground vehicles based on synthetic aperture radar (SAR), most of the available techniques are not suitable for inverse synthetic aperture radar (ISAR) because they cannot tackle the inherent unknown deformation (e.g., translation, scaling, and rotation) among the training and test samples. To achieve robust polarimetric-ISAR (Pol-ISAR) ATR, this paper proposes the spatial transformer-multi-channel-deep convolutional neural network, i.e., ST-MC-DCNN. In this structure, we adopt the double-layer spatial transformer network (STN) module to adjust the image deformation of each polarimetric channel and then perform a robust hierarchical feature extraction by MC-DCNN. Finally, we carry out feature fusion in the concatenation layer and output the recognition result by the softmax classifier. The proposed network is end-to-end trainable and could learn the optimal deformation parameters automatically from training samples. For the fully Pol-ISAR image database generated from electromagnetic (EM) echoes of four satellites, the proposed structure achieves higher recognition accuracy than traditional DCNN and MC-DCNN. Additionally, it has shown robustness to image scaling, rotation, and combined deformation.","","","10.1109/TGRS.2019.2930112","National Natural Science Foundation of China; Foundation for the Author of National Excellent Doctoral Dissertation of the People's Republic of China; Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8804365","Automatic target recognition (ATR);deep convolutional neural network (DCNN);image deformation;inverse synthetic aperture radar (ISAR)","Feature extraction;Scattering;Strain;Target recognition;Shape;Azimuth;Image recognition","convolutional neural nets;feature extraction;image classification;image fusion;image recognition;radar imaging;radar polarimetry;radar target recognition;synthetic aperture radar;telecommunication computing","robust Pol-ISAR target recognition;ST-MC-DCNN;automatic target recognition;inverse synthetic aperture radar;inherent unknown deformation;polarimetric-ISAR ATR;double-layer spatial transformer network module;image deformation;polarimetric channel;robust hierarchical feature extraction;concatenation layer;recognition result;optimal deformation parameters;training samples;Pol-ISAR image database;image scaling;electromagnetic echoes;feature fusion;spatial transformer-multichannel-deep convolutional neural network","","","62","IEEE","","","","IEEE","IEEE Journals"
"Capsulenet-Based Spatial–Spectral Classifier for Hyperspectral Images","P. V. Arun; K. M. Buddhiraju; A. Porwal","Indian Institute of Technology, Bombay, Mumbai, India; Indian Institute of Technology, Bombay, Mumbai, India; Indian Institute of Technology, Bombay, Mumbai, India","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","6","1849","1865","In this paper, a Capsulenet-based framework is proposed for extracting spectral and spatial features for improving hyperspectral image classification. Unlike conventional strategies, the proposed framework simultaneously optimizes both feature extraction and classification. The spectral features/patterns derived at different levels of hierarchies are remodeled as spectral-feature capsules. Consequently, unlike conventional convolutional neural network-based approaches, the relative locations as well as other properties such as depth, width, and position of the spectral patterns are taken into consideration. In addition to learning spectral features/patterns, a convolutional long short-term memory (conv-LSTM) is employed for sequentially integrating the spatial features learned from each band. The integrated spatial-feature representation, thus obtained from the final hidden state of conv-LSTM, forms spatial-feature capsules. The capsule-level integration of spatial and spectral features/patterns yields better convergence and accuracy as compared to both ensemble-based and kernel-level integrations. Along with the margin loss, a spectral-angle-based reconstruction loss is also minimized to regularize the learning of network weights. Experiments over different standard datasets indicate that the proposed approach performs better than other prominent hyperspectral classifiers. Furthermore, in comparison with the recent deep learning models, our approach is found to be less sensitive to the network parameters and achieves better accuracy even with lesser network depth.","","","10.1109/JSTARS.2019.2913097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715382","Capsulenet;classification;convolutional neural network (CNN);hyperspectral","Hyperspectral imaging;Feature extraction;Kernel;Convolution;Neurons;Earth","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image recognition;image representation;learning (artificial intelligence)","Capsulenet-based spatial-spectral classifier;hyperspectral images;Capsulenet-based framework;spatial features;hyperspectral image classification;feature extraction;spectral-feature capsules;conventional convolutional neural network-based approaches;spectral patterns;learning spectral features/patterns;conv-LSTM;integrated spatial-feature representation;forms spatial-feature capsules;capsule-level integration;kernel-level integrations;spectral-angle-based reconstruction loss;prominent hyperspectral classifiers","","","68","Traditional","","","","IEEE","IEEE Journals"
"MAVNet: An Effective Semantic Segmentation Micro-Network for MAV-Based Tasks","T. Nguyen; S. S. Shivakumar; I. D. Miller; J. Keller; E. S. Lee; A. Zhou; T. Özaslan; G. Loianno; J. H. Harwood; J. Wozencraft; C. J. Taylor; V. Kumar","GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; Tandon School of Engineering, New York University, Brooklyn, NY, USA; United States Army Corps of Engineers, Washington, DC, USA; United States Army Corps of Engineers, Washington, DC, USA; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA","IEEE Robotics and Automation Letters","","2019","4","4","3908","3915","Real-time semantic image segmentation on platforms subject to size, weight, and power constraints is a key area of interest for air surveillance and inspection. In this letter, we propose MAVNet: a small, light-weight, deep neural network for real-time semantic segmentation on micro aerial vehicles (MAVs). MAVNet, inspired by ERFNet [E. Romera, J. M. lvarez, L. M. Bergasa, and R. Arroyo, “ErfNet: Efficient residual factorized convnet for real-time semantic segmentation,” IEEE Trans. Intell. Transp. Syst., vol. 19, no. 1, pp. 263–272, Jan. 2018.], features 400 times fewer parameters and achieves comparable performance with some reference models in empirical experiments. Additionally, we provide two novel datasets that represent challenges in semantic segmentation for real-time MAV tracking and infrastructure inspection tasks and verify MAVNet on these datasets. Our algorithm and datasets are made publicly available.","","","10.1109/LRA.2019.2928734","MAST Collaborative Technology Alliance - Contract; ARL; ONR; ARO; NSF; DARPA; Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8764006","Object detection;segmentation and categorization;semantic scene understanding;aerial systems: perception and autonomy;recognition;semantic segmentation","Image segmentation;Semantics;Real-time systems;Target tracking;Autonomous aerial vehicles;Object detection;Deep learning","","","","","36","Traditional","","","","IEEE","IEEE Journals"
"Continuous Gesture Segmentation and Recognition Using 3DCNN and Convolutional LSTM","G. Zhu; L. Zhang; P. Shen; J. Song; S. A. A. Shah; M. Bennamoun","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; University of Western Australia, Perth, WA, Australia; University of Western Australia, Perth, WA, Australia","IEEE Transactions on Multimedia","","2019","21","4","1011","1021","Continuous gesture recognition aims at recognizing the ongoing gestures from continuous gesture sequences and is more meaningful for the scenarios, where the start and end frames of each gesture instance are generally unknown in practical applications. This paper presents an effective deep architecture for continuous gesture recognition. First, continuous gesture sequences are segmented into isolated gesture instances using the proposed temporal dilated Res3D network. A balanced squared hinge loss function is proposed to deal with the imbalance between boundaries and nonboundaries. Temporal dilation can preserve the temporal information for the dense detection of the boundaries at fine granularity, and the large temporal receptive field makes the segmentation results more reasonable and effective. Then, the recognition network is constructed based on the 3-D convolutional neural network (3DCNN), the convolutional long-short-term-memory network (ConvLSTM), and the 2-D convolutional neural network (2DCNN) for isolated gesture recognition. The “3DCNN-ConvLSTM-2DCNN” architecture is more effective to learn long-term and deep spatiotemporal features. The proposed segmentation and recognition networks obtain the Jaccard index of 0.7163 on the Chalearn LAP ConGD dataset, which is 0.106 higher than the winner of 2017 ChaLearn LAP Large-Scale Continuous Gesture Recognition Challenge.","","","10.1109/TMM.2018.2869278","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Key Research and Development Program of Shaanxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8458185","Continuous gesture recognition;3DCNN;convolutional LSTM;dilation","Gesture recognition;Spatiotemporal phenomena;Videos;Motion segmentation;Proposals;Three-dimensional displays;Fasteners","feature extraction;gesture recognition;image segmentation;image sequences;learning (artificial intelligence);neural nets;spatiotemporal phenomena","temporal receptive field;recognition network;3-D convolutional neural network;long-short-term-memory network;2-D convolutional neural network;isolated gesture recognition;gesture instance;effective deep architecture;isolated gesture instances;temporal dilated Res3D network;balanced squared hinge loss function;temporal dilation;gesture sequences;continuous gesture segmentation;ChaLearn LAP Large-Scale Continuous Gesture Recognition Challenge;3DCNN-ConvLSTM-2DCNN architecture","","1","60","","","","","IEEE","IEEE Journals"
"Fully Convolutional Networks for Monocular Retinal Depth Estimation and Optic Disc-Cup Segmentation","S. M. Shankaranarayana; K. Ram; K. Mitra; M. Sivaprakasam","Electrical Engineering, Indian Institute of Technology, Madras, Chennai, India; Centre for Visual Information Technology, International Institute of Information Technology, Hyderabad, India; Electrical Engineering, Indian Institute of Technology, Madras, Chennai, India; Electrical Engineering, Indian Institute of Technology, Madras, Chennai, India","IEEE Journal of Biomedical and Health Informatics","","2019","23","4","1417","1426","Glaucoma is a serious ocular disorder for which the screening and diagnosis are carried out by the examination of the optic nerve head (ONH). The color fundus image (CFI) is the most common modality used for ocular screening. In CFI, the central region which is the optic disc and the optic cup region within the disc are examined to determine one of the important cues for glaucoma diagnosis called the optic cup-to-disc ratio (CDR). CDR calculation requires accurate segmentation of optic disc and cup. Another important cue for glaucoma progression is the variation of depth in ONH region. In this paper, we first propose a deep learning framework to estimate depth from a single fundus image. For the case of monocular retinal depth estimation, we are also plagued by the labeled data insufficiency. To overcome this problem we adopt the technique of pretraining the deep network where, instead of using a denoising autoencoder, we propose a new pretraining scheme called pseudo-depth reconstruction, which serves as a proxy task for retinal depth estimation. Empirically, we show pseudo-depth reconstruction to be a better proxy task than denoising. Our results outperform the existing techniques for depth estimation on the INSPIRE dataset. To extend the use of depth map for optic disc and cup segmentation, we propose a novel fully convolutional guided network, where, along with the color fundus image the network uses the depth map as a guide. We propose a convolutional block called multimodal feature extraction block to extract and fuse the features of the color image and the guide image. We extensively evaluate the proposed segmentation scheme on three datasetsORIGA, RIMONEr3, and DRISHTI-GS. The performance of the method is comparable and in many cases, outperforms the most recent state of the art.","","","10.1109/JBHI.2019.2899403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642288","Glaucoma;Fully Convolutional Networks;Semantic Segmentation;Depth Estimation","Optical imaging;Estimation;Image segmentation;Retina;Biomedical optical imaging;Feature extraction;Image color analysis","biomedical optical imaging;diseases;eye;feature extraction;image segmentation;learning (artificial intelligence);medical image processing","color image;depth map;proxy task;pseudodepth reconstruction;deep network;single fundus image;ONH region;glaucoma progression;optic cup-to-disc ratio;glaucoma diagnosis;optic cup region;central region;ocular screening;CFI;color fundus image;optic nerve head;serious ocular disorder;optic disc-cup segmentation;monocular retinal depth estimation;fully convolutional networks","","2","34","","","","","IEEE","IEEE Journals"
"Salient Object Detection with Recurrent Fully Convolutional Networks","L. Wang; L. Wang; H. Lu; P. Zhang; X. Ruan","School of Information and Communication Engineering, Dalian University of Technology, Dalian, Ganjingzi, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, Ganjingzi, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, Ganjingzi, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, Ganjingzi, China; Tiwaki Co., Ltd., Kusatsu, Shiga, Japan","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","7","1734","1746","Deep networks have been proved to encode high-level features with semantic meaning and delivered superior performance in salient object detection. In this paper, we take one step further by developing a new saliency detection method based on recurrent fully convolutional networks (RFCNs). Compared with existing deep network based methods, the proposed network is able to incorporate saliency prior knowledge for more accurate inference. In addition, the recurrent architecture enables our method to automatically learn to refine the saliency map by iteratively correcting its previous errors, yielding more reliable final predictions. To train such a network with numerous parameters, we propose a pre-training strategy using semantic segmentation data, which simultaneously leverages the strong supervision of segmentation tasks for effective training and enables the network to capture generic representations to characterize category-agnostic objects for saliency detection. Extensive experimental evaluations demonstrate that the proposed method compares favorably against state-of-the-art saliency detection approaches. Additional validations are also performed to study the impact of the recurrent architecture and pre-training strategy on both saliency detection and semantic segmentation, which provides important knowledge for network design and training in the future research.","","","10.1109/TPAMI.2018.2846598","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8382302","Salient object detection;recurrent fully convolutional networks;saliency priors;network pre-training","Saliency detection;Semantics;Task analysis;Training;Computer architecture;Object detection;Image segmentation","convolutional neural nets;feature extraction;image segmentation;learning (artificial intelligence);neural net architecture;object detection;recurrent neural nets","pre-training strategy;network design;salient object detection;recurrent fully convolutional networks;deep network;high-level features;semantic meaning;saliency detection method;saliency prior knowledge;recurrent architecture;saliency map;semantic segmentation data;category-agnostic objects;saliency detection approaches","","3","57","","","","","IEEE","IEEE Journals"
"Weakly Supervised Estimation of Shadow Confidence Maps in Fetal Ultrasound Imaging","Q. Meng; M. Sinclair; V. Zimmer; B. Hou; M. Rajchl; N. Toussaint; O. Oktay; J. Schlemper; A. Gomez; J. Housden; J. Matthew; D. Rueckert; J. A. Schnabel; B. Kainz","Department of Computing, Biomedical Image AnalysisGroup, Imperial College London, London, U.K.; Department of Computing, Biomedical Image AnalysisGroup, Imperial College London, London, U.K.; School of Biomedical Engineering and Imaging Sciences, King’s College London, London, U.K.; Department of Computing, Biomedical Image AnalysisGroup, Imperial College London, London, U.K.; Department of Computing, Biomedical Image AnalysisGroup, Imperial College London, London, U.K.; School of Biomedical Engineering and Imaging Sciences, King’s College London, London, U.K.; Department of Computing, Biomedical Image AnalysisGroup, Imperial College London, London, U.K.; Department of Computing, Biomedical Image AnalysisGroup, Imperial College London, London, U.K.; School of Biomedical Engineering and Imaging Sciences, King’s College London, London, U.K.; School of Biomedical Engineering and Imaging Sciences, King’s College London, London, U.K.; School of Biomedical Engineering and Imaging Sciences, King’s College London, London, U.K.; Department of Computing, Biomedical Image AnalysisGroup, Imperial College London, London, U.K.; School of Biomedical Engineering and Imaging Sciences, King’s College London, London, U.K.; Department of Computing, Biomedical Image AnalysisGroup, Imperial College London, London, U.K.","IEEE Transactions on Medical Imaging","","2019","38","12","2755","2767","Detecting acoustic shadows in ultrasound images is important in many clinical and engineering applications. Real-time feedback of acoustic shadows can guide sonographers to a standardized diagnostic viewing plane with minimal artifacts and can provide additional information for other automatic image analysis algorithms. However, automatically detecting shadow regions using learning-based algorithms is challenging because pixel-wise ground truth annotation of acoustic shadows is subjective and time consuming. In this paper, we propose a weakly supervised method for automatic confidence estimation of acoustic shadow regions. Our method is able to generate a dense shadow-focused confidence map. In our method, a shadow-seg module is built to learn general shadow features for shadow segmentation, based on global image-level annotations as well as a small number of coarse pixel-wise shadow annotations. A transfer function is introduced to extend the obtained binary shadow segmentation to a reference confidence map. In addition, a confidence estimation network is proposed to learn the mapping between input images and the reference confidence maps. This network is able to predict shadow confidence maps directly from input images during inference. We use evaluation metrics such as DICE, inter-class correlation, and so on, to verify the effectiveness of our method. Our method is more consistent than human annotation and outperforms the state-of-the-art quantitatively in shadow segmentation and qualitatively in confidence estimation of shadow regions. Furthermore, we demonstrate the applicability of our method by integrating shadow confidence maps into tasks such as ultrasound image classification, multi-view image fusion, and automated biometric measurements.","","","10.1109/TMI.2019.2913311","Wellcome Trust; Engineering and Physical Sciences Research Council; FP7 Ideas: European Research Council; Wellcome/EPSRC Center for Medical Engineering; National Institute for Health Research (NIHR) Biomedical Research Center-based at Guy’s and St. Thomas’ NHS Foundation Trust, King’s College London and the NIHR Clinical Research Facility (CRF) at Guy’s and St Thomas’; CSC-Imperial Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698843","Ultrasound imaging;deep learning;weakly supervised;shadow detection;confidence estimation","Acoustics;Image segmentation;Estimation;Image analysis;Task analysis;Biomedical imaging","","","","","39","IEEE","","","","IEEE","IEEE Journals"
"Three-Stream Attention-Aware Network for RGB-D Salient Object Detection","H. Chen; Y. Li","Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Department of Mechanical Engineering, City University of Hong Kong, Hong Kong","IEEE Transactions on Image Processing","","2019","28","6","2825","2835","Previous RGB-D fusion systems based on convolutional neural networks typically employ a two-stream architecture, in which RGB and depth inputs are learned independently. The multi-modal fusion stage is typically performed by concatenating the deep features from each stream in the inference process. The traditional two-stream architecture might experience insufficient multi-modal fusion due to two following limitations: (1) the cross-modal complementarity is rarely studied in the bottom-up path, wherein we believe the cross-modal complements can be combined to learn new discriminative features to enlarge the RGB-D representation community and (2) the cross-modal channels are typically combined by undifferentiated concatenation, which appears ambiguous to selecting cross-modal complementary features. In this paper, we address these two limitations by proposing a novel three-stream attention-aware multi-modal fusion network. In the proposed architecture, a cross-modal distillation stream, accompanying the RGB-specific and depth-specific streams, is introduced to extract new RGB-D features in each level in the bottom-up path. Furthermore, the channel-wise attention mechanism is innovatively introduced to the cross-modal cross-level fusion problem to adaptively select complementary feature maps from each modality in each level. Extensive experiments report the effectiveness of the proposed architecture and the significant improvement over the state-of-the-art RGB-D salient object detection methods.","","","10.1109/TIP.2019.2891104","Research Grants Council, University Grants Committee; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603756","Three-stream;RGB-D;saliency detection;cross-modal cross-level attention","Feature extraction;Object detection;Saliency detection;Computer architecture;Cognition;Object recognition;Convolutional neural networks","convolutional neural nets;feature extraction;feature selection;image colour analysis;image fusion;learning (artificial intelligence);object detection","stream attention-aware network;RGB-D salient object detection;convolutional neural networks;two-stream architecture;multimodal fusion stage;deep features;insufficient multimodal fusion;cross-modal complementarity;discriminative features;RGB-D representation community;cross-modal channels;three-stream attention-aware multimodal fusion network;cross-modal distillation stream;depth-specific streams;RGB-D features;channel-wise attention mechanism;cross-modal cross-level fusion problem;complementary feature maps;cross-modal complementary feature selection;RGB-D fusion systems;inference process;undifferentiated concatenation","","1","51","","","","","IEEE","IEEE Journals"
"Semi-Supervised EEG Signals Classification System for Epileptic Seizure Detection","A. M. Abdelhameed; M. Bayoumi","Center for Advanced Computer Studies, University of Louisiana at Lafayette, Lafayette, LA, USA; Department of Electrical and Computer Engineering, University of Louisiana at Lafayette, Lafayette, LA, USA","IEEE Signal Processing Letters","","2019","26","12","1922","1926","In the past few decades, measuring and recording the brain electrical activities using Electroencephalogram (EEG) has become a standout amongst the tools utilized for neurological disorders’ diagnosis, especially seizure detection. In this letter, a novel epileptic seizure detection system based on classifying raw EEG signals’ recordings, eliminating the overhead of engineered feature extraction, is proposed. The system employs a mixing of unsupervised and supervised deep learning utilizing a one-dimensional convolutional variational autoencoder. To ascertain the robustness of the system against classifying unseen data, the evaluation of the proposed system is done using k-fold cross-validation. The classification results between normal and ictal cases have achieved a 100% accuracy while the classification results between the normal, inter-ictal and ictal cases accomplished a 99% overall accuracy which makes our system one of the most efficient among other state-of-the-art systems.","","","10.1109/LSP.2019.2953870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8903437","Classification;cross-validation;deep learning;epileptic seizure detection;feature extraction;variational autoencoder","","","","","","24","IEEE","","","","IEEE","IEEE Journals"
"Efficient Image Recognition and Retrieval on IoT-Assisted Energy-Constrained Platforms From Big Data Repositories","I. Mehmood; A. Ullah; K. Muhammad; D. Deng; W. Meng; F. Al-Turjman; M. Sajjad; V. H. C. de Albuquerque","Department of Software, School of Electronics and Information Engineering, Sejong University, Seoul, South Korea; Intelligent Media Laboratory, Digital Contents Research Institue, Sejong University, Seoul, South Korea; Department of Software, School of Electronics and Information Engineering, Sejong University, Seoul, South Korea; Department of Computer Science and Information Engineering, National Changhua University of Education, Changhua, Taiwan; Department of Applied Mathematics and Computer Science, Technical University of Denmark, Kongens Lyngby, Denmark; College of Engineering, Antalya Bilim University, Antalya, Turkey; Department of Computer Science, Digital Image Processing Laboratory, Islamia College, Peshawar, Peshawar, Pakistan; Graduate Program in Applied Informatics, Universidade de Fortaleza, Fortaleza, Brazil","IEEE Internet of Things Journal","","2019","6","6","9246","9255","The advanced computational capabilities of many resource constrained devices, such as smartphones have enabled various research areas including image retrieval from big data repositories for numerous Internet of Things (IoT) applications. The major challenges for image retrieval using smartphones in an IoT environment are the computational complexity and storage. To deal with big data in IoT environment for image retrieval, this paper proposes a light-weighted deep learning-based system for energy-constrained devices. The system first detects and crops face regions from an image using Viola–Jones algorithm with additional face and nonface classifier to eliminate the miss-detection problem. Second, the system uses convolutional layers of a cost effective pretrained CNN model with defined features to represent faces. Next, features of the big data repository are indexed to achieve a faster matching process for real-time retrieval. Finally, Euclidean distance is used to find similarity between query and repository images. For experimental evaluation, we created a local facial images dataset, including both single and group facial images. This dataset can be used by other researchers as a benchmark for comparison with other real-time facial image retrieval systems. The experimental results show that our proposed system outperforms other state-of-the-art feature extraction methods in terms of efficiency and retrieval for IoT-assisted energy-constrained platforms.","","","10.1109/JIOT.2019.2896151","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629991","Big data;convolutional neural network;deep learning;energy-constrained platforms;image retrieval;Internet of Things (IoT)","Feature extraction;Face;Internet of Things;Smart phones;Image retrieval;Big Data;Classification algorithms","","","","","49","IEEE","","","","IEEE","IEEE Journals"
"Automated Sleep Apnea Detection in Raw Respiratory Signals Using Long Short-Term Memory Neural Networks","T. Van Steenkiste; W. Groenendaal; D. Deschrijver; T. Dhaene","imec, IDLab, Ghent University - imec, Gent, Belgium; Holst Center/imec the Netherlands, Eindhoven, AE, The Netherlands; imec, IDLab, Ghent University - imec, Gent, Belgium; imec, IDLab, Ghent University - imec, Gent, Belgium","IEEE Journal of Biomedical and Health Informatics","","2019","23","6","2354","2364","Sleep apnea is one of the most common sleep disorders and the consequences of undiagnosed sleep apnea can be very severe, ranging from increased blood pressure to heart failure. However, many people are often unaware of their condition. The gold standard for diagnosing sleep apnea is an overnight polysomnography in a dedicated sleep laboratory. Yet, these tests are expensive and beds are limited as trained staff needs to analyze the entire recording. An automated detection method would allow a faster diagnosis and more patients to be analyzed. Most algorithms for automated sleep apnea detection use a set of human-engineered features, potentially missing important sleep apnea markers. In this paper, we present an algorithm based on state-of-the-art deep learning models for automatically extracting features and detecting sleep apnea events in respiratory signals. The algorithm is evaluated on the Sleep-Heart-Health-Study-1 dataset and provides per-epoch sensitivity and specificity scores comparable to the state of the art. Furthermore, when these predictions are mapped to the apnea–hypopnea index, a considerable improvement in per-patient scoring is achieved over conventional methods. This paper presents a powerful aid for trained staff to quickly diagnose sleep apnea.","","","10.1109/JBHI.2018.2886064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8571271","Sleep apnea;LSTM;deep learning, SHHS-1","Sleep apnea;Feature extraction;Physiology;Hidden Markov models;Informatics;Electrocardiography;Neural networks","","","","2","56","Traditional","","","","IEEE","IEEE Journals"
"Domain Progressive 3D Residual Convolution Network to Improve Low-Dose CT Imaging","X. Yin; Q. Zhao; J. Liu; W. Yang; J. Yang; G. Quan; Y. Chen; H. Shu; L. Luo; J. Coatrieux","Laboratory of Image Science and Technology, Southeast University, Nanjing, China; Laboratory of Image Science and Technology, Southeast University, Nanjing, China; College of Computer and Information, Anhui Polytechnic University, Wuhu, China; School of Biomedical Engineering, Southern Medical University, Guangzhou, China; Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Electronics, Beijing Institute of Technology, Beijing, China; CT RPA Department, United Imaging Healthcare Co., Ltd., Shanghai, China; Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, Nanjing, China; Laboratory of Image Science and Technology, Southeast University, Nanjing, China; Laboratory of Image Science and Technology, Southeast University, Nanjing, China; Centre de Recherche en Information Biomédicale Sino-Français, Rennes, France","IEEE Transactions on Medical Imaging","","2019","38","12","2903","2913","The wide applications of X-ray computed tomography (CT) bring low-dose CT (LDCT) into a clinical prerequisite, but reducing the radiation exposure in CT often leads to significantly increased noise and artifacts, which might lower the judgment accuracy of radiologists. In this paper, we put forward a domain progressive 3D residual convolution network (DP-ResNet) for the LDCT imaging procedure that contains three stages: sinogram domain network (SD-net), filtered back projection (FBP), and image domain network (ID-net). Though both are based on the residual network structure, the SD-net and ID-net provide complementary effect on improving the final LDCT quality. The experimental results with both simulated and real projection data show that this domain progressive deep-learning network achieves significantly improved performance by combing the network processing in the two domains.","","","10.1109/TMI.2019.2917258","State’s Key Project of Research and Development Plan; National Natural Science Foundation of China; Guangdong Science and Technology Department; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718010","Low dose computed tomography (LDCT);deep learning;residual network;artifacts reduction","Computed tomography;Image reconstruction;X-ray imaging;Convolution;Attenuation;Three-dimensional displays","","","","","60","IEEE","","","","IEEE","IEEE Journals"
"Automatic Recognition of Highway Tunnel Defects Based on an Improved U-Net Model","X. Miao; J. Wang; Z. Wang; Q. Sui; Y. Gao; P. Jiang","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Qilu Transportation, Shandong University, Jinan, Chian","IEEE Sensors Journal","","2019","19","23","11413","11423","It is critically important for the safe operation of highway tunnels that defects in tunnel linings be promptly identified. Recently, many effective identification models based on deep learning technology have emerged in the field of tunnel defect recognition. However, the results of previous studies indicate that imperfections exist in these models in which the boundaries of the defects can only be generally located in the complex visual environment of tunnel linings, which include noisy patterns and uneven light sources. In order to more accurately identify defect details in such complex environments, an improved U-net model including a combined Squeeze-and-Excitation (SE) and ResNet block is proposed and evaluated in this paper. The accuracy of the conventional U-net model is found to be considerably improved by the inclusion of the SE and ResNet blocks. An under-sampling strategy is utilized to address the problem of imbalanced crack image data and determine an appropriate ratio of negative to positive samples for the crack dataset. The effectiveness of the proposed model is then demonstrated using actual highway tunnel lining images to compare its results with those of the Gabor Filter, Multiscale DCNN, FCN, and conventional U-net defect recognition models. The accuracy of the proposed model was found to be quite competitive with extant models.","","","10.1109/JSEN.2019.2934897","National Key R&D Program of China; National Natural Science Foundation of China; Key R&D Project of Shandong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794836","Operational highway tunnels;structural defect recognition;deep learning;U-net","","","","","","39","IEEE","","","","IEEE","IEEE Journals"
"Occluded Pedestrian Detection Based on Depth Vision Significance in Biomimetic Binocular","W. Wei; L. Cheng; Y. Xia; P. Zhang; J. Gu; X. Liu","School of Optoelectronic Science and Engineering, Soochow University, Suzhou, China; School of Optoelectronic Science and Engineering, Soochow University, Suzhou, China; School of Optoelectronic Science and Engineering, Soochow University, Suzhou, China; School of Optoelectronic Science and Engineering, Soochow University, Suzhou, China; School of Optoelectronic Science and Engineering, Soochow University, Suzhou, China; Department of Mechanical and Industrial Engineering, University of Toronto, Toronto, ON, Canada","IEEE Sensors Journal","","2019","19","23","11469","11474","Pedestrian detection and tracking has become an important field in the field of computer vision research. However, the existing pedestrian detection algorithms have some problems, such as low accuracy and poor stability due to the similar background and overlapped occlusion interference. Therefore, an occluded pedestrian detection method based on binocular vision is proposed in this paper. We simulate the recognition of human brain and use the deep learning network MobileNet to detect and locate the initial pedestrians. Then, binocular depth is introduced as visual salience prior information, which solves the problem of identifying pedestrians with similar background and occlusion. The experimental results show that our pedestrian detection framework greatly improves the pedestrian error detection under similar background and occlusion conditions.","","","10.1109/JSEN.2019.2929527","Natural Science Research General Program of Higher Education of Jiangsu Province; Jiangsu Students’ Platform for Innovation and Entrepreneurship Training Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765820","Pedestrian detection;binocular vision;deep learning network;visual salience prior information","Visualization;Convolution;Cameras;Standards;Feature extraction;Hardware;Sensors","","","","","24","IEEE","","","","IEEE","IEEE Journals"
"Fully Convolutional Networks for Multisource Building Extraction From an Open Aerial and Satellite Imagery Data Set","S. Ji; S. Wei; M. Lu","School of Remote sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote sensing and Information Engineering, Wuhan University, Wuhan, China; Department of Physical Geography, Utrecht University, Utrecht, The Netherlands","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","1","574","586","The application of the convolutional neural network has shown to greatly improve the accuracy of building extraction from remote sensing imagery. In this paper, we created and made open a high-quality multisource data set for building detection, evaluated the accuracy obtained in most recent studies on the data set, demonstrated the use of our data set, and proposed a Siamese fully convolutional network model that obtained better segmentation accuracy. The building data set that we created contains not only aerial images but also satellite images covering 1000 km2 with both raster labels and vector maps. The accuracy of applying the same methodology to our aerial data set outperformed several other open building data sets. On the aerial data set, we gave a thorough evaluation and comparison of most recent deep learning-based methods, and proposed a Siamese U-Net with shared weights in two branches, and original images and their down-sampled counterparts as inputs, which significantly improves the segmentation accuracy, especially for large buildings. For multisource building extraction, the generalization ability is further evaluated and extended by applying a radiometric augmentation strategy to transfer pretrained models on the aerial data set to the satellite data set. The designed experiments indicate our data set is accurate and can serve multiple purposes including building instance segmentation and change detection; our result shows the Siamese U-Net outperforms current building extraction methods and could provide valuable reference.","","","10.1109/TGRS.2018.2858817","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444434","Building extraction;deep learning;full convolutional network;remote sensing building data set","Buildings;Remote sensing;Satellite broadcasting;Image resolution;Data mining;Satellites;Image segmentation","convolutional neural nets;geophysical image processing;image sampling;image segmentation;remote sensing","building instance segmentation;satellite data;open building data sets;aerial data;aerial images;segmentation accuracy;Siamese fully convolutional network model;building detection;high-quality multisource data;convolutional neural network;satellite imagery data set;multisource building extraction;fully convolutional networks","","2","40","","","","","IEEE","IEEE Journals"
"Cloud Detection in Remote Sensing Images Based on Multiscale Features-Convolutional Neural Network","Z. Shao; Y. Pan; C. Diao; J. Cai","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Department of Geography and GIScience, University of Illinois at Urbana–Champaign, Urbana, IL, USA; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","6","4062","4076","Cloud detection in remote sensing images is a challenging but significant task. Due to the variety and complexity of underlying surfaces, most of the current cloud detection methods have difficulty in detecting thin cloud regions. In fact, it is quite meaningful to distinguish thin clouds from thick clouds, especially in cloud removal and target detection tasks. Therefore, we propose a method based on multiscale features-convolutional neural network (MF-CNN) to detect thin cloud, thick cloud, and noncloud pixels of remote sensing images simultaneously. Landsat 8 satellite imagery with various levels of cloud coverage is used to demonstrate the effectiveness of our proposed MF-CNN model. We first stack visible, near-infrared, short-wave, cirrus, and thermal infrared bands of Landsat 8 imagery to obtain the combined spectral information. The MF-CNN model is then used to learn the multiscale global features of input images. The high-level semantic information obtained in the process of feature learning is integrated with low-level spatial information to classify the imagery into thick, thin and noncloud regions. The performance of our proposed model is compared to that of various commonly used cloud detection methods in both qualitative and quantitative aspects. Compared to other cloud detection methods, the experimental results show that our proposed method has a better performance not only in thick and thin clouds but also in the entire cloud regions.","","","10.1109/TGRS.2018.2889677","National Key R&D Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625476","Cloud detection;convolutional neural network (CNN);deep learning;multiscale features (MF);remote sensing images","Clouds;Remote sensing;Earth;Cloud computing;Feature extraction;Artificial satellites;Training","clouds;convolutional neural nets;geophysical image processing;image classification;object detection;remote sensing","cloud removal;multiscale features-convolutional neural network;remote sensing images;Landsat 8 satellite imagery;cloud coverage;MF-CNN model;Landsat 8 imagery;thermal infrared bands;cloud detection method;cloud regions","","","46","","","","","IEEE","IEEE Journals"
"Video Saliency Prediction Based on Spatial-Temporal Two-Stream Network","K. Zhang; Z. Chen","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","12","3544","3557","In this paper, we propose a novel two-stream neural network for video saliency prediction. Unlike some traditional methods based on hand-crafted feature extraction and integration, our proposed method automatically learns saliency related spatiotemporal features from human fixations without any pre-processing, post-processing, or manual tuning. Video frames are routed through the spatial stream network to compute static or color saliency maps for each of them. And a new two-stage temporal stream network is proposed, which is composed of a pre-trained 2D-CNN model (SF-Net) to extract saliency related features and a shallow 3D-CNN model (Te-Net) to process these features, for temporal or dynamic saliency maps. It can reduce the requirement of video gaze data, improve training efficiency, and achieve high performance. A fusion network is adopted to combine the outputs of both streams and generate the final saliency maps. Besides, a convolutional Gaussian priors (CGP) layer is proposed to learn the bias phenomenon in viewing behavior to improve the performance of the video saliency prediction. The proposed method is compared with state-of-the-art saliency models on two public video saliency benchmark datasets. The results demonstrate that our model can achieve advanced performance on video saliency prediction.","","","10.1109/TCSVT.2018.2883305","National Key R&D Program of China; National Natural Science Foundation of China; Wuhan Morning Light Plan of Youth Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543830","Video saliency;spatial-temporal features;visual attention;deep learning","Feature extraction;Predictive models;Streaming media;Visualization;Spatiotemporal phenomena;Computational modeling","","","","","81","IEEE","","","","IEEE","IEEE Journals"
"FV-GAN: Finger Vein Representation Using Generative Adversarial Networks","W. Yang; C. Hui; Z. Chen; J. Xue; Q. Liao","Graduate School at Shenzhen/Department of E.E., Shenzhen Key Laboratory of Information Science and Technology/Shenzhen Engineering Laboratory of IS&DCP, Tsinghua University, Shenzhen, China; Graduate School at Shenzhen/Department of E.E., Shenzhen Key Laboratory of Information Science and Technology/Shenzhen Engineering Laboratory of IS&DCP, Tsinghua University, Shenzhen, China; Graduate School at Shenzhen/Department of E.E., Shenzhen Key Laboratory of Information Science and Technology/Shenzhen Engineering Laboratory of IS&DCP, Tsinghua University, Shenzhen, China; Department of Statistical Science, University College London, London, U.K.; Graduate School at Shenzhen/Department of E.E., Shenzhen Key Laboratory of Information Science and Technology/Shenzhen Engineering Laboratory of IS&DCP, Tsinghua University, Shenzhen, China","IEEE Transactions on Information Forensics and Security","","2019","14","9","2512","2524","In finger vein verification, the most important and challenging part is to robustly extract finger vein patterns from low-contrast infrared finger images with limited a priori knowledge. Although recent convolutional neural network (CNN)-based methods for finger vein verification have shown powerful capacity for feature representation and promising perspective in this area, they still have two critical issues to address. First, these CNN-based methods unexceptionally utilize fully connected layers, which restrict the size of finger vein images to process and increase the processing time. Second, the capacity of CNN for feature representation generally suffers from the low quality of finger vein ground-truth pattern maps for training, particularly due to outliers and vessel breaks. To address these issues, in this paper, we propose a novel approach termed FV-GAN to finger vein extraction and verification, based on generative adversarial network (GAN), as the first attempt in this area. Unlike the CNN-based methods, FV-GAN learns from the joint distribution of finger vein images and pattern maps rather than the direct mapping between them, with the aim at achieving stronger robustness against outliers and vessel breaks. Moreover, FV-GAN adopts fully convolutional networks as the basic architecture and discards fully connected layers, which relaxes the constraint on the input image size and reduces the computational expenditure for feature extraction. Furthermore, we design an adversarial training strategy and propose a hybrid loss function for FV-GAN. The experimental results on two public databases show significant improvement by FV-GAN in finger vein verification in terms of both verification accuracy and equal error rate.","","","10.1109/TIFS.2019.2902819","National Natural Science Foundation of China; Special Foundation for the Development of Strategic Emerging Industries of Shenzhen; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658115","Finger vein verification;pattern extraction;convolutional neural network (CNN);generative adversarial network (GAN);cycle-consistent adversarial network (CycleGAN);deep convolutional generative adversarial network (DCGAN);U-Net","Veins;Gallium nitride;Feature extraction;Training;Generative adversarial networks;Generators;Skin","convolutional neural nets;feature extraction;image representation;infrared imaging;learning (artificial intelligence);vein recognition","FV-GAN;finger vein representation;generative adversarial network;finger vein verification;feature representation;CNN-based methods;finger vein images;vein extraction;convolutional neural network-based methods;fingervein ground-truth pattern maps;low-contrast infrared finger images;feature extraction","","","40","","","","","IEEE","IEEE Journals"
"Survey on GAN-based face hallucination with its model development","H. Liu; X. Zheng; J. Han; Y. Chu; T. Tao","Anhui University of Technology, Maxiang Road, Ma'anshan 243000, People's Republic of China; Anhui University of Technology, Maxiang Road, Ma'anshan 243000, People's Republic of China; School of Computing & Communications, Lancaster University, LA1 4YW, UK; Anhui University of Technology, Maxiang Road, Ma'anshan 243000, People's Republic of China; Anhui University of Technology, Maxiang Road, Ma'anshan 243000, People's Republic of China","IET Image Processing","","2019","13","14","2662","2672","Face hallucination aims to produce a high-resolution face image from an input low-resolution face image, which is of great importance for many practical face applications, such as face recognition and face verification. Since the structure of the face image is complex and sensitive, obtaining a super-resolved face image is more difficult than generic image super-resolution. Recently, with great success in the high-level face recognition task, deep learning methods, especially generative adversarial networks (GANs), have also been applied to the low-level vision task – face hallucination. This work is to provide a model evolvement survey on GAN-based face hallucination. The principles of image resolution degradation and GAN-based learning are presented firstly. Then, a comprehensive review of the state-of-art GAN-based face hallucination methods is provided. Finally, the comparisons of these GAN-based face hallucination methods and the discussions of the related issues for future research direction are also provided.","","","10.1049/iet-ipr.2018.6545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946945","","","image resolution;face recognition;learning (artificial intelligence)","high-resolution face image;input low-resolution face image;practical face applications;face verification;super-resolved face image;generic image super-resolution;high-level face recognition task;image resolution degradation;GAN-based learning;state-of-art GAN-based face hallucination","","","47","","","","","IET","IET Journals"
"Global Inference for Aspect and Opinion Terms Co-Extraction Based on Multi-Task Neural Networks","J. Yu; J. Jiang; R. Xia","School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","1","168","177","Extracting aspect terms and opinion terms are two fundamental tasks in opinion mining. The recent success of deep learning has inspired various neural network architectures, which have been shown to achieve highly competitive performance in these two tasks. However, most existing methods fail to explicitly consider the syntactic relations among aspect terms and opinion terms, which may lead to the inconsistencies between the model predictions and the syntactic constraints. To this end, we first apply a multi-task learning framework to implicitly capture the relations between the two tasks, and then propose a global inference method by explicitly modelling several syntactic constraints among aspect term extraction and opinion term extraction to uncover their intra-task and inter-task relationship, which seeks an optimal solution over the neural predictions for both tasks. Extensive evaluations on three benchmark datasets demonstrate that our global inference approach is able to bring consistent improvements over several base models in different scenarios.","","","10.1109/TASLP.2018.2875170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8488594","Natural language processing;sentiment analysis;opinion mining;neural networks","Task analysis;Neural networks;Syntactics;Benchmark testing;Sentiment analysis;Labeling;Standards","data mining;inference mechanisms;learning (artificial intelligence);neural nets","syntactic relations;aspect terms;syntactic constraints;multitask learning framework;global inference method;aspect term extraction;opinion term extraction;inter-task relationship;neural predictions;global inference approach;multitask neural networks;opinion mining;neural network architectures;opinion terms co-extraction","","1","42","","","","","IEEE","IEEE Journals"
"Sequence SAR Image Classification Based on Bidirectional Convolution-Recurrent Network","X. Bai; R. Xue; L. Wang; F. Zhou","National Lab of Radar Signal Processing, Xidian University, Xi’an, China; National Lab of Radar Signal Processing, Xidian University, Xi’an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology, Ministry of Education, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","9223","9235","Although the deep convolutional neural network (DCNN) has been successfully applied to target classification of military vehicles based on synthetic aperture radar (SAR), most of the available methods do not fully exploit the characteristics of continuous SAR imaging and only utilize single image for recognition. To extract significant identification features contained in the image sequence, this paper proposes a sequence of SAR target classification method based on bidirectional convolution-recurrent network. In this network, we extract spatial features of each image through DCNNs without the fully connected layer, and then learn sequence features by bidirectional long short-term memory networks. Finally, we design the average softmax classifier to obtain the classification results. Compared with the available methods, the proposed network takes advantage of the significant information in the image sequence and achieves higher classification accuracy in the moving and stationary target acquisition and recognition data set. In addition, it has shown robustness to large depression angle variants, configuration variants, and version variants.","","","10.1109/TGRS.2019.2925636","National Natural Science Foundation of China; Foundation for the Author of National Excellent Doctoral Dissertation of the People's Republic of China; Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772147","Deep convolutional neural network (DCNN);image sequence;long short-term memory (LSTM);synthetic aperture radar (SAR);target classification","Feature extraction;Synthetic aperture radar;Radar polarimetry;Convolution;Periodic structures;Image sequences;Kernel","convolutional neural nets;feature extraction;image classification;image sequences;radar imaging;radar target recognition;recurrent neural nets;synthetic aperture radar","short-term memory networks;image sequence;moving target acquisition;stationary target acquisition;sequence SAR image classification;bidirectional convolution-recurrent network;deep convolutional neural network;synthetic aperture radar;SAR imaging;SAR target classification method","","","59","","","","","IEEE","IEEE Journals"
"Data-Aided Channel Estimation for Multiple-Antenna Users in Massive MIMO Systems","A. S. Alwakeel; A. H. Mehana","Department of Electrical and Communications Engineering, KCST University, Doha 35007, Kuwait; Department of Electronics and Electrical Communications Engineering, Cairo University, Giza, Egypt","IEEE Transactions on Vehicular Technology","","2019","68","11","10752","10760","In this paper, we study the performance of large-scale MIMO network where the base station has a large number of antennas and performs semi-blind channel estimation. We show that one can increase the number of antennas at the user side without increasing the overhead imposed by the channel estimation linearly with the number of user antennas. Specifically, we propose a simple data-aided channel estimation scheme which exploits the quasi-orthogonality of the channel vectors and takes advantage of the conventional pilot-aided channel estimation performed for the first antenna at each user. Two schemes are then analyzed using the extra antenna(s) at the users: data-multiplexing and interference alignment. We obtain achievable rates for the proposed uplink multi-cell MIMO system in both schemes. The rate bounds are tight and the system performance is also analyzed in terms of the outage probability to study the effect of the data length used for channel estimation scheme.","","","10.1109/TVT.2019.2938344","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8821395","","Channel estimation;Antennas;Interference;Uplink;Deep learning","antenna arrays;channel estimation;MIMO communication","multiple-antenna users;massive MIMO systems;large-scale MIMO network;base station;semiblind channel estimation;user antennas;simple data-aided channel estimation scheme;channel vectors;pilot-aided channel estimation;data-multiplexing;uplink multicell MIMO system;system performance;data length;outage probability","","","41","IEEE","","","","IEEE","IEEE Journals"
"Superpixel Segmentation Based on Square-Wise Asymmetric Partition and Structural Approximation","H. Li; S. Kwong; C. Chen; Y. Jia; R. Cong","Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong; School of Software Engineering, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong; Institute of Information Science, Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Multimedia","","2019","21","10","2625","2637","Superpixel segmentation aims at grouping discretizing pixels into high-level correlative units and reducing the complexity of subsequent tasks, e.g., saliency detection and object tracking. Existing superpixel segmentation algorithms mainly focus on maintaining the geometrical information, while neglecting the irregular structure of superpixels. In this paper, a superpixel segmentation method is proposed to generate approximately structural superpixels with sharp boundary adherence and comprehensive semantic information. The superpixel segmentation is formulated as a square-wise asymmetric partition problem, where the semantic perceptual superpixels are recorded in a square level to preserve abundant semantic information and save storage simultaneously. Moreover, in order to achieve regular-shape superpixel units to better adhere to image boundaries and contours, a combinatorial optimization strategy is devised to achieve an optimal combination of squares and isolated pixels. Experimental comparisons with some state-of-the-art superpixel segmentation methods on the public benchmarks demonstrate the effectiveness of the proposed method quantitatively and qualitatively. In addition, we have applied the method to brain tissue segmentation to illustrate superior performance.","","","10.1109/TMM.2019.2907047","National Natural Science Foundation of China; Hong Kong RGC General Research Funds; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673630","Superpixel;square-wise asymmetric partition;structural approximation;combinatorial optimization","Image segmentation;Optimization;Shape;Urban areas;Deep learning;Semantics;Image color analysis","approximation theory;image resolution;image segmentation;object detection;object tracking;optimisation;semantic networks","semantic information;geometrical information;sharp boundary adherence;combinatorial optimization strategy;image boundaries;image contours;brain tissue segmentation;regular-shape superpixel units;semantic perceptual superpixels;square-wise asymmetric partition problem;superpixel segmentation method;object tracking;saliency detection;high-level correlative units;structural approximation","","","46","Traditional","","","","IEEE","IEEE Journals"
"A Novel Joint Change Detection Approach Based on Weight-Clustering Sparse Autoencoders","J. Fan; K. Lin; M. Han","Key Laboratory of Sea-Area Management Technology, Department of Ocean Remote Sensing, National Marine Environmental Monitoring Center, Dalian, China; School of Control Science and Engineering, Dalian University of Technology, Dalian, China; School of Control Science and Engineering, Dalian University of Technology, Dalian, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","2","685","699","With the rapid development of earth observation technology, the number of available remote sensing data has soared dramatically. It becomes a significant problem that how to use remote sensing images and how to improve the accuracy of change detection effectively. In this paper, a novel approach for change detection using weight-clustering sparse autoencoders (WCSAE) combined object-oriented classification with difference images (DIs) is proposed. First, bi-phase images are segmented as patches through the density-based spatial clustering of applications with noise algorithm. Afterward, the average and variance of superpixels are stacked as the input of WCSAE. To reduce the redundant information of extracted features, similar weights in the hidden layer of WCSAE are clustered layer-wise under termination conditions by using the hierarchical agglomerative clustering algorithm. For the improvement of the classification accuracy, L1/2 regularization is introduced in the objective function to extract more sparse features and avoid over-fitting. Next, the post-classification change detection map is obtained by means of comparing with classification results of two phase images. Then, by using the change vector analysis technology, the difference map is yielded and also classified under WCSAE to acquire the DI classification map. Finally, the joint probability judgment is implemented on the joint scopes to determine changed and unchanged areas. The effectiveness and superiority of the proposed method are verified in accordance with the experimental results on standard datasets and actual remote sensing images.","","","10.1109/JSTARS.2019.2892951","National Key R&D Program of China; National Natural Science Foundation of China; National High Resolution Special Research; Key Laboratory of Sea-Area Management Technology Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661576","Difference images (DIs);joint change detection;object-oriented classification;weight-clustering sparse autoencoders (WCSAE)","Feature extraction;Remote sensing;Image segmentation;Earth;Clustering algorithms;Deep learning;Fans","feature extraction;geophysical image processing;image classification;image segmentation;pattern clustering;probability;remote sensing","bi-phase images;noise algorithm;WCSAE;hierarchical agglomerative clustering algorithm;classification accuracy;sparse features;post-classification change detection map;change vector analysis technology;difference map;DI classification map;joint probability judgment;actual remote sensing images;novel joint change detection approach;earth observation technology;available remote sensing data;weight-clustering sparse autoencoders combined object-oriented classification;difference images;density-based spatial clustering","","2","57","","","","","IEEE","IEEE Journals"
"Multiscale Visual Attention Networks for Object Detection in VHR Remote Sensing Images","C. Wang; X. Bai; S. Wang; J. Zhou; P. Ren","School of Computer Science and Engineering, Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China; School of Computer Science and Engineering, Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China; School of Computer Science and Engineering, Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China; School of Information and Communication Technology, Griffith University, Nathan, QLD, Australia; College of Information and Control Engineering, China University of Petroleum—East China, Qingdao, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","2","310","314","Object detection plays an active role in remote sensing applications. Recently, deep convolutional neural network models have been applied to automatically extract features, generate region proposals, and predict corresponding object class. However, these models face new challenges in VHR remote sensing images due to the orientation and scale variations and the cluttered background. In this letter, we propose an end-to-end multiscale visual attention networks (MS-VANs) method. We use skip-connected encoder-decoder model to extract multiscale features from a full-size image. For feature maps in each scale, we learn a visual attention network, which is followed by a classification branch and a regression branch, so as to highlight the features from object region and suppress the cluttered background. We train the MS-VANs model by a hybrid loss function which is a weighted sum of attention loss, classification loss, and regression loss. Experiments on a combined data set consisting of Dataset for Object Detection in Aerial Images and NWPU VHR-10 show that the proposed method outperforms several state-of-the-art approaches.","","","10.1109/LGRS.2018.2872355","National Natural Science Foundation of China; National Natural Science Foundation of China; State Key Laboratory of Software Development Environment; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8513990","Multiscale feature;object detection;VHR remote sensing image;visual attention","Feature extraction;Visualization;Object detection;Remote sensing;Proposals;Geospatial analysis;Task analysis","feature extraction;feedforward neural nets;geophysical image processing;image classification;learning (artificial intelligence);object detection;pattern classification;remote sensing","end-to-end multiscale visual attention network method;object detection;aerial images;multiscale features;encoder-decoder model;VHR remote sensing images;deep convolutional neural network models;remote sensing applications;active role;VHR remote sensing Images;NWPU VHR-10 show;MS-VANs model;cluttered background;object region;visual attention network;feature maps;full-size image","","5","18","","","","","IEEE","IEEE Journals"
"SAR Image Retrieval Based on Unsupervised Domain Adaptation and Clustering","F. Ye; W. Luo; M. Dong; H. He; W. Min","School of Surveying and Mapping Engineering, East China University of Technology, Nanchang, China; School of Information Engineering, Nanchang University, Nanchang, China; School of Information Engineering, Nanchang University, Nanchang, China; School of Information Engineering, Nanchang University, Nanchang, China; School of Software, Nanchang University, Nanchang, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","9","1482","1486","Efficiently retrieving synthetic aperture radar (SAR) image is an important yet challenging task in the remote sensing field. Due to the shortage of labeled SAR images for fine-tuning convolutional neural network (CNN) models, this letter presents an unsupervised domain adaptation model based on CNN to learn the domain-invariant feature between SAR images and optical aerial images for SAR image retrieving, which can alleviate the burden of manual labeling. We extend a deep CNN to a novel adversarial network by adding the domain discriminator and the pseudolabel predictor. We improve the adaptation capacity of the adversarial network by utilizing the class information of SAR training images, which is obtained by clustering. Compared with the other related methods, the proposed method can enhance retrieval performance with our SAR data set.","","","10.1109/LGRS.2019.2896948","National Natural Science Foundation of China; Natural Science Foundation of Jiangxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660481","Clustering;remote sensing image retrieval (RSIR);synthetic aperture radar (SAR);unsupervised domain adaptation (DA)","Synthetic aperture radar;Image retrieval;Training;Adaptation models;Feature extraction;Task analysis;Remote sensing","convolutional neural nets;image retrieval;optical information processing;pattern clustering;radar computing;radar imaging;synthetic aperture radar;unsupervised learning","adversarial network adaptation capacity;pseudolabel predictor;manual labeling;SAR data set;deep CNN;retrieval performance;SAR training images;domain discriminator;optical aerial images;domain-invariant feature;unsupervised domain adaptation model;fine-tuning convolutional neural network models;labeled SAR images;remote sensing field;important yet challenging task;synthetic aperture radar image;SAR image retrieval","","1","21","","","","","IEEE","IEEE Journals"
"Transformer Fault Diagnosis Method Based on Self-Powered RFID Sensor Tag, DBN, and MKSVM","C. Zhang; Y. He; S. Jiang; T. Wang; L. Yuan; B. Li","School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Electrical Engineering and Automation, Wuhan University, Wuhan, China; School of Physics and Electronic Engineering, Anqing Normal University, Anqing, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China","IEEE Sensors Journal","","2019","19","18","8202","8214","A novel transformer fault diagnosis method using self-powered radio frequency identification (RFID) sensor tag, deep belief network (DBN), and multiple kernel support vector machine (MKSVM) is presented. The self-powered RFID sensor tag is applied to measure transformer vibration signals, which has advantages of convenience, long-term monitoring, low power consumption, and fast location. Then, the DBN is employed to extract features from the measured signals, and the DBN method's extraction performance is improved by optimizing its learning rates and momentum factors. The extracted features of the same fault are highly centralized, and the features of the different faults are obviously separated. Based on the features, the MKSVM is utilized to construct a transformer fault diagnosis model, and the MKSVM's penalty factor and kernel weights are optimized through a quantum-behaved particle swarm optimization (QPSO) algorithm. Finally, a mean signal deviation (MSD) metric is presented to locate the fault position. In this experiment, a 10 kV transformer is used to demonstrate the proposed transformer fault diagnosis procedure, and the diagnosis results reflect that the proposed method can effectively measure the transformer vibration signals, extract the essential features, construct an accurate diagnosis model, and locate the fault position.","","","10.1109/JSEN.2019.2919868","National Natural Science Foundation of China; State Key Program of National Natural Science Foundation of China; National Key Research and Development Plan “Important Scientific Instruments And Equipment Development”; Equipment Research Project in Advance; Natural Science Foundation of Hunan Province; Anhui Department of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8726315","Transformer;fault diagnosis;self-powered RFID;DBN;features extraction;MKSVM;MSD","Feature extraction;Radiofrequency identification;Fault diagnosis;Vibrations;Circuit faults;Kernel","belief networks;fault diagnosis;learning (artificial intelligence);particle swarm optimisation;power engineering computing;radiofrequency identification;support vector machines;transformers;vibrations","transformer fault diagnosis method;measured signals;low power consumption;multiple kernel support vector machine;deep belief network;self-powered radio frequency identification sensor tag;self-powered RFID sensor tag;accurate diagnosis model;transformer vibration signals;fault position;mean signal deviation metric;quantum-behaved particle swarm optimization algorithm;kernel weights;transformer fault diagnosis model;MKSVM;DBN method;voltage 10.0 kV","","","45","","","","","IEEE","IEEE Journals"
"Utterance Generation With Variational Auto-Encoder for Slot Filling in Spoken Language Understanding","Y. Shin; K. M. Yoo; S. Lee","Department of Computer Science Engineering, Seoul National University, Seoul, South Korea; Department of Computer Science Engineering, Seoul National University, Seoul, South Korea; Department of Computer Science Engineering, Seoul National University, Seoul, South Korea","IEEE Signal Processing Letters","","2019","26","3","505","509","Slot filling must be trained using human-labeled data that are expensive and only a limited amount of labeled utterances are readily available for learning. Data generation methods can help increase the size of the dataset and make variations to the training dataset by means of emerging new instances. We propose a novel labeled utterance generation algorithm to augment training data. Our hypothesis is that words in an utterance can be separated into the two parts, namely, slot values that are instances of slot types and the accompanying contexts. Our model aims to generate utterances that are diverse combinations of slot values and contexts that can appear together. To create various utterances containing a given condition, our deep generative model uses a conditional variational auto-encoder architecture. We conduct experiments on various slot filling datasets, specifically airline travel information systems (ATIS), Snips, and MIT Corpus. A quantitative analysis shows that the application of data augmentation using the proposed model improves the F1 score for slot filling. We also demonstrate that our labeled utterance generation model yields more desirable utterances.","","","10.1109/LSP.2019.2895284","National Research Foundation of Korea; Korea government (MSIT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625384","Variational auto-encoder (VAE);data generation;data augmentation;slot filling","Decoding;Generators;Training;Computational modeling;Training data;Filling;Context modeling","learning (artificial intelligence);natural language processing;speech processing;travel industry","spoken language understanding;data generation methods;training dataset;slot values;slot types;deep generative model;conditional variational auto-encoder architecture;slot filling datasets;data augmentation;labeled utterance generation model;airline travel information systems;ATIS;Snips;MIT Corpus;quantitative analysis;F1 score","","","25","","","","","IEEE","IEEE Journals"
"Deeply Supervised Depth Map Super-Resolution as Novel View Synthesis","X. Song; Y. Dai; X. Qin","School of Computer Science and Technology, Shandong University, Jinan, China; Shaanxi Key Lab of Information Acquisition and Processing, School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Software, Shandong University, Jinan, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","8","2323","2336","Deep convolutional neural network (DCNN) has been successfully applied to depth map super-resolution and outperforms existing methods by a wide margin. However, there still exist two major issues with these DCNN-based depth map super-resolution methods that hinder the performance: 1) the low-resolution depth maps either need to be up-sampled before feeding into the network or substantial deconvolution has to be used and 2) the supervision (high-resolution depth maps) is only applied at the end of the network, thus it is difficult to handle large up-sampling factors, such as x8 and x16. In this paper, we propose a new framework to tackle the above problems. First, we propose to represent the task of depth map superresolution as a series of novel view synthesis sub-tasks. The novel view synthesis sub-task aims at generating (synthesizing) a depth map from a different camera pose, which could be learned in parallel. Second, to handle large up-sampling factors, we present a deeply supervised network structure to enforce strong supervision in each stage of the network. Third, a multiscale fusion strategy is proposed to effectively exploit the feature maps at different scales and handle the blocking effect. In this way, our proposed framework could deal with challenging depth map super-resolution efficiently under large up-sampling factors (e.g., x8 and x16). Our method only uses the low-resolution depth map as input, and the support of color image is not needed, which greatly reduces the restriction of our method. Extensive experiments on various benchmarking data sets demonstrate the superiority of our method over current state-of-the-art depth map super-resolution methods.","","","10.1109/TCSVT.2018.2866399","National Natural Science Foundation of China; Australian Research Council; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8443445","Convolutional neural network;depth map;super-resolution;novel view synthesis","Color;Task analysis;Spatial resolution;Deconvolution;Cameras;DH-HEMTs","cameras;convolutional neural nets;feature extraction;image colour analysis;image fusion;image reconstruction;image resolution;learning (artificial intelligence)","supervised depth map super-resolution;DCNN-based depth map super-resolution methods;low-resolution depth map;high-resolution depth maps;deeply supervised network structure;feature maps;deep convolutional neural network;camera pose;multiscale fusion strategy;image colour analysis","","1","56","","","","","IEEE","IEEE Journals"
"Vehicle Classification Based on Seismic Signatures Using Convolutional Neural Network","G. Jin; B. Ye; Y. Wu; F. Qu","Department of Ocean Science and Engineering, Zhejiang University, Zhoushan, China; Department of Ocean Science and Engineering, Zhejiang University, Zhoushan, China; Department of Ocean Science and Engineering, Zhejiang University, Zhoushan, China; Department of Ocean Science and Engineering, Zhejiang University, Zhoushan, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","4","628","632","Seismic signals can be used for vehicle classification. However, this task becomes difficult as a result of various noises. Convolutional neural networks (CNNs) have been employed successfully in many fields as a result of its ability to learn low-/mid-/high-level features. This letter investigates the application of CNN to classify vehicles by means of the seismic trace that the geophone recorded. The study has two primary contributions. First, a deep CNN architecture for vehicle classification by seismic signal is proposed. Second, considering the similarities between speech recognition and vehicle classification based on seismic signal, log-scaled frequency cepstral coefficient (LFCC) matrix is proposed to extract features of seismic signals as the input of CNN. The data from DARPA's SensIt project, which contain seismic signals from two kinds of vehicles, are used to evaluate the method. By combining the proposed LFCC matrix and CNN architecture, the algorithm produces a state-of-the-art result compared with other methods.","","","10.1109/LGRS.2018.2879687","Harbin Engineering University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8558568","Classification;convolutional neural network (CNN);log-scaled frequency cepstral coefficients (LFCCs) matrix;seismic signal","Mel frequency cepstral coefficient;Convolution;Feature extraction;Computer architecture;Convolutional neural networks","cepstral analysis;convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);matrix algebra;neural net architecture;seismology;seismometers;speech recognition","vehicle classification;seismic signal;seismic signatures;convolutional neural network;seismic trace;deep CNN architecture;log-scaled frequency cepstral coefficient matrix;LFCC matrix;feature extraction;DARPA SensIt project","","","28","","","","","IEEE","IEEE Journals"
"Objective Detection of Eloquent Axonal Pathways to Minimize Postoperative Deficits in Pediatric Epilepsy Surgery Using Diffusion Tractography and Convolutional Neural Networks","H. Xu; M. Dong; M. Lee; N. O’Hara; E. Asano; J. Jeong","Department of Computer Science, Wayne State University, Detroit, MI, USA; Department of Computer Science, Wayne State University, Detroit, MI, USA; Department of Pediatrics and Neurology, Wayne State University, Detroit, MI, USA; Translational Neuroscience Program, Wayne State University, Detroit, MI, USA; Department of Pediatrics and Neurology, Wayne State University, Detroit, MI, USA; Department of Pediatrics and Neurology, Wayne State University, Detroit, MI, USA","IEEE Transactions on Medical Imaging","","2019","38","8","1910","1922","Convolutional neural networks (CNNs) have recently been used in biomedical imaging applications with great success. In this paper, we investigated the classification performance of CNN models on diffusion weighted imaging (DWI) streamlines defined by functional MRI (fMRI) and electrical stimulation mapping (ESM). To learn a set of discriminative and interpretable features from the extremely unbalanced dataset, we evaluated different CNN architectures with multiple loss functions (e.g., focal loss and center loss) and a soft attention mechanism and compared our models with current state-of-the-art methods. Through extensive experiments on streamlines collected from 70 healthy children and 70 children with focal epilepsy, we demonstrated that our deep CNN model with focal and central losses and soft attention outperforms all existing models in the literature and provides clinically acceptable accuracy (73%-100%) for the objective detection of functionally important white matter pathways, including ESM determined eloquent areas such as primary motors, aphasia, speech arrest, auditory, and visual functions. The findings of this paper encourage further investigations to determine if DWI-CNN analysis can serve as a noninvasive diagnostic tool during pediatric presurgical planning by estimating not only the location of essential cortices at the gyral level but also the underlying fibers connecting these cortical areas to minimize or predict postsurgical functional deficits. This paper translates an advanced CNN model to clinical practice in the pediatric population where currently available approaches (e.g., ESM and fMRI) are suboptimal. The implementation will be released at https://github.com/HaotianMXu/Brain-fiber-classification-using-CNNs.","","","10.1109/TMI.2019.2902073","National Science Foundation; National Institute of Neurological Disorders and Stroke; National Institute of Neurological Disorders and Stroke; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653838","Convolutional neural network;DWI streamline;eloquent function;epilepsy surgery","White matter;Epilepsy;Pediatrics;Surgery;Functional magnetic resonance imaging;Visualization","biodiffusion;biomedical MRI;brain;convolutional neural nets;image classification;learning (artificial intelligence);medical disorders;medical image processing;neurophysiology;paediatrics;surgery","CNN architectures;image classification performance;interpretable features;discriminative features;electrical stimulation mapping;diffusion weighted imaging;CNN models;biomedical imaging applications;convolutional neural networks;diffusion tractography;pediatric epilepsy surgery;minimize postoperative deficits;eloquent axonal pathways;pediatric population;postsurgical functional deficits;pediatric presurgical planning;DWI-CNN analysis;visual functions;ESM determined eloquent areas;functionally important white matter pathways;objective detection;clinically acceptable accuracy;deep CNN model;focal epilepsy;soft attention mechanism;multiple loss functions","","","61","","","","","IEEE","IEEE Journals"
"Anticipating Where People will Look Using Adversarial Networks","M. Zhang; K. T. Ma; J. H. Lim; Q. Zhao; J. Feng","National University of Singapore, Singapore; A*AI and I2R, A*STAR, Singapore; A*AI and I2R, A*STAR, Singapore; University of Minnesota, Minneapolis, MN, USA; National University of Singapore, Singapore","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","8","1783","1796","We introduce a new problem of gaze anticipation on future frames which extends the conventional gaze prediction problem to go beyond current frames. To solve this problem, we propose a new generative adversarial network based model, Deep Future Gaze (DFG), encompassing two pathways: DFG-P is to anticipate gaze prior maps conditioned on the input frame which provides task influences; DFG-G is to learn to model both semantic and motion information in future frame generation. DFG-P and DFG-G are then fused to anticipate future gazes. DFG-G consists of two networks: a generator and a discriminator. The generator uses a two-stream spatial-temporal convolution architecture (3D-CNN) for explicitly untangling the foreground and background to generate future frames. It then attaches another 3D-CNN for gaze anticipation based on these synthetic frames. The discriminator plays against the generator by distinguishing the synthetic frames of the generator from the real frames. Experimental results on the publicly available egocentric and third person video datasets show that DFG significantly outperforms all competitive baselines. We also demonstrate that DFG achieves better performance of gaze prediction on current frames in egocentric and third person videos than state-of-the-art methods.","","","10.1109/TPAMI.2018.2871688","Reverse Engineering Visual Intelligence for cognitiVe Enhancement (REVIVE); A*STAR; National University of Singapore; Ministry of Education of Singapore AcRF Tier One; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8471119","Egocentric videos;gaze anticipation;generative adversarial network;saliency;visual attention","Task analysis;Predictive models;Streaming media;Generators;Visualization;Generative adversarial networks;Training","convolutional neural nets;feature extraction;gaze tracking;learning (artificial intelligence);video signal processing","adversarial networks;gaze anticipation;generative adversarial network based model;DFG-P;input frame;DFG-G;semantic motion information;spatial-temporal convolution architecture;synthetic frames;frame generation;deep future gaze;gaze prediction problem;third person video datasets;egocentric datasets;3D-CNN","","","76","","","","","IEEE","IEEE Journals"
"Few-shot palmprint recognition via graph neural networks","H. Shao; D. Zhong","School of Electronic and Information Engineering, Xi'an Jiaotong University, People's Republic of China; School of Electronic and Information Engineering, Xi'an Jiaotong University, People's Republic of China","Electronics Letters","","2019","55","16","890","892","At present, palmprint recognition based on deep learning has been more and more widely used in identity recognition due to its many advantages. However, these algorithms often require a large amount of labelled data for training. In fact, it is difficult and expensive to get enough data that meets the requirements. In this Letter, based on a small amount of labelled images, the authors proposed a method for few-shot palmprint recognition using Graph Neural Networks (GNNs). The palmprint features extracted by the convolutional neural network are processed into nodes in the GNN. The edges in the GNN are used to represent similarities between image nodes. The parameters in the network are continuously optimised, and finally, the category to which each image belongs is obtained. Further, they adopted a mobile phone to create a palmprint database in an unconstrained way. Adequate experiments were performed on the benchmark database and the authors' self-built database. The experimental results show that their proposed GNN-based few-shot palmprint recognition can obtain state-of-the-art performance, where the accuracy is over 99.90%.","","","10.1049/el.2019.1221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789799","","","feature extraction;graph theory;image classification;image recognition;learning (artificial intelligence);neural nets;palmprint recognition","GNN-based;palmprint database;convolutional neural network;palmprint features;graph neural networks;few-shot palmprint recognition;labelled images;labelled data;identity recognition","","","8","","","","","IET","IET Journals"
"PFCN: a fully convolutional network for point cloud semantic segmentation","J. Lu; T. Liu; M. Luo; H. Cheng; K. Zhang","Xi'an Polytechnic University, People's Republic of China; Xi'an Polytechnic University, People's Republic of China; Xi'an Polytechnic University, People's Republic of China; Xi'an Polytechnic University, People's Republic of China; Xi'an Polytechnic University, People's Republic of China","Electronics Letters","","2019","55","20","1088","1090","It is a challenging task to use deep learning methods to understand point cloud data and assign semantics due to the complexity of point cloud data structure. In this Letter, a fully convolutional network is designed by the authors to perform point cloud semantic segmentation. The proposed network based on PointNet++ takes point cloud data as input and predicts a semantic label for each point. Their network consists of three parts. In the first, different scale features are extracted, the second part reduces the extracted features and then fuses them, and in the third part, more structural information of the point cloud is preserved by up-sampling with deconvolution to reconstruct the point cloud. They have carried out part segmentation and semantic segmentation of ShapeNet and S3DIS datasets, respectively, and the validity of the network has been verified.","","","10.1049/el.2019.1757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8855236","","","data structures;feature extraction;image recognition;image representation;image segmentation;learning (artificial intelligence);object recognition;solid modelling","part segmentation;semantic label;point cloud data structure;assign semantics;point cloud semantic segmentation;fully convolutional network","","","9","","","","","IET","IET Journals"
"Traffic Sign Detection Using a Multi-Scale Recurrent Attention Network","Y. Tian; J. Gelernter; X. Wang; J. Li; Y. Yu","School of Computer and Information Engineering, Zhejiang Gongshang University, Hangzhou, China; Information Science Department, Rutgers University, New Brunswick, NJ, USA; School of Computer and Information Engineering, Zhejiang Gongshang University, Hangzhou, China; Enjoyor Co., Ltd., Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Intelligent Transportation Systems","","2019","20","12","4466","4475","Traffic sign detection plays an important role in intelligent transportation systems. But traffic signs are still not well-detected by deep convolution neural network-based methods because the sizes of their feature maps are constrained, and the environmental context information has not been fully exploited by other researchers. What we need is a way to incorporate relevant context detail from the neighboring layers into the detection architecture. We have developed a novel traffic sign detection approach based on recurrent attention for multi-scale analysis and use of local context in the image. Experiments on the German traffic sign detection benchmark and the Tsinghua-Tencent 100K data set demonstrated that our approach obtained an accuracy comparable to the state-of-the-art approaches in traffic sign detection.","","","10.1109/TITS.2018.2886283","National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; Opening Foundation of Engineering Research Center of Intelligent Transport of Zhejiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8599130","Traffic sign detection;intelligent transportation system;deep learning","Feature extraction;Convolution;Object detection;Task analysis;Image color analysis;Image edge detection;Intelligent transportation systems","","","","1","48","IEEE","","","","IEEE","IEEE Journals"
"Convolutional Neural Networks Using Dynamic Functional Connectivity for EEG-Based Person Identification in Diverse Human States","M. Wang; H. El-Fiqi; J. Hu; H. A. Abbass","School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia","IEEE Transactions on Information Forensics and Security","","2019","14","12","3259","3272","Highly secure access control requires Swiss-cheese-type multi-layer security protocols. The use of electroencephalogram (EEG) to provide cognitive indicators for human workload and fatigue has created environments where the EEG data are well-integrated into systems, making it readily available for more forms of innovative uses including biometrics. However, most of the existing studies on EEG biometrics rely on resting state signals or require specific and repetitive sensory stimulation, limiting their uses in naturalistic settings. Moreover, the limited discriminatory power of uni-variate measures denies an opportunity to use dependences information inherent in brain regions to design more robust biometric identifiers. In this paper, we proposed a novel model for ongoing EEG biometric identification using EEG collected during a diverse set of tasks. The novelty lies in representing EEG signals as a graph based on within-frequency and cross-frequency functional connectivity estimates, and the use of graph convolutional neural network (GCNN) to automatically capture deep intrinsic structural representations from the EEG graphs for person identification. An extensive investigation was carried out to assess the robustness of the method against diverse human states, including resting states under eye-open and eye-closed conditions and active states drawn during the performance of four different tasks. We compared our method with the state-of-the-art EEG features, classifiers, and models of EEG biometrics. Results show that the representation drawn from EEG functional connectivity graphs demonstrates more robust biometric traits than direct use of uni-variate features. Moreover, the GCNN can effectively and efficiently capture discriminative traits, thus generalizing better over diverse human states.","","","10.1109/TIFS.2019.2916403","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8716699","EEG;biometrics;person identification;functional connectivity;convolutional neural network;deep learning","Electroencephalography;Biometrics (access control);Task analysis;Feature extraction;Biological system modeling;Brain modeling;Visualization","biometrics (access control);brain;convolutional neural nets;electroencephalography;feature extraction;medical signal processing","graph convolutional neural network;diverse human states;state-of-the-art EEG features;EEG biometrics;EEG functional connectivity graphs;robust biometric traits;dynamic functional connectivity;EEG-based person identification;highly secure access control;Swiss-cheese-type multilayer security protocols;fatigue;repetitive sensory stimulation;robust biometric identifiers;EEG biometric identification;cross-frequency functional connectivity estimation;electroencephalogram;cognitive indicators","","","50","","","","","IEEE","IEEE Journals"
"Bayesian Electromagnetic Spatio-Temporal Imaging of Extended Sources Based on Matrix Factorization","K. Liu; Z. L. Yu; W. Wu; Z. Gu; J. Zhang; L. Cen; S. Nagarajan; Y. Li","Chongqing Key Laboratory of Computational IntelligenceChongqing University of Posts and Telecommunications; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and EngineeringSouth China University of Technology; School of Automation Science and EngineeringSouth China University of Technology; College of Information EngineeringGuangdong University of Technology; School of Electrical and Electronic EngineeringNanyang Technological University; Department of Radiology and Biomedical Imaging, University of California, San Francisco; School of Automation Science and EngineeringSouth China University of Technology","IEEE Transactions on Biomedical Engineering","","2019","66","9","2457","2469","Accurate estimation of the locations and extents of neural sources from electroencephalography and magnetoencephalography (E/MEG) is challenging, especially for deep and highly correlated neural activities. In this study, we proposed a new fully data-driven source imaging method, source imaging based on spatio-temporal basis function (SI-STBF), which is built upon a Bayesian framework, to address this issue. The SI-STBF is based on the factorization of a source matrix as a product of a sparse coding matrix and a temporal basis function (TBF) matrix, which includes a few TBFs. The prior of the TBF is set in the empirical Bayesian manner. Similarly, for the spatial constraint, the SI-STBF assumes the prior covariance of the coding matrix as a weighted sum of several spatial covariance components. Both the TBFs and the coding matrix are learned from E/MEG simultaneously through variational Bayesian inference. To enable inference on high-resolution source space, we derived a scalable algorithm using convex analysis. The performance of the SI-STBF was assessed using both simulated and experimental E/MEG recordings. Compared with L2-norm constrained methods, the SI-STBF is superior in reconstructing extended sources with less spatial diffusion and less localization error. By virtue of the spatio-temporal factorization of source matrix, the SI-STBF also produces more accurate estimations than spatial-only constraint method for high correlated and deep sources.","","","10.1109/TBME.2018.2890291","National Natural Science Foundation of China; Chongqing Research Program of Application Foundation and Advanced Technology; Science and Technology Research Program of Chongqing Municipal Education Commission; Common Key Technology Innovation Special of Key Industries; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598754","EEG/MEG source imaging;matrix factorization;variational bayesian inference;empirical bayesian","Bayes methods;Imaging;Covariance matrices;Brain modeling;Sparse matrices;Image reconstruction;Estimation","Bayes methods;electroencephalography;image reconstruction;magnetoencephalography;matrix decomposition;medical image processing;neurophysiology;spatiotemporal phenomena;variational techniques","L2-norm constrained methods;patial diffusion;localization error;spatial-only constraint method;convex analysis;spatial covariance components;source imaging;magnetoencephalography;electroencephalography;deep sources;high correlated sources;spatio-temporal factorization;high-resolution source space;variational Bayesian inference;empirical Bayesian manner;temporal basis function matrix;sparse coding matrix;source matrix;Bayesian framework;SI-STBF;spatio-temporal basis function;fully data-driven source imaging method;highly correlated neural activities;deep activities;neural sources;matrix factorization;extended sources;Bayesian electromagnetic spatio-temporal imaging","","1","56","Traditional","","","","IEEE","IEEE Journals"
"Jointly Learning of Visual and Auditory: A New Approach for RS Image and Audio Cross-Modal Retrieval","M. Guo; C. Zhou; J. Liu","School of Geography and Ocean Science, Collaborative Innovation Center of South China Sea Studies, Nanjing University, Nanjing, China; School of Geography and Ocean Science, Collaborative Innovation Center of South China Sea Studies, Nanjing University, Nanjing, China; School of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","11","4644","4654","Remote sensing (RS) images are widely used in civilian and military fields. With the highly increasing image data, it has become a challenging issue to achieve fast and efficient RS image retrieval. However, the existing image retrieval methods, text-based or content-based, are still limited in the applications; for example, text input is inefficient, and the sample image for query is often unavailable. It is known that speech is a natural and convenient way of communication. Therefore, a novel speech-image cross-modal retrieval approach, named deep visual-audio network (DVAN), is presented in this article, which can establish the direct relationship between image and speech from paired image-audio data. The model mainly has three parts: 1) Image feature extraction, which is used to extract effective features of RS images; 2) audio feature learning, which is used to recognizing key information from raw data, and AudioNet, as part of DVAN, is proposed to obtain more distinguishing features; 3) multimodal embedding, which is used to learn the direct correlations of two modalities. Experimental results on RS image audio dataset demonstrate that the proposed method is effective and speech-image retrieval is feasible, and it provides a new way for faster and more convenient RS image retrieval.","","","10.1109/JSTARS.2019.2949220","National Key Research and Development Program of China Stem Cell and Translational Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8896932","Convolutional neural network;cross-modal;image retrieval;remote sensing;speech","","","","","","71","IEEE","","","","IEEE","IEEE Journals"
"Single Image Reflection Removal Using Convolutional Neural Networks","Y. Chang; C. Jung","School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Image Processing","","2019","28","4","1954","1966","When people take a picture through glass, the scene behind the glass is often interfered by specular reflection. Due to relatively easy implementation, most studies have tried to recover the transmitted scene from multiple images rather than single image. However, the use of multiple images is not practical for common users in real situations due to the critical shooting conditions. In this paper, we propose single-image reflection removal using convolutional neural networks. We provide a ghosting model that causes reflection effects in captured images. First, we synthesize multiple-reflection images from the input single one based on ghosting model and relative intensity. Then, we construct an end-to-end network that consists of encoder and decoder. To optimize the network parameters, we use a joint training strategy to learn the layer separation knowledge from the synthesized reflection images. For the loss function, we utilize both internal and external losses in optimization. Finally, we apply the proposed network to single-image reflection removal. Compared with the previous work, the proposed method does not need handcrafted features and specular filters for reflection removal. Experimental results show that the proposed method successfully removes reflection from both synthetic and real images as well as achieves the highest scores in peak signal-to-noise ratio, structural similarity, and feature similarity.","","","10.1109/TIP.2018.2880088","National Natural Science Foundation of China; International S&T Cooperation Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8529204","Deep learning;reflection removal;image restoration;convolutional neural networks;layer separation","Glass;Training;Convolutional neural networks;Image edge detection;Mobile handsets;Decoding","convolution;feedforward neural nets;image capture;image resolution;image restoration","single image reflection removal;convolutional neural networks;specular reflection;synthesized reflection images;image capture;ghosting model;end-to-end network;encoder;decoder","","3","42","","","","","IEEE","IEEE Journals"
"CAD-Net: A Context-Aware Detection Network for Objects in Remote Sensing Imagery","G. Zhang; S. Lu; W. Zhang","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Control Science and Engineering, Shandong University, Jinan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","12","10015","10024","Accurate and robust detection of multi-class objects in optical remote sensing images is essential to many real-world applications, such as urban planning, traffic control, searching, and rescuing. However, the state-of-the-art object detection techniques designed for images captured using ground-level sensors usually experience a sharp performance drop when directly applied to remote sensing images, largely due to the object appearance differences in remote sensing images in terms of sparse texture, low contrast, arbitrary orientations, and large-scale variations. This paper presents a novel object detection network [(context-aware detection network (CAD-Net)] that exploits attention-modulated features as well as global and local contexts to address the new challenges in detecting objects from remote sensing images. The proposed CAD-Net learns global and local contexts of objects by capturing their correlations with the global scene (at scene level) and the local neighboring objects or features (at object level), respectively. In addition, it designs a spatial-and-scale-aware attention module that guides the network to focus on more informative regions and features as well as more appropriate feature scales. Experiments over two publicly available object detection data sets for remote sensing images demonstrate that the proposed CAD-Net achieves superior detection performance. The implementation codes will be made publicly available for facilitating future works.","","","10.1109/TGRS.2019.2930982","Nanyang Technological University; National Key Research and Development Plan of China; National Natural Science Foundation of China; Major Research Program of Shandong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8804364","Convolutional neural networks (CNNs);deep learning;object detection;optical remote sensing images","Remote sensing;Object detection;Optical sensors;Optical imaging;Feature extraction;Detectors;Visualization","feature extraction;geophysical image processing;image texture;object detection;remote sensing","attention-modulated features;context-aware detection network;spatial-and-scale-aware attention module;object detection network;object appearance differences;object detection techniques;optical remote sensing images;multiclass object detection;remote sensing imagery;CAD-Net;object level;local neighboring objects","","","56","IEEE","","","","IEEE","IEEE Journals"
"Light Field Image Compression Based on Bi-Level View Compensation With Rate-Distortion Optimization","J. Hou; J. Chen; L. Chau","Department of Computer Science, City University of Hong Kong, Hong Kong; School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","2","517","530","Compared with conventional color images, light field images (LFIs) contain richer scene information, which allows a wide range of interesting applications. However, such additional information is obtained at the cost of generating substantially more data, which poses challenges to both data storage and transmission. In this paper, we propose a new hybrid framework for effective compression of LFIs. The proposed framework takes the particular characteristics of LFIs into account so that the inter- and intra-view correlations of LFIs can be more efficiently exploited to produce better compression performance. Specifically, the proposed scheme partitions sub-aperture images (SAIs) of an LFI into two groups, namely, key SAIs and non-key SAIs. Bi-level view compensation is proposed to exploit the inter-view correlation: first, based on the group of selected key SAIs, learning-based angular super-resolution is performed to compensate non-key SAIs in pixel-wise, during which heterogeneous inter-view correlation between the non-key SAIs is efficiently removed; second, the two groups of SAIs are respectively reorganized as pseudo-sequences, and block-wise motion compensation is carried out with a standard video encoder, during which the homogeneous inter-view correlation is subsequently exploited. The video encoder also helps to remove the intra-view correlation of the SAIs and finally generates the encoded bitstream. Moreover, the bits allocated to each group are optimally determined via model-based rate distortion optimization. Extensive experimental evaluations and comparisons demonstrate the advantage of the proposed framework over existing methods in terms of rate-distortion performance.","","","10.1109/TCSVT.2018.2802943","CityU Start-up Grant for New Faculty; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8283506","Light field;compression;rate distortion optimization;view compensation;disparity;deep learning","Correlation;Image coding;Cameras;Image resolution;Image reconstruction;Rate-distortion;Decoding","data compression;image colour analysis;motion compensation;rate distortion theory;video coding","sub-aperture images;video encoder;pseudo-sequences;encoded bitstream;conventional color images;rate-distortion optimization;Bi-Level View Compensation;light field image compression;rate-distortion performance;model-based rate distortion optimization;homogeneous inter-view correlation;block-wise motion compensation;heterogeneous inter-view correlation;selected key SAIs;compression performance;intra-view correlation;LFIs;hybrid framework;data storage","","7","50","","","","","IEEE","IEEE Journals"
"Adaptive Feature Recombination and Recalibration for Semantic Segmentation With Fully Convolutional Networks","S. Pereira; A. Pinto; J. Amorim; A. Ribeiro; V. Alves; C. A. Silva","CMEMS-UMinho Research Unit, University of Minho, Guimarães, Portugal; CMEMS-UMinho Research Unit, University of Minho, Guimarães, Portugal; CMEMS-UMinho Research Unit, University of Minho, Guimarães, Portugal; CMEMS-UMinho Research Unit, University of Minho, Guimarães, Portugal; Centro Algoritmi, University of Minho, Braga, Portugal; CMEMS-UMinho Research Unit, University of Minho, Guimarães, Portugal","IEEE Transactions on Medical Imaging","","2019","38","12","2914","2925","Fully convolutional networks have been achieving remarkable results in image semantic segmentation, while being efficient. Such efficiency results from the capability of segmenting several voxels in a single forward pass. So, there is a direct spatial correspondence between a unit in a feature map and the voxel in the same location. In a convolutional layer, the kernel spans over all channels and extracts information from them. We observe that linear recombination of feature maps by increasing the number of channels followed by compression may enhance their discriminative power. Moreover, not all feature maps have the same relevance for the classes being predicted. In order to learn the inter-channel relationships and recalibrate the channels to suppress the less relevant ones, squeeze and excitation blocks were proposed in the context of image classification with convolutional neural networks. However, this is not well adapted for segmentation with fully convolutional networks since they segment several objects simultaneously, hence a feature map may contain relevant information only in some locations. In this paper, we propose recombination of features and a spatially adaptive recalibration block that is adapted for semantic segmentation with fully convolutional networks— the SegSE block. Feature maps are recalibrated by considering the cross-channel information together with spatial relevance. The experimental results indicate that recombination and recalibration improve the results of a competitive baseline, and generalize across three different problems: brain tumor segmentation, stroke penumbra estimation, and ischemic stroke lesion outcome prediction. The obtained results are competitive or outperform the state of the art in the three applications.","","","10.1109/TMI.2019.2918096","Fundação para a Ciência e a Tecnologia; COMPETE 2020 with the code and COMPETE:; FCT within the Project Scope:; FCT, Portugal; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718639","Segmentation;deep learning;fully convolutional network;recalibration;recombination;adaptive","Image segmentation;Kernel;Semantics;Adaptive systems;Convolutional neural networks;Medical diagnostic imaging","","","","","43","IEEE","","","","IEEE","IEEE Journals"
"Mining Hard Augmented Samples for Robust Facial Landmark Localization With CNNs","Z. Feng; J. Kittler; X. Wu","Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; Jiangsu Provincial Laboratory of Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China","IEEE Signal Processing Letters","","2019","26","3","450","454","Effective data augmentation is crucial for facial landmark localization with convolutional neural networks (CNNs). In this letter, we investigate different data augmentation techniques that can be used to generate sufficient data for training CNN-based facial landmark localization systems. To the best of our knowledge, this is the first study that provides a systematic analysis of different data augmentation techniques in the area. In addition, an online hard augmented example mining (HAEM) strategy is advocated for further performance boosting. We examine the effectiveness of those techniques using a regression-based CNN architecture. The experimental results obtained on the AFLW and COFW datasets demonstrate the importance of data augmentation and the effectiveness of HAEM. The performance achieved using these techniques is superior to the state-of-the-art algorithms.","","","10.1109/LSP.2019.2895291","Engineering and Physical Sciences Research Council; National Natural Science Foundation of China; NVIDIA GPU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625468","Facial landmark localisation;deep neural networks;data augmentation;hard augmented example mining","Training;Face;Image color analysis;Data mining;Colored noise;Neural networks;Shape","convolutional neural nets;data mining;face recognition;learning (artificial intelligence);regression analysis","online hard augmented example mining strategy;regression-based CNN architecture;robust facial landmark localization;convolutional neural networks;sufficient data;data augmentation techniques;CNN-based facial landmark localization system training;hard augmented sample mining","","","39","","","","","IEEE","IEEE Journals"
"A Modified LSTM Model for Continuous Sign Language Recognition Using Leap Motion","A. Mittal; P. Kumar; P. P. Roy; R. Balasubramanian; B. B. Chaudhuri","Department of Computer Science Engineering, Indian Institute of Technology Roorkee, Roorkee, India; Department of Computer Science Engineering, Indian Institute of Technology Roorkee, Roorkee, India; Department of Computer Science Engineering, Indian Institute of Technology Roorkee, Roorkee, India; Department of Computer Science Engineering, Indian Institute of Technology Roorkee, Roorkee, India; Techno India University, Kolkata, India","IEEE Sensors Journal","","2019","19","16","7056","7063","Sign language facilitates communication between hearing impaired peoples and the rest of the society. A number of sign language recognition (SLR) systems have been developed by researchers, but they are limited to isolated sign gestures only. In this paper, we propose a modified long short-term memory (LSTM) model for continuous sequences of gestures or continuous SLR that recognizes a sequence of connected gestures. It is based on splitting of continuous signs into sub-units and modeling them with neural networks. Thus, the consideration of a different combination of sub-units is not required during training. The proposed system has been tested with 942 signed sentences of Indian Sign Language (ISL). These sign sentences are recognized using 35 different sign words. The average accuracy of 72.3% and 89.5% has been recorded on signed sentences and isolated sign words, respectively.","","","10.1109/JSEN.2019.2909837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8684245","Sign language recognition;depth sensors;deep neural networks;leap motion sensor","Feature extraction;Hidden Markov models;Assistive technology;Gesture recognition;Cameras;Three-dimensional displays;Sensors","handicapped aids;learning (artificial intelligence);natural language processing;recurrent neural nets;sign language recognition","sign sentences;modified LSTM model;leap motion;sign language recognition systems;isolated sign gestures;continuous SLR;connected gestures;continuous signs;Indian Sign Language;sign words;signed sentences;continuous sign language recognition;hearing impaired people communication;modified long short-term memory model;neural networks","","","29","","","","","IEEE","IEEE Journals"
"Multitask Cascade Convolution Neural Networks for Automatic Thyroid Nodule Detection and Recognition","W. Song; S. Li; J. Liu; H. Qin; B. Zhang; S. Zhang; A. Hao","State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; Department of Computer Science, Stony Brook University, Stony Brook, NY, USA; Peking Union Medical College Hospital, Beijing, China; Peking Union Medical College Hospital, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","1215","1224","Thyroid ultrasonography is a widely used clinical technique for nodule diagnosis in thyroid regions. However, it remains difficult to detect and recognize the nodules due to low contrast, high noise, and diverse appearance of nodules. In today's clinical practice, senior doctors could pinpoint nodules by analyzing global context features, local geometry structure, and intensity changes, which would require rich clinical experience accumulated from hundreds and thousands of nodule case studies. To alleviate doctors' tremendous labor in the diagnosis procedure, we advocate a machine learning approach to the detection and recognition tasks in this paper. In particular, we develop a multitask cascade convolution neural network (MC-CNN) framework to exploit the context information of thyroid nodules. It may be noted that our framework is built upon a large number of clinically confirmed thyroid ultrasound images with accurate and detailed ground truth labels. Other key advantages of our framework result from a multitask cascade architecture, two stages of carefully designed deep convolution networks in order to detect and recognize thyroid nodules in a pyramidal fashion, and capturing various intrinsic features in a global-to-local way. Within our framework, the potential regions of interest after initial detection are further fed to the spatial pyramid augmented CNNs to embed multiscale discriminative information for fine-grained thyroid recognition. Experimental results on 4309 clinical ultrasound images have indicated that our MC-CNN is accurate and effective for both thyroid nodules detection and recognition. For the correct diagnosis rate of malignant and benign thyroid nodules, its mean Average Precision (mAP) performance can achieve up to 98.2% accuracy, which outperforms the common CNNs by 5% on average. In addition, we conduct rigorous user studies to confirm that our MC-CNN outperforms experienced doctors, yet only consuming roughly 2% (1/48) of doctors' examination time on average. Therefore, the accuracy and efficiency of our new method exhibit its great potential in clinical applications.","","","10.1109/JBHI.2018.2852718","National Natural Science Foundation of China; Applied Basic Research Program of Qingdao; National Science Foundation of USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8402093","Thyroid nodules;detection;recognition;pyramid convolution neural networks","Feature extraction;Cancer;Ultrasonic imaging;Image recognition;Convolution;Shape;Predictive models","biomedical ultrasonics;convolutional neural nets;feature extraction;image recognition;learning (artificial intelligence);medical image processing","automatic thyroid nodule detection;thyroid ultrasonography;nodule diagnosis;global context features;local geometry structure;multitask cascade convolution neural network framework;MC-CNN;multitask cascade architecture;fine-grained thyroid recognition;malignant thyroid nodules;benign thyroid nodules;clinical applications;clinical technique;ground truth labels;intensity changes;machine learning approach;clinically confirmed thyroid ultrasound images;mean average precision;mAP","","7","30","","","","","IEEE","IEEE Journals"
"Cross-Generation Kinship Verification with Sparse Discriminative Metric","S. Wang; Z. Ding; Y. Fu","Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Department of Computer, Information and Technology, Indiana University-Purdue University Indianapolis, 420 University Blvd Indianapolis, Indianapolis, IN, USA; Department of Electrical and Computer Engineering, College of Computer and Information Science, Northeastern University, Boston, MA, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","11","2783","2790","Kinship verification is a very important technique in many real-world applications, e.g., personal album organization, missing person investigation and forensic analysis. However, it is extremely difficult to verify a family pair with generation gap, e.g., father and son, since there exist both age gap and identity variation. It is essential to well fight off such challenges to achieve promising kinship verification performance. To this end, we propose a towards-young cross-generation model for effective kinship verification by mitigating both age and identity divergences. Specifically, we explore a conditional generative model to bring in an intermediate domain to bridge each pair. Thus, we could extract more effective features through deep architectures with a newly-designed Sparse Discriminative Metric Loss (SDM-Loss), which is exploited to involve the positive and negative information. Experimental results on kinship benchmark demonstrate the superiority of our proposed model by comparing with the state-of-the-art kinship verification methods.","","","10.1109/TPAMI.2018.2861871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424076","Kinship verification;generative adversarial networks;metric learning","Face;Measurement;Task analysis;Aging;Machine learning;Generative adversarial networks;Training","feature extraction;image recognition","personal album organization;forensic analysis;towards-young cross-generation model;conditional generative model;cross-generation kinship verification methods;sparse discriminative metric loss;missing person investigation;SDM-Loss","","1","62","","","","","IEEE","IEEE Journals"
"Human Gaze-Driven Spatial Tasking of an Autonomous MAV","L. Yuan; C. Reardon; G. Warnell; G. Loianno","GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; U.S. Army Research Laboratory, Adelphi, MD, USA; U.S. Army Research Laboratory, Adelphi, MD, USA; Tandon School of Engineering, New York University, Brooklyn, NY, USA","IEEE Robotics and Automation Letters","","2019","4","2","1343","1350","In this letter, we address the problem of providing human-assisted quadrotor navigation using a set of eye tracking glasses. The advent of these devices (i.e., eye tracking glasses, virtual reality tools, etc.) provides the opportunity to create new, noninvasive forms of interaction between humans and robots. We show how a set of glasses equipped with gaze tracker, a camera, and an inertial measurement unit (IMU) can be used to estimate the relative position of the human with respect to a quadrotor, and decouple the gaze direction from the head orientation, which allows the human to spatially task (i.e., send new 3-D navigation waypoints to) the robot in an uninstrumented environment. We decouple the gaze direction from head motion by tracking the human's head orientation using a combination of camera and IMU data. In order to detect the flying robot, we train and use a deep neural network. We experimentally evaluate the proposed approach, and show that our pipeline has the potential to enable gaze-driven autonomy for spatial tasking. The proposed approach can be employed in multiple scenarios including inspection and first response, as well as by people with disabilities that affect their mobility.","","","10.1109/LRA.2019.2895419","Qualcomm Research and the ARL; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8626140","","Task analysis;Glass;Robot sensing systems;Drones;Cameras;Navigation","aircraft navigation;autonomous aerial vehicles;cameras;gaze tracking;helicopters;human-robot interaction;learning (artificial intelligence);neurocontrollers;object detection;object tracking;robot vision","eye tracking glasses;virtual reality tools;robots;gaze tracker;inertial measurement unit;gaze direction;head orientation;navigation waypoints;head motion;IMU data;gaze-driven autonomy;human gaze-driven spatial tasking;autonomous MAV;human-assisted quadrotor navigation;camera;quadrotor;uninstrumented environment;human head orientation tracking;flying robot detection;deep neural network;spatial tasking","","1","43","","","","","IEEE","IEEE Journals"
"Single-Image De-Raining With Feature-Supervised Generative Adversarial Network","P. Xiang; L. Wang; F. Wu; J. Cheng; M. Zhou","Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA","IEEE Signal Processing Letters","","2019","26","5","650","654","De-raining, which aims at rain-steak removal from images, is a practical task in computer vision. However, it is difficult due to its ill-posed nature. In this letter, we propose a deep neural network architecture, feature-supervised generative adversarial network (FS-GAN) for single-image rain removal. Its main idea is to train a generative adversarial network (GAN) for which the supervision from ground truth is imposed on different layers of the generator network. We design a feature-supervised generator, a discriminator, an optimization target, as well as the detailed structure of FS-GAN. Experiments show that the proposed FS-GAN achieves better performance than state-of-the-art de-raining methods on both synthetic and real-world images in terms of quantitative and visual quality.","","","10.1109/LSP.2019.2903874","National Key R&D Program of China; National Natural Science Foundation of China; Shenzhen Technology Project; CAS Key Technology Talent Program; Guangdong Technology Program; Shenzhen Engineering Laboratory for 3D Content Generating Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663315","Rain removal;generative adversarial network;convolutional neural networks","Feature extraction;Training;Generators;Gallium nitride;Generative adversarial networks;Rain;Indexes","computer vision;learning (artificial intelligence);neural nets","Single-Image De-Raining;feature-supervised generative adversarial network;rain-steak removal;deep neural network architecture;FS-GAN;single-image rain removal;generator network;feature-supervised generator;optimization target","","1","30","","","","","IEEE","IEEE Journals"
"Multi-Modality Multi-Task Recurrent Neural Network for Online Action Detection","J. Liu; Y. Li; S. Song; J. Xing; C. Lan; W. Zeng","Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","9","2667","2682","Online action detection is a brand new challenge and plays a critical role in visual surveillance analytics. It goes one step further than a conventional action recognition task, which recognizes human actions from well-segmented clips. Online action detection is desired to identify the action type and localize action positions on the fly from the untrimmed stream data. In this paper, we propose a multi-modality multi-task recurrent neural network, which incorporates both RGB and Skeleton networks. We design different temporal modeling networks to capture specific characteristics from various modalities. Then, a deep long short-term memory subnetwork is utilized effectively to capture the complex long-range temporal dynamics, naturally avoiding the conventional sliding window design and thus ensuring high computational efficiency. Constrained by a multi-task objective function in the training phase, this network achieves superior detection performance and is capable of automatically localizing the start and end points of actions more accurately. Furthermore, embedding subtask of regression provides the ability to forecast the action prior to its occurrence. We evaluate the proposed method and several other methods in action detection and forecasting on the online action detection data set and gaming action data set datasets. Experimental results demonstrate that our model achieves the state-of-the-art performance on both tasks.","","","10.1109/TCSVT.2018.2799968","National Natural Science Foundation of China; Microsoft Research; CCF-Tencent Open Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8274921","Action detection;recurrent neural network;multi-modality;joint classification-regression","Skeleton;Feature extraction;Recurrent neural networks;Hidden Markov models;Task analysis;Streaming media;Forecasting","feature extraction;image motion analysis;image representation;learning (artificial intelligence);object detection;recurrent neural nets;video signal processing","multimodality multitask recurrent neural network;multitask objective function;online action detection data;gaming action data set datasets;human actions;action positions;temporal modeling networks;complex long-range temporal dynamics;visual surveillance analytics;well-segmented clips;untrimmed stream data;RGB networks;skeleton networks;deep long short-term memory subnetwork","","4","85","","","","","IEEE","IEEE Journals"
"Adapting Semantic Segmentation Models for Changes in Illumination and Camera Perspective","W. Zhou; A. Zyner; S. Worrall; E. Nebot","Australian Centre for Field Robotics, University of Sydney, Sydney, NSW, Australia; Australian Centre for Field Robotics, University of Sydney, Sydney, NSW, Australia; Australian Centre for Field Robotics, University of Sydney, Sydney, NSW, Australia; Australian Centre for Field Robotics, University of Sydney, Sydney, NSW, Australia","IEEE Robotics and Automation Letters","","2019","4","2","461","468","Semantic segmentation using deep neural networks has been widely explored to generate high-level contextual information for autonomous vehicles. To acquire a complete 180° semantic understanding of the forward surroundings, we propose to stitch semantic images from multiple cameras with varying orientations. However, previously trained semantic segmentation models showed unacceptable performance after significant changes to the camera orientations and the lighting conditions. To avoid time-consuming hand labeling, we explore and evaluate the use of data augmentation techniques, specifically skew and gamma correction, from a practical real-world standpoint to extend the existing model and provide more robust performance. The experimental results presented have shown significant improvements with varying illumination and camera perspective changes. A comparison of the results from a high-performance network (PSPNet), and a realtime capable network (ENet) is provided.","","","10.1109/LRA.2019.2891027","Australian Centre for Field Robotics; Australian Research Council Discovery; University of Michigan/Ford Motors Company Contract; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603770","Computer vision for transportation;intelligent transportation systems","Cameras;Semantics;Image segmentation;Lighting;Robot vision systems;Real-time systems","cameras;image segmentation;learning (artificial intelligence);neural nets","high-performance network;illumination;deep neural networks;autonomous vehicles;forward surroundings;multiple cameras;trained semantic segmentation models;camera orientations;lighting conditions;data augmentation techniques;high-level contextual information generation;semantic image stitching;gamma correction;skew correction;PSPNet;real-time capable network;ENet;hand labeling","","2","37","","","","","IEEE","IEEE Journals"
"Reliability verification-based convolutional neural networks for object tracking","X. Hu; J. Li; Y. Yang; F. Wang","Dalian University of Technology, People's Republic of China; Dalian University of Technology, People's Republic of China; Dalian University of Technology, People's Republic of China; Dalian University of Technology, People's Republic of China","IET Image Processing","","2019","13","1","175","185","The authors propose a tracking algorithm based on the reliability analysis of the convolutional neural network to avoid drift. In general, most tracking algorithms implemented with the deep network consist of a single network; they obtain the tracking results according to the confidence and perform updates with the samples, which are collected based on the previous target state. However, this kind of algorithm relies heavily on the accuracy of tracking results, and slight deviations can lead to improperly labelled training samples and degrade the network. Therefore, they design a verification network to guarantee the reliability of the tracking network by correcting the results and it can be connected to a tracking network by sharing convolutional layers. The reliability verification network estimates the accuracy of the results of the tracking network and discards ambiguous results to avoid accumulating errors. Specifically, the verification network can distinguish the target from the confused candidates more precisely because of the optimised training data. The training samples of the verification network consist of characteristics and labels, and they are optimised by feature selection and label enhancement, respectively. The experimental results illustrate the outstanding performance compared with several state-of-the-art methods on the challenging video sequences.","","","10.1049/iet-ipr.2018.5785","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574081","","","feature selection;feedforward neural nets;learning (artificial intelligence);object tracking;reliability","convolutional layers;label enhancement;feature selection;improperly labelled training samples;single network;deep network;reliability analysis;tracking algorithm;object tracking;reliability verification-based convolutional neural networks;tracking network","","","38","","","","","IET","IET Journals"
"Dense small face detection based on regional cascade multi-scale method","X. Ke; J. Li; W. Guo","Fuzhou University, College of Mathematics and Computer Science, Fuzhou 350116, People's Republic of China; Fuzhou University, College of Mathematics and Computer Science, Fuzhou 350116, People's Republic of China; Fuzhou University, College of Mathematics and Computer Science, Fuzhou 350116, People's Republic of China","IET Image Processing","","2019","13","14","2796","2804","In the field of object detection, the research on the problem of detecting small face is the most extensive, but when there are objects with obvious scale differences in the image, the detection performance is not obvious, which is due to the scale invariance properties of the deep convolutional neural networks. Although in recent years, there have been some methods proposed to solve this problem such as FPN and SNIP, which is based on feature pyramid. However, they have not fundamentally solved the problem. A regional cascade multi-scale detection method has been proposed. First, a global detector and several local detectors have been trained, respectively. The global detector is trained by the original training set, while the local detector is trained by the sub-training set generated by the original training set. Second, the global detector can detect object roughly and the local detectors can produce more detailed results that improve the performance of global detector. Finally, to integrate the detection results of global detector and local detectors as the output, non-maximum suppression methods are used. The method can be carried in any depth model of object detection, has good scalability, and is more suitable for dense face detection.","","","10.1049/iet-ipr.2018.6571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946930","","","object detection;learning (artificial intelligence);feature extraction;face recognition;convolutional neural nets","local detector;nonmaximum suppression methods;object detection;dense face detection;regional cascade multiscale method;detection performance;scale invariance properties;deep convolutional neural networks;regional cascade multiscale detection method;global detector;sub-training set","","","34","","","","","IET","IET Journals"
"JigsawNet: Shredded Image Reassembly Using Convolutional Neural Network and Loop-Based Composition","C. Le; X. Li","Louisiana State University, Baton Rouge, LA, USA; School of Electrical Engineering and Computer Science, Louisiana State University, USA","IEEE Transactions on Image Processing","","2019","28","8","4000","4015","This paper proposes a novel algorithm to reassemble an arbitrarily shredded image to its original status. Existing reassembly pipelines commonly consist of a local matching stage and a global compositions stage. In the local stage, a key challenge is to reliably compute correct pairwise matching, for which most existing algorithms use handcrafted features, and cannot reliably handle complicated puzzles. We build a deep convolutional neural network (CNN) to detect the compatibility of pairwise stitching, and use it to prune computed pairwise matches. To improve the network efficiency and accuracy, we transfer the calculation of CNN to the stitching region and apply a boost training strategy. In the global composition stage, instead of using the widely adopted greedy edge selection strategies, we propose two new loop closure-based searching algorithms. Extensive experiments show that our algorithm significantly outperforms existing methods on solving various puzzles, especially challenging ones with many fragment pieces.","","","10.1109/TIP.2019.2903298","National Science Foundation; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661593","Shredded image reassembly;general Jigsaw puzzle solving;convolutional neural network;loop closure constraints","Image color analysis;Convolutional neural networks;Detectors;Reliability;Task analysis;Simultaneous localization and mapping;Feature extraction","graph theory;greedy algorithms;image classification;image matching;learning (artificial intelligence);neural nets;search problems","deep convolutional neural network;CNN;pairwise stitching;computed pairwise matches;network efficiency;stitching region;boost training strategy;global composition stage;shredded image reassembly;loop-based composition;arbitrarily shredded image;original status;local matching stage;global compositions stage;local stage;key challenge;complicated puzzles;loop closure;pairwise matching;greedy edge selection strategies","","","44","","","","","IEEE","IEEE Journals"
"SwapGAN: A Multistage Generative Approach for Person-to-Person Fashion Style Transfer","Y. Liu; W. Chen; L. Liu; M. S. Lew","Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands; Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands; College of System Engineering, National University of Defense Technology, Changsha, China; Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands","IEEE Transactions on Multimedia","","2019","21","9","2209","2222","Fashion style transfer has attracted significant attention because it both has interesting scientific challenges and it is also important to the fashion industry. This paper focuses on addressing a practical problem in fashion style transfer, person-to-person clothing swapping, which aims to visualize what the person would look like with the target clothes worn on another person instead of dressing them physically. This problem remains challenging due to varying pose deformations between different person images. In contrast to traditional nonparametric methods that blend or warp the target clothes for the reference person, in this paper we propose a multistage deep generative approach named SwapGAN that exploits three generators and one discriminator in a unified framework to fulfill the task end-to-end. The first and second generators are conditioned on a human pose map and a segmentation map, respectively, so that we can simultaneously transfer the pose style and the clothes style. In addition, the third generator is used to preserve the human body shape during the image synthesis process. The discriminator needs to distinguish two fake image pairs from the real image pair. The entire SwapGAN is trained by integrating the adversarial loss and the mask-consistency loss. The experimental results on the DeepFashion dataset demonstrate the improvements of SwapGAN over other existing approaches through both quantitative and qualitative evaluations. Moreover, we conduct ablation studies on SwapGAN and provide a detailed analysis about its effectiveness.","","","10.1109/TMM.2019.2897897","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636265","","Clothing;Generators;Image generation;Task analysis;Shape;Strain;Image segmentation","clothing;electronic commerce;image segmentation;learning (artificial intelligence);pose estimation;shape recognition","fashion industry;person-to-person clothing swapping;multistage deep generative approach;SwapGAN;pose style;clothes style;person-to-person fashion style transfer;person images;pose deformations;unified framework;human pose map;segmentation map;human body shape preservation;image synthesis;fake image pairs;adversarial loss;mask-consistency loss;DeepFashion dataset","","","51","Traditional","","","","IEEE","IEEE Journals"
"Deeply Supervised Salient Object Detection with Short Connections","Q. Hou; M. Cheng; X. Hu; A. Borji; Z. Tu; P. H. S. Torr","CCCE, Nankai University, Nankai, Qu, China; CCCE, Nankai University, Nankai, Qu, China; CCCE, Nankai University, Nankai, Qu, China; University of Central Florida, Orlando, FL; University of California at San Diego, La Jolla, CA; University of Oxford, Oxford, United Kingdom","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","4","815","828","Recent progress on salient object detection is substantial, benefiting mostly from the explosive development of Convolutional Neural Networks (CNNs). Semantic segmentation and salient object detection algorithms developed lately have been mostly based on Fully Convolutional Neural Networks (FCNs). There is still a large room for improvement over the generic FCN models that do not explicitly deal with the scale-space problem. The Holistically-Nested Edge Detector (HED) provides a skip-layer structure with deep supervision for edge and boundary detection, but the performance gain of HED on saliency detection is not obvious. In this paper, we propose a new salient object detection method by introducing short connections to the skip-layer structures within the HED architecture. Our framework takes full advantage of multi-level and multi-scale features extracted from FCNs, providing more advanced representations at each layer, a property that is critically needed to perform segment detection. Our method produces state-of-the-art results on 5 widely tested salient object detection benchmarks, with advantages in terms of efficiency (0.08 seconds per image), effectiveness, and simplicity over the existing algorithms. Beyond that, we conduct an exhaustive analysis of the role of training data on performance. We provide a training set for future research and fair comparisons.","","","10.1109/TPAMI.2018.2815688","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8315520","Salient object detection;short connection;deeply supervised network;semantic segmentation;edge detection","Object detection;Feature extraction;Image edge detection;Image segmentation;Semantics;Saliency detection;Computer architecture","convolutional neural nets;edge detection;feature extraction;image representation;image segmentation;learning (artificial intelligence);object detection","deep supervision;boundary detection;performance gain;saliency detection;scale-space problem;generic FCN models;supervised salient object detection;segment detection;multiscale features;HED architecture;skip-layer structure;short connections;salient object detection method;salient object detection benchmarks;multilevel features;edge detection;holistically-nested edge detector;semantic segmentation algorithm;fully convolutional neural networks;salient object detection algorithm","","10","65","","","","","IEEE","IEEE Journals"
"Interacting Multiple Model-Based Human Pose Estimation Using a Distributed 3D Camera Network","H. He; G. Liu; X. Zhu; L. He; G. Tian","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China","IEEE Sensors Journal","","2019","19","22","10584","10590","Distributed camera network for human pose estimation can solve the problem of limited view and occlusion of single view, which has great potential for wide area surveillance applications. To fuse different field of view information, we propose a distributed human pose estimation method by combining the interactive multiple model (IMM) algorithm with the distributed information fusion of human skeleton joints. Compared with the state-of-the-art works which often use a single motion model to depict the motion of human skeleton joints, e.g., constant velocity, the novelty of our work is that the maneuvering property of human action is handled by the IMM, i.e., the motion model of human skeleton joints in the filter is approximated using constant velocity, constant acceleration, and Singer motion models. Based on the advantages of IMM algorithm for maneuvering target tracking, our method can not only solve the single-view occlusion problem, but also solve the problem of joint point fluctuation caused by the estimation error of each sensor node after distributed information fusion. The final human action recognition experimental results show that the proposed method can improve the action recognition rate on the datasets captured by Kinect V2. In addition, we built a distributed camera network using embedded machine learning boards, such that deep learning-based human pose estimation methods can be employed in our framework to handle the limitations of original Kinect SDK.","","","10.1109/JSEN.2019.2931603","National Key R&D Program of China; National Natural Science Foundation of China; Shandong University; Shandong University; Natural Science Foundation of Shandong Province; Taishan Scholars Program of Shandong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778777","Human pose estimation;interacting multiple model;distributed camera network;information fusion;human action recognition","Pose estimation;Skeleton;Cameras;Sensors;Target tracking;Three-dimensional displays","cameras;pose estimation;target tracking","singer motion models;deep learning-based human pose estimation methods;estimation error;joint point fluctuation;single-view occlusion problem;IMM algorithm;constant acceleration;constant velocity;single motion model;human skeleton joints;distributed information fusion;interactive multiple model algorithm;estimation method;distributed human;view information;wide area surveillance applications;distributed camera network;distributed 3d camera network;multiple model-based human pose estimation","","","27","","","","","IEEE","IEEE Journals"
"Enhanced Bi-Prediction With Convolutional Neural Network for High-Efficiency Video Coding","Z. Zhao; S. Wang; S. Wang; X. Zhang; S. Ma; J. Yang","Key Laboratory of Mathematics and Its Applications (LMAM), School of Mathematical Sciences, Peking University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong; School of Electronics Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing, China; Viterbi School of Engineering, University of Southern California, Los Angeles, CA, USA; School of Electronics Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing, China; Key Laboratory of Mathematics and Its Applications (LMAM), School of Mathematical Sciences, Peking University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","11","3291","3301","In this paper, we propose an enhanced bi-prediction scheme based on the convolutional neural network (CNN) to improve the rate-distortion performance in video compression. In contrast to the traditional bi-prediction strategy which computes the linear superposition as the predictive signals with pixel-to-pixel correspondence, the proposed scheme employs CNN to directly infer the predictive signals in a data-driven manner. As such, the predicted blocks are fused in a nonlinear fashion to improve the coding performance. Moreover, the patch-to-patch inference strategy with CNN also improves the prediction accuracy since the patch-level information for the prediction of each individual pixel can be exploited. The proposed enhanced bi-prediction scheme is further incorporated into the high-efficiency video coding standard, and the experimental results exhibit a significant performance improvement under different coding configurations.","","","10.1109/TCSVT.2018.2876399","National Natural Science Foundation of China; National Basic Research Program of China (973 Program); Hong Kong RGC Early Career Scheme; City University of Hong Kong; Peking University; Microsoft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493529","Bi-prediction;deep neural network;inter prediction;video coding","Encoding;Video coding;Machine learning;Image restoration;Neural networks;Interpolation;Image coding","convolutional neural nets;data compression;image resolution;video coding","convolutional neural network;high-efficiency video coding;enhanced bi-prediction scheme;CNN;rate-distortion performance;video compression;traditional bi-prediction strategy;predictive signals;pixel-to-pixel correspondence;predicted blocks;coding performance;patch-to-patch inference strategy;prediction accuracy","","","48","","","","","IEEE","IEEE Journals"
"A Convolutional Recurrent Attention Model for Subject-Independent EEG Signal Analysis","D. Zhang; L. Yao; K. Chen; J. Monaghan","School of Computer Science and Engineering, University of New South Wales Sydney, Sydney, NSW, Australia; School of Computer Science and Engineering, University of New South Wales Sydney, Sydney, NSW, Australia; School of Computer Science and Engineering, University of New South Wales Sydney, Sydney, NSW, Australia; Department of Linguistics, Macquarie University, Macquarie Park, NSW, Australia","IEEE Signal Processing Letters","","2019","26","5","715","719","The electroencephalogram (EEG) signal is a medium to realize a brain-computer interface (BCI) system due to its zero clinical risk and portable acquisition devices. Current EEG-based BCI research usually requires a subject-specific adaptation step before a BCI can be employed by a new user. In contrast, the subject-independent scenario, where a well trained model can be directly applied to new users without precalibration, is particularly desired. Considering this critical gap, the focus in this letter is developing an effective EEG signal analysis adaptively applied to subject-independent settings. We present a convolutional recurrent attention model (CRAM) that utilizes a convolutional neural network to encode the high-level representation of EEG signals and a recurrent attention mechanism to explore the temporal dynamics of the EEG signals as well as to focus on the most discriminative temporal periods. Extensive experiments on a benchmark multiclass EEG dataset containing four movement intentions indicate that the proposed model is capable of exploiting the underlying invariant EEG patterns across different subjects and generalizing the patterns to new subjects with better performance than a series of state-of-the-art and baseline approaches by at least eight percentage points. The implementation code is made publicly available.11https://github.com/dalinzhang/CRAM.","","","10.1109/LSP.2019.2906824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675451","EEG;subject-independent;attention model;movement intention;deep learning","Electroencephalography;Brain modeling;Convolution;Feature extraction;Convolutional codes;Encoding;Electrodes","brain-computer interfaces;convolutional neural nets;electroencephalography;medical signal processing;recurrent neural nets;signal representation","zero clinical risk;portable acquisition devices;subject-specific adaptation step;subject-independent scenario;trained model;effective EEG signal analysis;subject-independent settings;convolutional recurrent attention model;convolutional neural network;recurrent attention mechanism;subject-independent EEG signal analysis;electroencephalogram signal;brain-computer interface system;EEG-based BCI research;high-level representation;temporal dynamics;benchmark multiclass EEG dataset;movement intentions;invariant EEG patterns","","1","19","","","","","IEEE","IEEE Journals"
"Video Person Re-Identification for Wide Area Tracking Based on Recurrent Neural Networks","N. McLaughlin; J. M. del Rincon; P. Miller","Centre for Secure Information Technologies, Queen’s University Belfast, Belfast, U.K.; Centre for Secure Information Technologies, Queen’s University Belfast, Belfast, U.K.; Centre for Secure Information Technologies, Queen’s University Belfast, Belfast, U.K.","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","9","2613","2626","In this paper, we propose a video-based person re-identification system for wide area tracking based on a recurrent neural network architecture. Given short video sequences of a person, generated by a tracking algorithm, our video re-identification algorithm links these tracklets in full trajectories across a network of non-overlapping cameras in an open-world scenario. In our system, features are first extracted from each frame using a convolutional neural network. Then, a recurrent layer combines information across time-steps. The features from all time-steps are finally combined using temporal pooling to give an overall appearance feature for the complete sequence. Our system is trained to perform re-identification using a Siamese network architecture. Experiments are conducted on the iLIDS-VID and PRID-2011 video re-identification data sets as well as in the DukeMTMC multi-camera tracking data set.","","","10.1109/TCSVT.2017.2736599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8003333","Deep learning;re-identification;recurrent neural networks;wide area tracking","Cameras;Feature extraction;Image color analysis;Neural networks;Optical imaging;Optical network units;Video sequences","cameras;convolutional neural nets;image representation;image sequences;neural nets;recurrent neural nets;video signal processing","convolutional neural network;time-steps;Siamese network architecture;DukeMTMC multicamera tracking data;wide area tracking;recurrent neural networks;video-based person;re-identification system;recurrent neural network architecture;tracking algorithm;nonoverlapping cameras;short video sequences;video person reidentification;video reidentification algorithm;PRID-2011 video reidentification data sets","","2","65","","","","","IEEE","IEEE Journals"
"A Real-Time Convolutional Neural Network for Super-Resolution on FPGA With Applications to 4K UHD 60 fps Video Services","Y. Kim; J. Choi; M. Kim","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","8","2521","2534","In this paper, we present a novel hardware-friendly super-resolution (SR) method based on a convolutional neural network (CNN) and its dedicated hardware (HW) on field programmable gate array (FPGA). Although CNN-based SR methods have shown very promising results for SR, their computational complexities are prohibitive for hardware implementation. To the best of our knowledge, we are the first to implement a real-time CNN-based SR HW that upscales 2K full high-definition video to 4K ultra high-definition (UHD) video at 60 frames per second (fps). In our dedicated CNN-based SR HW, low-resolution input frames are processed line-by-line, and the number of convolutional filter parameters is reduced significantly by incorporating depth-wise separable convolutions with a residual connection. Our CNN-based SR HW incorporates a cascade of 1D convolutions having large receptive fields along horizontal lines while keeping vertical receptive fields minimal, which allows us to save required line memory space in achieving comparable SR performance against full 2D convolution operations. For efficient HW implementation, we use a simple and effective quantization method with little peak signal-to-noise ratio (PSNR) degradation. Also, we propose a compression method to efficiently store intermediate feature map data to reduce the number of line memories used in HW. Our HW implementation on the FPGA generates 4K UHD frames of higher PSNR values at 60 fps and shows better visual quality, compared with conventional CNN-based SR methods that are trained and tested in software.","","","10.1109/TCSVT.2018.2864321","Institute for Information and communications Technology Promotion; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8429522","Super-resolution;4K UHD;deep learning;CNN;real-time;FPGA","Hardware;Quantization (signal);Streaming media;UHDTV;Real-time systems;Field programmable gate arrays;Interpolation","computational complexity;convolutional neural nets;field programmable gate arrays;image resolution;video coding;video signal processing","4K ultra high-definition video;dedicated CNN-based SR HW;low-resolution input frames;line-by-line;convolutional filter parameters;depth-wise separable convolutions;horizontal lines;vertical receptive fields;required line memory space;comparable SR performance;2D convolution operations;efficient HW implementation;effective quantization method;compression method;line memories;FPGA;4K UHD frames;conventional CNN-based SR methods;real-time convolutional neural network;4K UHD 60 fps video services;novel hardware-friendly super-resolution method;field programmable gate array;hardware implementation;real-time CNN-based SR HW","","3","62","","","","","IEEE","IEEE Journals"
"Pan-Sharpening Using an Efficient Bidirectional Pyramid Network","Y. Zhang; C. Liu; M. Sun; Y. Ou","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","8","5549","5563","Pan-sharpening is an important preprocessing step for remote sensing image processing tasks; it fuses a low-resolution multispectral image and a high-resolution (HR) panchromatic (PAN) image to reconstruct a HR multispectral (MS) image. This paper introduces a new end-to-end bidirectional pyramid network for pan-sharpening. The overall structure of the proposed network is a bidirectional pyramid, which permits the network to process MS and PAN images in two separate branches level by level. At each level of the network, spatial details extracted from the PAN image are injected into the upsampled MS image to reconstruct the pan-sharpened image from coarse resolution to fine resolution. Subpixel convolutional layers and the enhanced residual blocks are used to make the network efficient. Comparison of the results obtained with our proposed method and the results using other widely used state-of-the-art approaches confirms that our proposed method outperforms the others in visual appearance and objective indexes.","","","10.1109/TGRS.2019.2900419","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667448","Bidirectional pyramid network (BDPN);deep learning;image fusion;multilevel;pan-sharpening;remote sensing","Image reconstruction;Spatial resolution;Convolution;Sensors;Distortion;Indexes","geophysical image processing;geophysical techniques;image resolution;remote sensing","pan-sharpened image;coarse resolution;fine resolution;network efficient;efficient bidirectional pyramid network;remote sensing image processing tasks;low-resolution multispectral image;high-resolution panchromatic image;HR multispectral image;end-to-end bidirectional pyramid network;separate branches level;PAN image;upsampled MS image;preprocessing step;subpixel convolutional layers","","","52","","","","","IEEE","IEEE Journals"
"PalmNet: Gabor-PCA Convolutional Networks for Touchless Palmprint Recognition","A. Genovese; V. Piuri; K. N. Plataniotis; F. Scotti","Department of Computer Science, Università degli Studi di Milano, Milan, Italy; Department of Computer Science, Università degli Studi di Milano, Milan, Italy; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; Department of Computer Science, Università degli Studi di Milano, Milan, Italy","IEEE Transactions on Information Forensics and Security","","2019","14","12","3160","3174","Touchless palmprint recognition systems enable high-accuracy recognition of individuals through less-constrained and highly usable procedures that do not require the contact of the palm with a surface. To perform this recognition, methods based on local texture descriptors and convolutional neural networks (CNNs) are currently used to extract highly discriminative features while compensating for variations in scale, rotation, and illumination in biometric samples. In particular, the main advantage of CNN-based methods is their ability to adapt to biometric samples captured with heterogeneous devices. However, the current methods rely on either supervised training algorithms, which require class labels (e.g., the identities of the individuals) during the training phase, or filters pretrained on general-purpose databases, which may not be specifically suitable for palmprint data. To achieve a high-recognition accuracy with touchless palmprint samples captured using different devices while neither requiring class labels for training nor using pretrained filters, we introduce PalmNet, which is a novel CNN that uses a newly developed method to tune palmprint-specific filters through an unsupervised procedure based on Gabor responses and principal component analysis (PCA), not requiring class labels during training. PalmNet is a new method of applying Gabor filters in a CNN and is designed to extract highly discriminative palmprint-specific descriptors and to adapt to heterogeneous databases. We validated the innovative PalmNet on several palmprint databases captured using different touchless acquisition procedures and heterogeneous devices, and in all cases, a recognition accuracy greater than that of the current methods in this paper was obtained.","","","10.1109/TIFS.2019.2911165","Italian Ministry of Research (MIUR) through the PRIN COSMOS Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691498","Palmprint;touchless biometrics;less-constrained;CNN;deep learning;PCA;Gabor","Palmprint recognition;Databases;Training;Histograms;Feature extraction;Principal component analysis;Performance evaluation","convolutional neural nets;feature extraction;Gabor filters;palmprint recognition;principal component analysis","highly discriminative palmprint-specific descriptors;heterogeneous databases;palmprint databases;heterogeneous devices;Gabor-PCA convolutional networks;touchless palmprint recognition systems;high-accuracy recognition;local texture descriptors;convolutional neural networks;biometric samples;CNN-based methods;supervised training algorithms;training phase;palmprint data;high-recognition accuracy;touchless palmprint samples;pretrained filters;palmprint-specific filters;unsupervised procedure;Gabor responses;Gabor filters","","4","77","","","","","IEEE","IEEE Journals"
"Multiple Resolution Residually Connected Feature Streams for Automatic Lung Tumor Segmentation From CT Images","J. Jiang; Y. Hu; C. Liu; D. Halpenny; M. D. Hellmann; J. O. Deasy; G. Mageras; H. Veeraraghavan","Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, USA; Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, USA; Department of Nuclear Medicine, National Taiwan University Hospital Yunlin Branch, Douliou, Taiwan; Department of Radiology, Memorial Sloan Kettering Cancer Center, New York, NY, USA; Department of Medical Oncology, Memorial Sloan Kettering Cancer Center, New York, NY, USA; Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, USA; Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, USA; Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, USA","IEEE Transactions on Medical Imaging","","2019","38","1","134","144","Volumetric lung tumor segmentation and accurate longitudinal tracking of tumor volume changes from computed tomography images are essential for monitoring tumor response to therapy. Hence, we developed two multiple resolution residually connected network (MRRN) formulations called incremental-MRRN and dense-MRRN. Our networks simultaneously combine features across multiple image resolution and feature levels through residual connections to detect and segment the lung tumors. We evaluated our method on a total of 1210 non-small cell (NSCLC) lung tumors and nodules from three data sets consisting of 377 tumors from the open-source Cancer Imaging Archive (TCIA), 304 advanced stage NSCLC treated with anti- PD-1 checkpoint immunotherapy from internal institution MSKCC data set, and 529 lung nodules from the Lung Image Database Consortium (LIDC). The algorithm was trained using 377 tumors from the TCIA data set and validated on the MSKCC and tested on LIDC data sets. The segmentation accuracy compared to expert delineations was evaluated by computing the dice similarity coefficient, Hausdorff distances, sensitivity, and precision metrics. Our best performing incremental-MRRN method produced the highest DSC of 0.74 ± 0.13 for TCIA, 0.75±0.12 for MSKCC, and 0.68±0.23 for the LIDC data sets. There was no significant difference in the estimations of volumetric tumor changes computed using the incremental-MRRN method compared with the expert segmentation. In summary, we have developed a multi-scale CNN approach for volumetrically segmenting lung tumors which enables accurate, automated identification of and serial measurement of tumor volumes in the lung.","","","10.1109/TMI.2018.2857800","Breast Cancer Research Foundation; MSK Cancer Center Support; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417454","Deep learning;segmentation;longitudinal;lung cancer;detection","Image resolution;Tumors;Streaming media;Lung;Cancer;Feature extraction;Image segmentation","cancer;cellular biophysics;computerised tomography;feature extraction;image resolution;image segmentation;lung;medical image processing;radiation therapy;tumours","accurate longitudinal tracking;tumor volume changes;computed tomography images;multiple resolution residually connected network formulations;dense-MRRN;multiple image resolution;feature levels;nonsmall cell lung tumors;open-source Cancer Imaging Archive;internal institution MSKCC data;Lung Image Database Consortium;TCIA data;LIDC data sets;segmentation accuracy;volumetric tumor changes;expert segmentation;tumor volumes;tumor response;lung nodules;multiple resolution residually connected feature streams;antiPD-1 checkpoint immunotherapy;incremental-MRRN method;dice similarity coefficient;Hausdorff distances;precision metrics;volumetric lung tumor segmentation;CT images;automatic lung tumor segmentation","","2","39","","","","","IEEE","IEEE Journals"
"A Speech Synthesis Approach for High Quality Speech Separation and Generation","Q. Liu; P. J. Jackson; W. Wang","Centre for Vision, Speech and Signal Processing, University of Surrey, UK; Centre for Vision, Speech and Signal Processing, University of Surrey, UK; Centre for Vision, Speech and Signal Processing, University of Surrey, UK","IEEE Signal Processing Letters","","2019","26","12","1872","1876","We propose a new method for source separation by synthesizing the source from a speech mixture corrupted by various environmental noise. Unlike traditional source separation methods which estimate the source from the mixture as a replica of the original source (e.g. by solving an inverse problem), our proposed method is a synthesis-based approach which aims to generate a new signal (i.e. “fake” source) that sounds similar to the original source. The proposed system has an encoder-decoder topology, where the encoder predicts intermediate-level features from the mixture, i.e. Mel-spectrum of the target source, using a hybrid recurrent and hourglass network, while the decoder is a state-of-the-art WaveNet speech synthesis network conditioned on the Mel-spectrum, which directly generates time-domain samples of the sources. Both objective and subjective evaluations were performed on the synthesized sources, and show great advantages of our proposed method for high-quality speech source separation and generation.","","","10.1109/LSP.2019.2951894","Engineering and Physical Sciences Research Council; BBC Audio Research Partnership; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8892499","Deep learning;speech separation;speech synthesis;WaveNet;hourglass;high quality","Decoding;Time-domain analysis;Feature extraction;Training;Speech synthesis;Source separation;Recurrent neural networks","","","","","24","IEEE","","","","IEEE","IEEE Journals"
"Machine Comprehension of Spoken Content: TOEFL Listening Test and Spoken SQuAD","C. Lee; H. Lee; S. Wu; C. Liu; W. Fang; J. Hsu; B. Tseng","Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Communication, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Graduate Institute of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Engineering, University of Cambridge, Cambridge, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","9","1469","1480","A user can scan through a text easily, but it is not the case for spoken content, because they cannot be directly displayed on-screen. As a result, accessing large collections of spoken content is much more difficult and time-consuming than doing so for the text content. It would therefore be helpful to develop machines that understand spoken content. In this paper, we propose two new tasks for machine comprehension of spoken content. The first is a listening comprehension test for TOEFL, a challenging academic English examination for English learners who are not the native English speakers. We show that the proposed model outperforms the naive approaches and other neural network based models by exploiting the hierarchical structures of natural languages and the selective power of attention mechanism. For the second listening comprehension task - spoken SQuAD - we find that speech recognition errors severely impair machine comprehension; we propose the use of subword units to mitigate the impact of these errors.","","","10.1109/TASLP.2019.2913499","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8700217","Speech question answering;TOEFL;SQuAD;attention model;deep learning","Task analysis;Speech recognition;Knowledge discovery;Videos;Cognition;Speech processing;Visualization","computer aided instruction;natural language processing;neural nets;speech recognition","second listening comprehension task;native English speakers;English learners;academic English examination;TOEFL;spoken SQuAD;text content;listening comprehension task;listening comprehension test;machine comprehension;spoken content","","","63","","","","","IEEE","IEEE Journals"
"Regression-Based Three-Dimensional Pose Estimation for Texture-Less Objects","Y. Liu; L. Zhou; H. Zong; X. Gong; Q. Wu; Q. Liang; J. Wang","College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; National Key Laboratory of Science and Technology, Aerospace Intelligent Control, Beijing, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Avic Xi’an Aircraft Industry (Group) Co. Ltd., Xi’an, China; College of Mechanical and Electrical Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Transactions on Multimedia","","2019","21","11","2776","2789","3-D pose estimation for texture-less objects remains a challenging problem. Previous works either focus on a template matching method to find the nearest template as a candidate, or construct a Hough forest, which utilizes the offset of patches to vote for the object location and pose. By contrast, in this paper, we propose a comprehensive framework to directly regress 3-D poses for the candidates, in which a convolutional neural network-based triplet network is trained to extract discriminating features from the binary images. To make the features suitable for the regression task, a pose-guided method and a regression constraint are employed with the constructed triplet network. We show that the constraint reaches the goal of creating the correlation between the features and 3-D poses. Once the expected features are obtained, the object pose could be efficiently regressed, by training a regression network with a simple structure. For symmetric objects, depth images are treated as an additional channel to feed the triplet network. Experiments on the LineMOD and our own datasets demonstrate our method with high regression precision and efficiency.","","","10.1109/TMM.2019.2913321","National Natural Science Foundation of China; Fundamental Research Funds; Central Universities; Nanjing University of Aeronautics and Astronautics Fundamental Research Funds; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698890","Texture-less objects;pose estimation;triplet network;deep learning;pose regression","Three-dimensional displays;Pose estimation;Feature extraction;Training;Image edge detection;Correlation;Cost function","convolutional neural nets;feature extraction;image matching;pose estimation;regression analysis;stereo image processing;trees (mathematics)","LineMOD;depth images;regression-based 3D pose estimation;binary images;discriminating features;convolutional neural network;object location;Hough forest;template matching method;texture-less objects;symmetric objects;regression network;triplet network;regression constraint;pose-guided method","","","50","Traditional","","","","IEEE","IEEE Journals"
"FaultNet3D: Predicting Fault Probabilities, Strikes, and Dips With a Single Convolutional Neural Network","X. Wu; Y. Shi; S. Fomel; L. Liang; Q. Zhang; A. Z. Yusifov","School of Earth and Space Sciences, University of Science and Technology of China, Hefei, China; Bureau of Economic Geology, The University of Texas at Austin, Austin, TX, USA; Bureau of Economic Geology, The University of Texas at Austin, Austin, TX, USA; Applied Science Group, Microsoft, Redmond, WA, USA; BP America, Inc., Houston, TX, USA; BP America, Inc., Houston, TX, USA","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","11","9138","9155","We simultaneously estimate fault probabilities, strikes, and dips directly from a seismic image by using a single convolutional neural network (CNN). In this method, we assume a local 3-D fault is a plane defined by a single combination of strike and dip angles. We assume the fault strikes and dips, respectively, are in the ranges of [0°, 360°] and [64°, 85°], which are divided into 577 classes corresponding to the situation of no fault and 576 different combinations of strikes and dips. We construct a 7-layer CNN to classify the fault strike and dip in a local seismic cube and obtain the classification probability at the same time. With the fault probability, strike and dip estimated at some seismic pixel, we further compute a fault cube (centered at the pixel) with fault features elongated along the fault plane. By sliding the classification window within a full seismic image, we are able to obtain a lot of overlapping fault cubes which are stacked to compute three full images of enhanced and continuous fault probabilities, strikes, and dips. To train the CNN model, we propose an effective and efficient workflow to automatically create 900 000 synthetic seismic cubes and the corresponding fault class labels. Although trained with only synthetic data sets, our CNN model can be applied to accurately estimate fault probabilities, strikes, and dips within field seismic images that are acquired at totally different surveys. With the estimated three fault images, we further construct fault cells that are represented as small 3-D squares, each square is colored by fault probability and oriented by fault strike and dip. We recursively link the fault cells by following the fault strikes and dips to finally construct fault skins, which are simple linked data structures to represent fault surfaces.","","","10.1109/TGRS.2019.2925003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772081","Convolutional neural network (CNN);deep learning;fault;fault dip;fault probability;fault strike;seismic interpretation","Computational modeling;Fault detection;Convolutional neural networks;Feature extraction;Training;Geometry;Convolution","convolutional neural nets;faulting;geophysical techniques;seismology","single convolutional neural network;seismic image;dip angles;fault strikes;local seismic cube;fault cube;fault features;fault plane;enhanced fault probabilities;continuous fault probabilities;CNN model;field seismic images;fault cells;fault skins;fault surfaces;FaultNet3D;local 3-D fault;seismic pixel;synthetic seismic cubes;fault class labels","","","54","","","","","IEEE","IEEE Journals"
"Hippocampus Analysis by Combination of 3-D DenseNet and Shapes for Alzheimer's Disease Diagnosis","R. Cui; M. Liu","Department of Instrument Science and Engineering, School of EIEE, Shanghai Engineering Research Center for Intelligent Diagnosis and Treatment Instrument, Shanghai Jiao Tong University, Shanghai, China; Department of Instrument Science and Engineering, School of EIEE, Shanghai Engineering Research Center for Intelligent Diagnosis and Treatment Instrument, Shanghai Jiao Tong University, Shanghai, China","IEEE Journal of Biomedical and Health Informatics","","2019","23","5","2099","2107","Hippocampus is one of the first involved regions in Alzheimer's disease (AD) and mild cognitive impairment (MCI), a prodromal stage of AD. Hippocampal atrophy is a validated, easily accessible, and widely used biomarker for AD diagnosis. Most of existing methods compute the shape and volume features for hippocampus analysis using structural magnetic resonance images (MRI). However, the regions adjacent to hippocampus may be relevant to AD, and the visual features of the hippocampal region are important for disease diagnosis. In this paper, we have proposed a new hippocampus analysis method to combine the global and local features of hippocampus by three-dimensional densely connected convolutional networks and shape analysis for AD diagnosis. The proposed method can make use of the local visual and global shape features to enhance the classification. Tissue segmentation and nonlinear registration are not required in the proposed method. Our method is evaluated with the T1-weighted structural MRIs from 811 subjects including 192 AD, 396 MCI (231 stable MCI and 165 progressive MCI), and 223 normal control in Alzheimer's disease neuroimaging initiative database. Experimental results show the proposed method achieves a classification accuracy of 92.29% and area under the ROC curve of 96.95% for AD diagnosis. Results comparison demonstrates the proposed method performs better than other methods.","","","10.1109/JBHI.2018.2882392","National Natural Science Foundation of China; National Key Research and Development Program of China; National Key Basic Research Program of China; SMC Excellent Young Faculty Program of SJTU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540939","Alzheimer's disease;hippocampus;3D densenet;deep learning;structural magnetic resonance image","Hippocampus;Three-dimensional displays;Shape;Magnetic resonance imaging;Diseases;Feature extraction","biological tissues;biomedical MRI;brain;cognition;diseases;image registration;image segmentation;medical image processing;neurophysiology","three-dimensional densely connected convolutional networks;shape analysis;local visual shape features;global shape features;T1-weighted structural MRIs;Alzheimer's disease neuroimaging initiative database;3D DenseNet;biomarker;Alzheimer's disease diagnosis;tissue segmentation;nonlinear registration;hippocampus analysis method;hippocampal region;visual features;structural magnetic resonance images;volume features;AD diagnosis;hippocampal atrophy;prodromal stage;mild cognitive impairment","","","30","Traditional","","","","IEEE","IEEE Journals"
"Full-Waveform Airborne LiDAR Data Classification Using Convolutional Neural Networks","S. Zorzi; E. Maset; A. Fusiello; F. Crosilla","Institute of Computer Graphics and Vision (ICG), Graz University, Graz, Austria; Polytechnic Department of Engineering and Architecture (DPIA), University of Udine, Udine, Italy; Polytechnic Department of Engineering and Architecture (DPIA), University of Udine, Udine, Italy; Polytechnic Department of Engineering and Architecture (DPIA), University of Udine, Udine, Italy","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","10","8255","8261","Point cloud classification is one of the most important and time-consuming stages of airborne LiDAR (Light Detection and Ranging) data processing, playing a key role in the generation of cartographic products. This paper describes an innovative algorithm to perform LiDAR point-cloud classification, which relies on Convolutional Neural Networks (CNNs) and takes advantage of full-waveform data registered by modern laser scanners. The proposed method consists of two steps. First, a simple CNN is used to preprocess each waveform, providing a compact representation of the data. By exploiting the coordinates of the points associated with the waveforms, output vectors generated by the first CNN are then mapped into an image that is subsequently segmented by a Fully Convolutional Network (FCN): a label is assigned to each pixel and, consequently, to the point falling in the pixel. In this way, spatial positions and geometrical relationships between neighboring data are taken into account. These particular architectures allow to accurately identify even challenging classes such as power line and transmission tower.","","","10.1109/TGRS.2019.2919472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8737008","Convolutional Neural Network (CNN);deep learning;full-waveform classification;Light Detection and Ranging (LiDAR)","Laser radar;Surface emitting lasers;Three-dimensional displays;Image segmentation;Feature extraction;Vegetation mapping;Vegetation","cartography;convolutional neural nets;geophysical image processing;image classification;image segmentation;optical radar;optical scanners;remote sensing by laser beam","LiDAR point-cloud classification;Convolutional Neural Networks;modern laser scanners;Fully Convolutional Network;point cloud classification;cartographic products;CNN;full-waveform airborne LiDAR data classification","","","27","","","","","IEEE","IEEE Journals"
"Automatic Graph-Based Modeling of Brain Microvessels Captured With Two-Photon Microscopy","R. Damseh; P. Pouliot; L. Gagnon; S. Sakadzic; D. Boas; F. Cheriet; F. Lesage","Institute of Biomedical Engineering, École Polytechnique de Montréal, Montreal, Canada; Department of Electrical Engineering, École Polytechnique de Montréal, Montreal, Canada; Physics Department, Université Laval, Quebec, QC, Canada; Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA; Neurophotonics Center, Department of Biomedical Engineering, Boston University, Boston, MA, USA; Department of Computer and Software Engineering, École Polytechnique de Montréal, Montreal, QC, Canada; Institute of Biomedical Engineering and the Department of Electrical Engineering, École Polytechnique de Montréal, Montreal, QC, Canada","IEEE Journal of Biomedical and Health Informatics","","2019","23","6","2551","2562","Graph models of cerebral vasculature derived from two-photon microscopy have shown to be relevant to study brain microphysiology. Automatic graphing of these microvessels remain problematic due to the vascular network complexity and two-photon sensitivity limitations with depth. In this paper, we propose a fully automatic processing pipeline to address this issue. The modeling scheme consists of a fully-convolution neural network to segment microvessels, a three-dimensional surface model generator, and a geometry contraction algorithm to produce graphical models with a single connected component. Based on a quantitative assessment using NetMets metrics, at a tolerance of 60 μm, false negative and false positive geometric error 19 rates are 3.8% and 4.2%, respectively, whereas false nega20 tive and false positive topological error rates are 6.1% and 4.5%, respectively. Our qualitative evaluation confirms the efficiency of our scheme in generating useful and accurate graphical models.","","","10.1109/JBHI.2018.2884678","Canadian Institutes of Health Research; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8555544","Cerebral microvasculature;deep learning;convolution neural networks;segmentation;graph;two-photon microscopy","Image segmentation;Convolution;Microscopy;Three-dimensional displays;Brain modeling;Photonics;Solid modeling","blood vessels;brain;graph theory;image enhancement;image segmentation;medical image processing;neural nets","false positive geometric error rates;fully automatic processing pipeline;two-photon sensitivity limitations;vascular network complexity;automatic graphing;brain microphysiology;cerebral vasculature;graph models;two-photon microscopy;brain microvessels;automatic graph-based modeling;accurate graphical models;useful models;false positive topological error rates;false negative error rates;single connected component;geometry contraction algorithm;three-dimensional surface model generator;segment microvessels;fully-convolution neural network;modeling scheme;size 60.0 mum","","","57","Traditional","","","","IEEE","IEEE Journals"
"Three-Stream Network With Bidirectional Self-Attention for Action Recognition in Extreme Low Resolution Videos","D. Purwanto; R. R. A. Pramono; Y. Chen; W. Fang","Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan","IEEE Signal Processing Letters","","2019","26","8","1187","1191","This letter presents a novel three-stream network for action recognition in extreme low resolution (LR) videos. In contrast to the existing networks, the new network uses the trajectory-spatial network, which is robust against visual distortion, instead of the pose information to complement the two-stream network. Also, the new three-stream network is combined with the inflated 3D ConvNet (I3D) model pre-trained on kinetics to produce more discriminative spatio-temporal features in blurred LR videos. Moreover, a bidirectional self-attention network is aggregated with the three-stream network to further manifest various temporal dependence among the spatio-temporal features. A new fusion strategy is devised as well to integrate the information from the three different modalities. Simulations show that the new architecture outperforms the main state-of-the-art extreme LR action recognition methods on the HMDB-51 and IXMAS datasets.","","","10.1109/LSP.2019.2923918","Ministry of Science and Technology, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8741087","Action recognition;low resolution videos;self-attention;trajectory-spatial network;deep learning","Videos;Trajectory;Feature extraction;Training;Three-dimensional displays;Signal resolution;Visualization","feature extraction;image motion analysis;image recognition;image representation;image resolution;neural nets;spatiotemporal phenomena;video signal processing","fusion strategy;inflated 3D ConvNet model;IXMAS dataset;HMDB-51 dataset;LR action recognition methods;extreme low resolution videos;bidirectional self-attention network;blurred LR videos;trajectory-spatial network","","1","32","","","","","IEEE","IEEE Journals"
"How to Assess the Quality of Compressed Surveillance Videos Using Face Recognition","W. Heng; T. Jiang; W. Gao","School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","8","2229","2243","Video surveillance plays an important role in public security. To store the growing volume of surveillance videos, video compression is beneficial for reducing video volume; however, it is simultaneously harmful to the video quality. Video quality assessment (VQA) methods help to achieve a tradeoff between the data volume and perceptual quality of compressed surveillance videos. Generally speaking, surveillance video quality assessment (SVQA) is different from conventional VQA, because surveillance videos are usually used for specific tasks, e.g., pedestrian recognition, rather than for entertainment purposes. Therefore, in this paper, we propose two full-reference SVQA methods based on the concept of quality of recognition. We first design two new tasks, distorted face verification (DFV) and distorted face identification (DFI), based on which we further propose two SVQA methods, DFV-SVQA and DFI-SVQA, and corresponding quality metrics. The core components of the DFV-SVQA and DFI-SVQA methods are feature extractors (a DFV model and a DFI model), which we construct using convolutional-neural-network-based face recognition models. In addition, we construct a real-world surveillance video data set, based on which we analyze how various factors, including the video codec, compression level, face resolution, and light intensity, affect the quality of compressed surveillance videos. We find that, compared with conventional VQA methods, our methods are more effective in measuring the quality of surveillance videos while maintaining an acceptable time efficiency.","","","10.1109/TCSVT.2018.2866701","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Peking University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444458","Surveillance videos;quality assessment;deep learning;face recognition","Face;Surveillance;Task analysis;Face recognition;Feature extraction;Quality assessment;Video recording","convolutional neural nets;data compression;face recognition;feature extraction;image motion analysis;object detection;video codecs;video coding;video signal processing;video surveillance","compressed surveillance videos;video surveillance;video compression;video volume;perceptual quality;surveillance video quality assessment;full-reference SVQA methods;distorted face identification;DFV-SVQA;DFI-SVQA methods;convolutional-neural-network-based face recognition models;real-world surveillance video data;video codec;quality metrics","","","44","","","","","IEEE","IEEE Journals"
"Cross-Resolution Feature Fusion for Fast Hand Detection in Intelligent Homecare Systems","T. Le; S. Huang; D. Jaw","International Graduate Program in Electrical Engineering and Computer Science, National Taipei University of Technology, Taipei, Taiwan; Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","IEEE Sensors Journal","","2019","19","12","4696","4704","Fast hand detection plays an important role in Intelligent Homecare systems due to its close association with many human-related tasks, such as human activity and behavior recognition. In the last decade, although hand detection has been studied widely, fast hand detection has remained a challenge. In this paper, we employ a single shot multibox detector (SSD) as the base architecture and propose a novel cross-resolution feature fusion (CFF) approach to add contextual information and semantic information to shallower layers for fast hand detection. Our approach helps improve performance significantly, especially in small instances, owing to the inclusion of two important modules: a narrow atrous spatial pyramid pooling (N-ASPP) module and a richer semantic information generation (RSIG) module. The proposed N-ASPP module employs atrous convolution to capture multiscale context information by adopting different atrous rates. The proposed RSIG module uses a resolution-matching submodule to enlarge an input feature map and a ResNeXt block to exploit richer semantic information. In verification experiments, the proposed 2CFF-SSD model achieved 66.41% average precision (AP) in the Oxford hand test set conducted at 55 frames per second on a GTX 1080 Ti graphics processing unit, which is superior to the original SSD method and many other state-of-the-art methods in terms of accuracy, while maintaining the high speed.","","","10.1109/JSEN.2019.2901259","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8651316","Fast hand detection;deep learning;convolutional neural networks;intelligent homecare system;cross-resolution feature fusion","Convolution;Feature extraction;Semantics;Object detection;Detectors;Spatial resolution","feature extraction;image classification;image fusion;image representation;image resolution;object detection","Intelligent Homecare systems;fast hand detection;narrow atrous spatial pyramid pooling module;cross-resolution feature fusion approach;single shot multibox detector;contextual information;ResNeXt block;2CFF-SSD model;Oxford hand test set;richer semantic information generation module","","","40","","","","","IEEE","IEEE Journals"
"Monocular Camera Based Fruit Counting and Mapping With Semantic Data Association","X. Liu; S. W. Chen; C. Liu; S. S. Shivakumar; J. Das; C. J. Taylor; J. Underwood; V. Kumar","GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; School of Earth and Space Exploration, Arizona State University, Tempe, AZ, USA; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA; Australian Centre for Field Robotics, The University of Sydney, Camperdown, NSW, Australia; GRASP Lab, University of Pennsylvania, Philadelphia, PA, USA","IEEE Robotics and Automation Letters","","2019","4","3","2296","2303","In this letter, we present a cheap, lightweight, and fast fruit counting pipeline. Our pipeline relies only on a monocular camera, and achieves counting performance comparable to a state-of-the-art fruit counting system that utilizes an expensive sensor suite including a monocular camera, LiDAR and GPS/INS on a mango dataset. Our pipeline begins with a fruit and tree trunk detection component that uses state-of-the-art convolutional neural networks (CNNs). It then tracks fruits and tree trunks across images, with a Kalman Filter fusing measurements from the CNN detectors and an optical flow estimator. Finally, fruit count and map are estimated by an efficient fruit-as-feature semantic structure from motion algorithm that converts two-dimensional (2-D) tracks of fruits and trunks into 3-D landmarks, and uses these landmarks to identify double counting scenarios. There are many benefits of developing such a low cost and lightweight fruit counting system, including applicability to agriculture in developing countries, where monetary constraints or unstructured environments necessitate cheaper hardware solutions.","","","10.1109/LRA.2019.2901987","Army Research Laboratory, Distributed and Collaborative Intelligent Systems and Technology Collaborative Research Alliance; Army Research Office; United States Department of Agriculture; National Science Foundation; Australian Centre for Field Robotics; The University of Sydney; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653965","Robotics in agriculture and forestry;deep learning in robotics and automation;visual tracking;mapping;object detection;segmentation and categorization","Cameras;Vegetation;Robot sensing systems;Pipelines;Three-dimensional displays;Semantics;Laser radar","cameras;computer vision;crops;Global Positioning System;image sequences;Kalman filters;object detection;optical radar;sensor fusion","double counting scenarios;lightweight fruit counting system;semantic data association;cheap fruit counting pipeline;fast fruit counting pipeline;monocular camera;state-of-the-art fruit;expensive sensor suite;tree trunk detection component;state-of-the-art convolutional neural networks;tracks fruits;tree trunks;Kalman Filter fusing measurements;optical flow estimator;fruit count;efficient fruit-as-feature semantic structure;LiDAR;GPS-INS;mango dataset;CNN","","1","31","","","","","IEEE","IEEE Journals"
"Video Classification With CNNs: Using the Codec as a Spatio-Temporal Activity Sensor","A. Chadha; A. Abbas; Y. Andreopoulos","Electronic and Electrical Engineering Department, University College London, London, U.K.; Electronic and Electrical Engineering Department, University College London, London, U.K.; Electronic and Electrical Engineering Department, University College London, London, U.K.","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","2","475","485","We investigate video classification via a two-stream convolutional neural network (CNN) design that directly ingests information extracted from compressed video bitstreams. Our approach begins with the observation that all modern video codecs divide the input frames into macroblocks (MBs). We demonstrate that selective access to MB motion vector (MV) information within compressed video bitstreams can also provide for selective, motion-adaptive, MB pixel decoding (a.k.a., MB texture decoding). This in turn allows for the derivation of spatio-temporal video activity regions at extremely high speed in comparison to conventional full-frame decoding followed by optical flow estimation. In order to evaluate the accuracy of a video classification framework based on such activity data, we independently train two CNN architectures on MB texture and MV correspondences and then fuse their scores to derive the final classification of each test video. Evaluation on two standard data sets shows that the proposed approach is competitive with the best two-stream video classification approaches found in the literature. At the same time: 1) a CPU-based realization of our MV extraction is over 977 times faster than GPU-based optical flow methods; 2) selective decoding is up to 12 times faster than full-frame decoding; and 3) our proposed spatial and temporal CNNs perform inference at 5 to 49 times lower cloud computing cost than the fastest methods from the literature.","","","10.1109/TCSVT.2017.2786999","Engineering and Physical Sciences Research Council; Innovate UK; Leverhulme Trust; Royal Commission for the Exhibition of 1851; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8239643","Video coding;classification;deep learning","Computer architecture;Decoding;Three-dimensional displays;Two dimensional displays;Training;Complexity theory","convolutional neural nets;decoding;image classification;image sequences;motion estimation;video codecs;video coding;video signal processing","spatio-temporal activity sensor;two-stream convolutional neural network design;compressed video bitstreams;modern video codecs;selective access;MB motion vector information;selective motion-adaptive;MB pixel decoding;MB texture decoding;spatio-temporal video activity regions;conventional full-frame decoding;optical flow estimation;video classification framework;activity data;final classification;test video;two-stream video classification;MV extraction;GPU-based optical flow methods;temporal CNN;spatial CNN","","1","38","","","","","IEEE","IEEE Journals"
"Image Forgery Detection Based on Motion Blur Estimated Using Convolutional Neural Network","C. Song; P. Zeng; Z. Wang; T. Li; L. Qiao; L. Shen","Key Laboratory of Networked Control Systems, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Key Laboratory of Networked Control Systems, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Key Laboratory of Networked Control Systems, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Liaoning Electric Power Research Institute, State Grid Liaoning Electric Power Co., Ltd., Shenyang, China; State Grid Liaoning Electric Power Co., Ltd., Shenyang, China; State Grid Liaoning Electric Power Co., Ltd., Shenyang, China","IEEE Sensors Journal","","2019","19","23","11601","11611","Currently images are key evidences in many judicial or other identification occasions, and image forgery detection has become a research hotspot. This paper proposes a novel motion blur based image forgery detection method, which includes three steps. First, a convolutional neural network (CNN)-based motion blur kernel reliability estimation method is proposed, which is used to determine whether an image patch should be involved in the image forgery detection process. Second, a shared motion blur kernels-based image tamper detection method is proposed to detect whether a group of motion blur kernels are projected from the same 3D camera trajectory effectively. Third, a consistency propagation method is proposed to localize tampered regions efficiently. Experiments on synthetic images and natural images show the availability of the proposed method.","","","10.1109/JSEN.2019.2928480","National Key R&D Program of China; State Grid Corporation Science and Technology Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8760546","Digital forensics;image tamper detection;motion blur;deep learning","Estimation;Cameras;Forgery;Kernel;Trajectory;Sensors;Reliability","","","","","32","IEEE","","","","IEEE","IEEE Journals"
"Robust Single Accelerometer-Based Activity Recognition Using Modified Recurrence Plot","J. Lu; K. Tong","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Biomedical Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Sensors Journal","","2019","19","15","6317","6324","Using a single 3-axis accelerometer for human activity recognition is challenging but attractive in daily healthcare and monitoring with wearable sensors and devices. In this paper, an effective and efficient framework is proposed to address the recognition problem without any heavy preprocessing. We encode 3-axis signals as 3-channel images using a modified recurrence plot (RP) and train a tiny residual neural network to do image classification. The modified RP is first proposed in our paper to overcome its tendency confusion problem, which has improved our system performance significantly. We evaluate our algorithm on a new database and a public dataset. Results show that our recognition framework achieves highly competitive accuracies and good efficiencies with other state-of-the-art methods on both datasets. Moreover, our method shows stronger robustness to noise and low decimation rate through comparison experiments. Finally, we provide detailed discussion and analysis of our approach from two perspectives: the pattern analysis of encoding algorithm and the interpretation of classification model.","","","10.1109/JSEN.2019.2911204","Chinese University of Hong Kong; Hong Kong Applied Science and Technology Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691521","Accelerometer;activity recognition;deep learning;recurrence plot;signal recognition","Accelerometers;Time series analysis;Correlation;Activity recognition;Hidden Markov models;Encoding","accelerometers;gesture recognition;image classification;neural nets","robust single accelerometer-based activity recognition;modified recurrence plot;3-axis accelerometer;human activity recognition;daily healthcare;wearable sensors;recognition problem;3-axis signals;3-channel images;image classification;modified RP;tendency confusion problem;recognition framework;residual neural network","","","30","","","","","IEEE","IEEE Journals"
"End-to-End DSM Fusion Networks for Semantic Segmentation in High-Resolution Aerial Images","Z. Cao; K. Fu; X. Lu; W. Diao; H. Sun; M. Yan; H. Yu; X. Sun","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","11","1766","1770","Semantic segmentation in high-resolution aerial images is a fundamental research problem in remote sensing field for its wide range of applications. However, it is difficult to distinguish regions with similar spectral features using only multispectral data. Recent research studies have indicated that the introduction of multisource information can effectively improve the robustness of segmentation method. In this letter, we use digital surface models (DSMs) information as a complementary feature to further improve the semantic segmentation results. To this end, we propose a lightweight and simple DSM fusion (DSMF) branch structure module. Compared with the existing feature extraction structures, proposed DSMF module is simple and can be easily applied to other networks. In addition, we investigate four fusion strategies based on DSMF module to explore the optimal feature fusion strategy and four end-to-end DSMFNets are designed according to the corresponding strategies. We evaluate our models on International Society for Photogrammetry and Remote Sensing Vaihingen data set and all DSMFNets achieve promising results. In particular, DSMFNet-1 achieves an overall accuracy of 91.5% on the test data set.","","","10.1109/LGRS.2019.2907009","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8688433","Convolutional neural networks (CNNs);deep learning;high-resolution aerial images;semantic segmentation","Feature extraction;Image segmentation;Semantics;Training;Convolution;Decoding;Fuses","feature extraction;geophysical image processing;image fusion;image segmentation;photogrammetry;remote sensing","fusion networks;high-resolution aerial images;fundamental research problem;remote sensing field;spectral features;multispectral data;multisource information;segmentation method;digital surface models information;complementary feature;semantic segmentation results;feature fusion strategy;end-to-end DSMFNets;Remote Sensing Vaihingen data;end-to-end DSM fusion networks;feature extraction structures;photogrammetry","","","13","","","","","IEEE","IEEE Journals"
"Segmentation of Intra-Retinal Cysts From Optical Coherence Tomography Images Using a Fully Convolutional Neural Network Model","G. N. Girish; B. Thakur; S. R. Chowdhury; A. R. Kothari; J. Rajan","Department of Computer Science and Engineering, National Institute of Technology Karnataka, Surathkal, India; Department of Computer Science and Engineering, National Institute of Technology Karnataka, Surathkal, India; Department of Electrical and Computer Engineering, University of Washington, Bothell, WA, USA; Pink City Eye and Retina Center, Jaipur, India; Department of Computer Science and Engineering, National Institute of Technology Karnataka, Surathkal, India","IEEE Journal of Biomedical and Health Informatics","","2019","23","1","296","304","Optical coherence tomography (OCT) is an imaging modality that is used extensively for ophthalmic diagnosis, near-histological visualization, and quantification of retinal abnormalities such as cysts, exudates, retinal layer disorganization, etc. Intra-retinal cysts (IRCs) occur in several macular disorders such as, diabetic macular edema, retinal vascular disorders, age-related macular degeneration, and inflammatory disorders. Automated segmentation of IRCs poses challenges owing to variations in the acquisition system scan intensities, speckle noise, and imaging artifacts. Several segmentation methods have been proposed in the literature for IRC segmentation on vendor-specific OCT images that lack generalizability across imaging systems. In this paper, we propose a fully convolutional network (FCN) model for vendor-independent IRC segmentation. The proposed method counteracts image noise variabilities and trains FCN models on OCT sub-images from the OPTIMA cyst segmentation challenge dataset (with four different vendor-specific images, namely, Cirrus, Nidek, Spectralis, and Topcon). Further, optimal data augmentation and model hyperparametrization are shown to prevent over-fitting for IRC area segmentation. The proposed method is evaluated on the test dataset with a recall/precision rate of 0.66/0.79 across imaging vendors. The Dice correlation coefficient of the proposed method outperforms that of the published algorithms in the OPTIMA cyst segmentation challenge with a Dice rate of 0.71 across the vendors.","","","10.1109/JBHI.2018.2810379","Science and Engineering Research Board; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8304533","Optical coherence tomography;segmentation;retinal cyst;cystoid macular edema;convolutional neural networks;deep learning","Image segmentation;Retina;Convolution;Informatics;Speckle;Imaging;Data models","biomedical optical imaging;convolutional neural nets;diseases;eye;image denoising;image segmentation;medical image processing;optical tomography;vision defects","imaging artifacts;segmentation methods;vendor-specific OCT images;imaging systems;vendor-independent IRC segmentation;OPTIMA cyst segmentation challenge dataset;optimal data augmentation;model hyperparametrization;IRC area segmentation;imaging vendors;optical coherence tomography images;fully convolutional neural network model;imaging modality;retinal abnormalities;retinal layer disorganization;macular disorders;diabetic macular edema;retinal vascular disorders;age-related macular degeneration;inflammatory disorders;automated segmentation;acquisition system scan intensities;FCN models;intraretinal cysts;ophthalmic diagnosis;near-histological visualization;exudates;speckle noise;image noise variabilities;OCT subimages;Cirrus;Nidek;Spectralis;Topcon;hyperparametrization","","6","42","","","","","IEEE","IEEE Journals"
"A Recurrent CNN for Automatic Detection and Classification of Coronary Artery Plaque and Stenosis in Coronary CT Angiography","M. Zreik; R. W. van Hamersvelt; J. M. Wolterink; T. Leiner; M. A. Viergever; I. Išgum","Image Sciences Institute, University Medical Center Utrecht, Utrecht, GA, The Netherlands; Department of Radiology, University Medical Center Utrecht, Utrecht, GA, The Netherlands; Image Sciences Institute, University Medical Center Utrecht, Utrecht, GA, The Netherlands; Department of Radiology, University Medical Center Utrecht, Utrecht, GA, The Netherlands; Image Sciences Institute, University Medical Center Utrecht, Utrecht, GA, The Netherlands; Image Sciences Institute, University Medical Center Utrecht, Utrecht, GA, The Netherlands","IEEE Transactions on Medical Imaging","","2019","38","7","1588","1598","Various types of atherosclerotic plaque and varying grades of stenosis could lead to different management of patients with a coronary artery disease. Therefore, it is crucial to detect and classify the type of coronary artery plaque, as well as to detect and determine the degree of coronary artery stenosis. This paper includes retrospectively collected clinically obtained coronary CT angiography (CCTA) scans of 163 patients. In these, the centerlines of the coronary arteries were extracted and used to reconstruct multi-planar reformatted (MPR) images for the coronary arteries. To define the reference standard, the presence and the type of plaque in the coronary arteries (no plaque, non-calcified, mixed, calcified), as well as the presence and the anatomical significance of coronary stenosis (no stenosis, non-significant, i.e., <50% luminal narrowing, and significant, i.e., ≥50% luminal narrowing) were manually annotated in the MPR images by identifying the start- and end-points of the segment of the artery affected by the plaque. To perform an automatic analysis, a multi-task recurrent convolutional neural network is applied on coronary artery MPR images. First, a 3D convolutional neural network is utilized to extract features along the coronary artery. Subsequently, the extracted features are aggregated by a recurrent neural network that performs two simultaneous multi-class classification tasks. In the first task, the network detects and characterizes the type of the coronary artery plaque. In the second task, the network detects and determines the anatomical significance of the coronary artery stenosis. The network was trained and tested using the CCTA images of 98 and 65 patients, respectively. For detection and characterization of coronary plaque, the method was achieved an accuracy of 0.77. For detection of stenosis and determination of its anatomical significance, the method was achieved an accuracy of 0.80. The results demonstrate that automatic detection and classification of coronary artery plaque and stenosis are feasible. This may enable automated triage of patients to those without coronary plaque and those with coronary plaque and stenosis in need for further cardiovascular workup.","","","10.1109/TMI.2018.2883807","ZonMw; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550784","Coronary artery stenosis;coronary artery plaque;recurrent convolutional neural network;coronary CT angiography;deep learning;automatic classification","Arteries;Feature extraction;Computed tomography;Task analysis;Visualization;Biomedical imaging;Atherosclerosis","","","","","43","","","","","IEEE","IEEE Journals"
"Three-Dimensional Feature-Enhanced Network for Automatic Femur Segmentation","F. Chen; J. Liu; Z. Zhao; M. Zhu; H. Liao","Department of Biomedical Engineering, School of Medicine, Tsinghua University, Beijing, China; Department of Biomedical Engineering, School of Medicine, Tsinghua University, Beijing, China; Department of Orthopaedics, Beijing Tsinghua Changgung Hospital, Beijing, China; Department of Biomedical Engineering, School of Medicine, Tsinghua University, Beijing, China; Department of Biomedical Engineering, School of Medicine, Tsinghua University, Beijing, China","IEEE Journal of Biomedical and Health Informatics","","2019","23","1","243","252","Automatic femur segmentation from computed tomography volume is a crucial but challenging task for computer-aided diagnosis in orthopedic surgeries. The main obstacles are weak bone boundaries, narrowness of joint space, variations in femur density and shape, as well as diverse leg postures. In this paper, we presented a novel 3-D feature-enhanced network to address these challenges. The novelty of our approach lies in two feature enhancement modules, including the edge detection task and the multi-scale features fusion. First, the edge detection task was embedded into femur segmentation from computed tomography volume to solve the problems of narrow joint space and weak femur boundary. Crucially, a task-specific edge detector was used to optimize the performance of femur segmentation in an end-to-end trainable system. Second, the multi-scale features fusion provided both local and global contexts to handle the problems of large variations in leg postures as well as femur shape and density. The results demonstrated that accurate 3-D femur segmentation with a high Dice similarity coefficient of 96.88% was achieved using the developed method, and the segmentation of computed tomography volume took 0.93 s on an average.","","","10.1109/JBHI.2017.2785389","National Key Research and Development Program of China; National Natural Science Foundation of China; Beijing Municipal Science and Technology Commission; Beijing Municipal Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8231144","Convolutional neural networks;deep learning;femur segmentation;feature enhancement","Image segmentation;Three-dimensional displays;Image edge detection;Feature extraction;Computed tomography;Convolution","bone;computerised tomography;edge detection;feature extraction;image enhancement;image fusion;image segmentation;medical image processing;orthopaedics;shape recognition;surgery","automatic femur segmentation;computed tomography volume;computer-aided diagnosis;weak bone boundaries;femur density;diverse leg postures;feature enhancement modules;edge detection task;multiscale features fusion;weak femur boundary;task-specific edge detector;femur shape;three dimensional feature-enhanced network;orthopedic surgeries;Dice similarity coefficient","","2","39","","","","","IEEE","IEEE Journals"
"Attend and Imagine: Multi-Label Image Classification With Visual Attention and Recurrent Neural Networks","F. Lyu; Q. Wu; F. Hu; Q. Wu; M. Tan","Suzhou University of Science and Technology, Suzhou, China; Australian Centre for Visual Technologies, School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Electronic and Information Engineering, Suzhou University of Science and Technology, Suzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Multimedia","","2019","21","8","1971","1981","Real images often have multiple labels, i.e., each image is associated with multiple objects or attributes. Compared to single-label image classification, the multilabel classification problem is much more challenging due to several issues. At first, multiple objects can be anywhere in the image. Second, the importance of different regions in an image is different, and the regions of interest in a multilabel image can be very different from another one. Finally, multiple labels of an image can have label dependencies due to complex image structures. To address these challenges, in this paper, we propose to predict the labels sequentially by applying the recurrent neural networks (RNNs), which are used to encode the label dependencies. When predicting a specific label, we introduce a dynamic attention mechanism to enable the model to focus on only regions of interest in the image. Two benchmark datasets (i.e., Pascal VOC and MS-COCO) are adopted to demonstrate the effectiveness of our work. Moreover, we construct a new dataset, which includes many semantic dependent labels in each image, to verify the effectiveness of our model. Experimental results show that our method outperforms several state-of-the-arts, especially when predicting some semantic relative labels.","","","10.1109/TMM.2019.2894964","National Natural Science Foundation of China; Primary Research & Development Plan of Jiangsu Province; Fundamental Research Funds for the Central Universities; Program for Guangdong Introducing Innovative and Entrepreneurial Teams; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624407","Multi-label classification;visual attention;deep learning;Convolutional Neural Network (CNN);Recurrent Neural Network (RNN)","Feature extraction;Recurrent neural networks;Correlation;Semantics;Proposals;Visualization;Predictive models","image classification;recurrent neural nets","multilabel image classification;recurrent neural networks;single-label image classification;multilabel classification problem;dynamic attention mechanism;image structures","","1","60","Traditional","","","","IEEE","IEEE Journals"
"3-D Quantification of Filopodia in Motile Cancer Cells","C. Castilla; M. Maška; D. V. Sorokin; E. Meijering; C. Ortiz-de-Solórzano","Center for Applied Medical Research, University of Navarra, Pamplona, Spain; Centre for Biomedical Image Analysis, Faculty of Informatics, Masaryk University, Brno, Czech Republic; Centre for Biomedical Image Analysis, Faculty of Informatics, Masaryk University, Brno, Czech Republic; Departments of Medical Informatics and Radiology, Biomedical Imaging Group Rotterdam, Erasmus University Medical Center, Rotterdam, The Netherlands; Center for Applied Medical Research, University of Navarra, Pamplona, Spain","IEEE Transactions on Medical Imaging","","2019","38","3","862","872","We present a 3D bioimage analysis workflow to quantitatively analyze single, actin-stained cells with filopodial protrusions of diverse structural and temporal attributes, such as number, length, thickness, level of branching, and lifetime, in time-lapse confocal microscopy image data. Our workflow makes use of convolutional neural networks trained using real as well as synthetic image data, to segment the cell volumes with highly heterogeneous fluorescence intensity levels and to detect individual filopodial protrusions, followed by a constrained nearest-neighbor tracking algorithm to obtain valuable information about the spatio-temporal evolution of individual filopodia. We validated the workflow using real and synthetic 3-D time-lapse sequences of lung adenocarcinoma cells of three morphologically distinct filopodial phenotypes and show that it achieves reliable segmentation and tracking performance, providing a robust, reproducible and less time-consuming alternative to manual analysis of the 3D+t image data.","","","10.1109/TMI.2018.2873842","Spanish Ministry of Economy and Competitiveness Competitiveness [BES-2016-076280 to C.C, DPI2015-64221-C2-2 (MINECO/FEDER,UE) to C.O.S]; Grantová Agentura České Republiky; European Cooperation in Science and Technology; Russian Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8482350","Filopodium segmentation and tracking;actin cytoskeleton;confocal microscopy;3D skeletonization;Chan-Vese model;convolutional neural network;deep learning","Image segmentation;Three-dimensional displays;Biomedical imaging;Fluorescence;Manuals;Cancer;Microscopy","biomedical optical imaging;cancer;cell motility;convolutional neural nets;fluorescence;image segmentation;lung;medical image processing;optical microscopy","tracking performance;motile cancer cells;3D bioimage analysis workflow;actin-stained cells;time-lapse confocal microscopy image data;convolutional neural networks;synthetic image data;cell volumes;nearest-neighbor tracking algorithm;lung adenocarcinoma cells;morphologically distinct filopodial phenotypes;segmentation performance;filopodial protrusions;spatiotemporal evolution;heterogeneous fluorescence intensity levels;3D time-lapse sequences","","1","28","","","","","IEEE","IEEE Journals"
"Improving Dermoscopic Image Segmentation With Enhanced Convolutional-Deconvolutional Networks","Y. Yuan; Y. Lo","Department of Radiation Oncology, Icahn School of Medicine at Mount Sinai, New York, NY, USA; Department of Radiation Oncology, Icahn School of Medicine at Mount Sinai, New York, NY, USA","IEEE Journal of Biomedical and Health Informatics","","2019","23","2","519","526","Automatic skin lesion segmentation on dermoscopic images is an essential step in computer-aided diagnosis of melanoma. However, this task is challenging due to significant variations of lesion appearances across different patients. This challenge is further exacerbated when dealing with a large amount of image data. In this paper, we extended our previous work by developing a deeper network architecture with smaller kernels to enhance its discriminant capacity. In addition, we explicitly included color information from multiple color spaces to facilitate network training and thus to further improve the segmentation performance. We participated and extensively evaluated our method on the ISBI 2017 skin lesion segmentation challenge. By training with the 2000 challenge training images, our method achieved an average Jaccard Index of 0.765 on the 600 challenge testing images, which ranked itself in the first place among 21 final submissions in the challenge.","","","10.1109/JBHI.2017.2787487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8239798","Dermoscopic images;deep learning;fully convolutional neural networks;image segmentation;jaccard distance;melanoma","Image segmentation;Lesions;Training;Skin;Image color analysis;Malignant tumors;Biomedical imaging","cancer;convolutional neural nets;image colour analysis;image segmentation;medical image processing;neural net architecture;skin","dermoscopic image segmentation;automatic skin lesion segmentation;dermoscopic images;lesion appearances;deeper network architecture;discriminant capacity;network training;segmentation performance;ISBI 2017 skin lesion segmentation challenge;color spaces;convolutional deconvolutional networks;computer-aided diagnosis;melanoma;network architecture","","4","33","","","","","IEEE","IEEE Journals"
"Dynamic Working Memory for Context-Aware Response Generation","Z. Xu; C. Sun; Y. Long; B. Liu; B. Wang; M. Wang; M. Zhang; X. Wang","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Central South University, Changsha, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Tencent Co., Ltd, Beijing, China; Harbin Institute of Technology, Harbin, China; Research Center for Human Language Technology, School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","9","1419","1431","In human-to-human conversations, the context generally provides several backgrounds and strategic points for the following response. Therefore, many response generation approaches have explored the methodologies to incorporate the context into the encoder-decoder architecture, to generate context-aware responses that are remarkably relevant and cohesive to the given context. However, most approaches pay less attention to semantic interactions implicitly existing within contextual utterances, which are of great importance to capture semantic clues of the given dialog context, indeed. This paper proposes a dynamic working memory mechanism to model long-term semantic hints in the conversation context, by performing semantic interactions between utterances and updating context representation dynamically. Then, the outputs of the dynamic working memory are employed to provide helpful clues for the encoder-decoder architecture to generate responses to the given dialog. We have evaluated the proposed approach on Twitter Customer Service Corpus and OpenSubtitles Corpus, with several automatic evaluation metrics and the human evaluation, and the empirical results show the effectiveness of the proposed method.","","","10.1109/TASLP.2019.2915922","National Natural Science Foundation of China; National High Technology Research and Development Program (863 Program) of China; Shenzhen Basic Research Layout Project, in China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8713487","Response generation;conversation context modeling;conversational agents;deep learning","Semantics;Context modeling;History;Decoding;Memory modules;Memory management;Speech processing","customer services;interactive programming;interactive systems;social networking (online);ubiquitous computing","context-aware response generation;human-to-human conversations;encoder-decoder architecture;semantic interactions;contextual utterances;dynamic working memory mechanism;human evaluation;OpenSubtitles Corpus;Twitter customer service corpus;context representation;dialog context","","","55","","","","","IEEE","IEEE Journals"
"Validating Hyperspectral Image Segmentation","J. Nalepa; M. Myller; M. Kawulok","Silesian University of Technology, Gliwice, Poland; Silesian University of Technology, Gliwice, Poland; Silesian University of Technology, Gliwice, Poland","IEEE Geoscience and Remote Sensing Letters","","2019","16","8","1264","1268","Hyperspectral satellite imaging attracts enormous research attention in the remote sensing community, and hence, automated approaches for precise segmentation of such imagery are being rapidly developed. In this letter, we share our observations on the strategy for validating hyperspectral image segmentation algorithms currently followed in the literature, and show that it can lead to overoptimistic experimental insights. We introduce a new routine for generating segmentation benchmarks and use it to elaborate ready-to-use hyperspectral training-test data partitions. They can be utilized for fair validation of new and existing algorithms without any training-test data leakage.","","","10.1109/LGRS.2019.2895697","European Space Agency; Politechnika lska; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642388","Classification;deep learning (DL);hyperspectral imaging;segmentation;validation","Training;Benchmark testing;Image segmentation;Hyperspectral imaging;Imaging;Data mining","geophysical image processing;hyperspectral imaging;image segmentation;remote sensing","automated approaches;precise segmentation;hyperspectral image segmentation algorithms;overoptimistic experimental insights;segmentation benchmarks;ready-to-use hyperspectral training-test data partitions;fair validation;hyperspectral satellite imaging;remote sensing community;training-test data leakage","","1","23","","","","","IEEE","IEEE Journals"
"Hexagon-Based Convolutional Neural Network for Supply-Demand Forecasting of Ride-Sourcing Services","J. Ke; H. Yang; H. Zheng; X. Chen; Y. Jia; P. Gong; J. Ye","Department of Civil and Environmental Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong; Department of Civil and Environmental Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong; College of Civil Engineering and Architecture, Zhejiang University, Hangzhou, China; College of Civil Engineering and Architecture, Zhejiang University, Hangzhou, China; Didi Research Institute, Didi Chuxing, Beijing, China; Didi Research Institute, Didi Chuxing, Beijing, China; Didi Research Institute, Didi Chuxing, Beijing, China","IEEE Transactions on Intelligent Transportation Systems","","2019","20","11","4160","4173","Ride-sourcing services are becoming an increasingly popular transportation mode in cities all over the world. With real-time information from both drivers and passengers, the ride-sourcing platform can reduce matching frictions and improve efficiencies by surge pricing, optimal vehicle-trip assignment, and proactive ridesplitting strategies. An important foundation of these strategies is the short-term supply-demand forecasting. In this paper, we tackle the problem of predicting the short-term supply-demand gap of ride-sourcing services. In contrast to the previous studies that partitioned a city area into numerous square lattices, we partition the city area into various regular hexagon lattices, which is motivated by the fact that hexagonal segmentation has an unambiguous neighborhood definition, smaller edge-to-area ratio, and isotropy. To capture the spatio-temporal characteristics in a hexagonal manner, we propose three hexagon-based convolutional neural networks (H-CNN), both the input and output of which are numerous local hexagon maps. Moreover, a hexagon-based ensemble mechanism is developed to enhance the prediction performance. Validated by a 3-week real-world ride-sourcing dataset in Guangzhou, China, the H-CNN models are found to significantly outperform the benchmark algorithms in terms of accuracy and robustness. Our approaches can be further extended to a broad range of spatio-temporal forecasting problems in the domain of shared mobility and urban computing.","","","10.1109/TITS.2018.2882861","Hong Kong’s Research Grants Council; Natural Science Foundation of Zhejiang Province; National Natural Science Foundation of China; Northwestern Polytechnical University; Key Research and Development Program of Zhejiang; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8566163","Short-term supply-demand forecasting;deep learning (DL);hexagon-based convolutional neural network (H-CNN);on-demand ride service;ride-sourcing service","Forecasting;Urban areas;Automobiles;Convolutional neural networks;Pricing","convolutional neural nets;demand forecasting;driver information systems;optimisation;pricing;spatiotemporal phenomena;supply and demand","spatio-temporal forecasting problems;hexagon-based convolutional neural network;ride-sourcing services;transportation mode;optimal vehicle-trip assignment;hexagonal segmentation;supply-demand forecasting;pricing;square lattices;isotropy;Guangzhou;China;H-CNN models;urban computing;mobility computing;drivers","","1","46","","","","","IEEE","IEEE Journals"
"Focal Loss in 3D Object Detection","P. Yun; L. Tai; Y. Wang; C. Liu; M. Liu","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; College of Electrical and Information Engineering, Tongji University, Shanghai, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong","IEEE Robotics and Automation Letters","","2019","4","2","1263","1270","3D object detection is still an open problem in autonomous driving scenes. When recognizing and localizing key objects from sparse 3D inputs, autonomous vehicles suffer from a larger continuous searching space and higher fore-background imbalance compared to image-based object detection. In this letter, we aim to solve this fore-background imbalance in 3D object detection. Inspired by the recent use of focal loss in image-based object detection, we extend this hard-mining improvement of binary cross entropy to point-cloud-based object detection and conduct experiments to show its performance based on two different 3D detectors: 3D-FCN and VoxelNet. The evaluation results show up to 11.2AP gains through the focal loss in a wide range of hyperparameters for 3D object detection.","","","10.1109/LRA.2019.2894858","National Natural Science Foundation of China; Shenzhen Science Technology and Innovation Commission; Research Grant Council of Hong Kong SAR Government, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624385","Deep Learning in Robotics and Automation;Object Detection, Segmentation and Categorization;Recognition","Three-dimensional displays;Object detection;Detectors;Feature extraction;Two dimensional displays;Convolution;Entropy","data mining;entropy;object detection","sparse 3D inputs;image-based object detection;3D object detection;focal loss;point-cloud-based object detection;autonomous driving scenes;autonomous vehicles;hard-mining improvement;binary cross entropy;VoxelNet","","2","16","","","","","IEEE","IEEE Journals"
"Magic-Wall: Visualizing Room Decoration by Enhanced Wall Segmentation","T. Liu; Y. Wei; Y. Zhao; S. Liu; S. Wei","Institute of Information Science, Beijing Jiaotong University, Beijing, China; Beckman Institute, University of Illinois at Urbana–Champaign, Champaign, IL, USA; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Image Processing","","2019","28","9","4219","4232","This paper presents an intelligent system named Magic-wall, which enables visualization of the effect of room decoration automatically. Concretely, given an image of the indoor scene and a preferred color, the Magic-wall can automatically locate the wall regions in the image and smoothly replace the existing wall with the required one. The key idea of the proposed Magic-wall is to leverage visual semantics to guide the entire process of color substitution, including wall segmentation and replacement. To strengthen the reality of visualization, we make the following contributions. First, we propose an edge-aware fully convolutional neural network (Edge-aware-FCN) for indoor semantic scene parsing, in which a novel edge-prior branch is introduced to identify the boundary of different semantic regions better. To further polish the details between the wall and other semantic regions, we leverage the output of Edge-aware-FCN as the prior knowledge, concatenating with the image to form a new input for the Enhanced-Net. In such a case, the Enhanced-Net is able to capture more semantic-aware information from the input and polish some ambiguous regions. Finally, to naturally replace the color of the original walls, a simple yet effective color space conversion method is proposed for replacement with brightness reserved. We build a new indoor scene dataset upon ADE20K for training and testing, which includes six semantic labels. Extensive experimental evaluations and visualizations well demonstrate that the proposed Magic-wall is effective and can automatically generate a set of visually pleasing results.","","","10.1109/TIP.2019.2908064","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Program of China Scholarships Council; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675998","Deep learning;scene parsing;edge detection","Conferences;Indexes;Typesetting;Standards;Loading;Portable document format;Web sites","convolutional neural nets;image colour analysis;image segmentation;walls","Magic-wall;wall regions;indoor semantic scene parsing;convolutional neural network;room decoration visualization;enhanced wall segmentation;visual semantics;edge-aware-FCN;intelligent system","","","75","","","","","IEEE","IEEE Journals"
"Vehicle Detection From High-Resolution Remote Sensing Imagery Using Convolutional Capsule Networks","Y. Yu; T. Gu; H. Guan; D. Li; S. Jin","Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China; School of Remote Sensing and Geomatics Engineering, Nanjing University of Information Science and Technology, Nanjing, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China","IEEE Geoscience and Remote Sensing Letters","","2019","16","12","1894","1898","Vehicle detection plays an important role in a variety of traffic-related applications. However, due to the scale and orientation variations and partial occlusions of vehicles, it is still challengeable to accurately detect vehicles from remote sensing images. This letter proposes a convolutional capsule network for detecting vehicles from high-resolution remote sensing images. First, a test image is segmented into superpixels to generate meaningful and nonredundant patches. Then, these patches are input to a convolutional capsule network to label them into vehicles or the background. Finally, nonmaximum suppression is adopted to eliminate repetitive detections. Quantitative evaluations on four test data sets show that average completeness, correctness, quality, and F1-measure of 0.93, 0.97, 0.90, and 0.95, respectively, are obtained. Comparative studies with three existing methods confirm that the proposed method effectively performs in detecting vehicles of various conditions.","","","10.1109/LGRS.2019.2912582","Natural Science Research in Colleges and Universities of Jiangsu Province; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8709752","Convolutional capsule network;deep learning;remote sensing imagery;superpixel segmentation;vehicle detection","Vehicle detection;Remote sensing;Feature extraction;Training;Convolution;Kernel;Monitoring","convolutional neural nets;feature extraction;image resolution;image segmentation;object detection;remote sensing;road traffic;traffic engineering computing","vehicle detection;convolutional capsule network;traffic-related applications;orientation variations;high-resolution remote sensing images;image segmentation;road feature extraction","","","31","IEEE","","","","IEEE","IEEE Journals"
"Radiogenomics for Precision Medicine With a Big Data Analytics Perspective","A. S. Panayides; M. S. Pattichis; S. Leandrou; C. Pitris; A. Constantinidou; C. S. Pattichis","Department of Computer Science, University of Cyprus, Nicosia, Cyprus; Department of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, USA; Department of Health Sciences, European University Cyprus, Nicosia, Cyprus; Department of Electrical and Computer Engineering, University of Cyprus, Nicosia, Cyprus; Medical School, University of Cyprus, Nicosia, Cyprus; Department of Computer Science, University of Cyprus, Nicosia, Cyprus","IEEE Journal of Biomedical and Health Informatics","","2019","23","5","2063","2079","Precision medicine promises better healthcare delivery by improving clinical practice. Using evidence-based substratification of patients, the objective is to achieve better prognosis, diagnosis, and treatment that will transform existing clinical pathways toward optimizing care for the specific needs of each patient. The wealth of today's healthcare data, often characterized as big data, provides invaluable resources toward new knowledge discovery that has the potential to advance precision medicine. The latter requires interdisciplinary efforts that will capitalize the information, know-how, and medical data of newly formed groups fusing different backgrounds and expertise. The objective of this paper is to provide insights with respect to the state-of-the-art research in precision medicine. More specifically, our goal is to highlight the fundamental challenges in emerging fields of radiomics and radiogenomics by reviewing the case studies of Cancer and Alzheimer's disease, describe the computational challenges from a big data analytics perspective, and discuss standardization and open data initiatives that will facilitate the adoption of precision medicine methods and practices.","","","10.1109/JBHI.2018.2879381","Integrated Precision Medicine Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8588324","Precision medicine;quantitative imaging;radiomics;radiogenomics;cancer;Alzheimer;big data;deep learning","Big Data;Biomedical imaging;Genomics;Bioinformatics;Cancer","Big Data;cancer;data analysis;diagnostic radiography;diseases;health care;medical image processing;medical information systems;patient care;standardisation","open data initiatives;precision medicine methods;radiogenomics;healthcare delivery;clinical practice;clinical pathways;healthcare data;medical data;standardization;Big Data analytics;evidence-based substratification;cancer;Alzheimer disease","","","177","CCBY","","","","IEEE","IEEE Journals"
"A Comprehensive Study of the Effect of Spatial Resolution and Color of Digital Images on Vehicle Classification","K. F. Hussain; M. Afifi; G. Moussa","Computer Science Department, Faculty of Computers and Information, Assiut University, Asyut, Egypt; Electrical Engineering and Computer Science Department, Lassonde School of Engineering, York University, Toronto, ON, Canada; Civil Engineering Department, Faculty of Engineering, Assiut University, Asyut, Egypt","IEEE Transactions on Intelligent Transportation Systems","","2019","20","3","1181","1190","Vehicle-type classification is considered a core module for many intelligent transportation applications, such as speed monitoring, smart parking systems, and traffic analysis. In this paper, many vision-based classification techniques were presented relying only on a digital camera without the need for any extra hardware components. Dimension and color are two important characteristics of any digital image that affect the cost of the digital camera used in the image acquisition. In this paper, we present a comprehensive study of the effect of these two characteristics on the vehicle classification process in terms of accuracy and performance. We apply a set of different state-of-the-art image classifiers to the BIT-Vehicle and LabelMe data sets. Each data set is downscaled into different scales to generate a variety of spatial resolutions of each data set. Besides, we examine the effect of color by converting each color version to a gray-scale one. At last, we draw a valid conclusion in regards to the impact of these two characteristics (i.e., dimension and color) on the classification accuracy and performance of the image classification methods using more than 46 000 individual experiments. Experimental results show that there is no significant influence of both color and spatial resolutions of the vehicle images on the classification results obtained by most state-of-the-art image classification methods. However, there is a correlation between the spatial resolution and the processing time required by most image classification methods. Our findings can play an important role in saving not only money, but also time for vehicle-type classification systems.","","","10.1109/TITS.2018.2838117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8372944","Vehicle classification;vision-based classification;computer vision;deep learning","Spatial resolution;Image color analysis;Feature extraction;Visualization;Digital cameras;Digital images;Gray-scale","image classification;image colour analysis;traffic engineering computing","digital camera;image acquisition;vehicle classification process;BIT-Vehicle;LabelMe data sets;spatial resolution;color version;vehicle images;vehicle-type classification systems;intelligent transportation applications;speed monitoring;smart parking systems;vision-based classification techniques;image classifiers;image classification methods;digital image color","","1","43","","","","","IEEE","IEEE Journals"
"Real-Time 3-D Shape Instantiation for Partially Deployed Stent Segments From a Single 2-D Fluoroscopic Image in Fenestrated Endovascular Aortic Repair","J. Zheng; X. Zhou; C. Riga; G. Yang","Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.; Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.; Regional Vascular Unit, St Mary's Hospital, London, U.K.; Hamlyn Centre for Robotic Surgery, Imperial College London, London, U.K.","IEEE Robotics and Automation Letters","","2019","4","4","3703","3710","In fenestrated endovascular aortic repair (FEVAR), accurate alignment of stent graft fenestrations or scallops with aortic branches is essential for establishing complete blood flow perfusion. Current navigation is largely based on two-dimensional (2-D) fluoroscopic images, which lacks 3-D anatomical information, thus causing a longer operation time and high risks of radiation exposure. Previously, 3-D shape instantiation frameworks for realtime 3-D shape reconstruction of fully deployed or fully compressed stent grafts from a single 2-D fluoroscopic image have been proposed for 3-D navigation in FEVAR. However, these methods could not instantiate partially deployed stent segments, as the 3-D marker references are unknown. In this letter, an adapted graph convolutional network (GCN) is proposed to predict 3-D partially deployed marker references from 3-D fully deployed marker references. As the original GCN is for classification, in this letter, the coarsening layers are removed and the softmax function at the network end is replaced with linear mapping for regression. The derived 3-D marker references and the 2-D marker positions are used to instantiate the partially deployed stent segment, combined with the previous 3-D shape instantiation framework. Validations were performed on three typical stent grafts and five patient-specific 3-D printed aortic aneurysm phantoms. Reasonable performances with average mesh distance errors from 1.0 to 2.4 mm and average angular errors around 7.2° were achieved.","","","10.1109/LRA.2019.2928213","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8760435","Deep learning in robotics and automation;computer vision for medical robotics;surgical robotics: planning;visual-based navigation;motion and path planning","Three-dimensional displays;Shape;Two dimensional displays;Image segmentation;Navigation;Convolution;Computed tomography","blood vessels;cardiovascular system;diagnostic radiography;haemodynamics;haemorheology;image reconstruction;medical disorders;medical image processing;phantoms;stents;surgery","3D shape instantiation framework;realtime 3D shape reconstruction;compressed stent grafts;3D partially deployed marker;3D navigation;3D anatomical information;blood flow perfusion;2D fluoroscopic image;fluoroscopic images;aortic branches;scallops;stent graft fenestrations;fenestrated endovascular aortic repair;partially deployed stent segment","","2","17","Traditional","","","","IEEE","IEEE Journals"
"Action Machine: Toward Person-Centric Action Recognition in Videos","J. Zhu; W. Zou; Z. Zhu; L. Xu; G. Huang","Institute of AutomationChinese Academy of Sciences; Institute of AutomationChinese Academy of Sciences; Institute of AutomationChinese Academy of Sciences; Horozon Robotics Co., Ltd, Beijing, China; Horozon Robotics Co., Ltd, Beijing, China","IEEE Signal Processing Letters","","2019","26","11","1633","1637","Existing RGB and CNN-based methods in video action recognition mostly do not distinguish human body from the environment, thus easily overfit the scenes and objects of training sets. In this work, we present a conceptually simple, general and high-performance framework for action recognition in videos, aiming at person-centric modeling. The method, called Action Machine, is based on person bounding boxes for instance-level action analysis. It extends the Inflated 3D ConvNet (I3D) by adding a branch for human pose estimation and a 2D CNN for pose-based action recognition. Action Machine can benefit from the multi-task training of action recognition and pose estimation, the fusion of predictions from RGB images and poses. Experiments results are provided on trimmed video action datasets, NTU RGB+D, Northwestern UCLA Multiview Action3D, MSR Daily Activity3D. Action Machine achieves superior performance and generalizes well across datasets.","","","10.1109/LSP.2019.2942739","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8845617","Video action recognition;deep learning;pose estimation","Pose estimation;Videos;Two dimensional displays;Heating systems;Training;Magnetic heads;Head","convolutional neural nets;feature extraction;image colour analysis;image motion analysis;pose estimation;stereo image processing;video signal processing","video action recognition;person-centric modeling;person bounding boxes;instance-level action analysis;Inflated 3D ConvNet;human pose estimation;pose-based action recognition;person-centric action recognition;Action Machine;2D CNN;RGB images","","","34","Traditional","","","","IEEE","IEEE Journals"
"Intracranial Vessel Wall Segmentation Using Convolutional Neural Networks","F. Shi; Q. Yang; X. Guo; T. A. Qureshi; Z. Tian; H. Miao; D. Dey; D. Li; Z. Fan","Biomedical Imaging Research InstituteCedars-Sinai Medical Center; Biomedical Imaging Research InstituteCedars-Sinai Medical Center; Department of NeurologyXuanwu Hospital; Biomedical Imaging Research InstituteCedars-Sinai Medical Center; Biomedical Imaging Research InstituteCedars-Sinai Medical Center; Department of NeurologyXuanwu Hospital; Biomedical Imaging Research InstituteCedars-Sinai Medical Center; Biomedical Imaging Research InstituteCedars-Sinai Medical Center; Biomedical Imaging Research Institute, Cedars-Sinai Medical Center, Los Angeles, CA, USA","IEEE Transactions on Biomedical Engineering","","2019","66","10","2840","2847","Objective: To develop an automated vessel wall segmentation method using convolutional neural networks to facilitate the quantification on magnetic resonance (MR) vessel wall images of patients with intracranial atherosclerotic disease (ICAD). Methods: Vessel wall images of 56 subjects were acquired with our recently developed whole-brain three-dimensional (3-D) MR vessel wall imaging (VWI) technique. An intracranial vessel analysis (IVA) framework was presented to extract, straighten, and resample the interested vessel segment into 2-D slices. A U-net-like fully convolutional networks (FCN) method was proposed for automated vessel wall segmentation by hierarchical extraction of low- and high-order convolutional features. Results: The network was trained and validated on 1160 slices and tested on 545 slices. The proposed segmentation method demonstrated satisfactory agreement with manual segmentations with Dice coefficient of 0.89 for the lumen and 0.77 for the vessel wall. The method was further applied to a clinical study of additional 12 symptomatic and 12 asymptomatic patients with >50% ICAD stenosis at the middle cerebral artery (MCA). Normalized wall index at the focal MCA ICAD lesions was found significantly larger in symptomatic patients compared to asymptomatic patients. Conclusion: We have presented an automated vessel wall segmentation method based on FCN as well as the IVA framework for 3-D intracranial MR VWI. Significance: This approach would make large-scale quantitative plaque analysis more realistic and promote the adoption of MR VWI in ICAD management.","","","10.1109/TBME.2019.2896972","American Heart Association; National Institutes of Health; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8632719","Vessel wall imaging;deep learning;segmentation;quantification;intracranial atherosclerotic disease","Image segmentation;Three-dimensional displays;Feature extraction;Two dimensional displays;Biomedical imaging;Convolution","biomedical MRI;blood vessels;brain;cardiovascular system;convolutional neural nets;diseases;feature extraction;image segmentation;medical image processing","3D intracranial MR VWI;intracranial atherosclerotic disease;whole-brain three-dimensional MR vessel wall imaging;intracranial vessel analysis;U-net-like fully convolutional network method;Dice coefficient;focal MCA ICAD lesions;normalized wall index;high-order convolutional features;fully convolutional networks method;MR vessel wall imaging technique;magnetic resonance vessel wall images;automated vessel wall segmentation method;convolutional neural networks;intracranial vessel wall segmentation","","","40","Traditional","","","","IEEE","IEEE Journals"
"3D CNN-Based Semantic Labeling Approach for Mobile Laser Scanning Data","B. Nagy; C. Benedek","Machine Perception Research Laboratory, MTA SZTAKI, Budapest, Hungary; Machine Perception Research Laboratory, MTA SZTAKI, Budapest, Hungary","IEEE Sensors Journal","","2019","19","21","10034","10045","In this paper, we introduce a 3D convolutional neural network (CNN)-based method to segment point clouds obtained by mobile laser scanning (MLS) sensors into nine different semantic classes, which can be used for high definition city map generation. The main purpose of semantic point labeling is to provide a detailed and reliable background map for self-driving vehicles (SDV), which indicates the roads and various landmark objects for navigation and decision support of SDVs. Our approach considers several practical aspects of raw MLS sensor data processing, including the presence of diverse urban objects, varying point density, and strong measurement noise of phantom effects caused by objects moving concurrently with the scanning platform. We also provide a new manually annotated MLS benchmark set called SZTAKI CityMLS, which is used to evaluate the proposed approach, and to compare our solution to various reference techniques proposed for semantic point cloud segmentation. Apart from point level validation we also present a case study on Lidar-based accurate self-localization of SDVs in the segmented MLS map.","","","10.1109/JSEN.2019.2927269","Nemzeti Kutatási, Fejlesztési és Innovaciós Alap; Széchenyi 2020 Program; National Excellence Program; Janos Bolyai Research Scholarship of the Hungarian Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8756228","Semantic point cloud segmentation;deep learning;mobile laser scanning","Three-dimensional displays;Sensors;Semantics;Urban areas;Phantoms;Labeling;Roads","convolutional neural nets;image segmentation;image sensors;optical radar;optical scanners","mobile laser scanning data;mobile laser scanning sensors;high definition city map generation;semantic point labeling;self-driving vehicles;landmark objects;diverse urban objects;MLS benchmark;semantic point cloud segmentation;point level validation;segmented MLS map;Lidar-based accurate self-localization;SZTAKI CityMLS;MLS sensor data processing;3D CNN-based semantic labeling;measurement noise;background map;3D convolutional neural network","","","27","CCBY","","","","IEEE","IEEE Journals"
"A Context Encoder For Audio Inpainting","A. Marafioti; N. Perraudin; N. Holighaus; P. Majdak","Acoustics Research Institute, Austrian Academy of Sciences, Vienna, Austria; Swiss Data Science Center, ETH Zürich, Zürich, Switzerland; Acoustics Research Institute, Austrian Academy of Sciences, Vienna, Austria; Acoustics Research Institute, Austrian Academy of Sciences, Vienna, Austria","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","12","2362","2372","In this article, we study the ability of deep neural networks (DNNs) to restore missing audio content based on its context, i.e., inpaint audio gaps. We focus on a condition which has not received much attention yet: gaps in the range of tens of milliseconds. We propose a DNN structure that is provided with the signal surrounding the gap in the form of time-frequency (TF) coefficients. Two DNNs with either complex-valued TF coefficient output or magnitude TF coefficient output were studied by separately training them on inpainting two types of audio signals (music and musical instruments) having 64-ms long gaps. The magnitude DNN outperformed the complex-valued DNN in terms of signal-to-noise ratios and objective difference grades. Although, for instruments, a reference inpainting obtained through linear predictive coding performed better in both metrics, it performed worse than the magnitude DNN for music. This demonstrates the potential of the magnitude DNN, in particular for inpainting signals that are more complex than single instrument sounds.","","","10.1109/TASLP.2019.2947232","Austrian Science Fund (FWF); NVIDIA Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8867915","Music;machine learning;frequency-domain analysis;signal processing algorithms","Instruments;Time-domain analysis;Music;Image reconstruction;Reliability;Prediction algorithms;Psychoacoustic models","audio signal processing;neural nets","context encoder;audio inpainting;deep neural networks;DNN structure;time-frequency coefficients;audio signals;complex-valued TF coefficient output;magnitude TF coefficient output;objective difference grades;linear predictive coding","","","69","IEEE","","","","IEEE","IEEE Journals"
"RTFNet: RGB-Thermal Fusion Network for Semantic Segmentation of Urban Scenes","Y. Sun; W. Zuo; M. Liu","Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong SAR, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong SAR, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong SAR, China","IEEE Robotics and Automation Letters","","2019","4","3","2576","2583","Semantic segmentation is a fundamental capability for autonomous vehicles. With the advancements of deep learning technologies, many effective semantic segmentation networks have been proposed in recent years. However, most of them are designed using RGB images from visible cameras. The quality of RGB images is prone to be degraded under unsatisfied lighting conditions, such as darkness and glares of oncoming headlights, which imposes critical challenges for the networks that use only RGB images. Different from visible cameras, thermal imaging cameras generate images using thermal radiations. They are able to see under various lighting conditions. In order to enable robust and accurate semantic segmentation for autonomous vehicles, we take the advantage of thermal images and fuse both the RGB and thermal information in a novel deep neural network. The main innovation of this letter is the architecture of the proposed network. We adopt the encoder–decoder design concept. ResNet is employed for feature extraction and a new decoder is developed to restore the feature map resolution. The experimental results prove that our network outperforms the state of the arts.","","","10.1109/LRA.2019.2904733","Shenzhen Science, Technology, and Innovation Commission; National Natural Science Foundation of China; Hong Kong University of Science and Technology Project; Hong Kong Research Grant Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666745","Semantic Segmentation;Urban Scenes;Deep Neural Network;Thermal Images;Information Fusion","Decoding;Semantics;Image segmentation;Feature extraction;Image resolution","","","","4","30","","","","","IEEE","IEEE Journals"
"Triple-Frame-Based Bi-Directional Motion Estimation for Motion-Compensated Frame Interpolation","G. Choi; P. Heo; H. Park","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","5","1251","1258","In this paper, new motion estimation (ME) and motion vector refinement (MVR) methods for motion-compensated frame interpolation (MCFI) are proposed. To estimate motion vectors (MVs) with great reliability, triple-frame-based bi-directional ME (TFBME), which employs three sequential frames, is developed. In TFBME, the middle frame works as a reference to estimate MVs between two end frames, which leads to performance improvement especially when objects move linearly through three sequential frames. In addition, a modified bi-directional ME method, which employs only two successive frames, is combined with TFBME to handle cases in which objects move non-linearly through three sequential frames. In our proposed MVR, artifacts on interpolated frames are first detected by a convolutional neural network and then corrected by weighted vector median filtering. Experimental results show that the proposed MCFI method outperforms conventional methods in both subjective and objective evaluations.","","","10.1109/TCSVT.2018.2840842","Institute for Information and communications Technology Promotion; Korea Government (MSIT) (Development of Explainable Human-Level Deep Machine Learning Inference Framework); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8365824","Convolutional neural networks;frame rate up-conversion;motion-compensated frame interpolation;motion estimation;motion vector refinement","Bidirectional control;Reliability;Motion estimation;Interpolation;Filtering;Tracking;Time complexity","convolutional neural nets;interpolation;median filters;motion compensation;motion estimation;video signal processing","motion-compensated frame interpolation;triple-frame-based bi-directional ME;TFBME;triple-frame-based bi-directional motion estimation;motion vector refinement methods;MVR methods;convolutional neural network;MCFI method;weighted vector median filtering","","","20","","","","","IEEE","IEEE Journals"
"Synthesis of Robust Lane Keeping Systems: Impact of Controller and Design Parameters on System Performance","K. Lee; S. E. Li; D. Kum","Vehicular Systems Design and Control Laboratory, Graduate School of Green Transportation, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Automotive Engineering, State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing, China; Vehicular Systems Design and Control Laboratory, Graduate School of Green Transportation, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Intelligent Transportation Systems","","2019","20","8","3129","3141","The lane keeping system (LKS), a promising driver assistance system, is essential for autonomous vehicles. In realworld road conditions, it can be quite challenging because LKS must stay within the lane without causing passenger discomfort while both disturbances (e.g., road curvature, wind gusts, and hydroplaning) and model uncertainties in parameters [e.g., vehicle mass, center of gravity (CG), and tire cornering stiffness] are present. In this paper, the performance limits and tradeoffs between three performance criteria (lane tracking, stability robustness, and passenger comfort) are first investigated by exploring the entire design space of three prominent controllers, i.e., proportional-integral-derivative, linear-quadratic- Gaussian, and H-infinity (H∞). Then, a sensitivity study on the vehicle parameters is conducted in order to investigate the impact of the parameters on the three performance metrics. Based on the aforementioned studies, this paper concludes that a robust controller can provide the maximum performance limit with respect to the lane tracking and stability robustness, when properly designed. However, it is observed that the robust controller is still sensitive to a few design and model parameters, such as look-ahead distance and CG. Therefore, the sensitivity study suggests that for vehicles with excessive mass and CG changes, such as SUVs and trucks, the adaptation of controller and look-ahead distance may be necessary to maximize both tracking performance and passenger comfort over a wide range of vehicle speeds.","","","10.1109/TITS.2018.2873101","Technology Innovation Program; Development of Deep Learning Based Future Prediction and Risk Assessment technology considering Inter-vehicular Interaction in Cut-in Scenario; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8494807","Autonomous vehicle;advanced driver assist system (ADAS);lane keeping system (LKS);robust control;stability robustness;tracking performance","Stability criteria;Roads;Tires;Robustness;Mathematical model;Asymptotic stability","control system synthesis;mobile robots;position control;road traffic control;road vehicles;robust control;three-term control;tyres","robust controller;passenger comfort;design parameters;system performance;LKS;autonomous vehicles;tire cornering stiffness;lane tracking;stability robustness;proportional-integral-derivative;vehicle parameters;performance metrics;driver assistance system;robust lane keeping systems synthesis","","","38","","","","","IEEE","IEEE Journals"
"Infotainment Enabled Smart Cars: A Joint Communication, Caching, and Computation Approach","S. M. A. Kazmi; T. N. Dang; I. Yaqoob; A. Ndikumana; E. Ahmed; R. Hussain; C. S. Hong","Networks and Blockchain Lab, Institute of Information Security and Cyber Physical System, Innopolis University, Innopolis, Russia; Department of Computer Science and Engineering, Kyung Hee University, Seoul, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Seoul, South Korea; Department of Computer Science and Engineering, Kyung Hee University, Seoul, South Korea; Centre for Mobile Cloud Computing Research, University of Malaya, Kuala Lumpur, Malaysia; Networks and Blockchain Lab, Institute of Information Security and Cyber-Physical System, Innopolis University, Innopolis, Russia; Department of Computer Science and Engineering, Kyung Hee University, Seoul, South Korea","IEEE Transactions on Vehicular Technology","","2019","68","9","8408","8420","Remarkable prevalence of cloud computing has enabled smart cars to provide infotainment services. However, retrieving infotainment contents from long-distance data centers poses a significant delay, thus hindering to offer stringent latency-aware infotainment services. Multi-access edge computing is a promising option to meet strict latency requirements. However, it imposes severe resource constraints with respect to caching, and computation. Similarly, communication resources utilized to fetch the infotainment contents are scarce. In this paper, we jointly consider communication, caching, and computation (3C) to reduce infotainment content retrieval delay for smart cars. We formulate the problem as a mix-integer, nonlinear, and nonconvex optimization to minimize the latency. Furthermore, we relax the formulated problem from NP-hard to linear programming. Then, we propose a joint solution (3C) based on the alternative direction method of multipliers technique, which operates in a distributed manner. We compare the proposed 3C solution with various approaches, namely, greedy, random, and centralized. Simulation results reveal that the proposed solution reduces delay up to 9% and 28% compared to the greedy and random approaches, respectively.","","","10.1109/TVT.2019.2930601","Institute of Iunformation and communications Technology Planning and Evaluation (IITP); Korea Government (MSIT); Evolvable Deep Learning Model Generation Platform for Edge Computing; MSIT, Korea; Grand Information Technology Research Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8769920","Smart cars;caching;multi-access edge computing;5G network;vehicular networks","Automobiles;Delays;Data centers;Servers;Edge computing;Optimization;Entertainment industry","cloud computing;computer centres;concave programming;information retrieval;integer programming;linear programming;multi-access systems;nonlinear programming;resource allocation","multiaccess edge computing;communication resources;infotainment content retrieval delay;smart cars;greedy approaches;cloud computing;retrieving infotainment contents;long-distance data centers;latency-aware infotainment services;communication caching and computation;mix-integer optimization;nonlinear optimization;nonconvex optimization;NP-hard problem;linear programming;alternative direction method of multipliers technique","","","45","Traditional","","","","IEEE","IEEE Journals"
"The Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera and LiDAR using Road Information","J. Jeong; Y. Cho; A. Kim","Department of Civil and Environmental Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Civil and Environmental Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Civil and Environmental Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Robotics and Automation Letters","","2019","4","3","2831","2838","This letter presents a framework for the target-less extrinsic calibration of stereo cameras and light detection and ranging (LiDAR) sensors with a non-overlapping field of view (FOV). In order to solve extrinsic calibration problems under such challenging configurations, the proposed solution exploits road markings as static and robust features among the various objects that are present in urban environments. First, this study utilizes road markings that are commonly captured by the two sensor modalities to select informative images for estimating the extrinsic parameters. In order to accomplish stable optimization, multiple cost functions are defined, including normalized information distance (NID), edge alignment, and plane fitting cost. Therefore, a smooth cost curve is formed for global optimization to prevent convergence to the local optimal point. We further evaluate each cost function by examining parameter sensitivity near the optimal point. Another key characteristic of extrinsic calibration, repeatability, is analyzed by conducting the proposed method multiple times with varying randomly perturbed initial points.","","","10.1109/LRA.2019.2921648","Ministry of Land, Infrastructure and Transport; Deep Learning based Camera and LIDAR SLAM project; Naver Labs Corporation; MOLIT via “Innovative Talent Education Program for Smart City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736758","Calibration and identification;sensor fusion;range sensing;SLAM;field robots","Calibration;Laser radar;Roads;Cameras;Optimization;Sensor systems","","","","","20","","","","","IEEE","IEEE Journals"
"Trilateral convolutional neural network for 3D shape reconstruction of objects from a single depth view","P. Rivera; E. Valarezo Añazco; M. Choi; T. Kim","Kyung Hee University, Department of Biomedical Engineering, Yongin, Republic of Korea; Kyung Hee University, Department of Biomedical Engineering, Yongin, Republic of Korea; School of Mechanical Engineering, Sungkyunkwan University, Suwon, Republic of Korea; Kyung Hee University, Department of Biomedical Engineering, Yongin, Republic of Korea","IET Image Processing","","2019","13","13","2457","2466","In this study, the authors propose a novel three-dimensional (3D) convolutional neural network for shape reconstruction via a trilateral convolutional neural network (Tri-CNN) from a single depth view. The proposed approach produces a 3D voxel representation of an object, derived from a partial object surface in a single depth image. The proposed Tri-CNN combines three dilated convolutions in 3D to expand the convolutional receptive field more efficiently to learn shape reconstructions. To evaluate the proposed Tri-CNN in terms of reconstruction performance, the publicly available ShapeNet and Big Data for Grasp Planning data sets are utilised. The reconstruction performance was evaluated against four conventional deep learning approaches: namely, fully connected convolutional neural network, baseline CNN, autoencoder CNN, and a generative adversarial reconstruction network. The proposed experimental results show that Tri-CNN produces superior reconstruction results in terms of intersection over union values and Brier scores with significantly less number of model parameters and memory.","","","10.1049/iet-ipr.2019.0532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911571","","","image recognition;object recognition;medical image processing;image reconstruction;belief networks;neural nets;solid modelling;computer vision;learning (artificial intelligence);image representation","reconstruction performance;fully connected convolutional neural network;baseline CNN;autoencoder CNN;generative adversarial reconstruction network;Tri-CNN;superior reconstruction results;trilateral convolutional neural network;shape reconstruction;single depth view;three-dimensional convolutional neural network;3D voxel representation;partial object surface;single depth image;dilated convolutions;convolutional receptive field","","","29","","","","","IET","IET Journals"
"3D Auto-Context-Based Locality Adaptive Multi-Modality GANs for PET Synthesis","Y. Wang; L. Zhou; B. Yu; L. Wang; C. Zu; D. S. Lalush; W. Lin; X. Wu; J. Zhou; D. Shen","School of Computer Science, Sichuan University, Chengdu, China; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; Joint Department of Biomedical Engineering, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; School of Computer Science, Chengdu University of Information Technology, Chengdu, China; School of Computer Science, Chengdu University of Information Technology, Chengdu, China; Department of Radiology and BRIC, IDEA Lab, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Medical Imaging","","2019","38","6","1328","1339","Positron emission tomography (PET) has been substantially used recently. To minimize the potential health risk caused by the tracer radiation inherent to PET scans, it is of great interest to synthesize the high-quality PET image from the low-dose one to reduce the radiation exposure. In this paper, we propose a 3D auto-context-based locality adaptive multi-modality generative adversarial networks model (LA-GANs) to synthesize the high-quality FDG PET image from the low-dose one with the accompanying MRI images that provide anatomical information. Our work has four contributions. First, different from the traditional methods that treat each image modality as an input channel and apply the same kernel to convolve the whole image, we argue that the contributions of different modalities could vary at different image locations, and therefore a unified kernel for a whole image is not optimal. To address this issue, we propose a locality adaptive strategy for multimodality fusion. Second, we utilize 1 × 1 × 1 kernel to learn this locality adaptive fusion so that the number of additional parameters incurred by our method is kept minimum. Third, the proposed locality adaptive fusion mechanism is learned jointly with the PET image synthesis in a 3D conditional GANs model, which generates high-quality PET images by employing large-sized image patches and hierarchical features. Fourth, we apply the auto-context strategy to our scheme and propose an auto-context LA-GANs model to further refine the quality of synthesized images. Experimental results show that our method outperforms the traditional multi-modality fusion methods used in deep networks, as well as the state-of-the-art PET estimation approaches.","","","10.1109/TMI.2018.2884053","National Natural Science Foundation of China; Australian Research Council; NIH; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8552676","Image synthesis;positron emission topography (PET);generative adversarial networks (GANs);locality adaptive fusion;multi-modality","Positron emission tomography;Generators;Three-dimensional displays;Kernel;Gallium nitride;Image generation;Magnetic resonance imaging","image fusion;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography","LA-GANs model;synthesized images;traditional multimodality fusion methods;state-of-the-art PET estimation approaches;auto-context-based;PET synthesis;positron emission tomography;PET scans;high-quality PET image;high-quality FDG PET image;accompanying MRI images;image modality;locality adaptive strategy;locality adaptive fusion mechanism;PET image synthesis;3D conditional GANs model;large-sized image patches;auto-context strategy;image locations","","1","48","","","","","IEEE","IEEE Journals"
"Robust Visual Tracking via Hierarchical Convolutional Features","C. Ma; J. Huang; X. Yang; M. Yang","MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, P. R. China; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, P. R. China; School of Engineering, University of California, Merced, CA, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","11","2709","2723","Visual tracking is challenging as target objects often undergo significant appearance changes caused by deformation, abrupt motion, background clutter and occlusion. In this paper, we propose to exploit the rich hierarchical features of deep convolutional neural networks to improve the accuracy and robustness of visual tracking. Deep neural networks trained on object recognition datasets consist of multiple convolutional layers. These layers encode target appearance with different levels of abstraction. For example, the outputs of the last convolutional layers encode the semantic information of targets and such representations are invariant to significant appearance variations. However, their spatial resolutions are too coarse to precisely localize the target. In contrast, features from earlier convolutional layers provide more precise localization but are less invariant to appearance changes. We interpret the hierarchical features of convolutional layers as a nonlinear counterpart of an image pyramid representation and explicitly exploit these multiple levels of abstraction to represent target objects. Specifically, we learn adaptive correlation filters on the outputs from each convolutional layer to encode the target appearance. We infer the maximum response of each layer to locate targets in a coarse-to-fine manner. To further handle the issues with scale estimation and re-detecting target objects from tracking failures caused by heavy occlusion or out-of-the-view movement, we conservatively learn another correlation filter, that maintains a long-term memory of target appearance, as a discriminative classifier. We apply the classifier to two types of object proposals: (1) proposals with a small step size and tightly around the estimated location for scale estimation; and (2) proposals with large step size and across the whole image for target re-detection. Extensive experimental results on large-scale benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art tracking methods.","","","10.1109/TPAMI.2018.2865311","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; 111 Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8434334","Hierarchical convolutional features;correlation filters;visual tracking","Target tracking;Visualization;Correlation;Feature extraction;Semantics;Training;Estimation","adaptive filters;convolutional neural nets;image classification;image coding;image filtering;image representation;object detection;object recognition;object tracking","adaptive correlation filters;image pyramid representation;hierarchical convolutional feature layers;layer encode target appearance;tracking failure methods;re-detecting target objects;object recognition datasets;deep convolutional neural networks;occlusion;background clutter;robust visual tracking","","10","80","","","","","IEEE","IEEE Journals"
"Adaptive Hierarchical Energy Management Design for a Plug-In Hybrid Electric Vehicle","T. Liu; X. Tang; H. Wang; H. Yu; X. Hu","Department of Automotive Engineering and the State Key Laboratory of Mechanical Transmission, Chongqing University, Chongqing, China; State Key Laboratory of Mechanical Transmissions, College of Automotive Engineering, Chongqing University, Chongqing, China; Department of Mechanical and Mechatronics Engineering, Waterloo University, Waterloo, ON, Canada; Department of Mechanical and Mechatronics Engineering, Waterloo University, Waterloo, ON, Canada; Department of Automotive Engineering and the State Key Laboratory of Mechanical Transmission, Chongqing University, Chongqing, China","IEEE Transactions on Vehicular Technology","","2019","68","12","11513","11522","To promote the real-time application of the advanced energy management system in hybrid electric vehicles (HEVs), this paper proposes an adaptive hierarchical energy management strategy for a plug-in HEV. In this paper, deep learning (DL) and genetic algorithm (GA) are synthesized to derive the power split controls between the battery and internal combustion engine. First, the architecture of the multimode powertrain is founded, wherein the particular control actions, state variables, and optimization objective are explained. Then, the hierarchical framework for control actions generation is introduced. GA is utilized to search the global optimal controls based on the powertrain model provided in MATLAB/Simulink. DL is applied to train the neural network model that is connecting the inputs and control actions. Finally, the effectiveness of the presented integrated energy management strategy is validated via comparing with the original charge depleting/charge sustaining policy. Simulation results indicate that the proposed technique can highly improve the fuel economy. Furthermore, a hardware-in-the-loop is conducted to evaluate the adaptive and real-time characteristics of the designed energy management system.","","","10.1109/TVT.2019.2926733","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8755512","Chevrolet Volt;hierarchical energy management;deep neural network;genetic algorithm","","","","","","36","IEEE","","","","IEEE","IEEE Journals"
"DCSR: Dilated Convolutions for Single Image Super-Resolution","Z. Zhang; X. Wang; C. Jung","School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Image Processing","","2019","28","4","1625","1635","Dilated convolutions support expanding receptive field without parameter exploration or resolution loss, which turn out to be suitable for pixel-level prediction problems. In this paper, we propose multiscale single image super-resolution (SR) based on dilated convolutions. We adopt dilated convolutions to expand the receptive field size without incurring additional computational complexity. We mix standard convolutions and dilated convolutions in each layer, called mixed convolutions, i.e., in the mixed convolutional layer, and the feature extracted by dilated convolutions and standard convolutions are concatenated. We theoretically analyze the receptive field and intensity of mixed convolutions to discover their role in SR. Mixed convolutions remove blind spots and capture the correlation between low-resolution (LR) and high-resolution (HR) image pairs successfully, thus achieving good generalization ability. We verify those properties of mixed convolutions by training 5-layer and 10-layer networks. We also train a 20-layer deep network to compare the performance of the proposed method with those of the state-of-the-art ones. Moreover, we jointly learn maps with different scales from a LR image to its HR one in a single network. Experimental results demonstrate that the proposed method outperforms the state-of-the-art ones in terms of PSNR and SSIM, especially for a large-scale factor.","","","10.1109/TIP.2018.2877483","National Natural Science Foundation of China; International S&T Cooperation Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502129","Super-resolution;dilated convolutions;deep neural networks;receptive field;correlation analysis","Convolution;Standards;Convolutional codes;Correlation;Image resolution;Kernel;Task analysis","convolution;image reconstruction;image resolution;neural nets","dilated convolutions;standard convolutions;receptive field;multiscale single image super-resolution;mixed convolutional layer;large-scale factor","","5","39","","","","","IEEE","IEEE Journals"
"Variational Object-Aware 3-D Hand Pose From a Single RGB Image","Y. Gao; Y. Wang; P. Falco; N. Navab; F. Tombari","CAMP, Technische Universität München, Garching, Germany; CAMP, Technische Universität München, Garching, Germany; Department of Automation Solutions, ABB Corporate Research, Västerås, Sweden; CAMP, Technische Universität München, Garching, Germany; CAMP, Technische Universität München, Garching, Germany","IEEE Robotics and Automation Letters","","2019","4","4","4239","4246","We propose an approach to estimate the 3D pose of a human hand while grasping objects from a single RGB image. Our approach is based on a probabilistic model implemented with deep architectures, which is used for regressing, respectively, the 2D hand joints heat maps and the 3D hand joints coordinates. We train our networks so to make our approach robust to large objectand self-occlusions, as commonly occurring with the task at hand. Using specialized latent variables, the deep architecture internally infers the category of the grasped object so to enhance the 3D reconstruction, based on the underlying assumption that objects of a similar category, i.e., with similar shape and size, are grasped in a similar way. Moreover, given the scarcity of 3D hand-object manipulation benchmarks with joint annotations, we propose a new annotated synthetic dataset with realistic images, hand masks, joint masks and 3D joints coordinates. Our approach is flexible as it does not require depth information, sensor calibration, data gloves, or finger markers. We quantitatively evaluate it on synthetic datasets achieving state-of-the-art accuracy, as well as qualitatively on real sequences.","","","10.1109/LRA.2019.2930425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770083","Variational inference;triplet","Three-dimensional displays;Pose estimation;Heating systems;Two dimensional displays;Grasping;Image segmentation;Task analysis","image colour analysis;image enhancement;image reconstruction;learning (artificial intelligence);pose estimation;probability;regression analysis;stereo image processing","probabilistic model;single RGB imaging;self-occlusions;object grasping;synthetic dataset annotation;joint hand masks;regression analysis;3D hand joint coordination;2D hand joint coordination;3D reconstruction enhancement;object-aware 3D hand-object manipulation benchmark;sensor calibration;data gloves;variational object-aware 3D hand pose estimation","","","40","Traditional","","","","IEEE","IEEE Journals"
"Rejecting Motion Outliers for Efficient Crowd Anomaly Detection","M. U. K. Khan; H. Park; C. Kyung","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Electrical Engineering, Kongju National University, Gongju, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Information Forensics and Security","","2019","14","2","541","556","Crowd anomaly detection is a key research area in vision-based surveillance. Most of the crowd anomaly detection algorithms are either too slow, bulky, or power-hungry to be applicable for battery-powered surveillance cameras. In this paper, we present a new crowd anomaly detection algorithm. The proposed algorithm creates a feature for every superpixel that includes the contribution from the neighboring superpixels only if their direction of motion conforms with the dominant direction of motion in the region. We also propose using univariate Gaussian discriminant analysis with the K-means algorithm for classification. Our method provides superior accuracy over numerous deep learning-based and handcrafted feature-based approaches. We also present a low-power FPGA implementation of the proposed method. The algorithm is developed such that features are extracted over non-overlapping pixels. This allows gating inputs to numerous modules resulting in higher power efficiency. The maximum energy required per pixel is 2.43 nJ in our implementation. 126.65 Mpixels can be processed per second by the proposed implementation. The speed, power, and accuracy performance of our method make it competitive for surveillance applications, especially battery-powered surveillance cameras.","","","10.1109/TIFS.2018.2856189","Ministry of Science, ICT and Future Planning; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8410933","Crowded scene;FPGA;video analysis;surveillance;anomaly detection","Feature extraction;Anomaly detection;Surveillance;Hidden Markov models;Cameras;Hardware;Computational modeling","computer vision;feature extraction;field programmable gate arrays;Gaussian processes;image classification;learning (artificial intelligence);object detection;video cameras;video surveillance","vision-based surveillance;crowd anomaly detection algorithm;battery-powered surveillance cameras;low-power FPGA implementation;motion outlier rejection;univariate Gaussian discriminant analysis;K-means algorithm;feature extraction;nonoverlapping pixels","","1","64","","","","","IEEE","IEEE Journals"
"Convolutional Neural Network for Finger-Vein-Based Biometric Identification","R. Das; E. Piciucco; E. Maiorana; P. Campisi","Department of Engineering, Section of Applied Electronics, Roma Tre University, Rome, Italy; Department of Engineering, Section of Applied Electronics, Roma Tre University, Rome, Italy; Department of Engineering, Section of Applied Electronics, Roma Tre University, Rome, Italy; Department of Engineering, Section of Applied Electronics, Roma Tre University, Rome, Italy","IEEE Transactions on Information Forensics and Security","","2019","14","2","360","373","The use of human finger-vein traits for the purpose of automatic user recognition has gained a lot of attention in recent years. Current state-of-the-art techniques can provide relatively good performance, yet they are strongly dependent upon the quality of the analyzed finger-vein images. In this paper, we propose a convolutional-neural-network-based finger-vein identification system and investigate the capabilities of the designed network over four publicly available databases. The main purpose of this paper is to propose a deep-learning method for finger-vein identification, which is able to achieve stable and highly accurate performance when dealing with finger-vein images of different quality. The reported extensive set of experiments show that the accuracy achievable with the proposed approach can go beyond 95% correct identification rate for all the four considered publicly available databases.","","","10.1109/TIFS.2018.2850320","Italian Ministry of Education, University and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8395431","Convolutional neural network;finger-vein;biometrics;identification","Veins;Feature extraction;Indexes;Histograms;Measurement;Thumb","feature extraction;feedforward neural nets;fingerprint identification;learning (artificial intelligence);vein recognition","publicly available databases;convolutional neural network;finger-vein-based biometric identification;human finger-vein traits;automatic user recognition;analyzed finger-vein images;convolutional-neural-network-based finger-vein identification system","","10","59","","","","","IEEE","IEEE Journals"
"ASiam: adaptive Siamese regression tracking with adversarial template generation and motion-based failure recovery","X. Jiang; Z. Xiao; B. Zhang; X. Cao","School of Electronic and Information Engineering, Beihang University, Beijing, People's Republic of China; School of Electronic and Information Engineering, Beihang University, Beijing, People's Republic of China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, People's Republic of China; School of Electronic and Information Engineering, Beihang University, Beijing, People's Republic of China","IET Image Processing","","2019","13","14","2694","2705","Object tracking is challenged by the varying appearances of targets and the real-time requirement. Siamese regression trackers, being one of the most popular tracking paradigms, excel in efficiency but suffer at adaptability to cope with appearance variations. To improve their adaptability, the authors propose a new adaptive Siamese (ASiam) tracker, which integrates a novel adversarial template generation module and a motion-based failure recovery module. The template generation module exploits the temporal coherence and evolution of target appearance variations encoded in preceding tracklets and then generates an adaptive target template online which approximates the varying target in the coming frame. This generation module is optimised via adversarial learning to achieve accurate appearance prediction and sharp template quality. The generated template, together with a search region, are fed into a Siamese tracking backbone to compute an appearance response map via dense similarity computation in a sliding-window way. At frames where the Siamese tracking fails, the failure recovery module is invoked to perform deep frame differencing motion detection to provide a motion response map. By fusing different response maps, the drifted tracker can be re-calibrated. Extensive experiments on the OTB2013, OTB2015, and VOT2016 datasets prove the accuracy and efficiency of the proposed tracker.","","","10.1049/iet-ipr.2018.6699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946939","","","image motion analysis;object tracking;object detection;learning (artificial intelligence);target tracking;regression analysis","ASiam;novel adversarial template generation module;motion-based failure recovery module;temporal coherence;adaptive target template;sharp template quality;Siamese tracking backbone;motion response map;adaptive Siamese regression tracking;object tracking;temporal evolution;motion detection;motion-based failure recovery","","","87","","","","","IET","IET Journals"
"Scene Classification Using Hierarchical Wasserstein CNN","Y. Liu; C. Y. Suen; Y. Liu; L. Ding","School of Geography, South China Normal University, Guangzhou, China; Center for Pattern Recognition and Machine Intelligence, Concordia University, Montreal, Canada; School of Geography, South China Normal University, Guangzhou, China; School of Geography, South China Normal University, Guangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","","2019","57","5","2494","2509","In multiclass classification, convolutional neural network (CNN) is generally coupled with the cross-entropy (CE) loss, which only penalizes the predicted probability corresponding to a ground truth class and ignores the interclass relationship. We argue that CNN can be improved by using a better loss function. On the other hand, the Wasserstein distance (WD) is a well-known metric used to measure the distance between two distributions. Directly solving the WD problem requires a prohibitively large amount of computation time, whereas the cheaper iterative algorithms have a variety of shortcomings such as computational instability and difficulty in selecting parameters. In this paper, we address these issues by giving an analytical solution to the WD problem-for the first time, we find that for two distributions in hierarchically organized data space, WD has a closed-form solution, which we call “hierarchical WD (HWD).” We use this theory to construct novel loss functions that overcome the shortcomings of CE loss. To this end, multi-CNN information fusion that provides the basis for building category hierarchies is carried out first. Then, the semantic relationship among classes is modeled as a binary tree. Then, CNN coupled with an HWD-based loss, i.e., hierarchical Wasserstein CNN (HW-CNN), is trained to learn deep features. In this way, prior knowledge about the interclass relationship is embedded into HW-CNN, and information from several CNNs provides guidance in the process of training individual HW-CNNs. We conducted extensive experiments over two publicly available remote sensing data sets and achieved a state-of-the-art performance in scene classification tasks.","","","10.1109/TGRS.2018.2873966","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8513808","Category tree;cross-entropy (CE) loss;hierarchical Wasserstein convolutional neural network (HW-CNN);hierarchical Wasserstein distance (HWD);interclass relationship","Earth;Binary trees;Feature extraction;Training;Task analysis;Convolutional neural networks;Measurement","computer vision;convolutional neural nets;entropy;geophysical image processing;image classification;iterative methods;learning (artificial intelligence);probability;remote sensing","Wasserstein distance;computation time;computational instability;hierarchically organized data space;CE loss;multiCNN information fusion;HWD-based loss;hierarchical Wasserstein CNN;HW-CNN;scene classification tasks;multiclass classification;convolutional neural network;cross-entropy loss;iterative algorithms;probability;hierarchical WD problem;binary tree;remote sensing data sets","","4","66","","","","","IEEE","IEEE Journals"
"Advances in human action recognition: an updated survey","S. A. R. Abu-Bakar","Computer Vision, Video and Image Processing Research Lab, School of Electrical Engineering, Faculty of Engineering, Universiti Teknologi Malaysia, Malaysia","IET Image Processing","","2019","13","13","2381","2394","Research in human activity recognition (HAR) has seen tremendous growth and continuously receiving attention from both the Computer Vision and the Image Processing communities. Due to the existence of numerous publications in this field, undoubtedly, there have been a number of review papers on this subject that categorise these techniques. Many of the recent works have started to tackle more challenging problems and these proposed techniques are addressing more realistic real-world scenarios. Conspicuously, an updated survey that covers these methods is timely due. To simplify the categorisation, this study takes a two-layer hierarchical approach. At the top level, the categorisation is based on the basic process flow of HAR, i.e. input data-type, features-type, descriptor-type, and classifier-type. At the second layer, each of these components is further subcategorised based on the diversity of the proposed methods. Finally, a remark on the coming popularity of deep learning approach in this field is also given.","","","10.1049/iet-ipr.2019.0350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911597","","","learning (artificial intelligence);computer vision;image motion analysis;feature extraction;image recognition","review papers;categorise;updated survey;categorisation;two-layer hierarchical approach;basic process flow;input data-type;features-type;descriptor-type;classifier-type;human action recognition;human activity recognition;HAR;Computer Vision;Image Processing communities;numerous publications","","","95","","","","","IET","IET Journals"
"Temporal Action Localization in Untrimmed Videos Using Action Pattern Trees","H. Song; X. Wu; B. Zhu; Y. Wu; M. Chen; Y. Jia","Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Department of Electrical and Computer Engineering, State University of New York, Albany, NY, USA; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Multimedia","","2019","21","3","717","730","In this paper, we present a novel framework of automatically localizing action instances based on action pattern trees (AP-Trees) in a long untrimmed video. For localizing action instances in videos with varied temporal lengths, we first split videos into sequential segments and then use the AP-Trees to produce precise temporal boundaries of action instances. The AP-Trees can exploit the temporal information between segments of videos based on the label vectors of segments, by learning the occurrence frequency and order of segments. In AP-Trees, nodes stand for action class labels of segments and edges represent the temporal relationships between two consecutive segments. Thus, we can discover the occurrence frequencies of segments by searching paths of AP-Trees. In order to obtain accurate labels of video segments, we introduce deep neural networks to annotate the segments by simultaneously leveraging the spatio-temporal information and the high-level semantic feature of segments. In the networks, informative action maps are generated by a global average pooling layer to retain the spatio-temporal information of segments. An overlap loss function is employed to further improve the precision of label vectors of segments by considering the temporal overlap between segments and the ground truth. The experiments on THUMOS2014, MSR ActionII, and MPII Cooking datasets demonstrate the effectiveness of the method.","","","10.1109/TMM.2018.2866370","Natural Science Foundation of China (NSFC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8440749","Temporal action localization;action pattern tree;informative action maps;overlap loss function","Videos;Semantics;Training;Three-dimensional displays;Proposals;Data mining;Neural networks","data mining;feature extraction;image motion analysis;image segmentation;learning (artificial intelligence);neural nets;trees (mathematics);video signal processing","action class labels;consecutive segments;video segments;spatio-temporal information;informative action maps;temporal action localization;untrimmed videos;action pattern trees;action instances;long untrimmed video;varied temporal lengths;sequential segments;precise temporal boundaries;AP-trees","","","55","","","","","IEEE","IEEE Journals"
"Visual Quality Assessment for Super-Resolved Images: Database and Method","F. Zhou; R. Yao; B. Liu; G. Qiu","College of Information Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China; College of Information Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Image Processing","","2019","28","7","3528","3541","Image super-resolution (SR) has been an active research problem which has recently received renewed interest due to the introduction of new technologies such as deep learning. However, the lack of suitable criteria to evaluate the SR performance has hindered technology development. In this paper, we fill a gap in the literature by providing the first publicly available database as well as a new image quality assessment (IQA) method specifically designed for assessing the visual quality of super-resolved images (SRIs). In constructing the quality assessment database for SRIs (QADS), we carefully selected 20 reference images and created 980 SRIs using 21 image SR methods. Mean opinion score (MOS) for these SRIs is collected through 100 individuals participating in a suitably designed psychovisual experiment. Extensive numerical and statistical analysis is performed to show that the MOS of QADS has excellent suitability and reliability. The psychovisual experiment has led to the discovery that, unlike distortions encountered in other IQA databases, artifacts of the SRIs degenerate the image structure as well as the image texture. Moreover, the structural and textural degenerations have distinctive perceptual properties. Based on these insights, we propose a novel method to assess the visual quality of SRIs by separately considering the structural and textural components of images. Observing that textural degenerations are mainly attributed to dissimilar texture or checkerboard artifacts, we propose to measure the changes of textural distributions. We also observe that structural degenerations appear as blurring and jaggies artifacts in SRIs and develop separate similarity measures for different types of structural degenerations. A new pooling mechanism is then used to fuse the different similarities together to give the final quality score for an SRI. The experiments conducted on the QADS demonstrate that our method significantly outperforms the classical as well as current state-of-the-art IQA methods.","","","10.1109/TIP.2019.2898638","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8640853","Full reference;image database;image quality assessment;image super-resolution","Visualization;Indexes;Distortion;Spatial resolution;Kernel","data compression;distortion;image coding;image colour analysis;image reconstruction;image resolution;image texture;learning (artificial intelligence);statistical analysis","visual quality assessment;super-resolved images;active research problem;SR performance;technology development;publicly available database;image quality assessment method;quality assessment database;QADS;20 reference images;980 SRIs;21 image SR methods;MOS;extensive numerical analysis;statistical analysis;reliability;IQA databases;SRIs degenerate;image structure;image texture;structural degenerations;textural degenerations;structural components;textural components;dissimilar texture;final quality score;current state-of-the-art IQA methods;psychovisual experiment","","","76","","","","","IEEE","IEEE Journals"
"Abnormal region detection in cervical smear images based on fully convolutional network","J. Zhang; J. He; T. Chen; Z. Liu; D. Chen","School of Computer Science and Engineering, South China University of Technology, People's Republic of China; School of Computer Science and Engineering, South China University of Technology, People's Republic of China; Guangzhou LBP Medicine Science & Technology Co., Ltd., People's Republic of China; School of Computer Science and Engineering, South China University of Technology, People's Republic of China; School of Computer Science and Engineering, South China University of Technology, People's Republic of China","IET Image Processing","","2019","13","4","583","590","Automation-assisted cervical screening via liquid-based cytology has achieved great success using segmentation and classification methods. This work tries to do abnormal region detection on field of view cervical cell images based on deep learning, which is a novel way to solve cervical cytological screening problem. Since some abnormal nuclei gather in groups, the proposed method chooses abnormal regions instead of abnormal nuclei as the detection targets in order to locate the abnormal regions for the further diagnosis of the pathologists. In this study, a novel abnormal region detection approach for cervical screening is proposed based on a size-sensitive fully convolutional network (R-FCN). Due to the regular feature distribution, a fewer-layer convolutional neural backbone network is designed for more efficient feature extraction and less running time. In addition, a new measure named hit degree is defined to describe the degree how closely each detected region and the corresponding ground truth matches up. Experimental results show that an average precision of 93.2% is achieved for abnormal region detection in cervical smear images. The proposed method is promising for the development of computer-aided systems in clinical cervical cytological screening.","","","10.1049/iet-ipr.2018.6032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695221","","","feature extraction;learning (artificial intelligence);cancer;image classification;image segmentation;medical image processing;neural nets;biomedical optical imaging;gynaecology;patient diagnosis;cellular biophysics","cervical smear images;automation-assisted cervical screening;liquid-based cytology;view cervical cell images;cervical cytological screening problem;abnormal nuclei;abnormal regions;detection targets;novel abnormal region detection approach;size-sensitive fully convolutional network;fewer-layer convolutional neural backbone network;clinical cervical cytological screening","","","","","","","","IET","IET Journals"
"Local Feature Descriptor and Derivative Filters for Blind Image Quality Assessment","M. Oszust","Department of Computer and Control Engineering, Rzeszow University of Technology, Rzeszow, Poland","IEEE Signal Processing Letters","","2019","26","2","322","326","In this letter, a novel blind image quality assessment (BIQA) technique is introduced to provide an automatic and reproducible evaluation of distorted images. In the approach, the information carried by image derivatives of different orders is captured by local features and used for the image quality prediction. Since a typical local feature descriptor is designed to ensure a robust image patch representation, in this letter, a novel descriptor that additionally highlights local differences enhanced by the filtering is proposed. Furthermore, a set of derivative kernels is introduced. Finally, the support vector regression technique is used to map statistics of described local features into subjective scores, providing an objective quality score for an image. Extensive experimental validation on popular IQA image datasets reveals that the proposed method outperforms the state-of-the-art handcrafted and deep learning BIQA measures.","","","10.1109/LSP.2019.2891416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8604059","Blind image quality assessment;local features;feature descriptor;derivative filters;support vector regression","Distortion measurement;Kernel;Feature extraction;Distortion;Image quality;Detectors;Image color analysis","feature extraction;image filtering;image representation;learning (artificial intelligence);regression analysis;support vector machines","local features;local feature descriptor;blind image quality assessment technique;IQA image datasets;derivative kernels;robust image patch representation;image quality prediction;image derivatives;distorted images;reproducible evaluation;automatic evaluation;derivative filters;objective quality score;support vector regression technique","","1","43","","","","","IEEE","IEEE Journals"
"Joint Classification and Prediction CNN Framework for Automatic Sleep Stage Classification","H. Phan; F. Andreotti; N. Cooray; O. Y. Chén; M. De Vos","Institute of Biomedical Engineering, University of Oxford, Oxford, U.K.; Institute of Biomedical EngineeringUniversity of Oxford; Institute of Biomedical EngineeringUniversity of Oxford; Institute of Biomedical EngineeringUniversity of Oxford; Institute of Biomedical EngineeringUniversity of Oxford","IEEE Transactions on Biomedical Engineering","","2019","66","5","1285","1296","Correctly identifying sleep stages is important in diagnosing and treating sleep disorders. This paper proposes a joint classification-and-prediction framework based on convolutional neural networks (CNNs) for automatic sleep staging, and, subsequently, introduces a simple yet efficient CNN architecture to power the framework. Given a single input epoch, the novel framework jointly determines its label (classification) and its neighboring epochs' labels (prediction) in the contextual output. While the proposed framework is orthogonal to the widely adopted classification schemes, which take one or multiple epochs as contextual inputs and produce a single classification decision on the target epoch, we demonstrate its advantages in several ways. First, it leverages the dependency among consecutive sleep epochs while surpassing the problems experienced with the common classification schemes. Second, even with a single model, the framework has the capacity to produce multiple decisions, which are essential in obtaining a good performance as in ensemble-of-models methods, with very little induced computational overhead. Probabilistic aggregation techniques are then proposed to leverage the availability of multiple decisions. To illustrate the efficacy of the proposed framework, we conducted experiments on two public datasets: Sleep-EDF Expanded (Sleep-EDF), which consists of 20 subjects, and Montreal Archive of Sleep Studies (MASS) dataset, which consists of 200 subjects. The proposed framework yields an overall classification accuracy of 82.3% and 83.6%, respectively. We also show that the proposed framework not only is superior to the baselines based on the common classification schemes but also outperforms existing deep-learning approaches. To our knowledge, this is the first work going beyond the standard single-output classification to consider multitask neural networks for automatic sleep staging. This framework provides avenues for further studies of different neural-network architectures for automatic sleep staging.","","","10.1109/TBME.2018.2872652","NIHR Oxford Biomedical Research Centre; Wellcome Trust; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502139","Sleep stage classification;joint classification and prediction;convolutional neural network;multi-task","Sleep;Standards;Electroencephalography;Computational modeling;Task analysis;Electromyography;Electrooculography","learning (artificial intelligence);medical computing;neural nets;pattern classification;sleep","Sleep Studies dataset;classification accuracy;common classification schemes;standard single-output classification;automatic sleep staging;prediction CNN framework;automatic Sleep stage classification;efficient CNN architecture;single input epoch;neighboring epochs;widely adopted classification schemes;multiple epochs;single classification decision;target epoch;consecutive sleep epochs;multiple decisions;Sleep-EDF;sleep disorders;joint classification-and-prediction framework","","2","63","CCBY","","","","IEEE","IEEE Journals"
"Speckle-Noise-Invariant Convolutional Neural Network for SAR Target Recognition","Y. Kwak; W. Song; S. Kim","Department of Electronics Engineering, Pohang University of Science and Technology, Pohang, South Korea; Department of Electronics Engineering, Pohang University of Science and Technology, Pohang, South Korea; Department of Electronics and Control Engineering, Hanbat National University, Daejeon, South Korea","IEEE Geoscience and Remote Sensing Letters","","2019","16","4","549","553","Speckle noise is inherent to synthetic aperture radar (SAR) images and degrades the target recognition performance. Deep learning based on convolutional neural networks (CNNs) has been widely applied for SAR target recognition, but the extracted features are still sensitive to speckle noise. In addition, speckle noise has been seldom considered in such CNN-based approaches. In this letter, we propose a speckle-noise-invariant CNN that employs regularization for minimizing feature variations caused by this noise. Before CNN training, we performed SAR image despeckling using the improved Lee sigma filter for feature extraction. Then, we generated SAR images for CNN training by adding speckle noise to the despeckled images. The proposed regularization improves both the feature robustness to speckle noise and SAR target recognition. Experiments on the moving and stationary target acquisition and recognition database demonstrate that the proposed CNN notably improves the classification accuracy compared with the conventional methods.","","","10.1109/LGRS.2018.2877599","National Research Foundation of Korea; National Research Foundation of Korea; Ministry of Science and ICT, South Korea, under the ICT Consilience Creative Program supervised by the Institute for Information and communications Technology Promotion; LG Display; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8527544","Convolutional neural network (CNN);data augmentation;feature extraction;regularization;speckle noise;synthetic aperture radar (SAR)","Speckle;Synthetic aperture radar;Target recognition;Training;Feature extraction;Exponential distribution;Convolutional neural networks","feature extraction;geophysical image processing;image classification;learning (artificial intelligence);neural nets;radar imaging;radar target recognition;remote sensing by radar;speckle;synthetic aperture radar","SAR image despeckling;CNN training;SAR target recognition;stationary target acquisition;recognition database;speckle-noise-invariant convolutional neural network;synthetic aperture radar images;target recognition performance;convolutional neural networks;speckle-noise-invariant CNN;feature extraction;Lee sigma filter;despeckled images","","4","16","","","","","IEEE","IEEE Journals"
"Adversarial Approximate Inference for Speech to Electroglottograph Conversion","P. A. P.; V. Srivastava; M. Mishra","Indian Institute of Technology Delhi, New Delhi, India; Indian Institute of Technology Delhi, New Delhi, India; Indian Institute of Technology Delhi, New Delhi, India","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","12","2183","2196","Speech produced by human vocal apparatus conveys substantial non-semantic information including the gender of the speaker, voice quality, affective state, abnormalities in the vocal apparatus etc. Such information is attributed to the properties of the voice source signal, which is usually estimated from the speech signal. However, most of the source estimation techniques depend heavily on the goodness of the model assumptions and are prone to noise. A popular alternative is to indirectly obtain the source information through the Electroglottographic (EGG) signal that measures the electrical admittance around the vocal folds using dedicated hardware. In this paper, we address the problem of estimating the EGG signal directly from the speech signal, devoid of any hardware. Sampling from the intractable conditional distribution of the EGG signal given the speech signal is accomplished through optimization of an evidence lower bound. This is constructed via minimization of the KL-divergence between the true and the approximated posteriors of a latent variable learned using a deep neural auto-encoder that serves an informative prior. We demonstrate the efficacy of the method at generating the EGG signal by conducting several experiments on datasets comprising multiple speakers, voice qualities, noise settings and speech pathologies. The proposed method is evaluated on many benchmark metrics and is found to agree with the gold standard while proving better than the state-of-the-art algorithms on a few tasks such as epoch extraction.","","","10.1109/TASLP.2019.2942140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843916","Speech2EGG;Approximate inference;Adversarial learning;Eletroglottograph;epoch extraction;GCI detection","Speech processing;Estimation;Task analysis;Hardware;Pathology;Measurement;Data mining","","","","","58","IEEE","","","","IEEE","IEEE Journals"
"SCAN: Self-and-Collaborative Attention Network for Video Person Re-Identification","R. Zhang; J. Li; H. Sun; Y. Ge; P. Luo; X. Wang; L. Lin","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Sensetime Research, Shenzhen, China; Sensetime Research, Shenzhen, China; Sensetime Research, Shenzhen, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Image Processing","","2019","28","10","4870","4882","Video person re-identification has attracted much attention in recent years. It aims to match image sequences of pedestrians from different camera views. Previous approaches usually improve this task from three aspects, including: 1) selecting more discriminative frames; 2) generating more informative temporal representations; and 3) developing more effective distance metrics. To address the above issues, we present a novel and practical deep architecture for video person re-identification termed self-and-collaborative attention network (SCAN), which adopts the video pairs as the input and outputs their matching scores. SCAN has several appealing properties. First, SCAN adopts a non-parametric attention mechanism to refine the intra-sequence and inter-sequence feature representation of videos and outputs self-and-collaborative feature representation for each video, making the discriminative frames aligned between the probe and gallery sequences. Second, beyond the existing models, a generalized pairwise similarity measurement is proposed to generate the similarity feature representation of video pair by calculating the Hadamard product of their self-representation difference and collaborative-representation difference. Thus, the matching result can be predicted by the binary classifier. Third, a dense clip segmentation strategy is also introduced to generate rich probe-gallery pairs to optimize the model. In the test phase, the final matching score of two videos is determined by averaging the scores of top-ranked clip-pairs. Extensive experiments demonstrate the effectiveness of SCAN, which outperforms the top-1 accuracies of the best-performing baselines on iLIDS-VID, PRID2011, and MARS datasets, respectively.","","","10.1109/TIP.2019.2911488","National Basic Research Program of China (973 Program); SenseTime Group Ltd.; General Research Fund through the Research Grants Council of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703416","Temporal modeling;similarity measurement;collaborative representation;person re-identification;attention mechanism","Probes;Image sequences;Collaboration;Task analysis;Feature extraction;Measurement;Cameras","feature extraction;image representation;image sequences;learning (artificial intelligence);optimisation;video signal processing","final matching score;video person re-identification;discriminative frames;informative temporal representations;video pair;nonparametric attention mechanism;inter-sequence feature representation;outputs self;gallery sequences;similarity feature representation;self-representation difference;collaborative-representation difference;collaborative feature representation;Re","","","85","","","","","IEEE","IEEE Journals"
"A 68-mw 2.2 Tops/w Low Bit Width and Multiplierless DCNN Object Detection Processor for Visually Impaired People","X. Chen; J. Xu; Z. Yu","School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","11","3444","3453","Deep convolutional neural network (DCNN) object detection is a powerful solution in visual perception, but it requires huge computation and communication costs. We proposed a fast and low-power always-on object detection processor that allows visually impaired people to understand their surroundings. We designed an automatic DCNN quantization algorithm that successfully quantizes the data to 8-bit fix-points with 32 values and uses 5-bit indexes to represent them, reducing hardware cost by over 68% compared to the 16-bit DCNN, with negligible accuracy loss. A specific hardware accelerator is designed, which uses reconfigurable process engines to realize multi-layer pipelines to significantly reduce or eliminate the off-chip temporary data transfer. A lookup table is used to implement all multiplications in convolutions to reduce the power significantly. The design is fabricated in SMIC 55-nm technology, and the post-layout simulation shows only 68-mw power at 1.1-v voltage with 155 Go/s performance, achieving 2.2 Top/w energy efficiency.","","","10.1109/TCSVT.2018.2883087","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543599","DCNN;object detection;low-bit;multiplierless;visually impaired people;assist system","Quantization (signal);Hardware;Object detection;Optimization;Indexes;Computational efficiency;Energy efficiency","CMOS integrated circuits;convolutional neural nets;digital signal processing chips;handicapped aids;integrated circuit design;learning (artificial intelligence);low-power electronics;object detection;parallel architectures;quantisation (signal)","multiplierless DCNN object detection processor;visually impaired people;convolutional neural network object detection;visual perception;automatic DCNN quantization algorithm;hardware accelerator;off-chip temporary data transfer;2.2 tops/w Low Bit Width;power reduction;power 68 mW","","1","25","","","","","IEEE","IEEE Journals"
"Automatic Age Estimation and Majority Age Classification From Multi-Factorial MRI Data","D. Štern; C. Payer; N. Giuliani; M. Urschler","Ludwig Boltzmann Institute for Clinical Forensic Imaging, Graz, Austria; Institute of Computer Graphics and Vision, Graz University of Technology, Graz, Austria; Ludwig Boltzmann Institute for Clinical Forensic Imaging, Graz, Austria; Ludwig Boltzmann Institute for Clinical Forensic Imaging, Graz, Austria","IEEE Journal of Biomedical and Health Informatics","","2019","23","4","1392","1403","Age estimation from radiologic data is an important topic both in clinical medicine as well as in forensic applications, where it is used to assess unknown chronological age or to discriminate minors from adults. In this paper, we propose an automatic multi-factorial age estimation method based on MRI data of hand, clavicle, and teeth to extend the maximal age range from up to 19 years, as commonly used for age assessment based on hand bones, to up to 25 years, when combined with clavicle bones and wisdom teeth. Fusing age-relevant information from all three anatomical sites, our method utilizes a deep convolutional neural network that is trained on a dataset of 322 subjects in the age range between 13 and 25 years, to achieve a mean absolute prediction error in regressing chronological age of 1.01 ± 0.74 years. Furthermore, when used for majority age classification, we show that a classifier derived from thresholding our regression-based predictor is better suited than a classifier directly trained with a classification loss, especially when taking into account that those cases of minors being wrongly classified as adults need to be minimized. In conclusion, we overcome the limitations of the multi-factorial methods currently used in forensic practice, i.e., dependence on ionizing radiation, subjectivity in quantifying age-relevant information, and lack of an established approach to fuse this information from individual anatomical sites.","","","10.1109/JBHI.2018.2869606","Austrian Science Fund; city of Graz; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8470073","Information fusion;multi-factorial;convolutional neural network;age estimation;majority age classification;magnetic resonance imaging","Estimation;Magnetic resonance imaging;Bones;Forensics;Feature extraction;Informatics","ageing;biomedical MRI;bone;convolutional neural nets;data analysis;diagnostic radiography;feature extraction;image classification;learning (artificial intelligence);medical image processing;pattern classification;power apparatus;regression analysis","regression-based predictor;multifactorial methods;automatic age estimation;majority age classification;multifactorial MRI data;radiologic data;unknown chronological age;multifactorial age estimation method;maximal age range;age assessment;hand bones;clavicle bones;fusing age-relevant information","","","47","CCBY","","","","IEEE","IEEE Journals"
"Sound Event Detection and Time–Frequency Segmentation from Weakly Labelled Data","Q. Kong; Y. Xu; I. Sobieraj; W. Wang; M. D. Plumbley","Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2019","27","4","777","787","Sound event detection (SED) aims to detect when and recognize what sound events happen in an audio clip. Many supervised SED algorithms rely on strongly labelled data that contains the onset and offset annotations of sound events. However, many audio tagging datasets are weakly labelled, that is, only the presence of the sound events is known, without knowing their onset and offset annotations. In this paper, we propose a time-frequency (T-F) segmentation framework trained on weakly labelled data to tackle the sound event detection and separation problem. In training, a segmentation mapping is applied on a T-F representation, such as log mel spectrogram of an audio clip to obtain T-F segmentation masks of sound events. The T-F segmentation masks can be used for separating the sound events from the background scenes in the T-F domain. Then, a classification mapping is applied on the T-F segmentation masks to estimate the presence probabilities of the sound events. We model the segmentation mapping using a convolutional neural network and the classification mapping using a global weighted rank pooling. In SED, predicted onset and offset times can be obtained from the T-F segmentation masks. As a byproduct, separated waveforms of sound events can be obtained from the T-F segmentation masks. We remixed the DCASE 2018 Task 1 acoustic scene data with the DCASE 2018 Task 2 sound events data. When mixing under 0 dB, the proposed method achieved F1 scores of 0.534, 0.398, and 0.167 in audio tagging, frame-wise SED and event-wise SED, outperforming the fully connected deep neural network baseline of 0.331, 0.237, and 0.120, respectively. In T-F segmentation, we achieved an F1 score of 0.218, where previous methods were not able to do T-F segmentation.","","","10.1109/TASLP.2019.2895254","Engineering and Physical Sciences Research Council; China Scholarship Council; European Union's H2020 Framework Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8632940","Sound event detection;time-frequency segmentation;weakly labelled data;convolutional neural network","Event detection;Training;Spectrogram;Tagging;Convolutional neural networks;Task analysis;Time-frequency analysis","acoustic signal detection;acoustic signal processing;audio signal processing;feature extraction;learning (artificial intelligence);neural nets;probability;speech recognition","sound event detection;weakly labelled data;segmentation mapping;DCASE 2018 Task 2 sound events data;supervised SED algorithms;T-F segmentation masks;classification mapping;convolutional neural network","","3","57","","","","","IEEE","IEEE Journals"
"Dense Dilated Network for Video Action Recognition","B. Xu; H. Ye; Y. Zheng; H. Wang; T. Luwang; Y. Jiang","ZhongAn Technology, Shanghai, China; Videt Tech, Shanghai, China; Videt Tech, Shanghai, China; ZhongAn Technology, Shanghai, China; Shanghai Lunion Intelligent Technology Co., Ltd. (LunionData), Shanghai, China; School of Computer Science, Fudan University, Shanghai, China","IEEE Transactions on Image Processing","","2019","28","10","4941","4953","The ability to recognize actions throughout a video is essential for surveillance, self-driving, and many other applications. Although many researchers have investigated deep neural networks to get a better result in video action recognition, these networks usually require a large number of well-labeled data to train. In this paper, we introduce a dense dilated network to collect action information from snippet-level to global-level. The dilated dense network is composed of the blocks with densely connected dilated convolutions layers. Our proposed framework is capable of fusing outputs from each layer to learn high-level representations, and these representations are robust even with only a few training snippets. We study different spatial and temporal modality fusing configurations and introduce a novel temporal guided fusion upon the dense dilated network which can further boost the performance. We conduct extensive experiments on two popular video action datasets: UCF101 and HMDB51. The experiments demonstrate the effectiveness of our proposed framework.","","","10.1109/TIP.2019.2917283","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8720204","Action recognition;dilated convolution;two-stream fusion;video analysis","Feature extraction;Computer architecture;Optical imaging;Neural networks;Task analysis;Training data;Image recognition","image recognition;image representation;neural nets;video signal processing;video surveillance","dense dilated network;video action recognition;deep neural networks;self-driving;surveillance;dilated convolutions layers","","","45","","","","","IEEE","IEEE Journals"
"Robust fingerprint classification with Bayesian convolutional networks","T. Zia; M. Ghafoor; S. A. Tariq; I. A. Taj","COMSATS Institute of Information Technology, Pakistan; COMSATS Institute of Information Technology, Pakistan; COMSATS Institute of Information Technology, Pakistan; Capital University of Science and Technology, Pakistan","IET Image Processing","","2019","13","8","1280","1288","Fingerprint classification is vital for reducing the search time and computational complexity of the fingerprint identification system. The robustness of classifier relies on the strength of extracted features and the ability to deal with low-quality fingerprints. The proficiency to learn accurate features from raw fingerprint images rather than explicit feature extraction makes deep convolutional neural networks (DCNNs) attractive for fingerprint classification. The DCNNs use softmax for quantifying model confidence of a class for an input fingerprint image to make a prediction. However, the softmax probabilities are not a true representation of model confidence and often misleading in feature space that may not be represented with the available training examples. The primary goal of this study is to improve the efficacy of the fingerprint classification by dealing with false positives by employing Bayesian model uncertainty. The efficacy of the proposed method is shown through experimentations on NIST special database 4 (NIST-4) and fingerprint verification competition 2002 database 1-A (FVC DB1-A) 2002 and 2004 datasets. Results show that 0.8-1.0% of accuracy is improved with model uncertainty over the conventional DCNN.","","","10.1049/iet-ipr.2018.5466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8741332","","","Bayes methods;convolutional neural nets;feature extraction;fingerprint identification;image classification","input fingerprint image;deep convolutional neural networks;explicit feature extraction;raw fingerprint images;low-quality fingerprints;fingerprint identification system;Bayesian convolutional networks;robust fingerprint classification;Bayesian model uncertainty;feature space;model confidence","","","45","","","","","IET","IET Journals"
"Crowd Counting With Limited Labeling Through Submodular Frame Selection","Q. Zhou; J. Zhang; L. Che; H. Shan; J. Z. Wang","Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China; Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China; Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China; Department of Biomedical Engineering, Center for Biotechnology and Interdisciplinary Studies, Rensselaer Polytechnic Institute, Troy, NY, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA","IEEE Transactions on Intelligent Transportation Systems","","2019","20","5","1728","1738","Automated crowd counting is valuable for intelligent transportation systems, as it can help to improve the emergency planning and prevent congestion in transit hubs such as train stations and airports. Semi-supervised crowd counting aims to estimate the number of pedestrians in an ongoing scene using a combination of a small number of labeled frames and a large number of unlabeled ones. However, existing methods do not incorporate ways to effectively select informative frames as labeled training samples, resulting in low accuracy on unseen crowd scenes. We propose a submodular method to select the most informative frames from the image sequences of crowds. Specifically, the method selects the most representative images to guarantee the information coverage, by maximizing the similarities between the group of selected images and the image sequence. In addition, these frames are chosen to avoid redundancies and preserve diversity. Finally, our semi-supervised method incorporates graph Laplacian regularization and spatiotemporal constraints. Extensive experiments on three benchmark data sets demonstrate that our proposed approach achieves higher accuracy compared with the state-of-the-art regression methods and competitive performance with deep convolutional models, especially when the number of labeled data is exceptionally small.","","","10.1109/TITS.2018.2829987","National Natural Science Foundation of China; Science and Technology Commission of Shanghai Municipality; Pennsylvania State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360780","Intelligent transportation hubs;crowd counting;submodular subset selection;semi-supervised learning","Training;Task analysis;Image sequences;Redundancy;Intelligent transportation systems;Feature extraction;Labeling","feature selection;graph theory;image classification;image sequences;intelligent transportation systems;pedestrians","submodular frame selection;automated crowd counting;intelligent transportation systems;emergency planning;transit hubs;representative images;information coverage;image sequence;semisupervised method;frame labeling;semisupervised crowd counting;graph Laplacian regularization;spatiotemporal constraints;pedestrian estimation","","1","42","","","","","IEEE","IEEE Journals"
"Attention-Based Convolutional Neural Network for Weakly Labeled Human Activities’ Recognition With Wearable Sensors","K. Wang; J. He; L. Zhang","School of Electrical and Automation Engineering, Nanjing Normal University, Nanjing, China; School of Electronic and Information Engineering, Nanjing University of Information Science and Technology, Nanjing, China; School of Electrical and Automation Engineering, Nanjing Normal University, Nanjing, China","IEEE Sensors Journal","","2019","19","17","7598","7604","Traditional methods of human activity recognition usually require a large amount of strictly labeled data for training classifiers. However, it is hard for one to keep a fixed activity when collecting desired activity data by wearable sensors, and the weakly labeled data inevitably occurs in the process of data collection. For now, human activity recognition methods have seldom been researched according to weakly labeled data, which deserves deep investigation. In this paper, we proposed a novel attention-based human activity recognition method to process the weakly labeled activity data. The traditional convolutional neural network (CNN)-based human activity recognition is modified by attention mechanism, which computes the compatibility between the global features extracted at the final fully connected layers and the local features extracted at a given convolutional layer. The attention-based CNN architecture can amplify the salient activity information and suppress the irrelevant and potentially confusing information by weighing up their compatibility. Our methods are compared with two state-of-the-art methods, CNN and DeepConvLSTM. The experimental results show that our model is comparably well on the traditional UCI HAR dataset and outperforms them on the weakly labeled dataset in accuracy. Our method can greatly facilitate the process of sensor data annotation and makes data collection easier.","","","10.1109/JSEN.2019.2917225","National Natural Science Foundation of China; Industry-Academia Cooperation Innovation Fund Projection of Jiangsu Province; Foundation of Shanghai Key Laboratory of Navigation and Location-based Services, Shanghai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8716726","Human activity recognition;attention-based convolutional neural network;compatibility density;weakly supervised learning;wearable sensor data","Feature extraction;Activity recognition;Pipelines;Wearable sensors;Convolutional neural networks;Data models","convolutional neural nets;feature extraction","salient activity information;weakly labeled dataset;sensor data annotation;data collection;weakly labeled human activities;wearable sensors;strictly labeled data;fixed activity;weakly labeled data;human activity recognition methods;attention-based human activity recognition method;weakly labeled activity data;attention mechanism;attention-based CNN architecture;convolutional layer;convolutional neural network-based human activity recognition","","","29","","","","","IEEE","IEEE Journals"
"Semantic Segmentation of Pathological Lung Tissue With Dilated Fully Convolutional Networks","M. Anthimopoulos; S. Christodoulidis; L. Ebner; T. Geiser; A. Christe; S. Mougiakakou","ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland; ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland; Department of Diagnostic, Interventional and Pediatric Radiology, Bern University Hospital “Inselspital,”, Switzerland; University Clinic for Pneumonology, Bern University Hospital “Inselspital,”, Bern, Switzerland; Department of Diagnostic, Interventional and Pediatric Radiology, Bern University Hospital “Inselspital,”, Switzerland; Department of Diagnostic, Interventional and Pediatric Radiology, Bern University Hospital “Inselspital,”, Switzerland","IEEE Journal of Biomedical and Health Informatics","","2019","23","2","714","722","Early and accurate diagnosis of interstitial lung diseases (ILDs) is crucial for making treatment decisions, but can be challenging even for experienced radiologists. The diagnostic procedure is based on the detection and recognition of the different ILD pathologies in thoracic CT scans, yet their manifestation often appears similar. In this study, we propose the use of a deep purely convolutional neural network for the semantic segmentation of ILD patterns, as the basic component of a computer aided diagnosis system for ILDs. The proposed CNN, which consists of convolutional layers with dilated filters, takes as input a lung CT image of arbitrary size and outputs the corresponding label map. We trained and tested the network on a data set of 172 sparsely annotated CT scans, within a cross-validation scheme. The training was performed in an end-to-end and semisupervised fashion, utilizing both labeled and nonlabeled image regions. The experimental results show significant performance improvement with respect to the state of the art.","","","10.1109/JBHI.2018.2818620","Bern University Hospital, “Inselspital,” and the Swiss National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8325482","Interstitial lung disease;Fully convolutional neural networks;Dilated convolutions;Texture segmentation;Semi-supervised learning","Lung;Image segmentation;Semantics;Pathology;Training;Kernel;Diseases","computerised tomography;convolutional neural nets;diseases;image segmentation;lung;medical image processing","thoracic CT scans;semantic segmentation;ILD patterns;computer aided diagnosis system;convolutional layers;dilated filters;lung CT image;arbitrary size;pathological lung tissue;interstitial lung diseases;convolutional neural network;fully convolutional networks;ILD pathologies","","3","49","","","","","IEEE","IEEE Journals"
"LDLS: 3-D Object Segmentation Through Label Diffusion From 2-D Images","B. H. Wang; W. Chao; Y. Wang; B. Hariharan; K. Q. Weinberger; M. Campbell","Sibley School of Mechanical and Aerospace Engineering, Cornell University, Ithaca, NY, USA; Department of Computer Science, Cornell University, Ithaca, NY, USA; Department of Computer Science, Cornell University, Ithaca, NY, USA; Department of Computer Science, Cornell University, Ithaca, NY, USA; Department of Computer Science, Cornell University, Ithaca, NY, USA; Sibley School of Mechanical and Aerospace Engineering, Cornell University, Ithaca, NY, USA","IEEE Robotics and Automation Letters","","2019","4","3","2902","2909","Object segmentation in three-dimensional (3-D) point clouds is a critical task for robots capable of 3-D perception. Despite the impressive performance of deep learning-based approaches on object segmentation in 2-D images, deep learning has not been applied nearly as successfully for 3-D point cloud segmentation. Deep networks generally require large amounts of labeled training data, which are readily available for 2-D images but are difficult to produce for 3-D point clouds. In this letter, we present Label Diffusion Lidar Segmentation (LDLS), a novel approach for 3-D point cloud segmentation, which leverages 2-D segmentation of an RGB image from an aligned camera to avoid the need for training on annotated 3-D data. We obtain 2-D segmentation predictions by applying Mask-RCNN to the RGB image, and then link this image to a 3-D lidar point cloud by building a graph of connections among 3-D points and 2-D pixels. This graph then directs a semi-supervised label diffusion process, where the 2-D pixels act as source nodes that diffuse object label information through the 3-D point cloud, resulting in a complete 3-D point cloud segmentation. We conduct empirical studies on the KITTI benchmark dataset and on a mobile robot, demonstrating wide applicability and superior performance of LDLS compared with the previous state of the art in 3-D point cloud segmentation, without any need for either 3-D training data or fine tuning of the 2-D image segmentation model.","","","10.1109/LRA.2019.2922582","Naval Research; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735751","Object detection;segmentation and categorization;RGB-D perception","Three-dimensional displays;Two dimensional displays;Image segmentation;Laser radar;Sensors;Cameras;Task analysis","","","","","34","","","","","IEEE","IEEE Journals"
"Differential Features for Pedestrian Detection: A Taylor Series Perspective","J. Shen; X. Zuo; W. Yang; D. Prokhorov; X. Mei; H. Ling","School of Electrical and Information Engineering, Jiangsu University, Zhenjiang, China; School of Computer Science and Engineering, Jiangsu University of Science and Technology, Zhenjiang, China; School of Automation, Southeast University, Nanjing, China; Toyota Research Institute, North America, Ann Arbor, MI, USA; Toyota Research Institute, North America, Ann Arbor, MI, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA","IEEE Transactions on Intelligent Transportation Systems","","2019","20","8","2913","2922","Differential features are popularly used in computer vision tasks, such as object detection. In this paper, we revisit these features from a functional approximation perspective. In particular, we view an image as a 2-D functional and investigate its Taylor series approximation. Differential features are derived from the approximation coefficients and, therefore, are naturally collected for appearance representation. Thus motivated, we propose to use the zeroth-, first-, and second-order differential features for pedestrian detection and call such features Taylor feature transform (TAFT). In practice, the TAFT features are computed by discrete sampling to address scale issues and meanwhile achieve computational efficiency. In addition, orientation insensitivity is handled by using directional versions of differentials. When applied to pedestrian detection, the TAFT is sampled on grid pixels and calculated from multiple channels following previous solutions. In our extensive experiments on the INRIA, Caltech, TUD-Brussel, and KITTI data sets, the TAFT achieves state-of-the-art results. It outperforms all handcrafted features and performs on par with many deep-learning solutions. Moreover, when a low false-positive rate is requested, the TAFT generates results that are better than or comparable to the state-of-the-art deep learning-based methods. Meanwhile, our implementation runs at 33 fps for 640×480 images without GPU, making TAFT favorable in many practical scenarios.","","","10.1109/TITS.2018.2869087","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478149","Pedestrian detection;Taylor feature transform;integral channel feature","Feature extraction;Transforms;Taylor series;Graphics processing units;Task analysis;Robustness;Detectors","approximation theory;computer vision;object detection;pedestrians;transforms","pedestrian detection;computer vision tasks;object detection;functional approximation perspective;Taylor series approximation;second-order differential features;TAFT features;Taylor feature transform;first-order differential features;zeroth-order differential features","","1","49","","","","","IEEE","IEEE Journals"
"Convolutional Neural Network With Second-Order Pooling for Underwater Target Classification","X. Cao; R. Togneri; X. Zhang; Y. Yu","School of Marine Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Electrical, Electronics and Computer Engineering, The University of Western Australia, Perth, WA, Australia; School of Marine Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Marine Science and Technology, Northwestern Polytechnical University, Xi’an, China","IEEE Sensors Journal","","2019","19","8","3058","3066","Underwater target classification using passive sonar remains a critical issue due to the changeable ocean environment. Convolutional neural networks (CNNs) have shown success in learning invariant features using local filtering and max pooling. In this paper, we propose a novel classification framework which combines the CNN architecture with the second-order pooling (SOP) to capture the temporal correlations from the time-frequency (T-F) representation of the radiated acoustic signals. The convolutional layers are used to learn the local features with a set of kernel filters from the T-F inputs which are extracted by the constant-Q transform (CQT). Instead of using max pooling, the proposed SOP operator is designed to learn the co-occurrences of different CNN filters using the temporal feature trajectory of CNN features for each frequency subband. To preserve the frequency distinctions, the correlated features of each frequency subband are retained. The pooling results are normalized with signed square-root and l2 normalization, and then input into the softmax classifier. The whole network can be trained in an end-to-end fashion. To explore the generalization ability to unseen conditions, the proposed CNN model is evaluated on the real radiated acoustic signals recorded at new sea depths. The experimental results demonstrate that the proposed method yields an 8% improvement in classification accuracy over the state-of-the-art deep learning methods.","","","10.1109/JSEN.2018.2886368","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8573835","Underwater target classification;convolutional neural networks;second-order pooling;constant-Q transform","Feature extraction;Acoustics;Time-frequency analysis;Correlation;Computer architecture;Transforms;Convolutional neural networks","convolutional neural nets;sonar;underwater acoustic communication","radiated acoustic signals;convolutional layers;kernel filters;max pooling;SOP operator;temporal feature trajectory;CNN features;classification accuracy;convolutional neural network;second-order pooling;underwater target classification;passive sonar;invariant features;local filtering;CNN architecture;temporal correlations","","1","39","","","","","IEEE","IEEE Journals"
"Using an End-to-End Convolutional Network on Radar Signal for Human Activity Classification","W. Ye; H. Chen; B. Li","School of Electronic Science and Technology, Shenzhen University, Shenzhen, China; School of Optoelectronic Engineering, Shenzhen University, Shenzhen, China; School of Electronic Science and Technology, Shenzhen University, Shenzhen, China","IEEE Sensors Journal","","2019","19","24","12244","12252","Almost all existing methods for human activity classification based on micro-Doppler radar first manually convert the raw radar signal into a spectrogram using a short time Fourier transform. Then, the spectrogram features are either manually extracted using hand-crafted feature engineering or automatically extracted using deep convolutional networks and fed into a classifier such as a support vector machine, k-nearest neighbor or multi-layer perceptron. However, the optimality of this two-step process is limited by the use of spectrograms, which are a hand-crafted representation. In this paper, the first time truly end-to-end deep network that incorporates the signal representation process into the network is proposed. In the proposed network, which is called RadarNet, two one-dimensional convolutional layers are used to replace short time Fourier transform to obtain a learned radar signal representation. The experimental results show that the proposed RadarNet can achieve 96.35% accuracy in human sleep activity classification and 96.31% accuracy in human daily activity classification, which is 1.96% and 3.26% higher than those of the best existing method, respectively.","","","10.1109/JSEN.2019.2938997","Kongque Technology Innovation Foundation of Shenzhen; Shenzhen Science and Technology Development Funds; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8822709","One-dimensional convolution;human activity classification;convolutional neural network;micro-Doppler radar","Feature extraction;Radar;Convolution;Spectrogram;Time-frequency analysis;Fourier transforms;Signal representation","","","","","20","IEEE","","","","IEEE","IEEE Journals"
"Location Verification for Emerging Wireless Vehicular Networks","U. Ihsan; S. Yan; R. Malaney","School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia; School of Engineering, Macquarie University, Sydney, NSW, Australia; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia","IEEE Internet of Things Journal","","2019","6","6","10261","10272","The work reported here utilizes the best aspects of information theory and deep-learning concepts so as to provide, for the first time, a solution for a real-world location verification system (LVS) in the context of vehicular networks. it is well established that global positioning system coordinates supplied by vehicles will be a vital component of such emerging networks. This supplied location information, if erroneous and not verified, can seriously degrade the overall system performance and lead to significant safety issues. A number of location verification protocols and systems have been developed to address this important problem but all have operational constraints and performance limitations due to their requirement for ideal static channel conditions and assumed threat models. In this article, we remove such limitations by designing a neural-network-based LVS (NN-LVS) that can accommodate a priori unknown channel conditions and unknown threat models. Under most channel conditions, the NN-LVS shows a performance improvement of 50%, or more, relative to other LVSs. We also derive a new information-theoretic bound on the total error for an LVS and show how this new bound allows for a useful tradeoff in learning-time versus verification-performance for the NN-LVS. We demonstrate an improved performance for the NN-LVS within the context of vehicular networks using time of arrival measurements of the vehicles’ transmitted signals measured at multiple verifying base stations. The work reported here, we believe, paves the way to the actual deployment in real-world conditions of LVSs for emerging vehicular networks.","","","10.1109/JIOT.2019.2937315","Australian Government through its Research Training Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8813056","Artificial intelligence (AI);Internet of Things (IoT);intelligent transportation system (ITS);location verification;neural networks (NNs);vehicular ad-hoc network (VANET)","Artificial neural networks;Global Positioning System;Internet of Things;Protocols;Artificial intelligence;Training;Wireless communication","","","","","43","IEEE","","","","IEEE","IEEE Journals"
"Efficient Variable Rate Image Compression With Multi-Scale Decomposition Network","C. Cai; L. Chen; X. Zhang; Z. Gao","Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","12","3687","3700","While deep learning image compression methods have shown an impressive coding performance, most of them output a single-optimized-compression rate using a trained-specific network. However, in practice, it is essential to support the variable rate compression or meet a target rate with a high-coding performance. This paper proposes a novel image compression method, making it possible for a single convolutional neural network (CNN) model to generate the variable rate efficiently with an optimized rate-distortion (RD) performance. The method consists of CNN-based multi-scale decomposition transform and content adaptive rate allocation. Specifically, the transform network is learned to decompose the input image into several scales of representations while optimizing the RD performance for all scales. Rate allocation algorithms for two typical scenarios are provided to determine the optimal scale of each image block for a given target rate or quality factor. For a target rate, the allocation is adaptive based on content complexity. In addition, for a target quality factor which indicates a tradeoff between the rate and the quality, the optimal scale is determined by minimizing the RD cost. The experimental results have shown that our method has outperformed the JPEG2000 and BPG standards with high efficiency and the state-of-the-art RD performance as measured by the multi-scale structural similarity index metric. Moreover, our method can strictly control the rate to generate the target compression result.","","","10.1109/TCSVT.2018.2880492","Natural Science Foundation of Shanghai; National Natural Science Foundation of China; Shanghai Key Laboratory of Digital Media Processing and Transmission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8531758","Lossy image compression;multi-scale decomposition transform;content adaptive rate allocation;variable rate image compression;convolutional neural network","Image coding;Resource management;Transforms;Transform coding;Codecs;Laplace equations;Standards","","","","","37","IEEE","","","","IEEE","IEEE Journals"
"Characterizing Tree Species of a Tropical Wetland in Southern China at the Individual Tree Level Based on Convolutional Neural Network","Y. Sun; Q. Xin; J. Huang; B. Huang; H. Zhang","School of Geography and Planning, Sun Yat-Sen University, Guangzhou, China; School of Geography and Planning, Sun Yat-Sen University, Guangzhou, China; School of Geography and Planning, Sun Yat-Sen University, Guangzhou, China; Department of Geography and Resource Management, The Chinese University of Hong Kong, Shatin, Hong Kong; Department of Geography, The University of Hong Kong, Pokfulam, Hong Kong","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2019","12","11","4415","4425","Classification of species at the individual tree level would be beneficial to many applications including forest landscape visualization, forest management, and biodiversity monitoring. This article develops a patch-based classification algorithm of individual tree species based on convolutional neural network. The individual trees are first detected using the local maximum method from the canopy height model, as derived from light detection and ranging (LiDAR) data. The detected individual trees are then cropped into patches for classification based on the tree apexes, and three spatial scale image patches are chosen for analysis and discussion. A modified ResNet50 deep network is further employed for the cropped individual tree patches classification. The patch-based method accounts for the contexture information of a tree and does not require the feature selection or the feature reduction processes. About 1388 training samples including Ficus microcarpa Linn. f., Delonix regia, Chorisia speciosa A.St.-Hil., Dimocarpus longan Lour., Musa nana Lour., Carica papaya, and Others (the other tree species except the above six) were collected from both field work and visual interpretation. Aerial images, LiDAR data, and Worldview images were used for the tree species classification. For 362 test tree samples, the results of patch size 64 achieve the best accuracies, and the proposed method outperforms the traditional machine learning method with the overall accuracy of 89.06% + 0.58% using aerial images only. Transferability Study to the Luhu Park also indicated the feasibility of our method. While challenges in individual tree detection and multisource data fusion remain, the solution shows the potential in characterizing tree species at the individual tree level using remote sensing data.","","","10.1109/JSTARS.2019.2950721","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8905783","Aerial images;convolutional neural network (CNN);individual tree level;tree species classification;worldview","","","","","","46","IEEE","","","","IEEE","IEEE Journals"
"Mirror, Mirror, on the Wall, Who’s Got the Clearest Image of Them All?—A Tailored Approach to Single Image Reflection Removal","D. Heydecker; G. Maierhofer; A. I. Aviles-Rivero; Q. Fan; D. Chen; C. Schönlieb; S. Süsstrunk","DAMTP and DPMMS, University of Cambridge, Cambridge, U.K.; DAMTP and DPMMS, University of Cambridge, Cambridge, U.K.; DAMTP and DPMMS, University of Cambridge, Cambridge, U.K.; Computer Science and Technology School, Shandong University, Shandong, China; Microsoft Cloud and AI, Beijing, China; DAMTP and DPMMS, University of Cambridge, Cambridge, U.K.; School of Computer and Communication Sciences, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland","IEEE Transactions on Image Processing","","2019","28","12","6185","6197","Removing reflection artefacts from a single image is a problem of both theoretical and practical interest, which still presents challenges because of the massively ill-posed nature of the problem. In this paper, we propose a technique based on a novel optimization problem. First, we introduce a simple user interaction scheme, which helps minimize information loss in the reflection-free regions. Second, we introduce an H2 fidelity term, which preserves fine detail while enforcing the global color similarity. We show that this combination allows us to mitigate the shortcomings in structure and color preservation, which presents some of the most prominent drawbacks in the existing methods for reflection removal. We demonstrate, through numerical and visual experiments, that our method is able to outperform the state-of-the-art model-based methods and compete with recent deep-learning approaches.","","","10.1109/TIP.2019.2923559","University of Cambridge; Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752269","Reflection suppression;image enhancement;optical reflection","Optimization;Image color analysis;Hardware;Laplace equations;Fans;Visualization;Glass","human computer interaction;image colour analysis;image enhancement;optimisation","optimization problem;information loss minimization;reflection artefacts removal;user interaction scheme;H2 fidelity term;global color similarity;reflection-free regions;single image reflection removal","","","38","","","","","IEEE","IEEE Journals"
"Real-Time Wide-Baseline Place Recognition Using Depth Completion","F. Maffra; L. Teixeira; Z. Chen; M. Chli","Vision for Robotics Laboratory, ETH Zurich, Zurich, Switzerland; Vision for Robotics Laboratory, ETH Zurich, Zurich, Switzerland; Vision for Robotics Laboratory, ETH Zurich, Zurich, Switzerland; Vision for Robotics Laboratory, ETH Zurich, Zurich, Switzerland","IEEE Robotics and Automation Letters","","2019","4","2","1525","1532","Place recognition is an essential capability for robotic autonomy. While ground robots observe the world from generally similar viewpoints over repeated visits, other robots, such as small aircraft, experience far more different viewpoints, requiring place recognition for images captured from very wide baselines. While traditional feature-based methods fail dramatically under extreme viewpoint changes, deep learning approaches demand heavy runtime processing. Driven by the need for cheaper alternatives able to run on computationally restricted platforms, such as small aircraft, this letter proposes a novel real-time pipeline employing depth-completion on sparse feature maps that are anyway computed during robot localization and mapping, to enable place recognition at extreme viewpoint changes. The proposed approach demonstrates unprecedented precision-recall rates on challenging benchmarking and own synthetic and real datasets with up to 45° difference in viewpoints. In particular, our synthetic datasets are, to the best of our knowledge, the first to isolate the challenge of viewpoint changes for place recognition, addressing a crucial gap in the literature. All of the new datasets are publicly available to aid benchmarking.","","","10.1109/LRA.2019.2895826","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629023","Aerial systems: perception and autonomy;visual-based navigation;SLAM;localization;recognition","Three-dimensional displays;Simultaneous localization and mapping;Feature extraction;Pipelines;Image recognition;Visualization","image recognition;mobile robots;robot vision","depth completion;robotic autonomy;ground robots;generally similar viewpoints;aircraft;wide baselines;traditional feature-based methods;extreme viewpoint changes;depth-completion;robot localization;mapping;real-time wide-baseline place recognition;unprecedented precision-recall rates","","2","27","","","","","IEEE","IEEE Journals"
"Oil Rig Recognition Using Convolutional Neural Network on Sentinel-1 SAR Images","L. E. Falqueto; J. A. S. Sá; R. L. Paes; A. Passaro","Aeronautics Institute of Technology, São José dos Campos, PG-CTE, Brazil; Center of Natural Sciences and Technology, Pará State University, Belém, Brazil; IEAv Geointelligence Division, Institute of Advanced Studies, São José dos Campos, Brazil; Virtual Engineering Laboratory, Institute of Advanced Studies and PG-CTE, Aeronautics Institute of Technology, São José dos Campos, Brazil","IEEE Geoscience and Remote Sensing Letters","","2019","16","8","1329","1333","Recent advances in deep learning for automatic target recognition have inspired studies for maritime surveillance with synthetic aperture radar (SAR). In this context, oil rigs monitoring is of particular interest. There are offshore platforms of different types and sizes which poses challenges for classification methods. In this letter, we compare the recognition of oil rigs on Sentinel-1 SAR images, discriminating them from false alarms, through a convolutional neural network with a different number of layers named models VGG-16 and VGG-19. We used the Sentinel-1 GRD product, which has a resolution of about 20 m, with VV and VH channel image data sets separated to observe polarization influences on target recognition. The experiments show that the VH polarization image embedding contributes with the best classification values, with the mean global accuracy of fifty random sampling tests equal to 86.4% and 84.1% using the VGG-16 and VGG-19 architectures, respectively, to extract features and feed a logistic regression classifier method. Results suggest that these approaches are a useful alternative for maritime surveillance employing SAR images of medium resolution and large swaths. Furthermore, the database balancing for wind conditions did not provide significant contributions to the accuracy of classification algorithms.","","","10.1109/LGRS.2019.2894845","Conselho Nacional de Desenvolvimento Científico e Tecnológico; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642911","Convolutional neural network (CNN);image recognition;multilayer neural network;offshore platform;synthetic aperture radar (SAR)","Oils;Synthetic aperture radar;Databases;Feature extraction;Image recognition;Surveillance;Image resolution","feature extraction;image classification;neural nets;radar imaging;regression analysis;remote sensing by radar;synthetic aperture radar","oil rig recognition;convolutional neural network;Sentinel-1 SAR images;automatic target recognition;maritime surveillance;synthetic aperture radar;oil rigs monitoring;classification methods;models VGG-16;Sentinel-1 GRD product;VH polarization image;VGG-19 architectures;logistic regression classifier method;wind conditions","","","20","","","","","IEEE","IEEE Journals"
"Multilevel and Multiscale Network for Single-Image Super-Resolution","Y. Yang; D. Zhang; S. Huang; J. Wu","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Signal Processing Letters","","2019","26","12","1877","1881","In recent years, deep convolutional neural networks (CNNs) have achieved great success in Single Image Super-Resolution (SISR). Most existing networks for Super-Resolution (SR) concentrate on wider or deeper network designs, leading to neglect of the feature correlations of intermediate layers. In this letter, a novel Multilevel and Multiscale Network for SISR (M2SR) is presented. The proposed network framework consists of four parts, including the feature extraction network, the cascade residual U-shaped blocks, the channel-wise attention U-shaped block and the fusion reconstruction network. Specially, the residual U-shaped blocks are designed to extract different scales of features, which are stacked to better refine the multifeatures. Then, to fully exploit the different levels of features, a channel-wise attention U-shaped block (At-U) is proposed to adjust the feature weights, which can adaptively enhance the feature expression and correlation learning. Finally, a fusion reconstruction network is constructed to fuse the different scales of the enhanced features to achieve the reconstructed result. Quantitative and qualitative evaluations of four public datasets show that the proposed method can achieve better performance compared with the state-of-the-art SR methods.","","","10.1109/LSP.2019.2952047","National Natural Science Foundation of China; Natural Science Foundation of Jiangxi Province; Project of the Education Department of Jiangxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8892522","Super-resolution;convolutional neural networks;residual U-shaped blocks;channel-wise attention","Feature extraction;Image reconstruction;Convolution;Correlation;Deconvolution;Kernel","","","","","28","IEEE","","","","IEEE","IEEE Journals"
"A Hybrid R-BILSTM-C Neural Network Based Text Steganalysis","Y. Niu; J. Wen; P. Zhong; Y. Xue","College of Information and Electrical Engineering, China Agricultural University, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China; College of Science, China Agricultural University, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China","IEEE Signal Processing Letters","","2019","26","12","1907","1911","With the emergence of the generation-based steganography, the traditional text steganalysis methods show the unsatisfactory detection performance as the manually extracted features are simple and non-universal. The recently proposed deep learning-based text steganalysis methods can obtain the great detection accuracy by extracting the high-level features. In this letter, a hybrid text steganalysis method (R-BILSTM-C) is proposed through combining the advantages of Bidirectional Long Short Term Memory Recurrent Neural Network (Bi-LSTM) and Convolutional Neural Network (CNN). The proposed method can efficiently capture both local features and long-term semantic information from text to improve the detection accuracy. In the proposed method, the Bi-LSTM architecture is used to capture the long-term semantic information of texts. And the asymmetric convolution kernels with different sizes are applied to extract the local relationship between words. In addition, the high dimensional semantic feature space is visualized. Experimental results show that the proposed method adapts to the different steganographic algorithms efficiently, and achieves the comparable or superior detection performance for the various sentence lengths compared with other state-of-the-art text steganalysis methods.","","","10.1109/LSP.2019.2953953","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8903243","Text steganalysis;Bi-LSTM;CNN;long-term semantic feature;local feature","","","","","","28","IEEE","","","","IEEE","IEEE Journals"
"Point-Cloud-Based Place Recognition Using CNN Feature Extraction","T. Sun; M. Liu; H. Ye; D. Yeung","Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Computer Science, The Hong Kong University of Science and Technology, Hong Kong","IEEE Sensors Journal","","2019","19","24","12175","12186","This paper proposes a novel point-cloud-based place recognition system that adopts a deep learning approach for feature extraction. By using a convolutional neural network pre-trained on color images to extract features from a range image without fine-tuning on extra range images, significant improvement has been observed when compared to using hand-crafted features. The resulting system is illumination invariant, rotation invariant and robust against moving objects that are unrelated to the place identity. Apart from the system itself, we also bring to the community a new place recognition dataset (dataset is available at https://drive.google.com/file/d/1TYga-55gvyMKAaUqX_lxJQr2A5ZAOC00/view?usp=sharing) contai- ning both point cloud and grayscale images covering a full 360° environmental view. In addition, the dataset is organized in such a way that it facilitates experimental validation with respect to rotation invariance or robustness against unrelated moving objects separately.","","","10.1109/JSEN.2019.2937740","Hong Kong University of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8815832","Place recognition;point cloud;CNN","","","","","","61","IEEE","","","","IEEE","IEEE Journals"
"Spectrum Analysis and Convolutional Neural Network for Automatic Modulation Recognition","Y. Zeng; M. Zhang; F. Han; Y. Gong; J. Zhang","Academy for Advanced Interdisciplinary Studies, Southern University of Science and Technology, Shenzhen, China; Shenzhen Engineering Laboratory of Intelligent Information Processing for IoT, Southern University of Science and Technology, Shenzhen, China; Shenzhen Engineering Laboratory of Intelligent Information Processing for IoT, Southern University of Science and Technology, Shenzhen, China; Shenzhen Engineering Laboratory of Intelligent Information Processing for IoT, Southern University of Science and Technology, Shenzhen, China; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China","IEEE Wireless Communications Letters","","2019","8","3","929","932","Recent convolutional neural networks (CNNs)-based image processing methods have proven that CNNs are good at extracting features of spatial data. In this letter, we present a CNN-based modulation recognition framework for the detection of radio signals in communication systems. Since the frequency variation with time is the most important distinction among radio signals with different modulation types, we transform 1-D radio signals into spectrogram images using the short-time discrete Fourier transform. Furthermore, we analyze statistical features of the radio signals and use a Gaussian filter to reduce noise. We compare the proposed CNN framework with two existing methods from literature in terms of recognition accuracy and computational complexity. The experiments show that the proposed CNN architecture with spectrogram images as signal representation achieves better recognition accuracy than existing deep learning-based methods.","","","10.1109/LWC.2019.2900247","National Natural Science Foundation of China; Shenzhen Science and Technology Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643794","Modulation recognition;convolutional neural network;time-frequency analysis;noise reduction","Modulation;Signal to noise ratio;Spectrogram;Feature extraction;Time-frequency analysis;Training;Image recognition","convolutional neural nets;discrete Fourier transforms;signal detection;signal representation;spectral analysis","spectrogram images;short-time discrete Fourier;CNN framework;signal representation;spectrum analysis;convolutional neural network;automatic modulation recognition;CNN-based modulation recognition framework;radio signal detection;image processing methods;feature extraction;spatial data;1D radio signal transform;Gaussian filter;computational complexity","","2","14","","","","","IEEE","IEEE Journals"
"Multi-Level Dual-Attention Based CNN for Macular Optical Coherence Tomography Classification","S. S. Mishra; B. Mandal; N. B. Puhan","School of Electrical Sciences, Indian Institute of Technology of Bhubaneswar, Bhubaneswar, India; School of Computing and Mathematics, Keele University, Newcastle, U.K.; School of Electrical Sciences, Indian Institute of Technology of Bhubaneswar, Bhubaneswar, India","IEEE Signal Processing Letters","","2019","26","12","1793","1797","In this letter, we propose a multi-level dual-attention model to classify two common macular diseases, age-related macular degeneration (AMD) and diabetic macular edema (DME) from normal macular eye conditions using optical coherence tomography (OCT) imaging technique. Our approach unifies the dual-attention mechanism at multi-levels of the pre-trained deep convolutional neural network (CNN). It provides a focused learning mechanism by taking into account both multi-level features based attention focusing on the salient coarser features and self-attention mechanism attending higher entropy regions of the finer features. Our proposed method enables the network to automatically focus on the relevant parts of the input images at different levels of feature subspaces. This leads to a more locally deformation-aware feature generation and classification. The proposed approach does not require pre-processing steps such as extraction of region of interest, denoising, and retinal flattening, making the network more robust and fully automatic. Experimental results on two macular OCT databases show the superior performance of our proposed approach as compared to the current state-of-the-art methodologies.","","","10.1109/LSP.2019.2949388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8882308","Attention mechanism;age-related macular degeneration (AMD);diabetic macular edema (DME);multi-level dual-attention;optical coherence tomography (OCT)","Feature extraction;Integrated circuits;Databases;Diseases;Dams;Retina;Protocols","","","","","34","IEEE","","","","IEEE","IEEE Journals"
"Encryption-Free Framework of Privacy-Preserving Image Recognition for Photo-Based Information Services","K. Nakamura; N. Nitta; N. Babaguchi","Division of Electrical, Electronic, and Information Engineering, Graduate School of Engineering, Osaka University, Suita, Japan; Division of Electrical, Electronic, and Information Engineering, Graduate School of Engineering, Osaka University, Suita, Japan; Division of Electrical, Electronic, and Information Engineering, Graduate School of Engineering, Osaka University, Suita, Japan","IEEE Transactions on Information Forensics and Security","","2019","14","5","1264","1279","Nowadays, mobile devices, such as smartphones, have been widely used all over the world. In addition, the performance of image recognition has drastically increased with deep learning technologies. From these backgrounds, some photo-based information services provided in a client-server architecture are getting popular: client users take a photo of a certain spot and send it to a server, while the server identifies the spot with an image recognizer and returns its related information to the users. However, this kind of client-server image recognition can cause a privacy issue because image recognition results are sometimes privacy-sensitive. To tackle the privacy issue, in this paper, we propose a framework of privacy-preserving image recognition called EnfPire, in which the server cannot uniquely determine the recognition result but client users can do so. An overview of EnfPire is as follows. First, client users extract a visual feature from their taken photo and transform it so that the server cannot uniquely determine the recognition result. Then, the users send the transformed feature to the server that returns a set of candidates of the recognition result to the users. Finally, the users compare the candidates to the original visual feature for obtaining the final result. Our experimental results demonstrate that EnfPire successfully degrades the server's spot-recognition accuracy from 99.8% to 41.4% while keeping 86.9% of the spot-recognition accuracy on the user side.","","","10.1109/TIFS.2018.2876752","Japan Society for the Promotion of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8501982","Privacy protection;image recognition;client-server architectures;feature transformation;photo-based information services","Servers;Image recognition;Visualization;Privacy;Information services;Cryptography;Feature extraction","client-server systems;feature extraction;image recognition;image retrieval","spot-recognition accuracy;EnfPire;privacy-sensitive;privacy issue;client-server image recognition;image recognizer;client users;client-server architecture;photo-based information services;privacy-preserving image recognition;encryption-free framework","","","54","","","","","IEEE","IEEE Journals"
"Introduction to the Special Section on Deep Learning for Visual Surveillance","F. Porikli; L. S. Davis; Q. Wang; Y. Li; C. Regazzoni","The Australian National University, Canberra, ACT, Australia; University of Maryland, College Park, MD, USA; Northwestern Polytechnical University, Xi’an, China; Google Brain, Mountain View, CA, USA; DITEN Department, University of Genoa, Genoa, Italy","IEEE Transactions on Circuits and Systems for Video Technology","","2019","29","9","2535","2537","We are now living in an era of visual information where data is unceasingly generated and pushed into consumption at astounding rates. A remarkable portion of this sensory input comes in the form of videos streaming from large-scale surveillance infrastructures as well as consumer-grade monitoring systems. The sheer amount of ground-based, aerial and mobile video surveillance data demands fittingly competent, accurate, effective techniques to extract useful cues and provide assistance for detection, prevention, and intervention tasks in traffic, safety, security, defense, forensic, health, biology, ethology, and retail space management applications.","","","10.1109/TCSVT.2019.2936084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824173","","Special issues and sections;Surveillance;Deep learning;Object detection;Object recognition;Convolutional neural networks;Feature extraction","","","","","0","","","","","IEEE","IEEE Journals"
"Guest Editorial Special Issue on Discriminative Learning for Model Optimization and Statistical Inference","W. Zuo; X. Peng; L. Shao; D. Prokhorov; H. Bischof","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; College of Computer Science, Sichuan University, Chengdu, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Toyota Research Institute North America, Ann Arbor, MI, USA; Institute for Computer Graphics and Vision Graz University of Technology, Graz, Austria","IEEE Transactions on Neural Networks and Learning Systems","","2019","30","10","2894","2897","Model optimization and statistical inference have played a central role in various applications of computational intelligence, data analytics, and computer vision. Traditional approaches are usually based on model-centric learning. That is, even after model training, it is still required to design proper algorithms and to specify hand-crafted parameters for optimization and inference. Recently, discriminative learning has demonstrated its power for process-centric learning. Taking domain expertise and problem structure into account, problem-specific deep architectures can be formed by unfolding the model inference as an iterative process, and the parameters of the optimization process can then be learned from training data. These solutions are closely related with bilevel optimization, partial differential equation (PDE), as well as meta learning, and can provide new insights into the studies of versatile statistical and optimization models, such as sparse representation, structured regression, and conditional random fields. Moreover, generic deep network architectures are often referred to as “black-box” methods, while discriminative process-centric learning can provide a new perspective for the understanding and development of generic deep architectures. To sum up, connecting discriminative learning with model optimization and inference is not only helpful in analyzing convergence and generalization of deep architectures but also offers new perspectives for understanding and developing generic deep learning models.","","","10.1109/TNNLS.2019.2935802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844388","","Special issues and sections;Inference mechanisms;Optimization;Statistical analysis;Principal component analysis;Linear discriminant analysis;Machine learning;Support vector machines;Recurrent neural networks","","","","","0","","","","","IEEE","IEEE Journals"
"Machine Learning in Medical Imaging","","","IEEE Journal of Biomedical and Health Informatics","","2019","23","4","1361","1362","The papers in this special issue focus on machine learning for use in medical image processing applications. The use of machine learning in this area has become indispensable in diagnosis and treatment of many diseases. With advances in new imaging techniques, the need to take full advantage of abundant images draws more and more attention. Machine learning, including deep learning particularly, provides us a new paradigm to learn and to utilize the overwhelming volume of big imaging data smartly. Nowadays, machine learning in medical imaging has become one of the most promising and growing fields of research. The main aim of this special issue is to help advance the scientific research within the broad field of machine learning in medical imaging. The special issue was planned in conjunction with the International Workshop on Machine Learning in Medical Imaging (MLMI) 2017.","","","10.1109/JBHI.2019.2920801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752497","","Special issues and sections;Medical image processing;Biomedical imaging;Image segmentation;Deep learning;Computer architecture;Microprocessors;Machine learning;Meetings","","","","","0","","","","","IEEE","IEEE Journals"
"Guest Editors’ Introduction to the Special Section on Compact and Efficient Feature Representation and Learning in Computer Vision","L. Liu; M. Pietikäinen; J. Chen; G. Zhao; X. Wang; R. Chellappa","NA; NA; NA; NA; NA; NA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2019","41","10","2287","2290","The papers in this special section examine compact and efficient feature representation and learning in computer vision. ","","","10.1109/TPAMI.2019.2935426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824188","","Special issues and sections;Computer vision;Deep learning;Neural networks;Computational efficiency;Feature extraction;Image classification","","","","","0","","","","","IEEE","IEEE Journals"
"Guest Editorial: AI Enabled Connected Health Informatics","S. Zarar; G. Tourassi; C. Nugent","Microsoft Research, Redmond, WA, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Ulster University, Newtownabbey, U.K.","IEEE Journal of Biomedical and Health Informatics","","2019","23","3","921","922","The articles in this special section provide a snapshot of the latest research advancements in all aspects of connected health and informatic systems where artificial intelligence has been evident, including sensing, transfer, storage and analytics of biomedical data. These articles capture the end-to-end view of solutions that use automated informatics to address single ormultiple scenarios of health engineering such as primary care, preventive care, predictive technologies, hospitalization, home care, and occupational health.","","","10.1109/JBHI.2019.2910291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8705631","","Special issues and sections;Informatics;Biomedical imaging;Computational modeling;Deep learning;Analytical models;Bioinformatics;Gait analysis;Cloud computing","","","","","0","","","","","IEEE","IEEE Journals"
"Guest Editorial Special Issue on AI Enabled Cognitive Communication and Networking for IoT","K. Yang; S. Liu; L. Cai; Y. Yilmaz; P. Chen; A. Walid","Department of Computer Science, Tongji University, Shanghai, China; MIT–IBM Watson AI Lab IBM Research, Yorktown Heights, NY, USA; Department of Electrical and Computer Engineering, University of Victoria, Victoria, Canada; Electrical Engineering Department, University of South Florida, Tampa, FL, USA; AI Foundations Learning Group IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Nokia Bell Labs, Murray Hill, NJ, USA","IEEE Internet of Things Journal","","2019","6","2","1906","1910","As we enter the Internet of Things (IoT) era in which the communication network is becoming increasingly dynamic, heterogeneous, and complex, it is desirable to have cognitive communication systems and networks that possess multiple interacting capabilities for situation assessment, resource management, online/distributed learning, big-data processing, and intelligent decision making. AI techniques, such as deep learning, probabilistic graph model, and reinforcement learning, aided with big data and IoT, provide a wide variety of tools and solutions to many new problems encountered in the design, operation, and optimization of cognitive communication systems and networking, including resource management, situation assessment, channel identification, anomaly detection, root cause analysis, and online/distributed learning.","","","10.1109/JIOT.2019.2908443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8709909","","","","","","","0","","","","","IEEE","IEEE Journals"
"Special Issue on Artificial Intelligence and Machine Learning for Networking and Communications","P. Chemouil; P. Hui; W. Kellerer; Y. Li; R. Stadler; D. Tao; Y. Wen; Y. Zhang","Convergent Network Control Laboratory, Orange Labs, Châtillon, France; Department of Computer Science, University of Helsinki, Helsinki, Finland; Department of Electrical and Computer Engineering, Technical University of Munich, Munich, Germany; Department of Electronic Engineering, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering and IT, The University of Sydney, Sydney, NSW, Australia; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Network Infrastructure, Facebook, Menlo Park, CA, USA","IEEE Journal on Selected Areas in Communications","","2019","37","6","1185","1191","Research in large-scale networking systems has been shaped and will continue to be guided by specific characteristics of applications and the underlying platforms and infrastructures. On the one hand, applications are growing at an accelerated pace, which is fundamentally unpredictable in both breadth and depth. On the other hand, the underlying networking has been the focus of a huge transformation enabled by new models resulting from virtualization and cloud computing. This has led to a number of novel architectures supported by emerging technologies such as Software-Defined Networking (SDN), Network Function Virtualization (NFV), and more recently, edge cloud and fog networking, or network slicing [1], [2]. This evolution towards enhanced design opportunities along with increasing complexity in networking and its applications has fueled the need for improved network automation in agile infrastructures. At the same time, their complexity has dramatically increased. The networking dynamics have had the effect of making it even more important and challenging to design scalable network measurement and analysis techniques and associated tools. Critical applications such as resource allocation, network monitoring, security enforcement, or dynamic network management require real-time mechanisms for online analysis as well as efficient techniques for offline deep analysis of massive historical data.","","","10.1109/JSAC.2019.2909076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715830","","Special issues and sections;Artificial intelligence;Machine learning;Communication networks;Wireless networks;Decision making","","","","1","25","","","","","IEEE","IEEE Journals"
"Guest Editorial Introduction to the Special Issue on Intelligent Transportation Systems Empowered by AI Technologies","S. Kong; Y. Lv; H. L. Vu; J. Cano; J. Choi; D. Kum; B. T. Morris","Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; Chinese Academy of Sciences, Beijing, China; Monash University, Melbourne, VIC, Australia; Universitat Politecnica de Valencia, Valencia, Spain; Hanyang University, Seoul, South Korea; Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; University of Nevada, Las Vegas, NV, USA","IEEE Transactions on Intelligent Transportation Systems","","2019","20","10","3765","3770","There has been an increasing level of demand for faster, safer and greener transportation systems with higher levels of capacity and convenience, though the implementation of transportation systems overall is often restricted by geographical limitations, presenting a challenge to scientists and engineers in the field. However, we have been witnessing the evolution of the transportation systems over the last few decades, and at present we are facing a new era of intelligent transportation systems (ITS) empowered by artificial intelligence (AI) technologies. There have been classification, deep learning, and reinforcement learning techniques, to name a few, which collectively have enabled almost all technical elements of the ITS. For example, autonomous vehicle technologies are now mature enough to introduce self-driving cars, taxis, buses, and trucks on the roads and streets; traffic signals are controlled by AI-based systems for far more enhanced traffic efficiency; and machine learning based on big data is improving the operational performance of transportation systems to the next level of safety, efficiency, and sustainability.","","","10.1109/TITS.2019.2940856","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854960","","","","","","","0","","","","","IEEE","IEEE Journals"
"Guest Editorial Embedded and Networked Systems for Intelligent Vehicles and Robots","","","IEEE Transactions on Industrial Informatics","","2019","15","2","1035","1037","The papers in this special section focus on embedded and networked systems for intelligent vehicles and robots. Embedded and networked systems for intelligent vehicles and robots are expected to have a significant economic, societal, and technological impact on industrial and automotive applications. Among the aspects that will benefit from these technologies the first one is safety, thanks to the reduction of accidents caused by human errors. Another positive effect is expected on sustainability, thanks to the increase in transport systems efficiency. Comfort and inclusiveness will be also improved, ensuring users’ freedom for other activities and “mobility for all.” Logistics and factory automation are among the main areas that will take advantages from intelligent vehicles and robots, that are expected to play a key role in Industry 4.0 scenarios, the so-called fourth industrial revolution, where intelligent vehicles and industrial robots will move and operate autonomously and cooperatively. Such a revolution has many key enabling technologies, such as, networked sensors, actuators, and embedded computing and control platforms, that will be distributed on-board the vehicle/robot. The contribution of artificial intelligence and deep learning computing platforms is also emerging to achieve full intelligent autonomous mobility of vehicles and robots.","","","10.1109/TII.2018.2886529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633998","","Special issues and sections;Service robots;Embedded systems;Autonomous vehicles;Robot sensing;Safety;Manufacturing automation;Intelligent vehicles","","","","","0","","","","","IEEE","IEEE Journals"
"Guest Editorial Nature-Inspired Approaches for IoT and Big Data","A. H. Gandomi; M. Daneshmand; R. Jha; D. Kaur; H. Ning; C. Robinson; H. Schilling","Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; School of Business, Stevens Institute of Technology, Hoboken, NJ, USA; Electrical Engineering and Computing Systems Department, University of Cincinnati, Cincinnati, OH, USA; Electrical Engineering and Computer Science Engineering, University of Toledo, Toledo, OH, USA; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; Scientific Computing and Visualization NASA Glenn Research Center, Cleveland, OH, USA; Scientific Computing and Visualization NASA Glenn Research Center, Cleveland, OH, USA","IEEE Internet of Things Journal","","2019","6","6","9213","9216","Nature-inspired approaches have been widely used for different purposes over the last two decades and are still extensively researched, especially for complex real-world problems. Biological systems, or nature in general, serve as the source of the intelligence of nature-inspired approaches. The efficiency of nature-inspired approaches is due to their significant ability to imitate the best features of nature that evolved by natural selection over millions of years. These approaches have been successfully used for Internet of Things (IoT) and big data handling and relevant examples of these topics may be artificial neural networks (ANNs) and deep learning applications. On this basis, the main theme of this special issue (SI) addresses recent advances in the use of the nature-inspired approaches for IoT and big data problems.","","","10.1109/JIOT.2019.2947754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931289","","","","","","","0","IEEE","","","","IEEE","IEEE Journals"
"Guest Editorial: Soft Computing Applications for Novel and Upcoming Distributed and Parallel Systems From Cloud Computing and Beyond","","","IEEE Transactions on Industrial Informatics","","2019","15","10","5646","5647","The articles in this special issue aim at collecting contributions focused on the application of soft computing, including the areas of fuzzy logic, neural networks, evolutionary computing, rough sets, and other similar techniques, in novel distributed and/or parallel systems, where cloud computing is one of the widely known examples, in order to bring intelligence to all architectural layers for data processing, routing, etc. Brief summaries are provided for the included articles.","","","10.1109/TII.2019.2937918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8815946","","Special issues and sections;Cloud computing;Evolutionary computation;Edge computing;Neural networks;Fuzzy logic;Deep learning","","","","","0","Traditional","","","","IEEE","IEEE Journals"
