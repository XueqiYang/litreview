"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,"Reference Count","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"A Deep Neural Network-Driven Feature Learning Method for Multi-view Facial Expression Recognition","T. Zhang; W. Zheng; Z. Cui; Y. Zong; J. Yan; K. Yan","Key Laboratory of Child Development and Learning Science (Southeast University), Ministry of Education, Research Center for Learning Science, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science (Southeast University), Ministry of Education, Research Center for Learning Science, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science (Southeast University), Ministry of Education, Research Center for Learning Science, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science (Southeast University), Ministry of Education, Research Center for Learning Science, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science (Southeast University), Ministry of Education, Research Center for Learning Science, Southeast University, Nanjing, China; Key Laboratory of Child Development and Learning Science (Southeast University), Ministry of Education, Research Center for Learning Science, Southeast University, Nanjing, China","IEEE Transactions on Multimedia","","2016","18","12","2528","2536","In this paper, a novel deep neural network (DNN)-driven feature learning method is proposed and applied to multi-view facial expression recognition (FER). In this method, scale invariant feature transform (SIFT) features corresponding to a set of landmark points are first extracted from each facial image. Then, a feature matrix consisting of the extracted SIFT feature vectors is used as input data and sent to a well-designed DNN model for learning optimal discriminative features for expression classification. The proposed DNN model employs several layers to characterize the corresponding relationship between the SIFT feature vectors and their corresponding high-level semantic information. By training the DNN model, we are able to learn a set of optimal features that are well suitable for classifying the facial expressions across different facial views. To evaluate the effectiveness of the proposed method, two nonfrontal facial expression databases, namely BU-3DFE and Multi-PIE, are respectively used to testify our method and the experimental results show that our algorithm outperforms the state-of-the-art methods.","","","10.1109/TMM.2016.2598092","National Basic Research Program of China; National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530823","Deep neural network (DNN);multi-view facial expression recognition;scale invariant feature transform (SIFT)","Feature extraction;Face recognition;Facial features;Neural networks;Learning systems;Convolution;Semantics","emotion recognition;face recognition;feature extraction;image classification;learning (artificial intelligence);neural nets","deep neural network-driven feature learning method;multiview facial expression recognition;DNN-driven feature learning method;FER;scale invariant feature transform;SIFT feature extraction;feature matrix;discriminative feature learning;expression classification;high-level semantic information;BU-3DFE database;Multi-PIE database","","71","34","","","","","IEEE","IEEE Journals"
"Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing","F. C. Ghesu; E. Krubasik; B. Georgescu; V. Singh; Y. Zheng; J. Hornegger; D. Comaniciu","Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA; Robotics and Embedded Systems Department, Technische Universität München, München, Germany; Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA; Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA; Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA; Friedrich-Alexander-Universität Erlangen-Nürnberg, Pattern Recognition Lab, Erlangen, Germany; Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA","IEEE Transactions on Medical Imaging","","2016","35","5","1217","1228","Robust and fast solutions for anatomical object detection and segmentation support the entire clinical workflow from diagnosis, patient stratification, therapy planning, intervention and follow-up. Current state-of-the-art techniques for parsing volumetric medical image data are typically based on machine learning methods that exploit large annotated image databases. Two main challenges need to be addressed, these are the efficiency in scanning high-dimensional parametric spaces and the need for representative image features which require significant efforts of manual engineering. We propose a pipeline for object detection and segmentation in the context of volumetric image parsing, solving a two-step learning problem: anatomical pose estimation and boundary delineation. For this task we introduce Marginal Space Deep Learning (MSDL), a novel framework exploiting both the strengths of efficient object parametrization in hierarchical marginal spaces and the automated feature design of Deep Learning (DL) network architectures. In the 3D context, the application of deep learning systems is limited by the very high complexity of the parametrization. More specifically 9 parameters are necessary to describe a restricted affine transformation in 3D, resulting in a prohibitive amount of billions of scanning hypotheses. The mechanism of marginal space learning provides excellent run-time performance by learning classifiers in clustered, high-probability regions in spaces of gradually increasing dimensionality. To further increase computational efficiency and robustness, in our system we learn sparse adaptive data sampling patterns that automatically capture the structure of the input. Given the object localization, we propose a DL-based active shape model to estimate the non-rigid object boundary. Experimental results are presented on the aortic valve in ultrasound using an extensive dataset of 2891 volumes from 869 patients, showing significant improvements of up to 45.2% over the state-of-the-art. To our knowledge, this is the first successful demonstration of the DL potential to detection and segmentation in full 3D data with parametrized representations.","","","10.1109/TMI.2016.2538802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426845","Deep learning;image parsing;marginal space learning;sparse representations;three-dimensional (3D) object detection and segmentation","Three-dimensional displays;Machine learning;Shape;Image segmentation;Context;Robustness;Feature extraction","biomedical ultrasonics;feature extraction;image classification;image sampling;image segmentation;learning (artificial intelligence);medical image processing;pattern clustering;probability;ultrasonic imaging","marginal space deep learning;anatomical object detection;segmentation support;clinical workflow;diagnosis;patient stratification;therapy planning;volumetric medical image data parsing;machine learning methods;annotated image databases;scanning high-dimensional parametric spaces;representative image features;two-step learning problem;anatomical pose estimation;boundary delineation;object parametrization;hierarchical marginal spaces;automated feature design;deep learning network architectures;3D context;deep learning systems;restricted affine transformation;scanning hypotheses;run-time performance;learning classifiers;clustered high-probability regions;computational efficiency;sparse adaptive data sampling patterns;object localization;DL-based active shape model;nonrigid object boundary;aortic valve;ultrasound;extensive dataset;full 3D data segmentation;full 3D data detection;parametrized representations","Algorithms;Aortic Valve;Databases, Factual;Echocardiography, Transesophageal;Humans;Image Processing, Computer-Assisted;Machine Learning;Neural Networks (Computer);Pattern Recognition, Automated","52","46","","","","","IEEE","IEEE Journals"
"Deep Ranking for Person Re-Identification via Joint Representation Learning","S. Chen; C. Guo; J. Lai","School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Guangdong Province Key Laboratory of Information Security, Guangzhou, China","IEEE Transactions on Image Processing","","2016","25","5","2353","2367","This paper proposes a novel approach to person re-identification, a fundamental task in distributed multi-camera surveillance systems. Although a variety of powerful algorithms have been presented in the past few years, most of them usually focus on designing hand-crafted features and learning metrics either individually or sequentially. Different from previous works, we formulate a unified deep ranking framework that jointly tackles both of these key components to maximize their strengths. We start from the principle that the correct match of the probe image should be positioned in the top rank within the whole gallery set. An effective learning-to-rank algorithm is proposed to minimize the cost corresponding to the ranking disorders of the gallery. The ranking model is solved with a deep convolutional neural network (CNN) that builds the relation between input image pairs and their similarity scores through joint representation learning directly from raw image pixels. The proposed framework allows us to get rid of feature engineering and does not rely on any assumption. An extensive comparative evaluation is given, demonstrating that our approach significantly outperforms all the state-of-the-art approaches, including both traditional and CNN-based methods on the challenging VIPeR, CUHK-01, and CAVIAR4REID datasets. In addition, our approach has better ability to generalize across datasets without fine-tuning.","","","10.1109/TIP.2016.2545929","National Natural Science Foundation of China; Guangdong Program; Guangzhou Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439828","Person re-identification;deep convolutional neural network;learning to rank;Person re-identification;deep convolutional neural network;learning to rank","Measurement;Feature extraction;Image color analysis;Machine learning;Cameras;Algorithm design and analysis;Probes","image representation;learning (artificial intelligence);neural nets","person re-identification;distributed multicamera surveillance systems;joint representation learning;unified deep ranking framework;learning-to-rank algorithm;deep convolutional neural network;CNN;raw image pixels;feature engineering","Algorithms;Biometric Identification;Databases, Factual;Humans;Machine Learning;Neural Networks (Computer)","94","53","","","","","IEEE","IEEE Journals"
"Deep Transfer Metric Learning","J. Hu; J. Lu; Y. Tan; J. Zhou","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Automation, Tsinghua University, Beijing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","","2016","25","12","5576","5588","Conventional metric learning methods usually assume that the training and test samples are captured in similar scenarios so that their distributions are assumed to be the same. This assumption does not hold in many real visual recognition applications, especially when samples are captured across different data sets. In this paper, we propose a new deep transfer metric learning (DTML) method to learn a set of hierarchical nonlinear transformations for cross-domain visual recognition by transferring discriminative knowledge from the labeled source domain to the unlabeled target domain. Specifically, our DTML learns a deep metric network by maximizing the inter-class variations and minimizing the intra-class variations, and minimizing the distribution divergence between the source domain and the target domain at the top layer of the network. To better exploit the discriminative information from the source domain, we further develop a deeply supervised transfer metric learning (DSTML) method by including an additional objective on DTML, where the output of both the hidden layers and the top layer are optimized jointly. To preserve the local manifold of input data points in the metric space, we present two new methods, DTML with autoencoder regularization and DSTML with autoencoder regularization. Experimental results on face verification, person re-identification, and handwritten digit recognition validate the effectiveness of the proposed methods.","","","10.1109/TIP.2016.2612827","National Key Research and Development Program of China; National Natural Science Foundation of China; National 1000 Young Talents Plan Program; National Basic Research Program of China; Ministry of Education of China; Tsinghua University Initiative Scientific Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574389","Deep metric learning;deep transfer metric learning;transfer learning;face verification;person reidentification","Measurement;Training;Learning systems;Visualization;Machine learning;Neural networks;Manifolds","biometrics (access control);face recognition;handwritten character recognition;learning (artificial intelligence)","deep transfer metric learning method;hierarchical nonlinear transformations;cross-domain visual recognition;discriminative knowledge transfer;labeled source domain;unlabeled target domain;deep metric network;interclass variation maximization;intraclass variation minimization;distribution divergence minimization;deeply supervised transfer metric learning method;DTML-with-autoencoder regularization method;DSTML-with-autoencoder regularization method;face verification;person reidentification;handwritten digit recognition","","26","57","","","","","IEEE","IEEE Journals"
"Extreme Learning Machine for Multilayer Perceptron","J. Tang; C. Deng; G. Huang","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Neural Networks and Learning Systems","","2016","27","4","809","821","Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via ℓ1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme.","","","10.1109/TNNLS.2015.2424995","Excellent Young Scholars Research Fund of Beijing Institute of Technology; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7103337","Deep learning (DL);deep neural network (DNN);extreme learning machine (ELM);multilayer perceptron (MLP);random feature mapping.;Deep learning (DL);deep neural network (DNN);extreme learning machine (ELM);multilayer perceptron (MLP);random feature mapping","Feature extraction;Training;Nonhomogeneous media;Optimization;Least squares approximations;Artificial neural networks","feedforward neural nets;learning (artificial intelligence);multilayer perceptrons;pattern classification","extreme learning machine;multilayer perceptron;learning algorithm;generalized single hidden layer feedforward neural networks;feature learning;ELM-based hierarchical learning framework;self-taught feature extraction;supervised feature classification;random initialized hidden weights;unsupervised multilayer encoding;ELM-based sparse autoencoder;feature representations;ELM random feature mapping;hierarchically encoded outputs;decision making;learning speed;greedy layerwise training;deep learning;DL;hierarchical learning methods;computer vision","","436","33","","","","","IEEE","IEEE Journals"
"Deep Computation Model for Unsupervised Feature Learning on Big Data","Q. Zhang; L. T. Yang; Z. Chen","School of Software Technology, Dalian University of Technology, Dalian, China; Department of Mathematics, Statistics and Computer Science, St. Francis Xavier University, Antigonish, NS, Canada; School of Software Technology, Dalian University of Technology, Dalian, China","IEEE Transactions on Services Computing","","2016","9","1","161","171","Deep learning has been successfully applied to feature learning in speech recognition, image classification and language processing. However, current deep learning models work in the vector space, resulting in the failure to learn features for big data since a vector cannot model the highly non-linear distribution of big data, especially heterogeneous data. This paper proposes a deep computation model for feature learning on big data, which uses a tensor to model the complex correlations of heterogeneous data. To fully learn the underlying data distribution, the proposed model uses the tensor distance as the average sum-of-squares error term of the reconstruction error in the output layer. To train the parameters of the proposed model, the paper designs a high-order back-propagation algorithm (HBP) by extending the conventional back-propagation algorithm from the vector space to the high-order tensor space. To evaluate the performance of the proposed model, we carried out the experiments on four representative datasets by comparison with stacking auto-encoders and multimodal deep learning models. Experimental results clearly demonstrate that the proposed model is efficient to perform feature learning when evaluated using the STL-10, CUAVE, SANE and INEX datasets.","","","10.1109/TSC.2015.2497705","National Sciences and Engineering Research Council; Canada Foundation for Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7317793","Big Data;Deep learning model;Tensor auto-encoder;Back-propagation algorithm;Feature learning;Big data;deep learning model;tensor auto-encoder;back-propagation algorithm;feature  learning","Tensile stress;Data models;Big data;Machine learning;Computational modeling;Correlation;Stacking","backpropagation;Big Data;distributed databases;tensors;unsupervised learning","speech recognition;image classification;language processing;deep learning models;Big Data;deep computation model;complex correlations;heterogeneous data;data distribution;tensor distance;average sum-of-squares error term;reconstruction error;high-order backpropagation algorithm;HBP;vector space;high-order tensor space;STL-10 datasets;INEX datasets;SANE datasets;CUAVE datasets;unsupervised feature learning","","29","33","","","","","IEEE","IEEE Journals"
"Why Deep Learning Works: A Manifold Disentanglement Perspective","P. P. Brahma; D. Wu; Y. She","Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA; Department of Statistics, Florida State University, Tallahassee, FL, USA","IEEE Transactions on Neural Networks and Learning Systems","","2016","27","10","1997","2008","Deep hierarchical representations of the data have been found out to provide better informative features for several machine learning applications. In addition, multilayer neural networks surprisingly tend to achieve better performance when they are subject to an unsupervised pretraining. The booming of deep learning motivates researchers to identify the factors that contribute to its success. One possible reason identified is the flattening of manifold-shaped data in higher layers of neural networks. However, it is not clear how to measure the flattening of such manifold-shaped data and what amount of flattening a deep neural network can achieve. For the first time, this paper provides quantitative evidence to validate the flattening hypothesis. To achieve this, we propose a few quantities for measuring manifold entanglement under certain assumptions and conduct experiments with both synthetic and real-world data. Our experimental results validate the proposition and lead to new insights on deep learning.","","","10.1109/TNNLS.2015.2496947","National Science Foundation; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348689","Deep learning;disentanglement;manifold learning;unsupervised feature transformation","Manifolds;Neural networks;Machine learning;Nonhomogeneous media;Data models;Kernel;Principal component analysis","learning (artificial intelligence);neural nets","deep learning works;disentanglement perspective;deep hierarchical representations;machine learning applications;multilayer neural networks;unsupervised pretraining;manifold shaped data flattening","","46","29","","","","","IEEE","IEEE Journals"
"Traffic signal timing via deep reinforcement learning","L. Li; Y. Lv; F. Wang","Department of Automation, Tsinghua University, Beijing 100084, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China","IEEE/CAA Journal of Automatica Sinica","","2016","3","3","247","254","In this paper, we propose a set of algorithms to design signal timing plans via deep reinforcement learning. The core idea of this approach is to set up a deep neural network (DNN) to learn the Q-function of reinforcement learning from the sampled traffic state/control inputs and the corresponding traffic system performance output. Based on the obtained DNN, we can find the appropriate signal timing policies by implicitly modeling the control actions and the change of system states. We explain the possible benefits and implementation tricks of this new approach. The relationships between this new approach and some existing approaches are also carefully discussed.","","","10.1109/JAS.2016.7508798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508798","Traffic control;reinforcement learning;deep learning;deep reinforcement learning","Learning (artificial intelligence);Timing;Neural networks;Mathematical model;Optimization;Analytical models;Machine learning","learning (artificial intelligence);neural nets;road traffic control;traffic engineering computing","traffic signal timing;deep reinforcement learning;deep neural network;DNN;Q-function;traffic system performance output;signal timing policies","","34","","","","","","IEEE","IEEE Journals"
"AggNet: Deep Learning From Crowds for Mitosis Detection in Breast Cancer Histology Images","S. Albarqouni; C. Baur; F. Achilles; V. Belagiannis; S. Demirci; N. Navab","Chair for Computer Aided Medical Procedure (CAMP), Technische Universit&#x00E4;t M&#x00FC;nchen (TUM), Munich, Germany; Chair for Computer Aided Medical Procedure (CAMP), Technische Universit&#x00E4;t M&#x00FC;nchen (TUM), Munich, Germany; Chair for Computer Aided Medical Procedure (CAMP), Technische Universit&#x00E4;t M&#x00FC;nchen (TUM), Munich, Germany; Chair for Computer Aided Medical Procedure (CAMP), Technische Universit&#x00E4;t M&#x00FC;nchen (TUM); Chair for Computer Aided Medical Procedure (CAMP), Technische Universit&#x00E4;t M&#x00FC;nchen (TUM), Munich, Germany; Chair for Computer Aided Medical Procedure (CAMP), Technische Universit&#x00E4;t M&#x00FC;nchen (TUM), Munich, Germany","IEEE Transactions on Medical Imaging","","2016","35","5","1313","1321","The lack of publicly available ground-truth data has been identified as the major challenge for transferring recent developments in deep learning to the biomedical imaging domain. Though crowdsourcing has enabled annotation of large scale databases for real world images, its application for biomedical purposes requires a deeper understanding and hence, more precise definition of the actual annotation task. The fact that expert tasks are being outsourced to non-expert users may lead to noisy annotations introducing disagreement between users. Despite being a valuable resource for learning annotation models from crowdsourcing, conventional machine-learning methods may have difficulties dealing with noisy annotations during training. In this manuscript, we present a new concept for learning from crowds that handle data aggregation directly as part of the learning process of the convolutional neural network (CNN) via additional crowdsourcing layer (AggNet). Besides, we present an experimental study on learning from crowds designed to answer the following questions. 1) Can deep CNN be trained with data collected from crowdsourcing? 2) How to adapt the CNN to train on multiple types of annotation datasets (ground truth and crowd-based)? 3) How does the choice of annotation and aggregation affect the accuracy? Our experimental setup involved Annot8, a self-implemented web-platform based on Crowdflower API realizing image annotation tasks for a publicly available biomedical image database. Our results give valuable insights into the functionality of deep CNN learning from crowd annotations and prove the necessity of data aggregation integration.","","","10.1109/TMI.2016.2528120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405343","Aggregation;crowdsourcing;deep learning;gamification;online learning","Biomedical imaging;Crowdsourcing;Noise measurement;Machine learning;Computational modeling;Robustness;Data models","biological organs;cancer;data aggregation;image classification;image denoising;learning (artificial intelligence);medical image processing","crowd sourcing layer;crowd annotation datasets;biomedical image database;image annotation tasks;crowd flower API;self-implemented web-platform;deep CNN learning;convolutional neural network;learning process;data aggregation;noisy annotations;conventional machine-learning methods;learning annotation models;ground-truth data;breast cancer histology imaging;mitosis detection","Breast Neoplasms;Crowdsourcing;Female;Histocytochemistry;Humans;Image Interpretation, Computer-Assisted;Internet;Machine Learning;Mitosis;Neural Networks (Computer);Video Games","153","38","","","","","IEEE","IEEE Journals"
"Deep Learning of Transferable Representation for Scalable Domain Adaptation","M. Long; J. Wang; Y. Cao; J. Sun; P. S. Yu","School of Software, Tsinghua National Laboratory for Information Science and Techonolgy (TNList), Tsinghua University, Beijing, China; School of Software, Tsinghua National Laboratory for Information Science and Techonolgy (TNList), Tsinghua University, Beijing, China; School of Software, Tsinghua National Laboratory for Information Science and Techonolgy (TNList), Tsinghua University, Beijing, China; School of Software, Tsinghua National Laboratory for Information Science and Techonolgy (TNList), Tsinghua University, Beijing, China; Institute for Data Science, Tsinghua University, Haidian, Beijing, China","IEEE Transactions on Knowledge and Data Engineering","","2016","28","8","2027","2040","Domain adaptation generalizes a learning model across source domain and target domain that are sampled from different distributions. It is widely applied to cross-domain data mining for reusing labeled information and mitigating labeling consumption. Recent studies reveal that deep neural networks can learn abstract feature representation, which can reduce, but not remove, the cross-domain discrepancy. To enhance the invariance of deep representation and make it more transferable across domains, we propose a unified deep adaptation framework for jointly learning transferable representation and classifier to enable scalable domain adaptation, by taking the advantages of both deep learning and optimal two-sample matching. The framework constitutes two inter-dependent paradigms, unsupervised pre-training for effective training of deep models using deep denoising autoencoders, and supervised fine-tuning for effective exploitation of discriminative information using deep neural networks, both learned by embedding the deep representations to reproducing kernel Hilbert spaces (RKHSs) and optimally matching different domain distributions. To enable scalable learning, we develop a linear-time algorithm using unbiased estimate that scales linearly to large samples. Extensive empirical results show that the proposed framework significantly outperforms state of the art methods on diverse adaptation tasks: sentiment polarity prediction, email spam filtering, newsgroup content categorization, and visual object recognition.","","","10.1109/TKDE.2016.2554549","National Natural Science Foundation of China; China Postdoctoral Science Foundation; National Science and Technology Supporting Program; NSF; Tsinghua National Laboratory (TNList) Special Fund for Big Data Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7452659","Domain adaptation;deep learning;denoising autoencoder;neural network;two-sample test;multiple kernel learning","Kernel;Machine learning;Adaptation models;Neural networks;Noise reduction;Labeling;Object recognition","data mining;learning (artificial intelligence);neural nets","deep learning;transferable representation learning;scalable domain adaptation;cross-domain data mining;labeled information reuse;labeling consumption mitigation;deep neural networks;abstract feature representation learning;cross-domain discrepancy;optimal two-sample matching;deep denoising autoencoders;reproducing kernel Hilbert spaces;RKHS;scalable learning;linear-time algorithm;sentiment polarity prediction task;email spam filtering task;newsgroup content categorization task;visual object recognition task","","36","49","","","","","IEEE","IEEE Journals"
"A theoretical framework for deep transfer learning","T. Galanti; L. Wolf; T. Hazan","NA; NA; NA","Information and Inference: A Journal of the IMA","","2016","5","2","159","209","We generalize the notion of PAC learning to include transfer learning. In our framework, the linkage between the source and the target tasks is a result of having the sample distribution of all classes drawn from the same distribution of distributions, and by restricting all source and a target concepts to belong to the same hypothesis subclass. We have two models: an adversary model and a randomized model. In the adversary model, we show that for binary classification, conventional PAC-learning is equivalent to the new notion of PAC-transfer and to transfer generalization of the VC-dimension. For regression, we show that PAC-transferability may exist even in the absence of PAC-learning. In both adversary and randomized models, we provide PAC-Bayesian and VC-style generalization bounds to transfer learning. In the randomized model, we provide bounds specifically derived for Deep Learning. A wide discussion on the tradeoffs between the different involved parameters in the bounds is provided. We demonstrate both cases in which transfer does not reduce the sample size (‘trivial transfer’) and cases in which the sample size is reduced (‘non-trivial transfer’).","","","10.1093/imaiai/iaw008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8132288","transfer learning;PAC learning;PAC-Bayesian;deep learning","","","","","1","","","","","","OUP","OUP Journals"
"Cosaliency Detection Based on Intrasaliency Prior Transfer and Deep Intersaliency Mining","D. Zhang; J. Han; J. Han; L. Shao","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; Department of Computer Science and Digital Technologies, Northumbria University, Newcastle upon Tyne, U.K.; Department of Computer Science and Digital Technologies, Northumbria University, Newcastle upon Tyne, U.K.","IEEE Transactions on Neural Networks and Learning Systems","","2016","27","6","1163","1176","As an interesting and emerging topic, cosaliency detection aims at simultaneously extracting common salient objects in multiple related images. It differs from the conventional saliency detection paradigm in which saliency detection for each image is determined one by one independently without taking advantage of the homogeneity in the data pool of multiple related images. In this paper, we propose a novel cosaliency detection approach using deep learning models. Two new concepts, called intrasaliency prior transfer and deep intersaliency mining, are introduced and explored in the proposed work. For the intrasaliency prior transfer, we build a stacked denoising autoencoder (SDAE) to learn the saliency prior knowledge from auxiliary annotated data sets and then transfer the learned knowledge to estimate the intrasaliency for each image in cosaliency data sets. For the deep intersaliency mining, we formulate it by using the deep reconstruction residual obtained in the highest hidden layer of a self-trained SDAE. The obtained deep intersaliency can extract more intrinsic and general hidden patterns to discover the homogeneity of cosalient objects in terms of some higher level concepts. Finally, the cosaliency maps are generated by weighted integration of the proposed intrasaliency prior, deep intersaliency, and traditional shallow intersaliency. Comprehensive experiments over diverse publicly available benchmark data sets demonstrate consistent performance gains of the proposed method over the state-of-the-art cosaliency detection methods.","","","10.1109/TNNLS.2015.2495161","National Natural Science Foundation of China; Doctorate Foundation through Northwestern Polytechnical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7327212","Cosaliency detection;deep learning;prior transfer;stacked denoising autoencoder (SDAE).;Cosaliency detection;deep learning;prior transfer;stacked denoising autoencoder (SDAE)","Encoding;Machine learning;Data mining;Feature extraction;Training;Robustness;Visualization","data mining;image coding;image denoising;learning (artificial intelligence);object detection","cosaliency detection;intrasaliency prior transfer;deep intersaliency mining;saliency detection paradigm;data pool;deep learning models;stacked denoising autoencoder;auxiliary annotated datasets;deep reconstruction residual;self-trained SDAE;shallow intersaliency;publicly available benchmark datasets","","94","53","","","","","IEEE","IEEE Journals"
"Bottom–Up Visual Saliency Estimation With Deep Autoencoder-Based Sparse Reconstruction","C. Xia; F. Qi; G. Shi","Key Laboratory of Intelligent Perception and Image Understanding, School of Electronic Engineering, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, School of Electronic Engineering, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, School of Electronic Engineering, Ministry of Education, Xidian University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","","2016","27","6","1227","1240","Research on visual perception indicates that the human visual system is sensitive to center-surround (C-S) contrast in the bottom-up saliency-driven attention process. Different from the traditional contrast computation of feature difference, models based on reconstruction have emerged to estimate saliency by starting from original images themselves instead of seeking for certain ad hoc features. However, in the existing reconstruction-based methods, the reconstruction parameters of each area are calculated independently without taking their global correlation into account. In this paper, inspired by the powerful feature learning and data reconstruction ability of deep autoencoders, we construct a deep C-S inference network and train it with the data sampled randomly from the entire image to obtain a unified reconstruction pattern for the current image. In this way, global competition in sampling and learning processes can be integrated into the nonlocal reconstruction and saliency estimation of each pixel, which can achieve better detection results than the models with separate consideration on local and global rarity. Moreover, by learning from the current scene, the proposed model can achieve the feature extraction and interaction simultaneously in an adaptive way, which can form a better generalization ability to handle more types of stimuli. Experimental results show that in accordance with different inputs, the network can learn distinct basic features for saliency modeling in its code layer. Furthermore, in a comprehensive evaluation on several benchmark data sets, the proposed method can outperform the existing state-of-the-art algorithms.","","","10.1109/TNNLS.2015.2512898","Major State Basic Research Development Program of China (973 Program) within the Ministry of Science and Technology, China; National Natural Science Foundation of China; Ministry of Education, China; International Cooperation Project of Shaanxi Science and Technology Research and Development Program; Fundamental Scientific Research Funds of Xidian University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7386678","Autoencoder;center-surround (C-S) difference;deep learning;reconstruction;saliency;unsupervised feature learning.;Autoencoder;center–surround (C–S) difference;deep learning;reconstruction;saliency;unsupervised feature learning","Image reconstruction;Visualization;Estimation;Adaptation models;Computational modeling;Feature extraction;Image color analysis","feature extraction;image reconstruction;image sampling;inference mechanisms;learning (artificial intelligence)","bottom-up visual saliency estimation;deep autoencoder-based sparse reconstruction;visual perception;human visual system;center-surround contrast;C-S contrast;bottom-up saliency-driven attention process;contrast computation;feature difference;reconstruction-based methods;feature learning;data reconstruction ability;deep C-S inference network;sampling processes;learning processes;nonlocal reconstruction;local rarity;global rarity;feature extraction","","26","82","","","","","IEEE","IEEE Journals"
"Deep-STEP: A Deep Learning Approach for Spatiotemporal Prediction of Remote Sensing Data","M. Das; S. K. Ghosh","Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India","IEEE Geoscience and Remote Sensing Letters","","2016","13","12","1984","1988","With the advent of advanced remote sensing technologies in past few decades, acquiring higher resolution satellite images has become easier and cheaper in recent days. However, on the other hand, it has offered a big challenge to the remote sensing community in smart image interpretation from such huge volume of data. Deep learning, which offers efficient algorithms for extracting multiple levels of feature abstractions, may be suitable to serve the purpose. This letter presents a deep learning approach (Deep-STEP) for spatiotemporal prediction of satellite remote sensing data. The proposed learning architecture is derived from a deep stacking network, consisting of a stack of multilayer perceptron, each of which models the spatial feature of the associated region at a particular time instant. The proposed method has been demonstrated on normalized difference vegetation index (NDVI) data sets, derived from satellite remote sensing imagery, containing several thousands to millions of pixels/records. The experimental results (related to NDVI prediction) reveal that the proposed architecture exhibits fairly satisfactory performance with promising learning capabilities.","","","10.1109/LGRS.2016.2619984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752890","Deep learning;deep stacking network (DSN);satellite remote sensing imagery;spatiotemporal prediction","Spatiotemporal phenomena;Remote sensing;Machine learning;Satellites;Training;Stacking;Computational modeling","remote sensing;vegetation","Deep-STEP;deep learning approach;remote sensing technology;satellite images;satellite remote sensing data;deep stacking network;normalized difference vegetation index;NDVI dataset;satellite remote sensing imagery","","23","12","","","","","IEEE","IEEE Journals"
"Deep Metric Learning for Visual Tracking","J. Hu; J. Lu; Y. Tan","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Automation, Tsinghua University, Beijing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Circuits and Systems for Video Technology","","2016","26","11","2056","2068","In this paper, we propose a deep metric learning (DML) approach for robust visual tracking under the particle filter framework. Unlike most existing appearance-based visual trackers, which use hand-crafted similarity metrics, our DML tracker learns a nonlinear distance metric to classify the target object and background regions using a feed-forward neural network architecture. Since there are usually large variations in visual objects caused by varying deformations, illuminations, occlusions, motions, rotations, scales, and cluttered backgrounds, conventional linear similarity metrics cannot work well in such scenarios. To address this, our proposed DML tracker first learns a set of hierarchical nonlinear transformations in the feed-forward neural network to project both the template and particles into the same feature space where the intra-class variations of positive training pairs are minimized and the interclass variations of negative training pairs are maximized simultaneously. Then, the candidate that is most similar to the template in the learned deep network is identified as the true target. Experiments on the benchmark data set including 51 challenging videos show that our DML tracker achieves a very competitive performance with the state-of-the-art trackers.","","","10.1109/TCSVT.2015.2477936","Rapid-Rich Object Search Laboratory through the National Research Foundation, Singapore, under its Interactive Digital Media within the Strategic Research Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7258342","Deep learning;metric learning;visual tracking","Visualization;Target tracking;Neural networks;Machine learning;Training;Learning systems","feature extraction;feedforward neural nets;image classification;learning (artificial intelligence);neural net architecture;object tracking;particle filtering (numerical methods)","deep metric learning;visual tracking;particle filter;appearance-based visual trackers;DML tracker;nonlinear distance metric;object classification;background regions classification;feed-forward neural network architecture;visual objects;hierarchical nonlinear transformations;feature space;intra-class variations;interclass variations;deep network learning","","31","60","","","","","IEEE","IEEE Journals"
"Unsupervised Deep Feature Extraction for Remote Sensing Image Classification","A. Romero; C. Gatta; G. Camps-Valls","Dept. of Appl. Math. & Anal., Univ. de Barcelona, Barcelona, Spain; Comput. Vision Center, Univ. Autonoma de Barcelona, Barcelona, Spain; Image Process. Lab., Univ. de Valencia, València, Spain","IEEE Transactions on Geoscience and Remote Sensing","","2016","54","3","1349","1362","This paper introduces the use of single-layer and deep convolutional networks for remote sensing data analysis. Direct application to multi- and hyperspectral imagery of supervised (shallow or deep) convolutional networks is very challenging given the high input data dimensionality and the relatively small amount of available labeled data. Therefore, we propose the use of greedy layerwise unsupervised pretraining coupled with a highly efficient algorithm for unsupervised learning of sparse features. The algorithm is rooted on sparse representations and enforces both population and lifetime sparsity of the extracted features, simultaneously. We successfully illustrate the expressive power of the extracted representations in several scenarios: classification of aerial scenes, as well as land-use classification in very high resolution or land-cover classification from multi- and hyperspectral images. The proposed algorithm clearly outperforms standard principal component analysis (PCA) and its kernel counterpart (kPCA), as well as current state-of-the-art algorithms of aerial classification, while being extremely computationally efficient at learning representations of data. Results show that single-layer convolutional networks can extract powerful discriminative features only when the receptive field accounts for neighboring pixels and are preferred when the classification requires high resolution and detailed results. However, deep architectures significantly outperform single-layer variants, capturing increasing levels of abstraction and complexity throughout the feature hierarchy.","","","10.1109/TGRS.2015.2478379","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293195","Aerial image classification;classification;deep convolutional networks;deep learning;feature extraction;hyperspectral (HS) image;multispectral (MS) images;segmentation;sparse features learning;very high resolution (VHR);Aerial image classification;classification;deep convolutional networks;deep learning;feature extraction;hyperspectral (HS) image;multispectral (MS) images;segmentation;sparse features learning;very high resolution (VHR)","Feature extraction;Remote sensing;Training;Computer architecture;Unsupervised learning;Sociology;Statistics","feature extraction;geophysical image processing;hyperspectral imaging;image classification;land cover;land use;principal component analysis;remote sensing;unsupervised learning","unsupervised deep feature extraction;remote sensing image classification;single-layer network;deep convolutional network;remote sensing data analysis;multiimagery;hyperspectral imagery;supervised convolutional network;unsupervised learning;sparse feature;sparse representation;aerial scene classification;land use classification;land cover classification;multiimages;hyperspectral images;kernel principal component analysis","","250","67","","","","","IEEE","IEEE Journals"
"Deep Learning of Part-Based Representation of Data Using Sparse Autoencoders With Nonnegativity Constraints","E. Hosseini-Asl; J. M. Zurada; O. Nasraoui","Department of Electrical and Computer Engineering, University of Louisville, Louisville, KY, USA; Department of Electrical and Computer Engineering, University of Louisville, Louisville, KY, USA; Department of Computer Science, University of Louisville, Louisville, KY, USA","IEEE Transactions on Neural Networks and Learning Systems","","2016","27","12","2486","2498","We demonstrate a new deep learning autoencoder network, trained by a nonnegativity constraint algorithm (nonnegativity-constrained autoencoder), that learns features that show part-based representation of data. The learning algorithm is based on constraining negative weights. The performance of the algorithm is assessed based on decomposing data into parts and its prediction performance is tested on three standard image data sets and one text data set. The results indicate that the nonnegativity constraint forces the autoencoder to learn features that amount to a part-based representation of data, while improving sparsity and reconstruction quality in comparison with the traditional sparse autoencoder and nonnegative matrix factorization. It is also shown that this newly acquired representation improves the prediction performance of a deep neural network.","","","10.1109/TNNLS.2015.2479223","Kentucky Science and Engineering Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7310882","Autoencoder;deep architectures;feature learning;nonnegativity constraints;part-based representation","Training;Feature extraction;Artificial neural networks;Machine learning;Image reconstruction;Encoding;Cost function","constraint handling;data handling;data structures;learning (artificial intelligence)","data part-based representation;sparse autoencoders;nonnegativity constraints;deep learning autoencoder network;nonnegativity constraint algorithm;nonnegativity-constrained autoencoder;learning algorithm;constraining negative weights;data decomposition;image data sets;text data set;reconstruction quality;nonnegative matrix factorization;deep neural network","","88","43","","","","","IEEE","IEEE Journals"
"DISC: Deep Image Saliency Computing via Progressive Representation Learning","T. Chen; L. Lin; L. Liu; X. Luo; X. Li","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, P. R. China","IEEE Transactions on Neural Networks and Learning Systems","","2016","27","6","1135","1149","Salient object detection increasingly receives attention as an important component or step in several pattern recognition and image processing tasks. Although a variety of powerful saliency models have been intensively proposed, they usually involve heavy feature (or model) engineering based on priors (or assumptions) about the properties of objects and backgrounds. Inspired by the effectiveness of recently developed feature learning, we provide a novel deep image saliency computing (DISC) framework for fine-grained image saliency computing. In particular, we model the image saliency from both the coarse-and fine-level observations, and utilize the deep convolutional neural network (CNN) to learn the saliency representation in a progressive manner. In particular, our saliency model is built upon two stacked CNNs. The first CNN generates a coarse-level saliency map by taking the overall image as the input, roughly identifying saliency regions in the global context. Furthermore, we integrate superpixel-based local context information in the first CNN to refine the coarse-level saliency map. Guided by the coarse saliency map, the second CNN focuses on the local context to produce fine-grained and accurate saliency map while preserving object details. For a testing image, the two CNNs collaboratively conduct the saliency computing in one shot. Our DISC framework is capable of uniformly highlighting the objects of interest from complex background while preserving well object details. Extensive experiments on several standard benchmarks suggest that DISC outperforms other state-of-the-art methods and it also generalizes well across data sets without additional training. The executable version of DISC is available online: <;uri xlink:type=""simple"">http://vision.sysu.edu.cn/projects/DISC<;/uri>.","","","10.1109/TNNLS.2015.2506664","National Natural Science Foundation of China; Guangdong Natural Science Foundation; Guangdong Science and Technology Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372470","Convolutional neural network (CNN);image labeling;representation learning;saliency detection.;Convolutional neural network (CNN);image labeling;representation learning;saliency detection","Computational modeling;Context;Image color analysis;Visualization;Object detection;Computer vision;Histograms","feature extraction;image representation;image resolution;learning (artificial intelligence);neural nets;object detection","DISC;deep image saliency computing;progressive representation learning;salient object detection;pattern recognition;image processing;feature engineering;feature learning;fine-grained image saliency computing;fine-level observations;coarse-level observations;deep convolutional neural network;CNN;superpixel-based local context information;coarse-level saliency map","","64","54","","","","","IEEE","IEEE Journals"
"Unsupervised Deep Learning Applied to Breast Density Segmentation and Mammographic Risk Scoring","M. Kallenberg; K. Petersen; M. Nielsen; A. Y. Ng; P. Diao; C. Igel; C. M. Vachon; K. Holland; R. R. Winkel; N. Karssemeijer; M. Lillholm","University of Copenhagen, Copenhagen, Denmark; University of Copenhagen, Copenhagen, Denmark; University of Copenhagen, Copenhagen, Denmark; Stanford University, United States; University of Copenhagen, Copenhagen, Denmark; University of Copenhagen, Copenhagen, Denmark; Mayo Clinic Hospital, United States; Radboud University Nijmegen Medical Centre, Nijmegen, Netherlands; University Hospital of Copenhagen, Copenhagen, Denmark; Radboud University Nijmegen Medical Centre, Nijmegen, Netherlands; Biomediq A/S, Copenhagen, Denmark","IEEE Transactions on Medical Imaging","","2016","35","5","1322","1331","Mammographic risk scoring has commonly been automated by extracting a set of handcrafted features from mammograms, and relating the responses directly or indirectly to breast cancer risk. We present a method that learns a feature hierarchy from unlabeled data. When the learned features are used as the input to a simple classifier, two different tasks can be addressed: i) breast density segmentation, and ii) scoring of mammographic texture. The proposed model learns features at multiple scales. To control the models capacity a novel sparsity regularizer is introduced that incorporates both lifetime and population sparsity. We evaluated our method on three different clinical datasets. Our state-of-the-art results show that the learned breast density scores have a very strong positive relationship with manual ones, and that the learned texture scores are predictive of breast cancer. The model is easy to apply and generalizes to many other segmentation and scoring problems.","","","10.1109/TMI.2016.2532122","European Seventh Framework Programme FP7; Danish Advanced Technology Foundation; Innovation Fund Denmark; European Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7412749","Breast cancer;deep learning;mammograms;prognosis;risk factor;segmentation;unsupervised feature learning","Breast cancer;Computer architecture;Machine learning;Mammography;Manuals;Image segmentation","cancer;image classification;image segmentation;image texture;mammography;medical image processing;risk analysis;tumours;unsupervised learning","unsupervised deep learning;breast density segmentation;mammographic risk scoring;handcrafted feature extraction;breast cancer risk;unlabeled data;learned features;simple classifier;breast density segmentation;mammographic texture;sparsity regularizer;population sparsity;clinical datasets;learned texture scores","Adult;Aged;Breast;Breast Density;Breast Neoplasms;Female;Humans;Image Interpretation, Computer-Assisted;Mammography;Middle Aged;Risk Factors;Unsupervised Machine Learning","140","67","","","","","IEEE","IEEE Journals"
"Learning Deep Representation for Face Alignment with Auxiliary Attributes","Z. Zhang; P. Luo; C. C. Loy; X. Tang","Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","5","918","930","In this study, we show that landmark detection or face alignment task is not a single and independent problem. Instead, its robustness can be greatly improved with auxiliary information. Specifically, we jointly optimize landmark detection together with the recognition of heterogeneous but subtly correlated facial attributes, such as gender, expression, and appearance attributes. This is non-trivial since different attribute inference tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, which not only learns the inter-task correlation but also employs dynamic task coefficients to facilitate the optimization convergence when learning multiple complex tasks. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing face alignment methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to the state-of-the-art methods based on cascaded deep model.","","","10.1109/TPAMI.2015.2469286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208848","Face Alignment;Face Landmark Detection;Deep Learning;Convolutional Network;Face Alignment;face landmark detection;deep learning;convolutional network","Face;Training;Convergence;Covariance matrices;Correlation;Gaussian distribution;Glass","convergence;correlation methods;face recognition;image representation;learning (artificial intelligence);object detection;optimisation;pose estimation","learning deep representation;face alignment;auxiliary attributes;landmark detection;auxiliary information;facial attributes;attribute inference task;task-constrained deep model;intertask correlation;dynamic task coefficients;optimization convergence;task-constrained learning;occlusion;pose variation;cascaded deep model","","135","52","","","","","IEEE","IEEE Journals"
"A Perspective on Deep Imaging","G. Wang","Department of Biomedical Engineering, Biomedical Imaging Center, Center for Biotechnology and Interdisciplinary Studies, Rensselaer Polytechnic Institute, Troy, NY, USA","IEEE Access","","2016","4","","8914","8924","The combination of tomographic imaging and deep learning, or machine learning in general, promises to empower not only image analysis but also image reconstruction. The latter aspect is considered in this perspective article with an emphasis on medical imaging to develop a new generation of image reconstruction theories and techniques. This direction might lead to intelligent utilization of domain knowledge from big data, innovative approaches for image reconstruction, and superior performance in clinical and preclinical applications. To realize the full impact of machine learning for tomographic imaging, major theoretical, technical and translational efforts are immediately needed.","","","10.1109/ACCESS.2016.2624938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733110","Tomographic imaging;medical imaging;data acquisition;image reconstruction;image analysis;big data;machine learning;deep learning","Image processing;Tomography;Data acquisition;Image reconstruction;Machine learning;Deep learning","Big Data;image reconstruction;learning (artificial intelligence)","Big data;image reconstruction techniques;image reconstruction theories;medical imaging;image analysis;machine learning;deep learning;tomographic imaging;deep imaging","","126","49","","","","","IEEE","IEEE Journals"
"Change Detection in Synthetic Aperture Radar Images Based on Deep Neural Networks","M. Gong; J. Zhao; J. Liu; Q. Miao; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","","2016","27","1","125","138","This paper presents a novel change detection approach for synthetic aperture radar images based on deep learning. The approach accomplishes the detection of the changed and unchanged areas by designing a deep neural network. The main guideline is to produce a change detection map directly from two images with the trained deep neural network. The method can omit the process of generating a difference image (DI) that shows difference degrees between multitemporal synthetic aperture radar images. Thus, it can avoid the effect of the DI on the change detection results. The learning algorithm for deep architectures includes unsupervised feature learning and supervised fine-tuning to complete classification. The unsupervised feature learning aims at learning the representation of the relationships between the two images. In addition, the supervised fine-tuning aims at learning the concepts of the changed and unchanged pixels. Experiments on real data sets and theoretical analysis indicate the advantages, feasibility, and potential of the proposed method. Moreover, based on the results achieved by various traditional algorithms, respectively, deep learning can further improve the detection performance.","","","10.1109/TNNLS.2015.2435783","National Nature Science Foundation of China; National Program for Support of Top-Notch Young Professionals of China; Specialized Research Fund for the Doctoral Program of Higher Education; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120131","Deep learning;image change detection;neural network;synthetic aperture radar (SAR).;Deep learning;image change detection;neural network;synthetic aperture radar (SAR)","Neural networks;Synthetic aperture radar;Change detection algorithms;Algorithm design and analysis;Noise;Training;Joints","feature extraction;neural nets;object detection;synthetic aperture radar;unsupervised learning","supervised fine-tuning;unsupervised feature learning algorithm;deep architecture;multitemporal synthetic aperture radar image;change detection map;deep neural networks;change detection approach","","153","49","","","","","IEEE","IEEE Journals"
"Towards Bayesian Deep Learning: A Framework and Some Existing Methods","H. Wang; D. Yeung","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong","IEEE Transactions on Knowledge and Data Engineering","","2016","28","12","3395","3408","While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, subsequent tasks that involve inference, reasoning, and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This paper proposes a general framework for Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this paper, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as the Bayesian treatment of neural networks.","","","10.1109/TKDE.2016.2606428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7562516","Artificial intelligence;data mining;Bayesian networks;neural networks;deep learning;machine learning","Machine learning;Bayes methods;Neural networks;Artificial intelligence;Probabilistic logic;Recommender systems;Medical services;Learning systems","inference mechanisms;learning (artificial intelligence);probability","Bayesian deep learning;perception tasks;human intelligence;higher-level inference;probabilistic graphical models;principled probabilistic framework;general framework;reasoning;planning","","28","75","","","","","IEEE","IEEE Journals"
"Scalable High-Performance Image Registration Framework by Unsupervised Deep Feature Representations Learning","G. Wu; M. Kim; Q. Wang; B. C. Munsell; D. Shen","Department of Radiology and BRIC, The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology and BRIC, The University of North Carolina at Chapel Hill; Med-X Research Institute, Shanghai Jiao Tong University; Department of Computer Science, The College of Charleston; Department of Radiology and BRIC, the University of North Carolina at Chapel Hill","IEEE Transactions on Biomedical Engineering","","2016","63","7","1505","1516","Feature selection is a critical step in deformable image registration. In particular, selecting the most discriminative features that accurately and concisely describe complex morphological patterns in image patches improves correspondence detection, which in turn improves image registration accuracy. Furthermore, since more and more imaging modalities are being invented to better identify morphological changes in medical imaging data, the development of deformable image registration method that scales well to new image modalities or new image applications with little to no human intervention would have a significant impact on the medical image analysis community. To address these concerns, a learning-based image registration framework is proposed that uses deep learning to discover compact and highly discriminative features upon observed imaging data. Specifically, the proposed feature selection method uses a convolutional stacked autoencoder to identify intrinsic deep feature representations in image patches. Since deep learning is an unsupervised learning method, no ground truth label knowledge is required. This makes the proposed feature selection method more flexible to new imaging modalities since feature representations can be directly learned from the observed imaging data in a very short amount of time. Using the LONI and ADNI imaging datasets, image registration performance was compared to two existing state-of-the-art deformable image registration methods that use handcrafted features. To demonstrate the scalability of the proposed image registration framework, image registration experiments were conducted on 7.0-T brain MR images. In all experiments, the results showed that the new image registration framework consistently demonstrated more accurate registration results when compared to state of the art.","","","10.1109/TBME.2015.2496253","National Institute of Child Health and Human Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314894","Deformable image registration;deep learning;hierarchical feature representation;Deep learning;deformable image registration;hierarchical feature representation","Image registration;Machine learning;Biomedical imaging;Three-dimensional displays;Feature extraction;Unsupervised learning","","","Algorithms;Brain;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Unsupervised Machine Learning","55","70","","","","","IEEE","IEEE Journals"
"Deep Dictionary Learning","S. Tariyal; A. Majumdar; R. Singh; M. Vatsa","IIIT Delhi, New Delhi, India; IIIT Delhi, New Delhi, India; IIIT Delhi, New Delhi, India; IIIT Delhi, New Delhi, India","IEEE Access","","2016","4","","10096","10109","Two popular representation learning paradigms are dictionary learning and deep learning. While dictionary learning focuses on learning “basis” and “features” by matrix factorization, deep learning focuses on extracting features via learning “weights” or “filter” in a greedy layer by layer fashion. This paper focuses on combining the concepts of these two paradigms by proposing deep dictionary learning and show how deeper architectures can be built using the layers of dictionary learning. The proposed technique is compared with other deep learning approaches, such as stacked autoencoder, deep belief network, and convolutional neural network. Experiments on benchmark data sets show that the proposed technique achieves higher classification and clustering accuracies. On a real-world problem of electrical appliance classification, we show that deep dictionary learning excels where others do not yield at-par performance. We postulate that the proposed formulation can pave the path for a new class of deep learning tools.","","","10.1109/ACCESS.2016.2611583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7779008","Deep learning;dictionary learning;feature representation","Dictionaries;Feature extraction;Machine learning;Matrix decomposition;Sparse matrices;Neural networks","feature extraction;learning (artificial intelligence);pattern classification;pattern clustering","deep dictionary learning;matrix factorization;feature extraction;greedy layer by layer approach;benchmark data sets;classification accuracies;clustering accuracies;electrical appliance classification","","41","81","","","","","IEEE","IEEE Journals"
"Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and Recognition","D. Wu; L. Pigou; P. Kindermans; N. D. Le; L. Shao; J. Dambre; J. Odobez","Perception and Activity Understanding, IDIAP, Martigny, Valais, Switzerland; ELIS, Ghent University, Gent, Oost-Vlaanderen, Belgium; TU-Berlin, Berlin, Germany; Computer Vision, IDIAP Research Institute, Martigny, Valais, Switzerland; Department of Computer Science and Digital Technologies, Northumbria University, Newcastle upon Tyne; ELIS, Ghent University, Gent, Oost-Vlaanderen, Belgium; Computer Vision, IDIAP Research Institute, Martigny, Valais, Switzerland","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","8","1583","1597","This paper describes a novel method called Deep Dynamic Neural Networks (DDNN) for multimodal gesture recognition. A semi-supervised hierarchical dynamic framework based on a Hidden Markov Model (HMM) is proposed for simultaneous gesture segmentation and recognition where skeleton joint information, depth and RGB images, are the multimodal input observations. Unlike most traditional approaches that rely on the construction of complex handcrafted features, our approach learns high-level spatiotemporal representations using deep neural networks suited to the input modality: a Gaussian-Bernouilli Deep Belief Network (DBN) to handle skeletal dynamics, and a 3D Convolutional Neural Network (3DCNN) to manage and fuse batches of depth and RGB images. This is achieved through the modeling and learning of the emission probabilities of the HMM required to infer the gesture sequence. This purely data driven approach achieves a Jaccard index score of 0.81 in the ChaLearn LAP gesture spotting challenge. The performance is on par with a variety of state-of-the-art hand-tuned feature-based approaches and other learning-based methods, therefore opening the door to the use of deep learning techniques in order to further explore multimodal time series data.","","","10.1109/TPAMI.2016.2537340","European Unions Horizon 2020 research and innovation programme; Marie Sklodowska-Curie; BMBF; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7423804","Deep learning;convolutional neural networks;deep belief networks;hidden Markov models;gesture recognition","Hidden Markov models;Feature extraction;Neural networks;Gesture recognition;Three-dimensional displays;Data models;Skeleton","feature extraction;gesture recognition;hidden Markov models;image colour analysis;image segmentation;image sequences;learning (artificial intelligence);neural nets;probability;spatiotemporal phenomena","deep dynamic neural networks;multimodal gesture segmentation;multimodal gesture recognition;DDNN;semisupervised hierarchical dynamic framework;hidden Markov model;simultaneous gesture segmentation and recognition;RGB images;depth images;high-level spatio-temporal representations;input modality;Gaussian-Bernouilli deep belief network;Gaussian-Bernouilli DBN;skeletal dynamics;3D convolutional neural network;3DCNN;emission probability learning;HMM;gesture sequence;Jaccard index score;ChaLearn LAP gesture spotting challenge;hand-tuned feature-based approaches;learning-based methods;multimodal time series data","Algorithms;Gestures;Humans;Learning;Neural Networks (Computer);Normal Distribution;Pattern Recognition, Automated","134","68","","","","","IEEE","IEEE Journals"
"Spectral–Spatial Feature Extraction for Hyperspectral Image Classification: A Dimension Reduction and Deep Learning Approach","W. Zhao; S. Du","Institute of Remote Sensing and GIS, Peking University, Beijing, 100871, China; Institute of Remote Sensing and GIS, Peking University, Beijing, 100871, China","IEEE Transactions on Geoscience and Remote Sensing","","2016","54","8","4544","4554","In this paper, we propose a spectral-spatial feature based classification (SSFC) framework that jointly uses dimension reduction and deep learning techniques for spectral and spatial feature extraction, respectively. In this framework, a balanced local discriminant embedding algorithm is proposed for spectral feature extraction from high-dimensional hyperspectral data sets. In the meantime, convolutional neural network is utilized to automatically find spatial-related features at high levels. Then, the fusion feature is extracted by stacking spectral and spatial features together. Finally, the multiple-feature-based classifier is trained for image classification. Experimental results on well-known hyperspectral data sets show that the proposed SSFC method outperforms other commonly used methods for hyperspectral image classification.","","","10.1109/TGRS.2016.2543748","National Natural Science Foundation of China; Weng Hongwu Scientific Research Foundation of Peking University, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7450160","Balanced local discriminant embedding (BLDE);convolutional neural network (CNN);deep learning (DL);dimension reduction (DR);feature extraction;Balanced local discriminant embedding (BLDE);convolutional neural network (CNN);deep learning (DL);dimension reduction (DR);feature extraction","Feature extraction;Training;Hyperspectral imaging;Machine learning;Algorithm design and analysis","feature extraction;hyperspectral imaging;image classification","spectral-spatial feature extraction;hyperspectral image classification;deep learning approach;dimension reduction approach;spectral-spatial feature based classification framework;SSFC framework;deep learning techniques;balanced local discriminant embedding algorithm;high-dimensional hyperspectral data sets;convolutional neural network;fusion feature","","260","34","","","","","IEEE","IEEE Journals"
"A Deep Learning Framework of Quantized Compressed Sensing for Wireless Neural Recording","B. Sun; H. Feng; K. Chen; X. Zhu","School of Electrical Engineering and Automation, Tianjin University, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University, Tianjin, China","IEEE Access","","2016","4","","5169","5178","In low-power wireless neural recording tasks, signals must be compressed before transmission to extend battery life. Recently, compressed sensing (CS) theory has successfully demonstrated its potential in neural recording applications. In this paper, a deep learning framework of quantized CS, termed BW-NQ-DNN, is proposed, which consists of a binary measurement matrix, a non-uniform quantizer, and a non-iterative recovery solver. By training the BW-NQ-DNN, the three parts are jointly optimized. Experimental results on synthetic and real datasets reveal that BW-NQ-DNN not only drastically reduce the transmission bits but also outperforms the state-of-the-art CS-based methods. On the challenging high compression ratio task, the proposed approach still achieves high recovery performance and spike classification accuracy. This framework is of great values to wireless neural recoding devices, and many variants can be straightforwardly derived for low-power wireless telemonitoring applications.","","","10.1109/ACCESS.2016.2604397","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7560597","Wireless neural recording;quantized compressive sensing;non-uniform quantization;deep learning","Low power electronics;Wireless sensor networks;Wireless communication;Compressed sensing;Machine learning;Battery charge measurement;Deep learning","bioelectric phenomena;compressed sensing;learning (artificial intelligence);medical signal processing;neurophysiology","low-power wireless telemonitoring;wireless neural recoding devices;high recovery performance;spike classification accuracy;noniterative recovery solver;no-uniform quantizer;binary measurement matrix;BW-NQ-DNN;signal compression;wireless neural recording;quantized compressed sensing;deep learning","","16","50","","","","","IEEE","IEEE Journals"
"Deep Learning Driven Visual Path Prediction From a Single Image","S. Huang; X. Li; Z. Zhang; Z. He; F. Wu; W. Liu; J. Tang; Y. Zhuang","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Tencent AI Laboratory, Shenzhen, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Image Processing","","2016","25","12","5892","5904","Capabilities of inference and prediction are the significant components of visual systems. Visual path prediction is an important and challenging task among them, with the goal to infer the future path of a visual object in a static scene. This task is complicated as it needs high-level semantic understandings of both the scenes and underlying motion patterns in video sequences. In practice, cluttered situations have also raised higher demands on the effectiveness and robustness of models. Motivated by these observations, we propose a deep learning framework, which simultaneously performs deep feature learning for visual representation in conjunction with spatiotemporal context modeling. After that, a unified path-planning scheme is proposed to make accurate path prediction based on the analytic results returned by the deep context models. The highly effective visual representation and deep context models ensure that our framework makes a deep semantic understanding of the scenes and motion patterns, consequently improving the performance on visual path prediction task. In experiments, we extensively evaluate the model's performance by constructing two large benchmark datasets from the adaptation of video tracking datasets. The qualitative and quantitative experimental results show that our approach outperforms the state-of-the-art approaches and owns a better generalization capability.","","","10.1109/TIP.2016.2613686","National Natural Science Foundation of China; Zhejiang Provincial Engineering Center on Media Data Cloud Processing and Analysis; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7576681","Visual path prediction;visual context model;convolutional neural networks;deep learning","Visualization;Context modeling;Context;Adaptation models;Predictive models;Semantics;Machine learning","feature extraction;image motion analysis;image representation;image sequences;learning (artificial intelligence);video signal processing","visual path prediction;high-level semantic;motion pattern;video sequence;deep learning framework;deep feature learning;visual representation;spatiotemporal context modeling;unified path-planning scheme;video tracking dataset","","13","51","","","","","IEEE","IEEE Journals"
"Machine Learning in Genomic Medicine: A Review of Computational Problems and Data Sets","M. K. K. Leung; A. Delong; B. Alipanahi; B. J. Frey","NA; NA; NA; NA","Proceedings of the IEEE","","2016","104","1","176","197","In this paper, we provide an introduction to machine learning tasks that address important problems in genomic medicine. One of the goals of genomic medicine is to determine how variations in the DNA of individuals can affect the risk of different diseases, and to find causal explanations so that targeted therapies can be designed. Here we focus on how machine learning can help to model the relationship between DNA and the quantities of key molecules in the cell, with the premise that these quantities, which we refer to as cell variables, may be associated with disease risks. Modern biology allows high-throughput measurement of many such cell variables, including gene expression, splicing, and proteins binding to nucleic acids, which can all be treated as training targets for predictive models. With the growing availability of large-scale data sets and advanced computational techniques such as deep learning, researchers can help to usher in a new era of effective genomic medicine.","","","10.1109/JPROC.2015.2494198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7347331","Computational biology;deep learning;genetic variants;genome analysis;genome biology;genomic medicine;machine learning;precision medicine;Computational biology;deep learning;genetic variants;genome analysis;genome biology;genomic medicine;machine learning;precision medicine","Genomics;Bioinformatics;Big data;Diseases;Medical treatment;Machine learning","cellular biophysics;diseases;DNA;genetics;genomics;learning (artificial intelligence);medical computing;medicine","deep learning;large-scale data sets;nucleic acids;proteins;gene splicing;gene expression;high-throughput measurement;disease risks;cell variables;DNA variations;genomic medicine;machine learning","","70","231","","","","","IEEE","IEEE Journals"
"Learning Hierarchical Spectral–Spatial Features for Hyperspectral Image Classification","Y. Zhou; Y. Wei","Department of Computer and Information Science, University of Macau, Macau, China; School of Educational Information Technology, Central China Normal University, Wuhan, China","IEEE Transactions on Cybernetics","","2016","46","7","1667","1678","This paper proposes a spectral-spatial feature learning (SSFL) method to obtain robust features of hyperspectral images (HSIs). It combines the spectral feature learning and spatial feature learning in a hierarchical fashion. Stacking a set of SSFL units, a deep hierarchical model called the spectral-spatial networks (SSN) is further proposed for HSI classification. SSN can exploit both discriminative spectral and spatial information simultaneously. Specifically, SSN learns useful high-level features by alternating between spectral and spatial feature learning operations. Then, kernel-based extreme learning machine (KELM), a shallow neural network, is embedded in SSN to classify image pixels. Extensive experiments are performed on two benchmark HSI datasets to verify the effectiveness of SSN. Compared with state-of-the-art methods, SSN with a deep hierarchical architecture obtains higher classification accuracy in terms of the overall accuracy, average accuracy, and kappa (κ) coefficient of agreement, especially when the number of the training samples is small.","","","10.1109/TCYB.2015.2453359","Macau Science and Technology Development Fund; Research Committee at the University of Macau; Fundamental Research Funds for the Central Universities; Wuhan Science and Technology Plan Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169539","Hierarchical learning;hyperspectral image classification;kernel-based extreme learning machine;spectral-spatial feature;Hierarchical learning;hyperspectral image classification;kernel-based extreme learning machine;spectral–spatial feature","Training;Feature extraction;Machine learning;Accuracy;Hyperspectral sensors;Support vector machines;Learning systems","feature extraction;hyperspectral imaging;image classification;learning (artificial intelligence);neural nets","hierarchical spectral-spatial feature learning;SSFL method;hyperspectral image classification;HSI;deep hierarchical model;spectral-spatial networks;SSN;kernel-based extreme learning machine;KELM;shallow neural network;image pixel classification;HSI datasets;deep hierarchical architecture;kappa coefficient","","38","63","","","","","IEEE","IEEE Journals"
"Droiddetector: android malware characterization and detection using deep learning","Z. Yuan; Y. Lu; Y. Xue","Department of Automation and Research Institute of Information Technology (RIIT), Tsinghua University, Beijing 100084, China; Department of Antivirus, Baidu Inc., Beijing 100085, China; Research Institute of Information Technology (RIIT) and Tsinghua National Lab for Information Science and Technology (TNList), Tsinghua University, Beijing 100084, China","Tsinghua Science and Technology","","2016","21","1","114","123","Smartphones and mobile tablets are rapidly becoming indispensable in daily life. Android has been the most popular mobile operating system since 2012. However, owing to the open nature of Android, countless malwares are hidden in a large number of benign apps in Android markets that seriously threaten Android security. Deep learning is a new area of machine learning research that has gained increasing attention in artificial intelligence. In this study, we propose to associate the features from the static analysis with features from dynamic analysis of Android apps and characterize malware using deep learning techniques. We implement an online deep-learning-based Android malware detection engine (DroidDetector) that can automatically detect whether an app is a malware or not. With thousands of Android apps, we thoroughly test DroidDetector and perform an indepth analysis on the features that deep learning essentially exploits to characterize malware. The results show that deep learning is suitable for characterizing Android malware and especially effective with the availability of more training data. DroidDetector can achieve 96.76% detection accuracy, which outperforms traditional machine learning techniques. An evaluation of ten popular anti-virus softwares demonstrates the urgency of advancing our capabilities in Android malware detection.","","","10.1109/TST.2016.7399288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7399288","Android security;malware detection;characterization;deep learning;association rules mining","Malware;Androids;Humanoid robots;Machine learning;Feature extraction;Smart phones;Google","Android (operating system);data mining;invasive software;learning (artificial intelligence);mobile computing;notebook computers;program diagnostics;smart phones","DroidDetector;Android malware characterization;online deep-learning-based Android malware detection engine;smartphones;mobile tablets;mobile operating system;benign apps;Android markets;Android security;machine learning research;static analysis;Android apps dynamic analysis;anti virus softwares;association rules mining","","37","","","","","","TUP","TUP Journals"
"Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields","F. Liu; C. Shen; G. Lin; I. Reid","School of Computer Science, The University of Adelaide, Australia; ARC Centre of Excellence for Robotic Vision and the School of Computer Science, The University of Adelaide, Australia; ARC Centre of Excellence for Robotic Vision and the School of Computer Science, The University of Adelaide, Australia; ARC Centre of Excellence for Robotic Vision and the School of Computer Science, The University of Adelaide, Australia","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","10","2024","2039","In this article, we tackle the problem of depth estimation from single monocular images. Compared with depth estimation using multiple images such as stereo depth perception, depth from monocular images is much more challenging. Prior work typically focuses on exploiting geometric priors or additional sources of information, most using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) set new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimation can be naturally formulated as a continuous conditional random field (CRF) learning problem. Therefore, here we present a deep convolutional neural field model for estimating depths from single monocular images, aiming to jointly explore the capacity of deep CNN and continuous CRF. In particular, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. We then further propose an equally effective model based on fully convolutional networks and a novel superpixel pooling method, which is about 10 times faster, to speedup the patch-wise convolutions in the deep model. With this more efficient model, we are able to design deeper networks to pursue better performance. Our proposed method can be used for depth estimation of general scenes with no geometric priors nor any extra information injected. In our case, the integral of the partition function can be calculated in a closed form such that we can exactly solve the log-likelihood maximization. Moreover, solving the inference problem for predicting depths of a test image is highly efficient as closed-form solutions exist. Experiments on both indoor and outdoor scene datasets demonstrate that the proposed method outperforms state-of-the-art depth estimation approaches.","","","10.1109/TPAMI.2015.2505283","ARC Future Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346484","Depth estimation;conditional random field (CRF);deep convolutional neural networks (CNN);fully convolutional networks;superpixel pooling","Estimation;Semantics;Training;Labeling;Optimization;Approximation methods;Neural networks","learning (artificial intelligence);neural nets;stereo image processing","learning depth;single monocular images;deep convolutional neural field model;depth estimation problem;stereo depth perception;hand-crafted features;continuous conditional random field learning problem;CRF learning problem;continuous CRF;deep CNN capacity;deep structured learning scheme;unified deep CNN framework;superpixel pooling method;patch-wise convolutions;partition function;log-likelihood maximization;closed-form solutions;outdoor scene datasets;inference problem","","232","39","","","","","IEEE","IEEE Journals"
"A Deep Ensemble Learning Method for Monaural Speech Separation","X. Zhang; D. Wang","Department of Computer Science and Engineering and the Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering and the Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","5","967","977","Monaural speech separation is a fundamental problem in robust speech processing. Recently, deep neural network (DNN)-based speech separation methods, which predict either clean speech or an ideal time-frequency mask, have demonstrated remarkable performance improvement. However, a single DNN with a given window length does not leverage contextual information sufficiently, and the differences between the two optimization objectives are not well understood. In this paper, we propose a deep ensemble method, named multicontext networks, to address monaural speech separation. The first multicontext network averages the outputs of multiple DNNs whose inputs employ different window lengths. The second multicontext network is a stack of multiple DNNs. Each DNN in a module of the stack takes the concatenation of original acoustic features and expansion of the soft output of the lower module as its input, and predicts the ratio mask of the target speaker; the DNNs in the same module employ different contexts. We have conducted extensive experiments with three speech corpora. The results demonstrate the effectiveness of the proposed method. We have also compared the two optimization objectives systematically and found that predicting the ideal time-frequency mask is more efficient in utilizing clean training speech, while predicting clean speech is less sensitive to SNR variations.","","","10.1109/TASLP.2016.2536478","AFOSR; NIDCD; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422753","Deep neural networks;ensemble learning;mapping-based separation;masking-based separation;monaural speech separation;multi-context networks;Deep neural networks;ensemble learning;mapping-based separation;masking-based separation;monaural speech separation;multicontext networks","Speech;Training;Speech processing;Optimization;Signal to noise ratio;Context;Acoustics","learning (artificial intelligence);neural nets;optimisation;speech processing","deep ensemble learning method;Monaural speech separation;robust speech processing;deep neural network;DNN;speech separation methods;ideal time frequency mask;window length;contextual information;optimization objectives;multicontext networks;target speaker;speech corpora","","68","40","","","","","IEEE","IEEE Journals"
"Deep and Structured Robust Information Theoretic Learning for Image Analysis","Y. Deng; F. Bao; X. Deng; R. Wang; Y. Kong; Q. Dai","Automation Department, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Automation Department, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; Automation Department, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","","2016","25","9","4209","4221","This paper presents a robust information theoretic (RIT) model to reduce the uncertainties, i.e., missing and noisy labels, in general discriminative data representation tasks. The fundamental pursuit of our model is to simultaneously learn a transformation function and a discriminative classifier that maximize the mutual information of data and their labels in the latent space. In this general paradigm, we, respectively, discuss three types of the RIT implementations with linear subspace embedding, deep transformation, and structured sparse learning. In practice, the RIT and deep RIT are exploited to solve the image categorization task whose performances will be verified on various benchmark data sets. The structured sparse RIT is further applied to a medical image analysis task for brain magnetic resonance image segmentation that allows group-level feature selections on the brain tissues.","","","10.1109/TIP.2016.2588330","National Natural Science Foundation of China; National Science Foundation of Jiangsu Province, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506023","Data embedding;mutual information;deep learning;structured-sparse learning;image classification;brain MRI segmentation","Mutual information;Robustness;Entropy;Uncertainty;Data models;Probability density function;Noise measurement","biological tissues;image segmentation;learning (artificial intelligence);magnetic resonance imaging;medical image processing","deep robust information theoretic learning;structured robust information theoretic learning;image analysis;robust information theoretic model;transformation function;discriminative classifier;RIT implementations;linear subspace embedding;deep transformation;image categorization task;medical image analysis;brain magnetic resonance image segmentation;group-level feature selections;brain tissues","","5","55","","","","","IEEE","IEEE Journals"
"Deep Fusion of Multiple Semantic Cues for Complex Event Recognition","X. Zhang; H. Zhang; Y. Zhang; Y. Yang; M. Wang; H. Luan; J. Li; T. Chua","Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computing, National University of Singapore, Singapore; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computing, National University of Singapore, Singapore","IEEE Transactions on Image Processing","","2016","25","3","1033","1046","We present a deep learning strategy to fuse multiple semantic cues for complex event recognition. In particular, we tackle the recognition task by answering how to jointly analyze human actions (who is doing what), objects (what), and scenes (where). First, each type of semantic features (e.g., human action trajectories) is fed into a corresponding multi-layer feature abstraction pathway, followed by a fusion layer connecting all the different pathways. Second, the correlations of how the semantic cues interacting with each other are learned in an unsupervised cross-modality autoencoder fashion. Finally, by fine-tuning a large-margin objective deployed on this deep architecture, we are able to answer the question on how the semantic cues of who, what, and where compose a complex event. As compared with the traditional feature fusion methods (e.g., various early or late strategies), our method jointly learns the essential higher level features that are most effective for fusion and recognition. We perform extensive experiments on two real-world complex event video benchmarks, MED'11 and CCV, and demonstrate that our method outperforms the best published results by 21% and 11%, respectively, on an event recognition task.","","","10.1109/TIP.2015.2511585","National High Technology Research and Development Program of China; National University of Singapore-Tsinghua Extreme Search Project; National Nature Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364255","multimedia event recognition;deep learning;fusion;Multimedia event recognition;deep learning;fusion","Semantics;Correlation;Fuses;Feature extraction;Training;Machine learning;Kernel","feature extraction;image fusion;image motion analysis;image recognition;learning (artificial intelligence);video signal processing","deep fusion;multiple semantic cues;complex event recognition;deep learning strategy;human actions;semantic features;multilayer feature abstraction pathway;fusion layer;feature fusion methods","","30","48","","","","","IEEE","IEEE Journals"
"Wishart Deep Stacking Network for Fast POLSAR Image Classification","L. Jiao; F. Liu","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Image Processing","","2016","25","7","3273","3286","Inspired by the popular deep learning architecture, deep stacking network (DSN), a specific deep model for polarimetric synthetic aperture radar (POLSAR) image classification is proposed in this paper, which is named Wishart DSN (W-DSN). First of all, a fast implementation of Wishart distance is achieved by a special linear transformation, which speeds up the classification of POLSAR image and makes it possible to use this polarimetric information in the following neural network (NN). Then, a single-hidden-layer NN based on the fast Wishart distance is defined for POLSAR image classification, which is named Wishart network (WN) and improves the classification accuracy. Finally, a multi-layer NN is formed by stacking WNs, which is in fact the proposed deep learning architecture W-DSN for POLSAR image classification and improves the classification accuracy further. In addition, the structure of WN can be expanded in a straightforward way by adding hidden units if necessary, as well as the structure of the W-DSN. As a preliminary exploration on formulating specific deep learning architecture for POLSAR image classification, the proposed methods may establish a simple but clever connection between POLSAR image interpretation and deep learning. The experiment results tested on real POLSAR image show that the fast implementation of Wishart distance is very efficient (a POLSAR image with 768 000 pixels can be classified in 0.53 s), and both the single-hidden-layer architecture WN and the deep learning architecture W-DSN for POLSAR image classification perform well and work efficiently.","","","10.1109/TIP.2016.2567069","National Basic Research Program (973 Program) of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7468458","Deep Stacking Network (DSN);POLSAR image classification;Wishart Network (WN);Wishart Deep Stacking Network (W-DSN);Deep stacking network (DSN);POLSAR image classification;Wishart network (WN);Wishart deep stacking network (W-DSN)","Machine learning;Stacking;Covariance matrices;Training;Artificial neural networks;Labeling","image classification;learning (artificial intelligence);neural nets;radar imaging;radar polarimetry;synthetic aperture radar","W-DSN;polarimetric synthetic aperture radar image classification;POLSAR image classification;deep learning architecture;neural network;Wishart deep stacking network","","37","38","","","","","IEEE","IEEE Journals"
"Learning Cascaded Deep Auto-Encoder Networks for Face Alignment","R. Weng; J. Lu; Y. Tan; J. Zhou","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Automation, Tsinghua University, Beijing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Multimedia","","2016","18","10","2066","2078","In this paper, we propose a new cascaded deep auto-encoder networks (CDAN) approach for face alignment. Our framework consists of a global exemplar-based deep auto-encoder network (GEDAN) and a series of localized deep auto-encoder networks (LDAN) in a cascaded fashion. The global network takes a low-resolution holistic facial image as input and generates a preliminary facial landmark configuration. The following localized networks sample pose-indexed local features around current landmark positions, and refine the landmark positions with increasingly higher image resolutions. Our network architectures are designed to achieve greater robustness against pose variations as well as higher landmark estimation accuracy. Experimental results on three datasets show that the proposed approach achieves superior alignment accuracy with real-time speed.","","","10.1109/TMM.2016.2591508","National Key Research and Development Program of China; National Natural Science Foundation of China; National Basic Research Program of China; Ministry of Education of China; Tsinghua University Initiative Scientific Research Program; National Research Foundation; Prime Minister's Office, Singapore; IDM Futures Funding Initiative and administered; IDM Programme Office at the Rapid-Rich Object Search Laboratory, Nanyang Technological University, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7513378","Auto-encoder;deep learning;face alignment","Face;Feature extraction;Shape;Machine learning;Face recognition;Image resolution;Real-time systems","face recognition;feature extraction;image resolution;learning (artificial intelligence);pose estimation","deep learning;face alignment;cascaded deep auto-encoder networks;CDAN;global exemplar-based deep auto-encoder network;GEDAN;localized deep auto-encoder networks;LDAN;global network;low-resolution holistic facial image;facial landmark configuration;pose-indexed local features;landmark positions;image resolutions;network architectures;pose variations;landmark estimation accuracy","","20","51","","","","","IEEE","IEEE Journals"
"Severely Blurred Object Tracking by Learning Deep Image Representations","J. Ding; Y. Huang; W. Liu; K. Huang","People’s Public Security University of China, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Nanyang Normal University, Henan, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2016","26","2","319","331","An implicit assumption in many generic object trackers is that the videos are blur free. However, motion blur is very common in real videos. The performance of a generic object tracker may drop significantly when it is applied to videos with severe motion blur. In this paper, we propose a new Tracking-Learning-Data approach to transfer a generic object tracker to a blur-invariant object tracker without deblurring image sequences. Before object tracking, a large set of unlabeled images is used to learn objects' visual prior knowledge, which is then transferred to the appearance model of a specific target. During object tracking, online training samples are collected from the tracking results and the context information. Different blur kernels are involved with the training samples to increase the robustness of the appearance model to severe blur, and the motion parameters of the object are estimated in the particle filter framework. Extensive experimental results demonstrate that the proposed algorithm can robustly track objects not only in severely blurred videos but also in other challenging scenes.","","","10.1109/TCSVT.2015.2406231","Fundamental Research Funds for the Central Universities; National High Technology Research and Development Program of China; National Natural Science Foundation of China; SAMSUNG Global Research Outreach Program, CCF-Tencent Program and 360 OpenLab Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7046412","Object tracking;severe blur;deep learning;Deep learning;object tracking;severe blur","Videos;Target tracking;Training;Object tracking;Robustness;Algorithm design and analysis","image representation;image restoration;image sequences;learning (artificial intelligence);object tracking;particle filtering (numerical methods)","severely blurred object tracking;learning deep image representations;generic object trackers;tracking-learning-data approach;blur-invariant object tracker;image sequences;appearance model;online training samples;motion parameters;particle filter framework;blur kernels","","14","36","","","","","IEEE","IEEE Journals"
"Human Detection and Activity Classification Based on Micro-Doppler Signatures Using Deep Convolutional Neural Networks","Y. Kim; T. Moon","Dept. of Electr. & Comput. Eng., California State Univ., Fresno, CA, USA; Dept. of Inf. & Commun. Eng., Daegu Gyeongbuk Inst. of Sci. & Technol., Daegu, South Korea","IEEE Geoscience and Remote Sensing Letters","","2016","13","1","8","12","We propose the use of deep convolutional neural networks (DCNNs) for human detection and activity classification based on Doppler radar. Previously, proposed schemes for these problems remained in the conventional supervised learning paradigm that relies on the design of handcrafted features. Whereas these schemes attained high accuracy, the requirement for domain knowledge of each problem limits the scalability of the proposed schemes. In this letter, we present an alternative deep learning approach. We apply the DCNN, one of the most successful deep learning algorithms, directly to a raw micro-Doppler spectrogram for both human detection and activity classification problem. The DCNN can jointly learn the necessary features and classification boundaries using the measured data without employing any explicit features on the micro-Doppler signals. We show that the DCNN can achieve accuracy results of 97.6% for human detection and 90.9% for human activity classification.","","","10.1109/LGRS.2015.2491329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314905","Convolutional neural network;deep learning;human activity classification;human detection;micro-Doppler;Convolutional neural network;deep learning;human activity classification;human detection;micro-Doppler","Convolution;Spectrogram;Doppler radar;Machine learning;Feature extraction;Accuracy","Doppler radar;geophysical techniques;neural nets","human detection;microDoppler signatures;deep convolutional neural networks;DCNN;Doppler radar;conventional supervised learning paradigm;deep learning algorithms;microDoppler spectrogram;activity classification problem;classification boundaries","","139","23","","","","","IEEE","IEEE Journals"
"Locality Sensitive Deep Learning for Detection and Classification of Nuclei in Routine Colon Cancer Histology Images","K. Sirinukunwattana; S. E. A. Raza; Y. Tsang; D. R. J. Snead; I. A. Cree; N. M. Rajpoot","Department of Computer Science, University of Warwick, Coventry, UK; Department of Computer Science, University of Warwick, Coventry, UK; Department of Pathology, University Hospitals Coventry and Warwickshire, Walsgrave, Coventry, UK; Department of Pathology, University Hospitals Coventry and Warwickshire, Walsgrave, Coventry, UK; Department of Pathology, University Hospitals Coventry and Warwickshire, Walsgrave, Coventry, UK; Department of Computer Science and Engineering, Qatar University, Qatar","IEEE Transactions on Medical Imaging","","2016","35","5","1196","1206","Detection and classification of cell nuclei in histopathology images of cancerous tissue stained with the standard hematoxylin and eosin stain is a challenging task due to cellular heterogeneity. Deep learning approaches have been shown to produce encouraging results on histopathology images in various studies. In this paper, we propose a Spatially Constrained Convolutional Neural Network (SC-CNN) to perform nucleus detection. SC-CNN regresses the likelihood of a pixel being the center of a nucleus, where high probability values are spatially constrained to locate in the vicinity of the centers of nuclei. For classification of nuclei, we propose a novel Neighboring Ensemble Predictor (NEP) coupled with CNN to more accurately predict the class label of detected cell nuclei. The proposed approaches for detection and classification do not require segmentation of nuclei. We have evaluated them on a large dataset of colorectal adenocarcinoma images, consisting of more than 20,000 annotated nuclei belonging to four different classes. Our results show that the joint detection and classification of the proposed SC-CNN and NEP produces the highest average F1 score as compared to other recently published approaches. Prospectively, the proposed methods could offer benefit to pathology practice in terms of quantitative analysis of tissue constituents in whole-slide images, and potentially lead to a better understanding of cancer.","","","10.1109/TMI.2016.2525803","Qatar National Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7399414","Convolutional neural network;deep learning;histology image analysis;nucleus detection","Cancer;Machine learning;Tumors;Computer architecture;Feature extraction;Microprocessors;Shape","biological organs;biomedical optical imaging;cancer;cellular biophysics;image classification;learning (artificial intelligence);medical image processing;probability;tumours","locality sensitive deep learning;cell nuclei classification;cell nuclei detection;routine colon cancer histology images;cancerous tissue;histopathology images;standard hematoxylin;eosin stain;cellular heterogeneity;spatially constrained convolutional neural network;high-probability values;neighboring ensemble predictor;dataset;colorectal adenocarcinoma images;joint detection;SC-CNN;NEP;highest average F1 score;quantitative analysis;tissue constituents;whole-slide images","Cell Nucleus;Cell Proliferation;Colon;Colonic Neoplasms;Histocytochemistry;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Neural Networks (Computer)","290","38","","","","","IEEE","IEEE Journals"
"Towards Deep Developmental Learning","O. Sigaud; A. Droniou","Sorbonne Universités UPMC Univ Paris 06, UMR 7222, Paris, France; Sorbonne Universités UPMC Univ Paris 06, UMR 7222, Paris, France","IEEE Transactions on Cognitive and Developmental Systems","","2016","8","2","99","114","Deep learning techniques are having an undeniable impact on general pattern recognition issues. In this paper, from a developmental robotics perspective, we scrutinize deep learning techniques under the light of their capability to construct a hierarchy of meaningful multimodal representations from the raw sensors of robots. These investigations reveal the differences between the methodological constraints of pattern recognition and those of developmental robotics. In particular, we outline the necessity to rely on unsupervised rather than supervised learning methods and we highlight the need for progress towards the implementation of hierarchical predictive processing capabilities. Based on these new tools, we outline the emergence of a new domain that we call deep developmental learning.","","","10.1109/TAMD.2015.2496248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312936","Affordances;deep learning;developmental robotics;hierarchical predictive processing;sensorimotor contingencies","Robot sensing systems;Biological neural networks;Machine learning;Neurons;Computer architecture;Pattern recognition","pattern recognition;robots;unsupervised learning","deep developmental learning;general pattern recognition issues;developmental robotics perspective;multimodal representations;raw sensors;methodological constraints;unsupervised learning method;hierarchical predictive processing capabilities","","21","164","","","","","IEEE","IEEE Journals"
"The SP Theory of Intelligence: Distinctive Features and Advantages","J. G. Wolff","CognitionResearch.org, Menai Bridge, U.K.","IEEE Access","","2016","4","","216","246","This paper aims to highlight distinctive features of the SP theory of intelligence, realized in the SP computer model, and its apparent advantages compared with some AI-related alternatives. Perhaps most importantly, the theory simplifies and integrates observations and concepts in AI-related areas, and has potential to simplify and integrate of structures and processes in computing systems. Unlike most other AI-related theories, the SP theory is itself a theory of computing, which can be the basis for new architectures for computers. Fundamental in the theory is information compression via the matching and unification of patterns and, more specifically, via a concept of multiple alignment. The theory promotes transparency in the representation and processing of knowledge, and unsupervised learning of natural structures via information compression. It provides an interpretation of aspects of mathematics and an interpretation of phenomena in human perception and cognition. concepts in the theory may be realized in terms of neurons and their inter-connections (SP-neural). These features and advantages of the SP system are discussed in relation to AI-related alternatives: the concept of minimum length encoding and related concepts, how computational and energy efficiency in computing may be achieved, deep learning in neural networks, unified theories of cognition and related research, universal search, Bayesian networks and some other models for AI, IBM's Watson, solving problems associated with big data and in the development of intelligence in autonomous robots, pattern recognition and vision, the learning and processing of natural language, exact and inexact forms of reasoning, representation and processing of diverse forms of knowledge, and software engineering. In conclusion, the SP system can provide a firm foundation for the long-term development of AI and related areas, and at the same time, it may deliver useful results on relatively short timescales.","","","10.1109/ACCESS.2015.2513822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7369936","artificial intelligence;information compression;multiple alignment;perception;cognition;neural networks;deep learning;unsupervised learning;reasoning;mathematics;Artificial intelligence;information compression;multiple alignment;perception;cognition;neural networks;deep learning;unsupervised learning;reasoning;mathematics","Artificial intelligence;Computational modeling;Information compression;Computer architecture;Unsupervised learning;Turing machines;Neural networks;Mathematics;Unsupervised learning","knowledge representation;learning (artificial intelligence);neural nets;pattern matching","deep-learning;energy efficiency;computational efficiency;minimum length encoding;SP-neural;neuron interconnections;cognition;human perception;information compression;unsupervised learning;knowledge representation;knowledge processing;pattern unification;pattern matching;information compression;AI;SP computer model;SP intelligence theory","","5","83","","","","","IEEE","IEEE Journals"
"Deep Learning Earth Observation Classification Using ImageNet Pretrained Networks","D. Marmanis; M. Datcu; T. Esch; U. Stilla","German Aerosp. Center, German Remote Sensing Center, Wessling, Germany; German Aerosp. Center, Remote Sensing Technol. Inst., Wessling, Germany; German Aerosp. Center, German Remote Sensing Center, Wessling, Germany; Dept. of Photogrammetry & Remote Sensing, Tech. Univ. of Munich, Munich, Germany","IEEE Geoscience and Remote Sensing Letters","","2016","13","1","105","109","Deep learning methods such as convolutional neural networks (CNNs) can deliver highly accurate classification results when provided with large enough data sets and respective labels. However, using CNNs along with limited labeled data can be problematic, as this leads to extensive overfitting. In this letter, we propose a novel method by considering a pretrained CNN designed for tackling an entirely different classification problem, namely, the ImageNet challenge, and exploit it to extract an initial set of representations. The derived representations are then transferred into a supervised CNN classifier, along with their class labels, effectively training the system. Through this two-stage framework, we successfully deal with the limited-data problem in an end-to-end processing scheme. Comparative results over the UC Merced Land Use benchmark prove that our method significantly outperforms the previously best stated results, improving the overall accuracy from 83.1% up to 92.4%. Apart from statistical improvements, our method introduces a novel feature fusion algorithm that effectively tackles the large data dimensionality by using a simple and computationally efficient approach.","","","10.1109/LGRS.2015.2499239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7342907","Convolutional neural networks (CNNs);deep learning (DL);feature extraction;land-use classification;pretrained network;remote sensing (RS);Convolutional neural networks (CNNs);deep learning (DL);feature extraction;land-use classification;pretrained network;remote sensing (RS)","Feature extraction;Remote sensing;Arrays;Adaptation models;Data models;Neural networks;Training","geophysical techniques;neural nets","deep learning earth observation classification;ImageNet pretrained networks;deep learning method;convolutional neural networks;supervised CNN classifier;limited labeled data;limited-data problem;end-to-end processing scheme;UC Merced Land Use benchmark;fusion algorithm","","167","12","","","","","IEEE","IEEE Journals"
"Semantic Annotation of High-Resolution Satellite Images via Weakly Supervised Learning","X. Yao; J. Han; G. Cheng; X. Qian; L. Guo","School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Electronics and Information Engineering, Xi'an Jiaotong University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China","IEEE Transactions on Geoscience and Remote Sensing","","2016","54","6","3660","3671","In this paper, we focus on tackling the problem of automatic semantic annotation of high resolution (HR) optical satellite images, which aims to assign one or several predefined semantic concepts to an image according to its content. The main challenges arise from the difficulty of characterizing complex and ambiguous contents of the satellite images and the high human labor cost caused by preparing a large amount of training examples with high-quality pixel-level labels in fully supervised annotation methods. To address these challenges, we propose a unified annotation framework by combining discriminative high-level feature learning and weakly supervised feature transferring. Specifically, an efficient stacked discriminative sparse autoencoder (SDSAE) is first proposed to learn high-level features on an auxiliary satellite image data set for the land-use classification task. Inspired by the motivation that the encoder of the prelearned SDSAE can be regarded as a generic high-level feature extractor for HR optical satellite images, we then transfer the learned high-level features to semantic annotation. To compensate the difference between the auxiliary data set and the annotation data set, the transferred high-level features are further fine-tuned in a weakly supervised scheme by using the tile-level annotated training data. Finally, the fine-tuning process is formulated as an ultimate optimization problem, which can be solved efficiently with our proposed alternate iterative optimization method. Comprehensive experiments on a publicly available land-use classification data set and an annotation data set demonstrate the superiority of our SDSAE-based high-level feature learning method and the effectiveness of our weakly supervised semantic annotation framework compared with state-of-the-art fully supervised annotation methods.","","","10.1109/TGRS.2016.2523563","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7414501","Deep feature learning;feature transferring;satellite image annotation;weakly supervised learning (WSL);Deep feature learning;feature transferring;satellite image annotation;weakly supervised learning (WSL)","Satellites;Semantics;Training;Feature extraction;Learning systems;Remote sensing;Visualization","feature extraction;geophysical image processing;image classification;land use;learning (artificial intelligence);optimisation;semantic networks;set theory;terrain mapping","semantic annotation;weakly supervised learning;high resolution optical satellite image;automatic semantic annotation;image semantic concept;human labor;high-quality pixel-level label;efficient stacked discriminative sparse autoencoder;SDSAE;land-use classification task;high-level feature extractor;HR optical satellite image;tile-level annotated training data;weakly supervised scheme;fine-tuning process;ultimate optimization problem;alternate iterative optimization method;land-use classification data set;SDSAE-based high-level feature learning method;stacked discriminative sparse autoencoder;generic high-level feature extractor;learned high-level features;annotation data set;transferred high-level features;tile-level annotated training data;optimization problem;semantic annotation framework","","135","55","","","","","IEEE","IEEE Journals"
"Deep Learning Guided Partitioned Shape Model for Anterior Visual Pathway Segmentation","A. Mansoor; J. J. Cerrolaza; R. Idrees; E. Biggs; M. A. Alsharid; R. A. Avery; M. G. Linguraru","Children's National Health System, Washington; Children's National Health System, Washington; School of Medicine and Health Sciences, George Washington University, Washington; Children's National Health System, Washington; Khalifa University, UAE; Children's National Health System, Washington; Children's National Health System, Washington","IEEE Transactions on Medical Imaging","","2016","35","8","1856","1865","Analysis of cranial nerve systems, such as the anterior visual pathway (AVP), from MRI sequences is challenging due to their thin long architecture, structural variations along the path, and low contrast with adjacent anatomic structures. Segmentation of a pathologic AVP (e.g., with low-grade gliomas) poses additional challenges. In this work, we propose a fully automated partitioned shape model segmentation mechanism for AVP steered by multiple MRI sequences and deep learning features. Employing deep learning feature representation, this framework presents a joint partitioned statistical shape model able to deal with healthy and pathological AVP. The deep learning assistance is particularly useful in the poor contrast regions, such as optic tracts and pathological areas. Our main contributions are: 1) a fast and robust shape localization method using conditional space deep learning, 2) a volumetric multiscale curvelet transform-based intensity normalization method for robust statistical model, and 3) optimally partitioned statistical shape and appearance models based on regional shape variations for greater local flexibility. Our method was evaluated on MRI sequences obtained from 165 pediatric subjects. A mean Dice similarity coefficient of 0.779 was obtained for the segmentation of the entire AVP (optic nerve only =0.791) using the leave-one-out validation. Results demonstrated that the proposed localized shape and sparse appearance-based learning approach significantly outperforms current state-of-the-art segmentation approaches and is as robust as the manual segmentation.","","","10.1109/TMI.2016.2535222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420737","Anterior visual pathway;intensity normalization;MRI;partitioned statistical model;shape model;sparse learning","Biomedical optical imaging;Magnetic resonance imaging;Shape;Optical imaging;Machine learning;Pathology;Robustness","biomedical MRI;curvelet transforms;eye;image segmentation;image sequences;medical image processing;paediatrics;statistical analysis","deep learning guided partitioned shape model;anterior visual pathway segmentation;cranial nerve systems;MRI sequences;anatomic structures;pathologic AVP segmentation;low-grade gliomas;deep learning feature representation;joint partitioned statistical shape model;shape localization method;conditional space deep learning;volumetric multiscale curvelet transform-based intensity normalization method;robust statistical model;pediatric subjects;Dice similarity coefficient;optic nerve;sparse appearance-based learning approach","Humans;Magnetic Resonance Imaging;Models, Statistical;Reproducibility of Results;Visual Pathways","12","38","","","","","IEEE","IEEE Journals"
"Deep Learning-Based Model Reduction for Distributed Parameter Systems","M. Wang; H. Li; X. Chen; Y. Chen","Department of Systems Engineering and Engineering Management, City University of Hong Kong, Hong Kong; Department of Systems Engineering and Engineering Management, City University of Hong Kong, Hong Kong; Key Laboratory of Mechanical Equipment Manufacturing and Control Technology of Ministry of Education, Guangdong University of Technology, Guangzhou, China; Key Laboratory of Mechanical Equipment Manufacturing and Control Technology of Ministry of Education, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2016","46","12","1664","1674","This paper presents a deep learning-based model reduction method for distributed parameter systems (DPSs). The proposed method includes three phases. In phase I, numerical or experimental data of the spatiotemporal distribution is reduced into low-dimensional representations using the deep auto-encoder (DAE). In phase II, the low-dimensional representations are used to establish the reduced-order model. In phase III, the reduced model is then used to reconstruct the high-dimensional DPS. Experimental studies are conducted to validate the proposed method. The proposed method is compared with the classical proper orthogonal decomposition method and demonstrates better modeling accuracy and efficiency in the experiments.","","","10.1109/TSMC.2016.2605159","Projects from Research Grants Council of Hong Kong; Project from Guangdong Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7572934","Deep learning;distributed parameter system (DPS);model reduction;restricted Boltzmann machine (RBM);spatiotemporal dynamics","Reduced order systems;Spatiotemporal phenomena;Mathematical model;Machine learning;Distributed parameter systems;Modeling;Training","distributed parameter systems;learning (artificial intelligence)","deep learning-based model reduction;distributed parameter systems;spatiotemporal distribution;low-dimensional representations;deep auto-encoder;DAE;reduced-order model;high-dimensional DPS;orthogonal decomposition method","","21","41","","","","","IEEE","IEEE Journals"
"Unsupervised 3D Local Feature Learning by Circle Convolutional Restricted Boltzmann Machine","Z. Han; Z. Liu; J. Han; C. Vong; S. Bu; X. Li","Northwestern Polytechnical University, Xi’an, China; Northwestern Polytechnical University, Xi’an, China; Northwestern Polytechnical University, Xi’an, China; Department of Computer and Information Science, University of Macau, Macau, China; Northwestern Polytechnical University, Xi’an, China; State Key Laboratory of Transient Optics and Photonics, Center for OPTical IMagery Analysis and Learning, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China","IEEE Transactions on Image Processing","","2016","25","11","5331","5344","Extracting local features from 3D shapes is an important and challenging task that usually requires carefully designed 3D shape descriptors. However, these descriptors are hand-crafted and require intensive human intervention with prior knowledge. To tackle this issue, we propose a novel deep learning model, namely circle convolutional restricted Boltzmann machine (CCRBM), for unsupervised 3D local feature learning. CCRBM is specially designed to learn from raw 3D representations. It effectively overcomes obstacles such as irregular vertex topology, orientation ambiguity on the 3D surface, and rigid or slightly non-rigid transformation invariance in the hierarchical learning of 3D data that cannot be resolved by the existing deep learning models. Specifically, by introducing the novel circle convolution, CCRBM holds a novel ring-like multi-layer structure to learn 3D local features in a structure preserving manner. Circle convolution convolves across 3D local regions via rotating a novel circular sector convolution window in a consistent circular direction. In the process of circle convolution, extra points are sampled in each 3D local region and projected onto the tangent plane of the center of the region. In this way, the projection distances in each sector window are employed to constitute a novel local raw 3D representation called projection distance distribution (PDD). In addition, to eliminate the initial location ambiguity of a sector window, the Fourier transform modulus is used to transform the PDD into the Fourier domain, which is then conveyed to CCRBM. Experiments using the learned local features are conducted on three aspects: global shape retrieval, partial shape retrieval, and shape correspondence. The experimental results show that the learned local features outperform other state-of-the-art 3D shape descriptors.","","","10.1109/TIP.2016.2605920","National Natural Science Foundation of China; NWPU Basic Research Fund; Open Fund of State Key Laboratory of CAD and CG in Zhejiang University; Open Research Foundation of State Key Laboratory of Digital Manufacturing Equipment and Technology in Huazhong University of Science and Technology; Fund of National Engineering and Research Center for Commercial Aircraft Manufacturing; Shaanxi Natural Science Fund; University of Macau Research Funding; FDCT Macau; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559748","Circle convolutional restricted Boltzmann machine;deep learning;projection distance distribution;geometry processing;fourier transform modulus;3D shapes","Three-dimensional displays;Shape;Machine learning;Convolution;Solid modeling;Feature extraction;Two dimensional displays","Boltzmann machines;convolution;feature extraction;feedforward neural nets;Fourier transforms;image representation;image retrieval;unsupervised learning","partial shape retrieval;shape correspondence;global shape retrieval;Fourier transform modulus;PDD;projection distance distribution;novel local raw 3D representation;consistent circular direction;novel circular sector convolution window;3D local regions;novel ring-like multilayer structure;CCRBM;deep learning model;3D shape descriptors;local feature extraction;circle convolutional restricted Boltzmann machine;unsupervised 3D local feature learning","","13","71","","","","","IEEE","IEEE Journals"
"Intra- and Inter-Fractional Variation Prediction of Lung Tumors Using Fuzzy Deep Learning","S. Park; S. J. Lee; E. Weiss; Y. Motai","Department of Electrical and Computer Engineering, Virginia Commonwealth University, Richmond, VA, USA; Department of Computer Science, Texas A&M University–Texarkana, Texarkana, TX, USA; Department of Radiation Oncology, Virginia Commonwealth University, Richmond, VA, USA; Department of Electrical and Computer Engineering, Virginia Commonwealth University, Richmond, VA, USA","IEEE Journal of Translational Engineering in Health and Medicine","","2016","4","","1","12","Tumor movements should be accurately predicted to improve delivery accuracy and reduce unnecessary radiation exposure to healthy tissue during radiotherapy. The tumor movements pertaining to respiration are divided into intra-fractional variation occurring in a single treatment session and inter-fractional variation arising between different sessions. Most studies of patients' respiration movements deal with intra-fractional variation. Previous studies on inter-fractional variation are hardly mathematized and cannot predict movements well due to inconstant variation. Moreover, the computation time of the prediction should be reduced. To overcome these limitations, we propose a new predictor for intra- and inter-fractional data variation, called intra- and inter-fraction fuzzy deep learning (IIFDL), where FDL, equipped with breathing clustering, predicts the movement accurately and decreases the computation time. Through the experimental results, we validated that the IIFDL improved root-mean-square error (RMSE) by 29.98% and prediction overshoot by 70.93%, compared with existing methods. The results also showed that the IIFDL enhanced the average RMSE and overshoot by 59.73% and 83.27%, respectively. In addition, the average computation time of IIFDL was 1.54 ms for both intra- and inter-fractional variation, which was much smaller than the existing methods. Therefore, the proposed IIFDL might achieve real-time estimation as well as better tracking techniques in radiotherapy.","","","10.1109/JTEHM.2016.2516005","National Science Foundation CAREER; National Center for Advancing Translational Sciences through the CTSA Program; Center for Clinical and Translational Research Endowment Fund within Virginia Commonwealth University through the Presidential Research Incentive Program; American Cancer Society within the Institutional Research through the Massey Cancer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7377034","Fuzzy deep learning;intra-fractional variation;inter-fractional variation;breathing prediction;tumor tracking;Fuzzy deep learning;intra-fractional variation;inter-fractional variation;breathing prediction;tumor tracking","Tumors;Artificial neural networks;Machine learning;Training;Lungs;Cancer;Predictive models","feature extraction;fuzzy logic;fuzzy neural nets;lung;medical signal processing;pattern clustering;pneumodynamics;radiation therapy;tumours","tumor tracking;root-mean-square error;breathing clustering;respiration movements;radiotherapy;fuzzy deep learning;lung tumors;interfractional variation prediction;intrafractional variation prediction","","14","43","","","","","IEEE","IEEE Journals"
"Image-Based Monitoring of Jellyfish Using Deep Learning Architecture","H. Kim; J. Koo; D. Kim; S. Jung; J. Shin; S. Lee; H. Myung","Urban Robotics Laboratory, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Urban Robotics Laboratory, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Urban Robotics Laboratory, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Urban Robotics Laboratory, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Urban Robotics Laboratory, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Institute for Infocomm Research, Singapore; Urban Robotics Laboratory, Korea Advanced Institute of Science and Technology, Daejeon, Korea","IEEE Sensors Journal","","2016","16","8","2215","2216","Jellyfish blooms have caused great damage to the fishery industry. In efforts to solve this problem, various systems to remove jellyfish have been proposed. This letter presents preliminary results of applying an image-based jellyfish distribution recognition algorithm to increase the efficiency of an existing jellyfish removal system. By using a convolutional neural network and dedicated image processing techniques, the experimental results show reasonable performance for real-world application.","","","10.1109/JSEN.2016.2517823","Robot Industrial Cluster Construction Program through the Ministry of Trade, Industry and Energy and Korea Institute for Advancement of Technology; National Research Foundation of Korea within the Ministry of Science, ICT and Future Planning through the Korean Government; Ministry of Land, Infrastructure and Transport through the U-City Master and Doctor Course Grant Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381601","jellyfish monitoring;object recognition;convolutional neural network;Jellyfish monitoring;object recognition;convolutional neural network","Monitoring;Machine learning;Aquaculture;Image recognition;Robots;Classification algorithms;Collaboration","computerised monitoring;image recognition;learning (artificial intelligence);neural nets","dedicated image processing technique;convolutional neural network;jellyfish removal system;image-based jellyfish distribution recognition algorithm;fishery industry;jellyfish bloom;deep learning architecture;image-based monitoring system","","7","8","","","","","IEEE","IEEE Journals"
"Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?","R. Giryes; G. Sapiro; A. M. Bronstein","School of Electrical Engineering, Faculty of Engineering, Tel-Aviv University, Ramat Aviv, Israel; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; School of Electrical Engineering, Faculty of Engineering, Tel-Aviv University, Ramat Aviv, Israel","IEEE Transactions on Signal Processing","","2016","64","13","3444","3457","Three important properties of a classification machinery are i) the system preserves the core information of the input data; ii) the training examples convey information about unseen data; and iii) the system is able to treat differently points from different classes. In this paper, we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.","","","10.1109/TSP.2016.2546221","NSF; ONR; NGA; NSSEFF; ARO; ERC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439822","Artificial neural networks;computation theory;deep learning;learning systems;Artificial neural networks;computation theory;deep learning;learning systems","Training;Measurement;Neural networks;Compressed sensing;Wavelet transforms;Manifolds;Scattering","Gaussian processes;learning (artificial intelligence);neural nets;pattern classification","deep neural networks;random Gaussian weights;universal classification strategy;classification machinery;distance-preserving data embedding;in-class data;out-of-class data;compressed sensing;dictionary learning","","22","57","","","","","IEEE","IEEE Journals"
"A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots","A. Giusti; J. Guzzi; D. C. Cireşan; F. He; J. P. Rodríguez; F. Fontana; M. Faessler; C. Forster; J. Schmidhuber; G. D. Caro; D. Scaramuzza; L. M. Gambardella","Dalle Molle Institute for Artificial Intelligence (IDSIA), USI-SUPSI, Lugano, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), USI-SUPSI, Lugano, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), USI-SUPSI, Lugano, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), USI-SUPSI, Lugano, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), USI-SUPSI, Lugano, Switzerland; Robotics and Perception Group (RPG), University of Zurich, Zurich, Switzerland; Robotics and Perception Group (RPG), University of Zurich, Zurich, Switzerland; Robotics and Perception Group (RPG), University of Zurich, Zurich, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), USI-SUPSI, Lugano, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), USI-SUPSI, Lugano, Switzerland; Robotics and Perception Group (RPG), University of Zurich, Zurich, Switzerland; Dalle Molle Institute for Artificial Intelligence (IDSIA), USI-SUPSI, Lugano, Switzerland","IEEE Robotics and Automation Letters","","2016","1","2","661","667","We study the problem of perceiving forest or mountain trails from a single monocular image acquired from the viewpoint of a robot traveling on the trail itself. Previous literature focused on trail segmentation, and used low-level features such as image saliency or appearance contrast; we propose a different approach based on a deep neural network used as a supervised image classifier. By operating on the whole image at once, our system outputs the main direction of the trail compared to the viewing direction. Qualitative and quantitative results computed on a large real-world dataset (which we provide for download) show that our approach outperforms alternatives, and yields an accuracy comparable to the accuracy of humans that are tested on the same image classification task. Preliminary results on using this information for quadrotor control in unseen trails are reported. To the best of our knowledge, this is the first letter that describes an approach to perceive forest trials, which is demonstrated on a quadrotor micro aerial vehicle.","","","10.1109/LRA.2015.2509024","Swiss National Science Foundation (SNSF); National Centre of Competence in Research (NCCR) Robotics; Supervised Deep/Recurrent Nets; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358076","Visual-Based Navigation;Aerial Robotics;Machine Learning;Deep Learning;Visual-Based Navigation;Aerial Robotics;Machine Learning;Deep Learning","Cameras;Robot vision systems;Roads;Visual perception;Mobile robots;Image segmentation","autonomous aerial vehicles;helicopters;image classification;learning (artificial intelligence);microrobots;neural nets;robot vision","machine learning approach;visual perception;forest trails;mobile robots;monocular image;deep-neural network;supervised image classifier;viewing direction;qualitative analysis;quantitative analysis;quadrotor microaerial vehicle control","","154","35","","","","","IEEE","IEEE Journals"
"Classification of Exacerbation Frequency in the COPDGene Cohort Using Deep Learning with Deep Belief Networks","J. Ying; J. Dutta; N. Guo; C. Hu; D. Zhou; A. Sitek; Q. Li","NA; NA; NA; NA; NA; NA; Massachusetts General Hospital and Harvard Medical School.(email:li.quanzheng@mgh.harvard.edu)","IEEE Journal of Biomedical and Health Informatics","","2016","PP","99","1","1","This study aims to develop an automatic classifier based on deep learning for exacerbation frequency in patients with chronic obstructive pulmonary disease (COPD). A threelayer deep belief network (DBN) with two hidden layers and one visible layer was employed to develop classification models and the models’ robustness to exacerbation was analyzed. Subjects from the COPDGene cohort were labeled with exacerbation frequency, defined as the number of exacerbation events per year. 10,300 subjects with 361 features each were included in the analysis. After feature selection and parameter optimization, the proposed classification method achieved an accuracy of 91.99%, using a 10-fold cross validation experiment. The analysis of DBN weights showed that there was a good visual spatial relationship between the underlying critical features of different layers. Our findings show that the most sensitive features obtained from the DBN weights are consistent with the consensus showed by clinical rules and standards for COPD diagnostics. We thus demonstrate that DBN is a competitive tool for exacerbation risk assessment for patients suffering from COPD.","","","10.1109/JBHI.2016.2642944","National Heart Lung and Blood Institute; National Institute on Aging; National Institute of Biomedical Imaging and Bioengineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792616","Chronic obstructive pulmonary disease (COPD);exacerbation;deep learning;deep belief network (DBN);Fisher score","Diseases;Hospitals;Machine learning;Lungs;Electronic mail;Gold;Feature extraction","","","","1","","","","","","IEEE","IEEE Early Access Articles"
"Combining Generative and Discriminative Representation Learning for Lung CT Analysis With Convolutional Restricted Boltzmann Machines","G. van Tulder; M. de Bruijne","Biomedical Imaging Group, Erasmus MC, Rotterdam, The Netherlands; Biomedical Imaging Group, Erasmus MC, Rotterdam, The Netherlands","IEEE Transactions on Medical Imaging","","2016","35","5","1262","1272","The choice of features greatly influences the performance of a tissue classification system. Despite this, many systems are built with standard, predefined filter banks that are not optimized for that particular application. Representation learning methods such as restricted Boltzmann machines may outperform these standard filter banks because they learn a feature description directly from the training data. Like many other representation learning methods, restricted Boltzmann machines are unsupervised and are trained with a generative learning objective; this allows them to learn representations from unlabeled data, but does not necessarily produce features that are optimal for classification. In this paper we propose the convolutional classification restricted Boltzmann machine, which combines a generative and a discriminative learning objective. This allows it to learn filters that are good both for describing the training data and for classification. We present experiments with feature learning for lung texture classification and airway detection in CT images. In both applications, a combination of learning objectives outperformed purely discriminative or generative learning, increasing, for instance, the lung tissue classification accuracy by 1 to 8 percentage points. This shows that discriminative learning can help an otherwise unsupervised feature learner to learn filters that are optimized for classification.","","","10.1109/TMI.2016.2526687","Nederlandse Organisatie voor Wetenschappelijk Onderzoek; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401039","Deep learning;lung;machine learning;neural network;pattern recognition and classification;representation learning;restricted Boltzmann machine;segmentation;X-ray imaging and computed tomography","Lungs;Standards;Feature extraction;Computed tomography;Neural networks;Learning systems;Training data","biological tissues;Boltzmann machines;channel bank filters;computerised tomography;feature extraction;image classification;image filtering;image texture;lung;medical image processing;pneumodynamics","generative representation learning;discriminative representation learning;lung CT analysis;convolutional restricted Boltzmann machines;tissue classification system;standard predefined filter banks;feature description;training data;generative learning objective;unlabeled data representations;lung texture classification;airway detection;lung tissue classification accuracy","Algorithms;Humans;Image Processing, Computer-Assisted;Lung;Machine Learning;Neural Networks (Computer);Tomography, X-Ray Computed","48","47","","","","","IEEE","IEEE Journals"
"Multi-Instance Deep Learning: Discover Discriminative Local Anatomies for Bodypart Recognition","Z. Yan; Y. Zhan; Z. Peng; S. Liao; Y. Shinagawa; S. Zhang; D. N. Metaxas; X. S. Zhou","Department of Computer Science, Rutgers University, Piscataway, NJ, USA; Siemens Healthcare, Malvern, PA, USA; Siemens Healthcare, Malvern, PA, USA; Siemens Healthcare, Malvern, PA, USA; Siemens Healthcare, Malvern, PA, USA; Department of Computer Science, University of North Carolina, Charlotte, NC, USA; Department of Computer Science, Rutgers University, Piscataway, NJ, USA; Siemens Healthcare, Malvern, PA, USA","IEEE Transactions on Medical Imaging","","2016","35","5","1332","1343","In general image recognition problems, discriminative information often lies in local image patches. For example, most human identity information exists in the image patches containing human faces. The same situation stays in medical images as well. “Bodypart identity” of a transversal slice-which bodypart the slice comes from-is often indicated by local image information, e.g., a cardiac slice and an aorta arch slice are only differentiated by the mediastinum region. In this work, we design a multi-stage deep learning framework for image classification and apply it on bodypart recognition. Specifically, the proposed framework aims at: 1) discover the local regions that are discriminative and non-informative to the image classification problem, and 2) learn a image-level classifier based on these local regions. We achieve these two tasks by the two stages of learning scheme, respectively. In the pre-train stage, a convolutional neural network (CNN) is learned in a multi-instance learning fashion to extract the most discriminative and and non-informative local patches from the training slices. In the boosting stage, the pre-learned CNN is further boosted by these local patches for image classification. The CNN learned by exploiting the discriminative local appearances becomes more accurate than those learned from global image context. The key hallmark of our method is that it automatically discovers the discriminative and non-informative local patches through multi-instance deep learning. Thus, no manual annotation is required. Our method is validated on a synthetic dataset and a large scale CT dataset. It achieves better performances than state-of-the-art approaches, including the standard deep CNN.","","","10.1109/TMI.2016.2524985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398101","CNN;discriminative local information discovery;multi-instance;multi-stage","Image recognition;Algorithm design and analysis;Machine learning;Three-dimensional displays;Image analysis;DICOM","cardiology;computerised tomography;face recognition;image classification;learning (artificial intelligence);medical image processing","multiinstance deep learning;discriminative local anatomies;body-part recognition;image recognition problems;discriminative information;local image patches;human identity information;human faces;transversal slice;local image information;cardiac slice;aorta arch slice;mediastinum region;multistage deep learning framework;image classification problem;image-level classifier;pretrain stage;convolutional neural network;multiinstance learning fashion;prelearned CNN;discriminative local appearances;global image context;synthetic dataset;large scale CT dataset","Adolescent;Adult;Aged;Aged, 80 and over;Child;Child, Preschool;Extremities;Face;Humans;Image Interpretation, Computer-Assisted;Infant;Middle Aged;Neural Networks (Computer);Tomography, X-Ray Computed;Torso;Young Adult","71","51","","","","","IEEE","IEEE Journals"
"Salient Band Selection for Hyperspectral Image Classification via Manifold Ranking","Q. Wang; J. Lin; Y. Yuan","School of Computer Science and the Center for OPTical IMagery Analysis and Learning, Northwestern Polytechnical University, Xi’an, China; Center for OPTical Imagery Analysis and Learning, State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China; Center for OPTical Imagery Analysis and Learning, State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","","2016","27","6","1279","1289","Saliency detection has been a hot topic in recent years, and many efforts have been devoted in this area. Unfortunately, the results of saliency detection can hardly be utilized in general applications. The primary reason, we think, is unspecific definition of salient objects, which makes that the previously published methods cannot extend to practical applications. To solve this problem, we claim that saliency should be defined in a context and the salient band selection in hyperspectral image (HSI) is introduced as an example. Unfortunately, the traditional salient band selection methods suffer from the problem of inappropriate measurement of band difference. To tackle this problem, we propose to eliminate the drawbacks of traditional salient band selection methods by manifold ranking. It puts the band vectors in the more accurate manifold space and treats the saliency problem from a novel ranking perspective, which is considered to be the main contributions of this paper. To justify the effectiveness of the proposed method, experiments are conducted on three HSIs, and our method is compared with the six existing competitors. Results show that the proposed method is very effective and can achieve the best performance among the competitors.","","","10.1109/TNNLS.2015.2477537","National Basic Research Program of China (Youth 973 Program); State Key Program of National Natural Science of China; National Natural Science Foundation of China; Natural Science Foundation Research Project of Shaanxi Province; Fundamental Research Funds for Central Universities; Open Research Fund through the Key Laboratory of Spectral Imaging Technology, Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436783","Band selection;deep learning;hyperspectral image (HSI) classification;manifold ranking (MR);saliency;stacked autoencoders (SAEs).;Band selection;deep learning;hyperspectral image (HSI) classification;manifold ranking (MR);saliency;stacked autoencoders (SAEs)","Hyperspectral imaging;Manifolds;Feature extraction;Adaptation models;Machine learning;Transforms;Computer vision","feature extraction;hyperspectral imaging;image classification;object detection","hyperspectral image classification;manifold ranking;saliency detection;HSI;salient band selection methods;band difference measurement;band vectors","","197","53","","","","","IEEE","IEEE Journals"
"Transferred Deep Convolutional Neural Network Features for Extensive Facial Landmark Localization","S. Zhang; H. Yang; Z. Yin","State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Signal Processing Letters","","2016","23","4","478","482","Features are crucial for extensive facial landmark localization (EFLL), while deep convolutional neural network (DCNN) features lead to breakthroughs in diverse visual recognition tasks. However, there is little study of DCNN features for EFLL, mainly because less labeled data with extensive facial landmarks are available. In this letter, we employ transfer learning to overcome this limitation, and utilize DCNN for EFLL. We concentrate the power of DCNN on feature learning within a cascaded-regression framework (CRF). We present three transfer methods, which show the capacity of DCNN as a generic feature extractor, and the benefit of fine-tuning. The proposed specific fine-tuning method for cascaded regression, named cascade transfer, achieves competitive accuracy with state-of-the-art methods on the 300-W challenge dataset.","","","10.1109/LSP.2016.2533721","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7416197","Convolutional neural network;deep learning;face alignment;facial landmark localization;transfer learning;Convolutional neural network;deep learning;face alignment;facial landmark localization;transfer learning","Feature extraction;Face;Shape;Training;Convolution;Neural networks;Linear regression","face recognition;feature extraction;learning (artificial intelligence);neural nets;regression analysis","transferred deep convolutional neural network feature;extensive facial landmark localization;EFLL;transferred DCNN feature learning;diverse visual recognition task;cascaded-regression framework;CRF;fine-tuning method;named cascade transfer;state-of-the-art method;power 300 W","","9","45","","","","","IEEE","IEEE Journals"
"Understanding Deep Representations Learned in Modeling Users Likes","S. C. Guntuku; J. T. Zhou; S. Roy; W. Lin; I. W. Tsang","School of Computer Engineering, Nanyang Technological University, Singapore; Agency for Science, Technology and Research, Institute of High Performance Computing, Singapore; SAP Innovation Center Network, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore; University of Technology Sydney, Sydney, NSW, Australia","IEEE Transactions on Image Processing","","2016","25","8","3762","3774","Automatically understanding and discriminating different users' liking for an image is a challenging problem. This is because the relationship between image features (even semantic ones extracted by existing tools, viz., faces, objects, and so on) and users' likes is non-linear, influenced by several subtle factors. This paper presents a deep bi-modal knowledge representation of images based on their visual content and associated tags (text). A mapping step between the different levels of visual and textual representations allows for the transfer of semantic knowledge between the two modalities. Feature selection is applied before learning deep representation to identify the important features for a user to like an image. The proposed representation is shown to be effective in discriminating users based on images they like and also in recommending images that a given user likes, outperforming the state-of-the-art feature representations by ~15 %-20%. Beyond this test-set performance, an attempt is made to qualitatively understand the representations learned by the deep architecture used to model user likes.","","","10.1109/TIP.2016.2576278","Singapore Ministry of Education through the Tier 1 Project; Economic Development Board; National Research Foundation of Singapore; Australian Research Council Future Fellowship; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7484658","Deep Representations;User Likes;Image Recommendation;Semantic Structures;Deep representations;user likes;image recommendation;semantic structures","Visualization;Semantics;Context modeling;Feature extraction;Context;Clutter","feature extraction;feature selection;image representation;image retrieval;knowledge representation;learning (artificial intelligence)","feature representations;deep representation learning;feature selection;semantic knowledge transfer;visual representations;textual representations;tags;visual content;deep bimodal image knowledge representation;image features;automatic user liking discriminating;automatic user liking understanding;user liking modeling","Algorithms;Artificial Intelligence;Face;Humans;Image Interpretation, Computer-Assisted;Image Processing, Computer-Assisted;Information Storage and Retrieval;Pattern Recognition, Automated;Semantics","6","45","","","","","IEEE","IEEE Journals"
"Improving Traffic Flow Prediction With Weather Information in Connected Cars: A Deep Learning Approach","A. Koesdwiady; R. Soua; F. Karray","Centre for Pattern Analysis and Machine Intelligence, Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Waterloo, University of Luxembourg, Waterloo, ON, CanadaLuxembourg; Centre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, ON, Canada; Centre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Vehicular Technology","","2016","65","12","9508","9517","Transportation systems might be heavily affected by factors such as accidents and weather. Specifically, inclement weather conditions may have a drastic impact on travel time and traffic flow. This study has two objectives: first, to investigate a correlation between weather parameters and traffic flow and, second, to improve traffic flow prediction by proposing a novel holistic architecture. It incorporates deep belief networks for traffic and weather prediction and decision-level data fusion scheme to enhance prediction accuracy using weather conditions. The experimental results, using traffic and weather data originated from the San Francisco Bay Area of California, corroborate the effectiveness of the proposed approach compared with the state of the art.","","","10.1109/TVT.2016.2585575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7501574","Data fusion;deep learning;intelligent transportation systems (ITS);traffic prediction;weather information","Meteorology;Roads;Data integration;Machine learning;Automobiles;Correlation","belief networks;intelligent transportation systems;learning (artificial intelligence);road traffic;sensor fusion","traffic flow prediction;weather information;connected car;deep learning approach;transportation system;holistic architecture;deep belief network;weather prediction;decision-level data fusion;San Francisco Bay Area;California;intelligent transportation system;ITS","","76","42","","","","","IEEE","IEEE Journals"
"Deformable MR Prostate Segmentation via Deep Feature Learning and Sparse Patch Matching","Y. Guo; Y. Gao; D. Shen","Department of Radiology and BRIC, University of North Carolina, Chapel Hill, NC, USA; Department of Radiology and BRIC, University of North Carolina, Chapel Hill, NC, USA; Department of Radiology and BRIC, University of North Carolina, Chapel Hill, NC, USA","IEEE Transactions on Medical Imaging","","2016","35","4","1077","1089","Automatic and reliable segmentation of the prostate is an important but difficult task for various clinical applications such as prostate cancer radiotherapy. The main challenges for accurate MR prostate localization lie in two aspects: (1) inhomogeneous and inconsistent appearance around prostate boundary, and (2) the large shape variation across different patients. To tackle these two problems, we propose a new deformable MR prostate segmentation method by unifying deep feature learning with the sparse patch matching. First, instead of directly using handcrafted features, we propose to learn the latent feature representation from prostate MR images by the stacked sparse auto-encoder (SSAE). Since the deep learning algorithm learns the feature hierarchy from the data, the learned features are often more concise and effective than the handcrafted features in describing the underlying data. To improve the discriminability of learned features, we further refine the feature representation in a supervised fashion. Second, based on the learned features, a sparse patch matching method is proposed to infer a prostate likelihood map by transferring the prostate labels from multiple atlases to the new prostate MR image. Finally, a deformable segmentation is used to integrate a sparse shape model with the prostate likelihood map for achieving the final segmentation. The proposed method has been extensively evaluated on the dataset that contains 66 T2-wighted prostate MR images. Experimental results show that the deep-learned features are more effective than the handcrafted features in guiding MR prostate segmentation. Moreover, our method shows superior performance than other state-of-the-art segmentation methods.","","","10.1109/TMI.2015.2508280","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353170","Deformable model;MR prostate segmentation;sparse patch matching;stacked sparse auto-encoder (SSAE)","Image segmentation;Shape;Machine learning;Deformable models;Feature extraction;Cancer;Biomedical imaging","biological organs;biomedical MRI;cancer;feature extraction;image matching;image segmentation;learning (artificial intelligence);medical image processing;radiation therapy","deformable MR prostate segmentation;deep feature learning;sparse patch matching;clinical applications;prostate cancer radiotherapy;accurate MR prostate localization;inhomogeneous prostate boundary appearance;inconsistent prostate boundary appearance;shape variation;handcrafted features;latent feature representation;stacked sparse autoencoder;feature hierarchy;prostate likelihood map;dataset;T2-wighted prostate MR images;state-of-the-art segmentation methods","Algorithms;Humans;Machine Learning;Magnetic Resonance Imaging;Male;Prostate","73","56","","","","","IEEE","IEEE Journals"
"Privacy Preserving Deep Computation Model on Cloud for Big Data Feature Learning","Q. Zhang; L. T. Yang; Z. Chen","School of Software Technology, Dalian University of Technology, Dalian, Liaoning, China; Department of Computer Science, St. Francis Xavier University, Antigonish, NS, Canada; School of Software Technology, Dalian University of Technology, Dalian, Liaoning, China","IEEE Transactions on Computers","","2016","65","5","1351","1362","To improve the efficiency of big data feature learning, the paper proposes a privacy preserving deep computation model by offloading the expensive operations to the cloud. Privacy concerns become evident because there are a large number of private data by various applications in the smart city, such as sensitive data of governments or proprietary information of enterprises. To protect the private data, the proposed model uses the BGV encryption scheme to encrypt the private data and employs cloud servers to perform the high-order back-propagation algorithm on the encrypted data efficiently for deep computation model training. Furthermore, the proposed scheme approximates the Sigmoid function as a polynomial function to support the secure computation of the activation function with the BGV encryption. In our scheme, only the encryption operations and the decryption operations are performed by the client while all the computation tasks are performed on the cloud. Experimental results show that our scheme is improved by approximately 2.5 times in the training efficiency compared to the conventional deep computation model without disclosing the private data using the cloud computing including ten nodes. More importantly, our scheme is highly scalable by employing more cloud servers, which is particularly suitable for big data.","","","10.1109/TC.2015.2470255","Liaoning Provincial Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7210184","Smart city;Big data;Deep computation model;Cloud computing;BGV encryption;BGN encryption;High-order back-propagation;Smart city;big data;deep computation model;cloud computing;BGV encryption;BGN encryption;high-order back-propagation","Encryption;Computational modeling;Data models;Big data;Training;Data privacy","backpropagation;Big Data;cloud computing;cryptography;data privacy;learning (artificial intelligence);polynomial approximation;smart cities","privacy preserving deep computation model;Big Data feature learning;smart city;Sigmoid function;polynomial function;private data protection;BGV encryption scheme;private data encryption;cloud servers;high-order back-propagation algorithm;deep computation model training;encryption operations;decryption operations;cloud computing","","77","56","","","","","IEEE","IEEE Journals"
"Joint modulation format/bit-rate classification and signal-to-noise ratio estimation in multipath fading channels using deep machine learning","F. N. Khan; C. Lu; A. P. T. Lau","The Hong Kong Polytechnic University, Hong Kong; The Hong Kong Polytechnic University, Hong Kong; The Hong Kong Polytechnic University, Hong Kong","Electronics Letters","","2016","52","14","1272","1274","A novel algorithm for simultaneous modulation format/bit-rate classification and non-data-aided (NDA) signal-to-noise ratio (SNR) estimation in multipath fading channels by applying deep machine learning-based pattern recognition on signals’ asynchronous delay-tap plots (ADTPs) is proposed. The results for three widely-used modulation formats at two different bit-rates demonstrate classification accuracy of 99.8%. In addition, NDA SNR estimation over a wide range of 0−30 dB is shown with mean error of 1 dB. The proposed method requires low-speed, asynchronous sampling of signal and is thus ideal for low-cost multiparameter estimation under real-world channel conditions.","","","10.1049/el.2016.0876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7500204","","","fading channels;multipath channels;modulation;signal classification;learning (artificial intelligence);signal sampling;parameter estimation;mean square error methods","joint modulation format-bit rate classification;multipath fading channels;deep machine learning;simultaneous modulation format-bit rate classification algorithm;nondata-aided signal-to-noise ratio estimation;pattern recognition;asynchronous delay-tap plots;classification accuracy;NDA SNR estimation;low-speed asynchronous signal sampling;low-cost multiparameter estimation","","9","5","","","","","IET","IET Journals"
"Learning Stacked Image Descriptor for Face Recognition","Z. Lei; D. Yi; S. Z. Li","National Laboratory of Pattern Recognition, Center for Biometrics and Security Research, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Biometrics and Security Research, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Biometrics and Security Research, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2016","26","9","1685","1696","Learning-based face descriptors have constantly improved the face recognition performance. Compared with the hand-crafted features, learning-based features are considered to be able to exploit information with better discriminative ability for specific tasks. Motivated by the recent success of deep learning, in this paper, we extend the original shallow face descriptors to deep discriminant face features by introducing a stacked image descriptor (SID). With deep structure, more complex facial information can be extracted and the discriminant and compactness of feature representation can be improved. The SID is learned in a forward optimization way, which is computational efficient compared with deep learning. Extensive experiments on various face databases are conducted to show that SID is able to achieve high face recognition performance with compact face representation, compared with other state-of-the-art descriptors.","","","10.1109/TCSVT.2015.2473415","Chinese National Natural Science Foundation; National Science and Technology Support Program; Chinese Academy of Sciences; AuthenMetric Research and Development Funds; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7225130","Deep discriminant face representation;face recognition;learning-based descriptor;stacked image descriptor (SID)","Face;Face recognition;Feature extraction;Eigenvalues and eigenfunctions;Principal component analysis;Convolution;Tensile stress","face recognition;feature extraction;image representation;learning (artificial intelligence)","learning stacked image descriptor;face recognition performance;learning-based face descriptors;hand-crafted features;learning-based features;deep learning;shallow-face descriptors;deep-discriminant face features;facial information extraction;feature representation;SID;compact face representation","","24","63","","","","","IEEE","IEEE Journals"
"Learning to Hash for Indexing Big Data—A Survey","J. Wang; W. Liu; S. Kumar; S. Chang","Sch. of Comput. Sci. & Software Eng., East China Normal Univ., Shanghai, China; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; Google Res., New York, NY, USA; Dept. of Electr. Eng. & Comput. Sci., Columbia Univ., New York, NY, USA","Proceedings of the IEEE","","2016","104","1","34","57","The explosive growth in Big Data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, approximate nearest neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., locality-sensitive hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy, new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning-to-hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly, the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights, pros, and cons of the emerging techniques. We provide a comprehensive survey of the learning-to-hash framework and representative techniques of various types, including unsupervised, semisupervised, and supervised. In addition, we also summarize recent hashing approaches utilizing the deep learning models. Finally, we discuss the future direction and trends of research in this area.","","","10.1109/JPROC.2015.2487976","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7360966","Approximate nearest neighbor (ANN) search;deep learning;learning to hash;semisupervised learning;supervised learning;unsupervised learning","Big data;Artificial neural networks;Binary codes;Indexing;Semantics;Tagging ;Twitter","Big Data;computational complexity;file organisation;indexing;unsupervised learning","deep learning model;hash function;hash code;learning-to-hash method;data-independent hash function;LSH;locality-sensitive hashing;hashing technique;ANN search;approximate nearest neighbor search;computational complexity;pattern matching;search method;Big data;indexing","","136","149","","","","","IEEE","IEEE Journals"
"q-Space Deep Learning: Twelve-Fold Shorter and Model-Free Diffusion MRI Scans","V. Golkov; A. Dosovitskiy; J. I. Sperl; M. I. Menzel; M. Czisch; P. Sämann; T. Brox; D. Cremers","The Department of Computer Science, Technical University of Munich, Garching, Germany; The Department of Computer Science, University of Freiburg, Freiburg, Germany; The GE Global Research, Munich, Germany; The GE Global Research, Munich, Germany; The Max Planck Institute of Psychiatry, Munich, Germany; The Max Planck Institute of Psychiatry, Munich, Germany; The Department of Computer Science, University of Freiburg, Freiburg, Germany; The Department of Computer Science, Technical University of Munich, Garching, Germany","IEEE Transactions on Medical Imaging","","2016","35","5","1344","1351","Numerous scientific fields rely on elaborate but partly suboptimal data processing pipelines. An example is diffusion magnetic resonance imaging (diffusion MRI), a non-invasive microstructure assessment method with a prominent application in neuroimaging. Advanced diffusion models providing accurate microstructural characterization so far have required long acquisition times and thus have been inapplicable for children and adults who are uncooperative, uncomfortable, or unwell. We show that the long scan time requirements are mainly due to disadvantages of classical data processing. We demonstrate how deep learning, a group of algorithms based on recent advances in the field of artificial neural networks, can be applied to reduce diffusion MRI data processing to a single optimized step. This modification allows obtaining scalar measures from advanced models at twelve-fold reduced scan time and detecting abnormalities without using diffusion models. We set a new state of the art by estimating diffusion kurtosis measures from only 12 data points and neurite orientation dispersion and density measures from only 8 data points. This allows unprecedentedly fast and robust protocols facilitating clinical routine and demonstrates how classical data processing can be streamlined by means of deep learning.","","","10.1109/TMI.2016.2551324","GE Global Research; Deutsche Telekom Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448418","Artificial neural networks;diffusion kurtosis imaging (DKI);diffusion magnetic resonance imaging (diffusion MRI);neurite orientation dispersion and density imaging (NODDI)","Fitting;Machine learning;Data processing;Pipelines;Training;Diffusion tensor imaging","biodiffusion;biomedical MRI;data acquisition;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;optimisation","q-space deep learning;twelve-fold shorter diffusion MRI scans;model-free diffusion MRI scans;suboptimal data processing pipelines;diffusion magnetic resonance imaging;noninvasive microstructure assessment method;neuroimaging;accurate microstructural characterization;acquisition times;children;adults;classical data processing;artificial neural networks;diffusion MRI data processing;single optimized step;scalar measures;twelve-fold reduced scan time;diffusion kurtosis;data points;neurite orientation dispersion;clinical routine","Brain;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Magnetic Resonance Imaging;Neural Networks (Computer);Time Factors","60","52","","","","","IEEE","IEEE Journals"
"Biologically Inspired Model for Visual Cognition Achieving Unsupervised Episodic and Semantic Feature Learning","H. Qiao; Y. Li; F. Li; X. Xi; W. Wu","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Academy of Mathematics and Systems Science, Chinese Academy of Science, Institute of Applied Mathematics, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Cybernetics","","2016","46","10","2335","2347","Recently, many biologically inspired visual computational models have been proposed. The design of these models follows the related biological mechanisms and structures, and these models provide new solutions for visual recognition tasks. In this paper, based on the recent biological evidence, we propose a framework to mimic the active and dynamic learning and recognition process of the primate visual cortex. From principle point of view, the main contributions are that the framework can achieve unsupervised learning of episodic features (including key components and their spatial relations) and semantic features (semantic descriptions of the key components), which support higher level cognition of an object. From performance point of view, the advantages of the framework are as follows: 1) learning episodic features without supervision - for a class of objects without a prior knowledge, the key components, their spatial relations and cover regions can be learned automatically through a deep neural network (DNN); 2) learning semantic features based on episodic features - within the cover regions of the key components, the semantic geometrical values of these components can be computed based on contour detection; 3) forming the general knowledge of a class of objects - the general knowledge of a class of objects can be formed, mainly including the key components, their spatial relations and average semantic values, which is a concise description of the class; and 4) achieving higher level cognition and dynamic updating - for a test image, the model can achieve classification and subclass semantic descriptions. And the test samples with high confidence are selected to dynamically update the whole model. Experiments are conducted on face images, and a good performance is achieved in each layer of the DNN and the semantic description learning process. Furthermore, the model can be generalized to recognition tasks of other objects with learning ability.","","","10.1109/TCYB.2015.2476706","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272067","Biologically inspired;hierarchical model;key components learning;semantic description","Semantics;Face;Biological system modeling;Visualization;Cognition;Computational modeling","cognition;computer vision;face recognition;image classification;neural nets;unsupervised learning","visual cognition;semantic feature learning;biologically inspired visual computational models;visual recognition tasks;biological evidence;dynamic learning;recognition process;visual cortex;unsupervised learning;learning episodic features;deep neural network;DNN;semantic geometrical values;contour detection;higher level cognition;face images;semantic description learning process;learning ability;visual recognition tasks","Algorithms;Biometric Identification;Face;Humans;Image Processing, Computer-Assisted;Machine Learning;Models, Biological;Semantics;Visual Perception","11","44","","","","","IEEE","IEEE Journals"
"Where does AlphaGo go: from church-turing thesis to AlphaGo thesis and beyond","F. Wang; J. J. Zhang; X. Zheng; X. Wang; Y. Yuan; X. Dai; J. Zhang; L. Yang","Chinese Academy of Sciences (SKL-MCCS, CASIA), Beijing 100190, China; University of Denver, Denver, CO 80210, USA; University of Minnesota, Minneapolis, MN 55414, USA; Qingdao Academy of Intelligent Industries (QAII), Qingdao, Shandong, China; Qingdao Academy of Intelligent Industries (QAII), Qingdao, Shandong, China; University of Denver, Denver, CO 80210, USA; Qingdao Academy of Intelligent Industries (QAII), Qingdao, Shandong, China; Colorado State University, Fort Collins, CO 80523, USA","IEEE/CAA Journal of Automatica Sinica","","2016","3","2","113","120","An investigation on the impact and significance of the AlphaGo vs. Lee Sedol Go match is conducted, and concludes with a conjecture of the AlphaGo Thesis and its extension in accordance with the Church-Turing Thesis in the history of computing. It is postulated that the architecture and method utilized by the AlphaGo program provide an engineering solution for tackling issues in complexity and intelligence. Specifically, the AlphaGo Thesis implies that any effective procedure for hard decision problems such as NP-hard can be implemented with AlphaGo-like approach. Deep rule-based networks are proposed in attempt to establish an understandable structure for deep neural networks in deep learning. The success of AlphaGo and corresponding thesis ensure the technical soundness of the parallel intelligence approach for intelligent control and management of complex systems and knowledge automation.","","","10.1109/JAS.2016.7471613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471613","ACP;AlphaGo;AlphaGo Thesis;Church-Turing Thesis;deep learning;deep neural networks;deep rule-based networks;knowledge automation;parallel intelligence;parallel ontrol;parallel management","Complexity theory;Games;Neural networks;Computers;Machine learning;Decision making","computational complexity;learning (artificial intelligence);neural nets;Turing machines","Church-Turing thesis;AlphaGo thesis;Lee Sedol Go match;NP-hard problem;hard decision problems;deep rule-based networks;deep neural networks;deep learning;parallel intelligence approach;intelligent control;complex system management;knowledge automation","","18","","","","","","IEEE","IEEE Journals"
"Distributed Compressive Sensing: A Deep Learning Approach","H. Palangi; R. Ward; L. Deng","Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Microsoft Research, Redmond, WA, USA","IEEE Transactions on Signal Processing","","2016","64","17","4504","4518","Several recent studies on the compressed sensing problem with Multiple Measurement Vectors (MMVs) under the condition that the vectors in the different channels are jointly sparse have been recently carried. In this paper, this condition is relaxed. Instead, these sparse vectors are assumed to depend on each other but this dependency is assumed unknown. We capture this dependency by computing the conditional probability of each entry in each vector being non-zero, given the “residuals” of all previous vectors. To estimate these probabilities, we propose the use of the long short-term memory (LSTM), a data-driven model for sequence modeling that is deep in time. To learn the model parameters, we minimize a cross-entropy cost function. To reconstruct the sparse vectors at the decoder, we propose a greedy solver that uses the above model to estimate the conditional probabilities. By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms the general MMV solver (the Simultaneous Orthogonal Matching Pursuit (SOMP)) and a number of the model-based Bayesian methods. The proposed method does not add any complexity to the general compressive sensing encoder. The trained model is used at the decoder only. As the proposed method is a data-driven method, it is only applicable when training data is available. In many applications however, training data is indeed available, e.g., in recorded images for which our method is successfully applied as to be reported in this paper.","","","10.1109/TSP.2016.2557301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457684","Compressive sensing;deep learning;long short-term memory","Compressed sensing;Bayes methods;Training data;Reconstruction algorithms;Sensors;Data models;Decoding","compressed sensing;decoding;greedy algorithms;image coding;learning (artificial intelligence);minimisation;probability","decoder;general compressive sensing encoder;model-based Bayesian methods;SOMP;simultaneous orthogonal matching pursuit;MMV solver;conditional probability estimation;greedy solver;sparse vector reconstruction;cross-entropy cost function minimization;model parameter learning;sequence modeling;data-driven model;LSTM;long short-term memory;multiple measurement vectors;deep learning approach;distributed compressive sensing","","24","53","","","","","IEEE","IEEE Journals"
"Spectral–Spatial Classification of Hyperspectral Image Based on Deep Auto-Encoder","X. Ma; H. Wang; J. Geng","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2016","9","9","4073","4085","Deep learning, which represents data by a hierarchical network, has proven to be efficient in computer vision. To investigate the effect of deep features in hyperspectral image (HSI) classification, this paper focuses on how to extract and utilize deep features in HSI classification framework. First, in order to extract spectral-spatial information, an improved deep network, spatial updated deep auto-encoder (SDAE), is proposed. SDAE, which is an improved deep auto-encoder (DAE), considers sample similarity by adding a regularization term in the energy function, and updates features by integrating contextual information. Second, in order to deal with the small training set using deep features, a collaborative representation-based classification is applied. Moreover, in order to suppress salt-and-pepper noise and smooth the result, we compute the residual of collaborative representation of all samples as a residual matrix, which can be effectively used in a graph-cut-based spatial regularization. The proposed method inherits the advantages of deep learning and has solutions to add spatial information of HSI in the learning network. Using collaborative representation-based classification with deep features makes the proposed classifier extremely robust under a small training set. Extensive experiments demonstrate that the proposed method provides encouraging results compared with some related techniques.","","","10.1109/JSTARS.2016.2517204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405235","Deep auto-encoders (DAE);feature learning;hyperspectral image (HSI);supervised classification","Feature extraction;Training;Machine learning;Collaboration;Data mining;Hyperspectral imaging;Kernel","hyperspectral imaging;image classification;remote sensing","residual matrix;representation-based classification;spatial updated deep autoencoder;deep network;computer vision;deep learning;hyperspectral image spectral-spatial classification","","79","56","","","","","IEEE","IEEE Journals"
"Learning Deep Features for DNA Methylation Data Analysis","Z. Si; H. Yu; Z. Ma","Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Access","","2016","4","","2732","2737","Many studies demonstrated that the DNA methylation, which occurs in the context of a CpG, has strong correlation with diseases, including cancer. There is a strong interest in analyzing the DNA methylation data to find how to distinguish different subtypes of the tumor. However, the conventional statistical methods are not suitable for analyzing the highly dimensional DNA methylation data with bounded support. In order to explicitly capture the properties of the data, we design a deep neural network, which composes of several stacked binary restricted Boltzmann machines, to learn the low-dimensional deep features of the DNA methylation data. Experimental results show that these features perform best in breast cancer DNA methylation data cluster analysis, compared with some state-of-the-art methods.","","","10.1109/ACCESS.2016.2576598","National Natural Science Foundation of China; Beijing Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7484730","DNA Methylation;beat-value;deep neural network;restricted Boltzmann machine;DNA methylation;beat-value;deep neural network;restricted Boltzmann machine","DNA;Cancer;Neural networks;Feature extraction;Bioinformatics;Principal component analysis;Data analysis","Boltzmann machines;cancer;data analysis;diseases;DNA;learning (artificial intelligence);medical computing;pattern clustering;statistical analysis;tumours","breast cancer DNA methylation data cluster analysis;low-dimensional deep features;Boltzmann machines;deep neural network;statistical methods;tumor;diseases;CpG","","8","34","","","","","IEEE","IEEE Journals"
"Deep Haar scattering networks","X. Cheng; X. Chen; S. Mallat","NA; NA; NA","Information and Inference: A Journal of the IMA","","2016","5","2","105","133","An orthogonal Haar scattering transform is a deep network computed with a hierarchy of additions, subtractions and absolute values over pairs of coefficients. Unsupervised learning optimizes Haar pairs to obtain sparse representations of training data with an algorithm of polynomial complexity. For signals defined on a graph, a Haar scattering is computed by cascading orthogonal Haar wavelet transforms on the graph, with Haar wavelets having connected supports. It defines a representation which is invariant to local displacements of signal values on the graph. When the graph connectivity is unknown, unsupervised Haar learning can provide a consistent estimation of connected wavelet supports. Classification results are given on image data bases, defined on regular grids or graphs, with a connectivity which may be known or unknown.","","","10.1093/imaiai/iaw007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8132287","deep learning;neural network;scattering transform;Haar wavelet;classification;images;graphs","","","","","1","","","","","","OUP","OUP Journals"
"Modulation Format Identification in Coherent Receivers Using Deep Machine Learning","F. N. Khan; K. Zhong; W. H. Al-Arashi; C. Yu; C. Lu; A. P. T. Lau","Photonics Research Centre, The Hong Kong Polytechnic University, Hong Kong; Photonics Research Centre, The Hong Kong Polytechnic University, Hong Kong; Department of Electronic Engineering, University of Science and Technology, Sana’a, Yemen; Photonics Research Centre, The Hong Kong Polytechnic University, Hong Kong; Photonics Research Centre, The Hong Kong Polytechnic University, Hong Kong; Photonics Research Centre, The Hong Kong Polytechnic University, Hong Kong","IEEE Photonics Technology Letters","","2016","28","17","1886","1889","We propose a novel technique for modulation format identification (MFI) in digital coherent receivers by applying deep neural network (DNN) based pattern recognition on signals' amplitude histograms obtained after constant modulus algorithm (CMA) equalization. Experimental results for three commonly-used modulation formats demonstrate MFI with an accuracy of 100% over a wide optical signal-to-noise ratio (OSNR) range. The effects of fiber nonlinearity on the performance of MFI technique are also investigated. The proposed technique is non-data-aided (NDA) and avoids any additional hardware on top of standard digital coherent receiver. Therefore, it is ideal for simple and cost-effective MFI in future heterogeneous optical networks.","","","10.1109/LPT.2016.2574800","National Natural Science Foundation China; Hong Kong Government General Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482803","Modulation format identification;coherent detection;deep machine learning","Modulation;Histograms;Receivers;Feature extraction;Signal to noise ratio;Optical noise;Optical attenuators","learning (artificial intelligence);neural nets;optical fibre networks;optical modulation;optical receivers;pattern recognition;physics computing","modulation format identification;deep machine learning;deep neural network based pattern recognition;DNN;signal amplitude histograms;constant modulus algorithm equalization;CMA;optical signal-to-noise ratio;OSNR;fiber nonlinearity;MFI technique;nondata-aided technique;NDA;standard digital coherent receiver;heterogeneous optical networks","","54","12","","","","","IEEE","IEEE Journals"
"Target Classification Using the Deep Convolutional Networks for SAR Images","S. Chen; H. Wang; F. Xu; Y. Jin","Key Laboratory of Information Science of Electromagnetic Waves (Ministry of Education), Fudan University, Shanghai, China; Key Laboratory of Information Science of Electromagnetic Waves (Ministry of Education), Fudan University, Shanghai, China; Key Laboratory of Information Science of Electromagnetic Waves (Ministry of Education), Fudan University, Shanghai, China; Key Laboratory of Information Science of Electromagnetic Waves (Ministry of Education), Fudan University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","","2016","54","8","4806","4817","The algorithm of synthetic aperture radar automatic target recognition (SAR-ATR) is generally composed of the extraction of a set of features that transform the raw input into a representation, followed by a trainable classifier. The feature extractor is often hand designed with domain knowledge and can significantly impact the classification accuracy. By automatically learning hierarchies of features from massive training data, deep convolutional networks (ConvNets) recently have obtained state-of-the-art results in many computer vision and speech recognition tasks. However, when ConvNets was directly applied to SAR-ATR, it yielded severe overfitting due to limited training images. To reduce the number of free parameters, we present a new all-convolutional networks (A-ConvNets), which only consists of sparsely connected layers, without fully connected layers being used. Experimental results on the Moving and Stationary Target Acquisition and Recognition (MSTAR) benchmark data set illustrate that A-ConvNets can achieve an average accuracy of 99% on classification of ten-class targets and is significantly superior to the traditional ConvNets on the classification of target configuration and version variants.","","","10.1109/TGRS.2016.2551720","NSFC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7460942","Automatic target recognition (ATR);deep convolutional networks (ConvNets);deep learning;synthetic aperture radar (SAR);Automatic target recognition (ATR);deep convolutional networks (ConvNets);deep learning;synthetic aperture radar (SAR)","Convolution;Synthetic aperture radar;Feature extraction;Computer architecture;Training;Target recognition;Training data","geophysical image processing;image classification;radar imaging;remote sensing by radar;synthetic aperture radar","target classification;deep convolutional networks;SAR images;synthetic aperture radar automatic target recognition;SAR-ATR;raw input;feature automatically learning hierarchies;massive training data;computer vision;speech recognition tasks;sparsely connected layers;Moving and Stationary Target Acquisition and Recognition;MSTAR benchmark data set;ten-class target classification;target configuration classification","","259","36","","","","","IEEE","IEEE Journals"
"Classification of Hyperspectral Remote Sensing Image Using Hierarchical Local-Receptive-Field-Based Extreme Learning Machine","Q. Lv; X. Niu; Y. Dou; J. Xu; Y. Lei","Sch. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Sch. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Sch. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Sch. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Sch. of Comput., Nat. Univ. of Defense Technol., Changsha, China","IEEE Geoscience and Remote Sensing Letters","","2016","13","3","434","438","This letter proposes a novel classification approach for a hyperspectral image (HSI) using a hierarchical local-receptive-field (LRF)-based extreme learning machine (ELM). As a fast and accurate pattern classification algorithm, ELM has been applied in numerous fields, including the HSI classification. The LRF concept originates from research in neuroscience. Considering the local correlations of spectral features, it is promising to improve the performance of HSI classification by introducing the LRFs. Recent research on deep learning has shown that hierarchical architectures with more layers can potentially extract abstract representation and invariant features for better classification performance. Therefore, we further extend the LRF-based ELM method to a hierarchical model for HSI classification. Experimental results on two widely used real hyperspectral data sets confirm the effectiveness of the proposed HSI classification approach.","","","10.1109/LGRS.2016.2517178","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403893","Deep learning;extreme learning machine (ELM);hyperspectral image (HSI) classification;local receptive field (LRF);Deep learning;extreme learning machine (ELM);hyperspectral image (HSI) classification;local receptive field (LRF)","Convolution;Feature extraction;Training;Hyperspectral imaging;Neurons","geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);pattern classification;terrain mapping","hyperspectral remote sensing image classification;hierarchical local-receptive-field-based extreme learning machine;HSI classification;ELM;pattern classification algorithm;LRF-based ELM method;spectral feature;image representation;neuroscience","","13","19","","","","","IEEE","IEEE Journals"
"Microblog Dimensionality Reduction—A Deep Learning Approach","L. Xu; C. Jiang; Y. Ren; H. Chen","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Engineering Science, National Cheng Kung University, Tainan, Taiwan","IEEE Transactions on Knowledge and Data Engineering","","2016","28","7","1779","1789","Exploring potentially useful information from huge amount of textual data produced by microblogging services has attracted much attention in recent years. An important preprocessing step of microblog text mining is to convert natural language texts into proper numerical representations. Due to the short-length characteristics of microblog texts, using term frequency vectors to represent microblog texts will cause “sparse data” problem. Finding proper representations of microblog texts is a challenging issue. In this paper, we apply deep networks to map the high-dimensional representations of microblog texts to low-dimensional representations. To improve the result of dimensionality reduction, we take advantage of the semantic similarity derived from two types of microblogspecific information, namely the retweet relationship and hashtags. Two types of approaches, including modifying training data and modifying the training objective of deep networks, are proposed to make use of microblog-specific information. Experiment results show that the deep models perform better than traditional dimensionality reduction methods such as latent semantic analysis and latent Dirichlet allocation topic model, and the use of microblog-specific information can help to learn better representations.","","","10.1109/TKDE.2016.2540639","NSFC; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7430292","Microblog mining;Dimension reduction;Text representation;Semantic relatedness;Deep autoencoder;Microblog mining;dimension reduction;text representation;semantic relatedness;deep autoencoder","Machine learning;Training;Semantics;Tagging;Twitter;Text mining;Electronic mail","data mining;text analysis;Web sites","microblog dimensionality reduction;deep learning approach;microblog text mining;natural language texts;numerical representations;term frequency vectors;sparse data problem;low-dimensional representations;latent semantic analysis;latent Dirichlet allocation topic model","","8","36","","","","","IEEE","IEEE Journals"
"Action Recognition Based on Efficient Deep Feature Learning in the Spatio-Temporal Domain","F. Husain; B. Dellen; C. Torras","Institut de Robòtica i Informàtica Industrial, CSIC-UPC, Barcelona, Spain; RheinAhrCampus der Hochschule Koblenz, Remagen, Germany; Institut de Robòtica i Informàtica Industrial, CSIC-UPC, Barcelona, Spain","IEEE Robotics and Automation Letters","","2016","1","2","984","991","Hand-crafted feature functions are usually designed based on the domain knowledge of a presumably controlled environment and often fail to generalize, as the statistics of real-world data cannot always be modeled correctly. Data-driven feature learning methods, on the other hand, have emerged as an alternative that often generalize better in uncontrolled environments. We present a simple, yet robust, 2-D convolutional neural network extended to a concatenated 3-D network that learns to extract features from the spatio-temporal domain of raw video data. The resulting network model is used for content-based recognition of videos. Relying on a 2-D convolutional neural network allows us to exploit a pretrained network as a descriptor that yielded the best results on the largest and challenging ILSVRC-2014 dataset. Experimental results on commonly used benchmarking video datasets demonstrate that our results are state-of-the-art in terms of accuracy and computational time without requiring any preprocessing (e.g., optic flow) or a priori knowledge on data capture (e.g., camera motion estimation), which makes it more general and flexible than other approaches. Our implementation is made available.","","","10.1109/LRA.2016.2529686","CSIC; RobInstruct; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406684","Computer vision for automation;recognition;visual learning;Computer vision for automation;recognition;visual learning","Three-dimensional displays;Computational modeling;Feature extraction;Convolution;Robots;Data models;Optical imaging","feature extraction;image capture;image recognition;learning (artificial intelligence);neural nets;video signal processing","action recognition;deep feature learning;spatiotemporal domain;domain knowledge;data-driven feature learning method;2-D convolutional neural network;concatenated 3-D network;raw video data;content-based video recognition;pretrained network;data capture;feature extraction","","12","56","","","","","IEEE","IEEE Journals"
"Deep Relative Attributes","X. Yang; T. Zhang; C. Xu; S. Yan; M. S. Hossain; A. Ghoneim","National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia","IEEE Transactions on Multimedia","","2016","18","9","1832","1842","Relative attribute (RA) learning aims to learn the ranking function describing the relative strength of the attribute. Most of current learning approaches learn a linear ranking function for each attribute by use of the hand-crafted visual features. Different from the existing study, in this paper, we propose a novel deep relative attributes (DRA) algorithm to learn visual features and the effective nonlinear ranking function to describe the RA of image pairs in a unified framework. Here, visual features and the ranking function are learned jointly, and they can benefit each other. The proposed DRA model is comprised of five convolutional neural layers, five fully connected layers, and a relative loss function which contains the contrastive constraint and the similar constraint corresponding to the ordered image pairs and the unordered image pairs, respectively. To train the DRA model effectively, we make use of the transferred knowledge from the large scale visual recognition on ImageNet [1] to the RA learning task. We evaluate the proposed DRA model on three widely used datasets. Extensive experimental results demonstrate that the proposed DRA model consistently and significantly outperforms the state-of-the-art RA learning methods. On the public OSR, PubFig, and Shoes datasets, compared with the previous RA learning results [2], the average ranking accuracies have been significantly improved by about 8%, 9%, and 14%, respectively.","","","10.1109/TMM.2016.2582379","National Natural Science Foundation of China; National Program on Key Basic Research Project; Importation and Development of High-Caliber Talents Project of Beijing Municipal Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7494596","Deep learning;relative attributes (RA)","Visualization;Learning systems;Feature extraction;Neural networks;Measurement;Machine learning;Object recognition","image recognition;learning (artificial intelligence);neural nets","deep relative attribute;hand-crafted visual feature;nonlinear ranking function;convolutional neural layer;relative loss function;contrastive constraint;similar constraint;ordered image pairs;large scale visual recognition;ImageNet","","13","60","","","","","IEEE","IEEE Journals"
"CSI Phase Fingerprinting for Indoor Localization With a Deep Learning Approach","X. Wang; L. Gao; S. Mao","Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA","IEEE Internet of Things Journal","","2016","3","6","1113","1123","With the increasing demand of location-based services, indoor localization based on fingerprinting has become an increasingly important technique due to its high accuracy and low hardware requirement. In this paper, we propose PhaseFi, a fingerprinting system for indoor localization with calibrated channel state information (CSI) phase information. In PhaseFi, the raw phase information is first extracted from the multiple antennas and multiple subcarriers of the IEEE 802.11n network interface card by accessing the modified device driver. Then a linear transformation is applied to extract the calibrated phase information, which we prove to have a bounded variance. For the offline stage, we design a deep network with three hidden layers to train the calibrated phase data, and employ the weights of the deep network to represent fingerprints. A greedy learning algorithm is incorporated to train the weights layer-by-layer to reduce computational complexity, where a subnetwork between two consecutive layers forms a restricted Boltzmann machine. In the online stage, we use a probabilistic method based on the radial basis function for online location estimation. The proposed PhaseFi scheme is implemented and validated with extensive experiments in two representation indoor environments. It is shown to outperform three benchmark schemes based on CSI or received signal strength in both scenarios.","","","10.1109/JIOT.2016.2558659","U.S. National Science Foundation; Wireless Engineering Research and Education Center at Auburn University; IEEE GLOBECOM 2015, San Diego, CA, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7460092","Channel state information (CSI);deep learning;fingerprinting;indoor localization;phase calibration","OFDM;IEEE 802.11 Standard;Databases;Internet of things;Phase measurement;Antennas;Probabilistic logic","antenna arrays;Boltzmann machines;computational complexity;greedy algorithms;indoor navigation;indoor radio;learning (artificial intelligence);probability;radial basis function networks;RSSI;telecommunication computing;wireless LAN","location-based services;indoor localization;PhaseFi;fingerprinting system;calibrated channel state information phase information;raw phase information extraction;multiple antennas;multiple subcarriers;device driver;linear transformation;IEEE 802.11n network interface card;deep network;hidden layers;calibrated phase data;greedy learning algorithm;layer-by-layer training;computational complexity;Boltzmann machine;probabilistic method;radial basis function;online location estimation;representation indoor environments;CSI;received signal strength","","116","32","","","","","IEEE","IEEE Journals"
"Image Super-Resolution Using Deep Convolutional Networks","C. Dong; C. C. Loy; K. He; X. Tang","Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong, China; Microsoft Research Asia, Beijing, China; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","2","295","307","We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.","","","10.1109/TPAMI.2015.2439281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7115171","Super-resolution;deep convolutional neural networks;sparse coding;Super-resolution;deep convolutional neural networks;sparse coding","Image resolution;Neural networks;Image reconstruction;Convolutional codes;Feature extraction;Training","convolution;image resolution;image restoration;learning (artificial intelligence);neural nets","image super-resolution;deep learning method;end-to-end mapping;CNN;low-resolution image;color channel;deep convolutional neural network;reconstruction quality;sparse-coding;image restoration","","1515","50","","","","","IEEE","IEEE Journals"
"Feature-Level Change Detection Using Deep Representation and Feature Change Analysis for Multispectral Imagery","H. Zhang; M. Gong; P. Zhang; L. Su; J. Shi","Department of Integrated Circuit Design and Integrated System, School of Microelectronics, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xidian University, Xi'an, Xi'an, ChinaChina; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, School of Electronics and Information, Xidian University, Northwestern Polytechnical University, Xi'an, Xi'an, ChinaChina","IEEE Geoscience and Remote Sensing Letters","","2016","13","11","1666","1670","Due to the noise interference and redundancy in multispectral images, it is promising to transform the available spectral channels into a suitable feature space for relieving noise and reducing the redundancy. The booming of deep learning provides a flexible tool to learn abstract and invariant features directly from the data in their raw forms. In this letter, we propose an unsupervised change detection technique for multispectral images, in which we combine deep belief networks (DBNs) and feature change analysis to highlight changes. First, a DBN is established to capture the key information for discrimination and suppress the irrelevant variations. Second, we map bitemporal change feature into a 2-D polar domain to characterize the change information. Finally, an unsupervised clustering algorithm is adopted to distinguish the changed and unchanged pixels, and then, the changed types can be identified by classifying the changed pixels into several classes according to the directions of feature changes. The experimental results demonstrate the effectiveness and robustness of the proposed method.","","","10.1109/LGRS.2016.2601930","National Natural Science Foundation of China; National Program for the Support of Top-Notch Young Professionals of China; Specialized Research Fund for the Doctoral Program of Higher Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559716","Change detection;change vector analysis (CVA);cosine angle distance (CAD);deep belief networks (DBNs);multispectral images","Feature extraction;Transforms;Redundancy;Lighting;Interference;Machine learning;Principal component analysis","belief networks;feature extraction;geophysical image processing;image representation;learning (artificial intelligence);pattern clustering;vectors","feature-level change detection;deep representation;feature change analysis;multispectral imagery;deep learning;unsupervised change detection;deep belief network;DBN;bitemporal change feature mapping;unsupervised clustering algorithm;change vector analysis;CVA","","25","13","","","","","IEEE","IEEE Journals"
"Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning","H. Shin; H. R. Roth; M. Gao; L. Lu; Z. Xu; I. Nogues; J. Yao; D. Mollura; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory; Center for Infectious Disease Imaging; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory; Center for Infectious Disease Imaging; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory; Center for Infectious Disease Imaging; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory","IEEE Transactions on Medical Imaging","","2016","35","5","1285","1298","Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.","","","10.1109/TMI.2016.2528162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404017","Biomedical imaging;computer aided diagnosis;image analysis;machine learning;neural networks","Biomedical imaging;Computed tomography;Lungs;Diseases;Solid modeling;Lymph nodes;Computational modeling","computerised tomography;diseases;image classification;image representation;learning (artificial intelligence);lung;medical image processing;neurophysiology;reviews","axial CT slices;CNN model analysis;high performance CAD systems;five-fold cross-validation classification;mediastinal LN detection;state-of-the-art performance;interstitial lung disease classification;thoraco-abdominal lymph node detection;pretrained imagenet;spatial image context;computer-aided detection problems;medical image tasks;natural image dataset;fine-tuning CNN models;supervised fine-tuning;unsupervised CNN pretraining;off-the-shelf pretrained CNN features;medical image classification;medical imaging domain;highly representative hierarchical image features;learning data-driven;image recognition;transfer learning;dataset characteristics;CNN architectures;computer-aided detection;deep convolutional neural networks","Databases, Factual;Diagnosis, Computer-Assisted;Humans;Image Interpretation, Computer-Assisted;Lung Diseases, Interstitial;Lymph Nodes;Neural Networks (Computer);Reproducibility of Results","994","73","","","","","IEEE","IEEE Journals"
"Segmenting Retinal Blood Vessels With Deep Neural Networks","P. Liskowski; K. Krawiec","Institute of Computing Science, Poznan University of Technology, Poland; Institute of Computing Science, Poznan University of Technology, Poland","IEEE Transactions on Medical Imaging","","2016","35","11","2369","2380","The condition of the vascular network of human eye is an important diagnostic factor in ophthalmology. Its segmentation in fundus imaging is a nontrivial task due to variable size of vessels, relatively low contrast, and potential presence of pathologies like microaneurysms and hemorrhages. Many algorithms, both unsupervised and supervised, have been proposed for this purpose in the past. We propose a supervised segmentation technique that uses a deep neural network trained on a large (up to 400 \thinspace000) sample of examples preprocessed with global contrast normalization, zero-phase whitening, and augmented using geometric transformations and gamma corrections. Several variants of the method are considered, including structured prediction, where a network classifies multiple pixels simultaneously. When applied to standard benchmarks of fundus imaging, the DRIVE, STARE, and CHASE databases, the networks significantly outperform the previous algorithms on the area under ROC curve measure (up to > 0.99) and accuracy of classification (up to > 0.97). The method is also resistant to the phenomenon of central vessel reflex, sensitive in detection of fine vessels ( sensitivity > 0.87), and fares well on pathological cases.","","","10.1109/TMI.2016.2546227","Narodowe Centrum Badań i Rozwoju; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440871","Classification;deep learning;feature learning;fundus;neural networks;retina;retinopathy;structured prediction;vessel segmentation","Image segmentation;Biomedical imaging;Databases;Blood vessels;Neural networks;Pathology;Convolution","blood vessels;eye;image classification;image segmentation;medical image processing;neural nets;sensitivity analysis;unsupervised learning","retinal blood vessel segmentation;deep neural networks;vascular network;human eye;diagnostic factor;ophthalmology;fundus imaging;nontrivial task;microaneurysms;hemorrhages;supervised segmentation;global contrast normalization;zero-phase whitening;geometric transformations;gamma corrections;structured prediction;DRIVE databases;STARE databases;CHASE databases;ROC curve;image classification;central vessel reflex","Databases, Factual;Humans;Image Interpretation, Computer-Assisted;Neural Networks (Computer);Retinal Vessels;Supervised Machine Learning","241","39","","","","","IEEE","IEEE Journals"
"Deep 3D Convolutional Encoder Networks With Shortcuts for Multiscale Feature Integration Applied to Multiple Sclerosis Lesion Segmentation","T. Brosch; L. Y. W. Tang; Y. Yoo; D. K. B. Li; A. Traboulsee; R. Tam","Multiple Sclerosis/Magnetic Resonance Imaging Research Group, Division of Neurology, The University of British Columbia, Vancouver, Canada; Multiple Sclerosis/Magnetic Resonance Imaging Research Group, Division of Neurology, The University of British Columbia, Vancouver, Canada; Multiple Sclerosis/Magnetic Resonance Imaging Research Group, Division of Neurology, The University of British Columbia, Vancouver, Canada; Multiple Sclerosis/Magnetic Resonance Imaging Research Group, Division of Neurology, The University of British Columbia, Vancouver, Canada; Multiple Sclerosis/Magnetic Resonance Imaging Research Group, Division of Neurology, The University of British Columbia, Vancouver, Canada; Multiple Sclerosis/Magnetic Resonance Imaging Research Group, Division of Neurology, The University of British Columbia, Vancouver, Canada","IEEE Transactions on Medical Imaging","","2016","35","5","1229","1239","We propose a novel segmentation approach based on deep 3D convolutional encoder networks with shortcut connections and apply it to the segmentation of multiple sclerosis (MS) lesions in magnetic resonance images. Our model is a neural network that consists of two interconnected pathways, a convolutional pathway, which learns increasingly more abstract and higher-level image features, and a deconvolutional pathway, which predicts the final segmentation at the voxel level. The joint training of the feature extraction and prediction pathways allows for the automatic learning of features at different scales that are optimized for accuracy for any given combination of image types and segmentation task. In addition, shortcut connections between the two pathways allow high- and low-level features to be integrated, which enables the segmentation of lesions across a wide range of sizes. We have evaluated our method on two publicly available data sets (MICCAI 2008 and ISBI 2015 challenges) with the results showing that our method performs comparably to the top-ranked state-of-the-art methods, even when only relatively small data sets are available for training. In addition, we have compared our method with five freely available and widely used MS lesion segmentation methods (EMS, LST-LPA, LST-LGA, Lesion-TOADS, and SLS) on a large data set from an MS clinical trial. The results show that our method consistently outperforms these other methods across a wide range of lesion sizes.","","","10.1109/TMI.2016.2528821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404285","Convolutional neural networks;deep learning;machine learning;magnetic resonance imaging (MRI);multiple sclerosis lesions;segmentation","Image segmentation;Lesions;Convolution;Feature extraction;Imaging;Neural networks;Training","biomedical MRI;feature extraction;image segmentation;medical image processing;neural nets","deep 3D convolutional encoder networks;multiscale feature integration;multiple sclerosis lesion segmentation;shortcut connections;magnetic resonance images;neural network;interconnected pathways;convolutional pathway;higher-level image features;deconvolutional pathway;voxel level;feature extraction;prediction pathways;automatic feature learning;segmentation task;low-level features;publicly available data sets;MICCAI 2008 challenges;ISBI 2015 challenges;top-ranked state-of-the-art methods;MS lesion segmentation methods;EMS;LST-LPA;LST-LGA;Lesion-TOADS;SLS;MS clinical trial","Algorithms;Brain;Humans;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Machine Learning;Magnetic Resonance Imaging;Multiple Sclerosis;Neural Networks (Computer)","132","47","","","","","IEEE","IEEE Journals"
"MiRTDL: A Deep Learning Approach for miRNA Target Prediction","S. Cheng; M. Guo; C. Wang; X. Liu; Y. Liu; X. Wu","School of Computer Science and Technology, Harbin Institute of Technology, 92 West Dazhi Street, Nan Gang District, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, 92 West Dazhi Street, Nan Gang District, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, 92 West Dazhi Street, Nan Gang District, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, 92 West Dazhi Street, Nan Gang District, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, 92 West Dazhi Street, Nan Gang District, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, 92 West Dazhi Street, Nan Gang District, Harbin, China","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2016","13","6","1161","1169","MicroRNAs (miRNAs) regulate genes that are associated with various diseases. To better understand miRNAs, the miRNA regulatory mechanism needs to be investigated and the real targets identified. Here, we present miRTDL, a new miRNA target prediction algorithm based on convolutional neural network (CNN). The CNN automatically extracts essential information from the input data rather than completely relying on the input dataset generated artificially when the precise miRNA target mechanisms are poorly known. In this work, the constraint relaxing method is first used to construct a balanced training dataset to avoid inaccurate predictions caused by the existing unbalanced dataset. The miRTDL is then applied to 1,606 experimentally validated miRNA target pairs. Finally, the results show that our miRTDL outperforms the existing target prediction algorithms and achieves significantly higher sensitivity, specificity and accuracy of 88.43, 96.44, and 89.98 percent, respectively. We also investigate the miRNA target mechanism, and the results show that the complementation features are more important than the others.","","","10.1109/TCBB.2015.2510002","National Natural Science Foundation of China; National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362158","Constraint relaxation;convolutional neural network;miRNA;target prediction","Prediction algorithms;Neural networks;Support vector machines;Matched filters;Machine learning;RNA;Diseases","bioinformatics;biological techniques;diseases;genetics;learning (artificial intelligence);molecular biophysics;neural nets;RNA","MiRTDL;deep learning approach;microRNA;genes;diseases;miRNA regulatory mechanism;real targets;miRTDL;miRNA target prediction algorithm;convolutional neural network;CNN;essential information;balanced training dataset;miRNA target pairs","Algorithms;Computational Biology;Humans;Machine Learning;MicroRNAs;Sequence Analysis, RNA","20","34","","","","","IEEE","IEEE Journals"
"Deep Learning With Attribute Profiles for Hyperspectral Image Classification","E. Aptoula; M. C. Ozdemir; B. Yanikoglu","Gebze Technical University, Gebze, Turkey; Sabanci University, Istanbul, Turkey; Sabanci University, Istanbul, Turkey","IEEE Geoscience and Remote Sensing Letters","","2016","13","12","1970","1974","Effective spatial-spectral pixel description is of crucial significance for the classification of hyperspectral remote sensing images. Attribute profiles are considered as one of the most prominent approaches in this regard, since they can capture efficiently arbitrary geometric and spectral properties. Lately though, the advent of deep learning in its various forms has also led to remarkable classification performances by operating directly on hyperspectral input. In this letter, we explore the collaboration potential of these two powerful feature extraction approaches. Specifically, we propose a new strategy for hyperspectral image classification, where attribute filtered images are stacked and provided as input to convolutional neural networks. Our experiments with two real hyperspectral remote sensing data sets show that the proposed strategy leads to a performance improvement, as opposed to using each of the involved approaches individually.","","","10.1109/LGRS.2016.2619354","TUBITAK; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733086","Attribute profiles (APs);deep learning;hyperspectral images;mathematical morphology;pixel classification","Hyperspectral imaging;Machine learning;Feature extraction;Neural networks;Gray-scale","feature extraction;hyperspectral imaging;image classification;neural nets;remote sensing","hyperspectral remote sensing dataset;convolutional neural network;feature extraction;spectral properties;arbitrary geometric properties;hyperspectral remote sensing image classification;deep learning","","48","20","","","","","IEEE","IEEE Journals"
"Toward a Blind Deep Quality Evaluator for Stereoscopic Images Based on Monocular and Binocular Interactions","F. Shao; W. Tian; W. Lin; G. Jiang; Q. Dai","Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; Centre for Multimedia and Network Technology School of Computer Engineering, Nanyang Technological University, Singapore; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; Broadband Networks and Digital Media Laboratory, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","","2016","25","5","2059","2074","During recent years, blind image quality assessment (BIQA) has been intensively studied with different machine learning tools. Existing BIQA metrics, however, do not design for stereoscopic images. We believe this problem can be resolved by separating 3D images and capturing the essential attributes of images via deep neural network. In this paper, we propose a blind deep quality evaluator (DQE) for stereoscopic images (denoted by 3D-DQE) based on monocular and binocular interactions. The key technical steps in the proposed 3D-DQE are to train two separate 2D deep neural networks (2D-DNNs) from 2D monocular images and cyclopean images to model the process of monocular and binocular quality predictions, and combine the measured 2D monocular and cyclopean quality scores using different weighting schemes. Experimental results on four public 3D image quality assessment databases demonstrate that in comparison with the existing methods, the devised algorithm achieves high consistent alignment with subjective assessment.","","","10.1109/TIP.2016.2538462","National Natural Science Foundation of China; K. C. Wong Magna Fund in Ningbo University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426393","deep quality evaluator;blind image quality assessment;stereoscopic image;deep neural network;Deep quality evaluator;blind image quality assessment;stereoscopic image;deep neural network","Three-dimensional displays;Image quality;Stereo image processing;Predictive models;Training;Neural networks;Distortion","image capture;image resolution;neural nets;stereo image processing","cyclopean images;2D monocular images;2D-DNN;2D deep neural networks;3D DQE;image capturing;3D image quality assessment databases;machine learning tools;BIQA;blind image quality assessment;binocular interactions;monocular interactions;stereoscopic images;blind deep quality evaluator","","40","77","","","","","IEEE","IEEE Journals"
"Stacked Sparse Autoencoder (SSAE) for Nuclei Detection on Breast Cancer Histopathology Images","J. Xu; L. Xiang; Q. Liu; H. Gilmore; J. Wu; J. Tang; A. Madabhushi","Jiangsu Key Laboratory of Big Data Analysis Technique and CICAEET, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technique and CICAEET, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technique and CICAEET, Nanjing University of Information Science and Technology, Nanjing, China; Department of Pathology-Anatomic, University Hospitals Case Medical Center, Case Western Reserve University, OH, USA; Jiangsu Cancer Hospital, Nanjing, China; Jiangsu Cancer Hospital, Nanjing, China; Department of Biomedical Engineering, Case Western Reserve University, OH, USA","IEEE Transactions on Medical Imaging","","2016","35","1","119","130","Automated nuclear detection is a critical step for a number of computer assisted pathology related image analysis algorithms such as for automated grading of breast cancer tissue specimens. The Nottingham Histologic Score system is highly correlated with the shape and appearance of breast cancer nuclei in histopathological images. However, automated nucleus detection is complicated by 1) the large number of nuclei and the size of high resolution digitized pathology images, and 2) the variability in size, shape, appearance, and texture of the individual nuclei. Recently there has been interest in the application of “Deep Learning” strategies for classification and analysis of big image data. Histopathology, given its size and complexity, represents an excellent use case for application of deep learning strategies. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a deep learning strategy, is presented for efficient nuclei detection on high-resolution histopathological images of breast cancer. The SSAE learns high-level features from just pixel intensities alone in order to identify distinguishing features of nuclei. A sliding window operation is applied to each image in order to represent image patches via high-level features obtained via the auto-encoder, which are then subsequently fed to a classifier which categorizes each image patch as nuclear or non-nuclear. Across a cohort of 500 histopathological images (2200 × 2200) and approximately 3500 manually segmented individual nuclei serving as the groundtruth, SSAE was shown to have an improved F-measure 84.49% and an average area under Precision-Recall curve (AveP) 78.83%. The SSAE approach also out-performed nine other state of the art nuclear detection strategies.","","","10.1109/TMI.2015.2458702","National Natural Science Foundation of China; National Institute of Diabetes and Digestive and Kidney Diseases; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163353","Automated nuclei detection;breast cancer histopathology;feature representation learning;stacked sparse autoencoder;digital pathology;deep learning","Feature extraction;Training;Breast cancer;Pathology;Image color analysis;Decoding","biological tissues;cancer;image classification;image coding;image representation;image resolution;learning (artificial intelligence);medical image processing","stacked sparse autoencoder;nuclei detection;automated nuclear detection;computer assisted pathology related image analysis algorithms;automated grading;breast cancer tissue specimens;Nottingham histologic score system;high resolution digitized pathology images;deep learning strategy;high-resolution breast cancer histopathological images;high-level features;pixel intensity;sliding window operation;image classifier;image patch representation;average area under Precision-Recall curve","Algorithms;Breast Neoplasms;Cell Nucleus;Female;Histocytochemistry;Humans;Image Processing, Computer-Assisted;Machine Learning","239","43","","","","","IEEE","IEEE Journals"
"Learning of Multimodal Representations With Random Walks on the Click Graph","F. Wu; X. Lu; J. Song; S. Yan; Z. M. Zhang; Y. Rui; Y. Zhuang","School of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Computer Science and Technology, Zhejiang University, Hangzhou, China; Electrical Engineering Department, National University of Singapore, Singapore; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Microsoft Research Asia, Beijing, China; School of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Image Processing","","2016","25","2","630","642","In multimedia information retrieval, most classic approaches tend to represent different modalities of media in the same feature space. With the click data collected from the users' searching behavior, existing approaches take either one-to-one paired data (text-image pairs) or ranking examples (text-query-image and/or image-query-text ranking lists) as training examples, which do not make full use of the click data, particularly the implicit connections among the data objects. In this paper, we treat the click data as a large click graph, in which vertices are images/text queries and edges indicate the clicks between an image and a query. We consider learning a multimodal representation from the perspective of encoding the explicit/implicit relevance relationship between the vertices in the click graph. By minimizing both the truncated random walk loss as well as the distance between the learned representation of vertices and their corresponding deep neural network output, the proposed model which is named multimodal random walk neural network (MRW-NN) can be applied to not only learn robust representation of the existing multimodal data in the click graph, but also deal with the unseen queries and images to support cross-modal retrieval. We evaluate the latent representation learned by MRW-NN on a public large-scale click log data set Clickture and further show that MRW-NN achieves much better cross-modal retrieval performance on the unseen queries/images than the other state-of-the-art methods.","","","10.1109/TIP.2015.2507401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350243","cross-media search;click log;latent representation;deep learning;Cross-media search;click log;latent representation;deep learning","Neural networks;Semantics;Training;Data models;Search engines;Electronic mail;Image processing","graph theory;image representation;image retrieval;learning (artificial intelligence);neural nets","multimodal representation learning;click graph;multimedia information retrieval;click data collection;truncated random walk loss;multimodal random walk neural network;MRW-NN;cross-modal retrieval;public large-scale click log data set Clickture","","20","39","","","","","IEEE","IEEE Journals"
"Robust Single Image Super-Resolution via Deep Networks With Sparse Prior","D. Liu; Z. Wang; B. Wen; J. Yang; W. Han; T. S. Huang","Department of Electrical and Computer EngineeringBeckman Institute, University of Illinois at Urbana–Champaign, Urbana, IL, USA; Adobe Systems Inc., San Jose, CA, USA; Department of Electrical and Computer EngineeringCoordinated Science Laboratory, University of Illinois at Urbana–Champaign, Urbana, IL, USA; Snapchat Inc., Venice, CA, USA; Department of Electrical and Computer EngineeringBeckman Institute, University of Illinois at Urbana–Champaign, Urbana, IL, USA; Department of Electrical and Computer EngineeringBeckman Institute, University of Illinois at Urbana–Champaign, Urbana, IL, USA","IEEE Transactions on Image Processing","","2016","25","7","3194","3207","Single image super-resolution (SR) is an ill-posed problem, which tries to recover a high-resolution image from its low-resolution observation. To regularize the solution of the problem, previous methods have focused on designing good priors for natural images, such as sparse representation, or directly learning the priors from a large data set with models, such as deep neural networks. In this paper, we argue that domain expertise from the conventional sparse coding model can be combined with the key ingredients of deep learning to achieve further improved results. We demonstrate that a sparse coding model particularly designed for SR can be incarnated as a neural network with the merit of end-to-end optimization over training data. The network has a cascaded structure, which boosts the SR performance for both fixed and incremental scaling factors. The proposed training and testing schemes can be extended for robust handling of images with additional degradation, such as noise and blurring. A subjective assessment is conducted and analyzed in order to thoroughly evaluate various SR techniques. Our proposed model is tested on a wide range of images, and it significantly outperforms the existing state-of-the-art methods for various scaling factors both quantitatively and perceptually.","","","10.1109/TIP.2016.2564643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7466062","image super-resolution;deep neural networks;sparse coding;Image super-resolution;deep neural networks;sparse coding","Image coding;Neural networks;Training;Dictionaries;Machine learning;Image resolution;Degradation","image coding;image resolution;neural nets","robust single image super-resolution;sparse prior;ill-posed problem;low-resolution observation;natural images;deep neural networks;conventional sparse coding;key ingredients;deep learning;end-to-end optimization;training data;cascaded structure;fixed scaling factors;incremental scaling factors;subjective assessment","","116","50","","","","","IEEE","IEEE Journals"
"Learning Geographical Hierarchy Features via a Compositional Model","X. Zhang; X. Hu; S. Wang; Y. Yang; Z. Li; J. Zhou","Beijing Key Laboratory of Network Technology, Beihang University, Beijing, China; Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Beijing Key Laboratory of Network Technology, Beihang University, Beijing, China; Beijing Key Laboratory of Network Technology, Beihang University, Beijing, China; Beijing Advanced Innovation Center for Imaging Technology, Capital Normal University, Beijing, China","IEEE Transactions on Multimedia","","2016","18","9","1855","1868","Image location prediction is used to estimate the geolocation where an image is taken, which is important for many image applications, such as image retrieval, image browsing, and organization. Since a social image contains heterogeneous contents, such as visual content and textual content, effectively incorporating these contents to predict location is nontrivial. Moreover, it is observed that image content patterns and the locations where they may appear correlate hierarchically. Traditional image location prediction methods mainly adopt a single-level architecture and assume images are independently distributed in geographical space, which is not directly adaptable to the hierarchical correlation. In this paper, we propose a geographically hierarchical bi-modal deep belief network (GH-BDBN) model, which is a compositional learning architecture that integrates multi-modal deep learning model with a non-parametric hierarchical prior model. GH-BDBN learns a joint representation capturing the correlations among different types of image content using a bi-modal DBN, with a geographically hierarchical prior over the joint representation to model the hierarchical correlation between image content and location. Then, an efficient inference algorithm is proposed to learn the parameters and the geographical hierarchical structure of geographical locations. Experimental results demonstrate the superiority of our model for image location prediction.","","","10.1109/TMM.2016.2574122","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Fund of the State Key Laboratory of Software Development Environment; Beijing Advanced Innovation Center for Imaging Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7480446","Hierarchical features;image location;image topic;multi-modal deep model;multi-modal feature","Visualization;Correlation;Predictive models;Flickr;Urban areas;Adaptation models;Prediction algorithms","belief networks;geophysical image processing;image representation;learning (artificial intelligence);neural nets","geographical hierarchy feature learning;compositional model;social image;heterogeneous contents;visual content;textual content;image content patterns;image location prediction methods;single-level architecture;hierarchical correlation;geographically hierarchical bi-modal deep belief network model;GH-BDBN model;compositional learning architecture;multimodal deep learning model;nonparametric hierarchical prior model;inference algorithm;geographical hierarchical structure;joint image content representation","","3","46","","","","","IEEE","IEEE Journals"
"Deep Model Based Transfer and Multi-Task Learning for Biological Image Analysis","W. Zhang; R. Li; T. Zeng; Q. Sun; S. Kumar; J. Ye; S. Ji","Wenlu Zhang is with the Department of Computer Science, Old Dominion University, Norfolk, VA, 23529.(email: wzhang@cs.odu.edu); NA; NA; NA; NA; NA; NA","IEEE Transactions on Big Data","","2016","PP","99","1","1","A central theme in learning from image data is to develop appropriate representations for the specific task at hand. Thus, a practical challenge is to determine what features are appropriate for specific tasks. For example, in the study of gene expression patterns in Drosophila, texture features were particularly effective for determining the developmental stages from in situ hybridization images. Such image representation is however not suitable for controlled vocabulary term annotation. Here, we developed feature extraction methods to generate hierarchical representations for ISH images. Our approach is based on the deep convolutional neural networks that can act on image pixels directly. To make the extracted features generic, the models were trained using a natural image set with millions of labeled examples. These models were transferred to the ISH image domain. To account for the differences between the source and target domains, we proposed a partial transfer learning scheme in which only part of the source model is transferred. We employed multi-task learning method to fine-tune the pre-trained models with labeled ISH images. Results showed that feature representations computed by deep models based on transfer and multi-task learning significantly outperformed other methods for annotating gene expression patterns at different stage ranges.","","","10.1109/TBDATA.2016.2573280","Division of Information and Intelligent Systems; National Institutes of Health; Division of Biological Infrastructure; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7480825","Deep learning;transfer learning;multi-task learning;image analysis;bioinformatics","Feature extraction;Computational modeling;Biological system modeling;Gene expression;Data models;Training","","","","4","","","","","","IEEE","IEEE Early Access Articles"
"Cross-Modal Retrieval via Deep and Bidirectional Representation Learning","Y. He; S. Xiang; C. Kang; J. Wang; C. Pan","National Laboratory of Pattern Recognition, Institute of Automation, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Beijing, China","IEEE Transactions on Multimedia","","2016","18","7","1363","1377","Cross-modal retrieval emphasizes understanding inter-modality semantic correlations, which is often achieved by designing a similarity function. Generally, one of the most important things considered by the similarity function is how to make the cross-modal similarity computable. In this paper, a deep and bidirectional representation learning model is proposed to address the issue of image-text cross-modal retrieval. Owing to the solid progress of deep learning in computer vision and natural language processing, it is reliable to extract semantic representations from both raw image and text data by using deep neural networks. Therefore, in the proposed model, two convolution-based networks are adopted to accomplish representation learning for images and texts. By passing the networks, images and texts are mapped to a common space, in which the cross-modal similarity is measured by cosine distance. Subsequently, a bidirectional network architecture is designed to capture the property of the cross-modal retrieval-the bidirectional search. Such architecture is characterized by simultaneously involving the matched and unmatched image-text pairs for training. Accordingly, a learning framework with maximum likelihood criterion is finally developed. The network parameters are optimized via backpropagation and stochastic gradient descent. A great deal of experiments are conducted to sufficiently evaluate the proposed method on three publicly released datasets: IAPRTC-12, Flickr30k, and Flickr8k. The overall results definitely show that the proposed architecture is effective and the learned representations have good semantics to achieve superior cross-modal retrieval performance.","","","10.1109/TMM.2016.2558463","National Basic Research Program of China; Strategic Priority Research Program of the CAS; National Natural Science Foundation of China; Beijing Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7460254","Cross-Modal Retrieval;Convolutional Neural Network;Representation Learning;Word Embedding;Bidirectional Modeling;Bidirectional modeling;convolutional neural network;cross-modal retrieval;representation learning;word embedding","Convolution;Semantics;Neural networks;Feature extraction;Computational modeling;Training;Correlation","backpropagation;computer vision;convolution;gradient methods;information retrieval;knowledge representation;maximum likelihood estimation;natural language processing;neural nets;optimisation;stochastic processes;text analysis","image-text cross-modal retrieval;deep neural network;bidirectional representation learning;similarity function;computer vision;natural language processing;convolution-based network;cosine distance;bidirectional network architecture;maximum likelihood criterion;network parameter optimization;backpropagation;stochastic gradient descent","","30","61","","","","","IEEE","IEEE Journals"
"Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks","A. Dosovitskiy; P. Fischer; J. T. Springenberg; M. Riedmiller; T. Brox","Computer Science Department at the University of Freiburg; Computer Science Department at the University of Freiburg; Computer Science Department at the University of Freiburg; Computer Science Department at the University of Freiburg; Computer Science Department at the University of Freiburg","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","9","1734","1747","Deep convolutional networks have proven to be very successful in learning task specific features that allow for unprecedented performance on various computer vision tasks. Training of such networks follows mostly the supervised learning paradigm, where sufficiently many input-output pairs are required for training. Acquisition of large training sets is one of the key challenges, when approaching a new task. In this paper, we aim for generic feature learning and present an approach for training a convolutional network using only unlabeled data. To this end, we train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled `seed' image patch. In contrast to supervised network training, the resulting feature representation is not class specific. It rather provides robustness to the transformations that have been applied during training. This generic feature representation allows for classification results that outperform the state of the art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101, Caltech-256). While features learned with our approach cannot compete with class specific features from supervised training on a classification task, we show that they are advantageous on geometric matching problems, where they also outperform the SIFT descriptor.","","","10.1109/TPAMI.2015.2496141","ERC; BrainLinks-BrainTools Cluster of Excellence; German Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312476","Convolutional networks;unsupervised learning;feature learning;image classification;descriptor matching","Training;Accuracy;Feature extraction;Neural networks;Image color analysis;Support vector machines;Unsupervised learning","computer vision;feature extraction;image matching;image representation;learning (artificial intelligence);neural nets;transforms","discriminative unsupervised feature learning;exemplar convolutional neural networks;computer vision tasks;supervised learning;generic feature learning;unlabeled data;image patch;supervised network training;generic feature representation;geometric matching problems;SIFT descriptor","","66","46","","","","","IEEE","IEEE Journals"
"Visual Understanding via Multi-Feature Shared Learning With Global Consistency","L. Zhang; D. Zhang","College of Communication Engineering, Chongqing University, Chongqing; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Multimedia","","2016","18","2","247","259","Image/video data is usually represented with multiple visual features. Fusion of multi-source information for establishing attributes has been widely recognized. Multi- feature visual recognition has recently received much attention in multimedia applications. This paper studies visual understanding via a newly proposed l<sub>2</sub>-norm-based multi-feature shared learning framework, which can simultaneously learn a global label matrix and multiple sub-classifiers with the labeled multi-feature data. Additionally, a group graph manifold regularizer composed of the Laplacian and Hessian graph is proposed. It can better preserve the manifold structure of each feature, such that the label prediction power is much improved through semi-supervised learning with global label consistency. For convenience, we call the proposed approach global-label- consistent classifier (GLCC). The merits of the proposed method include the following: 1) the manifold structure information of each feature is exploited in learning, resulting in a more faithful classification owing to the global label consistency; 2) a group graph manifold regularizer based on the Laplacian and Hessian regularization is constructed ; and 3) an efficient alternative optimization method is introduced as a fast solver owing its speed to convex sub-problems. Experiments on several benchmark visual datasets-the 17-category Oxford Flower dataset, the challenging 101- category Caltech dataset, the YouTube and Consumer Videos dataset, and the large-scale NUS-WIDE dataset-have been used for multimedia understanding . The results demonstrate that the proposed approach compares favorably with state-of-the-art algorithms. An extensive experiment using the deep convolutional activation features also shows the effectiveness of the proposed approach. The code will be available on http://www.escience.cn/people/lei/index.html.","","","10.1109/TMM.2015.2510509","National Natural Science Foundation of China; Hong Kong Scholar Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7360941","Multi-feature learning;multimedia understanding;semi-supervised learning;visual recognition","Manifolds;Visualization;Laplace equations;Semisupervised learning;Multimedia communication;Kernel;Training","feature extraction;graph theory;image classification;image representation;learning (artificial intelligence);multimedia computing","visual understanding;multifeature shared learning;image-video data;multisource information fusion;multifeature visual recognition;multimedia applications;l<sub>2</sub>-norm-based multifeature shared learning framework;global label matrix;labeled multifeature data;group graph manifold regularizer;Laplacian graph;Hessian graph;semisupervised learning;global label consistency;global-label-consistent classifier;GLCC approach;Hessian regularization;Laplacian regularization;17-category Oxford Flower dataset;Caltech dataset;YouTube Videos dataset;Consumer Videos dataset;large-scale NUS-WIDE dataset;multimedia understanding","","30","57","","","","","IEEE","IEEE Journals"
"StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity","M. J. Shafiee; P. Siva; A. Wong","Department of Systems Design Engineering, University of Waterloo, Waterloo, ON, Canada; Aimetis Corporation, Waterloo, ON, Canada; Department of Systems Design Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Access","","2016","4","","1915","1924","Deep neural networks are a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. One area in deep neural networks that are ripe for exploration is neural connectivity formation. A pivotal study on the brain tissue of rats found that synaptic formation for specific functional connectivity in neocortical neural microcircuits can be surprisingly well modeled and predicted as a random formation. Motivated by this intriguing finding, we introduce the concept of StochasticNet where deep neural networks are formed via stochastic connectivity between neurons. As a result, any type of deep neural networks can be formed as a StochasticNet by allowing the neuron connectivity to be stochastic. Stochastic synaptic formations in a deep neural network architecture can allow for efficient utilization of neurons for performing specific tasks. To evaluate the feasibility of such a deep neural network architecture, we train a StochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and STL-10). Experimental results show that a StochasticNet using less than half the number of neural connections as a conventional deep neural network achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST, and SVHN data sets. Interestingly, StochasticNet with less than half the number of neural connections, achieved a higher accuracy (relative improvement in test error rate of ~6% compared to ConvNet) on the STL-10 data set than a conventional deep neural network. Finally, the StochasticNets have faster operational speeds while achieving better or similar accuracy performances.","","","10.1109/ACCESS.2016.2551458","Natural Sciences and Engineering Research Council of Canada through the Canada Research Chairs Program; Ontario Ministry of Research and Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448375","Deep Convolutional Nueral Network;StochasticNet;Random Graph;Deep convolutional nueral network;StochasticNet;random graph","Neural networks;Stochastic processes;Random graphs;Biological neural networks;Brain modeling;Data models","graph theory;learning (artificial intelligence);neural nets;stochastic processes","stochasticNet;deep neural networks;stochastic connectivity;machine learning;neural connectivity formation;neurons;stochastic synaptic formations;neural connections;CIFAR-10;MNIST;SVHN","","11","25","","","","","IEEE","IEEE Journals"
"An Automatic Learning-Based Framework for Robust Nucleus Segmentation","F. Xing; Y. Xie; L. Yang","Department of Electrical and Computer Engineering, University of Florida, Gainesville; J. Crayton Pruitt Family Department of Biomedical Engineering, University of Florida, Gainesville; J. Crayton Pruitt Family Department of Biomedical Engineering, University of Florida, Gainesville","IEEE Transactions on Medical Imaging","","2016","35","2","550","566","Computer-aided image analysis of histopathology specimens could potentially provide support for early detection and improved characterization of diseases such as brain tumor, pancreatic neuroendocrine tumor (NET), and breast cancer. Automated nucleus segmentation is a prerequisite for various quantitative analyses including automatic morphological feature computation. However, it remains to be a challenging problem due to the complex nature of histopathology images. In this paper, we propose a learning-based framework for robust and automatic nucleus segmentation with shape preservation. Given a nucleus image, it begins with a deep convolutional neural network (CNN) model to generate a probability map, on which an iterative region merging approach is performed for shape initializations. Next, a novel segmentation algorithm is exploited to separate individual nuclei combining a robust selection-based sparse shape model and a local repulsive deformable model. One of the significant benefits of the proposed framework is that it is applicable to different staining histopathology images. Due to the feature learning characteristic of the deep CNN and the high level shape prior modeling, the proposed method is general enough to perform well across multiple scenarios. We have tested the proposed algorithm on three large-scale pathology image datasets using a range of different tissue and stain preparations, and the comparative experiments with recent state of the arts demonstrate the superior performance of the proposed approach.","","","10.1109/TMI.2015.2481436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7274740","Deep convolutional neural network;nucleus segmentation;sparse representation","Shape;Image segmentation;Image color analysis;Robustness;Computational modeling;Tumors;Breast cancer","cancer;diagnostic radiography;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;probability;tumours","automatic learning-based framework;robust nucleus segmentation;computer-aided image analysis;histopathology specimens;early detection;diseases;brain tumor;pancreatic neuroendocrine tumor;breast cancer;automated nucleus segmentation;quantitative analysis;automatic morphological feature computation;histopathology imaging;deep convolutional neural network model;deep CNN model;probability map;iterative region merging approach;robust selection-based sparse shape model;local repulsive deformable model;staining histopathology images;feature learning characteristic;high level shape prior modeling;large-scale pathology image datasets;tissue","Algorithms;Brain;Brain Neoplasms;Breast;Breast Neoplasms;Cell Nucleus;Female;Humans;Image Processing, Computer-Assisted;Neural Networks (Computer)","96","83","","","","","IEEE","IEEE Journals"
"Fast Convolutional Neural Network Training Using Selective Data Sampling: Application to Hemorrhage Detection in Color Fundus Images","M. J. J. P. van Grinsven; B. van Ginneken; C. B. Hoyng; T. Theelen; C. I. Sánchez","Diagnostic Image Analysis Group, Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands; Diagnostic Image Analysis Group, Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands; Department of Ophthalmology, Radboud University Medical Center, Nijmegen, The Netherlands; Department of Ophthalmology, Radboud University Medical Center, Nijmegen, The Netherlands; Diagnostic Image Analysis Group, Department of Radiology and Nuclear Medicine and with the Department of Ophthalmology, Radboud University Medical Center, Nijmegen, The Netherlands","IEEE Transactions on Medical Imaging","","2016","35","5","1273","1284","Convolutional neural networks (CNNs) are deep learning network architectures that have pushed forward the state-of-the-art in a range of computer vision applications and are increasingly popular in medical image analysis. However, training of CNNs is time-consuming and challenging. In medical image analysis tasks, the majority of training examples are easy to classify and therefore contribute little to the CNN learning process. In this paper, we propose a method to improve and speed-up the CNN training for medical image analysis tasks by dynamically selecting misclassified negative samples during training. Training samples are heuristically sampled based on classification by the current status of the CNN. Weights are assigned to the training samples and informative samples are more likely to be included in the next CNN training iteration. We evaluated and compared our proposed method by training a CNN with (SeS) and without (NSeS) the selective sampling method. We focus on the detection of hemorrhages in color fundus images. A decreased training time from 170 epochs to 60 epochs with an increased performance-on par with two human experts-was achieved with areas under the receiver operating characteristics curve of 0.894 and 0.972 on two data sets. The SeS CNN statistically outperformed the NSeS CNN on an independent test set.","","","10.1109/TMI.2016.2526689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401052","Convolutional neural network;deep learning;hemorrhage;selective sampling","Training;Hemorrhaging;Biomedical imaging;Observers;Image color analysis;Image analysis;Databases","biomedical optical imaging;blood;computer vision;image classification;image colour analysis;image sampling;learning (artificial intelligence);medical image processing;neural nets;sensitivity analysis","fast convolutional neural network training;selective data sampling;hemorrhage detection;color fundus images;deep learning network architectures;computer vision applications;medical image analysis tasks;CNN learning process;dynamically selecting misclassified negative samples;CNN training iteration;selective sampling method;receiver operating characteristics curve;independent test set","Databases, Factual;Fundus Oculi;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Neural Networks (Computer);Retinal Hemorrhage","119","48","","","","","IEEE","IEEE Journals"
"Two-Stage Learning to Predict Human Eye Fixations via SDAEs","J. Han; D. Zhang; S. Wen; L. Guo; T. Liu; X. Li","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; Department of Computer Science, The University of Georgia, Athens, GA, USA; Center for OPTical IMagery Analysis and LearningState Key Laboratory of Transient Optics and Photonics, Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China","IEEE Transactions on Cybernetics","","2016","46","2","487","498","Saliency detection models aiming to quantitatively predict human eye-attended locations in the visual field have been receiving increasing research interest in recent years. Unlike traditional methods that rely on hand-designed features and contrast inference mechanisms, this paper proposes a novel framework to learn saliency detection models from raw image data using deep networks. The proposed framework mainly consists of two learning stages. At the first learning stage, we develop a stacked denoising autoencoder (SDAE) model to learn robust, representative features from raw image data under an unsupervised manner. The second learning stage aims to jointly learn optimal mechanisms to capture the intrinsic mutual patterns as the feature contrast and to integrate them for final saliency prediction. Given the input of pairs of a center patch and its surrounding patches represented by the features learned at the first stage, a SDAE network is trained under the supervision of eye fixation labels, which achieves both contrast inference and contrast integration simultaneously. Experiments on three publically available eye tracking benchmarks and the comparisons with 16 state-of-the-art approaches demonstrate the effectiveness of the proposed framework.","","","10.1109/TCYB.2015.2404432","National Natural Science Foundation of China; Doctoral Fund of Ministry of Education of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051244","Deep networks;eye fixation prediction;saliency detection;stacked denoising autoencoders (SDAEs);Deep networks;eye fixation prediction;saliency detection;stacked denoising autoencoders (SDAEs)","Feature extraction;Visualization;Computational modeling;Data models;Noise reduction;Image color analysis;Robustness","image denoising;inference mechanisms;learning (artificial intelligence)","two-stage learning;human eye fixation prediction;SDAE;saliency detection models;human eye-attended locations;contrast inference mechanisms;raw image data;deep networks;stacked denoising autoencoder;SDAE model;intrinsic mutual patterns;feature contrast;SDAE network;eye fixation labels;contrast inference;contrast integration","Algorithms;Fixation, Ocular;Humans;Models, Statistical;Pattern Recognition, Automated","70","60","","","","","IEEE","IEEE Journals"
"Scene Parsing With Integration of Parametric and Non-Parametric Models","B. Shuai; Z. Zuo; G. Wang; B. Wang","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Image Processing","","2016","25","5","2379","2391","We adopt convolutional neural networks (CNNs) to be our parametric model to learn discriminative features and classifiers for local patch classification. Based on the occurrence frequency distribution of classes, an ensemble of CNNs (CNN-Ensemble) are learned, in which each CNN component focuses on learning different and complementary visual patterns. The local beliefs of pixels are output by CNN-Ensemble. Considering that visually similar pixels are indistinguishable under local context, we leverage the global scene semantics to alleviate the local ambiguity. The global scene constraint is mathematically achieved by adding a global energy term to the labeling energy function, and it is practically estimated in a non-parametric framework. A large margin-based CNN metric learning method is also proposed for better global belief estimation. In the end, the integration of local and global beliefs gives rise to the class likelihood of pixels, based on which maximum marginal inference is performed to generate the label prediction maps. Even without any post-processing, we achieve the state-of-the-art results on the challenging SiftFlow and Barcelona benchmarks.","","","10.1109/TIP.2016.2533862","Rapid-Rich Object Search (ROSE) Laboratory, Nanyang Technological University, Singapore; Singapore Ministry of Education; Singapore Agency for Science, Technology and Research within the Science and Engineering Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7416177","Scene Parsing;Convolution Neural Network;CNN-Ensemble;Global Scene Constraint;Local Ambiguity;Deep Learning;Scene parsing;convolution neural network;CNN-ensemble;global scene constraint;local ambiguity;deep learning","Labeling;Neural networks;Semantics;Context;Feature extraction;Visualization;Measurement","feedforward neural nets;image classification;inference mechanisms;learning (artificial intelligence)","scene parsing;parametric model;nonparametric model;convolutional neural networks;CNN-Ensemble;discriminative features learning;discriminative classifiers learning;local patch classification;global scene semantics;global energy term;labeling energy function;margin-based CNN metric learning method;global belief estimation;label prediction maps;marginal inference;SiftFlow benchmark;Barcelona benchmark","","10","54","","","","","IEEE","IEEE Journals"
"Robust Visual Tracking via Convolutional Networks Without Training","K. Zhang; Q. Liu; Y. Wu; M. Yang","Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of Information Science and Technology, Nanjing, China; School of Engineering, University of California at Merced, Merced, CA, USA","IEEE Transactions on Image Processing","","2016","25","4","1779","1792","Deep networks have been successfully applied to visual tracking by learning a generic representation offline from numerous training images. However, the offline training is time-consuming and the learned generic representation may be less discriminative for tracking specific objects. In this paper, we present that, even without offline training with a large amount of auxiliary data, simple two-layer convolutional networks can be powerful enough to learn robust representations for visual tracking. In the first frame, we extract a set of normalized patches from the target region as fixed filters, which integrate a series of adaptive contextual filters surrounding the target to define a set of feature maps in the subsequent frames. These maps measure similarities between each filter and useful local intensity patterns across the target, thereby encoding its local structural information. Furthermore, all the maps together form a global representation, via which the inner geometric layout of the target is also preserved. A simple soft shrinkage method that suppresses noisy values below an adaptive threshold is employed to de-noise the global representation. Our convolutional networks have a lightweight structure and perform favorably against several state-of-the-art methods on the recent tracking benchmark data set with 50 challenging videos.","","","10.1109/TIP.2016.2531283","Natural Science Foundation of China; National Science Foundation of Jiangsu Province; Startup Foundation for Introducing Talent of Nanjing University of Information Science and Technology; National Science Foundation CAREER; Information and Intelligent Systems Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410052","Visual tracking;Convolutional Networks;Deep learning;Visual tracking;convolutional networks;deep learning","Feature extraction;Visualization;Target tracking;Robustness;Layout;Training;Computer architecture","convolution;image denoising;image filtering;image representation;learning (artificial intelligence)","adaptive threshold;noise suppression;soft shrinkage method;encoding;intensity pattern;adaptive contextual filter;auxiliary data;offline training;learned generic representation;deep network;convolutional network;robust visual tracking","","109","50","","","","","IEEE","IEEE Journals"
"Learning Human Identity From Motion Patterns","N. Neverova; C. Wolf; G. Lacey; L. Fridman; D. Chandra; B. Barbello; G. Taylor","Laboratoire d'InfoRmatique en Image et Systemes, Centre National de la Recherche Scientifique, Institut National des Sciences Appliquees de Lyon, Universite de Lyon, Lyon, France; Laboratoire d'InfoRmatique en Image et Systemes, Centre National de la Recherche Scientifique, Institut National des Sciences Appliquees de Lyon, Universite de Lyon, Lyon, France; School of Engineering, University of Guelph, Guelph, ON, Canada; Massachusetts Institute of Technology, Cambridge, MA, USA; Google, Mountain View, CA, USA; Google, Mountain View, CA, USA; School of Engineering, University of Guelph, Guelph, ON, Canada","IEEE Access","","2016","4","","1810","1820","We present a large-scale study exploring the capability of temporal deep neural networks to interpret natural human kinematics and introduce the first method for active biometric authentication with mobile inertial sensors. At Google, we have created a first-of-its-kind data set of human movements, passively collected by 1500 volunteers using their smartphones daily over several months. We compare several neural architectures for efficient learning of temporal multi-modal data representations, propose an optimized shift-invariant dense convolutional mechanism, and incorporate the discriminatively trained dynamic features in a probabilistic generative framework taking into account temporal characteristics. Our results demonstrate that human kinematics convey important information about user identity and can serve as a valuable component of multi-modal authentication systems. Finally, we demonstrate that the proposed model can also be successfully applied in a visual context.","","","10.1109/ACCESS.2016.2557846","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7458136","Authentication;Biometrics (access control);Learning;Mobile computing;Recurrent neural networks;Authentication;biometrics (access control);learning;mobile computing;recurrent neural networks","Authentication;Biometrics;Access control;Learning systems;Mobile computing;Neural networks;Recurrent neural networks;Context modeling;Kinematics;Biosensors","image motion analysis;kinematics;learning (artificial intelligence);neural nets;sensors","human identity;motion patterns;learning;temporal deep neural networks;natural human kinematics;active biometric authentication;mobile inertial sensors;Google;human movements;temporal multimodal data representations;optimized shift-invariant dense convolutional mechanism;discriminatively trained dynamic features;probabilistic generative framework;temporal characteristics;multimodal authentication systems;visual context","","41","34","","","","","IEEE","IEEE Journals"
"Efficient Saliency-Based Object Detection in Remote Sensing Images Using Deep Belief Networks","W. Diao; X. Sun; X. Zheng; F. Dou; H. Wang; K. Fu","NA; NA; NA; NA; NA; NA","IEEE Geoscience and Remote Sensing Letters","","2016","13","2","137","141","Object detection has been one of the hottest issues in the field of remote sensing image analysis. In this letter, an efficient object detection framework is proposed, which combines the strength of the unsupervised feature learning of deep belief networks (DBNs) and visual saliency. In particular, we propose an efficient coarse object locating method based on a saliency mechanism. The method could avoid an exhaustive search across the image and generate a small number of bounding boxes, which can locate the object quickly and precisely. After that, the trained DBN is used for feature extraction and classification on subimages. The feature learning of the DBN is operated by pretraining each layer of restricted Boltzmann machines (RBMs) using the general layerwise training algorithm. An unsupervised blockwise pretraining strategy is introduced to train the first layer of RBMs, which combines the raw pixels with a saliency map as inputs. This makes an RBM generate local and edge filters. The precise edge position information and pixel value information are more efficient to build a good model of images. Comparative experiments are conducted on the data set acquired by QuickBird with a 60-cm resolution. The results demonstrate the accuracy and efficiency of our method.","","","10.1109/LGRS.2015.2498644","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7378278","Deep belief networks (DBNs);object detection;visual saliency;Deep belief networks (DBNs);object detection;visual saliency","Training;Feature extraction;Remote sensing;Object detection;Image segmentation;Support vector machines;Sun","belief networks;Boltzmann machines;feature extraction;image classification;object detection;remote sensing;unsupervised learning","saliency-based object detection;deep belief network;remote sensing image analysis;unsupervised feature learning;visual saliency;feature extraction;subimage classification;Boltzmann machine;general layerwise training algorithm;unsupervised blockwise pretraining strategy;edge filter","","62","15","","","","","IEEE","IEEE Journals"
"DeepTrack: Learning Discriminative Feature Representations Online for Robust Visual Tracking","H. Li; Y. Li; F. Porikli","School of Computer and Information Engineering, Jiangxi Normal University, Nanchang, China; Canberra Research Lab, National ICT Australia, ACT, Australia; Canberra Research Lab, National ICT Australia, ACT, Australia","IEEE Transactions on Image Processing","","2016","25","4","1834","1848","Deep neural networks, albeit their great success on feature learning in various computer vision tasks, are usually considered as impractical for online visual tracking, because they require very long training time and a large number of training samples. In this paper, we present an efficient and very robust tracking algorithm using a single convolutional neural network (CNN) for learning effective feature representations of the target object in a purely online manner. Our contributions are multifold. First, we introduce a novel truncated structural loss function that maintains as many training samples as possible and reduces the risk of tracking error accumulation. Second, we enhance the ordinary stochastic gradient descent approach in CNN training with a robust sample selection mechanism. The sampling mechanism randomly generates positive and negative samples from different temporal distributions, which are generated by taking the temporal relations and label noise into account. Finally, a lazy yet effective updating scheme is designed for CNN training. Equipped with this novel updating algorithm, the CNN model is robust to some long-existing difficulties in visual tracking, such as occlusion or incorrect detections, without loss of the effective adaption for significant appearance changes. In the experiment, our CNN tracker outperforms all compared state-of-the-art methods on two recently proposed benchmarks, which in total involve over 60 video sequences. The remarkable performance improvement over the existing trackers illustrates the superiority of the feature representations, which are learned purely online via the proposed deep learning framework.","","","10.1109/TIP.2015.2510583","National Natural Science Foundation of China; Australian Research Council’s Discovery Projects Funding Scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362006","Convolutional neural network;deep learning;visual tracking","Training;Visualization;Robustness;Target tracking;Neural networks;Feature extraction","computer vision;gradient methods;learning (artificial intelligence);neural nets;stochastic processes","DeepTrack;learning discriminative feature representations;robust visual tracking;deep neural networks;computer vision;online visual tracking;convolutional neural network;truncated structural loss function;tracking error accumulation;stochastic gradient descent approach;CNN training;temporal distributions;visual tracking;CNN tracker;video sequences","","106","53","","","","","IEEE","IEEE Journals"
"Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images","G. Cheng; P. Zhou; J. Han","School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China","IEEE Transactions on Geoscience and Remote Sensing","","2016","54","12","7405","7415","Object detection in very high resolution optical remote sensing images is a fundamental problem faced for remote sensing image analysis. Due to the advances of powerful feature representations, machine-learning-based object detection is receiving increasing attention. Although numerous feature representations exist, most of them are handcrafted or shallow-learning-based features. As the object detection task becomes more challenging, their description capability becomes limited or even impoverished. More recently, deep learning algorithms, especially convolutional neural networks (CNNs), have shown their much stronger feature representation power in computer vision. Despite the progress made in nature scene images, it is problematic to directly use the CNN feature for object detection in optical remote sensing images because it is difficult to effectively deal with the problem of object rotation variations. To address this problem, this paper proposes a novel and effective approach to learn a rotation-invariant CNN (RICNN) model for advancing the performance of object detection, which is achieved by introducing and learning a new rotation-invariant layer on the basis of the existing CNN architectures. However, different from the training of traditional CNN models that only optimizes the multinomial logistic regression objective, our RICNN model is trained by optimizing a new objective function via imposing a regularization constraint, which explicitly enforces the feature representations of the training samples before and after rotating to be mapped close to each other, hence achieving rotation invariance. To facilitate training, we first train the rotation-invariant layer and then domain-specifically fine-tune the whole RICNN network to further boost the performance. Comprehensive evaluations on a publicly available ten-class object detection data set demonstrate the effectiveness of the proposed method.","","","10.1109/TGRS.2016.2601622","National Science Foundation of China; Fundamental Research Funds for the Central Universities; Innovation Foundation for Doctor Dissertation of NPU; Aerospace Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7560644","Convolutional neural networks (CNNs);machine learning;object detection;remote sensing images;rotation-invariant CNN (RICNN)","Object detection;Feature extraction;Training;Remote sensing;Optical imaging;Optical sensors;Computer architecture","image processing;learning (artificial intelligence);neural nets;object detection;remote sensing","multinomial logistic regression;rotation-invariant layer;nature scene image;deep learning algorithm;shallow-learning-based feature;machine learning;remote sensing image analysis;very-high-resolution optical remote sensing image;object detection;convolutional neural networks;learning rotation-invariant CNN model","","429","51","","","","","IEEE","IEEE Journals"
"Hybrid Deep Learning for Face Verification","Y. Sun; X. Wang; X. Tang","Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","10","1997","2009","This paper proposes a hybrid convolutional network (ConvNet)-Restricted Boltzmann Machine (RBM) model for face verification. A key contribution of this work is to learn high-level relational visual features with rich identity similarity information. The deep ConvNets in our model start by extracting local relational visual features from two face images in comparison, which are further processed through multiple layers to extract high-level and global relational features. To keep enough discriminative information, we use the last hidden layer neuron activations of the ConvNet as features for face verification instead of those of the output layer. To characterize face similarities from different aspects, we concatenate the features extracted from different face region pairs by different deep ConvNets. The resulting high-dimensional relational features are classified by an RBM for face verification. After pre-training each ConvNet and the RBM separately, the entire hybrid network is jointly optimized to further improve the accuracy. Various aspects of the ConvNet structures, relational features, and face verification classifiers are investigated. Our model achieves the state-of-the-art face verification performance on the challenging LFW dataset under both the unrestricted protocol and the setting when outside data is allowed to be used for training.","","","10.1109/TPAMI.2015.2505293","CUHK Computer Vision Cooperation; Huawei; Research Grants Council of Hong Kong; National Natural Science Foundation of China; Guangdong Innovative Research Team Program; Shenzhen Basic Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346495","Convolutional networks;deep learning;face recognition","Feature extraction;Face;Face recognition;Computational modeling;Machine learning;Data models;Sun","","","","42","61","","","","","IEEE","IEEE Journals"
"An Investigation of a Two-Tier Test Strategy in a University Calculus Course: Causes versus Consequences","T. C. Yang; S. Y. Chen; M. C. Chen","Institute of Information Science, Academia Sinica, Taipei, Taiwan; Graduate Institute of Network Learning Technology, National Central University, Taoyuan, Taiwan; Institute of Information Science, Academia Sinica, Taipei, Taiwan","IEEE Transactions on Learning Technologies","","2016","9","2","146","156","Online tests have been identified as a core learning activity in higher education. Unlike conventional online tests, which cannot completely reflect students' learning status, two-tier tests not only consider students' answers, but also take into account reasons for their answers. Due to such significance, research into the two-tier tests had mushroomed but few studies examined why the two-tier test approach was effective. To this end, we conducted an empirical study, where a lag sequential analysis was used to analyze behavior patterns while qualitative data from the questionnaire were applied to explain why these behavior patterns were happened. The results indicated students with a two-tier test tended to realize the rationale of a concept, instead of relying on their memories. In other words, the two-tier test can facilitate students to develop deep thinking skills. This may be because they considered the two-tier test as a learning tool, instead of an assessment tool only.","","","10.1109/TLT.2015.2510003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7360189","Two-tier test;Lag sequential analysis;higher education;Two-tier test;Lag sequential analysis;learning behavior analysis;higher education;online assessment","Calculus;Learning systems;Education;Sequential analysis;Instruments;Reliability;Statistical analysis","calculus;computer aided instruction;educational courses;Internet;mathematics computing","two-tier test strategy;university calculus course;higher education;learning activity;online tests;behavior patterns;deep thinking skills;assessment tool","","","43","","","","","IEEE","IEEE Journals"
"ModDrop: Adaptive Multi-Modal Gesture Recognition","N. Neverova; C. Wolf; G. Taylor; F. Nebout","INSA-Lyon, LIRIS, UMR5205, F-69621, Université de Lyon, CNRS, France; INSA-Lyon, LIRIS, UMR5205, F-69621, Université de Lyon, CNRS, France; School of Engineering, University of Guelph, Canada; Awabot, Villeurbanne, Rhône-Alpes, France","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","8","1692","1706","We present a method for gesture detection and localisation based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at three temporal scales. Key to our technique is a training strategy which exploits: i) careful initialization of individual modalities; and ii) gradual fusion involving random dropping of separate channels (dubbed ModDrop) for learning cross-modality correlations while preserving uniqueness of each modality-specific representation. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams. Fusing multiple modalities at several spatial and temporal scales leads to a significant increase in recognition rates, allowing the model to compensate for errors of the individual classifiers as well as noise in the separate channels. Furthermore, the proposed ModDrop training technique ensures robustness of the classifier to missing signals in one or several channels to produce meaningful predictions from any number of available modalities. In addition, we demonstrate the applicability of the proposed fusion scheme to modalities of arbitrary nature by experiments on the same dataset augmented with audio.","","","10.1109/TPAMI.2015.2461544","Investissement's d’Avenir; Briques Génériques du Logiciel Embarqué; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169562","Gesture recognition;convolutional neural networks;multi-modal learning;deep learning","Joints;Training;Streaming media;Feature extraction;Machine learning;Context","audio streaming;gesture recognition;learning (artificial intelligence);pattern classification;random processes;sensor fusion","gesture detection;gesture localisation;multiscale multimodal deep learning;visual modality;spatial information;temporal scales;training strategy;gradual fusion;random dropping;dubbed ModDrop;cross-modality correlations;modality-specific representation;multiple modality fusion;spatial scales;ModDrop training technique;classifier robustness;adaptive multimodal gesture recognition","","75","65","","","","","IEEE","IEEE Journals"
"Boosting Contextual Information for Deep Neural Network Based Voice Activity Detection","X. Zhang; D. Wang","Department of Computer Science & Engineering and Center for Cognitive & Brain Sciences, The Ohio State University, Columbus, OH, USA; Department of Computer Science & Engineering and Center for Cognitive & Brain Sciences, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","2","252","264","Voice activity detection (VAD) is an important topic in audio signal processing. Contextual information is important for improving the performance of VAD at low signal-to-noise ratios. Here we explore contextual information by machine learning methods at three levels. At the top level, we employ an ensemble learning framework, named multi-resolution stacking (MRS), which is a stack of ensemble classifiers. Each classifier in a building block inputs the concatenation of the predictions of its lower building blocks and the expansion of the raw acoustic feature by a given window (called a resolution). At the middle level, we describe a base classifier in MRS, named boosted deep neural network (bDNN). bDNN first generates multiple base predictions from different contexts of a single frame by only one DNN and then aggregates the base predictions for a better prediction of the frame, and it is different from computationally-expensive boosting methods that train ensembles of classifiers for multiple base predictions. At the bottom level, we employ the multi-resolution cochleagram feature, which incorporates the contextual information by concatenating the cochleagram features at multiple spectrotemporal resolutions. Experimental results show that the MRS-based VAD outperforms other VADs by a considerable margin. Moreover, when trained on a large amount of noise types and a wide range of signal-to-noise ratios, the MRS-based VAD demonstrates surprisingly good generalization performance on unseen test scenarios, approaching the performance with noise-dependent training.","","","10.1109/TASLP.2015.2505415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7347379","Cochleagram;deep neural network;ensemble learning;multi-resolution stacking;noise-independent training;voice activity detection","Training;Acoustics;Speech;Signal to noise ratio;Neural networks;Stacking","acoustic signal detection;audio signal processing;learning (artificial intelligence);neural nets;prediction theory;signal resolution;speech processing","contextual information boosting;voice activity detection;audio signal processing;machine learning method;multiresolution stacking;raw acoustic feature expansion;boosted deep neural network;frame prediction;multiresolution cochleagram feature;multiple spectrotemporal resolution;signal-to-noise ratio","","46","51","","","","","IEEE","IEEE Journals"
"Automated Detection of Three-Dimensional Cars in Mobile Laser Scanning Point Clouds Using DBM-Hough-Forests","Y. Yu; J. Li; H. Guan; C. Wang","Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University, Xiamen, China; Department of Geography and Environmental ManagementFujian Key Laboratory of Sensing and Computing for Smart Cities, School of Information Science and Engineering, Xiamen University, Xiamen, ON, China; College of Geography and Remote Sensing, Nanjing University of Information Science and Technology, Nanjing, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Information Science and Engineering, Xiamen University, Xiamen, China","IEEE Transactions on Geoscience and Remote Sensing","","2016","54","7","4130","4142","This paper presents an automated algorithm for rapidly and effectively detecting cars directly from large-volume 3-D point clouds. Rather than using low-order descriptors, a multilayer feature generation model is created to obtain high-order feature representations for 3-D local patches through deep learning techniques. To handle cars with different levels of incompleteness caused by data acquisition ways and occlusions, a hierarchical visibility estimation model is developed to augment Hough voting. Considering scale and orientation variations in the azimuth direction, a set of multiscale Hough forests is constructed to rotationally cast votes to estimate cars' centroids. Quantitative assessments show that the proposed algorithm achieves average completeness, correctness, quality, and F1-measure of 0.94, 0.96, 0.90, and 0.95, respectively, in detecting 3-D cars. Comparative studies also demonstrate that the proposed algorithm outperforms the other four existing algorithms in accurately and completely detecting 3-D cars from large-scale 3-D point clouds.","","","10.1109/TGRS.2016.2537830","National Natural Science Foundation of China; PAPD; CICAEET; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442541","Car detection;deep learning;Hough forest;mobile laser scanning (MLS);point cloud;visibility estimation;Car detection;deep learning;Hough forest;mobile laser scanning (MLS);point cloud;visibility estimation","Automobiles;Solid modeling;Three-dimensional displays;Nonhomogeneous media;Estimation;Training;Object detection","automobiles;computer graphics;estimation theory;Hough transforms;learning (artificial intelligence);object detection;statistical analysis;traffic engineering computing","three-dimensional car detection;mobile laser scanning point cloud;DBM-Hough-Forests;large-volume 3D point cloud;multilayer feature generation model;high-order feature representation;3D local patches;deep learning technique;hierarchical visibility estimation model;Hough voting;scale variation;orientation variation;azimuth direction;multiscale Hough forests;F1-measure;large-scale 3D point cloud","","18","33","","","","","IEEE","IEEE Journals"
"Graph-Based Semisupervised Learning for Acoustic Modeling in Automatic Speech Recognition","Y. Liu; K. Kirchhoff","Electrical Engineering Department, University of Washington, Seattle, WA, USA; Electrical Engineering Department, University of Washington, Seattle, WA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","11","1946","1956","In this paper, we investigate how to apply graph-based semisupervised learning to acoustic modeling in speech recognition. Graph-based semisupervised learning is a widely used transductive semisupervised learning method in which labeled and unlabeled data are jointly represented as a weighted graph; the resulting graph structure is then used as a constraint during the classification of unlabeled data points. We investigate suitable graph-based learning algorithms for speech data and evaluate two different frameworks for integrating graph-based learning into state-of-the-art, deep neural network (DDN)-based speech recognition systems. The first framework utilizes graph-based learning in parallel with a DNN classifier within a lattice-rescoring framework, whereas the second framework relies on an embedding of graph neighborhood information into continuous space using an autoencoder. We demonstrate significant improvements in framelevel phonetic classification accuracy and consistent reductions in word error rate on large-vocabulary conversational speech recognition tasks.","","","10.1109/TASLP.2016.2593800","Division of Information and Intelligent Systems; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518590","Automatic speech recognition","Speech recognition;Acoustics;Speech;Adaptation models;Speech processing;Semisupervised learning;Training","graph theory;learning (artificial intelligence);neural nets;pattern classification;speech recognition","graph-based semisupervised learning;acoustic modeling;automatic speech recognition;transductive semisupervised learning method;unlabeled data points;graph-based learning algorithms;speech data;deep neural network;speech recognition systems;DNN classifier;lattice-rescoring framework;graph neighborhood information;framelevel phonetic classification accuracy;large-vocabulary conversational speech recognition tasks","","5","42","","","","","IEEE","IEEE Journals"
"A Cross-Modality Learning Approach for Vessel Segmentation in Retinal Images","Q. Li; B. Feng; L. Xie; P. Liang; H. Zhang; T. Wang","Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Medical Imaging","","2016","35","1","109","118","This paper presents a new supervised method for vessel segmentation in retinal images. This method remolds the task of segmentation as a problem of cross-modality data transformation from retinal image to vessel map. A wide and deep neural network with strong induction ability is proposed to model the transformation, and an efficient training strategy is presented. Instead of a single label of the center pixel, the network can output the label map of all pixels for a given image patch. Our approach outperforms reported state-of-the-art methods in terms of sensitivity, specificity and accuracy. The result of cross-training evaluation indicates its robustness to the training set. The approach needs no artificially designed feature and no preprocessing step, reducing the impact of subjective factors. The proposed method has the potential for application in image diagnosis of ophthalmologic diseases, and it may provide a new, general, high-performance computing framework for image segmentation.","","","10.1109/TMI.2015.2457891","Project of the National Science Foundation of China; Shenzhen Science Plan of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7161344","Cross-modality learning;deep learning;retinal image;vessel segmentation","Image segmentation;Retina;Training;Neural networks;Accuracy;Feature extraction;Deformable models","eye;image segmentation;medical image processing;neural nets","cross-modality learning approach;vessel segmentation;retinal images;vessel map;neural network;induction ability;center pixel;cross-training evaluation;image diagnosis;ophthalmologic diseases;computing framework;image segmentation","Algorithms;Databases, Factual;Humans;Image Processing, Computer-Assisted;Machine Learning;Retinal Vessels","157","35","","","","","IEEE","IEEE Journals"
"Deep Aging Face Verification With Large Gaps","L. Liu; C. Xiong; H. Zhang; Z. Niu; M. Wang; S. Yan","National University of Singapore, Singapore; North Acton, London; National University of Singapore, Singapore; National University of Singapore, Singapore; School of Computer and Information, Hefei University of Technology, Hefei, China; National University of Singapore, Singapore","IEEE Transactions on Multimedia","","2016","18","1","64","75","Along with the long-time evolution of popular social networks, e.g. Facebook, social media analysis research inevitably arrived at the era of considering face/user recognition with large age gaps. However, related research with adequate subjects and large age gaps is surprisingly rare. In this work, we first collect a so-called cross-age face (CAFE) dataset, ranging from child, to young, to adult, to old groups. Then, we propose a novel framework, called deep aging face verification (DAFV), for this challenging task. DAFV includes two modules: aging pattern synthesis and aging face verification. The aging pattern synthesis module synthesizes the faces of all age groups for the input face of an arbitrary age, and the core structure is a deep aging-aware denoising auto-encoder ( a2-DAE) with multiple outputs. The aging face verification module then takes the synthesized aging patterns of a face pair as the input, and each pair of synthesized images of the same age group is fed into a parallel CNN; finally, all parallel CNN outputs are fused to provide similar/dissimilar prediction. For DAFV, the training of the aging face verification module easily suffers from the overfitting results from the aging pattern synthesis module, and we propose to use the cross- validation strategy to produce error-aware outputs for the synthesis module. Extensive experiments on the CAFE dataset well demonstrate the superiority of the proposed DAFV framework over other solutions for aging face verification.","","","10.1109/TMM.2015.2500730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328752","Cross-age;deep learning;face verification","Face;Aging;Face recognition;Training;Image reconstruction;Machine learning;Testing","face recognition;image denoising;neural nets","deep aging face verification;social networks;face recognition;user recognition;cross-age face dataset;CAFE dataset;DAFV;aging pattern synthesis module;deep aging-aware denoising auto-encoder;parallel CNN;cross-validation strategy","","26","38","","","","","IEEE","IEEE Journals"
"Deep Neural Networks for Identifying Cough Sounds","J. Amoh; K. Odame","Thayer School of Engineering, Dartmouth College, Hanover, NH, USA; Thayer School of Engineering, Dartmouth College, Hanover, NH, USA","IEEE Transactions on Biomedical Circuits and Systems","","2016","10","5","1003","1011","In this paper, we consider two different approaches of using deep neural networks for cough detection. The cough detection task is cast as a visual recognition problem and as a sequence-to-sequence labeling problem. A convolutional neural network and a recurrent neural network are implemented to address these problems, respectively. We evaluate the performance of the two networks and compare them to other conventional approaches for identifying cough sounds. In addition, we also explore the effect of the network size parameters and the impact of long-term signal dependencies in cough classifier performance. Experimental results show both network architectures outperform traditional methods. Between the two, our convolutional network yields a higher specificity 92.7% whereas the recurrent attains a higher sensitivity of 87.7%.","","","10.1109/TBCAS.2016.2598794","Neukom Institute for Computational Science at Dartmouth College; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7570164","Deep learning;machine learning;medical devices;wearables","Convolution;Hidden Markov models;Labeling;Biological neural networks;Visualization;Acoustics;Training","acoustic signal detection;learning (artificial intelligence);medical signal detection;neural nets;speech processing","deep neural networks;cough sound detection;visual recognition problem;sequence-to-sequence labeling problem;convolutional neural network;long-term signal dependencies;cough classifier performance","Algorithms;Auscultation;Cough;Diagnosis, Computer-Assisted;Humans;Neural Networks (Computer);Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Sound Spectrography","14","41","","","","","IEEE","IEEE Journals"
"A Pairwise Algorithm Using the Deep Stacking Network for Speech Separation and Pitch Estimation","X. Zhang; H. Zhang; S. Nie; G. Gao; W. Liu","Department of Computer Science, Inner Mongolia University, Hohhot, China; Department of Computer Science, Inner Mongolia University, Hohhot, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation, University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Inner Mongolia University, Hohhot, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation, University of Chinese Academy of Sciences, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","6","1066","1078","Speech separation and pitch estimation in noisy conditions are considered to be a “chicken-and-egg” problem. On one hand, pitch information is an important cue for speech separation. On the other hand, speech separation makes pitch estimation easier when background noise is removed. In this paper, we propose a supervised learning architecture to solve these two problems iteratively. The proposed algorithm is based on the deep stacking network (DSN), which provides a method for stacking simple processing modules to build deep architectures. Each module is a classifier whose target is the ideal binary mask (IBM), and the input vector includes spectral features, pitch-based features and the output from the previous module. During the testing stage, we estimate the pitch using the separation results and update the pitch-based features to the next module. When embedded into the DSN, pitch estimation and speech separation each run several times. We obtain the final results from the last module. Systematic evaluations show that the proposed system results in both a high quality estimated binary mask and accurate pitch estimation and outperforms recent systems in its generalization ability.","","","10.1109/TASLP.2016.2540805","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7430309","Speech separation;Pitch estimation;Computational auditory scene analysis;Supervised learning;Speech separation;Pitch estimation;Computational auditory scene analysis;Supervised learning","Speech;Estimation;Noise measurement;Speech processing;Stacking;Training;Feature extraction","learning (artificial intelligence);speech processing","pairwise algorithm;deep stacking network;speech separation;pitch estimation;chicken-and-egg problem;pitch information;supervised learning architecture;DSN;ideal binary mask;IBM;input vector","","12","42","","","","","IEEE","IEEE Journals"
"Introduction: Special issue: Deep learning","F. Bach; T. Poggio","NA; NA","Information and Inference: A Journal of the IMA","","2016","5","2","103","104","","","","10.1093/imaiai/iaw010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8132290","","","","","","","","","","","","OUP","OUP Journals"
"A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data","Y. Zheng; Y. Zhang; H. Larochelle","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Départment d'informatique, Université de Sherbrooke, Sherbrooke, QC, Canada","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","6","1056","1069","Topic modeling based on latent Dirichlet allocation (LDA) has been a framework of choice to deal with multimodal data, such as in image annotation tasks. Another popular approach to model the multimodal data is through deep neural networks, such as the deep Boltzmann machine (DBM). Recently, a new type of topic model called the Document Neural Autoregressive Distribution Estimator (DocNADE) was proposed and demonstrated state-of-the-art performance for text document modeling. In this work, we show how to successfully apply and extend this model to multimodal data, such as simultaneous image classification and annotation. First, we propose SupDocNADE, a supervised extension of DocNADE, that increases the discriminative power of the learned hidden topic features and show how to employ it to learn a joint representation from image visual words, annotation words and class label information. We test our model on the LabelMe and UIUC-Sports data sets and show that it compares favorably to other topic models. Second, we propose a deep extension of our model and provide an efficient way of training the deep model. Experimental results show that our deep model outperforms its shallow version and reaches state-of-the-art performance on the Multimedia Information Retrieval (MIR) Flickr data set.","","","10.1109/TPAMI.2015.2476802","Natural Sciences and Engineering Research Council of Canada and Compute Canada; National Nature Science Foundation of China; Specialized Research Fund for the Doctoral Program of Higher Education in China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7258387","Multimodal data modeling;Topic model;Neural autoregressive model;Deep neural network;Multimodal data modeling;topic model;neural autoregressive model;deep neural network","Visualization;Computational modeling;Data models;Mathematical model;Training;Joints;Neural networks","autoregressive processes;Boltzmann machines;document image processing;image representation;information retrieval;multimedia systems;text analysis","topic modeling;multimodal data;latent Dirichlet allocation;LDA;image annotation tasks;deep neural networks;deep Boltzmann machine;DBM;document neural autoregressive distribution estimator;DocNADE;text document modeling;joint representation;image visual words;annotation words;class label information;LabelMe;UIUC-Sports data sets;multimedia information retrieval;MIR;Flickr data set","","20","35","","","","","IEEE","IEEE Journals"
"Deep Learning for Surface Material Classification Using Haptic and Visual Information","H. Zheng; L. Fang; M. Ji; M. Strese; Y. Özer; E. Steinbach","Hong Kong University of Science and Technology, Hong KongChina; Hong Kong University of Science and Technology, Hong KongChina; Hong Kong University of Science and Technology, Hong KongChina; Technische Universität München, München, Germany; Technische Universität München, München, Germany; Technische Universität München, München, Germany","IEEE Transactions on Multimedia","","2016","18","12","2407","2416","When a user scratches a hand-held rigid tool across an object surface, an acceleration signal can be captured, which carries relevant information about the surface material properties. More importantly, such haptic acceleration signals can be used together with surface images to jointly recognize the surface material. In this paper, we present a novel deep learning method dealing with the surface material classification problem based on a fully convolutional network, which takes the aforementioned acceleration signal and a corresponding image of the surface texture as inputs. Compared to the existing surface material classification solutions which rely on a careful design of hand-crafted features, our method automatically extracts discriminative features utilizing advanced deep learning methodologies. Experiments performed on the TUM surface material database demonstrate that our method achieves state-of-the-art classification accuracy robustly and efficiently.","","","10.1109/TMM.2016.2598140","Alexander von Humboldt Foundation; Natural Science Foundation of China; GRF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530831","Convolutional neural network;haptic signal;hybrid inputs;surface material classification","Haptic interfaces;Convolution;Acceleration;Speech recognition;Image recognition;Neural networks;Feature extraction","haptic interfaces;image classification;learning (artificial intelligence);object detection","deep learning;surface material classification;haptic information;visual information;user scratch;hand-held rigid tool;object surface;surface images;convolutional network","","22","46","","","","","IEEE","IEEE Journals"
"Action Recognition From Depth Maps Using Deep Convolutional Neural Networks","P. Wang; W. Li; Z. Gao; J. Zhang; C. Tang; P. O. Ogunbona","Advanced Multimedia Research Laboratory, University of Wollongong, Wollongong, N.S.W., Australia; Advanced Multimedia Research Laboratory, University of Wollongong, Wollongong, N.S.W., Australia; Advanced Multimedia Research Laboratory, University of Wollongong, Wollongong, N.S.W., Australia; Advanced Multimedia Research Laboratory, University of Wollongong, Wollongong, N.S.W., Australia; School of Electronic Information Engineering, Tianjin University, Tianjin, China; Advanced Multimedia Research Laboratory, University of Wollongong, Wollongong, N.S.W., Australia","IEEE Transactions on Human-Machine Systems","","2016","46","4","498","509","This paper proposes a new method, i.e., weighted hierarchical depth motion maps (WHDMM) + three-channel deep convolutional neural networks (3ConvNets), for human action recognition from depth maps on small training datasets. Three strategies are developed to leverage the capability of ConvNets in mining discriminative features for recognition. First, different viewpoints are mimicked by rotating the 3-D points of the captured depth maps. This not only synthesizes more data, but also makes the trained ConvNets view-tolerant. Second, WHDMMs at several temporal scales are constructed to encode the spatiotemporal motion patterns of actions into 2-D spatial structures. The 2-D spatial structures are further enhanced for recognition by converting the WHDMMs into pseudocolor images. Finally, the three ConvNets are initialized with the models obtained from ImageNet and fine-tuned independently on the color-coded WHDMMs constructed in three orthogonal planes. The proposed algorithm was evaluated on the MSRAction3D, MSRAction3DExt, UTKinect-Action, and MSRDailyActivity3D datasets using cross-subject protocols. In addition, the method was evaluated on the large dataset constructed from the above datasets. The proposed method achieved 2-9% better results on most of the individual datasets. Furthermore, the proposed method maintained its performance on the large dataset, whereas the performance of existing methods decreased with the increased number of actions.","","","10.1109/THMS.2015.2504550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358110","Action recognition;deep learning;depth maps;pseudocolor coding","Feature extraction;Cameras;Yttrium;Training;Spatiotemporal phenomena;Image recognition;Neural networks","data mining;feature extraction;image coding;image colour analysis;image recognition;learning (artificial intelligence);motion estimation;neural nets;spatiotemporal phenomena","deep convolutional neural networks;weighted hierarchical depth motion maps;three-channel deep convolutional neural networks;3ConvNets;human action recognition;discriminative feature mining;temporal scales;spatiotemporal motion pattern encoding;2D spatial structures;pseudocolor images;ImageNet;color-coded WHDMM;orthogonal planes;MSRAction3D dataset;MSRAction3DExt dataset;UTKinect-Action dataset;MSRDailyActivity3D dataset;cross-subject protocols;3D points","","111","30","","","","","IEEE","IEEE Journals"
"Object Classification and Grasp Planning Using Visual and Tactile Sensing","F. Sun; C. Liu; W. Huang; J. Zhang","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of InformaticsTechnical Aspects of Multimodal Systems, University of Hamburg, Hamburg, Germany","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2016","46","7","969","979","The perception of the tactile and vision modalities plays a crucial role in object classification and robot precise operations. On one hand, the visual perception helps acquire the objects apparent characteristics (e.g., shape and color) that are paramount for grasp planning. On the other hand, when grasping the object, the tactile sensors can detect the softness or stiffness and the surface texture of the object, thus can be used to classify the objects. In this paper, two different tactile models are first developed to model the tactile sequences, namely bag-of-system (BoS) and deep dynamical system (DDS). To be specific, BoS applies the extreme learning machine method as the classifier, and DDS employs deep neural networks as the feature mapping functions in order to deal with the highly nonlinear data. As for visual sensing, we formulate a signature of histograms of orientations descriptor to model the shape of objects. Moreover, by using human experience, we identify the graspable components of objects on the category level instead of the object level. Finally, a fast planning method considering both contact points exaction and hand kinematics is proposed to accomplish the grasping manipulation. Comparative experiments demonstrate the effectiveness of our proposed methods on the tasks including the robot grasping and the object classification.","","","10.1109/TSMC.2016.2524059","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448968","Classification;experience learning;extreme learning machine (ELM);grasp planning;tactile model;vision model;Classification;experience learning;extreme learning machine (ELM);grasp planning;tactile model;vision model","Feature extraction;Planning;Shape;Grasping;Visualization;Robot sensing systems","image classification;learning (artificial intelligence);manipulator kinematics;neural nets;pattern classification;robot vision;sensors;surface texture","object classification;robot grasp planning;visual sensing;tactile sensing;tactile modalities;vision modalities;visual perception;objects apparent characteristics;tactile sensors;object surface texture;tactile sequences;bag-of-system;BoS;deep dynamical system;DDS;learning machine method;deep neural networks;feature mapping functions;nonlinear data;visual sensing;histogram signature;orientation descriptor;hand kinematics;grasping manipulation","","18","62","","","","","IEEE","IEEE Journals"
"Learning Contextual Dependence With Convolutional Hierarchical Recurrent Neural Networks","Z. Zuo; B. Shuai; G. Wang; X. Liu; X. Wang; B. Wang; Y. Chen","Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Harbin Institute of Technology, China","IEEE Transactions on Image Processing","","2016","25","7","2983","2996","Deep convolutional neural networks (CNNs) have shown their great success on image classification. CNNs mainly consist of convolutional and pooling layers, both of which are performed on local image areas without considering the dependence among different image regions. However, such dependence is very important for generating explicit image representation. In contrast, recurrent neural networks (RNNs) are well known for their ability of encoding contextual information in sequential data, and they only require a limited number of network parameters. Thus, we proposed the hierarchical RNNs (HRNNs) to encode the contextual dependence in image representation. In HRNNs, each RNN layer focuses on modeling spatial dependence among image regions from the same scale but different locations. While the cross RNN scale connections target on modeling scale dependencies among regions from the same location but different scales. Specifically, we propose two RNN models: 1) hierarchical simple recurrent network (HSRN), which is fast and has low computational cost and 2) hierarchical long-short term memory recurrent network, which performs better than HSRN with the price of higher computational cost. In this paper, we integrate CNNs with HRNNs, and develop end-to-end convolutional hierarchical RNNs (C-HRNNs) for image classification. C-HRNNs not only utilize the discriminative representation power of CNNs, but also utilize the contextual dependence learning ability of our HRNNs. On four of the most challenging object/scene image classification benchmarks, our C-HRNNs achieve the state-of-the-art results on Places 205, SUN 397, and MIT indoor, and the competitive results on ILSVRC 2012.","","","10.1109/TIP.2016.2548241","National Research Foundation, Prime Ministers Office, Singapore, through the Rapid-Rich Object Search Laboratory under its Interactive and Digital Media (IDM) Futures Funding Initiative and administered by the IDM Programme Office; Singapore Ministry of Education within the Tier 2 Research Project; Singapore Agency for Science, Technology and Research within the Science and Engineering Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442840","Deep Learning;Image Classification;Recurrent Neural Networks;Convolutional Neural Networks;Deep learning;image classification;recurrent neural networks;convolutional neural networks","Recurrent neural networks;Context modeling;Natural language processing;Logic gates;Image representation;Computer vision","computational complexity;image classification;image representation;recurrent neural nets","contextual dependence learning;convolutional hierarchical recurrent neural networks;image representation;image regions spatial dependence;hierarchical simple recurrent network;computational cost;hierarchical long-short term memory recurrent network;C-HRNN;object image classification;scene image classification","","30","72","","","","","IEEE","IEEE Journals"
"Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval","H. Palangi; L. Deng; Y. Shen; J. Gao; X. He; J. Chen; X. Song; R. Ward","Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","4","694","707","This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks (RNN) with Long Short-Term Memory (LSTM) cells. The proposed LSTM-RNN model sequentially takes each word in a sentence, extracts its information, and embeds it into a semantic vector. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detect the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms Paragraph Vector method for web document retrieval task.","","","10.1109/TASLP.2016.2520371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7389336","Deep Learning;Long Short-Term Memory;Sentence Embedding;Deep learning;long short-term memory;sentence embedding","Semantics;Recurrent neural networks;Web search;Data visualization;IEEE transactions;Speech;Speech processing","document handling;information retrieval;natural language processing;recurrent neural nets;search engines","deep sentence embedding;long short-term memory networks;information retrieval;natural language processing research;recurrent neural networks;LSTM-RNN model;semantic representation;weakly supervised manner;commercial Web search engine;automatic keyword detection;Web document retrieval tasks","","151","47","","","","","IEEE","IEEE Journals"
"Convolutional Fusion Network for Face Verification in the Wild","C. Xiong; L. Liu; X. Zhao; S. Yan; T. Kim","Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.","IEEE Transactions on Circuits and Systems for Video Technology","","2016","26","3","517","528","Part-based methods have seen popular applications for face verification in the wild, since they are more robust to local variations in terms of pose, illumination, and so on. However, most of the part-based approaches are built on hand-crafted features, which may not be suitable for the specific face verification purpose. In this paper, we propose to learn a part-based feature representation under the supervision of face identities through a deep model that ensures that the generated representations are more robust and suitable for face verification. The proposed framework consists of the following two deliberate components: 1) a deep mixture model (DMM) to find accurate patch correspondence and 2) a convolutional fusion network (CFN) to extract the part-based facial features. Specifically, DMM robustly depicts the spatial-appearance distribution of patch features over the faces via several Gaussian mixtures, which provide more accurate patch correspondence even in the presence of local distortions. Then, DMM only feeds the patches which preserve the identity information to the following CFN. The proposed CFN is a two-layer cascade of convolutional neural networks: 1) a local layer built on face patches to deal with local variations and 2) a fusion layer integrating the responses from the local layer. CFN jointly learns and fuses multiple local responses to optimize the verification performance. The composite representation obtained possesses certain robustness to pose and illumination variations and shows comparable performance with the state-of-the-art methods on two benchmark data sets.","","","10.1109/TCSVT.2015.2406191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7047804","Deep Learning;Part-based Representation;Feature Learning;Deep learning;face verification;feature learning;mixture model;part-based representation","Face;Feature extraction;Robustness;Lighting;Neural networks;Accuracy;Detectors","face recognition;mixture models;neural nets","convolutional fusion network;face verification;part-based feature representation;deep mixture model;DMM;spatial-appearance distribution;Gaussian mixtures;convolutional neural networks","","16","45","","","","","IEEE","IEEE Journals"
"Deep Feature Extraction and Classification of Hyperspectral Images Based on Convolutional Neural Networks","Y. Chen; H. Jiang; C. Li; X. Jia; P. Ghamisi","Department of Information Engineering, School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Engineering and Information Technology, The University of New South Wales, Canberra, A.C.T., Australia; Signal Processing in Earth Observation, Remote Sensing Technology Institute (IMF), Technische Universität München, German Aerospace Center (DLR), Munich, Weßling, GermanyGermany","IEEE Transactions on Geoscience and Remote Sensing","","2016","54","10","6232","6251","Due to the advantages of deep learning, in this paper, a regularized deep feature extraction (FE) method is presented for hyperspectral image (HSI) classification using a convolutional neural network (CNN). The proposed approach employs several convolutional and pooling layers to extract deep features from HSIs, which are nonlinear, discriminant, and invariant. These features are useful for image classification and target detection. Furthermore, in order to address the common issue of imbalance between high dimensionality and limited availability of training samples for the classification of HSI, a few strategies such as L2 regularization and dropout are investigated to avoid overfitting in class data modeling. More importantly, we propose a 3-D CNN-based FE model with combined regularization to extract effective spectral-spatial features of hyperspectral imagery. Finally, in order to further improve the performance, a virtual sample enhanced method is proposed. The proposed approaches are carried out on three widely used hyperspectral data sets: Indian Pines, University of Pavia, and Kennedy Space Center. The obtained results reveal that the proposed models with sparse constraints provide competitive results to state-of-the-art methods. In addition, the proposed deep FE opens a new window for further research.","","","10.1109/TGRS.2016.2584107","Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7514991","Convolutional neural network (CNN);deep learning;feature extraction (FE);hyperspectral image (HSI) classification","Feature extraction;Iron;Hyperspectral imaging;Training;Machine learning;Data mining","feature extraction;geophysical techniques;hyperspectral imaging;neural nets","hyperspectral images;convolutional neural networks;regularized deep feature extraction;class data modeling;CNN-based FE model;Indian Pines;University of Pavia;Kennedy Space Center;pooling layers;state-of-the-art methods;class data modeling","","488","51","","","","","IEEE","IEEE Journals"
"Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation","H. R. Roth; L. Lu; J. Liu; J. Yao; A. Seff; K. Cherry; L. Kim; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA","IEEE Transactions on Medical Imaging","","2016","35","5","1170","1181","Automated computer-aided detection (CADe) has been an important tool in clinical practice and research. State-of-the-art methods often show high sensitivities at the cost of high false-positives (FP) per patient rates. We design a two-tiered coarse-to-fine cascade framework that first operates a candidate generation system at sensitivities ~ 100% of but at high FP levels. By leveraging existing CADe systems, coordinates of regions or volumes of interest (ROI or VOI) are generated and function as input for a second tier, which is our focus in this study. In this second stage, we generate 2D (two-dimensional) or 2.5D views via sampling through scale transformations, random translations and rotations. These random views are used to train deep convolutional neural network (ConvNet) classifiers. In testing, the ConvNets assign class (e.g., lesion, pathology) probabilities for a new set of random views that are then averaged to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. The methods are evaluated on three data sets: 59 patients for sclerotic metastasis detection, 176 patients for lymph node detection, and 1,186 patients for colonic polyp detection. Experimental results show the ability of ConvNets to generalize well to different medical imaging CADe applications and scale elegantly to various data sets. Our proposed methods improve performance markedly in all cases. Sensitivities improved from 57% to 70%, 43% to 77%, and 58% to 75% at 3 FPs per patient for sclerotic metastases, lymph nodes and colonic polyps, respectively.","","","10.1109/TMI.2015.2482920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279156","Computer aided diagnosis;computed tomography;medical diagnostic imaging;machine learning;object detection;artificial neural networks;multi-layer neural network;deep learning","Computed tomography;Lymph nodes;Training;Colonic polyps;Feature extraction;Three-dimensional displays","computerised tomography;image classification;learning (artificial intelligence);medical image processing;neural nets;probability","computed tomography;medical imaging;colonic polyp detection;lymph node detection;sclerotic metastasis detection;false positives;classification probability;deep convolutional neural network classifier training;random rotations;random translations;scale transformations;two-tiered coarse-to-fine cascade framework;random view aggregation;computer-aided detection","Adolescent;Adult;Aged;Child;Colonic Polyps;Databases, Factual;Female;Humans;Lymph Nodes;Machine Learning;Male;Middle Aged;Neural Networks (Computer);Radiographic Image Interpretation, Computer-Assisted;Spinal Neoplasms;Tomography, X-Ray Computed;Young Adult","159","60","","","","","IEEE","IEEE Journals"
"Video Super-Resolution With Convolutional Neural Networks","A. Kappeler; S. Yoo; Q. Dai; A. K. Katsaggelos","Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL, USA; Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL, USA; Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL, USA; Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL, USA","IEEE Transactions on Computational Imaging","","2016","2","2","109","122","Convolutional neural networks (CNN) are a special type of deep neural networks (DNN). They have so far been successfully applied to image super-resolution (SR) as well as other image restoration tasks. In this paper, we consider the problem of video super-resolution. We propose a CNN that is trained on both the spatial and the temporal dimensions of videos to enhance their spatial resolution. Consecutive frames are motion compensated and used as input to a CNN that provides super-resolved video frames as output. We investigate different options of combining the video frames within one CNN architecture. While large image databases are available to train deep neural networks, it is more challenging to create a large video database of sufficient quality to train neural nets for video restoration. We show that by using images to pretrain our model, a relatively small video database is sufficient for the training of our model to achieve and even improve upon the current state-of-the-art. We compare our proposed approach to current video as well as image SR algorithms.","","","10.1109/TCI.2016.2532323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444187","Deep Learning;Deep Neural Networks;Convolutional Neural Networks;Video Super-Resolution;Deep Learning;Deep Neural Networks;Convolutional Neural Networks;Video Super-Resolution","Dictionaries;Image reconstruction;Training;Neural networks;Spatial resolution;Computer architecture","image resolution;image restoration;motion compensation;neural nets;video databases;video signal processing","video super-resolution;deep neural networks;DNN;image super-resolution;image SR;image restoration;spatial video dimensions;temporal video dimensions;motion compensation;super-resolved video frames;CNN architecture;image databases;video database;video restoration;convolutional neural networks","","120","43","","","","","IEEE","IEEE Journals"
"Fusion Methods for Speech Enhancement and Audio Source Separation","X. Jaureguiberry; E. Vincent; G. Richard","Zenly, Paris, France; INRIA, Villers-lès-Nancy, France; Télécom ParisTech, Paris, France","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","7","1266","1279","A wide variety of audio source separation techniques exist and can already tackle many challenging industrial issues. However, in contrast with other application domains, fusion principles were rarely investigated in audio source separation despite their demonstrated potential in classification tasks. In this paper, we propose a general fusion framework which takes advantage of the diversity of existing separation techniques in order to improve separation quality. We obtain new source estimates by summing the individual estimates given by different separation techniques weighted by a set of fusion coefficients. We investigate three alternative fusion methods which are based on standard nonlinear optimization, Bayesian model averaging, or deep neural networks. Experiments conducted for both speech enhancement and singing voice extraction demonstrate that all the proposed methods outperform traditional model selection. The use of deep neural networks for the estimation of time-varying coefficients notably leads to large quality improvements, up to 3 dB in terms of signal-to-distortion ratio compared to model selection.","","","10.1109/TASLP.2016.2553441","Research Program EDiSon3D; ANR; French State agency for research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451223","audio source separation;fusion;aggregation;ensemble;deep neural networks;deep learning;variational Bayes, model averaging;non-negative matrix factorization;speech enhancement;singing voice extraction;Audio source separation;fusion;aggregation;ensemble;deep neural networks (DNNs);deep learning;variational Bayes;model averaging;non-negative matrix factorization (NMF);speech enhancement;singing voice extraction","Source separation;Speech enhancement;Neural networks;Estimation;Training;Time-frequency analysis;Speech","Bayes methods;estimation theory;neural nets;source separation;speech enhancement","fusion method;speech enhancement;audio source separation technique;separation quality;source estimation;fusion coefficient;standard nonlinear optimization;Bayesian model averaging;deep neural network;singing voice extraction;time-varying coefficient;signal-to-distortion ratio","","11","57","","","","","IEEE","IEEE Journals"
"Hyperspectral Image Classification Based on Nonlinear Spectral–Spatial Network","B. Pan; Z. Shi; N. Zhang; S. Xie","Image Processing Center, State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics and the Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China; Image Processing Center, State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics and the Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China; Shanghai Aerospace Electronic Technology Institute, Shanghai, China; Shanghai Academy of Spaceflight Technology, Shanghai, China","IEEE Geoscience and Remote Sensing Letters","","2016","13","12","1782","1786","Recently, for the task of hyperspectral image classification, deep-learning-based methods have revealed promising performance. However, the complex network structure and the time-consuming training process have restricted their applications. In this letter, we construct a much simpler network, i.e., the nonlinear spectral-spatial network (NSSNet), for hyperspectral image classification. NSSNet is developed from the basic structure of a principal component analysis network. Nonlinear information is included in NSSNet, to generate a more discriminative feature expression. Moreover, spectral and spatial features are combined to further improve the classification accuracy. Experimental results indicate that our method achieves better performance than state-of-the-art deep-learning-based methods.","","","10.1109/LGRS.2016.2608963","National Natural Science Foundation of China; Beijing Natural Science Foundation; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University; Shanghai Association for Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7580567","Deep learning;hyperspectral image classification;nonlinear spectral–spatial network (NSSNet)","Hyperspectral imaging;Feature extraction;Kernel;Principal component analysis;Imaging;Machine learning","hyperspectral imaging;image classification;learning (artificial intelligence);remote sensing","hyperspectral image classification;nonlinear spectral-spatial network;deep-learning-based method;complex network structure;time-consuming training process;simpler network;principal component analysis;PCA network;classification accuracy","","27","16","","","","","IEEE","IEEE Journals"
"Leakage Prototype Learning for Profiled Differential Side-Channel Cryptanalysis","T. Bartkewitz","Horst Görtz Institute for IT-Security, Ruhr University Bochum, Bochum, Germany","IEEE Transactions on Computers","","2016","65","6","1761","1774","Profiling in side-channel cryptanalysis is a powerful tool to assess the resistance of embedded cryptographic implementations and therefore embodies a powerful attack. The adversary has to have access to a second device, similar to the device under test, that he fully controls to conduct a profiling of physically observable information about secret internals. By knowledge of implementation details the adversary is able to nearly defeat any countermeasure, at least those of algorithmic nature. To date there exist different branches, including template attacks, stochastic model, and machine learning based approaches. In this work we propose a new attack called leakage prototype learning that aims for determining unbiased side-channel leakages instead of estimating them. Furthermore, it encompasses the locating, respectively selection, of leakage dependent time-instants with clear criteria. For one thing we provide a deep theoretical analysis by discussing mathematical foundations and properties, and for another thing a thorough analysis by practical means including performance comparisons to meaningful variants of several common profiled side-channel attacks.","","","10.1109/TC.2015.2455974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7156072","Side-channel Analysis;Side-channel Leakage Profiling;Template Attacks;Leakage Prototype Learning;Side-channel analysis;side-channel leakage profiling;template attacks;leakage prototype learning","Prototypes;Principal component analysis;Cryptography;Correlation;Random variables;Stochastic processes;Noise","cryptography;learning (artificial intelligence);stochastic processes","side-channel leakage profiling;machine learning;stochastic model;template attack;profiled differential side-channel cryptanalysis;leakage prototype learning","","3","19","","","","","IEEE","IEEE Journals"
"Learning to Deblur","C. J. Schuler; M. Hirsch; S. Harmeling; B. Schölkopf","MPI for Intelligent Systems, Spemannstr. 38, Tuebingen, Germany; MPI for Intelligent Systems, Spemannstr. 38, Tuebingen, Germany; MPI for Intelligent Systems, Spemannstr. 38, Tuebingen, Germany; MPI for Intelligent Systems, Spemannstr. 38, Tuebingen, Germany","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","7","1439","1451","We describe a learning-based approach to blind image deconvolution. It uses a deep layered architecture, parts of which are borrowed from recent work on neural network learning, and parts of which incorporate computations that are specific to image deconvolution. The system is trained end-to-end on a set of artificially generated training examples, enabling competitive performance in blind deconvolution, both with respect to quality and runtime.","","","10.1109/TPAMI.2015.2481418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7274732","Sharpening and deblurring;neural networks;machine learning;Sharpening and deblurring;neural networks;machine learning","Kernel;Estimation;Feature extraction;Artificial neural networks;Deconvolution;Training;Convolution","deconvolution;image restoration;learning (artificial intelligence);neural nets","learning-based approach;blind image deconvolution;neural network learning","","93","36","","","","","IEEE","IEEE Journals"
"Detecting Facial Retouching Using Supervised Deep Learning","A. Bharati; R. Singh; M. Vatsa; K. W. Bowyer","Indraprastha Institute of Information Technology, Delhi, India; Indraprastha Institute of Information Technology, Delhi, India; Indraprastha Institute of Information Technology, Delhi, India; University of Notre Dame, Notre Dame, IN, USA","IEEE Transactions on Information Forensics and Security","","2016","11","9","1903","1913","Digitally altering, or retouching, face images is a common practice for images on social media, photo sharing websites, and even identification cards when the standards are not strictly enforced. This research demonstrates the effect of digital alterations on the performance of automatic face recognition, and also introduces an algorithm to classify face images as original or retouched with high accuracy. We first introduce two face image databases with unaltered and retouched images. Face recognition experiments performed on these databases show that when a retouched image is matched with its original image or an unaltered gallery image, the identification performance is considerably degraded, with a drop in matching accuracy of up to 25%. However, when images are retouched with the same style, the matching accuracy can be misleadingly high in comparison with matching original images. To detect retouching in face images, a novel supervised deep Boltzmann machine algorithm is proposed. It uses facial parts to learn discriminative features to classify face images as original or retouched. The proposed approach for classifying images as original or retouched yields an accuracy of over 87% on the data sets introduced in this paper and over 99% on three other makeup data sets used by previous researchers. This is a substantial increase in accuracy over the previous state-of-the-art algorithm, which has shown <;50% accuracy in classifying original and retouched images from the ND-IIITD retouched faces database.","","","10.1109/TIFS.2016.2561898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7464282",";Image forensics;face recognition;face image retouching;face image alteration;biometric spoofing","Face;Databases;Face recognition;Probes;Classification algorithms;Media;Skin","Boltzmann machines;face recognition;image matching;learning (artificial intelligence);visual databases;Web sites","detecting facial retouching;supervised deep learning;face images;social media;photo sharing Web sites;identification cards;digital alterations;automatic face recognition;image databases;unaltered gallery image;image matching;Boltzmann machine algorithm","","21","19","","","","","IEEE","IEEE Journals"
"Deep Filter Banks for Land-Use Scene Classification","H. Wu; B. Liu; W. Su; W. Zhang; J. Sun","Institute of Medical Equipment, Academy of Military Medical Science, Tianjin, China; Institute of Medical Equipment, Academy of Military Medical Science, Tianjin, China; Institute of Medical Equipment, Academy of Military Medical Science, Tianjin, China; State Key Laboratory of Intelligent Technology and System, Computer Science and Technology School, Tsinghua University, Beijing, China; Institute of Medical Equipment, Academy of Military Medical Science, Tianjin, China","IEEE Geoscience and Remote Sensing Letters","","2016","13","12","1895","1899","Land-use (LU) scene classification is one of the most challenging tasks in the field of remote sensing (RS) image processing due to its high intraclass variability and low interclass distance. Motivated by the challenge posed by this problem, we propose a novel hybrid architecture, deep filter banks, combining multicolumn stacked denoising sparse autoencoder (SDSAE) and Fisher vector (FV) to automatically learn the representative and discriminative features in a hierarchical manner for LU scene classification. SDSAE kernels describe local patches and a robust global feature of the RS image is built through the FV pooling layer. Unlike previous handcrafted features, we use machine-learning mechanisms to optimize our proposed feature extractor so that it can learn more suitable internal features from the RS data, boosting the final performance. Our approach achieves superior performance compared with the state-of-the-art methods, obtaining average classification accuracies of 92.7% and 90.4%, respectively, on the UC Merced and RSSCN7 data sets.","","","10.1109/LGRS.2016.2616440","Science and Technology Pillar Program, Tianjin, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7676333","Deep filter banks;Fisher vector (FV);land-use (LU) scene classification;stacked denoising sparse autoencoder (SDSAE)","Feature extraction;Noise reduction;Robustness;Kernel;Semantics;Data models;Encoding","feature extraction;geophysical image processing;image classification;image denoising;image filtering;land use;learning (artificial intelligence);remote sensing","image classification accuracy;feature extractor;machine-learning mechanisms;Fisher vector;stacked denoising sparse autoencoder;multicolumn SDSAE kernel;image processing;remote sensing image;land-use scene classification;deep filter bank","","15","23","","","","","IEEE","IEEE Journals"
"A Regression Approach to Single-Channel Speech Separation Via High-Resolution Deep Neural Networks","J. Du; Y. Tu; L. Dai; C. Lee","National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","8","1424","1437","We propose a novel data-driven approach to single-channel speech separation based on deep neural networks (DNNs) to directly model the highly nonlinear relationship between speech features of a mixed signal containing a target speaker and other interfering speakers. We focus our discussion on a semisupervised mode to separate speech of the target speaker from an unknown interfering speaker, which is more flexible than the conventional supervised mode with known information of both the target and interfering speakers. Two key issues are investigated. First, we propose a DNN architecture with dual outputs of the features of both the target and interfering speakers, which is shown to achieve a better generalization capability than that with output features of only the target speaker. Second, we propose using a set of multiple DNNs, each intending to be signal-noise-dependent (SND), to cope with the difficulty that one single general DNN could not well accommodate all the speaker mixing variabilities at different signal-to-noise ratio (SNR) levels. Experimental results on the speech separation challenge (SSC) data demonstrate that our proposed framework achieves better separation results than other conventional approaches in a supervised or semisupervised mode. SND-DNNs could also yield significant performance improvements over a general DNN for speech separation in low SNR cases. Furthermore, for automatic speech recognition (ASR) following speech separation, this purely front-end processing with a single set of speaker-independent ASR acoustic models, achieves a relative word error rate (WER) reduction of 11.6% over a state-of-the-art separation and recognition system where a complicated joint back-end decoding framework with multiple sets of speaker-dependent ASR acoustic models needs to be implemented. When speaker-adaptive ASR acoustic models for the target speakers are adopted for the enhanced signals, another 12.1% WER reduction over our best speaker-independent ASR system is achieved.","","","10.1109/TASLP.2016.2558822","Strategic Priority Research Program of the Chinese Academy of Sciences; National Key Technology Support Program; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7460094","Deep neural network;dual outputs, divide and conquer;speech separation;robust speech recognition;Deep neural network;divide and conquer;dual outputs;robust speech recognition;speech separation","Speech;Speech recognition;Hidden Markov models;Speech processing;Training;Signal to noise ratio;Acoustics","learning (artificial intelligence);neural nets;regression analysis;source separation;speaker recognition","joint back-end decoding framework;WER reduction;relative word error rate reduction;speaker-independent ASR acoustic models;front-end processing;automatic speech recognition;SSC data;speech separation challenge data;signal-to-noise ratio levels;SND;signal-noise-dependent;DNN architecture;supervised mode;semisupervised mode;data-driven approach;high-resolution deep neural networks;single-channel speech separation;regression approach","","33","66","","","","","IEEE","IEEE Journals"
"On invariance and selectivity in representation learning","F. Anselmi; L. Rosasco; T. Poggio","NA; NA; NA","Information and Inference: A Journal of the IMA","","2016","5","2","134","158","We study the problem of learning from data representations that are invariant to transformations, and at the same time selective, in the sense that two points have the same representation if one is the transformation of the other. The mathematical results here sharpen some of the key claims of i-theory—a recent theory of feedforward processing in sensory cortex (Anselmi et al., 2013, Theor. Comput. Sci. and arXiv:1311.4158; Anselmi et al., 2013, Magic materials: a theory of deep hierarchical architectures for learning sensory representations. CBCL Paper; Anselmi & Poggio, 2010, Representation learning in sensory cortex: a theory. CBMM Memo No. 26).","","","10.1093/imaiai/iaw009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8132289","invariance;machine learning","","","","","4","","","","","","OUP","OUP Journals"
"Blind Image Blur Estimation via Deep Learning","R. Yan; L. Shao","College of Electronic and Information Engineering, Nanjing University of Information Science and Technology, Nanjing, China; College of Electronic and Information Engineering, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Transactions on Image Processing","","2016","25","4","1910","1921","Image blur kernel estimation is critical to blind image deblurring. Most existing approaches exploit handcrafted blur features that are optimized for a certain uniform blur across the image, which is unrealistic in a real blind deconvolution setting, where the blur type is often unknown. To deal with this issue, we aim at identifying the blur type for each input image patch, and then estimating the kernel parameter in this paper. A learning-based method using a pre-trained deep neural network (DNN) and a general regression neural network (GRNN) is proposed to first classify the blur type and then estimate its parameters, taking advantages of both the classification ability of DNN and the regression ability of GRNN. To the best of our knowledge, this is the first time that pre-trained DNN and GRNN have been applied to the problem of blur analysis. First, our method identifies the blur type from a mixed input of image patches corrupted by various blurs with different parameters. To this aim, a supervised DNN is trained to project the input samples into a discriminative feature space, in which the blur type can be easily classified. Then, for each blur type, the proposed GRNN estimates the blur parameters with very high accuracy. Experiments demonstrate the effectiveness of the proposed method in several tasks with better or competitive results compared with the state of the art on two standard image data sets, i.e., the Berkeley segmentation data set and the Pascal VOC 2007 data set. In addition, blur region segmentation and deblurring on a number of real photographs show that our method outperforms the previous techniques even for non-uniformly blurred images.","","","10.1109/TIP.2016.2535273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420686","blur classification;blur parameter estimation;blind image deblurring;Blur classification;blur parameter estimation;blind image deblurring;general regression neural network","Neural networks;Kernel;Feature extraction;Estimation;Training;Parameter estimation;Image edge detection","deconvolution;image restoration;image segmentation;neural nets;parameter estimation;regression analysis","blind image blur estimation;deep learning;image blur kernel estimation;blind deconvolution setting;pretrained deep neural network;DNN;general regression neural network;GRNN;parameter estimation;blur analysis;image patch;discriminative feature space;Berkeley segmentation data set;Pascal VOC 2007 data set;photograph deblurring","","26","46","","","","","IEEE","IEEE Journals"
"Partial Copy Detection in Videos: A Benchmark and an Evaluation of Popular Methods","Y. Jiang; J. Wang","School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China","IEEE Transactions on Big Data","","2016","2","1","32","42","The goal of partial video copy detection is to find one or more segments of a query video which have (transformed) copies in a large dataset. Previous related research in this field used either small-scale datasets or large datasets with simulated partial copies by imposing several pre-defined transformations (e.g., photometric changes) due to the extremely time-consuming annotation of real copies. It is still unknown how well the techniques developed on simulated datasets perform on real copies, which are much more challenging and too complex to be simulated. In this paper, we introduce a large-scale video copy database (VCDB) with over 100,000 videos, and more than 9,000 copy pairs found by manual annotation. A state-of-the-art system of video copy detection is evaluated on VCDB to show the limitations of existing techniques. We also evaluate deep learning features learned by two neural networks: one is independently trained on a different dataset and the other is tailored to deal with the copy detection task. Our evaluation suggests that all the existing techniques, including the deep learning features, are far from satisfactory in detecting complex real copies.","","","10.1109/TBDATA.2016.2530714","National 863 Program; NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7416633","Video copy detection;benchmark dataset;frame matching;temporal alignment;deep learning;Video copy detection;benchmark dataset;frame matching;temporal alignment;deep learning","Videos;Feature extraction;Machine learning;Benchmark testing;TV;Manuals;Internet","simulated annealing;visual databases","VCDB;simulated datasets;extremely time-consuming annotation;photometric changes;small-scale datasets;partial copy detection","","21","36","","","","","IEEE","IEEE Journals"
"Multiple deep features learning for object retrieval in surveillance videos","H. Guo; J. Wang; H. Lu","Institute of Automation, Chinese Academy of Sciences, People's Republic of China; Institute of Automation, Chinese Academy of Sciences, People's Republic of China; Institute of Automation, Chinese Academy of Sciences, People's Republic of China","IET Computer Vision","","2016","10","4","268","271","Efficient indexing and retrieving objects of interest from large-scale surveillance videos are a significant and challenging topic. In this study, the authors present an effective multiple deep features learning approach for object retrieval in surveillance videos. Based on the discriminative convolutional neural network (CNN), they can learn multiple deep features to comprehensively describe the visual object. To be specific, they utilise the CNN model pre-trained on ImageNet ILSVRC12 and fine-tuned on our dataset to abstract structure information. In addition, they train another CNN model supervised by 11 colour names to deliver the colour information. To improve the retrieval performance, the deep features are encoded into short binary codes by locality-sensitive hash and fused to fast retrieve the object of interest. Retrieval experiments are performed on a dataset of 100k objects extracted from multi-camera surveillance videos. Comparison results with other common visual features show the effectiveness of the proposed approach.","","","10.1049/iet-cvi.2015.0291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503638","","","binary codes;feature extraction;feedforward neural nets;file organisation;image coding;image colour analysis;image fusion;indexing;video retrieval;video surveillance","multiple deep features learning;object retrieval;object indexing;large-scale surveillance videos;discriminative convolutional neural network;CNN model;ImageNet ILSVRC12;colour information;retrieval performance improvement;short binary codes;locality-sensitive hash","","2","25","","","","","IET","IET Journals"
"Instance-Aware Hashing for Multi-Label Image Retrieval","H. Lai; P. Yan; X. Shu; Y. Wei; S. Yan","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Technology, Nanjing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Image Processing","","2016","25","6","2469","2479","Similarity-preserving hashing is a commonly used method for nearest neighbor search in large-scale image retrieval. For image retrieval, deep-network-based hashing methods are appealing, since they can simultaneously learn effective image representations and compact hash codes. This paper focuses on deep-network-based hashing for multi-label images, each of which may contain objects of multiple categories. In most existing hashing methods, each image is represented by one piece of hash code, which is referred to as semantic hashing. This setting may be suboptimal for multi-label image retrieval. To solve this problem, we propose a deep architecture that learns instance-aware image representations for multi-label image data, which are organized in multiple groups, with each group containing the features for one category. The instance-aware representations not only bring advantages to semantic hashing but also can be used in category-aware hashing, in which an image is represented by multiple pieces of hash codes and each piece of code corresponds to a category. Extensive evaluations conducted on several benchmark data sets demonstrate that for both the semantic hashing and the category-aware hashing, the proposed method shows substantial improvement over the state-of-the-art supervised and unsupervised hashing methods.","","","10.1109/TIP.2016.2545300","Natural Science Foundation of Guangdong Province; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7438833","Multi-Label;Image Retrieval;Instance-Aware Image Representation;Category-Aware Hashing;Semantic Hashing;Deep Learning;Multi-label;image retrieval;instance-aware image representation;category-aware hashing;semantic hashing;deep learning","Proposals;Semantics;Image retrieval;Probability;Image representation;Convolution;Convolutional codes","file organisation;image representation;image retrieval","similarity-preserving hashing;nearest neighbor search;large-scale image retrieval;deep-network-based hashing methods;compact hash codes;semantic hashing;multi-label image retrieval;instance-aware image representations;multi-label image data;instance-aware representations;category-aware hashing","","30","38","","","","","IEEE","IEEE Journals"
"Demonstration of Convolution Kernel Operation on Resistive Cross-Point Array","L. Gao; P. Chen; S. Yu","School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA; School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ, USA","IEEE Electron Device Letters","","2016","37","7","870","873","Convolution is the key operation in the convolutional neural network, one of the most popular deep learning algorithms. The implementation of the convolution kernel on the resistive cross-point array is different than the implementation of the matrix-vector multiplication in prior works. In this letter, we propose a dimensional reduction of 2-D kernel matrix into 1-D column vector, i.e., a column of the array, and enable the parallel readout of multiple 2-D kernels simultaneously. As a proof-of-concept demonstration, we use the Prewitt kernels to detect both horizontal and vertical edges of the 20 × 20 pixels of black and-white MNIST handwritten digits. The experiments were performed on the fabricated 12 × 12 resistive cross-point array based on the Pt/HfOx/TiN structure. The experimental results of the Prewitt kernel operation perfectly matches the simulation results, indicating the feasibility of the proposed implementation methodology of the convolution kernel on resistive cross-point array.","","","10.1109/LED.2016.2573140","NSF-CCF-1552687; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7479502","convolution kernel;neuromorphic computing;cross-point array;resistive memory;Convolution kernel;neuromorphic computing;cross-point array;resistive memory","Kernel;Convolution;Image edge detection;Neurons;Object recognition;Machine learning","convolution;handwriting recognition;image classification;learning (artificial intelligence);matrix algebra;neural nets;vectors","proof-of-concept demonstration;convolution kernel operation;resistive cross-point array;convolutional neural network;deep learning algorithm;dimensional reduction;2D kernel matrix;1D column vector;Prewitt kernel;MNIST handwritten digit;classification layer","","36","18","","","","","IEEE","IEEE Journals"
"Lung Pattern Classification for Interstitial Lung Diseases Using a Deep Convolutional Neural Network","M. Anthimopoulos; S. Christodoulidis; L. Ebner; A. Christe; S. Mougiakakou","ARTORG Center for Biomedical Engineering Research, University of Bern, Switzerland; ARTORG Center for Biomedical Engineering Research, University of Bern, Switzerland; Department of Diagnostic, Interventional and Pediatric Radiology, Bern University Hospital “Inselspital”, Switzerland; Department of Diagnostic, Interventional and Pediatric Radiology, Bern University Hospital “Inselspital”, Switzerland; ARTORG Center for Biomedical Engineering Research, University of Bern, Switzerland","IEEE Transactions on Medical Imaging","","2016","35","5","1207","1216","Automated tissue characterization is one of the most crucial components of a computer aided diagnosis (CAD) system for interstitial lung diseases (ILDs). Although much research has been conducted in this field, the problem remains challenging. Deep learning techniques have recently achieved impressive results in a variety of computer vision problems, raising expectations that they might be applied in other domains, such as medical image analysis. In this paper, we propose and evaluate a convolutional neural network (CNN), designed for the classification of ILD patterns. The proposed network consists of 5 convolutional layers with 2 × 2 kernels and LeakyReLU activations, followed by average pooling with size equal to the size of the final feature maps and three dense layers. The last dense layer has 7 outputs, equivalent to the classes considered: healthy, ground glass opacity (GGO), micronodules, consolidation, reticulation, honeycombing and a combination of GGO/reticulation. To train and evaluate the CNN, we used a dataset of 14696 image patches, derived by 120 CT scans from different scanners and hospitals. To the best of our knowledge, this is the first deep CNN designed for the specific problem. A comparative analysis proved the effectiveness of the proposed CNN against previous methods in a challenging dataset. The classification performance ( ~ 85.5%) demonstrated the potential of CNNs in analyzing lung patterns. Future work includes, extending the CNN to three-dimensional data provided by CT volume scans and integrating the proposed method into a CAD system that aims to provide differential diagnosis for ILDs as a supportive tool for radiologists.","","","10.1109/TMI.2016.2535865","Bern University hospital Inselspital; Swiss National Science Foundation SNSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422082","Convolutional neural networks;interstitial lung diseases;texture classification","Lungs;Computed tomography;Diseases;Feature extraction;Convolution;Design automation;Neural networks","biological tissues;computerised tomography;convolution;diseases;feature extraction;image classification;learning (artificial intelligence);lung;medical image processing;neural nets","lung pattern classification;interstitial lung diseases;deep convolutional neural network;automated tissue characterization;computer aided diagnosis system;deep learning techniques;computer vision problems;medical image analysis;ILD pattern classification;feature maps;ground glass opacity;micronodules;consolidation;reticulation;honeycombing;CT volume scans","Algorithms;Humans;Image Interpretation, Computer-Assisted;Lung;Lung Diseases, Interstitial;Neural Networks (Computer);Tomography, X-Ray Computed","310","42","","","","","IEEE","IEEE Journals"
"Object Recognition Using Deep Convolutional Features Transformed by a Recursive Network Structure","H. M. Bui; M. Lech; E. Cheng; K. Neville; I. S. Burnett","School of Engineering, RMIT University, GPO Box 2476, Melbourne, VIC, Australia; School of Engineering, RMIT University, GPO Box 2476, Melbourne, VIC, Australia; School of Engineering, RMIT University, GPO Box 2476, Melbourne, VIC, Australia; School of Engineering, RMIT University, GPO Box 2476, Melbourne, VIC, Australia; Faculty of Engineering and Information Technology, University of Technology Sydney, PO Box 123, Broadway, NSW, Australia","IEEE Access","","2016","4","","10059","10066","Deep neural networks (DNNs) trained on large data sets have been shown to be able to capture high-quality features describing image data. Numerous studies have proposed various ways to transfer DNN structures trained on large data sets to perform classification tasks represented by relatively small data sets. Due to the limitations of these proposals, it is not well known how to effectively adapt the pre-trained model into the new task. Typically, the transfer process uses a combination of fine-tuning and training of adaptation layers; however, both tasks are susceptible to problems with data shortage and high computational complexity. This paper proposes an improvement to the well-known AlexNet feature extraction technique. The proposed approach applies a recursive neural network structure on features extracted by a deep convolutional neural network pre-trained on a large data set. Object recognition experiments conducted on the Washington RGBD image data set have shown that the proposed method has the advantages of structural simplicity combined with the ability to provide higher recognition accuracy at a low computational cost compared with other relevant methods. The new approach requires no training at the feature extraction phase, and can be performed very efficiently as the output features are compact and highly discriminative, and can be used with a simple classifier in object recognition settings.","","","10.1109/ACCESS.2016.2639543","Center of Technology, RMIT University, Vietnam; RMIT University, Melbourne, Australia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869276","Machine learning;pattern recognition;neural networks;knowledge transfer","Feature extraction;Neural networks;Object recognition;Training;Machine learning;Data mining;Australia","convolution;data analysis;feature extraction;neural nets;object recognition;recursive estimation","object recognition settings;Washington RGBD image data set;recursive neural network structure;AlexNet feature extraction technique;data shortage;computational complexity;image data;DNN;deep neural networks;deep convolutional features","","20","30","","","","","IEEE","IEEE Journals"
"Neural Network Based Multi-Factor Aware Joint Training for Robust Speech Recognition","Y. Qian; T. Tan; D. Yu","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Microsoft Research, Redmond, WA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","12","2231","2240","Although great progress has been made in automatic speech recognition (ASR), significant performance degradation still exists in noisy environments. In this paper, a novel factor-aware training framework, named neural network-based multifactor aware joint training, is proposed to improve the recognition accuracy for noise robust speech recognition. This approach is a structured model which integrates several different functional modules into one computational deep model. We explore and extract speaker, phone, and environment factor representations using deep neural networks (DNNs), which are integrated into the main ASR DNN to improve classification accuracy. In addition, the hidden activations in the main ASR DNN are used to improve factor extraction, which in turn helps the ASR DNN. All the model parameters, including those in the ASR DNN and factor extraction DNNs, are jointly optimized under the multitask learning framework. Unlike prior traditional techniques for the factor-aware training, our approach requires no explicit separate stages for factor extraction and adaptation. Moreover, the proposed neural network-based multifactor aware joint training can be easily combined with the conventional factor-aware training which uses the explicit factors, such as i-vector, noise energy, and T60 value to obtain additional improvement. The proposed method is evaluated on two main noise robust tasks: the AMI single distant microphone task in which reverberation is the main concern, and the Aurora4 task in which multiple noise types exist. Experiments on both tasks show that the proposed model can significantly reduce word error rate (WER). The best configuration achieved more than 15% relative reduction in WER over the baselines on these two tasks.","","","10.1109/TASLP.2016.2598308","Johns Hopkins University; National Science Foundation; DARPA; Google; Microsoft; Amazon; Mitsubishi Electric Research Laboratories; Shanghai Sailing Program; Program for Professor of Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher Learning; NSFC; Interdisciplinary Program (14JCZ03); Shanghai Jiao Tong University in China; ICASSP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7533439","Deep neural network;factor-aware training;factor representation;multi-task learning;robust speech recognition","Training;Speech recognition;Speech;Neural networks;Noise measurement;Adaptation models;Feature extraction","microphones;neural nets;speech recognition","neural network;multifactor aware joint training;robust speech recognition;automatic speech recognition;ASR;noisy environments;computational deep model;deep neural networks;DNNs;multitask learning framework;distant microphone;word error rate;WER","","13","43","","","","","IEEE","IEEE Journals"
"Novel Unsupervised Auditory Filterbank Learning Using Convolutional RBM for Speech Recognition","H. B. Sailor; H. A. Patil","Research Lab, Dhirubhai Ambani Institute of Information and Communication Technology (DA-IICT), Gandhinagar, Gujarat, India; Research Lab, Dhirubhai Ambani Institute of Information and Communication Technology (DA-IICT), Gandhinagar, Gujarat, India","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","12","2341","2353","To learn auditory filterbanks, recently, we have proposed an unsupervised learning model based on convolutional restricted Boltzmann machine (RBM) with rectified linear units. In this paper, theory, training algorithm of our proposed model, and detailed analysis of learned filterbank are being presented. Learning of the model with different databases shows that the model is able to learn cochlear-like impulse responses that are localized in frequency-domain. An auditory-like scale obtained from filterbanks learned from clean and noisy datasets resembles the Mel scale, which is known to mimic perceptually relevant aspect of speech. We have experimented with both cepstral (denoted as ConvRBM-CC) as well as filterbank features (denoted as ConvRBM-BANK). On large vocabulary continuous speech recognition task, we achieved relative improvement of 7.21-17.8% in word error rate (WER) compared to Mel frequency cepstral coefficient (MFCC) features and 1.35-6.82% compared to Mel filterbank (FBANK) features. On AURORA 4 multicondition training database, the relative improvement in WER by 4.8-13.65% was achieved using a Hybrid Deep Neural Network-Hidden Markov Model (DNN-HMM) system with ConvRBM-CC features. Using ConvRBM-BANK features, we achieve absolute reduction of 1.25-3.85% in WER on AURORA 4 test sets compared to FBANK features. A context-dependent DNN-HMM system further improves performance with a relative improvement of 3.6-4.6% on an average for bigram 5k and tri-gram 5k language models. Hence, our proposed learned filterbank performs better than traditional MFCC and Mel-filterbank features for both clean and multicondition automatic speech recognition (ASR) tasks. A system combination of ConvRBM-BANK and FBANK features further improve performance in all ASR tasks. Cross-domain experiments where subband filters trained on one database are used for the ASR task of another database show that model learns generalized representations of speech signals.","","","10.1109/TASLP.2016.2607341","Department of Electronics and Information Technology, Government of India; TTS; ASR; DA-IICT, Gandhinagar, India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7563327","Auditory processing;ConvRBM;filterbank;subband filters;speech recognition","Speech processing;Hidden Markov models;Convolution;Databases;Mel frequency cepstral coefficient;Mathematical model","Boltzmann machines;channel bank filters;frequency-domain analysis;hidden Markov models;speech recognition;unsupervised learning","novel unsupervised auditory filterbank learning;convolutional RBM;speech recognition;unsupervised learning model;convolutional restricted Boltzmann machine;frequency-domain;large vocabulary continuous speech recognition task;word error rate;WER;AURORA 4 multicondition training database;hybrid deep neural network-hidden Markov model;ConvRBM-CC features;context-dependent DNN-HMM system;multicondition automatic speech recognition;ASR","","17","46","","","","","IEEE","IEEE Journals"
"Polarimetric SAR Image Classification Using Deep Convolutional Neural Networks","Y. Zhou; H. Wang; F. Xu; Y. Jin","Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, shanghai, China","IEEE Geoscience and Remote Sensing Letters","","2016","13","12","1935","1939","Deep convolutional neural networks have achieved great success in computer vision and many other areas. They automatically extract translational-invariant spatial features and integrate with neural network-based classifier. This letter investigates the suitability and potential of deep convolutional neural network in supervised classification of polarimetric synthetic aperture radar (POLSAR) images. The multilooked POLSAR data in the format of coherency or covariance matrix is first converted into a normalized 6-D real feature vector. The six-channel real image is then fed into a four-layer convolutional neural network tailored for POLSAR classification. With two cascaded convolutional layers, the designed deep neural network can automatically learn hierarchical polarimetric spatial features from the data. Two experiments are presented using the AIRSAR data of San Francisco, CA, and Flevoland, The Netherlands. Classification result of the San Francisco case shows that slant built-up areas, which are conventionally mixed with vegetated area in polarimetric feature space, can now be successfully distinguished after taking into account spatial features. Quantitative analysis with respect to ground truth information available for the Flevoland test site shows that the proposed method achieves an accuracy of 92.46% in classifying the considered 15 classes. Such results are comparable with the state of the art.","","","10.1109/LGRS.2016.2618840","Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762055","Deep convolutional neural network;polarimetric synthetic aperture radar (POLSAR);supervised classification","Feature extraction;Neural networks;Training;Convolution;Synthetic aperture radar;Machine learning;Data mining","geophysical image processing;image classification;neural nets;radar polarimetry;remote sensing by radar;synthetic aperture radar","polarimetric feature space;vegetated area;Netherlands;USA;Flevoland;California;San Francisco;AIRSAR data;normalized 6D real feature vector;covariance matrix;POLSAR images;polarimetric synthetic aperture radar images;neural network based classifier;translational invariant spatial features;computer vision;deep convolutional neural networks;polarimetric SAR image classification","","104","19","","","","","IEEE","IEEE Journals"
"Very Deep Convolutional Neural Networks for Noise Robust Speech Recognition","Y. Qian; M. Bi; T. Tan; K. Yu","Computer Science and Engineering Department, Shanghai Jiao Tong University, Shanghai, China; Computer Science and Engineering Department, Shanghai Jiao Tong University, Shanghai, China; Computer Science and Engineering Department, Shanghai Jiao Tong University, Shanghai, China; Computer Science and Engineering Department, Shanghai Jiao Tong University, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","12","2263","2276","Although great progress has been made in automatic speech recognition, significant performance degradation still exists in noisy environments. Recently, very deep convolutional neural networks (CNNs) have been successfully applied to computer vision and speech recognition tasks. Based on our previous work on very deep CNNs, in this paper this architecture is further developed to improve recognition accuracy for noise robust speech recognition. In the proposed very deep CNN architecture, we study the best configuration for the sizes of filters, pooling, and input feature maps: the sizes of filters and poolings are reduced and dimensions of input features are extended to allow for adding more convolutional layers. Then the appropriate pooling, padding, and input feature map selection strategies are investigated and applied to the very deep CNN to make it more robust for speech recognition. In addition, an in-depth analysis of the architecture reveals key characteristics, such as compact model scale, fast convergence speed, and noise robustness. The proposed new model is evaluated on two tasks: Aurora4 task with multiple additive noise types and channel mismatch, and the AMI meeting transcription task with significant reverberation. Experiments on both tasks show that the proposed very deep CNNs can significantly reduce word error rate (WER) for noise robust speech recognition. The best architecture obtains a 10.0% relative reduction over the traditional CNN on AMI, competitive with the long short-term memory recurrent neural networks (LSTM-RNN) acoustic model. On Aurora4, even without feature enhancement, model adaptation, and sequence training, it achieves a WER of 8.81%, a 17.0% relative improvement over the LSTM-RNN. To our knowledge, this is the best published result on Aurora4.","","","10.1109/TASLP.2016.2602884","Shanghai Sailing Program; Program for Professor of Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher Learning; NSFC; Interdisciplinary Program; Shanghai Jiao Tong University in China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552554","Convolutional neural networks;very deep CNNs;robust speech recognition;acoustic modeling","Speech recognition;Convolution;Neural networks;Noise measurement;Noise robustness","recurrent neural nets;speech recognition","very deep convolutional neural networks;noise robust speech recognition;automatic speech recognition;noisy environments;computer vision;CNN architecture;filters;compact model scale;fast convergence speed;noise robustness;word error rate;WER;long short-term memory recurrent neural networks;LSTM-RNN acoustic model","","87","42","","","","","IEEE","IEEE Journals"
"Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation","P. Swietojanski; J. Li; S. Renals","Centre for Speech Technology Research, University of Edinburgh, Edinburgh, U.K.; Microsoft Corporation, Redmond, WA, USA; Centre for Speech Technology Research, University of Edinburgh, Edinburgh, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","8","1450","1463","This work presents a broad study on the adaptation of neural network acoustic models by means of learning hidden unit contributions (LHUC) - a method that linearly re-combines hidden units in a speaker- or environment-dependent manner using small amounts of unsupervised adaptation data. We also extend LHUC to a speaker adaptive training (SAT) framework that leads to a more adaptable DNN acoustic model, working both in a speaker-dependent and a speaker-independent manner, without the requirements to maintain auxiliary speaker-dependent feature extractors or to introduce significant speaker-dependent changes to the DNN structure. Through a series of experiments on four different speech recognition benchmarks (TED talks, Switchboard, AMI meetings, and Aurora4) comprising 270 test speakers, we show that LHUC in both its test-only and SAT variants results in consistent word error rate reductions ranging from 5% to 23% relative depending on the task and the degree of mismatch between training and test data. In addition, we have investigated the effect of the amount of adaptation data per speaker, the quality of unsupervised adaptation targets, the complementarity to other adaptation techniques, one-shot adaptation, and an extension to adapting DNNs trained in a sequence discriminative manner.","","","10.1109/TASLP.2016.2560534","EPSRC; Natural Speech Technology (NST); European Union; SUMMA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7462247","Adaptation;deep neural networks (DNNs);factorisation;learning hidden unit contributions (lHUC)","Adaptation models;Acoustics;Neural networks;Training;Hidden Markov models;Transforms;Feature extraction","acoustic signal processing;feature extraction;neural nets;speaker recognition;unsupervised learning","learning hidden unit contributions;LHUC;unsupervised acoustic model adaptation;neural network acoustic models;speaker adaptive training framework;SAT;DNN acoustic model;speaker-dependent manner;speaker-independent manner;speech recognition benchmarks;TED talks;Switchboard;AMI meetings;Aurora4;consistent word error rate reductions","","27","84","","","","","IEEE","IEEE Journals"
"SmartCrawler: A Two-Stage Crawler for Efficiently Harvesting Deep-Web Interfaces","F. Zhao; J. Zhou; C. Nie; H. Huang; H. Jin","Services Computing Technology and System Lab & Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Services Computing Technology and System Lab & Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Engineering, The Pennsylvania State University, University Park, PA; Services Computing Technology and System Lab & Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Services Computing","","2016","9","4","608","620","As deep web grows at a very fast pace, there has been increased interest in techniques that help efficiently locate deep-web interfaces. However, due to the large volume of web resources and the dynamic nature of deep web, achieving wide coverage and high efficiency is a challenging issue. We propose a two-stage framework, namely SmartCrawler, for efficient harvesting deep web interfaces. In the first stage, SmartCrawler performs site-based searching for center pages with the help of search engines, avoiding visiting a large number of pages. To achieve more accurate results for a focused crawl, SmartCrawler ranks websites to prioritize highly relevant ones for a given topic. In the second stage, SmartCrawler achieves fast in-site searching by excavating most relevant links with an adaptive link-ranking. To eliminate bias on visiting some highly relevant links in hidden web directories, we design a link tree data structure to achieve wider coverage for a website. Our experimental results on a set of representative domains show the agility and accuracy of our proposed crawler framework, which efficiently retrieves deep-web interfaces from large-scale sites and achieves higher harvest rates than other crawlers.","","","10.1109/TSC.2015.2414931","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064719","Deep web;two-stage crawler;feature selection;ranking;adaptive learning","Crawlers;Search engines;Google;Uniform resource locators;Indexes","Internet;search engines;Web sites","SmartCrawler;two-stage crawler;deep Web interfaces;Web resources;search engines;Web sites;adaptive link-ranking;hidden Web directories;link tree data structure;representative domains;crawler framework","","21","42","","","","","IEEE","IEEE Journals"
"Maximum Margin Learning of t-SPNs for Cell Classification With Filtered Input","H. Kang; C. D. Yoo; Y. Na","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong Gu, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong Gu, Daejeon, South Korea; Interdisciplinary Program for Future Vehicles, Korea Advanced Institute of Science and Technology, Yuseong Gu, Daejeon, South Korea","IEEE Journal of Selected Topics in Signal Processing","","2016","10","1","130","139","An algorithm based on a deep probabilistic architecture referred to as tree-structured sum-product network (t-SPN) is considered for cells classification. The t-SPN is a rooted acyclic graph constructed as a tree of several sum-product networks where each network is constructed over a subset of most confusing class features. The constructed t-SPN architecture is learned by maximizing the margin which is defined to be the difference in the conditional probability between the true and the most competitive false labels. To enhance generalization, l2-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate compared to other state-of-the-art algorithms that include convolutional neural network (CNN) based algorithms. Ideal high-pass filter was more effective on the HEp-2 dataset which is based on immunofluorescence staining while the LOG was more effective on Feulgen dataset which is based on Feulgen staining.","","","10.1109/JSTSP.2015.2502542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332738","t-SPNs;sub-SPNs;maximum margin;confusing classes","Computer architecture;Microprocessors;Signal processing;Input variables;Microscopy;Special issues and sections","biomedical optical imaging;cellular biophysics;fluorescence;high-pass filters;image classification;learning (artificial intelligence);medical image processing;neural nets;probability","Feulgen staining;immunofluorescence staining;convolutional neural network;Feulgen dataset;HEp-2 dataset;Laplacian-of-Gaussian filtering;generic high-pass filter;cell feature;learning process;maximum margin criterion;l2-regularization;t-SPN architecture;rooted acyclic graph;deep probabilistic architecture;filtered input;cell classification;tree-structured sum-product network;maximum margin learning","","","18","","","","","IEEE","IEEE Journals"
"Factors of Transferability for a Generic ConvNet Representation","H. Azizpour; A. S. Razavian; J. Sullivan; A. Maki; S. Carlsson","Computer Vision and Active Perception Lab (CVAP), Royal Institute of Technology (KTH), Stockholm, Sweden; Computer Vision and Active Perception Lab (CVAP), Royal Institute of Technology (KTH), Stockholm, Sweden; Computer Vision and Active Perception Lab (CVAP), Royal Institute of Technology (KTH), Stockholm, Sweden; Computer Vision and Active Perception Lab (CVAP), Royal Institute of Technology (KTH), Stockholm, Sweden; Computer Vision and Active Perception Lab (CVAP), Royal Institute of Technology (KTH), Stockholm, Sweden","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","9","1790","1802","Evidence is mounting that Convolutional Networks (ConvNets) are the most effective representation learning method for visual recognition tasks. In the common scenario, a ConvNet is trained on a large labeled dataset (source) and the feed-forward units activation of the trained network, at a certain layer of the network, is used as a generic representation of an input image for a task with relatively smaller training set (target). Recent studies have shown this form of representation transfer to be suitable for a wide range of target visual recognition tasks. This paper introduces and investigates several factors affecting the transferability of such representations. It includes parameters for training of the source ConvNet such as its architecture, distribution of the training data, etc. and also the parameters of feature extraction such as layer of the trained ConvNet, dimensionality reduction, etc. Then, by optimizing these factors, we show that significant improvements can be achieved on various (17) visual recognition tasks. We further show that these visual recognition tasks can be categorically ordered based on their similarity to the source task such that a correlation between the performance of tasks and their similarity to the source task w.r.t. the proposed factors is observed.","","","10.1109/TPAMI.2015.2500224","Swedish Foundation for Strategic Research; VINST; CVPR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328311","Convolutional neural networks;transfer learning;representation learning;deep learning;visual recognition","Visualization;Training;Image recognition;Target recognition;Training data;Standards;Sun","feature extraction;image representation;learning (artificial intelligence)","generic ConvNet representation;transferability factors;convolutional networks;representation learning method;visual recognition;feed-forward units activation;trained network;generic representation;feature extraction","","77","56","","","","","IEEE","IEEE Journals"
"Multi-loss Regularized Deep Neural Network","C. Xu; C. Lu; X. Liang; J. Gao; W. Zheng; T. Wang; S. Yan","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Computing and Mathematics, Charles Sturt University, Bathurst, NSW, Australia; Beijing Samsung Telecom Research and Development Center, Beijing, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Circuits and Systems for Video Technology","","2016","26","12","2273","2283","A proper strategy to alleviate overfitting is critical to a deep neural network (DNN). In this paper, we introduce the cross-loss-function regularization for boosting the generalization capability of the DNN, which results in the multi-loss regularized DNN (ML-DNN) framework. For a particular learning task, e.g., image classification, only a single-loss function is used for all previous DNNs, and the intuition behind the multiloss framework is that the extra loss functions with different theoretical motivations (e.g., pairwise loss and LambdaRank loss) may drag the algorithm away from overfitting to one particular single-loss function (e.g., softmax loss). In the training stage, we pretrain the model with the single-core-loss function and then warm start the whole ML-DNN with the convolutional parameters transferred from the pretrained model. In the testing stage, the outputs by the ML-DNN from different loss functions are fused with average pooling to produce the ultimate prediction. The experiments conducted on several benchmark datasets (CIFAR-10, CIFAR-100, MNIST, and SVHN) demonstrate that the proposed ML-DNN framework, instantiated by the recently proposed network in network, considerably outperforms all other state-of-the-art methods.","","","10.1109/TCSVT.2015.2477937","National Natural Science Foundation of China; Australian Research Council Discovery Projects Funding Scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7258343","Deep neural network (DNN);multi-loss;overfitting;visual classification","Neural networks;Image classification;Loss measurement;Stochastic processes","generalisation (artificial intelligence);image classification;learning (artificial intelligence);neural nets","multiloss regularized deep neural network;cross-loss-function regularization;DNN generalization capability;multiloss regularized DNN;ML-DNN;image classification;single-loss function;LambdaRank loss;pairwise loss;softmax loss;single-core-loss function;convolutional parameters;CIFAR-10 datasets;CIFAR-100 datasets;SVHN datasets;MNIST datasets","","25","35","","","","","IEEE","IEEE Journals"
"Training a Probabilistic Graphical Model With Resistive Switching Electronic Synapses","S. B. Eryilmaz; E. Neftci; S. Joshi; S. Kim; M. BrightSky; H. Lung; C. Lam; G. Cauwenberghs; H. P. Wong","Electrical Engineering Department, Stanford University, Stanford, CA, USA; Department of Cognitive Sciences, University of California at Irvine, Irvine, CA, USA; Department of Electrical and Computer Engineering, University of California at San Diego, San Diego, CA, USA; IBM Research, Yorktown Heights, NY, USA; IBM Research, Yorktown Heights, NY, USA; Emerging Central Lab., Macronix International Co., Ltd., Hsinchu Science Park, Taiwan; IBM Research, Yorktown Heights, NY, USA; Department of Bioengineering, University of California at San Diego, San Diego, CA, USA; Electrical Engineering Department, Stanford University, Stanford, CA, USA","IEEE Transactions on Electron Devices","","2016","63","12","5004","5011","Current large-scale implementations of deep learning and data mining require thousands of processors, massive amounts of off-chip memory, and consume gigajoules of energy. New memory technologies, such as nanoscale two-terminal resistive switching memory devices, offer a compact, scalable, and low-power alternative that permits on-chip colocated processing and memory in fine-grain distributed parallel architecture. Here, we report the first use of resistive memory devices for implementing and training a restricted Boltzmann machine (RBM), a generative probabilistic graphical model as a key component for unsupervised learning in deep networks. We experimentally demonstrate a 45-synapse RBM realized with 90 resistive phase change memory (PCM) elements trained with a bioinspired variant of the contrastive divergence algorithm, implementing Hebbian and anti-Hebbian weight updates. The resistive PCM devices show a twofold to tenfold reduction in error rate in a missing pixel pattern completion task trained over 30 epochs, compared with untrained case. Measured programming energy consumption is 6.1 nJ per epoch with the PCM devices, a factor of ~ 150 times lower than the conventional processor-memory systems. We analyze and discuss the dependence of learning performance on cycle-to-cycle variations and number of gradual levels in the PCM analog memory devices.","","","10.1109/TED.2016.2616483","SONIC, one of six centers of STARnet, a Semiconductor Research Corporation Program sponsored by MARCO and DARPA; NSF Expedition on Computing under Visual Cortex on Silicon; Member Companies of the Stanford Non-Volatile Memory Technology Research Initiative and the Stanford SystemX Alliance; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7728051","Brain-inspired hardware;cognitive computing;neuromorphic computing;phase change memory(PCM);resistive memory","Phase change memory;Neurons;Resistive RAM;Data models;Neuromorphic engineering;Hardware;Probabilistic logic","analogue storage;Boltzmann machines;graph theory;Hebbian learning;phase change memories;resistive RAM;unsupervised learning","resistive switching electronic synapses;deep learning;data mining;off-chip memory;nanoscale two-terminal resistive switching memory devices;on-chip colocated processing;fine-grain distributed parallel architecture;resistive memory devices;restricted Boltzmann machine;RBM;generative probabilistic graphical model;unsupervised learning;resistive phase change memory;Hebbian-antiHebbian weight updates;pixel pattern completion task;processor-memory systems;cycle-to-cycle variations;PCM analog memory devices","","16","51","","","","","IEEE","IEEE Journals"
"Video anomaly detection using deep incremental slow feature analysis network","X. Hu; S. Hu; Y. Huang; H. Zhang; H. Wu","University of Shanghai For Science and Technology, People's Republic of China; Shanghai Jiao Tong University, People's Republic of China; University of Shanghai For Science and Technology, People's Republic of China; Shanghai Jiao Tong University, People's Republic of China; Anhui University of Technology, People's Republic of China","IET Computer Vision","","2016","10","4","258","265","Existing anomaly detection (AD) approaches rely on various hand-crafted representations to represent video data and can be costly. The choice or designing of hand-crafted representation can be difficult when faced with a new dataset without prior knowledge. Motivated by feature learning, e.g. deep leaning and the ability to directly learn useful representations and model high-level abstraction from raw data, the authors investigate the possibility of using a universal approach. The objective is learning data-driven high-level representation for the task of video AD without relying on hand-crafted representation. A deep incremental slow feature analysis (D-IncSFA) network is constructed and applied to directly learning progressively abstract and global high-level representations from raw data sequence. The D-IncSFA network has the functionalities of both feature extractor and anomaly detector that make AD completion in one step. The proposed approach can precisely detect global anomaly such as crowd panic. To detect local anomaly, a set of anomaly maps, produced from the network at different scales, is used. The proposed approach is universal and convenient, working well in different types of scenarios with little human intervention and low memory and computational requirements. The advantages are validated by conducting extensive experiments on different challenge datasets.","","","10.1049/iet-cvi.2015.0271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503634","","","video signal processing","video anomaly detection;deep incremental slow feature analysis network;hand-crafted representations;video data;feature learning;deep leaning;D-IncSFA network;global anomaly;crowd panic","","8","66","","","","","IET","IET Journals"
"Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?","N. Tajbakhsh; J. Y. Shin; S. R. Gurudu; R. T. Hurst; C. B. Kendall; M. B. Gotway; J. Liang","Department of Biomedical Informatics, Arizona State University, Scottsdale, AZ, USA; Department of Biomedical Informatics, Arizona State University, Scottsdale, AZ, USA; Division of Gastroenterology and Hepatology, Mayo Clinic, Scottsdale, AZ, USA; Division of Cardiovascular Diseases, Mayo Clinic, Scottsdale, AZ, USA; Division of Cardiovascular Diseases, Mayo Clinic, Scottsdale, AZ, USA; Department of Radiology, Mayo Clinic, Scottsdale, AZ, USA; Department of Biomedical Informatics, Arizona State University, Scottsdale, AZ, USA","IEEE Transactions on Medical Imaging","","2016","35","5","1299","1312","Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.","","","10.1109/TMI.2016.2535302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426826","Carotid intima-media thickness;computer-aided detection;convolutional neural networks;deep learning;fine-tuning;medical image analysis;polyp detection;pulmonary embolism detection;video quality assessment","Biomedical imaging;Training;Feature extraction;Image segmentation;Computed tomography;Image analysis;Tuning","biomedical optical imaging;endoscopes;image classification;image segmentation;medical image processing;neural nets","medical image analysis;deep convolutional neural network;labeled training data;distinct medical imaging applications;radiology;cardiology;gastroenterology;classification;segmentation;imaging modalities;layer-wise fine-tuning scheme","Colonic Polyps;Colonoscopy;Computed Tomography Angiography;Diagnostic Imaging;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Neural Networks (Computer);Pulmonary Embolism;ROC Curve","582","76","","","","","IEEE","IEEE Journals"
"An End-to-End Neural Network for Polyphonic Piano Music Transcription","S. Sigtia; E. Benetos; S. Dixon","Centre for Digital Music, School of Electronic Engineering, and Computer Science, Queen Mary University of London, London, U.K.; Centre for Digital Music, School of Electronic Engineering, and Computer Science, Queen Mary University of London, London, U.K.; Centre for Digital Music, School of Electronic Engineering, and Computer Science, Queen Mary University of London, London, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","5","927","939","We present a supervised neural network model for polyphonic piano music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We perform two sets of experiments. We investigate various neural network architectures for the acoustic models and also investigate the effect of combining acoustic and music language model predictions using the proposed architecture. We compare performance of the neural network-based acoustic models with two popular unsupervised acoustic models. Results show that convolutional neural network acoustic models yield the best performance across all evaluation metrics. We also observe improved performance with the application of the music language models. Finally, we present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications.","","","10.1109/TASLP.2016.2533858","Royal Academy of Engineering Research Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7416164","Automatic Music Transcription,;Deep Learning;Recurrent Neural Networks;Music Language Models;Automatic music transcription;deep learning;recurrent neural networks;music language models","Hidden Markov models;Acoustics;Computational modeling;Feature extraction;Recurrent neural networks;Spectrogram","information retrieval;learning (artificial intelligence);music;musical instruments;recurrent neural nets;search problems;speech recognition","end-to-end neural network;polyphonic piano music transcription;supervised neural network model;speech recognition systems;acoustic model;music language model;audio frame;recurrent neural network;polyphonic music;language model predictions;probabilistic graphical model;beam search algorithm;acoustic models;music language model predictions;unsupervised acoustic models;evaluation metrics;music language models;music information retrieval;MIR","","59","52","","","","","IEEE","IEEE Journals"
"Automatic Shadow Detection and Removal from a Single Image","S. H. Khan; M. Bennamoun; F. Sohel; R. Togneri","School of Computer Science and Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, WA, Australia; School of Computer Science and Software Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, WA, Australia; School of Engineering and Information Technology, Murdoch University, 90 South St, Murdoch, WA; School of Electrical, Electronic and Computer Engineering, The University of Western Australia, 35 Stirling Highway, Crawley, WA, Australia","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","3","431","446","We present a framework to automatically detect and remove shadows in real world scenes from a single image. Previous works on shadow detection put a lot of effort in designing shadow variant and invariant hand-crafted features. In contrast, our framework automatically learns the most relevant features in a supervised manner using multiple convolutional deep neural networks (ConvNets). The features are learned at the super-pixel level and along the dominant boundaries in the image. The predicted posteriors based on the learned features are fed to a conditional random field model to generate smooth shadow masks. Using the detected shadow masks, we propose a Bayesian formulation to accurately extract shadow matte and subsequently remove shadows. The Bayesian formulation is based on a novel model which accurately models the shadow generation process in the umbra and penumbra regions. The model parameters are efficiently estimated using an iterative optimization procedure. Our proposed framework consistently performed better than the state-of-the-art on all major shadow databases collected under a variety of conditions.","","","10.1109/TPAMI.2015.2462355","IPRS; University of Western Australia; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7172555","Feature Learning;Bayesian shadow removal;Conditional Random Field;ConvNets;Shadow detection;Shadow matting;Feature learning;Bayesian shadow removal;conditional random field;convnets;shadow detection;shadow matting","Feature extraction;Image color analysis;Lighting;Training;Bayes methods;Visualization;Databases","Bayes methods;feedforward neural nets;image processing;iterative methods;learning (artificial intelligence);optimisation","automatic shadow detection;single image;automatic shadow removal;shadow variant;invariant hand-crafted features;convolutional deep neural networks;ConvNets;super-pixel level;predicted posteriors;learned features;conditional random field model;smooth shadow masks;Bayesian formulation;shadow matte;shadow generation process;umbra regions;penumbra regions;iterative optimization procedure;shadow databases","","70","54","","","","","IEEE","IEEE Journals"
"Region-Based Convolutional Networks for Accurate Object Detection and Segmentation","R. Girshick; J. Donahue; T. Darrell; J. Malik","Microsoft Research, Redmond, WA; Department of Electrical Engineering and Computer Science, UC Berkeley, Berkeley, CA; Department of Electrical Engineering and Computer Science, UC Berkeley, Berkeley, CA; Department of Electrical Engineering and Computer Science, UC Berkeley, Berkeley, CA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","1","142","158","Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.","","","10.1109/TPAMI.2015.2437384","US National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7112511","Object recognition;detection;semantic segmentation;convolutional networks;deep learning;transfer learning;Object recognition;detection;semantic segmentation;convolutional networks;deep learning;transfer learning","Proposals;Object detection;Feature extraction;Training;Image segmentation;Support vector machines;Detectors","convolutional codes;image coding;image segmentation;object detection;source coding","region-based convolutional networks;object detection;object segmentation;canonical PASCAL VOC Challenge datasets;mean average precision;mAP;high-capacity convolutional networks;source code","","559","76","","","","","IEEE","IEEE Journals"
"Data Randomization and Cluster-Based Partitioning for Botnet Intrusion Detection","O. Y. Al-Jarrah; O. Alhussein; P. D. Yoo; S. Muhaidat; K. Taha; K. Kim","Electrical and Computer Engineering Department, Khalifa University of Science Technology and Research, Abu Dhabi, UAE; School of Engineering Science, Simon Fraser University, Burnaby, BC, Canada; Department of Computing and Informatics, Bournemouth University, Poole, U.K.; Electrical and Computer Engineering Department, Khalifa University of Science Technology and Research, Abu Dhabi, UAE; Electrical and Computer Engineering Department, Khalifa University of Science Technology and Research, Abu Dhabi, UAE; School of Computing, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Korea","IEEE Transactions on Cybernetics","","2016","46","8","1796","1806","Botnets, which consist of remotely controlled compromised machines called bots, provide a distributed platform for several threats against cyber world entities and enterprises. Intrusion detection system (IDS) provides an efficient countermeasure against botnets. It continually monitors and analyzes network traffic for potential vulnerabilities and possible existence of active attacks. A payload-inspection-based IDS (PI-IDS) identifies active intrusion attempts by inspecting transmission control protocol and user datagram protocol packet's payload and comparing it with previously seen attacks signatures. However, the PI-IDS abilities to detect intrusions might be incapacitated by packet encryption. Traffic-based IDS (T-IDS) alleviates the shortcomings of PI-IDS, as it does not inspect packet payload; however, it analyzes packet header to identify intrusions. As the network's traffic grows rapidly, not only the detection-rate is critical, but also the efficiency and the scalability of IDS become more significant. In this paper, we propose a state-of-the-art T-IDS built on a novel randomized data partitioned learning model (RDPLM), relying on a compact network feature set and feature selection techniques, simplified subspacing and a multiple randomized meta-learning technique. The proposed model has achieved 99.984% accuracy and 21.38 s training time on a well-known benchmark botnet dataset. Experiment results demonstrate that the proposed methodology outperforms other well-known machine-learning models used in the same detection task, namely, sequential minimal optimization, deep neural network, C4.5, reduced error pruning tree, and randomTree.","","","10.1109/TCYB.2015.2490802","Khalifa University of Science, Technology and Research-Korea Institute of Science and Technology (KAIST) Institute; KAIST, Korea; National Research Foundation of Korea through the Korea government (MSIP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312964","Botnet intrusion detection;efficient learning;ensembles;feature selection;machine-learning (ML)","Feature extraction;Computational modeling;Accuracy;Partitioning algorithms;Clustering algorithms;Intrusion detection;Data models","computer network security;digital signatures;learning (artificial intelligence);pattern clustering;telecommunication traffic;transport protocols","cluster-based partitioning;botnet intrusion detection;remotely-controlled compromised machines;distributed platform;intrusion detection system;network traffic;active attacks;payload-inspection-based IDS;active intrusion;transmission control protocol;user datagram protocol packet payload;attack signatures;PI-IDS;packet encryption;traffic-based IDS;T-IDS;packet header analysis;critical detection-rate;randomized data partitioned learning model;RDPLM;compact network feature set;feature selection techniques;subspacing technique;multiple randomized meta-learning technique;benchmark botnet dataset","","34","53","","","","","IEEE","IEEE Journals"
"Factorized Hidden Layer Adaptation for Deep Neural Network Based Acoustic Modeling","L. Samarakoon; K. C. Sim","the Department of Computer Science, National University of Singapore, Singapore; Google, Inc., USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","12","2241","2250","In this paper, we propose the factorized hidden layer (FHL) approach to adapt the deep neural network (DNN) acoustic models for automatic speech recognition (ASR). FHL aims at modeling speaker dependent (SD) hidden layers by representing an SD affine transformation as a linear combination of bases. The combination weights are low-dimensional speaker parameters that can be initialized using speaker representations like i-vectors and then reliably refined in an unsupervised adaptation fashion. Therefore, our method provides an efficient way to perform both adaptive training and (test-time) adaptation. Experimental results have shown that the FHL adaptation improves the ASR performance significantly, compared to the standard DNN models, as well as other state-of-the-art DNN adaptation approaches, such as training with the speaker-normalized CMLLR features, speaker-aware training using i-vector and learning hidden unit contributions (LHUC). For Aurora 4, FHL achieves 3.8% and 2.3% absolute improvements over the standard DNNs trained on the LDA + STC and CMLLR features, respectively. It also achieves 1.7% absolute performance improvement over a system that combines the i-vector adaptive training with LHUC adaptation. For the AMI dataset, FHL achieved 1.4% and 1.9% absolute improvements over the sequence-trained CMLLR baseline systems, for the IHM and SDM tasks, respectively.","","","10.1109/TASLP.2016.2601146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546852","Automatic speech recognition;deep neural networks;speaker adaptation","Adaptation models;Training;Hidden Markov models;Acoustics;Mathematical model;Standards;Interpolation","neural nets;speech recognition","factorized hidden layer adaptation;deep neural network;FHL;DNN acoustic models;automatic speech recognition;ASR;speaker dependent;SD hidden layers;speaker parameters;speaker representations;unsupervised adaptation fashion;learning hidden unit contributions;LHUC","","22","76","","","","","IEEE","IEEE Journals"
"Visual Saliency Detection Based on Multiscale Deep CNN Features","G. Li; Y. Yu","Sun Yat-sen University, Guangzhou, China; Department of Computer Science, The University of Hong Kong, Hong Kong","IEEE Transactions on Image Processing","","2016","25","11","5012","5024","Visual saliency is a fundamental problem in both cognitive and computational sciences, including computer vision. In this paper, we discover that a high-quality visual saliency model can be learned from multiscale features extracted using deep convolutional neural networks (CNNs), which have had many successes in visual recognition tasks. For learning such saliency models, we introduce a neural network architecture, which has fully connected layers on top of CNNs responsible for feature extraction at three different scales. The penultimate layer of our neural network has been confirmed to be a discriminative high-level feature vector for saliency detection, which we call deep contrast feature. To generate a more robust feature, we integrate handcrafted low-level features with our deep contrast feature. To promote further research and evaluation of visual saliency models, we also construct a new large database of 4447 challenging images and their pixelwise saliency annotations. Experimental results demonstrate that our proposed method is capable of achieving the state-of-the-art performance on all public benchmarks, improving the F-measure by 6.12% and 10%, respectively, on the DUT-OMRON data set and our new data set (HKU-IS), and lowering the mean absolute error by 9% and 35.3%, respectively, on these two data sets.","","","10.1109/TIP.2016.2602079","Hong Kong Postgraduate Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7548372","Convolutional neural networks;saliency detection;deep contrast feature","Feature extraction;Visualization;Computational modeling;Image color analysis;Biological neural networks;Semantics","feature extraction;image recognition;neural nets;vectors","visual saliency detection model;multiscale deep CNN feature extraction;computer vision;convolutional neural network;visual recognition;discriminative high-level feature vector;DUT-OMRON data set;HKU-IS","","92","56","","","","","IEEE","IEEE Journals"
"Unseen Noise Estimation Using Separable Deep Auto Encoder for Speech Enhancement","M. Sun; X. Zhang; H. Van hamme; T. F. Zheng","Lab of Intelligent Information Processing, PLA University of Science and Technology, Nanjing, China; Lab of Intelligent Information Processing, PLA University of Science and Technology, Nanjing, China; Speech Processing Research Group, Electrical Engineering Department (ESAT), KU Leuven, Leuven, Belgium; Research Institute of Information Technology, Tsinghua University, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","1","93","104","Unseen noise estimation is a key yet challenging step to make a speech enhancement algorithm work in adverse environments. At worst, the only prior knowledge we know about the encountered noise is that it is different from the involved speech. Therefore, by subtracting the components which cannot be adequately represented by a well defined speech model, the noises can be estimated and removed. Given the good performance of deep learning in signal representation, a deep auto encoder (DAE) is employed in this work for accurately modeling the clean speech spectrum. In the subsequent stage of speech enhancement, an extra DAE is introduced to represent the residual part obtained by subtracting the estimated clean speech spectrum (by using the pre-trained DAE) from the noisy speech spectrum. By adjusting the estimated clean speech spectrum and the unknown parameters of the noise DAE, one can reach a stationary point to minimize the total reconstruction error of the noisy speech spectrum. The enhanced speech signal is thus obtained by transforming the estimated clean speech spectrum back into time domain. The above proposed technique is called separable deep auto encoder (SDAE). Given the under-determined nature of the above optimization problem, the clean speech reconstruction is confined in the convex hull spanned by a pre-trained speech dictionary. New learning algorithms are investigated to respect the non-negativity of the parameters in the SDAE. Experimental results on TIMIT with 20 noise types at various noise levels demonstrate the superiority of the proposed method over the conventional baselines.","","","10.1109/TASLP.2015.2498101","Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; KULeuven research; National Natural Science Foundation of China; National Basic Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320993","Deep auto encoder;source separation;speech enhancement;unseen noise compensation","Speech enhancement;Hidden Markov models;Training;Noise measurement","convex programming;signal representation;speech enhancement","unseen noise estimation;separable deep auto encoder;speech enhancement;signal representation;noisy speech spectrum;enhanced speech signal;clean speech reconstruction;convex hull;pre-trained speech dictionary","","28","30","","","","","IEEE","IEEE Journals"
"Cluster Adaptive Training for Deep Neural Network Based Acoustic Model","T. Tan; Y. Qian; K. Yu","Computer Science and Engineering Department, Shanghai Jiao Tong University, Shanghai, China; Computer Science and Engineering Department, Shanghai Jiao Tong University, Shanghai, China; Computer Science and Engineering Department, Shanghai Jiao Tong University, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","3","459","468","Although context-dependent DNN-HMM systems have achieved significant improvements over GMM-HMM systems, significant performance degradation has been observed if the acoustic condition of the test data mismatches that of the training data. Hence, adaptation and adaptive training of DNN are of great research interest. Previous DNN adaptation works mainly focus on adapting parameters of a single DNN by applying linear transformations to feature or hidden-layer output; introducing vector representation of non-speech variability into the input. In these methods, large number of parameters are required to be estimated during adaptation. In this paper, the cluster adaptive training (CAT) framework is employed for DNN adaptive training. Here, multiple weight matrices are constructed to form the basis of a canonical parametric space. During adaptation, for a new acoustic condition, an interpolation vector is estimated to combine the weight basis into a single adapted weight matrix. Since only the interpolation vector need to be estimated during adaptation, the number of updated parameters is much smaller than existing DNN adaptation methods. The CAT-DNN approach was evaluated on an English switchboard task in unsupervised adaptation mode. It achieved significant WER reductions over the unadapted DNN-HMM, relative 7.6% to 10.6%, with only 10 parameters.","","","10.1109/TASLP.2015.2511922","China NSFC Project; JiangSu NSF Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364207","Adaptation;cluster adaptive training;deep neural network","Hidden Markov models;Training;Adaptation models;Interpolation;Acoustics;Mathematical model;Speech","Gaussian processes;hidden Markov models;interpolation;learning (artificial intelligence);matrix algebra;mixture models;neural nets;pattern clustering;speech recognition","deep neural network based acoustic model;context-dependent DNN-HMM system;GMM-HMM system;speech recognition;WER reduction;unsupervised adaptation mode;English switchboard task;interpolation vector;canonical parametric space;multiple weight matrices;cluster adaptive training framework;CAT-DNN approach;linear transformation;performance degradation","","18","38","","","","","IEEE","IEEE Journals"
"Temporal Pattern Recognition in Gait Activities Recorded With a Footprint Imaging Sensor System","O. Costilla-Reyes; P. Scully; K. B. Ozanyan","School of Electrical and Electronic Engineering, Photon Science Institute, University of Manchester, Manchester, U.K.; School of Chemical Engineering and Analytical Science, Photon Science Institute, University of Manchester, Manchester, U.K.; School of Electrical and Electronic Engineering, Photon Science Institute, University of Manchester, Manchester, U.K.","IEEE Sensors Journal","","2016","16","24","8815","8822","In this paper, we assess the capability of a unique unobtrusive footprint imaging sensor system, based on plastic optical fiber technology, to allow efficient gait analysis from time domain sensor data by pattern recognition techniques. Trial gait classification experiments are executed as ten manners of walking, affecting the amplitude and frequency characteristics of the temporal signals. The data analysis involves the design of five temporal features, subsequently analyzed in 14 different machine learning models, representing linear, non-linear, ensemble, and deep learning models. The model performance is presented as cross-validated accuracy scores for the best model-feature combinations, along with the optimal hyper-parameters for each of them. The best classification performance was observed for a random forest model with the adjacent mean feature, yielding a mean validation score of 90.84% ± 2.46%. We conclude that the floor sensor system is capable of detecting changes in gait by means of pattern recognition techniques applied in the time domain. This suggests that the footprint imaging sensor system is suitable for gait analysis applications ranging from healthcare to security.","","","10.1109/JSEN.2016.2583260","U.K. Engineering and Physical Sciences Research Council through the Knowledge Transfer Scheme; Consejo Nacional de Ciencia y Tecnología; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7496802","Floor sensor;sensor fusion;gait analysis;pattern recognition;machine learning","Sensor systems;Legged locomotion;Imaging;Intelligent sensors;Floors;Optical fibers","biomedical optical imaging;fibre optic sensors;gait analysis;image classification;learning (artificial intelligence);medical image processing","temporal pattern recognition;gait activities;footprint imaging sensor system;plastic optical fiber technology;time domain sensor data;gait classification experiments;walking;temporal signals;machine learning models;nonlinear model;ensemble model;deep learning model;model-feature combinations;random forest model;healthcare","","15","27","","","","","IEEE","IEEE Journals"
"Driver state estimation by convolutional neural network using multimodal sensor data","S. Lim; J. H. Yang","Kookmin University, Korea; Kookmin University, Korea","Electronics Letters","","2016","52","17","1495","1497","A driver state estimation algorithm that uses multimodal vehicular and physiological sensor data is proposed. Deep learning is applied to the fused multimodal data rather than each modality being treated as a different feature. A convolutional neural network model is developed and the driver state estimation algorithm is implemented using Google TensorFlow. The results show that deep learning is a very promising approach for driver state estimation compared with previously studied algorithms, such as dynamic Bayesian networks.","","","10.1049/el.2016.1393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7535149","","","driver information systems;learning (artificial intelligence);neural nets;sensor fusion;state estimation","Google TensorFlow;convolutional neural network model;deep learning;physiological sensor data;multimodal vehicular data;driver state estimation algorithm;multimodal sensor data","","3","6","","","","","IET","IET Journals"
"Understanding Innovation Engines: Automated Creativity and Improved Stochastic Optimization via Deep Learning","A. Nguyen; J. Yosinski; J. Clune","University of Wyoming anguyen8@uwyo.edu; Cornell University & Geometric Intelligence jason@geometricintelligence.com; University of Wyoming jeffclune@uwyo.edu","Evolutionary Computation","","2016","24","3","545","572","The Achilles Heel of stochastic optimization algorithms is getting trapped on local optima. Novelty Search mitigates this problem by encouraging exploration in all interesting directions by replacing the performance objective with a reward for novel behaviors. This reward for novel behaviors has traditionally required a human-crafted, behavioral distance function. While Novelty Search is a major conceptual breakthrough and outperforms traditional stochastic optimization on certain problems, it is not clear how to apply it to challenging, high-dimensional problems where specifying a useful behavioral distance function is difficult. For example, in the space of images, how do you encourage novelty to produce hawks and heroes instead of endless pixel static? Here we propose a new algorithm, the Innovation Engine, that builds on Novelty Search by replacing the human-crafted behavioral distance with a Deep Neural Network (DNN) that can recognize interesting differences between phenotypes. The key insight is that DNNs can recognize similarities and differences between phenotypes at an abstract level, wherein novelty means interesting novelty. For example, a DNN-based novelty search in the image space does not explore in the low-level pixel space, but instead creates a pressure to create new types of images (e.g., churches, mosques, obelisks, etc.). Here, we describe the long-term vision for the Innovation Engine algorithm, which involves many technical challenges that remain to be solved. We then implement a simplified version of the algorithm that enables us to explore some of the algorithm’s key motivations. Our initial results, in the domain of images, suggest that Innovation Engines could ultimately automate the production of endless streams of interesting solutions in any domain: for example, producing intelligent software, robot controllers, optimized physical components, and art.","","","10.1162/EVCO_a_00189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7570499","Genetic algorithms;deep neural networks;CPPNs;MAP-Elites","","","","","4","","","","","","MITP",""
"A Joint Training Framework for Robust Automatic Speech Recognition","Z. Wang; D. Wang","Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","4","796","806","Robustness against noise and reverberation is critical for ASR systems deployed in real-world environments. In robust ASR, corrupted speech is normally enhanced using speech separation or enhancement algorithms before recognition. This paper presents a novel joint training framework for speech separation and recognition. The key idea is to concatenate a deep neural network (DNN) based speech separation frontend and a DNN-based acoustic model to build a larger neural network, and jointly adjust the weights in each module. This way, the separation frontend is able to provide enhanced speech desired by the acoustic model and the acoustic model can guide the separation frontend to produce more discriminative enhancement. In addition, we apply sequence training to the jointly trained DNN so that the linguistic information contained in the acoustic and language models can be back-propagated to influence the separation frontend at the training stage. To further improve the robustness, we add more noise- and reverberation-robust features for acoustic modeling. At the test stage, utterance-level unsupervised adaptation is performed to adapt the jointly trained network by learning a linear transformation of the input of the separation frontend. The resulting sequence-discriminative jointly-trained multistream system with run-time adaptation achieves 10.63% average word error rate (WER) on the test set of the reverberant and noisy CHiME-2 dataset (task-2), which represents the best performance on this dataset and a 22.75% error reduction over the best existing method.","","","10.1109/TASLP.2016.2528171","NSF; AFOSR; Ohio Supercomputer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403942","CHiME-2;deep neural networks (DNN);joint training;robust automatic speech recognition;speech separation;time-frequency masking;CHiME-2;deep neural networks (DNN);joint training;robust automatic speech recognition;speech separation;time-frequency masking","Speech;Acoustics;Training;Speech recognition;Robustness;Noise measurement;Spectrogram","backpropagation;neural nets;speech enhancement;speech recognition;unsupervised learning","joint training framework;robust automatic speech recognition;ASR systems;speech enhancement algorithm;deep neural network based speech separation frontend;DNN- based acoustic model;acoustic modeling;noise-robust features;reverberation-robust features;utterance-level unsupervised adaptation;sequence-discriminative jointly-trained multistream system;run-time adaptation;word error rate;noisy CHiME-2 dataset","","35","58","","","","","IEEE","IEEE Journals"
"Multi-View 3D Object Retrieval With Deep Embedding Network","H. Guo; J. Wang; Y. Gao; J. Li; H. Lu","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; Key Laboratory for Information System Security, Ministry of Education, Tsinghua National Laboratory for Information Science and Technology (TNList), School of Software, Tsinghua University, Beijing, China; Beijing Engineering Research Center for IoT Software and Systems, School of Software Engineering, Beijing University of Technology, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences","IEEE Transactions on Image Processing","","2016","25","12","5526","5537","In multi-view 3D object retrieval, each object is characterized by a group of 2D images captured from different views. Rather than using hand-crafted features, in this paper, we take advantage of the strong discriminative power of convolutional neural network to learn an effective 3D object representation tailored for this retrieval task. Specifically, we propose a deep embedding network jointly supervised by classification loss and triplet loss to map the high-dimensional image space into a low-dimensional feature space, where the Euclidean distance of features directly corresponds to the semantic similarity of images. By effectively reducing the intra-class variations while increasing the inter-class ones of the input images, the network guarantees that similar images are closer than dissimilar ones in the learned feature space. Besides, we investigate the effectiveness of deep features extracted from different layers of the embedding network extensively and find that an efficient 3D object representation should be a tradeoff between global semantic information and discriminative local characteristics. Then, with the set of deep features extracted from different views, we can generate a comprehensive description for each 3D object and formulate the multi-view 3D object retrieval as a set-to-set matching problem. Extensive experiments on SHREC'15 data set demonstrate the superiority of our proposed method over the previous state-of-the-art approaches with over 12% performance improvement.","","","10.1109/TIP.2016.2609814","863 Program; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7569026","Convolutional neural network;multi-view 3D object retrieval;triplet loss","Three-dimensional displays;Feature extraction;Two dimensional displays;Solid modeling;Shape;Semantics;Visualization","convolution;feature extraction;image capture;image classification;image matching;image representation;image retrieval;neural nets","multiview 3D object retrieval;deep embedding network;captured 2D images;convolutional neural network;3D object representation;classification loss;triplet loss;high-dimensional image space;low-dimensional feature space;Euclidean distance;image semantic similarity;input images intraclass variations;input images interclass variations;deep feature extraction;set-to-set matching problem;SHREC'15 data set","","25","46","","","","","IEEE","IEEE Journals"
"Exploring Representation Learning With CNNs for Frame-to-Frame Ego-Motion Estimation","G. Costante; M. Mancini; P. Valigi; T. A. Ciarfuglia","Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy; Department of Engineering, University of Perugia, Perugia, Italy","IEEE Robotics and Automation Letters","","2016","1","1","18","25","Visual ego-motion estimation, or briefly visual odometry (VO), is one of the key building blocks of modern SLAM systems. In the last decade, impressive results have been demonstrated in the context of visual navigation, reaching very high localization performance. However, all ego-motion estimation systems require careful parameter tuning procedures for the specific environment they have to work in. Furthermore, even in ideal scenarios, most state-of-the-art approaches fail to handle image anomalies and imperfections, which results in less robust estimates. VO systems that rely on geometrical approaches extract sparse or dense features and match them to perform frame-to-frame (F2F) motion estimation. However, images contain much more information that can be used to further improve the F2F estimation. To learn new feature representation, a very successful approach is to use deep convolutional neural networks. Inspired by recent advances in deep networks and by previous work on learning methods applied to VO, we explore the use of convolutional neural networks to learn both the best visual features and the best estimator for the task of visual ego-motion estimation. With experiments on publicly available datasets, we show that our approach is robust with respect to blur, luminance, and contrast anomalies and outperforms most state-of-the-art approaches even in nominal conditions.","","","10.1109/LRA.2015.2505717","NVIDIA Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7347378","Visual Learning;Visual-Based Navigation;Visual Learning;Visual-Based Navigation","Feature extraction;Motion estimation;Simultaneous localization and mapping;Neural networks","","","","54","28","","","","","IEEE","IEEE Journals"
"Fingerprint Liveness Detection Using Convolutional Neural Networks","R. F. Nogueira; R. de Alencar Lotufo; R. Campos Machado","Department of Computer Science, New York University, New York, NY, USA; Department of Electrical and Computer Engineering, University of Campinas, Campinas, Brazil; Center for Information Technology Renato Archer, Campinas, Brazil","IEEE Transactions on Information Forensics and Security","","2016","11","6","1206","1213","With the growing use of biometric authentication systems in the recent years, spoof fingerprint detection has become increasingly important. In this paper, we use convolutional neural networks (CNNs) for fingerprint liveness detection. Our system is evaluated on the data sets used in the liveness detection competition of the years 2009, 2011, and 2013, which comprises almost 50 000 real and fake fingerprints images. We compare four different models: two CNNs pretrained on natural images and fine-tuned with the fingerprint images, CNN with random weights, and a classical local binary pattern approach. We show that pretrained CNNs can yield the state-of-the-art results with no need for architecture or hyperparameter selection. Data set augmentation is used to increase the classifiers performance, not only for deep architectures but also for shallow ones. We also report good accuracy on very small training sets (400 samples) using these large pretrained networks. Our best model achieves an overall rate of 97.1% of correctly classified samples-a relative improvement of 16% in test error when compared with the best previously published results. This model won the first prize in the fingerprint liveness detection competition 2015 with an overall accuracy of 95.5%.","","","10.1109/TIFS.2016.2520880","Conselho Nacional de Desenvolvimento Científico e Tecnológico; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390065","Fingerprint recognition;machine learning;supervised learning;neural networks","Feature extraction;Support vector machines;Principal component analysis;Kernel;Convolution;Fingerprint recognition;Neural networks","feedforward neural nets;fingerprint identification;learning (artificial intelligence);object detection","fingerprint liveness detection;convolutional neural networks;biometric authentication systems;data sets;random weights;local binary pattern approach;fingerprint recognition;machine learning;supervised learning","","80","45","","","","","IEEE","IEEE Journals"
"Artificial Vision Techniques to Optimize Strawberry's Industrial Classification","P. Constante; A. Gordon; O. Chang; E. Pruna; F. Acuna; I. Escobar","ESPE, Univ. de las Fuerzas Armadas, Sangolqui, Ecuador; ESPE, Univ. de las Fuerzas Armadas, Sangolqui, Ecuador; ESPE, Univ. de las Fuerzas Armadas, Sangolqui, Ecuador; ESPE, Univ. de las Fuerzas Armadas, Sangolqui, Ecuador; ESPE, Univ. de las Fuerzas Armadas, Sangolqui, Ecuador; ESPE, Univ. de las Fuerzas Armadas, Sangolqui, Ecuador","IEEE Latin America Transactions","","2016","14","6","2576","2581","This research presents novel artificial vision techniques applied to the detection of features for strawberries used in the food industry. For this purpose, a computer vision system based in artificial neural networks is used, organized as a deep architecture and trained with noise compensated learning. This combination originates a strong network - object relations which makes possible the recognition of complex strawberry features under changing conditions of lightning, size and orientation. The programming uses OpenCV libraries and fruits databases captured with a webcam. The images used to train the Artificial Neural Network are defined with canny edge detection and a moving region of interest (ROI). After training, the network recognizes important features such as shape, color and anomalies. The system has been tested in real time with real images.","","","10.1109/TLA.2016.7555221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7555221","Artificial intelligence;Artificial neural networks;Image processing;Machine vision","Webcams;Color;Neural networks;Image color analysis;Visualization;Fluorescence;High definition video","computer vision;edge detection;feature extraction;food processing industry;image classification;learning (artificial intelligence);neural nets","artificial vision technique;strawberry industrial classification;feature detection;food industry;computer vision system;artificial neural network;deep architecture;noise compensated learning;complex strawberry feature recognition;OpenCV libraries;fruit database;webcam;canny edge detection;moving region of interest;moving ROI","","2","","","","","","IEEE","IEEE Journals"
"HCP: A Flexible CNN Framework for Multi-Label Image Classification","Y. Wei; W. Xia; M. Lin; J. Huang; B. Ni; J. Dong; Y. Zhao; S. Yan","Institute of Information Science, Beijing Jiaotong University, Beijing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electronic Engineering, Shanghai Jiaotong University, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","9","1901","1907","Convolutional Neural Network (CNN) has demonstrated promising performance in single-label image classification tasks. However, how CNN best copes with multi-label images still remains an open problem, mainly due to the complex underlying object layouts and insufficient multi-label training images. In this work, we propose a flexible deep CNN infrastructure, called Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment hypotheses are taken as the inputs, then a shared CNN is connected with each hypothesis, and finally the CNN output results from different hypotheses are aggregated with max pooling to produce the ultimate multi-label predictions. Some unique characteristics of this flexible deep CNN infrastructure include: 1) no ground-truth bounding box information is required for training; 2) the whole HCP infrastructure is robust to possibly noisy and/or redundant hypotheses; 3) the shared CNN is flexible and can be well pre-trained with a large-scale single-label image dataset, e.g., ImageNet; and 4) it may naturally output multi-label prediction results. Experimental results on Pascal VOC 2007 and VOC 2012 multi-label image datasets well demonstrate the superiority of the proposed HCP infrastructure over other state-of-the-arts. In particular, the mAP reaches 90.5% by HCP only and 93.2% after the fusion with our complementary result in [12] based on hand-crafted features on the VOC 2012 dataset.","","","10.1109/TPAMI.2015.2491929","National Basic Research Program of China; Fundamental Scientific Research Project; National NSF of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7305792","Deep Learning;CNN;Multi-label Classification","Training;Noise measurement;Neural networks;Image edge detection;Robustness;Fuses;Predictive models","image classification;image fusion;learning (artificial intelligence);neural nets","multilabel image classification;convolutional neural network;flexible deep CNN infrastructure;hypotheses-CNN-pooling;object segment hypotheses;multilabel predictions;HCP infrastructure;large-scale single-label image dataset;multilabel image datasets","","133","44","","","","","IEEE","IEEE Journals"
"Text-Attentional Convolutional Neural Network for Scene Text Detection","T. He; W. Huang; Y. Qiao; J. Yao","Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Image Processing","","2016","25","6","2529","2541","Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature globally computed from a whole image component (patch), where the cluttered background information may dominate true text features in the deep representation. This leads to less discriminative power and poorer robustness. In this paper, we present a new system for scene text detection by proposing a novel text-attentional convolutional neural network (Text-CNN) that particularly focuses on extracting text-related regions and features from the image components. We develop a new learning mechanism to train the Text-CNN with multi-level and rich supervised information, including text region mask, character label, and binary text/non-text information. The rich supervision information enables the Text-CNN with a strong capability for discriminating ambiguous texts, and also increases its robustness against complicated background components. The training process is formulated as a multi-task learning problem, where low-level supervised information greatly facilitates the main task of text/non-text classification. In addition, a powerful low-level detector called contrast-enhancement maximally stable extremal regions (MSERs) is developed, which extends the widely used MSERs by enhancing intensity contrast between text patterns and background. This allows it to detect highly challenging text patterns, resulting in a higher recall. Our approach achieved promising results on the ICDAR 2013 data set, with an F-measure of 0.82, substantially improving the state-of-the-art results.","","","10.1109/TIP.2016.2547588","National Natural Science Foundation of China; Guangdong Natural Science Foundation; Shenzhen Research Program; National High-Tech Research and Development Program of China; Guangdong Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442550","Maximally Stable Extremal Regions;text detector;convolutional neural networks;multi-level supervised information;multi-task learning;Maximally stable extremal regions;text detector;convolutional neural networks;multi-level supervised information;multi-task learning","Detectors;Feature extraction;Robustness;Computational modeling;Neural networks;Text recognition;Training","image enhancement;neural nets;text detection","text-attentional convolutional neural network;scene text detection;deep learning models;text region mask;character label;binary text/non-text information;ambiguous texts;contrast-enhancement maximally stable extremal regions;ICDAR 2013 data set","Algorithms;Humans;Natural Language Processing;Neural Networks (Computer)","96","63","","","","","IEEE","IEEE Journals"
"Structural Design of Convolutional Neural Networks for Steganalysis","G. Xu; H. Wu; Y. Shi","Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA; Southwest Jiaotong University, Chengdu, China; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA","IEEE Signal Processing Letters","","2016","23","5","708","712","Recent studies have indicated that the architectures of convolutional neural networks (CNNs) tailored for computer vision may not be best suited to image steganalysis. In this letter, we report a CNN architecture that takes into account knowledge of steganalysis. In the detailed architecture, we take absolute values of elements in the feature maps generated from the first convolutional layer to facilitate and improve statistical modeling in the subsequent layers; to prevent overfitting, we constrain the range of data values with the saturation regions of hyperbolic tangent (TanH) at early stages of the networks and reduce the strength of modeling using 1×1 convolutions in deeper layers. Although it learns from only one type of noise residual, the proposed CNN is competitive in terms of detection performance compared with the SRM with ensemble classifiers on the BOSSbase for detecting S-UNIWARD and HILL. The results have implied that well-designed CNNs have the potential to provide a better detection performance in the future.","","","10.1109/LSP.2016.2548421","NJIT Faculty Seed Grant Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444146","Convolutional neural networks (CNN);deep learning;steganalysis;forensics;Convolutional neural networks (CNNs);deep learning;forensics;steganalysis","Convolution;Kernel;Mathematical model;Data models;Neural networks;Computer architecture;Error analysis","image processing;neural nets;steganography","convolutional neural networks;CNN structural design;computer vision;image steganalysis;CNN architecture;feature maps;convolutional layer;statistical modeling;hyperbolic tangent saturation region;noise residual;SRM;ensemble classifiers;BOSSbase;S-UNIWARD detection;HILL detection","","115","37","","","","","IEEE","IEEE Journals"
"Support Tool for the Combined Software/Hardware Design of On-Chip ELM Training for SLFF Neural Networks","M. Bataller-Mompeán; J. M. Martínez-Villena; A. Rosado-Muñoz; J. V. Francés-Víllora; J. F. Guerrero-Martínez; M. Wegrzyn; M. Adamski","Group for Digital Design and Processing (GDDP), Department of Electronic Engineering, Universitat de Valencia, Valencia, Spain; Group for Digital Design and Processing (GDDP), Department of Electronic Engineering, Universitat de Valencia, Valencia, Spain; Group for Digital Design and Processing (GDDP), Department of Electronic Engineering, Universitat de Valencia, Valencia, Spain; Group for Digital Design and Processing (GDDP), Department of Electronic Engineering, Universitat de Valencia, Valencia, Spain; Group for Digital Design and Processing (GDDP), Department of Electronic Engineering, Universitat de Valencia, Valencia, Spain; Group for Digital Design and Processing (GDDP), Department of Electronic Engineering, Universitat de Valencia, Valencia, Spain; Group for Digital Design and Processing (GDDP), Department of Electronic Engineering, Universitat de Valencia, Valencia, Spain","IEEE Transactions on Industrial Informatics","","2016","12","3","1114","1123","Typically, hardware implemented neural networks are trained before implementation. Extreme learning machine (ELM) is a noniterative training method for single-layer feed-forward (SLFF) neural networks well suited for hardware implementation. It provides fixed-time learning and simplifies retraining of a neural network once implemented, which is very important in applications demanding on-chip training. This study proposes the data flow of a software support tool in the design process of a hardware implementation of on-chip ELM learning for SLFF neural networks. The software tool allows the user to obtain the optimal definition of functional and hardware parameters for any application, and enables the user to interact throughout the design process. Combining in a transparent way for the user, simulation and Xilinx synthesis tools, the tool recommends the optimal configuration, generating, finally, a synthesizable IP-core. As application, the field-programmable gate array implementation for real-time detection of brain areas in electrode positioning during a deep brain stimulation surgery is described. The generated IP-core can execute a peak of 95 ELM trainings per second on a low-cost Spartan 6 device, making possible its real-time use in this application.","","","10.1109/TII.2016.2554521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7452636","Embedded systems;Hardware implementation;Extreme Learning Machine (ELM);on-chip ELM Training;Software Design Environment;Neural Network hardware design;FPGA;Embedded systems;extreme learning machine (ELM);field-programmable gate array (FPGA);hardware implementation;neural network hardware design;on-chip ELM training;software design environment","Hardware;Training;Biological neural networks;Topology;Software tools;System-on-chip","field programmable gate arrays;hardware-software codesign;neural nets","combined software/hardware design;on-chip ELM training;SLFF neural networks;extreme learning machine;noniterative training method;single-layer feed-forward neural networks;fixed-time learning;on-chip training;software support tool;on-chip ELM learning;software tool;optimal definition;Xilinx synthesis tools;synthesizable IP-core;field programmable gate array;electrode positioning;brain stimulation surgery;Spartan 6 device","","5","23","","","","","IEEE","IEEE Journals"
"GSNs: generative stochastic networks","G. Alain; Y. Bengio; L. Yao; J. Yosinski; É. Thibodeau-Laufer; S. Zhang; P. Vincent","NA; NA; NA; NA; NA; NA; NA","Information and Inference: A Journal of the IMA","","2016","5","2","210","249","We introduce a novel training principle for generative probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSNs) framework generalizes Denoising Auto-Encoders (DAEs), and is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution is a conditional distribution that generally involves a small move, so it has fewer dominant modes and is unimodal in the limit of small moves. This simplifies the learning problem, making it less like density estimation and more akin to supervised function approximation, with gradients that can be obtained by backprop. The theorems provided here provide a probabilistic interpretation for DAEs and generalize them; seen in the context of this framework, auto-encoders that learn with injected noise are a special case of GSNs and can be interpreted as generative models. The theorems also provide an interesting justification for dependency networks and generalized pseudolikelihood, and define an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the others. Experiments validating these theoretical results are conducted on both synthetic datasets and image datasets. The experiments employ a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler, but that allows training to proceed with backprop through a recurrent neural network with noise injected inside and without the need for layerwise pretraining.","","","10.1093/imaiai/iaw003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8132286","deep learning;auto-encoders;generative models","","","","","1","","","","","","OUP","OUP Journals"
"Stacked Sparse Autoencoder in PolSAR Data Classification Using Local Spatial Information","L. Zhang; W. Ma; D. Zhang","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi'an, China","IEEE Geoscience and Remote Sensing Letters","","2016","13","9","1359","1363","Terrain classification is an important topic in polarimetric synthetic aperture radar (PolSAR) image processing. Among various classification techniques, the stacked sparse autoencoder (SSAE) is a kind of deep learning method that can automatically learn useful features layer by layer in an unsupervised manner. However, the scattering measurements of individual pixels in PolSAR images are affected by the speckle; hence, the performance of pixel-based classification approaches would be poor. In this situation, a novel framework is proposed to learn robust features of PolSAR data. The local spatial information is introduced into SSAE to learn the deep spatial sparse features automatically for the first time. Furthermore, the influences of the neighbor pixels on the central pixel are controlled depending on the spatial distances from the neighbor pixels to the central pixel. Experimental results with fully PolSAR data indicate that the proposed method provides a competitive solution.","","","10.1109/LGRS.2016.2586109","National Basic Research Program of China; National Natural Science Foundation of China; Fund for Foreign Scholars in University Research and Teaching Programs; Major Research Plan of the National Natural Science Foundation of China; Program for Cheung Kong Scholars and Innovative Research Team in University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518668","Deep learning;image classification;local spatial information;polarimetric synthetic aperture radar (PolSAR);sparse;stacked sparse autoencoder (SSAE)","Feature extraction;Training;Speckle;Cost function;Machine learning;Data mining;Scattering","geophysical image processing;image classification;remote sensing by radar;synthetic aperture radar;terrain mapping","pixel-based classification;PolSAR images;classification techniques;PolSAR image processing;polarimetric synthetic aperture radar;terrain classification;local spatial information;PolSAR data classification;stacked sparse autoencoder","","39","11","","","","","IEEE","IEEE Journals"
"A Fast and Accurate Unconstrained Face Detector","S. Liao; A. K. Jain; S. Z. Li","National Laboratory of Pattern Recognition and the Center for Biometrics and Security Research, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI; National Laboratory of Pattern Recognition and the Center for Biometrics and Security Research, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2016","38","2","211","223","We propose a method to address challenges in unconstrained face detection, such as arbitrary pose variations and occlusions. First, a new image feature called Normalized Pixel Difference (NPD) is proposed. NPD feature is computed as the difference to sum ratio between two pixel values, inspired by the Weber Fraction in experimental psychology. The new feature is scale invariant, bounded, and is able to reconstruct the original image. Second, we propose a deep quadratic tree to learn the optimal subset of NPD features and their combinations, so that complex face manifolds can be partitioned by the learned rules. This way, only a single soft-cascade classifier is needed to handle unconstrained face detection. Furthermore, we show that the NPD features can be efficiently obtained from a look up table, and the detection template can be easily scaled, making the proposed face detector very fast. Experimental results on three public face datasets (FDDB, GENKI, and CMU-MIT) show that the proposed method achieves state-of-the-art performance in detecting unconstrained faces with arbitrary pose variations and occlusions in cluttered scenes.","","","10.1109/TPAMI.2015.2448075","NSFC; National Science and Technology Support Program; CAS Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7130626","Unconstrained face detection;normalized pixel difference;regression tree;AdaBoost;cascade classifier;Unconstrained face detection;normalized pixel difference;deep quadratic tree;AdaBoost;cascade classifier","Face;Face detection;Feature extraction;Detectors;Training;Robustness;Image reconstruction","face recognition;image classification;image reconstruction;object detection;table lookup;trees (mathematics)","unconstrained face detection;image feature;normalized pixel difference;NPD feature;difference-to-sum ratio;Weber fraction;experimental psychology;scale invariant feature;bounded feature;image reconstruction;deep quadratic tree;complex face manifolds;soft-cascade classifier;look up table;detection template","Algorithms;Biometric Identification;Databases, Factual;Face;Humans;Image Processing, Computer-Assisted;Machine Learning;ROC Curve;Software","116","61","","","","","IEEE","IEEE Journals"
"Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks","K. Zhang; Z. Zhang; Z. Li; Y. Qiao","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China","IEEE Signal Processing Letters","","2016","23","10","1499","1503","Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations, and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this letter, we propose a deep cascaded multitask framework that exploits the inherent correlation between detection and alignment to boost up their performance. In particular, our framework leverages a cascaded architecture with three stages of carefully designed deep convolutional networks to predict face and landmark location in a coarse-to-fine manner. In addition, we propose a new online hard sample mining strategy that further improves the performance in practice. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging face detection dataset and benchmark and WIDER FACE benchmarks for face detection, and annotated facial landmarks in the wild benchmark for face alignment, while keeps real-time performance.","","","10.1109/LSP.2016.2603342","External Cooperation Program of BIC; Chinese Academy of Sciences; Shenzhen Research Program; Guangdong Research Program; Natural Science Foundation of Guangdong Province; Key Laboratory of Human Machine Intelligence-Synergy Systems; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7553523","Cascaded convolutional neural network (CNN);face alignment;face detection","Face;Face detection;Training;Convolution;Detectors;Computer architecture;Benchmark testing","data mining;face recognition;learning (artificial intelligence)","annotated facial landmark;detection benchmark;detection dataset;WIDER FACE benchmark;state-of-the-art technique;online hard sample mining strategy;coarse-to-fine manner;landmark location prediction;face location prediction;deep cascaded multitask framework;deep learning approach;unconstrained environment;multitask cascaded convolutional network;joint face detection and alignment","","603","30","","","","","IEEE","IEEE Journals"
"NeRD: A Neural Response Divergence Approach to Visual Saliency Detection","M. J. Shafiee; P. Siva; C. Scharfenberger; P. Fieguth; A. Wong","Department of Systems Design Engineering, University of Waterloo, Waterloo, ON, Canada; Aimetis Corporation, Waterloo, ON, Canada; Department of Systems Design Engineering, University of Waterloo, Waterloo, ON, Canada; Department of Systems Design Engineering, University of Waterloo, Waterloo, ON, Canada; Department of Systems Design Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Signal Processing Letters","","2016","23","10","1404","1408","In this letter, a novel approach to visual saliency detection via neural response divergence (NeRD) is proposed, where synaptic portions of deep neural networks, previously trained for complex object recognition, are leveraged to compute low-level cues that can be used to compute image region distinctiveness. Based on this concept, an efficient visual saliency detection framework is proposed using deep convolutional StochasticNets. Experimental results using complex scene saliency dataset and MSRA10k natural image datasets show that the proposed NeRD approach can achieve improved performance when compared to state-of-the-art image saliency approaches, while attaining low computational complexity necessary for near-real-time computer vision applications.","","","10.1109/LSP.2016.2600592","Natural Sciences and Engineering Research Council of Canada; Canada Research Chairs Program; Ontario Ministry of Research and Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7544474","Deep learning;neural response distinctiveness;neural response divergence (NeRD);saliency detection;StochasticNet","Object recognition;Visualization;Biological neural networks;Computational complexity;Convolution;Computational modeling;Feature extraction","computer vision;neural nets;object detection;object recognition","visual saliency detection;neural response divergence approach;NeRD;synaptic portions;deep neural networks;complex object recognition;image region distinctiveness;deep convolutional stochasticnets;complex scene saliency dataset;MSRA10k natural image datasets;computational complexity;near-real-time computer vision","","","48","","","","","IEEE","IEEE Journals"
"Seismic features and automatic discrimination of deep and shallow induced-microearthquakes using neural network and logistic regression","S. M. Mousavi; S. P. Horton; C. A. Langston; B. Samei","NA; NA; NA; NA","Geophysical Journal International","","2016","207","1","29","46","We develop an automated strategy for discriminating deep microseismic events from shallow ones on the basis of the waveforms recorded on a limited number of surface receivers. Machine-learning techniques are employed to explore the relationship between event hypocentres and seismic features of the recorded signals in time, frequency and time–frequency domains. We applied the technique to 440 microearthquakes−1.7<Mw<1.29, induced by an underground cavern collapse in the Napoleonville Salt Dome in Bayou Corne, Louisiana. Forty different seismic attributes of whole seismograms including degree of polarization and spectral attributes were measured. A selected set of features was then used to train the system to discriminate between deep and shallow events based on the knowledge gained from existing patterns. The cross-validation test showed that events with depth shallower than 250m can be discriminated from events with hypocentral depth between 1000 and 2000m with 88 per cent and 90.7 per cent accuracy using logistic regression and artificial neural network models, respectively. Similar results were obtained using single station seismograms. The results show that the spectral features have the highest correlation to source depth. Spectral centroids and 2-D cross-correlations in the time–frequency domain are two new seismic features used in this study that showed to be promising measures for seismic event classification. The used machine-learning techniques have application for efficient automatic classification of low energy signals recorded at one or more seismic stations.","","","10.1093/gji/ggw258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8146787","Neural networks, fuzzy logic;Wavelet transform;Earthquake source observations;Seismic monitoring and test-ban treaty verification;Volcano seismology","","","","","4","","","","","","OUP","OUP Journals"
"Automatic Environmental Sound Recognition: Performance Versus Computational Cost","S. Sigtia; A. M. Stark; S. Krstulović; M. D. Plumbley","Queen Mary University of London, London, U.K.; Queen Mary University of London, London, U.K.; Audio Analytic Ltd., Cambridge, U.K.; University of Surrey, Guildford, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","11","2096","2107","In the context of the Internet of Things, sound sensing applications are required to run on embedded platforms where notions of product pricing and form factor impose hard constraints on the available computing power. Whereas Automatic Environmental Sound Recognition (AESR) algorithms are most often developed with limited consideration for computational cost, this paper seeks which AESR algorithm can make the most of a limited amount of computing power by comparing the sound classification performance as a function of its computational cost. Results suggest that Deep Neural Networks yield the best ratio of sound classification accuracy across a range of computational costs, while Gaussian Mixture Models offer a reasonable accuracy at a consistently small cost, and Support Vector Machines stand between both in terms of compromise between accuracy and computational cost.","","","10.1109/TASLP.2016.2592698","Innovate U.K.; U.K. Engineering and Physical Sciences Research Council; Audio Analytic Ltd., Cambridge, U.K.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515194","Automatic environmental sound recognition;computational auditory scene analysis;deep learning;machine learning","Computational efficiency;Speech;Speech recognition;Acoustics;Internet of things;IEEE transactions;Speech processing","acoustic signal processing;Gaussian processes;mixture models;signal classification;speech recognition;support vector machines","automatic speech recognition;support vector machines;Gaussian mixture models;deep neural networks;sound classification performance;AESR algorithms;automatic environmental sound recognition algorithm;form factor;product pricing;embedded platforms;sound sensing;Internet of Things;performance cost;computational cost function","","20","57","","","","","IEEE","IEEE Journals"
"Detection of Fragmented Rectangular Enclosures in Very High Resolution Remote Sensing Images","I. Zingman; D. Saupe; O. A. B. Penatti; K. Lambers","Department of Computer and Information Science, University of Konstanz, Konstanz, Germany; Department of Computer and Information Science, University of Konstanz, Konstanz, Germany; Advanced Technologies Group, Samsung Research Institute, Campinas, SP, Brazil; Faculty of Archaeology, Leiden University, CC Leiden, The Netherlands","IEEE Transactions on Geoscience and Remote Sensing","","2016","54","8","4580","4593","We develop an approach for the detection of ruins of livestock enclosures (LEs) in alpine areas captured by high-resolution remotely sensed images. These structures are usually of approximately rectangular shape and appear in images as faint fragmented contours in complex background. We address this problem by introducing a rectangularity feature that quantifies the degree of alignment of an optimal subset of extracted linear segments with a contour of rectangular shape. The rectangularity feature has high values not only for perfectly regular enclosures but also for ruined ones with distorted angles, fragmented walls, or even a completely missing wall. Furthermore, it has a zero value for spurious structures with less than three sides of a perceivable rectangle. We show how the detection performance can be improved by learning a linear combination of the rectangularity and size features from just a few available representative examples and a large number of negatives. Our approach allowed detection of enclosures in the Silvretta Alps that were previously unknown. A comparative performance analysis is provided. Among other features, our comparison includes the state-of-the-art features that were generated by pretrained deep convolutional neural networks (CNNs). The deep CNN features, although learned from a very different type of images, provided the basic ability to capture the visual concept of the LEs. However, our handcrafted rectangularity-size features showed considerably higher performance.","","","10.1109/TGRS.2016.2545919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7452408","Deep features;incomplete rectangles;man-made structures;maximal cliques;object detection;rectangularity feature;Deep features;incomplete rectangles;man-made structures;maximal cliques;object detection;rectangularity feature","Feature extraction;Image edge detection;Buildings;Shape;Remote sensing;Image resolution;Detectors","feature extraction;geophysical image processing;remote sensing","fragmented rectangular enclosure detection;very high resolution remote sensing images;livestock enclosures;rectangular shape;faint fragmented contours;complex background;extracted linear segments;distorted angles;fragmented walls;spurious structures;Silvretta Alps;state-of-the-art features;pretrained deep convolutional neural networks","","8","71","","","","","IEEE","IEEE Journals"
"Maximum Likelihood Nonlinear Transformations Based on Deep Neural Networks","X. Cui; V. Goel","IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","11","2023","2031","Feature transformations are commonly used in speech recognition to account for distribution mismatches between the source and target domains (also referred to as covariate shift). Linear (affine) or piecewise linear transformations are typically considered. In this paper, we present deep neural network (DNN) based nonlinear feature transformations estimated under the maximum likelihood criterion. We use the hidden Markov model (HMM) to model speech feature sequences and features in each HMM state assume a Gaussian mixture model (GMM) distribution. The network is pre-trained close to a linear transformation followed by a fine-tuning using the gradient descent algorithm. Due to the nonlinearity, the gradients and the partition functions of GMM-HMM state distributions are evaluated using the Monte Carlo (MC) method based on importance sampling. In addition, a deep stacked architecture is proposed to hierarchically build a DNN as a series of sub-networks with each representing a nonlinear transformation itself, which can be learned using a block-wise learning strategy. Applications of the proposed nonlinear transformations in speaker/environment adaptation and acoustic modeling in large vocabulary continuous speech recognition tasks show its superior performance over the widely-used constrained maximum likelihood linear regression (CMLLR).","","","10.1109/TASLP.2016.2594255","Intelligence Advanced Research Projects Activity; Department of Defense U.S. Army Research Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523300","Automatic speech recognition;deep neural network;importance sampling;monte Carlo;nonlinear transformation","Hidden Markov models;Neural networks;Monte Carlo methods;Acoustics;Automatic speech recognition;Jacobian matrices","","","","2","40","","","","","IEEE","IEEE Journals"
"Stochastic-Based Deep Convolutional Networks with Reconfigurable Logic Fabric","M. Alawad; M. Lin","Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL; Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL","IEEE Transactions on Multi-Scale Computing Systems","","2016","2","4","242","256","Large-scale convolutional neural network is a fundamental algorithmic building block in many computer vision and artificial intelligence applications that follow the deep learning principle. However, a typically-sized CNN is well known to be computationally intensive. This work presents a novel stochastic-based and scalable hardware architecture and circuit design that computes a large-scale CNN with FPGA. The key idea is to implement all key components of a deep learning CNN, including multi-dimensional convolution, activation, and pooling layers, completely in the probabilistic computing domain in order to achieve high computing robustness, high performance, and low hardware usage. Our approach has three advantages. First, it can achieve significantly lower algorithmic complexity for any given accuracy requirement. For a $N$  dimensional image feature map, we have theoretically proven that a random sample size of  $k^* \log (N)$  is sufficient to achieve no more than 0.05 error at 95 percent confidence level, where $k^*$  is a constant of 510. This computing complexity, when compared with that of conventional multiplier-based architecture, represents on average 8.97$\times$  and 6.98 $\times$  performance improvement for SCNN and Deep SCNN, respectively. Second, this proposed stochastic-based architecture is highly fault-tolerant because the information to be processed is encoded with a large ensemble of random samples. As such, the local perturbations of its computing accuracy will be dissipated globally, thus becoming inconsequential to the final overall results. More interestingly, our measured results have shown that 0.1 percent degradation in computing accuracy of CNN can actually mitigate the well-known overfitting problem. Overall, being highly scalable and energy efficient, our stochastic-based convolutional neural network architecture is well-suited for a modular vision engine with the goal of performing real-time detection, recognition, and segmentation of mega-pixel images, especially those perception-based computing tasks that are inherently fault-tolerant, while still requiring high energy efficiency.","","","10.1109/TMSCS.2016.2601326","ARO DURIP; NSF BRIGE; NSF CCF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7547913","Stochastic convolution;FPGA;convolutional neural network","Convolution;Feature extraction;Computer architecture;Machine learning;Neuromorphics;Field programmable gate arrays;Neural networks;Convolutional neural networks","","","","9","23","","","","","IEEE","IEEE Journals"
"Deep Independence Network Analysis of Structural Brain Imaging: Application to Schizophrenia","E. Castro; R. D. Hjelm; S. M. Plis; L. Dinh; J. A. Turner; V. D. Calhoun","The Mind Research Network, NM, USA; University of New Mexico, NM, USA; The Mind Research Network, NM, USA; Université de Montréal, CAN; Georgia State University, GA, USA; The Mind Research Network, NM, USA","IEEE Transactions on Medical Imaging","","2016","35","7","1729","1740","Linear independent component analysis (ICA) is a standard signal processing technique that has been extensively used on neuroimaging data to detect brain networks with coherent brain activity (functional MRI) or covarying structural patterns (structural MRI). However, its formulation assumes that the measured brain signals are generated by a linear mixture of the underlying brain networks and this assumption limits its ability to detect the inherent nonlinear nature of brain interactions. In this paper, we introduce nonlinear independent component estimation (NICE) to structural MRI data to detect abnormal patterns of gray matter concentration in schizophrenia patients. For this biomedical application, we further addressed the issue of model regularization of nonlinear ICA by performing dimensionality reduction prior to NICE, together with an appropriate control of the complexity of the model and the usage of a proper approximation of the probability distribution functions of the estimated components. We show that our results are consistent with previous findings in the literature, but we also demonstrate that the incorporation of nonlinear associations in the data enables the detection of spatial patterns that are not identified by linear ICA. Specifically, we show networks including basal ganglia, cerebellum and thalamus that show significant differences in patients versus controls, some of which show distinct nonlinear patterns.","","","10.1109/TMI.2016.2527717","National Institute of Mental Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405347","Deep learning;NICE;nonlinear ICA;schizophrenia;structural MRl","Jacobian matrices;Estimation;Magnetic resonance imaging;Brain;Couplings;Probability density function","","","Brain;Brain Mapping;Humans;Magnetic Resonance Imaging;Neuroimaging;Schizophrenia","11","34","","","","","IEEE","IEEE Journals"
"Automatic Segmentation of MR Brain Images With a Convolutional Neural Network","P. Moeskops; M. A. Viergever; A. M. Mendrik; L. S. de Vries; M. J. N. L. Benders; I. Išgum","Image Sciences Institute, University Medical Center Utrecht, The Netherlands; Image Sciences Institute, University Medical Center Utrecht, The Netherlands; Image Sciences Institute, University Medical Center Utrecht, The Netherlands; Department of Neonatology, University Medical Center Utrecht, The Netherlands; Department of Neonatology, University Medical Center Utrecht, The Netherlands; Image Sciences Institute, University Medical Center Utrecht, The Netherlands","IEEE Transactions on Medical Imaging","","2016","35","5","1252","1261","Automatic segmentation in MR brain images is important for quantitative analysis in large-scale studies with images acquired at all ages. This paper presents a method for the automatic segmentation of MR brain images into a number of tissue classes using a convolutional neural network. To ensure that the method obtains accurate segmentation details as well as spatial consistency, the network uses multiple patch sizes and multiple convolution kernel sizes to acquire multi-scale information about each voxel. The method is not dependent on explicit features, but learns to recognise the information that is important for the classification based on training data. The method requires a single anatomical MR image only. The segmentation method is applied to five different data sets: coronal T2-weighted images of preterm infants acquired at 30 weeks postmenstrual age (PMA) and 40 weeks PMA, axial T2-weighted images of preterm infants acquired at 40 weeks PMA, axial T1-weighted images of ageing adults acquired at an average age of 70 years, and T1-weighted images of young adults acquired at an average age of 23 years. The method obtained the following average Dice coefficients over all segmented tissue classes for each data set, respectively: 0.87, 0.82, 0.84, 0.86, and 0.91. The results demonstrate that the method obtains accurate segmentations in all five sets, and hence demonstrates its robustness to differences in age and acquisition protocol.","","","10.1109/TMI.2016.2548501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444155","Adult brain;automatic image segmentation;convolutional neural networks;deep learning;MRI;preterm neonatal brain","Image segmentation;Brain;Pediatrics;Kernel;Convolution;Biomedical imaging;Aging","biological tissues;biomedical MRI;brain;image segmentation;medical image processing;neural nets;neurophysiology;paediatrics","automatic MR brain image segmentation;convolutional neural network;quantitative analysis;spatial consistency;multiple patch sizes;multiple convolution kernel sizes;multiscale information;training data;coronal T2-weighted images;preterm infants;postmenstrual age;axial T2-weighted images;ageing adults;average Dice coefficients;segmented tissue classes;acquisition protocol","Adult;Aged;Brain;Humans;Image Processing, Computer-Assisted;Infant, Newborn;Infant, Premature;Machine Learning;Magnetic Resonance Imaging;Neural Networks (Computer);Young Adult","257","46","","","","","IEEE","IEEE Journals"
"A CNN Regression Approach for Real-Time 2D/3D Registration","S. Miao; Z. J. Wang; R. Liao","Department of Electrical and Computer Engineering, University of British Columbia, Vancouver; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, Canada; Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA","IEEE Transactions on Medical Imaging","","2016","35","5","1352","1363","In this paper, we present a Convolutional Neural Network (CNN) regression approach to address the two major limitations of existing intensity-based 2-D/3-D registration technology: 1) slow computation and 2) small capture range. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the digitally reconstructed radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. An automatic feature extraction step is introduced to calculate 3-D pose-indexed features that are sensitive to the variables to be regressed while robust to other factors. The CNN regressors are then trained for local zones and applied in a hierarchical manner to break down the complex regression task into multiple simpler sub-tasks that can be learned separately. Weight sharing is furthermore employed in the CNN regression model to reduce the memory footprint. The proposed approach has been quantitatively evaluated on 3 potential clinical applications, demonstrating its significant advantage in providing highly accurate real-time 2-D/3-D registration with a significantly enlarged capture range when compared to intensity-based methods.","","","10.1109/TMI.2016.2521800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7393571","2-D/3-D registration;convolutional neural network;deep learning;image guided intervention","X-ray imaging;Feature extraction;Attenuation;Real-time systems;Computed tomography;Biomedical imaging","diagnostic radiography;feature extraction;image reconstruction;image registration;iterative methods;medical image processing;neural nets;optimisation;regression analysis","CNN regression approach;real-time 2D-3D registration;convolutional neural network regression approach;intensity-based 2D-3D registration technology;optimization-based methods;iterative optimization;formation parameters;scalar-valued metric function;digitally reconstructed radiograph;X-ray images;CNN regressors;transformation parameters;automatic feature extraction step;3D pose-indexed features;complex regression task;multiple simpler subtasks;memory footprint;intensity-based methods","Arthroplasty, Replacement, Knee;Echocardiography, Transesophageal;Humans;Imaging, Three-Dimensional;Knee Joint;Machine Learning;Neural Networks (Computer);Radiography;Regression Analysis","105","31","","","","","IEEE","IEEE Journals"
"Pulmonary Nodule Detection in CT Images: False Positive Reduction Using Multi-View Convolutional Networks","A. A. A. Setio; F. Ciompi; G. Litjens; P. Gerke; C. Jacobs; S. J. van Riel; M. M. W. Wille; M. Naqibullah; C. I. Sánchez; B. van Ginneken","Diagnostic Image Analysis Group at the Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands; Diagnostic Image Analysis Group at the Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands; Diagnostic Image Analysis Group at the Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands; Diagnostic Image Analysis Group at the Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands; Diagnostic Image Analysis Group at the Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands; Diagnostic Image Analysis Group at the Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands; Department of Respiratory Medicine, Gentofte Hospital, University of Copenhagen, Hellerup, Denmark; Department of Respiratory Medicine, Gentofte Hospital, University of Copenhagen, Hellerup, Denmark; Diagnostic Image Analysis Group at the Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands; Diagnostic Image Analysis Group at the Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands","IEEE Transactions on Medical Imaging","","2016","35","5","1160","1169","We propose a novel Computer-Aided Detection (CAD) system for pulmonary nodules using multi-view convolutional networks (ConvNets), for which discriminative features are automatically learnt from the training data. The network is fed with nodule candidates obtained by combining three candidate detectors specifically designed for solid, subsolid, and large nodules. For each candidate, a set of 2-D patches from differently oriented planes is extracted. The proposed architecture comprises multiple streams of 2-D ConvNets, for which the outputs are combined using a dedicated fusion method to get the final classification. Data augmentation and dropout are applied to avoid overfitting. On 888 scans of the publicly available LIDC-IDRI dataset, our method reaches high detection sensitivities of 85.4% and 90.1% at 1 and 4 false positives per scan, respectively. An additional evaluation on independent datasets from the ANODE09 challenge and DLCST is performed. We showed that the proposed multi-view ConvNets is highly suited to be used for false positive reduction of a CAD system.","","","10.1109/TMI.2016.2536809","The Netherlands Organization for Scientific Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422783","Computed tomography;computer-aided detection;convolutional networks;deep learning;lung cancer;pulmonary nodule","Design automation;Solids;Cancer;Lungs;Computed tomography;Lesions;Feature extraction","cancer;computerised tomography;feature extraction;image classification;image fusion;medical image processing;tumours","pulmonary nodule detection;CT images;false positive reduction;multiview convolutional networks;computer-aided detection system;discriminative features;training data;nodule candidates;2D patches;differently oriented planes;multiple streams;2D ConvNets;dedicated fusion method;final classification;data augmentation;publicly available LIDC-IDRI dataset;ANODE09 challenge;multiview ConvNets;CAD system","Algorithms;Humans;Lung Neoplasms;Machine Learning;Pattern Recognition, Automated;Radiographic Image Interpretation, Computer-Assisted;Solitary Pulmonary Nodule;Tomography, X-Ray Computed","301","47","","","","","IEEE","IEEE Journals"
"Integration of Optimized Modulation Filter Sets Into Deep Neural Networks for Automatic Speech Recognition","N. Moritz; B. Kollmeier; J. Anemüller","Fraunhofer Institute for Digital Media Technology, Project Group for Hearing, Speech, and Audio Technology, Oldenburg, Germany; Fraunhofer Institute for Digital Media Technology, Project Group for Hearing, Speech, and Audio Technology, Oldenburg, Germany; University of Oldenburg, Oldenburg, Germany","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","12","2439","2452","Inspired by physiological studies on the human auditory system and by results from psychoacoustics, an amplitude modulation filter bank (AMFB) has been developed and successfully applied to feature extraction for automatic speech recognition (ASR) in earlier work. Here, we address the question as to which amplitude modulation (AM) frequency decomposition leads to optimal ASR performance by proposing a parameterized functional relationship between modulation center frequency and modulation bandwidth. Word error rates (WERs) of ASR experiments with 1551 different AMFBs are systematically evaluated and compared, resulting in the identification of a comparatively narrow range of optimal modulation frequency to modulation bandwidth characteristics. To integrate modulation processing with deep neural network (DNN) acoustic modeling, we propose merging of modulation filter coefficients with DNN weights prior to a final training step and an improved mean-variance normalization scheme for AMFBs. These modifications are shown to result in further reduction of WERs and are indicative of the proposed system's improved generalization ability, when compared across corpora of 100-960 h of data with mismatched training and test conditions. Analysis of DNN-learned temporal AM filtering properties is carried out and implications for the relevance of different modulation regions as well as the relation to psychoacoustic findings are discussed. ASR experiments with the proposed system demonstrate a high degree of robustness against extrinsic acoustic distortions, resulting in, e.g., an average WER of 9.79% on the Aurora-4 task.","","","10.1109/TASLP.2016.2615239","Federal Ministry of Education and Research; European Commission; KANTATE; DFG SFB/TRR 31; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582478","Amplitude modulation filter bank;automatic speech recognition;feature extraction;modulation frequency resolution;neural net filter properties","Filter banks;Frequency modulation;Feature extraction;Neural networks;Automatic speech recognition","acoustic distortion;amplitude modulation;channel bank filters;feature extraction;neural nets;speech recognition","automatic speech recognition;modulation filter set;deep neural network;human auditory system;psychoacoustics;amplitude modulation filter bank;AMFB;feature extraction;ASR;amplitude modulation frequency decomposition;AM frequency decomposition;ASR performance;word error rate;WER;DNN acoustic modeling;mean-variance normalization scheme;corpora;DNN-learned temporal AM filtering property analysis;acoustic distortion","","5","55","","","","","IEEE","IEEE Journals"
"Audio Recapture Detection With Convolutional Neural Networks","X. Lin; J. Liu; X. Kang","Guangdong Key Lab of Information Security, School of Electronics and Information Technology, Sun Yat-Sen University, Guangzhou, China; Guangdong Key Lab of Information Security, School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China; Guangdong Key Lab of Information Security, School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China","IEEE Transactions on Multimedia","","2016","18","8","1480","1487","In this paper, we investigate how features can be effectively learned by deep neural networks for audio forensic problems. By providing a preliminary feature preprocessing based on electric network frequency (ENF) analysis, we propose a convolutional neural network (CNN) for training and classification of genuine and recaptured audio recordings. Hierarchical representations which contain levels of details of the ENF components are learned from the deep neural networks and can be used for further classification. The proposed method works for small audio clips of 2 second duration, whereas the state of the art may fail with such small audio clips. Experimental results demonstrate that the proposed network yields high detection accuracy with each ENF harmonic component represented as a single-channel input. The performance can be further improved by a combined input representation which incorporates both the fundamental ENF and its harmonics. The convergence property of the network and the effect of using an analysis window with various sizes are also studied. Performance comparison against the support tensor machine demonstrates the advantage of using CNN for the task of audio recapture detection. Moreover, visualization of the intermediate feature maps provides some insight into what the deep neural networks actually learn and how they make decisions.","","","10.1109/TMM.2016.2571999","NSFC; NSF of Guangdong province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478134","Audio recapture detection;convolutional neural network (CNN);electric network frequency (ENF)","Harmonic analysis;Audio recording;Videos;Neural networks;Spectrogram;Forensics;Multimedia communication","audio recording;audio signal processing;learning (artificial intelligence);neural nets;signal classification","convolutional neural networks;feature learning;deep-neural networks;audio forensic problems;feature preprocessing;electric network frequency;genuine audio recordings;recaptured audio recordings;hierarchical representations;audio clips;ENF harmonic component;single-channel input;performance improvement;combined input representation;fundamental ENF harmonics;convergence property;CNN;audio recapture detection;intermediate feature map visualization","","11","27","","","","","IEEE","IEEE Journals"
"Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images","S. Pereira; A. Pinto; V. Alves; C. A. Silva","CMEMS-UMinho Research Unit, University of Minho, Guimarães, Portugal; CMEMS-UMinho Research Unit, University of Minho, Guimarães, Portugal; Centro Algoritmi, Universidade do Minho, Portugal; CMEMS-UMinho Research Unit, University of Minho, Guimarães, Portugal","IEEE Transactions on Medical Imaging","","2016","35","5","1240","1251","Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 ×3 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.","","","10.1109/TMI.2016.2538465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426413","Brain tumor;brain tumor segmentation;convolutional neural networks;deep learning;glioma;magnetic resonance imaging","Tumors;Image segmentation;Magnetic resonance imaging;Kernel;Training;Brain modeling;Context","biomedical MRI;brain;cancer;image segmentation;medical image processing;neurophysiology;tumours","brain tumor segmentation;convolutional neural networks;MRI images;gliomas;quality-of-life;oncological patients;magnetic resonance imaging;imaging technique;manual segmentation;precise quantitative measurements;clinical practice;automatic segmentation methods;reliable segmentation methods;spatial variability;structural variability;automatic segmentation;kernels;intensity normalization;preprocessing step;CNN-based segmentation methods;data augmentation;Dice similarity coefficient metrics;online evaluation platform;on-site BRATS 2015 Challenge","Brain Neoplasms;Glioma;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Magnetic Resonance Imaging;Neural Networks (Computer)","491","52","","","","","IEEE","IEEE Journals"
"Representation Learning of Temporal Dynamics for Skeleton-Based Action Recognition","Y. Du; Y. Fu; L. Wang","National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Electrical and Computer EngineeringCollege of Engineering and the College of Computer and Information Science, Northeastern University, Boston, MA, USA; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences (CAS), Beijing, China","IEEE Transactions on Image Processing","","2016","25","7","3010","3022","Motion characteristics of human actions can be represented by the position variation of skeleton joints. Traditional approaches generally extract the spatial-temporal representation of the skeleton sequences with well-designed hand-crafted features. In this paper, in order to recognize actions according to the relative motion between the limbs and the trunk, we propose an end-to-end hierarchical RNN for skeleton-based action recognition. We divide human skeleton into five main parts in terms of the human physical structure, and then feed them to five independent subnets for local feature extraction. After the following hierarchical feature fusion and extraction from local to global, dimensions of the final temporal dynamics representations are reduced to the same number of action categories in the corresponding data set through a single-layer perceptron. In addition, the output of the perceptron is temporally accumulated as the input of a softmax layer for classification. Random scale and rotation transformations are employed to improve the robustness during training. We compare with five other deep RNN variants derived from our model in order to verify the effectiveness of the proposed network. In addition, we compare with several other methods on motion capture and Kinect data sets. Furthermore, we evaluate the robustness of our model trained with random scale and rotation transformations for a multiview problem. Experimental results demonstrate that our model achieves the state-of-the-art performance with high computational efficiency.","","","10.1109/TIP.2016.2552404","National Basic Research Program of China; National Science Foundation; Strategic Priority Research Program within the Chinese Academy of Sciences; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7450165","Action Recognition;Hierarchical Recurrent Neural Network;Random Scale & Rotation Transformations;Skeleton;Action recognition;hierarchical recurrent neural network;random scale & rotation transformations;skeleton","Skeleton;Hidden Markov models;Recurrent neural networks;Feature extraction;Robustness;Training;Computational modeling","bone;feature extraction;image motion analysis;learning (artificial intelligence);perceptrons","Kinect data set;motion capture;rotation transformation;softmax layer;single-layer perceptron;temporal dynamics representation;feature fusion;feature extraction;independent subnet;human physical structure;human skeleton;end-to-end hierarchical RNN;trunk;limbs;relative motion;handcrafted feature;skeleton sequence;spatial-temporal representation;skeleton joint;position variation;human action;motion characteristic;skeleton-based action recognition;temporal dynamics;representation learning","Algorithms;Humans;Motion;Neural Networks (Computer);Skeleton","43","51","","","","","IEEE","IEEE Journals"
"Partial Occlusion Handling in Pedestrian Detection With a Deep Model","W. Ouyang; X. Zeng; X. Wang","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Circuits and Systems for Video Technology","","2016","26","11","2123","2137","Part-based models have demonstrated their merit in object detection. However, there is a key issue to be solved on how to integrate the inaccurate scores of part detectors when there are occlusions, abnormal deformations, appearances, or illuminations. To handle the imperfection of part detectors, this paper presents a probabilistic pedestrian detection framework. In this framework, a deformable part-based model is used to obtain the scores of part detectors and the visibilities of parts are modeled as hidden variables. Once the occluded parts are identified, their effects are properly removed from the final detection score. Unlike previous occlusion handling approaches that assumed independence among the visibility probabilities of parts or manually defined rules for the visibility relationship, a deep model is proposed in this paper for learning the visibility relationship among overlapping parts at multiple layers. The proposed approach can be viewed as a general postprocessing of part-detection results and can take detection scores of existing part-based models as input. The experimental results on three public datasets (Caltech, ETH, and Daimler) and a new CUHK occlusion dataset (<;uri xlink:type=""simple"">http://www.ee.cuhk.edu.hk/~xgwang/CUHK_pedestrian.html<;/uri>), which is specially designed for the evaluation of occlusion handling approaches, show the effectiveness of the proposed approach.","","","10.1109/TCSVT.2015.2501940","General Research Fund through the Research Grants Council of Hong Kong; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7331638","Deep model;human detection;object detection;occlusion handling;pedestrian detection","Detectors;Deformable models;Estimation;Object detection;Support vector machines;Probabilistic logic;Visualization","object detection;pedestrians;probability","partial occlusion handling approach;object detection;part-based models;probabilistic pedestrian detection framework;visibility probabilities","","22","116","","","","","IEEE","IEEE Journals"
"Improved RGB-D-T based face recognition","M. O. Simón; C. Corneanu; K. Nasrollahi; O. Nikisins; S. Escalera; Y. Sun; H. Li; Z. Sun; T. B. Moeslund; M. Greitans","University of Barcelona, Spain; University of Barcelona, Spain; Aalborg University, Denmark; Institute of Electronics and Computer Science, Latvia; University of Barcelona, Spain; Institute of Automation, Chinese Academy of Sciences (CASIA), People's Republic of China; Institute of Automation, Chinese Academy of Sciences (CASIA), People's Republic of China; Institute of Automation, Chinese Academy of Sciences (CASIA), People's Republic of China; Aalborg University, Denmark; Institute of Electronics and Computer Science, Latvia","IET Biometrics","","2016","5","4","297","303","Reliable facial recognition systems are of crucial importance in various applications from entertainment to security. Thanks to the deep-learning concepts introduced in the field, a significant improvement in the performance of the unimodal facial recognition systems has been observed in the recent years. At the same time a multimodal facial recognition is a promising approach. This study combines the latest successes in both directions by applying deep learning convolutional neural networks (CNN) to the multimodal RGB, depth, and thermal (RGB-D-T) based facial recognition problem outperforming previously published results. Furthermore, a late fusion of the CNN-based recognition block with various hand-crafted features (local binary patterns, histograms of oriented gradients, Haar-like rectangular features, histograms of Gabor ordinal measures) is introduced, demonstrating even better recognition performance on a benchmark RGB-D-T database. The obtained results in this study show that the classical engineered features and CNN-based features can complement each other for recognition purposes.","","","10.1049/iet-bmt.2015.0057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7746050","","","face recognition;image colour analysis;learning (artificial intelligence);neural nets;visual databases","improved RGB-D-T based face recognition;unimodal facial recognition systems;multimodal facial recognition;deep learning convolutional neural networks;multimodal RGB-depth-thermal based facial recognition;CNN-based recognition block;handcrafted features;local binary patterns;histogram of oriented gradients;Haar-like rectangular features;histogram of Gabor ordinal measures;RGB-D-T database","","8","","","","","","IET","IET Journals"
"On the Use of Acoustic Unit Discovery for Language Recognition","S. H. Shum; D. F. Harwath; N. Dehak; J. R. Glass","Computer Science and Artificial Intelligence Laboratory Massachusetts Intitute of Technology, Cambridge, MA, USA; Computer Science and Artificial Intelligence Laboratory Massachusetts Intitute of Technology, Cambridge, MA, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Computer Science and Artificial Intelligence Laboratory Massachusetts Intitute of Technology, Cambridge, MA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","9","1665","1676","In this paper, we explore the use of large-scale acoustic unit discovery for language recognition. The deep neural network-based approaches that have achieved recent success in this task require transcribed speech and pronunciation dictionaries, which may be limited in availability and expensive to obtain. We aim to replace the need for such supervision via the unsupervised discovery of acoustic units. In this work, we present a parallelized version of a Bayesian nonparametric model from previous work and use it to learn acoustic units from a few hundred hours of multilingual data. These unit (or senone) sequences are then used as targets to train a deep neural network-based i-vector language recognition system. We find that a score-level fusion of our unsupervised system with an acoustic baseline can shrink the gap significantly between the baseline and a supervised benchmark system built using transcribed English. Subsequent experiments also show that an improved acoustic representation of the data can yield substantial performance gains and that language specificity is important for discovering meaningful acoustic units. We validate the generalizability of our proposed approach by presenting state-of-the-art results that exhibit similar trends on the NIST Language Recognition Evaluations from 2011 and 2015.","","","10.1109/TASLP.2016.2582260","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7494635","Acoustic unit discovery (AUD);bottleneck features;deep neural networks (DNNs);i-vector;language recognition;senone posteriors","Acoustics;Hidden Markov models;Speech;Speech processing;Speech recognition;Data models;Bayes methods","neural nets;speech recognition","NIST language recognition evaluation;language specificity;improved acoustic representation;transcribed English;score-level fusion;deep neural network-based i-vector language recognition system;multilingual data;Bayesian nonparametric model;pronunciation dictionaries;speech dictionaries;deep neural network-based approach;large-scale acoustic unit unsupervised discovery;language recognition","","2","53","","","","","IEEE","IEEE Journals"
"Exploring the spectroscopic diversity of Type Ia supernovae with dracula: a machine learning approach","M. Sasdelli; E. E. O. Ishida; R. Vilalta; M. Aguena; V. C. Busti; H. Camacho; A. M. M. Trindade; F. Gieseke; R. S. de Souza; Y. T. Fantaye; P. A. Mazzali","NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","Monthly Notices of the Royal Astronomical Society","","2016","461","2","2044","2059","The existence of multiple subclasses of Type Ia supernovae (SNe Ia) has been the subject of great debate in the last decade. One major challenge inevitably met when trying to infer the existence of one or more subclasses is the time consuming, and subjective, process of subclass definition. In this work, we show how machine learning tools facilitate identification of subtypes of SNe Ia through the establishment of a hierarchical group structure in the continuous space of spectral diversity formed by these objects. Using deep learning, we were capable of performing such identification in a four-dimensional feature space (+1 for time evolution), while the standard principal component analysis barely achieves similar results using 15 principal components. This is evidence that the progenitor system and the explosion mechanism can be described by a small number of initial physical parameters. As a proof of concept, we show that our results are in close agreement with a previously suggested classification scheme and that our proposed method can grasp the main spectral features behind the definition of such subtypes. This allows the confirmation of the velocity of lines as a first-order effect in the determination of SN Ia subtypes, followed by 91bg-like events. Given the expected data deluge in the forthcoming years, our proposed approach is essential to allow a quick and statistically coherent identification of SNe Ia subtypes (and outliers). All tools used in this work were made publicly available in the python package Dimensionality Reduction And Clustering for Unsupervised Learning in Astronomy (dracula) and can be found within COINtoolbox (https://github.com/COINtoolbox/DRACULA).","","","10.1093/mnras/stw1228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8151590","methods: data analysis;methods: miscellaneous;methods: statistical;supernovae: general","","","","","","","","","","","OUP","OUP Journals"
"Understanding Short Texts through Semantic Enrichment and Hashing","Z. Yu; H. Wang; X. Lin; M. Wang","East China Normal University, Shanghai, China; Google Research, Mountain View, CA; East China Normal University, Shanghai, China; Google Research, Mountain View, CA","IEEE Transactions on Knowledge and Data Engineering","","2016","28","2","566","579","Clustering short texts (such as news titles) by their meaning is a challenging task. The semantic hashing approach encodes the meaning of a text into a compact binary code. Thus, to tell if two texts have similar meanings, we only need to check if they have similar codes. The encoding is created by a deep neural network, which is trained on texts represented by word-count vectors (bag-of-word representation). Unfortunately, for short texts such as search queries, tweets, or news titles, such representations are insufficient to capture the underlying semantics. To cluster short texts by their meanings, we propose to add more semantic signals to short texts. Specifically, for each term in a short text, we obtain its concepts and co-occurring terms from a probabilistic knowledge base to enrich the short text. Furthermore, we introduce a simplified deep learning network consisting of a 3-layer stacked auto-encoders for semantic hashing. Comprehensive experiments show that, with more semantic signals, our simplified deep learning model is able to capture the semantics of short texts, which enables a variety of applications including short text retrieval, classification, and general purpose text processing.","","","10.1109/TKDE.2015.2485224","NSFC; ARC; ARC; ARC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7286811","Short text;semantic enrichment;semantic hashing;deep neural network;Short text;semantic enrichment;semantic hashing;deep neural network","Semantics;Neural networks;Tablet computers;Training;Companies;Context;Mice","","","","16","34","","","","","IEEE","IEEE Journals"
"Semantic Labeling of Aerial and Satellite Imagery","S. Paisitkriangkrai; J. Sherrah; P. Janney; A. van den Hengel","SA, Australian Centre for Visual Technology, The University of Adelaide, Adelaide, Australia; Department of Defence, Defence Science and Technology Group, Edinburgh, SA, Australia; Department of Defence, Defence Science and Technology Group, Edinburgh, SA, Australia; SA, Australian Centre for Visual Technology, The University of Adelaide, Adelaide, Australia","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2016","9","7","2868","2881","Inspired by the recent success of deep convolutional neural networks (CNNs) and feature aggregation in the field of computer vision and machine learning, we propose an effective approach to semantic pixel labeling of aerial and satellite imagery using both CNN features and hand-crafted features. Both CNN and hand-crafted features are applied to dense image patches to produce per-pixel class probabilities. Conditional random fields (CRFs) are applied as a postprocessing step. The CRF infers a labeling that smooths regions while respecting the edges present in the imagery. The combination of these factors leads to a semantic labeling framework which outperforms all existing algorithms on the International Society of Photogrammetry and Remote Sensing (ISPRS) two-dimensional Semantic Labeling Challenge dataset. We advance state-of-the-art results by improving the overall accuracy to 88% on the ISPRS Semantic Labeling Contest. In this paper, we also explore the possibility of applying the proposed framework to other types of data. Our experimental results demonstrate the generalization capability of our approach and its ability to produce accurate results.","","","10.1109/JSTARS.2016.2582921","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516568","Aerial imagery;conditional random fields;convolutional neural networks;deep learning;satellite imagery and remote sensing;semantic labeling","Labeling;Semantics;Feature extraction;Convolution;Visualization;Neurons;Remote sensing","geophysical image processing;geophysical techniques;image classification;photogrammetry;remote sensing","aerial imagery semantic pixel labeling;satellite imagery semantic pixel labeling;deep convolutional neural networks;hand-crafted features;CNN features;dense image patches;per-pixel class probabilities;conditional random fields;semantic labeling framework;International Society of Photogrammetry and Remote Sensing;IS-PRS two-dimensional Semantic Labeling Challenge dataset;ISPRS Semantic Labeling Contest","","50","51","","","","","IEEE","IEEE Journals"
"DNN-Based Feature Enhancement Using DOA-Constrained ICA for Robust Speech Recognition","H. Lee; J. Cho; M. Kim; H. Park","Department of Electronic Engineering, Sogang University, Seoul, South Korea; Department of Electronic Engineering, Sogang University, Seoul, South Korea; Department of Electronic Engineering, Sogang University, Seoul, South Korea; Department of Electronic Engineering, Sogang University, Seoul, South Korea","IEEE Signal Processing Letters","","2016","23","8","1091","1095","The performance of automatic speech recognition (ASR) system is often degraded in adverse real-world environments. In recent times, deep learning has successfully emerged as a breakthrough for acoustic modeling in ASR; accordingly, deep neural network (DNN)-based speech feature enhancement (FE) approaches have attracted much attention owing to their powerful modeling capabilities. However, DNN-based approaches are unable to achieve remarkable performance improvements for speech with severe distortion in the test environments different from training environments. In this letter, we propose a DNN-based FE method where the DNN inputs include preenhanced spectral features computed from multichannel input signals to reconstruct noise-robust features. The preenhanced spectral features are obtained by direction-of-arrival (DOA)-constrained independent component analysis (DCICA) followed by Bayesian FE using a hidden-Markov-model prior, to exploit the capabilities of efficient online target speech extraction and efficient FE with prior information for robust ASR. In addition, noise spectral features computed from DCICA are included for further improvement. Therefore, the DNN is trained to reconstruct a clean spectral feature vector, from a sequence of corrupted input feature vectors in addition to the corresponding preenhanced and noise feature vectors. Experimental results demonstrate that the proposed method significantly improves recognition performance, even in mismatched noise conditions.","","","10.1109/LSP.2016.2583658","Basic Science Research Program; National Research Foundation of Korea; Ministry of Science, ICT, and future Planning; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497454","Deep neural networks (DNNs);feature enhancement (FE);independent component analysis (ICA);robustspeech recognition","Speech;Iron;Robustness;Speech recognition;Bayes methods;Noise measurement;Speech enhancement","direction-of-arrival estimation;independent component analysis;neural nets;speech recognition;vectors","DNN;DOA-constrained ICA;ASR system;deep learning;automatic speech recognition system;speech feature enhancement approaches;FE approaches;performance improvements;training environments;direction-of-arrival;independent component analysis;DCICA;speech extraction;spectral feature vector;noise feature vectors","","5","27","","","","","IEEE","IEEE Journals"
"A Framework for Classifying Online Mental Health-Related Communities With an Interest in Depression","B. Saha; T. Nguyen; D. Phung; S. Venkatesh","Centre for Pattern Recognition and Data Analytics, Faculty of Science and Technology, Deakin University, Vic., Australia; Centre for Pattern Recognition and Data Analytics, Faculty of Science and Technology, Deakin University, Vic., Australia; Centre for Pattern Recognition and Data Analytics, Faculty of Science and Technology, Deakin University, Vic., Australia; Centre for Pattern Recognition and Data Analytics, Faculty of Science and Technology, Deakin University, Vic., Australia","IEEE Journal of Biomedical and Health Informatics","","2016","20","4","1008","1015","Mental illness has a deep impact on individuals, families, and by extension, society as a whole. Social networks allow individuals with mental disorders to communicate with others sufferers via online communities, providing an invaluable resource for studies on textual signs of psychological health problems. Mental disorders often occur in combinations, e.g., a patient with an anxiety disorder may also develop depression. This co-occurring mental health condition provides the focus for our work on classifying online communities with an interest in depression. For this, we have crawled a large body of 620 000 posts made by 80 000 users in 247 online communities. We have extracted the topics and psycholinguistic features expressed in the posts, using these as inputs to our model. Following a machine learning technique, we have formulated a joint modeling framework in order to classify mental health-related co-occurring online communities from these features. Finally, we performed empirical validation of the model on the crawled dataset where our model outperforms recent state-of-the-art baselines.","","","10.1109/JBHI.2016.2543741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436759","Health Information Management;statistical learning;Predictive Models","Feature extraction;Covariance matrices;Informatics;Pragmatics;Correlation;Media;Blogs","learning (artificial intelligence);medical disorders;medical information systems;psychology;social networking (online)","online mental health-related communities;depression;mental illness;social networks;textual signs;psychological health problems;mental disorders;anxiety disorder;psycholinguistic features;machine learning","Blogging;Depression;Health Information Management;Humans;Mental Health;Models, Statistical;Social Media","8","29","","","","","IEEE","IEEE Journals"
"Accelerating Convolutional Sparse Coding for Curvilinear Structures Segmentation by Refining SCIRD-TS Filter Banks","R. Annunziata; E. Trucco","School of Science and Engineering (Computing), University of Dundee, Dundee, U.K.; School of Science and Engineering (Computing), University of Dundee, Dundee, U.K.","IEEE Transactions on Medical Imaging","","2016","35","11","2381","2392","Deep learning has shown great potential for curvilinear structure (e.g., retinal blood vessels and neurites) segmentation as demonstrated by a recent auto-context regression architecture based on filter banks learned by convolutional sparse coding. However, learning such filter banks is very time-consuming, thus limiting the amount of filters employed and the adaptation to other data sets (i.e., slow re-training). We address this limitation by proposing a novel acceleration strategy to speed-up convolutional sparse coding filter learning for curvilinear structure segmentation. Our approach is based on a novel initialisation strategy (warm start), and therefore it is different from recent methods improving the optimisation itself. Our warm-start strategy is based on carefully designed hand-crafted filters (SCIRD-TS), modelling appearance properties of curvilinear structures which are then refined by convolutional sparse coding. Experiments on four diverse data sets, including retinal blood vessels and neurites, suggest that the proposed method reduces significantly the time taken to learn convolutional filter banks (i.e., up to -82%) compared to conventional initialisation strategies. Remarkably, this speed-up does not worsen performance; in fact, filters learned with the proposed strategy often achieve a much lower reconstruction error and match or exceed the segmentation performance of random and DCT-based initialisation, when used as input to a random forest classifier.","","","10.1109/TMI.2016.2570123","EU Marie Curie ITN REVAMMAD 316990; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7470472","Convolutional sparse coding;neurites;retinal blood vessels;segmentation","Detectors;Blood vessels;Biomedical imaging;Convolutional codes;Encoding;Acceleration;Image segmentation","biomedical optical imaging;blood vessels;channel bank filters;convolutional codes;eye;image classification;image coding;image filtering;image reconstruction;image segmentation;medical image processing;neurophysiology;optimisation;regression analysis","accelerating convolutional sparse coding;curvilinear structure segmentation;SCIRD-TS filter bank refining;deep learning;retinal blood vessels;neurites;autocontext regression architecture;data sets;convolutional sparse coding filter learning;initialisation strategy;optimisation;warm-start strategy;handcrafted filters;reconstruction error;DCT-based initialisation;random forest classifier","Algorithms;Humans;Image Interpretation, Computer-Assisted;Image Processing, Computer-Assisted;Neural Networks (Computer);Neurites;Retinal Vessels","13","45","","","","","IEEE","IEEE Journals"
"Visual Place Recognition: A Survey","S. Lowry; N. Sünderhauf; P. Newman; J. J. Leonard; D. Cox; P. Corke; M. J. Milford","Australian Centre for Robotic Vision, School of Electrical Engineering and Computer Science, Queensland University of Technology, Brisbane, QLD, Australia; Australian Centre for Robotic Vision, School of Electrical Engineering and Computer Science, Queensland University of Technology, Brisbane, QLD, Australia; Mobile Robotics Group, Department of Engineering Science, University of Oxford, Oxford, U.K.; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Molecular and Cellular Biology, the School of Engineering and Applied Science, and the Center for Brain Science, Harvard University, Cambridge, MA, USA; Australian Centre for Robotic Vision, School of Electrical Engineering and Computer Science, Queensland University of Technology, Brisbane, QLD, Australia; Australian Centre for Robotic Vision, School of Electrical Engineering and Computer Science, Queensland University of Technology, Brisbane, QLD, Australia","IEEE Transactions on Robotics","","2016","32","1","1","19","Visual place recognition is a challenging problem due to the vast range of ways in which the appearance of real-world places can vary. In recent years, improvements in visual sensing capabilities, an ever-increasing focus on long-term mobile robot autonomy, and the ability to draw on state-of-the-art research in other disciplines-particularly recognition in computer vision and animal navigation in neuroscience-have all contributed to significant advances in visual place recognition systems. This paper presents a survey of the visual place recognition research landscape. We start by introducing the concepts behind place recognition-the role of place recognition in the animal kingdom, how a “place” is defined in a robotics context, and the major components of a place recognition system. Long-term robot operations have revealed that changing appearance can be a significant factor in visual place recognition failure; therefore, we discuss how place recognition solutions can implicitly or explicitly account for appearance change within the environment. Finally, we close with a discussion on the future of visual place recognition, in particular with respect to the rapid advances being made in the related fields of deep learning, semantic scene understanding, and video description.","","","10.1109/TRO.2015.2496823","ARC Future Fellowship; Australian Centre for Robotic Vision; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339473","Visual place recognition;place recognition;Visual place recognition;place recognition","Robot sensing systems;Visualization;Navigation;Animals;Conferences;Computer vision","learning (artificial intelligence);mobile robots;object recognition;robot vision;video signal processing","visual place recognition system;visual sensing capabilities;long-term mobile robot autonomy;computer vision;animal navigation;visual place recognition research landscape;animal kingdom;robotics context;deep learning;semantic scene understanding;video description","","215","237","","","","","IEEE","IEEE Journals"
"Animal Detection From Highly Cluttered Natural Scenes Using Spatiotemporal Object Region Proposals and Patch Verification","Z. Zhang; Z. He; G. Cao; W. Cao","Department of Electrical and Computer Engineering, University of Missouri, Columbia, MO, USA; Department of Electrical and Computer Engineering, University of Missouri, Columbia, MO, USA; School of Computer Science and Software Engineering, East China Normal University, Shanghai, China; College of Information Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Multimedia","","2016","18","10","2079","2092","In this paper, we consider the animal object detection and segmentation from wildlife monitoring videos captured by motion-triggered cameras, called camera-traps. For these types of videos, existing approaches often suffer from low detection rates due to low contrast between the foreground animals and the cluttered background, as well as high false positive rates due to the dynamic background. To address this issue, we first develop a new approach to generate animal object region proposals using multilevel graph cut in the spatiotemporal domain. We then develop a cross-frame temporal patch verification method to determine if these region proposals are true animals or background patches. We construct an efficient feature description for animal detection using joint deep learning and histogram of oriented gradient features encoded with Fisher vectors. Our extensive experimental results and performance comparisons over a diverse set of challenging camera-trap data demonstrate that the proposed spatiotemporal object proposal and patch verification framework outperforms the state-of-the-art methods, including the recent Faster-RCNN method, on animal object detection accuracy by up to 4.5%.","","","10.1109/TMM.2016.2594138","National Science Foundation; National Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523423","Background modeling;camera-trap images;graph cut;object proposal;object verification","Animals;Proposals;Object detection;Image segmentation;Feature extraction;Videos;Neural networks","feature extraction;graph theory;image segmentation;learning (artificial intelligence);object detection;video signal processing","highly cluttered natural scene;spatiotemporal object region proposal;patch verification;animal object detection;animal object segmentation;wildlife monitoring videos;motion-triggered cameras;camera-traps;cluttered background;false positive rates;multilevel graph cut;spatiotemporal domain;cross-frame temporal patch verification method;feature description;joint deep learning;histogram-of-oriented gradient features;Faster-RCNN method","","20","53","","","","","IEEE","IEEE Journals"
"Stochastic Spectral Descent for Discrete Graphical Models","D. Carlson; Y. Hsieh; E. Collins; L. Carin; V. Cevher","Department of Statistics and Grossman Center for the Statistics of Mind, Columbia University, New York; Laboratory for Information and Inference Systems (LIONS), École Polytechnique Fédérale de Lausanne (EPFL), Switzerland; Laboratory for Information and Inference Systems (LIONS), École Polytechnique Fédérale de Lausanne (EPFL), Switzerland; Department of Electrical and Computer Engineering, Duke University, Durham; Laboratory for Information and Inference Systems (LIONS), École Polytechnique Fédérale de Lausanne (EPFL), Switzerland","IEEE Journal of Selected Topics in Signal Processing","","2016","10","2","296","311","Interest in deep probabilistic graphical models has increased in recent years, due to their state-of-the-art performance on many machine learning applications. Such models are typically trained with the stochastic gradient method, which can take a significant number of iterations to converge. Since the computational cost of gradient estimation is prohibitive even for modestly sized models, training becomes slow and practically usable models are kept small. In this paper we propose a new, largely tuning-free algorithm to address this problem. Our approach derives novel majorization bounds based on the Schatten- ∞ norm. Intriguingly, the minimizers of these bounds can be interpreted as gradient methods in a non-Euclidean space. We thus propose using a stochastic gradient method in non-Euclidean space. We both provide simple conditions under which our algorithm is guaranteed to converge, and demonstrate empirically that our algorithm leads to dramatically faster training and improved predictive ability compared to stochastic gradient descent for both directed and undirected graphical models.","","","10.1109/JSTSP.2015.2505684","ARO; DARPA; DOE; NGA; ONR; European Commission; ERC Future Proof; Swiss Science Foundation; NCCR Marvel; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7347351","Gradient methods;graphical models;maximum likelihood estimation;Monte Carlo simulation methods;Boltzmann distributions","Geometry;Graphical models;Signal processing algorithms;Radio frequency;Stochastic processes;Computational modeling;Convergence","directed graphs;gradient methods;learning (artificial intelligence);stochastic processes","undirected graphical models;directed graphical models;predictive ability improvement;nonEuclidean space;Schatten-∞ norm;majorization bounds;largely tuning-free algorithm;gradient estimation;stochastic gradient method;machine learning applications;deep probabilistic graphical models;discrete graphical models;stochastic spectral descent","","4","38","","","","","IEEE","IEEE Journals"
"Exploiting Attribute Dependency for Attribute Assignment in Crowded Scenes","C. Deng; Z. Cao; Y. Xiao; H. Lu; K. Xian; Y. Chen","National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; Department of Mathematics, Taiyuan University of Technology, Taiyuan, China","IEEE Signal Processing Letters","","2016","23","10","1325","1329","Attributes now play a vital role for characterizing a crowded scene. Compared to low-level visual features, processing informed by attributes can capture rich semantic information. However, to effectively assign attributes to a crowded scene still remains a challenging task. In this letter, inspired by a recently proposed zero-shot learning framework, a novel attribute assignment method that maps low-level features to predefined attributes is proposed. In particular, we propose to exploit the attribute dependency during the phase of attribute assignment, which can be regarded as our main contribution. In addition, to further enhance the performance, an effective low-level feature extraction mechanism is also proposed. More precisely, appearance and motion features are first simultaneously extracted from several sampled video frames and corresponding optical flow fields via deep convolutional neural network and then, respectively, aggregated by using Fisher vector encoding to form the low-level representation of crowded scenes. Experimental results on the challenging WWW dataset demonstrate that both the proposed attribute assignment method and the low-level feature extraction mechanism outperform the state of the art.","","","10.1109/LSP.2016.2592689","National High-tech R&D Program of China; National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515214","Attribute assignment (AA);attribute dependency;convolutional neural network (CNN);crowded scene;Fisher vector (FV)","Feature extraction;Training;World Wide Web;Motion segmentation;Visualization;Semantics;Image analysis","feature extraction;learning (artificial intelligence);neural nets","Fisher vector encoding;deep convolutional neural network;low-level feature extraction mechanism;attribute assignment method;zero-shot learning framework;crowded scenes;attribute assignment;attribute dependency","","1","30","","","","","IEEE","IEEE Journals"
"ApesNet: A pixel-wise efficient segmentation network for embedded devices","C. Wu; H. Cheng; S. Li; H. Li; Y. Chen","University of Pittsburgh, USA; University of Pittsburgh, USA; University of Pittsburgh, USA; University of Pittsburgh, USA; University of Pittsburgh, USA","IET Cyber-Physical Systems: Theory & Applications","","2016","1","1","78","85","Road scene understanding and semantic segmentation is an on-going issue for computer vision. A precise segmentation can help a machine learning model understand the real world more accurately. In addition, a well-designed efficient model can be used on source limited devices. The authors aim to implement an efficient high-level, scene understanding model in an embedded device with finite power and resources. Toward this goal, the authors propose ApesNet, an efficient pixel-wise segmentation network which understands road scenes in near real-time and has achieved promising accuracy. The key findings in the authors' experiments are significantly lower the classification time and achieving a high accuracy compared with other conventional segmentation methods. The model is characterised by an efficient training and a sufficient fast testing. Experimentally, the authors use two road scene benchmarks, CamVid and Cityscapes to show the advantages of ApesNet. The authors' compare the proposed architecture's accuracy and time performance with SegNet-Basic, a deep convolutional encoder-decoder architecture. ApesNet is 37% smaller than SegNet-Basic in terms of model size. With this advantage, the combining encoding and decoding time for each image is 2.5 times faster than SegNet-Basic.","","","10.1049/iet-cps.2016.0027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7805368","","","computer vision;embedded systems;image segmentation;learning (artificial intelligence)","ApesNet;pixel-wise efficient segmentation network;embedded devices;semantic segmentation;road scene understanding;computer vision;machine learning model;high-level scene understanding model;classification time;CamVid;Cityscapes;SegNet-Basic;deep convolutional encoder-decoder architecture","","2","","","","","","IET","IET Journals"
"Scene Classification via a Gradient Boosting Random Convolutional Network Framework","F. Zhang; B. Du; L. Zhang","State Key Lab. of Inf. Eng. in Surveying, Wuhan Univ., Wuhan, China; Sch. of Comput. Sci., Wuhan Univ., Wuhan, China; State Key Lab. of Inf. Eng. in Surveying, Wuhan Univ., Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2016","54","3","1793","1802","Due to the recent advances in satellite sensors, a large amount of high-resolution remote sensing images is now being obtained each day. How to automatically recognize and analyze scenes from these satellite images effectively and efficiently has become a big challenge in the remote sensing field. Recently, a lot of work in scene classification has been proposed, focusing on deep neural networks, which learn hierarchical internal feature representations from image data sets and produce state-of-the-art performance. However, most methods, including the traditional shallow methods and deep neural networks, only concentrate on training a single model. Meanwhile, neural network ensembles have proved to be a powerful and practical tool for a number of different predictive tasks. Can we find a way to combine different deep neural networks effectively and efficiently for scene classification? In this paper, we propose a gradient boosting random convolutional network (GBRCN) framework for scene classification, which can effectively combine many deep neural networks. As far as we know, this is the first time that a deep ensemble framework has been proposed for scene classification. Moreover, in the experiments, the proposed method was applied to two challenging high-resolution data sets: 1) the UC Merced data set containing 21 different aerial scene categories with a submeter resolution and 2) a Sydney data set containing eight land-use categories with a 1.0-m spatial resolution. The proposed GBRCN framework outperformed the state-of-the-art methods with the UC Merced data set, including the traditional single convolutional network approach. For the Sydney data set, the proposed method again obtained the best accuracy, demonstrating that the proposed framework can provide more accurate classification results than the state-of-the-art methods.","","","10.1109/TGRS.2015.2488681","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Natural Science Foundation of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7310864","Convolutional networks (CNets);gradient boosting machine (GBM);scene classification;Convolutional networks (CNets);gradient boosting machine (GBM);scene classification","Feature extraction;Neural networks;Boosting;Remote sensing;Satellites;Semantics;Training","geophysical image processing;geophysical techniques;image classification;image resolution;land use;neural nets;remote sensing","gradient boosting random convolutional network framework;satellite sensors;high-resolution remote sensing images;remote sensing field;scene classification;hierarchical internal feature representations;image data sets;shallow methods;deep neural network ensembles;deep ensemble framework;high-resolution data;UC Merced data set;aerial scene categories;submeter resolution;Sydney data set;land-use categories;spatial resolution;traditional single convolutional network approach","","154","33","","","","","IEEE","IEEE Journals"
"DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection","X. Li; L. Zhao; L. Wei; M. Yang; F. Wu; Y. Zhuang; H. Ling; J. Wang","College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; Electrical Engineering and Computer Science, University of California at Merced, Merced, CA, USA; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA; Visual Computing Group, Microsoft Research Asia, Beijing, China","IEEE Transactions on Image Processing","","2016","25","8","3919","3930","A key problem in salient object detection is how to effectively model the semantic properties of salient objects in a data-driven manner. In this paper, we propose a multi-task deep saliency model based on a fully convolutional neural network with global input (whole raw images) and global output (whole saliency maps). In principle, the proposed saliency model takes a data-driven strategy for encoding the underlying saliency prior information, and then sets up a multi-task learning scheme for exploring the intrinsic correlations between saliency detection and semantic image segmentation. Through collaborative feature learning from such two correlated tasks, the shared fully convolutional layers produce effective features for object perception. Moreover, it is capable of capturing the semantic information on salient objects across different levels using the fully convolutional layers, which investigate the feature-sharing properties of salient object detection with a great reduction of feature redundancy. Finally, we present a graph Laplacian regularized nonlinear regression model for saliency refinement. Experimental results demonstrate the effectiveness of our approach in comparison with the state-of-the-art approaches.","","","10.1109/TIP.2016.2579306","National Natural Science Foundation of China; National Basic Research Program of China; Fundamental Research Funds for the Central Universities; NSF CAREER; NSF IIS; National Natural Science Foundation of China; NSF IIS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488288","salient object detection;CNN;multi-task;datadriven;Salient object detection;CNN;multi-task;data-driven","Semantics;Object detection;Image segmentation;Feature extraction;Neural networks;Convolution;Computational modeling;Regression analysis","","","","187","60","","","","","IEEE","IEEE Journals"
"When Correlation Filters Meet Convolutional Neural Networks for Visual Tracking","C. Ma; Y. Xu; B. Ni; X. Yang","Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Signal Processing Letters","","2016","23","10","1454","1458","Correlation filters have been widely applied to visual tracking in recent years as adaptive correlation filters with short-term memory are robust to large appearance changes. However, tracking methods relying on correlation filters are prone to drifting due to noisy updates. Moreover, these methods are unable to recover from tracking failures caused by temporary or persistent heavy occlusions. In this paper, we interpret correlation filters as the counterparts of convolution filters in deep neural networks. Correlation filters encode the holistic template of target appearance, while convolution filters with smaller size encode the part-based template. In the light of this idea, we propose to exploit deep convolutional networks that directly learn mapping as a spatial correlation between two consecutive frames for visual tracking. We show that these deeply learned networks are effective in maintaining the long-term memory of target appearance for handling heavy occlusion or out-of-view. We further take the response maps both from the deep networks and conventional correlation filters into account for precisely locating the target. Experimental results on large-scale benchmark sequences show that the proposed algorithm performs favorably against the state-of-the-art methods.","","","10.1109/LSP.2016.2601691","State Key Research and Development Program; National Natural Science Foundation of China; Shanghai Science and Technology Committee; 111 Program; China's Thousand Youth Talents Plan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7547968","Convolutional filters;correlation filters;neural networks;visual tracking","Correlation;Target tracking;Convolution;Visualization;Neural networks;Information filters","adaptive filters;image coding;image filtering;image sequences;neural nets;tracking filters","deeply-learned networks;large-scale benchmark sequences;consecutive frames;spatial correlation;part-based template encoding;holistic template encoding;deep neural networks;convolution filters;tracking failures;short-term memory;adaptive correlation filters;visual tracking;convolutional neural networks","","29","36","","","","","IEEE","IEEE Journals"
"Deep 3-GHz observations of the Lockman Hole North with the Very Large Array – I. Source extraction and uncertainty analysis","T. Vernstrom; D. Scott; J. V. Wall; J. J. Condon; W. D. Cotton; R. A. Perley","NA; NA; NA; NA; NA; NA","Monthly Notices of the Royal Astronomical Society","","2016","461","3","2879","2895","This is the first of two papers describing the observations and cataloguing of deep 3-GHz observations of the Lockman Hole North using the Karl G. Jansky Very Large Array. The aim of this paper is to investigate, through the use of simulated images, the uncertainties and accuracy of source-finding routines, as well as to quantify systematic effects due to resolution, such as source confusion and source size. While these effects are not new, this work is intended as a particular case study that can be scaled and translated to other surveys. We use the simulations to derive uncertainties in the fitted parameters, as well as bias corrections for the actual catalogue (presented in Paper II). We compare two different source-finding routines, OBIT and AEGEAN, and two different effective resolutions, 8 and 2.75  arcsec. We find that the two routines perform comparably well, with OBIT being slightly better at de-blending sources, but slightly worse at fitting resolved sources. We show that 30–70 per cent of sources are missed or fit inaccurately once the source size becomes larger than the beam, possibly explaining source count errors in high-resolution surveys. We also investigate the effect of blending, finding that any sources with separations smaller than the beam size are fit as single sources. We show that the use of machine-learning techniques can correctly identify blended sources up to 90 per cent of the time, and prior-driven fitting can lead to a 70 per cent improvement in the number of de-blended sources.","","","10.1093/mnras/stw1530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8161413","methods: data analysis;methods: statistical;cosmology: observations;radio continuum: galaxies","","","","","","","","","","","OUP","OUP Journals"
"Differentiable Pooling for Unsupervised Acoustic Model Adaptation","P. Swietojanski; S. Renals","Centre for Speech Technology Research, University of Edinburgh, Edinburgh, U.K.; Centre for Speech Technology Research, University of Edinburgh, Edinburgh, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2016","24","10","1773","1784","We present a deep neural network (DNN) acoustic model that includes parametrised and differentiable pooling operators. Unsupervised acoustic model adaptation is cast as the problem of updating the decision boundaries implemented by each pooling operator. In particular, we experiment with two types of pooling parametrisations: learned Lp-norm pooling and weighted Gaussian pooling, in which the weights of both operators are treated as speaker-dependent. We perform investigations using three different large vocabulary speech recognition corpora: AMI meetings, TED talks, and Switchboard conversational telephone speech. We demonstrate that differentiable pooling operators provide a robust and relatively low-dimensional way to adapt acoustic models, with relative word error rates reductions ranging from 5-20% with respect to unadapted systems, which themselves are better than the baseline fully-connected DNN-based acoustic models. We also investigate how the proposed techniques work under various adaptation conditions including the quality of adaptation data and complementarity to other feature- and model-space adaptation methods, as well as providing an analysis of the characteristics of each of the proposed approaches.","","","10.1109/TASLP.2016.2584700","EPSRC; Natural Speech Technology and the European Union; SUMMA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7499870","Adaptation;differentiable pooling;neural networks","Adaptation models;Acoustics;Hidden Markov models;Speech;Data models;Training;Transforms","Gaussian processes;neural nets;speech recognition;unsupervised learning","unsupervised acoustic model adaptation;differentiable pooling;deep neural network;DNN acoustic model;differentiable pooling operators;decision boundaries;pooling parametrisations;weighted Gaussian pooling;vocabulary speech recognition corpora;switchboard conversational telephone speech;word error rates reductions","","2","87","","","","","IEEE","IEEE Journals"
"Beyond Object Proposals: Random Crop Pooling for Multi-Label Image Recognition","M. Wang; C. Luo; R. Hong; J. Tang; J. Feng","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science, Nanjing University of Science and Technology, Nanjing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Image Processing","","2016","25","12","5678","5688","Learning high-level image representations using object proposals has achieved remarkable success in multi-label image recognition. However, most object proposals provide merely coarse information about the objects, and only carefully selected proposals can be helpful for boosting the performance of multi-label image recognition. In this paper, we propose an object-proposal-free framework for multi-label image recognition: random crop pooling (RCP). Basically, RCP performs stochastic scaling and cropping over images before feeding them to a standard convolutional neural network, which works quite well with a max-pooling operation for recognizing the complex contents of multi-label images. To better fit the multi-label image recognition task, we further develop a new loss function-the dynamic weighted Euclidean loss-for the training of the deep network. Our RCP approach is amazingly simple yet effective. It can achieve significantly better image recognition performance than the approaches using object proposals. Moreover, our adapted network can be easily trained in an end-to-end manner. Extensive experiments are conducted on two representative multi-label image recognition data sets (i.e., PASCAL VOC 2007 and PASCAL VOC 2012), and the results clearly demonstrate the superiority of our approach.","","","10.1109/TIP.2016.2612829","National 973 Program of China; National Nature Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574320","Multi-label image recognition;object proposal;random crop pooling;convolutional neural networks","Image recognition;Proposals;Training;Agriculture;Feature extraction;Standards;Neural networks","image recognition;image representation;learning (artificial intelligence);neural nets;stochastic processes","random crop pooling;learning high-level image representation;multilabel image recognition;object-proposal-free framework;stochastic scaling;image cropping;convolutional neural network;max-pooling operation;dynamic weighted Euclidean loss;RCP approach;PASCAL VOC 2007;PASCAL VOC 2012","","16","37","","","","","IEEE","IEEE Journals"
"Enhanced Restricted Boltzmann Machine With Prognosability Regularization for Prognostics and Health Assessment","L. Liao; W. Jin; R. Pavel","System Sciences Laboratory, Palo Alto Research Center, Palo Alto, CA, USA; System Sciences Laboratory, Palo Alto Research Center, Palo Alto, CA, USA; TechSolve, Inc., Cincinnati, OH, USA","IEEE Transactions on Industrial Electronics","","2016","63","11","7076","7083","In the Internet-of-Things environment, it is critical to bridge the gap between business decision-making and real-time factory data to let companies transfer from condition-based maintenance service to predictive maintenance service. Condition monitoring systems have been widely applied to many industries to acquire operation and equipment related data, through which machine health state can be evaluated. One of the challenges of predicting future machine health lies in extracting the right features that are correlated well with the fault progression/degradation. We propose an enhanced restricted Boltzmann machine with a novel regularization term to automatically generate features that are suitable for remaining useful life prediction. The regularization term tries to maximize the trendability of the output features, which potentially better represent the degradation pattern of a system. The proposed method is benchmarked with regular restricted Boltzmann machine algorithm and principal component analysis. The generated features are used as input to a similarity-based method for life prediction. Run-to-failure datasets collected from two rotating systems are used for validation.","","","10.1109/TIE.2016.2586442","Palo Alto Research Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502088","Deep learning;feature extraction;prognostics and health management (PHM);regularization;remaining useful life (RUL) prediction;restricted Boltzmann machine (RBM)","Feature extraction;Prediction algorithms;Degradation;Algorithm design and analysis;Prognostics and health management;Industries;Training","Boltzmann machines;condition monitoring;decision making;failure analysis;fault diagnosis;feature extraction;Internet of Things;machinery;mechanical engineering computing;preventive maintenance;principal component analysis;remaining life assessment","rotating systems;run-to-failure datasets;feature extraction;principal component analysis;fault degradation;fault progression;machine health;condition monitoring systems;condition-based maintenance service;real-time factory data;business decision-making;Internet-of-Things environment;prognosability regularization;restricted Boltzmann machine","","55","20","","","","","IEEE","IEEE Journals"
"Automatic Detection of Cerebral Microbleeds From MR Images via 3D Convolutional Neural Networks","Q. Dou; H. Chen; L. Yu; L. Zhao; J. Qin; D. Wang; V. C. Mok; L. Shi; P. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, HK, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, HK, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, HK, China; Department of Medicine and Therapeutics, The Chinese University of Hong Kong, HK, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, School of Medicine, Shenzhen University, Shenzhen, China; Department of Imaging and Interventional Radiology, The Chinese University of Hong Kong, HK, China; Chow Yuk Ho Technology Center for Innovative Medicine, Therese Pei Fong Chow Research Center for Prevention of Dementia, Department of Medicine and Therapeutics, The Chinese University of Hong Kong, HK, China; Chow Yuk Ho Technology Center for Innovative Medicine, Therese Pei Fong Chow Research Center for Prevention of Dementia, Department of Medicine and Therapeutics, The Chinese University of Hong Kong, HK, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, HK, China","IEEE Transactions on Medical Imaging","","2016","35","5","1182","1195","Cerebral microbleeds (CMBs) are small haemorrhages nearby blood vessels. They have been recognized as important diagnostic biomarkers for many cerebrovascular diseases and cognitive dysfunctions. In current clinical routine, CMBs are manually labelled by radiologists but this procedure is laborious, time-consuming, and error prone. In this paper, we propose a novel automatic method to detect CMBs from magnetic resonance (MR) images by exploiting the 3D convolutional neural network (CNN). Compared with previous methods that employed either low-level hand-crafted descriptors or 2D CNNs, our method can take full advantage of spatial contextual information in MR volumes to extract more representative high-level features for CMBs, and hence achieve a much better detection accuracy. To further improve the detection performance while reducing the computational cost, we propose a cascaded framework under 3D CNNs for the task of CMB detection. We first exploit a 3D fully convolutional network (FCN) strategy to retrieve the candidates with high probabilities of being CMBs, and then apply a well-trained 3D CNN discrimination model to distinguish CMBs from hard mimics. Compared with traditional sliding window strategy, the proposed 3D FCN strategy can remove massive redundant computations and dramatically speed up the detection process. We constructed a large dataset with 320 volumetric MR scans and performed extensive experiments to validate the proposed method, which achieved a high sensitivity of 93.16% with an average number of 2.74 false positives per subject, outperforming previous methods using low-level descriptors or 2D CNNs by a significant margin. The proposed method, in principle, can be adapted to other biomarker detection tasks from volumetric medical data.","","","10.1109/TMI.2016.2528129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403984","3D convolutional neural networks;biomarker detection;cerebral microbleeds;deep learning;susceptibility-weighted imaging","Three-dimensional displays;Feature extraction;Kernel;Biomarkers;MIMICs;Medical diagnostic imaging","biomedical MRI;blood;blood vessels;brain;cognition;diseases;feature extraction;haemodynamics;medical image processing;neurophysiology;probability","automatic cerebral microbleed detection;3D convolutional neural networks;haemorrhages;blood vessels;diagnostic biomarkers;cerebrovascular diseases;cognitive dysfunctions;current clinical routine;radiologists;magnetic resonance images;MRI;low-level hand-crafted descriptors;spatial contextual information;MR volume extraction;representative high-level features;CMB detection;3D fully convolutional network strategy;probabilities;well-trained 3D CNN discrimination;traditional sliding window strategy;3D FCN strategy;massive redundant computations","Biomarkers;Brain;Cerebral Hemorrhage;Databases, Factual;Humans;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Neural Networks (Computer)","195","52","","","","","IEEE","IEEE Journals"
"Scale-Aware Pixelwise Object Proposal Networks","Z. Jie; X. Liang; J. Feng; W. F. Lu; E. H. F. Tay; S. Yan","Department of Mechanical Engineering, National University of Singapore, Singapore; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Qihoo 360 Technology Company, Ltd., Artificial Intelligence Institute, Beijing, China","IEEE Transactions on Image Processing","","2016","25","10","4525","4539","Object proposal is essential for current state-of-the-art object detection pipelines. However, the existing proposal methods generally fail in producing results with satisfying localization accuracy. The case is even worse for small objects, which, however, are quite common in practice. In this paper, we propose a novel scale-aware pixelwise object proposal network (SPOP-net) to tackle the challenges. The SPOP-net can generate proposals with high recall rate and average best overlap, even for small objects. In particular, in order to improve the localization accuracy, a fully convolutional network is employed which predicts locations of object proposals for each pixel. The produced ensemble of pixelwise object proposals enhances the chance of hitting the object significantly without incurring heavy extra computational cost. To solve the challenge of localizing objects at small scale, two localization networks, which are specialized for localizing objects with different scales are introduced, following the divide-and-conquer philosophy. Location outputs of these two networks are then adaptively combined to generate the final proposals by a large-/small-size weighting network. Extensive evaluations on PASCAL VOC 2007 and COCO 2014 show the SPOP network is superior over the state-of-the-art models. The high-quality proposals from SPOP-net also significantly improve the mean average precision of object detection with Fast-Regions with CNN features framework. Finally, the SPOP-net (trained on PASCAL VOC) shows great generalization performance when testing it on ILSVRC 2013 validation set.","","","10.1109/TIP.2016.2593342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516632","Object proposal;convolutional neural networks;deep learning","Proposals;Image segmentation;Computational efficiency;Object detection;Neural networks;Image edge detection;Training","neural nets;object detection","scale-aware pixelwise object proposal networks;object detection;localization accuracy;convolutional network;computational cost;divide-and-conquer;large-/small-size weighting network;PASCAL VOC 2007;COCO 2014;ILSVRC 2013","","10","39","","","","","IEEE","IEEE Journals"
"A Self-Improving Convolution Neural Network for the Classification of Hyperspectral Data","P. Ghamisi; Y. Chen; X. X. Zhu","Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Signal Processing in Earth Observation, Technische Universität München (TUM), Wessling, Munich, GermanyGermany; Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Signal Processing in Earth Observation, Technische Universität München (TUM), Wessling, Munich, GermanyGermany","IEEE Geoscience and Remote Sensing Letters","","2016","13","10","1537","1541","In this letter, a self-improving convolutional neural network (CNN) based method is proposed for the classification of hyperspectral data. This approach solves the so-called curse of dimensionality and the lack of available training samples by iteratively selecting the most informative bands suitable for the designed network via fractional order Darwinian particle swarm optimization. The selected bands are then fed to the classification system to produce the final classification map. Experimental results have been conducted with two well-known hyperspectral data sets: Indian Pines and Pavia University. Results indicate that the proposed approach significantly improves a CNN-based classification method in terms of classification accuracy. In addition, this letter uses the concept of dither for the first time in the remote sensing community to tackle overfitting.","","","10.1109/LGRS.2016.2595108","Alexander von Humboldt Fellowship; Helmholtz Young Investigators Group “SiPEO”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7544576","Convolutional neural network (CNN);deep learning;feature selection;fractional order Darwinian particle swarm optimization (FODPSO);hyperspectral image classification","Hyperspectral imaging;Training;Convolution;Neural networks;Neurons;Feature extraction","geophysical techniques;neural nets;particle swarm optimisation","self-improving convolution neural network based method;hyperspectral data classification;CNN-based classification method;fractional order Darwinian particle swarm optimization;Indian Pines;Pavia University;remote sensing community","","49","14","","","","","IEEE","IEEE Journals"
"What Would They Say? Predicting User's Comments in Pinterest","J. C. Gomez; T. Tommasi; S. Zoghbi; M. F. Moens","KU Leuven, Leuven, Belgium; UNC Chapel Hill, Chapel Hill, NC, USA; KU Leuven, Leuven, Belgium; KU Leuven, Leuven, Belgium","IEEE Latin America Transactions","","2016","14","4","2013","2019","When we refer to an image that attracts our attention, it is natural to mention not only what is literally depicted in the image, but also the sentiments, thoughts and opinions that it invokes in ourselves. In this work we deviate from the standard mainstream tasks of associating tags or keywords to an image, or generating content image descriptions, and we introduce the novel task of automatically generate user comments for an image. We present a new dataset collected from the social media Pinterest and we propose a strategy based on building joint textual and visual user models, tailored to the specificity of the mentioned task. We conduct an extensive experimental analysis of our approach on both qualitative and quantitative terms, which allows to assess the value of the proposed approach and shows its encouraging results against several existing image-to-text methods.","","","10.1109/TLA.2016.7483548","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7483548","Deep-Learning Representation;Multimodal Clustering;Pinterest;Social Media;User Generated Content","Pins;Visualization;Media;Facebook;Standards;Buildings;User-generated content","image processing;social networking (online)","user comment prediction;standard mainstream;image descriptions;social media Pinterest;visual user models;image-to-text methods","","1","","","","","","IEEE","IEEE Journals"
"Task-Driven Progressive Part Localization for Fine-Grained Object Recognition","C. Huang; Z. He; G. Cao; W. Cao","Department of Electrical and Computer Engineering, University of Missouri, Columbia, MO, USA; Department of Electrical and Computer Engineering, University of Missouri, Columbia, MO, USA; School of Computer Science and Software Engineering, East China Normal University, Shanghai, China; College of Information Engineering, Shenzhen University, P. R. China","IEEE Transactions on Multimedia","","2016","18","12","2372","2383","The problem of fine-grained object recognition is very challenging due to the subtle visual differences between different object categories. In this paper, we propose a task-driven progressive part localization (TPPL) approach for fine-grained object recognition. Most existing methods follow a two-step approach that first detects salient object parts to suppress the interference from background scenes and then classifies objects based on features extracted from these regions. The part detector and object classifier are often independently designed and trained. In this paper, our major finding is that the part detector should be jointly designed and progressively refined with the object classifier so that the detected regions can provide the most distinctive features for final object recognition. Specifically, we develop a part-based SPP-net (Part-SPP) as our baseline part detector. We then establish a TPPL framework, which takes the predicted boxes of Part-SPP as an initial guess, and then examines new regions in the neighborhood using a particle swarm optimization approach, searching for more discriminative image regions to maximize the objective function and the recognition performance. This procedure is performed in an iterative manner to progressively improve the joint part detection and object classification performance. Experimental results on the Caltech-UCSD-200-2011 dataset demonstrate that our method outperforms state-of-the-art fine-grained categorization methods both in part localization and classification, even without requiring a bounding box during testing.","","","10.1109/TMM.2016.2602060","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7548295","Deep learning;deformable part-based model;fine-grained recognition;regional convolutional neural network;spatial pyramid pooling","Feature extraction;Object recognition;Detectors;Visualization;Image recognition;Correlation;Semantics","feature extraction;image classification;object detection;object recognition;particle swarm optimisation","task-driven progressive part localization;TPPL approach;fine-grained object recognition;salient object parts detection;feature extraction;object classifier;part-based SPP-net;particle swarm optimization;objective function maximization;Caltech-UCSD-200-2011 dataset;fine-grained categorization","","9","51","","","","","IEEE","IEEE Journals"
"Classification of Detected Changes From Multitemporal High-Res Xband SAR Images: Intensity and Texture Descriptors From SuperPixels","T. L. M. Barreto; R. A. S. Rosa; C. Wimmer; J. R. Moreira; L. S. Bins; F. A. M. Cappabianco; J. Almeida","Department of Remote Sensing Engineering, Bradar Indústria, São José dos Campos, Brazil; Department of Remote Sensing Engineering, Bradar Indústria, São José dos Campos, Brazil; Department of Remote Sensing Engineering, Bradar Indústria, São José dos Campos, Brazil; Department of Remote Sensing Engineering, Bradar Indústria, São José dos Campos, Brazil; National Institute for Space Research (INPE), São José dos Campos, Brazil; Institute of Science and Technology, Federal University of São Paulo (UNIFESP), São José dos Campos, Brazil; Department of Remote Sensing Engineering, Bradar Indústria, São José dos Campos, Brazil","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2016","9","12","5436","5448","Remote sensing has been widely employed for monitoring land cover and usage by change detection techniques. In this paper, we cope with the early detection of the first signs of deforestation, which is the gateway for illegal activities, such as unauthorized urban sprawl and grazing use. In recent years, object-based approaches have emerged as a more suitable alternative than pixel-based methods for change detection in remote sensing images. Even though several classifiers have been tested, there was little effort in selecting appropriated features for the classification of detected changes. After a deep analysis of the existing segmentation, feature extraction, and classification approaches, we propose an object-based methodology that consists of: 1) segmenting multitemporal Xband high-resolution synthetic aperture radar (SAR) images into superpixels employing the simple linear iterative clustering algorithm; 2) extracting features using the object correlation images framework and with the gray-level cooccurrence matrix; and 3) classifying areas into unchanged, deforestation, and other changes by means of a multilayer perceptron supervised learning technique. Experiments were performed using high-resolution SAR images obtained by the airborne sensor OrbiSAR-2 from BRADAR in challenging scenarios of the Brazilian Atlantic Forest, including a wide variety of vegetation, rivers, sea coasts, urban, harvest and open areas, and humidity changes. We perform an extensive experimental analysis of the results, comparing the proposed method with a state-of-the-art approach. The results demonstrate that our method yields an improvement of over 10% in the accuracy while detecting changes and classifying deforested areas.","","","10.1109/JSTARS.2016.2621818","São Paulo Research Foundation—FAPESP; National Counsel of Technological and Scientific Development–CNPq; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7769315","Change detection;classification;multilayer perceptron (MLP);object correlation images (OCIs);remote sensing;simple linear iterative clustering (SLIC);superpixel;synthetic aperture radar (SAR) images","Synthetic aperture radar;Feature extraction;Image segmentation;Remote sensing;Image resolution;Change detection algorithms;Robustness","airborne radar;feature extraction;geophysical image processing;geophysical techniques;image classification;image segmentation;image texture;iterative methods;multilayer perceptrons;radar imaging;synthetic aperture radar;terrain mapping","multitemporal high resolution Xband SAR image;texture descriptor;intensity descriptor;SuperPixels;change detection classification;grazing use;urban sprawl;object-based approach;deep analysis;object-based methodology;feature extraction;linear iterative clustering algorithm;gray-level cooccurrence matrix;multilayer perceptron;high-resolution SAR image;OrbiSAR-2 airborne sensor;BRADAR;Brazilian Atlantic forest;vegetation;river;sea coast;urban mapping;state-of-the-art approach;humidity change;deforested area classification;segmenting multitemporal X-band high-resolution synthetic aperture radar;multilayer perceptron supervised learning technique","","9","57","","","","","IEEE","IEEE Journals"
"SAR ATR by a combination of convolutional neural network and support vector machines","S. A. Wagner","Fraunhofer Institute for High Frequency Physics and Radar Techniques Wachtberg, Germany","IEEE Transactions on Aerospace and Electronic Systems","","2016","52","6","2861","2872","A combination of a convolutional neural network, which belongs to the deep learning research field, and support vector machines is presented as an efficient automatic target recognition system. Additional training methods that incorporate prior knowledge to the classifier and further improve its robustness against imaging errors and target variations are also presented. These methods generate artificial training data by elastic distortion and affine transformations that represent typical examples of image errors, like a changing range scale dependent on the depression angle or an incorrectly estimated aspect angle. With these examples presented to the classifier during the training, the system should become invariant against these variations and thus more robust. For the classification, the spotlight synthetic aperture radar images of the moving and stationary target acquisition and recognition database are used. Results are shown for the ten class database with a forced decision classification as well as with rejection class.","","","10.1109/TAES.2016.160061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7855589","","Training;Synthetic aperture radar;Biological neural networks;Neurons;Machine learning;Target recognition;Support vector machines","convolution;neural nets;object recognition;radar imaging;support vector machines;synthetic aperture radar","target recognition;stationary target acquisition;moving target acquisition;spotlight synthetic aperture radar images;affine transformations;elastic distortion;artificial training data;support vector machines;convolutional neural network;SAR ATR","","45","43","","","","","IEEE","IEEE Journals"
"Multilayer Unmixing for Hyperspectral Imagery With Fast Kernel Archetypal Analysis","G. Zhao; C. Zhao; X. Jia","College of Information and Telecommunications, Harbin Engineering University, Harbin, China; College of Information and Telecommunications, Harbin Engineering University, Harbin, China; School of Engineering and Information Technology, The University of New South Wales, Canberra, Australia","IEEE Geoscience and Remote Sensing Letters","","2016","13","10","1532","1536","The multilayer network in deep learning provides a promising means for rich data representation. Inspired by this approach, we investigate multilayer unmixing for spectral decomposition with fast kernel archetypal analysis (KAA). KAA is used for endmember extraction and abundance estimation simultaneously. To refine the initial unmixing results, a multilayer process is utilized to provide final unmixing results at the end of the network. Moreover, a fast implementation of KAA is proposed via using the Nyström method to relieve KAA's memory issue and decrease the processing time. The proposed method is tested on both synthetic and real hyperspectral image data sets. The results demonstrate that the multilayer unmixing algorithm outperforms the conventional unmixing techniques.","","","10.1109/LGRS.2016.2595102","National Natural Science Foundation of China; Key Program of Heilongjiang Natural Science Foundation; Program Excellent Academic Leaders of Harbin; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542548","Hyperspectral imagery;kernel archetypal analysis (KAA);multilayer network;Nyström method;spectral unmixing","Nonhomogeneous media;Kernel;Hyperspectral imaging;Machine learning;Neurons;Estimation","geophysical image processing;hyperspectral imaging;remote sensing","hyperspectral imagery;fast kernel archetypal analysis;spectral decomposition;Nystrom method;multilayer unmixing algorithm","","5","18","","","","","IEEE","IEEE Journals"
"A Low Power Trainable Neuromorphic Integrated Circuit That Is Tolerant to Device Mismatch","C. S. Thakur; R. Wang; T. J. Hamilton; J. Tapson; A. van Schaik","MARCS Inst., Western Sydney Univ., Sydney, NSW, Australia; MARCS Inst., Western Sydney Univ., Sydney, NSW, Australia; MARCS Inst., Western Sydney Univ., Sydney, NSW, Australia; MARCS Inst., Western Sydney Univ., Sydney, NSW, Australia; MARCS Inst., Western Sydney Univ., Sydney, NSW, Australia","IEEE Transactions on Circuits and Systems I: Regular Papers","","2016","63","2","211","221","Random device mismatch that arises as a result of scaling of the CMOS (complementary metal-oxide semiconductor) technology into the deep submicrometer regime degrades the accuracy of analog circuits. Methods to combat this increase the complexity of design. We have developed a novel neuromorphic system called a trainable analog block (TAB), which exploits device mismatch as a means for random projections of the input to a higher dimensional space. The TAB framework is inspired by the principles of neural population coding operating in the biological nervous system. Three neuronal layers, namely input, hidden, and output, constitute the TAB framework, with the number of hidden layer neurons far exceeding the input layer neurons. Here, we present measurement results of the first prototype TAB chip built using a 65 nm process technology and show its learning capability for various regression tasks. Our TAB chip is tolerant to inherent randomness and variability arising due to the fabrication process. Additionally, we characterize each neuron and discuss the statistical variability of its tuning curve that arises due to random device mismatch, a desirable property for the learning capability of the TAB. We also discuss the effect of the number of hidden neurons and the resolution of output weights on the accuracy of the learning capability of the TAB. We show that the TAB is a low power system-the power dissipation in the TAB with 456 neuron blocks is 1.38 μW.","","","10.1109/TCSI.2015.2512743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403921","Analog integrated circuit design;neural network hardware;neuromorphic engineering;stochastic electronics;Analog integrated circuit design;neural network hardware;neuromorphic engineering;stochastic electronics","Neurons;Sociology;Statistics;Encoding;Tuning;Integrated circuits;Transistors","CMOS analogue integrated circuits;integrated circuit design;low-power electronics;neural chips","low power trainable neuromorphic integrated circuit;device mismatch;CMOS technology;complementary metal-oxide semiconductor technology;analog circuits;trainable analog block;TAB chip;neural population coding;biological nervous system;neuronal layers;hidden layer neurons;size 65 nm;power 1.38 muW","","14","52","","","","","IEEE","IEEE Journals"
"Analyzing Enterprise Storage Workloads With Graph Modeling and Clustering","Y. Zhou; L. Liu; S. Seshadri; L. Chiu","College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; IBM Almaden Research Center, San Jose, CA, USA; IBM Almaden Research Center, San Jose, CA, USA","IEEE Journal on Selected Areas in Communications","","2016","34","3","551","574","Utilizing graph analysis models and algorithms to exploit complex interactions over a network of entities is emerging as an attractive network analytic technology. In this paper, we show that traditional column or row-based trace analysis may not be effective in deriving deep insights hidden in the storage traces collected over complex storage applications, such as complex spatial and temporal patterns, hotspots and their movement patterns. We propose a novel graph analytics framework, GraphLens, for mining and analyzing real world storage traces with three unique features. First, we model storage traces as heterogeneous trace graphs in order to capture multiple complex and heterogeneous factors, such as diverse spatial/temporal access information and their relationships, into a unified analytic framework. Second, we employ and develop an innovative graph clustering method that employs two levels of clustering abstractions on storage trace analysis. We discover interesting spatial access patterns and identify important temporal correlations among spatial access patterns. This enables us to better characterize important hotspots and understand hotspot movement patterns. Third, at each level of abstraction, we design a unified weighted similarity measure through an iterative dynamic weight learning algorithm. With an optimal weight assignment scheme, we can efficiently combine the correlation information for each type of storage access patterns, such as random versus sequential, read versus write, to identify interesting spatial/temporal correlations hidden in the traces. Some optimization techniques on matrix computation are proposed to further improve the efficiency of our clustering algorithm on large trace datasets. Extensive evaluation on real storage traces shows GraphLens can provide broad and deep trace analysis for better storage strategy planning and efficient data placement guidance. GraphLens can be applied to both a single PC with multiple disks and a distributed network across a cluster of compute nodes to offer a few opportunities for optimization of storage performance.","","","10.1109/JSAC.2016.2525478","NSF CISE NetSE program; SaTC program; I/UCRC; IBM faculty award; Intel ICST on Cloud Computing; IEEE International Congress on Big Data; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7397870","Heterogeneous Trace Graph;Random Walk;Unified Weighted Spatial/Temporal Similarity;Spatial Extent Clustering;Temporal Cycle Clustering;Full-rank Approximation;Heterogeneous Trace Graph;Random Walk;Unified Weighted Spatial/Temporal Similarity;Spatial Extent Clustering;Temporal Cycle Clustering;Full-rank Approximation","Analytical models;Correlation;Optimization;Servers;Algorithm design and analysis;Clustering algorithms;Performance evaluation","data mining;graph theory;matrix algebra;optimisation;storage management","enterprise storage workloads;graph modeling;graph clustering;graph analysis;attractive network analytic technology;column-based trace analysis;row-based trace analysis;GraphLens;spatial access patterns;optimization;matrix computation","","1","59","","","","","IEEE","IEEE Journals"
"Generalized Independent Component Analysis Over Finite Alphabets","A. Painsky; S. Rosset; M. Feder","Statistics Department, Tel Aviv University, Tel Aviv, Israel; Statistics Department, Tel Aviv University, Tel Aviv, Israel; Department of Electrical Engineering, Tel Aviv University, Tel Aviv, Israel","IEEE Transactions on Information Theory","","2016","62","2","1038","1053","Independent component analysis (ICA) is a statistical method for transforming an observable multi-dimensional random vector into components that are as statistically independent as possible from each other. Usually, the ICA framework assumes a model according to which the observations are generated (such as a linear transformation with additive noise). ICA over finite fields is a special case of ICA in which both the observations and the independent components are over a finite alphabet. In this paper, we consider a generalization of this framework in which an observation vector is decomposed to its independent components (as much as possible) with no prior assumption on the way it was generated. This generalization is also known as Barlow's minimal redundancy representation problem and is considered an open problem. We propose several theorems and show that this hard problem can be accurately solved with a branch and bound search tree algorithm, or tightly approximated with a series of linear problems. Our contribution provides the first efficient set of solutions to Barlow's problem. The minimal redundancy representation (also known as factorial code) has many applications, mainly in the fields of neural networks and deep learning. The binary ICA is also shown to have applications in several domains, including medical diagnosis, multi-cluster assignment, network tomography, and internet resource management. In this paper, we show that this formulation further applies to multiple disciplines in source coding, such as predictive coding, distributed source coding, and coding of large alphabet sources.","","","10.1109/TIT.2015.2510657","Amichai Painsky from the Israeli Ministry of Science, and by Israeli Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362043","Independent Component Analysis;BICA;ICA over Galois Field;Blind Source Separation;Minimal Redundancy Representation;Minimum Entropy Codes;factorial Codes;Predictive Coding;Distributed Source Coding;Neural Networks;Independent component analysis;BICA;ICA over galois field;blind source separation;minimal redundancy representation;minimum entropy codes;factorial codes;predictive coding;distributed source coding;neural networks","Redundancy;Entropy;Source coding;Complexity theory;Independent component analysis","independent component analysis;search problems;source coding;tree searching;trees (mathematics)","generalized independent component analysis;finite alphabets;ICA;multidimensional random vector;Barlow's minimal redundancy representation problem;branch and bound search tree algorithm;neural networks;factorial code;medical diagnosis;multicluster assignment;network tomography;internet resource management;source coding;predictive coding;distributed source coding;large alphabet sources","","6","35","","","","","IEEE","IEEE Journals"
"Study of Sentiment Classification for Chinese Microblog Based on Recurrent Neural Network","Y. Zhang; Y. Jiang; Y. Tong","NA; NA; NA","Chinese Journal of Electronics","","2016","25","4","601","607","The sentiment classification of Chinese Microblog is a meaningful topic. Many studies has been done based on the methods of rule and word-bag, and to understand the structure information of a sentence will be the next target. We proposed a sentiment classification method based on Recurrent neural network (RNN). We adopted the technology of distributed word representation to construct a vector for each word in a sentence; then train sentence vectors with fixed dimension for different length sentences with RNN, so that the sentence vectors contain both word semantic features and word sequence features; at last use softmax regression classifier in the output layer to predict each sentence's sentiment orientation. Experiment results revealed that our method can understand the structure information of negative sentence and double negative sentence and achieve better accuracy. The way of calculating sentence vector can help to learn the deep structure of sentence and will be valuable for different research area.","","","10.1049/cje.2016.07.002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7534806","","","pattern classification;recurrent neural nets;sentiment analysis;social networking (online)","Chinese microblog;sentiment classification;recurrent neural network;distributed word representation;train sentence vectors;RNN;word semantic features;word sequence features;softmax regression classifier;sentiment orientation;double negative sentence","","5","","","","","","IET","IET Journals"
"TimeSpan: Using Visualization to Explore Temporal Multi-dimensional Data of Stroke Patients","M. H. Loorak; C. Perin; N. Kamal; M. Hill; S. Carpendale","Department of Computer Science, University of Calgary; Department of Computer Science, University of Calgary; Department of Clinical Neurosciences, University of Calgary; Department of Clinical Neurosciences, University of Calgary; Department of Computer Science, University of Calgary","IEEE Transactions on Visualization and Computer Graphics","","2016","22","1","409","418","We present TimeSpan, an exploratory visualization tool designed to gain a better understanding of the temporal aspects of the stroke treatment process. Working with stroke experts, we seek to provide a tool to help improve outcomes for stroke victims. Time is of critical importance in the treatment of acute ischemic stroke patients. Every minute that the artery stays blocked, an estimated 1.9 million neurons and 12 km of myelinated axons are destroyed. Consequently, there is a critical need for efficiency of stroke treatment processes. Optimizing time to treatment requires a deep understanding of interval times. Stroke health care professionals must analyze the impact of procedures, events, and patient attributes on time-ultimately, to save lives and improve quality of life after stroke. First, we interviewed eight domain experts, and closely collaborated with two of them to inform the design of TimeSpan. We classify the analytical tasks which a visualization tool should support and extract design goals from the interviews and field observations. Based on these tasks and the understanding gained from the collaboration, we designed TimeSpan, a web-based tool for exploring multi-dimensional and temporal stroke data. We describe how TimeSpan incorporates factors from stacked bar graphs, line charts, histograms, and a matrix visualization to create an interactive hybrid view of temporal data. From feedback collected from domain experts in a focus group session, we reflect on the lessons we learned from abstracting the tasks and iteratively designing TimeSpan.","","","10.1109/TVCG.2015.2467325","AITF, NSERC, GRAND, Surfnet, and SMART Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192713","Multi-dimensional data,;Temporal event sequences;Electronic health records;Multi-dimensional data;Temporal event sequences;Electronic health records","Data visualization;Hospitals;Delays;Visualization;Interviews;Computed tomography;Needles","data visualisation;diseases;Internet;medical computing;patient treatment","temporal multidimensional stroke data;TimeSpan;exploratory visualization tool;temporal aspects;stroke victims;acute ischemic stroke patients treatment;artery;interactive hybrid view;matrix visualization;histograms;line charts;stacked bar graphs;Web-based tool;analytical tasks;quality of life;stroke health care professionals;stroke treatment processes;myelinated axons;neurons","Computer Graphics;Electronic Health Records;Humans;Medical Informatics;Stroke;Time Factors","21","37","","","","","IEEE","IEEE Journals"
"Guest Editorial Special Section on Learning in Non-(geo)metric Spaces","M. Pelillo; E. R. Hancock; X. Li; V. Murino","Department of Environmental SciencesInformatics and Statistics, Ca’ Foscari University of Venice Via Torino 155, Venezia Mestre, Italy; Department of Computer Science, University of York, York, U.K.; Chinese Academy of Sciences Xi’an, Xi’an Institute of Optics and Precision Mechanics, Shaanxi, P. R. China; Pattern Analysis & Computer Vision (PAVIS), Istituto Italiano di Tecnologia (IIT) Via Morego 30, Genova, Italy","IEEE Transactions on Neural Networks and Learning Systems","","2016","27","6","1290","1293","Traditional machine learning and pattern recognition techniques are intimately linked to the notion of feature spaces. Adopting this view, each object is described in terms of a vector of numerical attributes and is, therefore, mapped to a point in a Euclidean (geometric) vector space, so that the distances between the points reflect the observed (dis)similarities between the respective objects. This kind of representation is attractive because geometric spaces offer powerful analytical as well as computational tools that are simply not available in other representations. Indeed, classical machine learning methods are tightly related to geometrical concepts, and numerous powerful tools have been developed during the last few decades, starting from the maximal likelihood method in the 1920s to perceptrons in the 1960s and, more recently, to kernel machines and deep learning architectures.","","","10.1109/TNNLS.2016.2522770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7470326","","Special issues and sections;Machine learning;Learning systems;Pattern recognition;Integer linear programming;Geometry;Deep learning;Computational modeling","","","","","","","","","","IEEE","IEEE Journals"
"Guest Editorial Deep Learning in Medical Imaging: Overview and Future Promise of an Exciting New Technique","H. Greenspan; B. van Ginneken; R. M. Summers","Biomedical Image Computing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel-Aviv University, Tel-Aviv, Israel; Diagnostic Image Analysis Group, Radboud University Medical Center, Nijmegen, The Netherlands; Imaging Biomarkers and Computer-Aided Diagnosis Lab, Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Bethesda, MD, USA","IEEE Transactions on Medical Imaging","","2016","35","5","1153","1159","The papers in this special section focus on the technology and applications supported by deep learning. Deep learning is a growing trend in general data analysis and has been termed one of the 10 breakthrough technologies of 2013. Deep learning is an improvement of artificial neural networks, consisting of more layers that permit higher levels of abstraction and improved predictions from data. To date, it is emerging as the leading machine-learning tool in the general imaging and computer vision domains. In particular, convolutional neural networks (CNNs) have proven to be powerful tools for a broad range of computer vision tasks. Deep CNNs automatically learn mid-level and high-level abstractions obtained from raw data (e.g., images). Recent results indicate that the generic descriptors extracted from CNNs are extremely effective in object recognition and localization in natural images. Medical image analysis groups across the world are quickly entering the field and applying CNNs and other deep learning methodologies to a wide variety of applications. ","","","10.1109/TMI.2016.2553401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7463094","","Special issues and sections;Machine learning;Computer vision;Biomedical image processing;Data analysis;Artificial neural networks","","","","478","38","","","","","IEEE","IEEE Journals"
"Editorial: “One Robot is Robotics, Ten Robots is Automation”","K. Goldberg","University of California at Berkeley","IEEE Transactions on Automation Science and Engineering","","2016","13","4","1418","1419","Automation has come of age. As Raja Chatila aptly summarized in the quote above, automation addresses the challenges that arise when robots scale beyond proof-of-concept. Germany approved a $5B sale of Kuka to Midea Corp in China to provide robots for assembly automation. General Electric Corporation is now focusing on automation algorithms and data analytics and predicts this will be a $225 billion market within five years. These developments build on ongoing research in Big Data, Cloud Computing, Deep Learning, Open-Source Software, and Government/Industry initiatives, such as The “Internet of Things,” “Smarter Planet,” “Industrial Internet,” “Industrie 4.0,” and “Made in China 2025.” Automation is playing an increasingly central role in the global economy and in our daily lives.","","","10.1109/TASE.2016.2606859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7583767","","","","","","","","","","","","IEEE","IEEE Journals"
"Ieee transactions on neural networks and learning systems special section on deep reinforcement learning and adaptive dynamic programming","","","IEEE Transactions on Neural Networks and Learning Systems","","2016","27","11","2454","2454","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","","","10.1109/TNNLS.2016.2608440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7605593","","","","","","","","","","","","IEEE","IEEE Journals"
"IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming","","","IEEE Transactions on Neural Networks and Learning Systems","","2016","27","12","2776","2776","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","","","10.1109/TNNLS.2016.2621245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7746018","","","","","","","","","","","","IEEE","IEEE Journals"
"Special Issue on Deep Learning for Visual Surveillance","","","IEEE Transactions on Circuits and Systems for Video Technology","","2016","26","11","2159","2160","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","","","10.1109/TCSVT.2016.2620358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724015","","","","","","","","","","","","IEEE","IEEE Journals"
