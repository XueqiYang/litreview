"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,"Reference Count","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Deep Cascade Model-Based Face Recognition: When Deep-Layered Learning Meets Small Data","L. Zhang; J. Liu; B. Zhang; D. Zhang; C. Zhu","School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China; School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China; Department of Computer and Information Science, University of Macau, Macau, China; School of Science and Engineering, The Chinese University of Hong Kong at Shenzhen, Shenzhen, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Image Processing","","2020","29","","1016","1029","Sparse representation based classification (SRC), nuclear-norm matrix regression (NMR), and deep learning (DL) have achieved a great success in face recognition (FR). However, there still exist some intrinsic limitations among them. SRC and NMR based coding methods belong to one-step model, such that the latent discriminative information of the coding error vector cannot be fully exploited. DL, as a multi-step model, can learn powerful representation, but relies on large-scale data and computation resources for numerous parameters training with complicated back-propagation. Straightforward training of deep neural networks from scratch on small-scale data is almost infeasible. Therefore, in order to develop efficient algorithms that are specifically adapted for small-scale data, we propose to derive the deep models of SRC and NMR. Specifically, in this paper, we propose an end-to-end deep cascade model (DCM) based on SRC and NMR with hierarchical learning, nonlinear transformation and multi-layer structure for corrupted face recognition. The contributions include four aspects. First, an end-to-end deep cascade model for small-scale data without back-propagation is proposed. Second, a multi-level pyramid structure is integrated for local feature representation. Third, for introducing nonlinear transformation in layer-wise learning, softmax vector coding of the errors with class discrimination is proposed. Fourth, the existing representation methods can be easily integrated into our DCM framework. Experiments on a number of small-scale benchmark FR datasets demonstrate the superiority of the proposed model over state-of-the-art counterparts. Additionally, a perspective that deep-layered learning does not have to be convolutional neural network with back-propagation optimization is consolidated. The demo code is available in https://github.com/liuji93/DCM","","","10.1109/TIP.2019.2938307","National Natural Science Foundation of China; Chongqing Youth Talent Program; Fundamental Research Funds of Chongqing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825855","Deep cascade model;softmax vector;representation learning;face recognition;corruption","Image coding;Encoding;Face recognition;Nuclear magnetic resonance;Deep learning;Data models;Sparse matrices","face recognition;feature extraction;image classification;image representation;learning (artificial intelligence);neural nets;regression analysis","deep neural networks;SRC;NMR;end-to-end deep cascade model;hierarchical learning;nonlinear transformation;multilayer structure;corrupted face recognition;multilevel pyramid structure;layer-wise learning;softmax vector coding;small-scale benchmark FR datasets;deep-layered learning;deep cascade model-based face recognition;meets small data;sparse representation;nuclear-norm matrix regression;deep learning;coding methods;one-step model;coding error vector;multistep model;large-scale data","","","55","","","","","IEEE","IEEE Journals"
"Bayesian Deep-Learning-Based Health Prognostics Toward Prognostics Uncertainty","W. Peng; Z. Ye; N. Chen","School of Intelligent Systems Engineering, Sun Yat-sen University, Guangzhou, China; Department of Industrial Systems Engineering and Management, Sembcorp-NUS Corporate Laboratory, Faculty of Engineering, National University of Singapore, Singapore; Department of Industrial Systems Engineering and Management, Sembcorp-NUS Corporate Laboratory, Faculty of Engineering, National University of Singapore, Singapore","IEEE Transactions on Industrial Electronics","","2020","67","3","2283","2293","Deep-learning-based health prognostics is receiving ever-increasing attention. Most existing methods leverage advanced neural networks for prognostics performance improvement, providing mainly point estimates as prognostics results without addressing prognostics uncertainty. However, uncertainty is critical for both health prognostics and subsequent decision making, especially for safety-critical applications. Inspired by the idea of Bayesian machine learning, a Bayesian deep-learning-based (BDL-based) method is proposed in this paper for health prognostics with uncertainty quantification. State-of-the-art deep learning models are extended into Bayesian neural networks (BNNs), and a variational-inference-based method is presented for the BNNs learning and inference. The proposed method is validated through a ball bearing dataset and a turbofan engine dataset. Other than point estimates, health prognostics using the BDL-based method is enhanced with uncertainty quantification. Scalability and generalization ability of state-of-the-art deep learning models can be well inherited. Stochastic regularization techniques, widely available in mainstream software libraries, can be leveraged to efficiently implement the BDL-based method for practical applications.","","","10.1109/TIE.2019.2907440","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; National Research Foundation; National University of Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8681720","Bayesian deep neural network;Bayesian reliability;deep learning;health prognostics;remaining useful life (RUL)","Uncertainty;Bayes methods;Deep learning;Data models;Neural networks;Training","Bayes methods;condition monitoring;learning (artificial intelligence);neural nets;production engineering computing;remaining life assessment;stochastic processes","prognostics performance improvement;prognostics uncertainty;Bayesian machine learning;uncertainty quantification;Bayesian neural networks;variational-inference-based method;BDL-based method;Bayesian deep-learning-based health prognostics;deep learning models;ball bearing dataset;turbofan engine dataset;stochastic regularization techniques","","","46","Traditional","","","","IEEE","IEEE Journals"
"Variational-Based Mixed Noise Removal With CNN Deep Learning Regularization","F. Wang; H. Huang; J. Liu","Laboratory of Mathematics and Complex Systems (Ministry of Education of China), School of Mathematical Sciences, Beijing Normal University, Beijing, China; Laboratory of Mathematics and Complex Systems (Ministry of Education of China), School of Mathematical Sciences, Beijing Normal University, Beijing, China; Laboratory of Mathematics and Complex Systems (Ministry of Education of China), School of Mathematical Sciences, Beijing Normal University, Beijing, China","IEEE Transactions on Image Processing","","2020","29","","1246","1258","In this paper, the traditional model based variational methods and deep learning based algorithms are naturally integrated to address mixed noise removal, specially for Gaussian mixture noise and Gaussian-impulse noise removal problem. To be different from single type noise (e.g. Gaussian) removal, it is a challenge problem to accurately discriminate noise types and levels for each pixel. We propose a variational method to iteratively estimate the noise parameters, and then the algorithm can automatically classify the noise according to the different statistical parameters. The proposed variational problem can be separated into regularization, synthesis, parameters estimation and noise classification four steps with the operator splitting scheme. Each step is related to an optimization subproblem. To enforce the regularization, the deep learning method is employed to learn the natural images prior. Compared with some model based regularizations, the CNN regularizer can significantly improve the quality of the restored images. Compared with some learning based methods, the synthesis step can produce better reconstructions by analyzing the types and levels of the recognized noise. In our method, the convolution neutral network (CNN) can be regarded as an operator which associated to a variational functional. From this viewpoint, the proposed method can be extended to many image reconstruction and inverse problems. Numerical experiments in the paper show that our method can achieve some state-of-the-art results for Gaussian mixture noise and Gaussian-impulse noise removal.","","","10.1109/TIP.2019.2940496","National Basic Research Program of China (973 Program); National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8848828","Deep learning;CNN;regularization;mixed noise;EM algorithm;image restoration","Deep learning;Noise reduction;Image restoration;Learning systems;TV;Data models","convolutional neural nets;Gaussian noise;image classification;image denoising;image restoration;impulse noise;inverse problems;learning (artificial intelligence);variational techniques","Gaussian mixture noise;Gaussian-impulse noise removal problem;parameters estimation;model based regularizations;image reconstruction;inverse problems;CNN deep learning regularization;deep learning based algorithms;variational-based mixed noise removal;noise classification;natural image learning;restored image quality;convolution neutral network","","","48","","","","","IEEE","IEEE Journals"
"Mumford–Shah Loss Functional for Image Segmentation With Deep Learning","B. Kim; J. C. Ye","Department of Bio and Brain Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; Department of Bio and Brain Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea","IEEE Transactions on Image Processing","","2020","29","","1856","1866","Recent state-of-the-art image segmentation algorithms are mostly based on deep neural networks, thanks to their high performance and fast computation time. However, these methods are usually trained in a supervised manner, which requires large number of high quality ground-truth segmentation masks. On the other hand, classical image segmentation approaches such as level-set methods are formulated in a self-supervised manner by minimizing energy functions such as Mumford-Shah functional, so they are still useful to help generate segmentation masks without labels. Unfortunately, these algorithms are usually computationally expensive and often have limitation in semantic segmentation. In this paper, we propose a novel loss function based on Mumford-Shah functional that can be used in deep-learning based image segmentation without or with small labeled data. This loss function is based on the observation that the softmax layer of deep neural networks has striking similarity to the characteristic function in the Mumford-Shah functional. We show that the new loss function enables semi-supervised and unsupervised segmentation. In addition, our loss function can also be used as a regularized function to enhance supervised semantic segmentation algorithms. Experimental results on multiple datasets demonstrate the effectiveness of the proposed method.","","","10.1109/TIP.2019.2941265","National Research Foundation of Korea; Industrial Strategic Technology Development Program, Development of Novel Artificial Intelligence Technologies to Assist Imaging Diagnosis of Pulmonary, Hepatic, and Cardiac Disease and Their Integration into Commercial Clinical PACS Platforms); Ministry of Trade, Industry and Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851405","Semi-supervised learning;unsupervised learning;image segmentation;Mumford-Shah functional","Image segmentation;Semantics;Neural networks;Minimization;Deep learning;Training data;Unsupervised learning","image segmentation;neural nets;supervised learning;unsupervised learning","loss function;supervised semantic segmentation algorithms;Mumford-Shah loss functional;deep learning;deep neural networks;high quality ground-truth segmentation masks;image segmentation;energy functions;unsupervised segmentation;semi-supervised segmentation","","","53","IEEE","","","","IEEE","IEEE Journals"
"Semi-Supervised Deep Coupled Ensemble Learning With Classification Landmark Exploration","J. Li; S. Wu; C. Liu; Z. Yu; H. Wong","School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Department of Computer Science, Shantou University, Shantou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Department of Computer Science, City University of Hong Kong, Hong Kong","IEEE Transactions on Image Processing","","2020","29","","538","550","Using an ensemble of neural networks with consistency regularization is effective for improving performance and stability of deep learning, compared to the case of a single network. In this paper, we present a semi-supervised Deep Coupled Ensemble (DCE) model, which contributes to ensemble learning and classification landmark exploration for better locating the final decision boundaries in the learnt latent space. First, multiple complementary consistency regularizations are integrated into our DCE model to enable the ensemble members to learn from each other and themselves, such that training experience from different sources can be shared and utilized during training. Second, in view of the possibility of producing incorrect predictions on a number of difficult instances, we adopt class-wise mean feature matching to explore important unlabeled instances as classification landmarks, on which the model predictions are more reliable. Minimizing the weighted conditional entropy on unlabeled data is able to force the final decision boundaries to move away from important training data points, which facilitates semi-supervised learning. Ensemble members could eventually have similar performance due to consistency regularization, and thus only one of these members is needed during the test stage, such that the efficiency of our model is the same as the non-ensemble case. Extensive experimental results demonstrate the superiority of our proposed DCE model over existing state-of-the-art semi-supervised learning methods.","","","10.1109/TIP.2019.2933724","Research Grants Council of the Hong Kong Special Administration Region; National Natural Science Foundation of China; City University of Hong Kong; Natural Science Foundation of Guangdong Province; Fundamental Research Funds for the Central Universities; National Key R&D Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8796363","Semi-supervised classification;deep ensemble;consistency regularization;landmark learning","Training;Predictive models;Semisupervised learning;Data models;Reliability;Entropy;Computer science","entropy;image classification;image matching;learning (artificial intelligence);neural nets","classification landmark exploration;neural networks;consistency regularization;stability;deep learning;Ensemble model;learnt latent space;multiple complementary consistency regularizations;DCE model;ensemble members;training experience;classification landmarks;model predictions;nonensemble case;training data points;semisupervised deep coupled ensemble learning;unlabeled instances;class-wise mean feature matching","","","59","","","","","IEEE","IEEE Journals"
"Multi-Task Deep Relative Attribute Learning for Visual Urban Perception","W. Min; S. Mei; L. Liu; Y. Wang; S. Jiang","Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2020","29","","657","669","Visual urban perception aims to quantify perceptual attributes (e.g., safe and depressing attributes) of physical urban environment from crowd-sourced street-view images and their pairwise comparisons. It has been receiving more and more attention in computer vision for various applications, such as perceptive attribute learning and urban scene understanding. Most existing methods adopt either 1) a regression model trained using image features and ranked scores converted from pairwise comparisons for perceptual attribute prediction or 2) a pairwise ranking algorithm to independently learn each perceptual attribute. However, the former fails to directly exploit pairwise comparisons while the latter ignores the relationship among different attributes. To address them, we propose a multi-task deep relative attribute learning network (MTDRALN) to learn all the relative attributes simultaneously via multi-task Siamese networks, where each Siamese network will predict one relative attribute. Combined with deep relative attribute learning, we utilize the structured sparsity to exploit the prior from natural attribute grouping, where all the attributes are divided into different groups based on semantic relatedness in advance. As a result, MTDRALN is capable of learning all the perceptual attributes simultaneously via multi-task learning. Besides the ranking sub-network, MTDRALN further introduces the classification sub-network, and these two types of losses from two sub-networks jointly constrain parameters of the deep network to make the network learn more discriminative visual features for relative attribute learning. In addition, our network can be trained in an end-to-end way to make deep feature learning and multi-task relative attribute learning reinforces each other. Extensive experiments on the large-scale Place Pulse 2.0 dataset validate the advantage of our proposed network. Our qualitative results along with visualization of saliency maps also show that the proposed network is able to learn effective features for perceptual attributes.","","","10.1109/TIP.2019.2932502","National Natural Science Foundation of China; Natural Science Foundation of Beijing Municipality; National Program for Special Support of Eminent Professionals; National Program for Support of Top-notch Young Professionals; State Key Laboratory of Robotics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8790964","Visual urban perception;relative attribute;multi-task learning","Visualization;Task analysis;Deep learning;Urban areas;Correlation;Computer vision;Predictive models","computer vision;data visualisation;feature extraction;learning (artificial intelligence);regression analysis;visual perception","urban scene understanding;natural attribute grouping;ranking sub-network;visual urban perception;physical urban environment;crowd-sourced street-view images;multitask deep relative attribute learning;computer vision;perceptive attribute learning;regression model;image features;ranked scores;pairwise ranking algorithm;MTDRALN;Siamese network;relative attribute prediction;discriminative visual features;large-scale Place Pulse 2.0 dataset;saliency map visualization","","","56","","","","","IEEE","IEEE Journals"
"Deep-NFVOrch: leveraging deep reinforcement learning to achieve adaptive vNF service chaining in DCI-EONs","B. Li; W. Lu; Z. Zhu","School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui 230027, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui 230027, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui 230027, China","IEEE/OSA Journal of Optical Communications and Networking","","2020","12","1","A18","A27","Due to the rise of cloud computing, how to realize adaptive and cost-effective virtual network function service chaining (vNF-SC) in a datacenter interconnection based on an elastic optical network (DCI-EON) has become an interesting but challenging problem. In this work, we tackle this problem by optimizing the design of a deep reinforcement learning (DRL)-based adaptive service framework, namely, Deep-NFVOrch. Specifically, Deep-NFVOrch works in service cycles and tries to reduce the setup latency of vNF-SC by invoking request prediction and pre-deployment at the beginning of each service cycle. We introduce a DRL-based observer (DRL-Observer) to select the duration of each service cycle adaptively according to the network status. The DRL-Observer is designed based on the advantage actor critic, which can interact with the network environment constantly through its deep neural network and learn how to make wise decisions based on the environment's feedback. Our simulation results demonstrate that the DRL-Observer converges fast in online training with the help of a few asynchronous training threads, and the Deep-NFVOrch with it achieves better performance than several benchmarks, in terms of balancing the tradeoffs among overall resource utilization, vNF-SC request-blocking probability, and the number of network reconfigurations in a DCI-EON.","","","10.1364/JOCN.12.000A18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854564","","Training;Optical fiber networks;Resource management;Reinforcement learning;Adaptive systems;Neural networks","cloud computing;computer centres;learning (artificial intelligence);neural nets;observers;optical communication;probability;resource allocation;synchronisation;telecommunication control;virtualisation","Deep-NFVOrch;deep reinforcement learning;datacenter interconnection;cloud computing;DRL-based observer;elastic optical network;adaptive vNF service chaining;DCI-EON;vNF-SC request-blocking probability;deep neural network","","","","","","","","IEEE","IEEE Journals"
"VerifyNet: Secure and Verifiable Federated Learning","G. Xu; H. Li; S. Liu; K. Yang; X. Lin","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Computer Science, The University of Memphis, Memphis, TN, USA; School of Computer Science, University of Guelph, Guelph, ON, Canada","IEEE Transactions on Information Forensics and Security","","2020","15","","911","926","As an emerging training model with neural networks, federated learning has received widespread attention due to its ability to update parameters without collecting users' raw data. However, since adversaries can track and derive participants' privacy from the shared gradients, federated learning is still exposed to various security and privacy threats. In this paper, we consider two major issues in the training process over deep neural networks (DNNs): 1) how to protect user's privacy (i.e., local gradients) in the training process and 2) how to verify the integrity (or correctness) of the aggregated results returned from the server. To solve the above problems, several approaches focusing on secure or privacy-preserving federated learning have been proposed and applied in diverse scenarios. However, it is still an open problem enabling clients to verify whether the cloud server is operating correctly, while guaranteeing user's privacy in the training process. In this paper, we propose VerifyNet, the first privacy-preserving and verifiable federated learning framework. In specific, we first propose a double-masking protocol to guarantee the confidentiality of users' local gradients during the federated learning. Then, the cloud server is required to provide the Proof about the correctness of its aggregated results to each user. We claim that it is impossible that an adversary can deceive users by forging Proof, unless it can solve the NP-hard problem adopted in our model. In addition, VerifyNet is also supportive of users dropping out during the training process. The extensive experiments conducted on real-world data also demonstrate the practical performance of our proposed scheme.","","","10.1109/TIFS.2019.2929409","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Fundamental Research Funds for Chinese Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765347","Privacy-preserving;deep learning;verifiable federated learning;cloud computing","Training;Servers;Deep learning;Privacy;Biological neural networks;Security","cloud computing;computational complexity;data privacy;learning (artificial intelligence);neural nets","VerifyNet;shared gradients;deep neural networks;cloud server;verifiable federated learning framework;secure federated learning;participants privacy;privacy-preserving federated learning;users raw data;double-masking protocol;users local gradients;NP-hard problem;users dropping","","2","37","","","","","IEEE","IEEE Journals"
"Deep Coupled ISTA Network for Multi-Modal Image Super-Resolution","X. Deng; P. L. Dragotti","Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.","IEEE Transactions on Image Processing","","2020","29","","1683","1698","Given a low-resolution (LR) image, multi-modal image super-resolution (MISR) aims to find the high-resolution (HR) version of this image with the guidance of an HR image from another modality. In this paper, we use a model-based approach to design a new deep network architecture for MISR. We first introduce a novel joint multi-modal dictionary learning (JMDL) algorithm to model cross-modality dependency. In JMDL, we simultaneously learn three dictionaries and two transform matrices to combine the modalities. Then, by unfolding the iterative shrinkage and thresholding algorithm (ISTA), we turn the JMDL model into a deep neural network, called deep coupled ISTA network. Since the network initialization plays an important role in deep network training, we further propose a layer-wise optimization algorithm (LOA) to initialize the parameters of the network before running back-propagation strategy. Specifically, we model the network initialization as a multi-layer dictionary learning problem, and solve it through convex optimization. The proposed LOA is demonstrated to effectively decrease the training loss and increase the reconstruction accuracy. Finally, we compare our method with other state-of-the-art methods in the MISR task. The numerical results show that our method consistently outperforms others both quantitatively and qualitatively at different upscaling factors for various multi-modal scenarios.","","","10.1109/TIP.2019.2944270","CSC Imperial Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858035","Multi-modal image super-resolution;ISTA;dictionary learning;deep neural network","Machine learning;Dictionaries;Neural networks;Thresholding (Imaging);Iterative algorithms;Image reconstruction","convex programming;image reconstruction;image resolution;image segmentation;iterative methods;learning (artificial intelligence);neural net architecture;optimisation","multimodal image super-resolution;low-resolution image;high-resolution version;deep network architecture;joint multimodal dictionary learning;cross-modality dependency;deep neural network;network initialization;deep network training;multimodal scenarios;deep coupled ISTA network;multilayer dictionary learning;iterative shrinkage and thresholding algorithm;layer-wise optimization algorithm","","1","75","IEEE","","","","IEEE","IEEE Journals"
"Deep Learning Hierarchical Representation From Heterogeneous Flow-Level Communication Data","G. Shao; X. Chen; X. Zeng; L. Wang","Cybersecurity Research Institute, Sichuan University, Chengdu, China; College of Cybersecurity, Sichuan University, Chengdu, China; Cybersecurity Research Institute, Sichuan University, Chengdu, China; College of Cybersecurity, Sichuan University, Chengdu, China","IEEE Transactions on Information Forensics and Security","","2020","15","","1525","1540","The success of a detection model depends heavily on feature engineering. Deep learning has been successfully applied in numerous research fields as a universal representation learning method. However, the heterogeneity of flow-level communication data obstructs the application of deep learning to communication representation learning, and research on this problem is still lacking. To cope with this problem, we propose a heterogeneous communication data-encoding approach to extract fixed-size encoding data to apply deep learning to heterogeneous communication data by preserving the spatiotemporal characteristics of the data. Then, we propose a feature extractor based on deep learning to automatically learn hierarchical and robust communication representations without expert knowledge. We show that the proposed approach can replicate and optimize the key steps of feature engineering well and learn hierarchical representations directly from heterogeneous communication data. Moreover, compared with features extracted with principal component analysis (PCA), manifold learning and manually crafted methods, the features extracted by deep learning are more robust and are characterized by their better adaptability to various classifiers and datasets. To the best of our knowledge, the initial work here is the first to apply deep learning techniques to heterogeneous flow-level data; consequently, the heterogeneous communication data processing method can provide technical means for the application of deep learning in other communication-related research fields.","","","10.1109/TIFS.2019.2943236","National Natural Science Foundation of China; National Entrepreneurship and Innovation Demonstration Base of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846762","Deep learning;communication behavior;representation learning;feature extraction;convolution neural network","Feature extraction;Deep learning;Computer security;Data mining;Encoding;Principal component analysis;Spatiotemporal phenomena","","","","","35","IEEE","","","","IEEE","IEEE Journals"
"Deep MR Brain Image Super-Resolution Using Spatio-Structural Priors","V. Cherukuri; T. Guo; S. J. Schiff; V. Monga","Department of Electrical Engineering, The Pennsylvania State University, University Park, PA, USA; Department of Electrical Engineering, The Pennsylvania State University, University Park, PA, USA; Center for Neural Engineering, The Pennsylvania State University, University Park, PA, USA; Department of Electrical Engineering, The Pennsylvania State University, University Park, PA, USA","IEEE Transactions on Image Processing","","2020","29","","1368","1383","High resolution Magnetic Resonance (MR) images are desired for accurate diagnostics. In practice, image resolution is restricted by factors like hardware and processing constraints. Recently, deep learning methods have been shown to produce compelling state-of-the-art results for image enhancement/super-resolution. Paying particular attention to desired hi-resolution MR image structure, we propose a new regularized network that exploits image priors, namely a low-rank structure and a sharpness prior to enhance deep MR image super-resolution (SR). Our contributions are then incorporating these priors in an analytically tractable fashion as well as towards a novel prior guided network architecture that accomplishes the super-resolution task. This is particularly challenging for the low rank prior since the rank is not a differentiable function of the image matrix (and hence the network parameters), an issue we address by pursuing differentiable approximations of the rank. Sharpness is emphasized by the variance of the Laplacian which we show can be implemented by a fixed feedback layer at the output of the network. As a key extension, we modify the fixed feedback (Laplacian) layer by learning a new set of training data driven filters that are optimized for enhanced sharpness. Experiments performed on publicly available MR brain image databases and comparisons against existing state-of-the-art methods show that the proposed prior guided network offers significant practical gains in terms of improved SNR/image quality measures. Because our priors are on output images, the proposed method is versatile and can be combined with a wide variety of existing network architectures to further enhance their performance.","","","10.1109/TIP.2019.2942510","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8848868","MR;deep learning;priors;low-rank","Laplace equations;Deep learning;Training;Spatial resolution;Interpolation","biomedical MRI;brain;image enhancement;image filtering;learning (artificial intelligence);medical image processing","super-resolution task;image matrix;network parameters;fixed feedback layer;prior guided network;high resolution magnetic resonance images;accurate diagnostics;image resolution;deep learning methods;regularized network;image priors;deep MR brain image super-resolution;image quality","","","54","","","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Weak Human Activity Localization","W. Xu; Z. Miao; J. Yu; Q. Ji","Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Department of Electrical and Computer Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA","IEEE Transactions on Image Processing","","2020","29","","1522","1535","Human activity localization aims at recognizing contents and detecting locations of activities in video sequences. With an increasing number of untrimmed video data, traditional activity localization methods always suffer from two major limitations. First, detailed annotations are needed in most existing methods, i.e., bounding-box annotations in every frame, which are both expensive and time consuming. Second, the search space is too large for 3D activity localization, which requires generating a large number of proposals. In this paper, we propose a unified deep Q-network with weak reward and weak loss (DWRLQN) to address the two problems. Certain weak knowledge and weak constraints involving the temporal dynamics of human activity are incorporated into a deep reinforcement learning framework under sparse spatial supervision, where we assume that only a portion of frames are annotated in each video sequence. Experiments on UCF-Sports, UCF-101 and sub-JHMDB demonstrate that our proposed model achieves promising performance by only utilizing a very small number of proposals. More importantly, our DWRLQN trained with partial annotations and weak information even outperforms fully supervised methods.","","","10.1109/TIP.2019.2942814","National Natural Science Foundation of China; China Postdoctoral Science Foundation; National Key Technology R&D Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8850319","Activity localization;deep reinforcement learning;weak constraint;weak supervision","Reinforcement learning;Proposals;Video sequences;Training;Feature extraction;Task analysis;Computational modeling","feature extraction;image motion analysis;image sequences;object detection;supervised learning;video signal processing","video sequence;supervised methods;video data;3D activity localization;deep reinforcement learning;human activity localization;deep Q-network with weak reward and weak loss;sparse spatial supervision","","","56","","","","","IEEE","IEEE Journals"
"Scalable Deep Hashing for Large-Scale Social Image Retrieval","H. Cui; L. Zhu; J. Li; Y. Yang; L. Nie","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Technology, Shandong University, Jinan, China","IEEE Transactions on Image Processing","","2020","29","","1271","1284","Recent years have witnessed the wide application of hashing for large-scale image retrieval, because of its high computation efficiency and low storage cost. Particularly, benefiting from current advances in deep learning, supervised deep hashing methods have greatly boosted the retrieval performance, under the strong supervision of large amounts of manually annotated semantic labels. However, their performance is highly dependent upon the supervised labels, which significantly limits the scalability. In contrast, unsupervised deep hashing without label dependence enjoys the advantages of well scalability. Nevertheless, due to the relaxed hash optimization, and more importantly, the lack of semantic guidance, existing methods suffer from limited retrieval performance. In this paper, we propose a SCAlable Deep Hashing (SCADH) to learn enhanced hash codes for social image retrieval. We formulate a unified scalable deep hash learning framework which explores the weak but free supervision of discriminative user tags that are commonly accompanied with social images. It jointly learns image representations and hash functions with deep neural networks, and simultaneously enhances the discriminative capability of image hash codes with the refined semantics from the accompanied social tags. Further, instead of simple relaxed hash optimization, we propose a discrete hash optimization method based on Augmented Lagrangian Multiplier to directly solve the hash codes and avoid the binary quantization information loss. Experiments on two standard social image datasets demonstrate the superiority of the proposed approach compared with state-of-the-art shallow and deep hashing techniques.","","","10.1109/TIP.2019.2940693","National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839750","Unsupervised deep hashing;social tags;augmented Lagrangian multiplier;discrete optimization","Semantics;Image retrieval;Optimization;Hash functions;Scalability;Quantization (signal);Visualization","cryptography;file organisation;image coding;image representation;image retrieval;neural nets;optimisation;social networking (online);supervised learning;unsupervised learning","large-scale social image retrieval;deep learning;supervised deep hashing;unsupervised deep hashing;image representations;deep neural networks;image hash codes;social tags;discrete hash optimization;scalable deep hash learning;augmented Lagrangian multiplier;binary quantization information loss;social Websites;social network images;social image databases","","","53","","","","","IEEE","IEEE Journals"
"Deep Learning in Ultrasound Imaging","R. J. G. van Sloun; R. Cohen; Y. C. Eldar","Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, MB, The Netherlands; Technion, Haifa, Israel; Faculty of Mathematics and Computer Science, Weizmann Institute of Science, Rehovot, Israel","Proceedings of the IEEE","","2020","108","1","11","29","In this article, we consider deep learning strategies in ultrasound systems, from the front end to advanced applications. Our goal is to provide the reader with a broad understanding of the possible impact of deep learning methodologies on many aspects of ultrasound imaging. In particular, we discuss methods that lie at the interface of signal acquisition and machine learning, exploiting both data structure (e.g., sparsity in some domain) and data dimensionality (big data) already at the raw radio-frequency channel stage. As some examples, we outline efficient and effective deep learning solutions for adaptive beamforming and adaptive spectral Doppler through artificial agents, learn compressive encodings for the color Doppler, and provide a framework for structured signal recovery by learning fast approximations of iterative minimization problems, with applications to clutter suppression and super-resolution ultrasound. These emerging technologies may have a considerable impact on ultrasound imaging, showing promise across key components in the receive processing chain.","","","10.1109/JPROC.2019.2932116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8808885","Beamforming;compression;deep learning;deep unfolding;Doppler;image reconstruction;super resolution;ultrasound imaging","Imaging;Ultrasonic imaging;Doppler effect;Deep learning;Array signal processing;Image resolution;Clutter approximation","","","","1","124","IEEE","","","","IEEE","IEEE Journals"
"A Fast Inversion of Induction Logging Data in Anisotropic Formation Based on Deep Learning","G. Zhu; M. Gao; F. Kong; K. Li","School of Information Science and Engineering, Shandong University, Qingdao 266237, China.; School of Information Science and Engineering, Shandong University, Qingdao 266237, China.; School of Information Science and Engineering, Shandong University, Qingdao 266237, China.; School of Information Science and Engineering, Shandong University, Qingdao 266237, China (e-mail: kangli@sdu.edu.cn).","IEEE Geoscience and Remote Sensing Letters","","2020","PP","99","1","5","With the development of large gradient wells and horizontal wells, resistivity anisotropy has become an important concern. Most of the inversion problems encountered in geophysical applications are time-consuming. In this letter, we propose a novel approach based on deep learning (DL) to solve the time-consuming problem of the inversion of induction logging data in layered anisotropic formation. A deep neural network (DNN) architecture combined with an adaptive moment estimation (Adam) optimization algorithm is utilized. Batch normalization is used to accelerate the training convergence. Data sets are obtained from forward modeling in different anisotropic formations, by which the DNN is trained, validated, and tested. A series of experiments are carried out to test the accuracy, robustness, and speed of the inversion based on DL in the anisotropic formation. The experimental results indicate that the approach based on DL can achieve the inversion with high accuracy. Furthermore, the trained DNN model for the inversion is two orders of magnitude faster than the most commonly used nonlinear iterative algorithm that achieves the same inversion precision. The trained DNN model considerably reduces the inversion computation time without compromising accuracy. Moreover, the approach overcomes the defect of the dependence of the nonlinear iterative algorithm on the initial values. Our study has shown the promising potential of the approach based on DL for inverting the logging data in anisotropic formations that are difficult to quickly invert by the conventional method.","","","10.1109/LGRS.2019.2961374","National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951098","Anisotropic formation;deep learning (DL);deep neural networks (DNNs);induction logging data;inversion modeling.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Deep-Learning-Based Image Reconstruction and Enhancement in Optical Microscopy","K. de Haan; Y. Rivenson; Y. Wu; A. Ozcan","Electrical and Computer Engineering Department, University of California at Los Angeles, Los Angeles, CA, USA; Electrical and Computer Engineering Department, University of California at Los Angeles, Los Angeles, CA, USA; Electrical and Computer Engineering Department, University of California at Los Angeles, Los Angeles, CA, USA; Electrical and Computer Engineering Department, University of California at Los Angeles, Los Angeles, CA, USA","Proceedings of the IEEE","","2020","108","1","30","50","In recent years, deep learning has been shown to be one of the leading machine learning techniques for a wide variety of inference tasks. In addition to its mainstream applications, such as classification, it has created transformative opportunities for image reconstruction and enhancement in optical microscopy. Some of these emerging applications of deep learning range from image transformations between microscopic imaging systems to adding new capabilities to existing imaging techniques, as well as solving various inverse problems based on microscopy image data. Deep learning is helping us move toward data-driven instrument designs that blend microscopy and computing to achieve what neither can do alone. This article provides an overview of some of the recent work using deep neural networks to advance computational microscopy and sensing systems, also covering their current and future biomedical applications.","","","10.1109/JPROC.2019.2949575","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8901171","Biomedical imaging;deep learning","Deep learning;Biomedical imaging;Optical imaging;Microscopy;Image reconstruction;Machine learning","","","","","148","CCBY","","","","IEEE","IEEE Journals"
"Progressive Object Transfer Detection","H. Chen; Y. Wang; G. Wang; X. Bai; Y. Qiao","Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Shenzhen Key Laboratory of CVPR, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Shenzhen Key Laboratory of CVPR, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China","IEEE Transactions on Image Processing","","2020","29","","986","1000","Recent development of object detection mainly depends on deep learning with large-scale benchmarks. However, collecting such fully-annotated data is often difficult or expensive for real-world applications, which restricts the power of deep neural networks in practice. Alternatively, humans can detect new objects with little annotation burden, since humans often use the prior knowledge to identify new objects with few elaborately-annotated examples, and subsequently generalize this capacity by exploiting objects from wild images. Inspired by this procedure of learning to detect, we propose a novel Progressive Object Transfer Detection (POTD) framework. Specifically, we make three main contributions in this paper. First, POTD can leverage various object supervision of different domains effectively into a progressive detection procedure. Via such human-like learning, one can boost a target detection task with few annotations. Second, POTD consists of two delicate transfer stages, i.e., Low-Shot Transfer Detection (LSTD), and Weakly-Supervised Transfer Detection (WSTD). In LSTD, we distill the implicit object knowledge of source detector to enhance target detector with few annotations. It can effectively warm up WSTD later on. In WSTD, we design a recurrent object labelling mechanism for learning to annotate weakly-labeled images. More importantly, we exploit the reliable object supervision from LSTD, which can further enhance the robustness of target detector in the WSTD stage. Finally, we perform extensive experiments on a number of challenging detection benchmarks with different settings. The results demonstrate that, our POTD outperforms the recent state-of-the-art approaches. The codes and models are available at https://github.com/Cassie94/LSTD/tree/lstd.","","","10.1109/TIP.2019.2938680","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Shenzhen Basic Research Program; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8826606","Object detection;deep learning;transfer learning;weakly/semi-supervised detection;low-shot learning","Detectors;Object detection;Proposals;Task analysis;Benchmark testing;Deep learning;Labeling","image classification;learning (artificial intelligence);neural nets;object detection","POTD;WSTD;recurrent object labelling mechanism;reliable object supervision;deep learning;deep neural networks;progressive detection procedure;progressive object transfer detection framework;low-shot transfer detection;weakly-supervised transfer detection","","","28","","","","","IEEE","IEEE Journals"
"Cost-Effective Vehicle Type Recognition in Surveillance Images With Deep Active Learning and Web Data","Y. Huang; Z. Liu; M. Jiang; X. Yu; X. Ding","Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Fujian, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Fujian, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Fujian, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Fujian, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Fujian, China","IEEE Transactions on Intelligent Transportation Systems","","2020","21","1","79","86","Recently, vehicle type recognition in surveillance images with deep learning has received significant attention in various applications of intelligent transportation systems. However, annotating large-scale images from many surveillance images is tedious and time-consuming, which impedes its application in the real world. This paper aims to resolve this problem by reducing manual labeling in surveillance images, and then maximizing the effect of the few tagged data. Thus, a deep active learning method with a new query strategy is proposed in this paper for vehicle type recognition in surveillance images. First, the proposed method constructs a memory space using a large-scale fully labeled auxiliary dataset collected from the Internet. Subsequently, two metrics, the similarity measurement in memory space and the entropy, are used to simultaneously emphasize the diversity and uncertainty in the query strategy. Moreover, an additional label-consistent term apart from the hyper-parameters is used to adaptively adjust the combination of the two principles in active learning. The proposed method was evaluated on the Comprehensive Cars dataset. The experimental results demonstrated that the proposed method could effectively reduce the annotation cost by up to 40% in surveillance vehicle type recognition compared with the random selection method.","","","10.1109/TITS.2018.2888698","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Natural Science Foundation of Fujian Province; CCF-Tencent Open Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606452","Active learning;deep learning;surveillance images;vehicle type recognition","Surveillance;Image recognition;Entropy;Training;Feature extraction;Deep learning;Uncertainty","","","","","21","IEEE","","","","IEEE","IEEE Journals"
"Wireless Capsule Endoscopy: A New Tool for Cancer Screening in the Colon With Deep-Learning-Based Polyp Recognition","X. Jia; X. Xing; Y. Yuan; L. Xing; M. Q. -. Meng","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Electrical Engineering, City University of Hong Kong, Hong Kong; Department of Radiation Oncology, Stanford University, Palo Alto, CA, USA; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong","Proceedings of the IEEE","","2020","108","1","178","197","Accurate recognition of polyps is crucial for early colorectal cancer diagnosis and treatment. Wireless capsule endoscopy (WCE) is a noninvasive, wireless imaging tool that allows direct visualization of the entire colon without discomfort to patients and has the potential to revolutionize the screening workup for colorectal diseases. However, current manual review is laborious and time consuming, requiring the undivided concentration of the gastroenterologist. Computational methods that can assist automated polyp recognition will enhance the outcome both in terms of diagnostic accuracy and efficiency of WCE. This review introduces the computer-assisted algorithms as applied to colorectal polyp screening, focusing on the successes of deep-learning-based strategies in the WCE sequences. We survey key applications of WCE polyp recognition, covering deep-learning-based image-level classification, lesion region detection, and pixel-accurate segmentation. We conclude by discussing emerging research challenges, possible trends, and future directions.","","","10.1109/JPROC.2019.2950506","Research Grants Council, University Grants Committee; Shenzhen Science and Technology Innovation Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8903282","Cancer screening;deep learning;polyp recognition;wireless capsule endoscopy (WCE)","Cancer;Endoscopes;Deep learning;Machine learning;Medical treatment;Biomedical imaging","","","","","149","IEEE","","","","IEEE","IEEE Journals"
"Studying the Manifold Structure of Alzheimer's Disease: A Deep Learning Approach Using Convolutional Autoencoders","F. J. Martinez-Murcia; A. Ortiz; J. Gorriz; J. Ramirez; D. Castillo-Barnes","Department of Communications Engineering, University of Malaga, Malaga, Spain; Department of Communications Engineering, University of Malaga, Malaga, Spain; Department of Signal Theory, Networking and Communications, University of Granada, Granada, Spain; Department of Signal Theory, Networking and Communications, University of Granada, Granada, Spain; Department of Signal Theory, Networking and Communications, University of Granada, Granada, Spain","IEEE Journal of Biomedical and Health Informatics","","2020","24","1","17","26","Many classical machine learning techniques have been used to explore Alzheimer's disease (AD), evolving from image decomposition techniques such as principal component analysis toward higher complexity, non-linear decomposition algorithms. With the arrival of the deep learning paradigm, it has become possible to extract high-level abstract features directly from MRI images that internally describe the distribution of data in low-dimensional manifolds. In this work, we try a new exploratory data analysis of AD based on deep convolutional autoencoders. We aim at finding links between cognitive symptoms and the underlying neurodegeneration process by fusing the information of neuropsychological test outcomes, diagnoses, and other clinical data with the imaging features extracted solely via a data-driven decomposition of MRI. The distribution of the extracted features in different combinations is then analyzed and visualized using regression and classification analysis, and the influence of each coordinate of the autoencoder manifold over the brain is estimated. The imaging-derived markers could then predict clinical variables with correlations above 0.6 in the case of neuropsychological evaluation variables such as the MMSE or the ADAS11 scores, achieving a classification accuracy over 80% for the diagnosis of AD.","","","10.1109/JBHI.2019.2914970","MINECO/FEDER; Ministerio de Ciencia e Innovación; Alzheimer's Disease Neuroimaging Initiative; National Institutes of Health; U.S. Department of Defense; National Institute on Aging; National Institute of Biomedical Imaging and Bioengineering; AbbVie; Alzheimer's Association; Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica; Biogen; Bristol-Myers Squibb; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; Roche; Genentech; Fujirebio Europe; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical Research & Development LLC.; Lumosity; H. Lundbeck A/S; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer; Piramal Imaging; Servier; Takeda Pharmaceutical Company; Transition Therapeutics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8737996","Alzheimer's disease;deep learning;convolutional autoencoder;manifold learning;data fusion","Feature extraction;Magnetic resonance imaging;Manifolds;Alzheimer's disease;Computer architecture;Deep learning","","","","","56","CCBY","","","","IEEE","IEEE Journals"
"Cross-View Gait Recognition by Discriminative Feature Learning","Y. Zhang; Y. Huang; S. Yu; L. Wang","National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, University of Chinese Academy of Sciences, Beijing, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2020","29","","1001","1015","Recently, deep learning-based cross-view gait recognition has become popular owing to the strong capacity of convolutional neural networks (CNNs). Current deep learning methods often rely on loss functions used widely in the task of face recognition, e.g., contrastive loss and triplet loss. These loss functions have the problem of hard negative mining. In this paper, a robust, effective, and gait-related loss function, called angle center loss (ACL), is proposed to learn discriminative gait features. The proposed loss function is robust to different local parts and temporal window sizes. Different from center loss which learns a center for each identity, the proposed loss function learns multiple sub-centers for each angle of the same identity. Only the largest distance between the anchor feature and the corresponding cross-view sub-centers is penalized, which achieves better intra-subject compactness. We also propose to extract discriminative spatial-temporal features by local feature extractors and a temporal attention model. A simplified spatial transformer network is proposed to localize the suitable horizontal parts of the human body. Local gait features for each horizontal part are extracted and then concatenated as the descriptor. We introduce long short-term memory (LSTM) units as the temporal attention model to learn the attention score for each frame, e.g., focusing more on discriminative frames and less on frames with bad quality. The temporal attention model shows better performance than the temporal average pooling or gait energy images (GEI). By combing the three aspects, we achieve state-of-the-art results on several cross-view gait recognition benchmarks.","","","10.1109/TIP.2019.2926208","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759096","Gait recognition;discriminative feature learning;angle center loss;spatial-temporal features","Gait recognition;Feature extraction;Three-dimensional displays;Generative adversarial networks;Face recognition;Deep learning;Clothing","convolutional neural nets;face recognition;feature extraction;gait analysis;learning (artificial intelligence);recurrent neural nets","local feature extractors;temporal attention model;local gait features;cross-view gait recognition;discriminative feature learning;deep learning;loss function;discriminative spatial-temporal feature extraction;angle center loss;discriminative gait feature learning;LSTM;long short-term memory;convolutional neural networks;CNN;face recognition","","","70","","","","","IEEE","IEEE Journals"
"Deep Adversarial Metric Learning","Y. Duan; J. Lu; W. Zheng; J. Zhou","Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), the State Key Lab of Intelligent Technologies and Systems, Tsinghua University, Beijing, China; Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), the State Key Lab of Intelligent Technologies and Systems, Tsinghua University, Beijing, China; Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), the State Key Lab of Intelligent Technologies and Systems, Tsinghua University, Beijing, China; Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), the State Key Lab of Intelligent Technologies and Systems, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","","2020","29","1","2037","2051","Learning an effective distance measurement between sample pairs plays an important role in visual analysis, where the training procedure largely relies on hard negative samples. However, hard negative samples usually account for the tiny minority in the training set, which may fail to fully describe the data distribution close to the decision boundary. In this paper, we present a deep adversarial metric learning (DAML) framework to generate synthetic hard negatives from the original negative samples, which is widely applicable to existing supervised deep metric learning algorithms. Different from existing sampling strategies which simply ignore numerous easy negatives, our DAML aim to exploit them by generating synthetic hard negatives adversarial to the learned metric as complements. We simultaneously train the feature embedding and hard negative generator in an adversarial manner, so that adequate and targeted synthetic hard negatives are created to learn more precise distance metrics. As a single transformation may not be powerful enough to describe the global input space under the attack of the hard negative generator, we further propose a deep adversarial multi-metric learning (DAMML) method by learning multiple local transformations for more complete description. We simultaneously exploit the collaborative and competitive relationships among multiple metrics, where the metrics display unity against the generator for effective distance measurement as well as compete for more training data through a metric discriminator to avoid overlapping. Extensive experimental results on five benchmark datasets show that our DAML and DAMML effectively boost the performance of existing deep metric learning approaches through adversarial learning.","","","10.1109/TIP.2019.2948472","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8883191","Metric learning;deep learning;adversarial learning;hard negative generation;multi-metric","Measurement;Training;Microstrip;Generators;Learning systems;Visualization;Task analysis","","","","","75","IEEE","","","","IEEE","IEEE Journals"
"Deep-Reinforcement-Learning-Based Autonomous Voltage Control for Power Grid Operations","J. Duan; D. Shi; R. Diao; H. Li; Z. Wang; B. Zhang; D. Bian; Z. Yi","GEIRI North America, San Jose, CA, USA; GEIRI North America, San Jose, CA, USA; GEIRI North America, San Jose, CA, USA; State Grid Jiangsu Electric Power Company, Nanjing, China; GEIRI North America, San Jose, CA, USA; GEIRI North America, San Jose, CA, USA; GEIRI North America, San Jose, CA, USA; GEIRI North America, San Jose, CA, USA","IEEE Transactions on Power Systems","","2020","35","1","814","817","In this letter, a novel autonomous control framework “Grid Mind” is proposed for the secure operation of power grids based on cutting-edge artificial intelligence (AI) technologies. The proposed platform provides a data-driven, model-free and closed-loop control agent trained using deep reinforcement learning (DRL) algorithms by interacting with massive simulations and/or real environment of a power grid. The proposed agent learns from scratch to master the power grid voltage control problem purely from data. It can make autonomous voltage control (AVC) strategies to support grid operators in making effective and timely control actions, according to the current system conditions detected by real-time measurements from supervisory control and data acquisition (SCADA) or phasor measurement units (PMUs). Two state-of-the-art DRL algorithms, namely deep Q-network (DQN) and deep deterministic policy gradient (DDPG), are proposed to formulate the AVC problem with performance compared. Case studies on a realistic 200-bus test system demonstrate the effectiveness and promising performance of the proposed framework.","","","10.1109/TPWRS.2019.2941134","SGCC Science and Technology Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8834806","AI;autonomous voltage control;deep reinforcement learning;DQN;DDPG;Grid Mind;PMU","Automatic voltage control;Power grids;Generators;Training;Artificial intelligence","","","","","10","IEEE","","","","IEEE","IEEE Journals"
"Fuzzified Image Enhancement for Deep Learning in Iris Recognition","M. Liu; Z. Zhou; P. Shang; D. Xu","Department of Applied Mathematics, School of Mathematics and Statistics Sciences, Changchun University of Technology, Changchun, China; Christopher S. Bond Life Sciences Center and the Department of Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA; Christopher S. Bond Life Sciences Center and the Department of Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA; Christopher S. Bond Life Sciences Center and the Department of Electrical Engineering and Computer Science, University of Missouri, Columbia, MO, USA","IEEE Transactions on Fuzzy Systems","","2020","28","1","92","99","Deep learning techniques such as convolutional neural network and capsule network have attained good results in iris recognition. However, due to the influence of eyelashes, skin, and background noises, the model often needs many iterations to retrieve informative iris patterns. Also because of some nonideal situations, such as reflection of glasses and facula on the eyeball, it is hard to detect the boundary of pupil and iris perfectly. Under such a circumstance, discarding the rest parts beyond the boundary may cause losing useful information. Hence, we use Gaussian, triangular fuzzy average, and triangular fuzzy median smoothing filters to preprocess the image by fuzzifying the region beyond the boundary to improve the signal-to-noise ratios. We applied the enhanced images through fuzzy operations to train deep learning methods, which speeds up the process of convergence and also increases the recognition accuracy rate. The saliency maps show that fuzzified image filters make the images more informative for deep learning. The proposed fuzzy operation of images may be a robust technique in many other deep-learning applications of image processing, analysis, and prediction.","","","10.1109/TFUZZ.2019.2912576","National Natural Science Foundation of China; National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695141","Capsule Network;convolutional neural network (CNN);deep learning;feature extraction;fuzzy operation;iris recognition;saliency map","Iris recognition;Iris;Deep learning;Image enhancement;Feature extraction;Transforms;Standards","","","","","28","IEEE","","","","IEEE","IEEE Journals"
"Pixel-Level Cracking Detection on 3D Asphalt Pavement Images Through Deep-Learning- Based CrackNet-V","Y. Fei; K. C. P. Wang; A. Zhang; C. Chen; J. Q. Li; Y. Liu; G. Yang; B. Li","School of Civil and Environmental Engineering, Oklahoma State University–Stillwater, Stillwater, OK, USA; School of Civil and Environmental Engineering, Oklahoma State University–Stillwater, Stillwater, OK, USA; School of Civil and Environmental Engineering, Oklahoma State University, Stillwater, OK, USA; School of Civil and Environmental Engineering, Oklahoma State University–Stillwater, Stillwater, OK, USA; School of Civil and Environmental Engineering, Oklahoma State University–Stillwater, Stillwater, OK, USA; School of Civil and Environmental Engineering, Oklahoma State University–Stillwater, Stillwater, OK, USA; School of Civil and Environmental Engineering, Oklahoma State University–Stillwater, Stillwater, OK, USA; School of Civil and Environmental Engineering, Southwest Jiaotong University, Chengdu, China","IEEE Transactions on Intelligent Transportation Systems","","2020","21","1","273","284","A few recent developments have demonstrated that deep-learning-based solutions can outperform traditional algorithms for automated pavement crack detection. In this paper, an efficient deep network called CrackNet-V is proposed for automated pixel-level crack detection on 3D asphalt pavement images. Compared with the original CrackNet, CrackNet-V has a deeper architecture but fewer parameters, resulting in improved accuracy and computation efficiency. Inspired by CrackNet, CrackNet-V uses invariant spatial size through all layers such that supervised learning can be conducted at pixel level. Following the VGG network, CrackNet-V uses  $3\times 3$  size of filters for the first six convolutional layers and stacks several  $3\times 3$  convolutional layers together for deep abstraction, resulting in reduced number of parameters and efficient feature extraction. CrackNet-V has 64113 parameters and consists of ten layers, including one pre-process layer, eight convolutional layers, and one output layer. A new activation function leaky rectified tanh is proposed in this paper for higher accuracy in detecting shallow cracks. The training of CrackNet-V was completed after 3000 iterations, which took only one day on a GeForce GTX 1080Ti device. According to the experimental results on 500 testing images, CrackNet-V achieves a high performance with a Precision of 84.31%, Recall of 90.12%, and an F-1 score of 87.12%. It is shown that CrackNet-V yields better overall performance particularly in detecting fine cracks compared with CrackNet. The efficiency of CrackNet-V further reveals the advantages of deep learning techniques for automated pixel-level pavement crack detection.","","","10.1109/TITS.2019.2891167","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620557","CrackNet;CrackNet-V;deep learning;surface cracks","Three-dimensional displays;Surface cracks;Asphalt;Libraries;Feature extraction;Deep learning;Kernel","","","","3","58","IEEE","","","","IEEE","IEEE Journals"
"Intelligent Resource Allocation for Train-to-Train Communication: A Multi-Agent Deep Reinforcement Learning Approach","J. Zhao; Y. Zhang; Y. Nie; J. Liu","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing 100044, China and School of Information Engineering, East China Jiaotong University, Nanchang 330013, China. (e-mail: junhuizhao@hotmail.com); School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing 100044, China.; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing 100044, China.; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing 100044, China.","IEEE Access","","2020","PP","99","1","1","The application of train-to-train (T2T) communication in urban rail transit is expected to simplify system structure, reduce maintenance costs, and improve operational efficiency. In particular, train-to-wayside (T2W) communication coexist with T2T communication in the train control system based on T2T communication. To make full use of limited spectrum resources, frequency reuse is adopted as an efficient technique, but it brings the co-channel interference unfortunately, which affects the quality of service (QoS) for T2T and T2W users. In this paper, we propose a multi-agent deep reinforcement learning (MADRL) based autonomous channel selection and transmission power selection algorithm for T2T communication to reduce the co-channel interference. Specifically, each agent interacts with the environment and selects actions to implement a distributed resource allocation mechanism independently, adopting asynchronous updates to avoid different agents choosing the same sub-band. Simulation results show the superiority of our proposed algorithm: compared with the existing resource allocation schemes for T2T communication, the system throughput and the successful transmission probability of T2T links are greatly improved.","","","10.1109/ACCESS.2019.2963751","National Natural Science Foundation of China; Jiangxi Provincial Cultivation Program for Academic and Technical Leaders of Major Subjects; Open Research Fund of National Mobile Communications Research Laboratory Southeast University; National Science and Technology Major Project of the Ministry of Science and Technology of China; Key Technology Research and Development Program of Jiangxi Province; Beijing Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949541","Train-to-train (T2T) communication;resource allocation;multi-agent deep reinforcement learning (MADRL);urban rail transit","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Exploiting Related and Unrelated Tasks for Hierarchical Metric Learning and Image Classification","Y. Zheng; J. Fan; J. Zhang; X. Gao","School of Cyber Engineering, Xidian University, Xi’an, China; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Image Processing","","2020","29","","883","896","In multi-task learning, multiple interrelated tasks are jointly learned to achieve better performance. In many cases, if we can identify which tasks are related, we can also clearly identify which tasks are unrelated. In the past, most researchers emphasized exploiting correlations among interrelated tasks while completely ignoring the unrelated tasks that may provide valuable prior knowledge for multi-task learning. In this paper, a new approach is developed to hierarchically learn a tree of multi-task metrics by leveraging prior knowledge about both the related tasks and unrelated tasks. First, a visual tree is constructed to hierarchically organize large numbers of image categories in a coarse-to-fine fashion. Over the visual tree, a multi-task metric classifier is learned for each node by exploiting both the related and unrelated tasks, where the learning tasks for training the classifiers for the sibling child nodes under the same parent node are treated as the interrelated tasks, and the others are treated as the unrelated tasks. In addition, the node-specific metric for the parent node is propagated to its sibling child nodes to control inter-level error propagation. Our experimental results demonstrate that our hierarchical metric learning algorithm achieves better results than other state-of-the-art algorithms.","","","10.1109/TIP.2019.2938321","National Natural Science Foundation of China; National Key Research and Development Program of China; National High-Level Talents Special Support Program of China; China Postdoctoral Science Foundation; Northwestern Polytechnical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825856","Hierarchical metric learning;multi-task learning;related and unrelated tasks;visual tree","Task analysis;Visualization;Measurement;Training;Correlation;Semantics;Deep learning","image classification;learning (artificial intelligence)","multitask learning;multitask metrics;multitask metric classifier;hierarchical metric learning algorithm;image classification;image categories","","","62","","","","","IEEE","IEEE Journals"
"DeepDrawing: A Deep Learning Approach to Graph Drawing","Y. Wang; Z. Jin; Q. Wang; W. Cui; T. Ma; H. Qu","Hong Kong University of Science and Technology (HKUST); Hong Kong University of Science and Technology (HKUST); Hong Kong University of Science and Technology (HKUST); Microsoft Research Asia; IBM T. J. Watson Research Center; Hong Kong University of Science and Technology (HKUST)","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","676","686","Node-link diagrams are widely used to facilitate network explorations. However, when using a graph drawing technique to visualize networks, users often need to tune different algorithm-specific parameters iteratively by comparing the corresponding drawing results in order to achieve a desired visual effect. This trial and error process is often tedious and time-consuming, especially for non-expert users. Inspired by the powerful data modelling and prediction capabilities of deep learning techniques, we explore the possibility of applying deep learning techniques to graph drawing. Specifically, we propose using a graph-LSTM-based approach to directly map network structures to graph drawings. Given a set of layout examples as the training dataset, we train the proposed graph-LSTM-based model to capture their layout characteristics. Then, the trained model is used to generate graph drawings in a similar style for new networks. We evaluated the proposed approach on two special types of layouts (i.e., grid layouts and star layouts) and two general types of layouts (i.e., ForceAtlas2 and PivotMDS) in both qualitative and quantitative ways. The results provide support for the effectiveness of our approach. We also conducted a time cost assessment on the drawings of small graphs with 20 to 50 nodes. We further report the lessons we learned and discuss the limitations and future work.","","","10.1109/TVCG.2019.2934798","MSRA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807275","Graph Drawing;Deep Learning;LSTM;Procrustes Analysis","Deep learning;Layout;Neural networks;Training;Convolution;Data models;Visualization","","","","","82","","","","","IEEE","IEEE Journals"
"Deep Unbiased Embedding Transfer for Zero-Shot Learning","Z. Jia; Z. Zhang; L. Wang; C. Shan; T. Tan","National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Artificial Intelligence Research, CAS (CAS-AIR), Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2020","29","","1958","1971","Zero-shot learning aims to recognize objects which do not appear in the training dataset. Previous prevalent mapping-based zero-shot learning methods suffer from the projection domain shift problem due to the lack of image classes in the training stage. In order to alleviate the projection domain shift problem, a deep unbiased embedding transfer (DUET) model is proposed in this paper. The DUET model is composed of a deep embedding transfer (DET) module and an unseen visual feature generation (UVG) module. In the DET module, a novel combined embedding transfer net which integrates the complementary merits of the linear and nonlinear embedding mapping functions is proposed to connect the visual space and semantic space. What's more, the end-to-end joint training process is implemented to train the visual feature extractor and the combined embedding transfer net simultaneously. In the UVG module, a visual feature generator trained with a conditional generative adversarial framework is used to synthesize the visual features of the unseen classes to ease the disturbance of the projection domain shift problem. Furthermore, a quantitative index, namely the score of resistance on domain shift (ScoreRDS), is proposed to evaluate different models regarding their resistance capability on the projection domain shift problem. The experiments on five zero-shot learning benchmarks verify the effectiveness of the proposed DUET model. As demonstrated by the qualitative and quantitative analysis, the unseen class visual feature generation, the combined embedding transfer net and the end-to-end joint training process all contribute to alleviating projection domain shift in zero-shot learning.","","","10.1109/TIP.2019.2947780","National Key R&D Program of China; National Natural Science Foundation of China; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8879688","Zero-shot learning;image classification;projection domain shift;convolutional neural network;generative adversarial network","Visualization;Feature extraction;Semantics;Training;Seals;Prototypes;Indexes","convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);object recognition","projection domain shift problem;deep unbiased embedding transfer model;DUET model;deep embedding transfer module;unseen visual feature generation module;DET module;nonlinear embedding mapping functions;end-to-end joint training process;visual feature extractor;visual feature generator;zero-shot learning;unseen class visual feature generation;embedding transfer net;score of resistance on domain shift;ScoreRDS;image classification","","","39","IEEE","","","","IEEE","IEEE Journals"
"Efficient Evaluation of Image Quality via Deep-Learning Approximation of Perceptual Metrics","A. Artusi; F. Banterle; F. Carra; A. Moreno","MRG DeepCamera Group, RISE Ltd., Nicosia, Cyprus; ISTI-CNR, Pisa, Italy; ISTI-CNR, Pisa, Italy; ISTI-CNR, Pisa, Italy","IEEE Transactions on Image Processing","","2020","29","","1843","1855","Image metrics based on Human Visual System (HVS) play a remarkable role in the evaluation of complex image processing algorithms. However, mimicking the HVS is known to be complex and computationally expensive (both in terms of time and memory), and its usage is thus limited to a few applications and to small input data. All of this makes such metrics not fully attractive in real-world scenarios. To address these issues, we propose Deep Image Quality Metric (DIQM), a deep-learning approach to learn the global image quality feature (mean-opinion-score). DIQM can emulate existing visual metrics efficiently, reducing the computational costs by more than an order of magnitude with respect to existing implementations.","","","10.1109/TIP.2019.2944079","European Union’s Horizon 2020 Research and Innovation Programme; Government of the Republic of Cyprus through the Directorate General for European Programmes, Coordination and Development; European Union’s Horizon 2020 Research and Innovation Programme; “Automatic Data and Documents Analysis to Enhance Human-Based Processes” (ADA), CUP CIPE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8861304","Convolutional neural networks (CNNs);objective metrics;image evaluation;human visual system;JPEG-XT;and HDR imaging","Measurement;Image quality;Visualization;Distortion;Indexes;Feature extraction","approximation theory;computer vision;feature extraction;image enhancement;learning (artificial intelligence);neural nets","deep-learning approximation;perceptual metrics;human visual system;HVS;image processing algorithms;Deep Image Quality Metric;DIQM;deep-learning approach;visual metrics;image quality feature","","","61","IEEE","","","","IEEE","IEEE Journals"
"Dynamic-DeepHit: A Deep Learning Approach for Dynamic Survival Analysis With Competing Risks Based on Longitudinal Data","C. Lee; J. Yoon; M. v. d. Schaar","Department of Electrical and Computer Engineering, University of California, Los Angeles, Los Angeles, CA, USA; Department of Electrical and Computer Engineering, University of California, Los Angeles; Department of EngineeringUniversity of Cambridge","IEEE Transactions on Biomedical Engineering","","2020","67","1","122","133","Currently available risk prediction methods are limited in their ability to deal with complex, heterogeneous, and longitudinal data such as that available in primary care records, or in their ability to deal with multiple competing risks. This paper develops a novel deep learning approach that is able to successfully address current limitations of standard statistical approaches such as landmarking and joint modeling. Our approach, which we call Dynamic-DeepHit, flexibly incorporates the available longitudinal data comprising various repeated measurements (rather than only the last available measurements) in order to issue dynamically updated survival predictions for one or multiple competing risk(s). Dynamic-DeepHit learns the time-to-event distributions without the need to make any assumptions about the underlying stochastic models for the longitudinal and the time-to-event processes. Thus, unlike existing works in statistics, our method is able to learn data-driven associations between the longitudinal data and the various associated risks without underlying model specifications. We demonstrate the power of our approach by applying it to a real-world longitudinal dataset from the U.K. Cystic Fibrosis Registry, which includes a heterogeneous cohort of 5883 adult patients with annual follow-ups between 2009 to 2015. The results show that Dynamic-DeepHit provides a drastic improvement in discriminating individual risks of different forms of failures due to cystic fibrosis. Furthermore, our analysis utilizes post-processing statistics that provide clinical insight by measuring the influence of each covariate on risk predictions and the temporal importance of longitudinal measurements, thereby enabling us to identify covariates that are influential for different competing risks.","","","10.1109/TBME.2019.2909027","National Science Foundation; Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8681104","Dynamic survival analysis;competing risks;longitudinal measurements;time-to-event data;deep learning;cystic fibrosis","Time measurement;Diseases;Biomarkers;Biological system modeling;Deep learning;Computational modeling","","","","","60","IEEE","","","","IEEE","IEEE Journals"
"Deepzzle: Solving Visual Jigsaw Puzzles with Deep Learning and Shortest Path Optimization","M. Paumard; D. Picard; H. Tabia","Information Processing and System Teams (ETIS), UMR 8051, Université Paris Seine, Université Cergy-Pontoise, ENSEA, CNRS, Cergy, 95000, France.; Information Processing and System Teams (ETIS), UMR 8051, Université Paris Seine, Université Cergy-Pontoise, ENSEA, CNRS, Cergy, 95000, France.; Information Processing and System Teams (ETIS), UMR 8051, Université Paris Seine, Université Cergy-Pontoise, ENSEA, CNRS, Cergy, 95000, France.","IEEE Transactions on Image Processing","","2020","PP","99","1","1","We tackle the image reassembly problem with wide space between the fragments, in such a way that the patterns and colors continuity is mostly unusable. The spacing emulates the erosion of which the archaeological fragments suffer. We crop-square the fragments borders to compel our algorithm to learn from the content of the fragments. We also complicate the image reassembly by removing fragments and adding pieces from other sources. We use a two-step method to obtain the reassemblies: 1) a neural network predicts the positions of the fragments despite the gaps between them; 2) a graph that leads to the best reassemblies is made from these predictions. In this paper, we notably investigate the effect of branch-cut in the graph of reassemblies. We also provide a comparison with the literature, solve complex images reassemblies, explore at length the dataset, and propose a new metric that suits its specificities.","","","10.1109/TIP.2019.2963378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951449","Image reassembly;Jigsaw puzzle;deep learning;graph;branch-cut;cultural heritage","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Machine Learning in PET: From Photon Detection to Quantitative Image Reconstruction","K. Gong; E. Berg; S. R. Cherry; J. Qi","Department of Biomedical Engineering, University of California at Davis, Davis, CA, USA; Department of Biomedical Engineering, University of California at Davis, Davis, CA, USA; Department of Biomedical Engineering and Department of Radiology, University of California at Davis, Davis, CA, USA; Department of Biomedical Engineering, University of California at Davis, Davis, CA, USA","Proceedings of the IEEE","","2020","108","1","51","68","Machine learning has found unique applications in nuclear medicine from photon detection to quantitative image reconstruction. Although there have been impressive strides in detector development for time-of-flight positron emission tomography (PET), most detectors still make use of simple signal processing methods to extract the time and position information from the detector signals. Now, with the availability of fast waveform digitizers, machine learning techniques have been applied to estimate the position and arrival time of high-energy photons. In quantitative image reconstruction, machine learning has been used to estimate various corrections factors, including scattered events and attenuation images, as well as to reduce statistical noise in reconstructed images. Here, machine learning either provides a faster alternative to an existing time-consuming computation, such as in the case of scatter estimation, or creates a data-driven approach to map an implicitly defined function, such as in the case of estimating the attenuation map for PET/MR scans. In this article, we will review the above-mentioned applications of machine learning in nuclear medicine.","","","10.1109/JPROC.2019.2936809","National Institute of Biomedical Imaging and Bioengineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844693","Attenuation correction;deep learning;denoising;image reconstruction;machine learning;positron emission tomography (PET);scatter correction;timing resolution","Photonics;Detectors;Machine learning;Image reconstruction;Deep learning;Positrons;Positron emission tomography","","","","","167","IEEE","","","","IEEE","IEEE Journals"
"Attention-Aware Multi-Task Convolutional Neural Networks","K. Lyu; Y. Li; Z. Zhang","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Image Processing","","2020","29","","1867","1878","Multi-task deep learning methods learn multiple tasks simultaneously and share representations amongst them, so information from related tasks improves learning within one task. The generalization capabilities of the produced models are substantially enhanced. Typical multi-task deep learning models usually share representations of different tasks in lower layers of the network, and separate representations of different tasks in higher layers. However, different groups of tasks always have different requirements for sharing representations, so the required design criterion does not necessarily guarantee that the obtained network architecture is optimal. In addition, most existing methods ignore the redundancy problem and lack the pre-screening process for representations before they are shared. Here, we propose a model called Attention-aware Multi-task Convolutional Neural Network, which automatically learns appropriate sharing through end-to-end training. The attention mechanism is introduced into our architecture to suppress redundant contents contained in the representations. The shortcut connection is adopted to preserve useful information. We evaluate our model by carrying out experiments on different task groups and different datasets. Our model demonstrates an improvement over existing techniques in many experiments, indicating the effectiveness and the robustness of the model. We also demonstrate the importance of attention mechanism and shortcut connection in our model.","","","10.1109/TIP.2019.2944522","National Natural Science Foundation of China; Key R&D Program of Zhejiang Province; Zhejiang Laboratory; Fundamental Research Funds for the Central Universities; Zhejiang University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859643","Attention mechanism;convolutional neural network;multi-task learning;representation sharing","Task analysis;Deep learning;Feature extraction;Training;Estimation;Semantics;Convolutional neural networks","convolutional neural nets;image representation;learning (artificial intelligence)","attention mechanism;attention-aware multitask convolutional neural networks;multitask deep learning methods;multitask deep learning models;shortcut connection;representations sharing","","","43","IEEE","","","","IEEE","IEEE Journals"
"Graph Sequence Recurrent Neural Network for Vision-Based Freezing of Gait Detection","K. Hu; Z. Wang; W. Wang; K. A. Ehgoetz Martens; L. Wang; T. Tan; S. J. G. Lewis; D. D. Feng","School of Computer Science, The University of Sydney, Sydney, NSW, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; National Laboratory of Pattern Recognition (NLPR), Center for Research on Intelligent Perception and Computing (CRIPAC), Institute of Automation Chinese Academy of Sciences (CASIA), University of Chinese Academy of Sciences (UCAS), Beijing, China; Parkinson’s Disease Research Clinic, Brain and Mind Centre, The University of Sydney, Sydney, NSW, Australia; National Laboratory of Pattern Recognition (NLPR), Center for Research on Intelligent Perception and Computing (CRIPAC), Institute of Automation Chinese Academy of Sciences (CASIA), University of Chinese Academy of Sciences (UCAS), Beijing, China; National Laboratory of Pattern Recognition (NLPR), Center for Research on Intelligent Perception and Computing (CRIPAC), Institute of Automation Chinese Academy of Sciences (CASIA), University of Chinese Academy of Sciences (UCAS), Beijing, China; Parkinson’s Disease Research Clinic, Brain and Mind Centre, The University of Sydney, Sydney, NSW, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Image Processing","","2020","29","","1890","1901","Freezing of gait (FoG) is one of the most common symptoms of Parkinson's disease (PD), a neurodegenerative disorder which impacts millions of people around the world. Accurate assessment of FoG is critical for the management of PD and to evaluate the efficacy of treatments. Currently, the assessment of FoG requires well-trained experts to perform time-consuming annotations via vision-based observations. Thus, automatic FoG detection algorithms are needed. In this study, we formulate vision-based FoG detection, as a fine-grained graph sequence modelling task, by representing the anatomic joints in each temporal segment with a directed graph, since FoG events can be observed through the motion patterns of joints. A novel deep learning method is proposed, namely graph sequence recurrent neural network (GS-RNN), to characterize the FoG patterns by devising graph recurrent cells, which take graph sequences of dynamic structures as inputs. For the cases of which prior edge annotations are not available, a data-driven based adjacency estimation method is further proposed. To the best of our knowledge, this is one of the first studies on vision-based FoG detection using deep neural networks designed for graph sequences of dynamic structures. Experimental results on more than 150 videos collected from 45 patients demonstrated promising performance of the proposed GS-RNN for FoG detection with an AUC value of 0.90.","","","10.1109/TIP.2019.2946469","Australian Research Council; NHMRC-ARC Dementia Fellowship; National Health and Medical Research Council; Dementia Research Team; NeuroSleepCentre of Research Excellence; ARC Centre of Excellence in Cognition and Its Disorders Memory Program; University of Sydney; National Natural Science Foundation of China; Parkinson Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870255","Parkinson’s disease;freezing of gait detection;deep learning;recurrent neural network;graph sequence","Videos;Deep learning;Recurrent neural networks;Task analysis;Feature extraction;Legged locomotion","computer vision;directed graphs;diseases;gait analysis;learning (artificial intelligence);medical computing;medical disorders;medical image processing;recurrent neural nets","vision-based freezing;gait detection;PD;vision-based FoG detection;fine-grained graph sequence modelling task;directed graph;FoG patterns;graph recurrent cells;data-driven based adjacency estimation method;deep neural networks;graph sequence recurrent neural network;Parkinson's disease;freezing of gait;deep learning","","","54","IEEE","","","","IEEE","IEEE Journals"
"A Survey of Deep Learning Applications to Autonomous Vehicle Control","S. Kuutti; R. Bowden; Y. Jin; P. Barber; S. Fallah","Centre for Automotive Engineering, University of Surrey, Guildford GU2 7XH, U.K. (e-mail: s.j.kuutti@surrey.ac.uk).; Centre for Vision Speech and Signal Processing, University of Surrey, Guildford GU2 7XH, U.K..; Department of Computer Science, University of Surrey, Guildford GU2 7XH, U.K..; Jaguar Land Rover Ltd., Coventry CV4 7JJ, U.K. He is now with Phil Barber Industries, Yorkshire DL7 9UN, U.K..; Centre for Automotive Engineering, University of Surrey, Guildford GU2 7XH, U.K..","IEEE Transactions on Intelligent Transportation Systems","","2020","PP","99","1","22","Designing a controller for autonomous vehicles capable of providing adequate performance in all driving scenarios is challenging due to the highly complex environment and inability to test the system in the wide variety of scenarios which it may encounter after deployment. However, deep learning methods have shown great promise in not only providing excellent performance for complex and non-linear control problems, but also in generalising previously learned rules to new scenarios. For these reasons, the use of deep learning for vehicle control is becoming increasingly popular. Although important advancements have been achieved in this field, these works have not been fully summarised. This paper surveys a wide range of research works reported in the literature which aim to control a vehicle through deep learning methods. Although there exists overlap between control and perception, the focus of this paper is on vehicle control, rather than the wider perception problem which includes tasks such as semantic segmentation and object detection. The paper identifies the strengths and limitations of available deep learning methods through comparative analysis and discusses the research challenges in terms of computation, architecture selection, goal specification, generalisation, verification and validation, as well as safety. Overall, this survey brings timely and topical information to a rapidly evolving field relevant to intelligent transportation systems.","","","10.1109/TITS.2019.2962338","U.K.-Engineering and Physical Sciences Research Council EPSRC; Jaguar Land Rover; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951131","Machine learning;neural networks;intelligent control;computer vision;advanced driver assistance;autonomous vehicles.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Enhancement of Acoustic Microscopy Lateral Resolution: A Comparison Between Deep Learning and Two Deconvolution Methods","Á. Makra; W. Bost; I. Kalló; A. Horváth; M. Fournelle; M. Gyöngy","Faculty of Information Technology and Bionics, Pázmány Péter Catholic University, Budapest, Hungary; Fraunhofer Institute for Biomedical Engineering, Sulzbach, Germany; Laboratory of Endocrine Neurobiology, Institute of Experimental Medicine, Hungarian Academy of Sciences (HAS), Budapest, Hungary; Faculty of Information Technology and Bionics, Pázmány Péter Catholic University, Budapest, Hungary; Fraunhofer Institute for Biomedical Engineering, Sulzbach, Germany; Faculty of Information Technology and Bionics, Pázmány Péter Catholic University, Budapest, Hungary","IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control","","2020","67","1","136","145","Scanning acoustic microscopy (SAM) provides high-resolution images of biological tissues. Since higher transducer frequencies limit penetration depth, image resolution enhancement techniques could help in maintaining sufficient lateral resolution without sacrificing penetration depth. Compared with existing SAM research, this work introduces two novelties. First, deep learning (DL) is used to improve lateral resolution of 180-MHz SAM images, comparing it with two deconvolution-based approaches. Second, 316-MHz images are used as ground truth in order to quantitatively evaluate image resolution enhancement. The samples used were mouse and rat brain sections. The results demonstrate that DL can closely approximate ground truth (NRMSE = 0.056 and PSNR = 28.4 dB) even with a relatively limited training set (four images, each smaller than 1 mm  $\times 1$  mm). This study suggests the high potential of using DL as a single image superresolution method in SAM.","","","10.1109/TUFFC.2019.2940003","Pázmány University; European Social Fund; NTP-NFTÖ-17 Young Talent Scholarship; ÚNKP-18-3 New National Excellence Program through the Hungarian Ministry of Human Capacities; János Bolyai Scholarship of the Hungarian Academy of Sciences and the Hungarian National Research, Development and Innovation Office (NKFIH); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827572","Deconvolution;deep learning (DL);resolution enhancement;scanning acoustic microscopy (SAM);single image superresolution (SR);U-Net;ultrasound image formation model","Image resolution;Training;Transducers;Deconvolution;Acoustics;Imaging;Deep learning","","","","","77","IEEE","","","","IEEE","IEEE Journals"
"Optical Rectifying Linear Units for Back-Propagation Learning in a Deep Holographic Convolutional Neural Network","K. H. Wagner; S. McComb","University of Colorado Boulder, Boulder, CO, USA; University of Colorado Boulder, Boulder, CO, USA","IEEE Journal of Selected Topics in Quantum Electronics","","2020","26","1","1","18","We present the design for a bidirectional coherent optical Rectifying Linear Unit (ReLU) device capable of phase thresholding and appropriately rectifying forward-propagating optical-neuron activity and gating optically back-propagating error that will enable the construction of an all-optical deep learning system. The ReLU device can be fabricated in large arrays using high-speed liquid-crystal-on-Silicon (LCoS) smart-pixel technology capable of implementing arrays of feature planes needed for convolutional neural networks. Interferometric detection of the phase of a split-off fraction of the forward-propagating neuron input is used to set the state of the bidirectional switch and gate both the rest of the forward-propagating neuron input as well as the back-propagating error signals. We show how the array of convolutional adaptive interconnections needed for deep learning can be physically implemented and learned in an all-optical multistage dynamic holographically-interconnected architecture using lenslet arrays addressing thick Fourier-plane dynamic holograms. This optical architecture is self-aligned, phase-calibrated, and aberration compensated by using phase-conjugate mirrors to record the dynamic-holographic interconnections in each layer. This system has the potential to achieve a computational throughput approaching that of supercomputer clusters at a much lower energy cost by synergistically combining the analog computational properties of coherent Fourier optics with the hardware fault-tolerance provided by error-driven deep-learning algorithms.","","","10.1109/JSTQE.2019.2946655","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8863932","Backpropagation;holographic recording;phase conjugation;optical computing;optical neural networks;optical signal processing","Neurons;Adaptive optics;Optical network units;Nonlinear optics;Holography;Holographic optical components","aberrations;backpropagation;convolutional neural nets;Fourier transform optics;holography;mirrors;optical interconnections;optical neural nets;optical phase conjugation","bidirectional coherent optical rectifying linear unit device;forward-propagating optical-neuron activity;optically back-propagating error gating;high-speed liquid-crystal-on-silicon smart-pixel technology;forward-propagating neuron input;all-optical multistage dynamic holographically-interconnected architecture;feature planes;supercomputer clusters;analog computational properties;hardware fault-tolerance;error-driven deep-learning algorithms;coherent Fourier optics;phase-conjugate mirrors;phase-calibration;Fourier-plane dynamic holograms;lenslet arrays;convolutional adaptive interconnections;back-propagating error signals;bidirectional switch;ReLU device;all-optical deep learning system;phase thresholding;deep holographic convolutional neural network;back-propagation learning","","","99","IEEE","","","","IEEE","IEEE Journals"
"Model-aided Deep Learning Method for Path Loss Prediction in Mobile Communication Systems at 2.6 GHz","J. Thrane; D. Zibar; H. L. Christiansen","Department of Photonics Engineering, Technical University of Denmark, Kgs. Lyngby 2800 DK.; Department of Photonics Engineering, Technical University of Denmark, Kgs. Lyngby 2800 DK.; Department of Photonics Engineering, Technical University of Denmark, Kgs. Lyngby 2800 DK.","IEEE Access","","2020","PP","99","1","1","Accurate channel models are essential to evaluate mobile communication system performance and optimize coverage for existing deployments. The introduction of various transmission frequencies for 5G imposes new challenges for accurate radio performance prediction. This paper compares traditional channel models to a channel model obtained using Deep Learning (DL)-techniques utilizing satellite images aided by a simple path loss model. Experimental measurements are gathered and compose the training and test set. This paper considers path loss modelling techniques offered by state-of-the-art stochastic models and a ray-tracing model for comparison and evaluation. The results show that 1) the satellite images offer an increase in predictive performance by ≈ 0.8 dB, 2) The model-aided technique offers an improvement of ≈ 1 dB, and 3) that the proposed DL model is capable of improving path loss prediction at unseen locations for 811 MHz with ≈ 1 dB and ≈ 4.7 dB for 2630 MHz.","","","10.1109/ACCESS.2020.2964103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950164","5G Mobile Communication;Channel models;Wireless communication;Computer vision;Machine learning;Supervised learning","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Vesti: Energy-Efficient In-Memory Computing Accelerator for Deep Neural Networks","S. Yin; Z. Jiang; M. Kim; T. Gupta; M. Seok; J. Seo","School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; Department of Electrical Engineering, Columbia University, New York City, NY, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; Synopsys, Mountain View, CA, USA; Department of Electrical Engineering, Columbia University, New York City, NY, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2020","28","1","48","61","To enable essential deep learning computation on energy-constrained hardware platforms, including mobile, wearable, and Internet of Things (IoT) devices, a number of digital ASIC designs have presented customized dataflow and enhanced parallelism. However, in conventional digital designs, the biggest bottleneck for energy-efficient deep neural networks (DNNs) has reportedly been the data access and movement. To eliminate the storage access bottleneck, new SRAM macros that support in-memory computing have been recently demonstrated. Several in-SRAM computing works have used the mix of analog and digital circuits to perform XNOR-and-ACcumulate (XAC) operation without row-by-row memory access and can map a subset of DNNs with binary weights and binary activations. In the single array level, large improvement in energy efficiency (e.g., two orders of magnitude improvement) has been reported in computing XAC over digital-only hardware performing the same operation. In this article, by integrating many instances of such in-memory computing SRAM macros with an ensemble of peripheral digital circuits, we architect a new DNN accelerator, titled Vesti. This new accelerator is designed to support configurable multibit activations and large-scale DNNs seamlessly while substantially improving the chip-level energy-efficiency with favorable accuracy tradeoff compared to conventional digital ASIC. Vesti also employs double-buffering with two groups of in-memory computing SRAMs, effectively hiding the row-by-row write latencies of in-memory computing SRAMs. The Vesti accelerator is fully designed and laid out in 65-nm CMOS, demonstrating ultralow energy consumption of <20 nJ for MNIST classification and  $< 40~ \mu \text{J}$  for CIFAR-10 classification at 1.0-V supply.","","","10.1109/TVLSI.2019.2940649","National Science Foundation; Center for Brain-Inspired Computing (C-BRIC), one of the six centers in Joint University Microelectronics Program (JUMP); Semiconductor Research Corporation; Defense Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8867863","Deep learning accelerator;deep neural networks (DNNs);double-buffering;in-memory computing;SRAM","Random access memory;Hardware;Neural networks;Logic arrays;System-on-chip;Deep learning;Parallel processing","","","","","50","IEEE","","","","IEEE","IEEE Journals"
"A Practical Solution for Non-Intrusive Type II Load Monitoring Based on Deep Learning and Post-Processing","W. Kong; Z. Y. Dong; B. Wang; J. Zhao; J. Huang","School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia; School of Science and Engineering, Chinese University of Hong Kong Shenzhen, Shenzhen, China; State Key Laboratories on Smart Grid Protection Operation and Control, State Grid Electric Power Research Institute, Nanjing, China","IEEE Transactions on Smart Grid","","2020","11","1","148","160","This paper presents a practical and effective non-intrusive load monitoring (NILM) solution to estimate the energy consumption for common multi-functional home appliances (type II appliances). Type II home appliances are usually the most challenging cases in load disaggregation because they usually have multiple power consumption states, complex state transitions, and multiple operational modes. The practicality of the proposed deep convolutional neural networks-based approach comes from the minimum prerequisite information from the previously unseen customers. That means no sub-metered information for the target appliances in the NILM service subscriber’s house is needed to provide appliance level identification and estimate under the proposed architecture. Our solution also includes a novel post-processing technique that boost the performance significantly on type II home appliances. The effectiveness of the solution is evaluated on a public dataset to allow comparison with the previous works.","","","10.1109/TSG.2019.2918330","Australian Research Council; ARC Industrial Transformation Research Hubs; Open Grant of the State Key Laboratories on Smart Grid Protection Operation and Control, Shenzhen Municipal Science and Technology Innovation Committee Basic Research Project; National Natural Science Foundation of China; UNSW Industry Ph.D. Support; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8720065","Non-intrusive load monitoring;energy disaggregation;convolutional neural network;deep learning","Home appliances;Hidden Markov models;Smart meters;Power demand;Deep learning;Monitoring;Convolutional neural networks","","","","1","49","IEEE","","","","IEEE","IEEE Journals"
"Double Deep  $Q$ -Learning-Based Distributed Operation of Battery Energy Storage System Considering Uncertainties","V. Bui; A. Hussain; H. Kim","Department of Electrical Engineering, Incheon National University, Incheon, South Korea; Department of Electrical Engineering, Incheon National University, Incheon, South Korea; Department of Electrical Engineering, Incheon National University, Incheon, South Korea","IEEE Transactions on Smart Grid","","2020","11","1","457","469"," $Q$ -learning-based operation strategies are being recently applied for optimal operation of energy storage systems, where, a  $Q$ -table is used to store  $Q$ -values for all possible state-action pairs. However,  $Q$ -learning faces challenges when it comes to large state space problems, i.e., continuous state space problems or problems with environment uncertainties. In order to address the limitations of  $Q$ -learning, this paper proposes a distributed operation strategy using double deep  $Q$ -learning method. It is applied to managing the operation of a community battery energy storage system (CBESS) in a microgrid system. In contrast to  $Q$ -learning, the proposed operation strategy is capable of dealing with uncertainties in the system in both grid-connected and islanded modes. This is due to the utilization of a deep neural network as a function approximator to estimate the  $Q$ -values. Moreover, the proposed method can mitigate the overestimation that is the major drawback of the standard deep  $Q$ -learning. The proposed method trains the model faster by decoupling the selection and evaluation processes. Finally, the performance of the proposed double deep  $Q$ -learning-based operation method is evaluated by comparing its results with a centralized approach-based operation.","","","10.1109/TSG.2019.2924025","Korea Institute of Energy Technology Evaluation and Planning; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8742669","Artificial intelligence;battery energy storage system;double deep Q-learning-based operation;microgrid operation;Q-learning;optimization","Energy management;Uncertainty;Microgrids;Batteries;Neural networks;Optimization","","","","","38","IEEE","","","","IEEE","IEEE Journals"
"Image Compressed Sensing Using Convolutional Neural Network","W. Shi; F. Jiang; S. Liu; D. Zhao","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Image Processing","","2020","29","","375","388","In the study of compressed sensing (CS), the two main challenges are the design of sampling matrix and the development of reconstruction method. On the one hand, the usually used random sampling matrices (e.g., GRM) are signal independent, which ignore the characteristics of the signal. On the other hand, the state-of-the-art image CS methods (e.g., GSR and MH) achieve quite good performance, however with much higher computational complexity. To deal with the two challenges, we propose an image CS framework using convolutional neural network (dubbed CSNet) that includes a sampling network and a reconstruction network, which are optimized jointly. The sampling network adaptively learns the sampling matrix from the training images, which makes the CS measurements retain more image structural information for better reconstruction. Specifically, three types of sampling matrices are learned, i.e., floating-point matrix, {0, 1}-binary matrix, and {-1, +1}-bipolar matrix. The last two matrices are specially designed for easy storage and hardware implementation. The reconstruction network, which contains a linear initial reconstruction network and a non-linear deep reconstruction network, learns an end-to-end mapping between the CS measurements and the reconstructed images. Experimental results demonstrate that CSNet offers state-of-the-art reconstruction quality, while achieving fast running speed. In addition, CSNet with {0, 1}-binary matrix, and {-1, +1}-bipolar matrix gets comparable performance with the existing deep learning-based CS methods, outperforms the traditional CS methods. Experimental results further suggest that the learned sampling matrices can improve the traditional image CS reconstruction methods significantly.","","","10.1109/TIP.2019.2928136","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765626","Compressed sensing;deep learning;convolutional neural network;sampling matrix;image reconstruction","Image reconstruction;Reconstruction algorithms;Matching pursuit algorithms;Deep learning;Computational complexity;Hardware;Compressed sensing","compressed sensing;convolutional neural nets;image reconstruction;image sampling;learning (artificial intelligence);matrix algebra;optimisation","image compressed sensing;convolutional neural network;sampling matrix design;reconstruction method development;sampling network;joint optimisation;network adaptive learning;image structural information;sampling matrices;floating-point matrix;binary matrix;bipolar matrix;nonlinear deep reconstruction network;end-to-end mapping","","","48","","","","","IEEE","IEEE Journals"
"Intelligent Fault Identification Based on Multisource Domain Generalization Towards Actual Diagnosis Scenario","H. Zheng; R. Wang; Y. Yang; Y. Li; M. Xu","Deep Space Exploration Research Center, Harbin Institute of Technology, Harbin, China; Deep Space Exploration Research Center, Harbin Institute of Technology, Harbin, China; Deep Space Exploration Research Center, Harbin Institute of Technology, Harbin, China; Deep Space Exploration Research Center, Harbin Institute of Technology, Harbin, China; Deep Space Exploration Research Center, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Industrial Electronics","","2020","67","2","1293","1304","The data-driven diagnosis methods based on conventional machine-learning techniques have been widely developed in recent years. However, the assumption of conventional methods that the training and test data should be identically distributed is usually unsatisfied in actual diagnosis scenario. While there are several existing works that have been studied to construct diagnosis models by transfer learning methods, most of them are only focused on learning from a single source. Actually, how to discover effective and general diagnosis knowledge from multiple related source domains and further generalize the learned knowledge to new target tasks is crucial to data-driven fault diagnosis. To this end, this paper proposes a novel intelligent fault identification method based on multiple source domains. First, the method describes the discriminant structure of each source domain as a point of Grassmann manifold using local Fisher discriminant analysis. Through preserving the within-class local structure, local Fisher discriminant analysis can learn effective discriminant directions from multimodal fault data. Second, the mean subspace of source domains is computed on the Grassmann manifold through Karcher mean. The mean subspace can be viewed as a representation of the general diagnosis structure that can facilitate the construction of the diagnosis model for the target domain. Experiments on bearing fault diagnosis tasks verify the effectiveness of the proposed method.","","","10.1109/TIE.2019.2898619","Key Laboratory Opening Funding; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643085","Actual diagnosis scenario;data-driven fault diagnosis;multisource domain generalization","Fault diagnosis;Manifolds;Training;Task analysis;Neural networks;Data models;Knowledge transfer","fault diagnosis;learning (artificial intelligence);linear discriminant analysis","local Fisher discriminant analysis;multimodal fault data;fault diagnosis tasks;multisource domain generalization;data-driven diagnosis methods;conventional machine-learning techniques;transfer learning methods;data-driven fault diagnosis;intelligent fault identification method;Grassmann manifold","","1","54","Traditional","","","","IEEE","IEEE Journals"
"A Deep Learning approach to Photoacoustic Wavefront Localization in Deep-Tissue Medium","K. Johnstonbaugh; S. Agrawal; D. A. Durairaj; C. Fadden; A. Dangi; S. P. K. Karri; S. Kothapalli","Department of Biomedical Engineering, Pennsylvania State University, University Park, State College, Pennsylvania, USA, 16802.; Department of Biomedical Engineering, Pennsylvania State University, University Park, State College, Pennsylvania, USA, 16802.; Department of Electrical Engineering, Pennsylvania State University, University Park, State College, Pennsylvania, USA, 16802.; Department of Electrical Engineering, Pennsylvania State University, University Park, State College, Pennsylvania, USA, 16802.; Department of Biomedical Engineering, Pennsylvania State University, University Park, State College, Pennsylvania, USA, 16802.; Department of Electrical Engineering, National Institute of Technology Andhra Pradesh, Tadepalligudem, AP, 534102, India.; Department of Biomedical Engineering, Pennsylvania State University, University Park, State College, Pennsylvania, USA, 16802 and Penn State Cancer Institute, Pennsylvania State University, Hershey, Pennsylvania, USA, 17033 and Graduate Program in Acoustics and Pennsylvania State University, University Park, Pennsylvania, USA 16802.","IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control","","2020","PP","99","1","1","Optical photons undergo strong scattering when propagating beyond one mm deep inside biological tissue. Finding the origin of these diffused optical wavefronts is a challenging task. Breaking through the optical diffusion limit, photoacoustic imaging (PAI) provides high-resolution and label-free images of human vasculature with high-contrast due to optical absorption of hemoglobin. In real time PAI, an ultrasound transducer array detects photoacoustic (PA) signals, and B-mode images are formed via delay-and-sum or frequency domain beamforming. Fundamentally, the strength of a PA signal is proportional to the local optical fluence, which decreases with increasing depth due to depth-dependent optical attenuation. This limits the visibility of deep tissue vasculature or other light absorbing photoacoustic targets. To address this practical challenge, an encoder-decoder convolutional neural network architecture was constructed with custom modules, and trained to identify the origin of photoacoustic wavefronts inside an optically scattering deep-tissue medium. A comprehensive ablation study provides strong evidence that each module improves localization accuracy. The network was trained on model-based simulated photoacoustic signals produced by 16,240 blood vessel targets subjected to both optical scattering and Gaussian noise. Test results on 4,600 simulated and five experimental PA signals collected under various scattering conditions show the network can localize the targets with a mean error less than 30 μm (standard deviation 20.9 μm) for the targets below 40 mm imaging depth, and 1.06 mm (standard deviation 2.68 mm) for targets at a depth between 40 mm and 60 mm. The proposed work has broad applications such as diffused optical wavefront shaping, circulating melanoma cell detection, and in real time vascular surgeries (e.g., deep vein thrombosis).","","","10.1109/TUFFC.2020.2964698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951262","abc","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Privacy-Preserving Collaborative Deep Learning With Unreliable Participants","L. Zhao; Q. Wang; Q. Zou; Y. Zhang; Y. Chen","School of Cyber Science and Engineering, Wuhan University, Wuhan, China; School of Cyber Science and Engineering, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China","IEEE Transactions on Information Forensics and Security","","2020","15","","1486","1500","With powerful parallel computing GPUs and massive user data, neural-network-based deep learning can well exert its strong power in problem modeling and solving, and has archived great success in many applications such as image classification, speech recognition and machine translation etc. While deep learning has been increasingly popular, the problem of privacy leakage becomes more and more urgent. Given the fact that the training data may contain highly sensitive information, e.g., personal medical records, directly sharing them among the users (i.e., participants) or centrally storing them in one single location may pose a considerable threat to user privacy. In this paper, we present a practical privacy-preserving collaborative deep learning system that allows users to cooperatively build a collective deep learning model with data of all participants, without direct data sharing and central data storage. In our system, each participant trains a local model with their own data and only shares model parameters with the others. To further avoid potential privacy leakage from sharing model parameters, we use functional mechanism to perturb the objective function of the neural network in the training process to achieve  $\epsilon $ -differential privacy. In particular, for the first time, we consider the existence of unreliable participants, i.e., the participants with low-quality data, and propose a solution to reduce the impact of these participants while protecting their privacy. We evaluate the performance of our system on two well-known real-world datasets for regression and classification tasks. The results demonstrate that the proposed system is robust against unreliable participants, and achieves high accuracy close to the model trained in a traditional centralized manner while ensuring rigorous privacy protection.","","","10.1109/TIFS.2019.2939713","National Natural Science Foundation of China; Equipment Pre-Research Joint Fund of Ministry of Education of China (Youth Talent); Outstanding Youth Foundation of Hubei Province; Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China; Natural Science Foundation of Hubei Province; National Natural Science Foundation of China; Natural Science Foundation of Hubei Province; Hubei Provincial Technological Innovation Special Funding Major Projects; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825829","Collaborative learning;deep learning;privacy","","","","","","50","IEEE","","","","IEEE","IEEE Journals"
"Few-Shot Deep Adversarial Learning for Video-Based Person Re-Identification","L. Wu; Y. Wang; H. Yin; M. Wang; L. Shao","Key Laboratory of Knowledge Engineering with Big Data, School of Computer Science and Information Engineering, Ministry of Education, Hefei University of Technology, Hefei, China; Key Laboratory of Knowledge Engineering with Big Data, School of Computer Science and Information Engineering, Ministry of Education, Hefei University of Technology, Hefei, China; School of Information Technology and Electrical Engineering, The University of Queensland, St Lucia, QLD, Australia; Key Laboratory of Knowledge Engineering with Big Data, School of Computer Science and Information Engineering, Ministry of Education, Hefei University of Technology, Hefei, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates","IEEE Transactions on Image Processing","","2020","29","","1233","1245","Video-based person re-identification (re-ID) refers to matching people across camera views from arbitrary unaligned video footages. Existing methods rely on supervision signals to optimise a projected space under which the distances between inter/intra-videos are maximised/minimised. However, this demands exhaustively labelling people across camera views, rendering them unable to be scaled in large networked cameras. Also, it is noticed that learning effective video representations with view invariance is not explicitly addressed for which features exhibit different distributions otherwise. Thus, matching videos for person re-ID demands flexible models to capture the dynamics in time-series observations and learn view-invariant representations with access to limited labeled training samples. In this paper, we propose a novel few-shot deep learning approach to video-based person re-ID, to learn comparable representations that are discriminative and view-invariant. The proposed method is developed on the variational recurrent neural networks (VRNNs) and trained adversarially to produce latent variables with temporal dependencies that are highly discriminative yet view-invariant in matching persons. Through extensive experiments conducted on three benchmark datasets, we empirically show the capability of our method in creating view-invariant temporal features and state-of-the-art performance achieved by our method.","","","10.1109/TIP.2019.2940684","National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839731","Video-based person re-identification;variational recurrent neural networks;adversarial learning","Feature extraction;Cameras;Visualization;Training;Measurement;Recurrent neural networks;Video sequences","cameras;image matching;image representation;learning (artificial intelligence);recurrent neural nets;video signal processing","few-shot deep adversarial learning;video-based person re-identification;camera views;video footages;video representations;view invariance;view-invariant representations;video-based person re-ID;view-invariant temporal features;persons matching;variational recurrent neural networks","","","78","","","","","IEEE","IEEE Journals"
"Deep0Tag: Deep Multiple Instance Learning for Zero-Shot Image Tagging","S. Rahman; S. Khan; N. Barnes","Research School of Engineering, The Australian National University, Canberra, ACT, Australia; Research School of Engineering, The Australian National University, Canberra, ACT, Australia; Research School of Engineering, The Australian National University, Canberra, ACT, Australia","IEEE Transactions on Multimedia","","2020","22","1","242","255","Zero-shot learning aims to perform visual reasoning about unseen objects. In-line with the success of deep learning on object recognition problems, several end-to-end deep models for zero-shot recognition have been proposed in the literature. These models are successful in predicting a single unseen label given an input image but do not scale to cases where multiple unseen objects are present. Here, we focus on the challenging problem of zero-shot image tagging, where multiple labels are assigned to an image, that may relate to objects, attributes, actions, events, and scene type. Discovery of these scene concepts requires the ability to process multi-scale information. To encompass global as well as local image details, we propose an automatic approach to locate relevant image patches and model image tagging within the Multiple Instance Learning (MIL) framework. To the best of our knowledge, we propose the first end-to-end trainable deep MIL framework for the multi-label zero-shot tagging problem. We explore several alternatives for instance-level evidence aggregation and perform an extensive ablation study to identify the optimal pooling strategy. Due to its novel design, the proposed framework has several interesting features: 1) unlike previous deep MIL models, it does not use any off-line procedure (e.g., Selective Search or EdgeBoxes) for bag generation. 2) During test time, it can process any number of unseen labels given their semantic embedding vectors. 3) Using only image-level seen labels as weak annotation, it can produce a localized bounding box for each predicted label. We experiment with the large-scale NUS-WIDE and MS-COCO datasets and achieve superior performance across conventional, zero-shot, and generalized zero-shot tagging tasks.","","","10.1109/TMM.2019.2924511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744401","Deep learning;Multiple instance learning;Feature pooling;Object detection;Zero-shot tagging","","","","","","67","IEEE","","","","IEEE","IEEE Journals"
"Vehicle Re-Identification Using Quadruple Directional Deep Learning Features","J. Zhu; H. Zeng; J. Huang; S. Liao; Z. Lei; C. Cai; L. Zheng","College of Engineering, Huaqiao University, Quanzhou, China; College of Information Science and Engineering, Huaqiao University, Xiamen, China; Science and Technology on Micro-system Laboratory, Shanghai Institute of Micro-System and Information Technology, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Biometrics and Security Research, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Biometrics and Security Research, Institute of Automation, Chinese Academy of Sciences, Beijing, China; College of Engineering, Huaqiao University, Quanzhou, China; College of Engineering, Huaqiao University, Quanzhou, China","IEEE Transactions on Intelligent Transportation Systems","","2020","21","1","410","420","In order to resist the adverse effect of viewpoint variations, we design quadruple directional deep learning networks to extract quadruple directional deep learning features (QD-DLF) of vehicle images for improving vehicle re-identification performance. The quadruple directional deep learning networks are of similar overall architecture, including the same basic deep learning architecture but different directional feature pooling layers. Specifically, the same basic deep learning architecture that is a shortly and densely connected convolutional neural network is utilized to extract the basic feature maps of an input square vehicle image in the first stage. Then, the quadruple directional deep learning networks utilize different directional pooling layers, i.e., horizontal average pooling layer, vertical average pooling layer, diagonal average pooling layer, and anti-diagonal average pooling layer, to compress the basic feature maps into horizontal, vertical, diagonal, and anti-diagonal directional feature maps, respectively. Finally, these directional feature maps are spatially normalized and concatenated together as a quadruple directional deep learning feature for vehicle re-identification. The extensive experiments on both VeRi and VehicleID databases show that the proposed QD-DLF approach outperforms multiple state-of-the-art vehicle re-identification methods.","","","10.1109/TITS.2019.2901312","National Natural Science Foundation of China; Natural Science Foundation of Fujian Province; Natural Science Foundation for Outstanding Young Scholars of Fujian Province; Science and Technology Bureau of Quanzhou; Science and Technology Bureau of Xiamen; Promotion Program for Young and Middle-aged Teacher in Science and Technology Research of Huaqiao University; Huaqiao University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667847","Computer vision;artificial neural networks;feature extraction;image classification","Deep learning;Feature extraction;Convolutional neural networks;Databases;Measurement;Cameras;Intelligent transportation systems","","","","4","30","IEEE","","","","IEEE","IEEE Journals"
"Multi-Scale Multi-View Deep Feature Aggregation for Food Recognition","S. Jiang; W. Min; L. Liu; Z. Luo","Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2020","29","","265","276","Recently, food recognition has received more and more attention in image processing and computer vision for its great potential applications in human health. Most of the existing methods directly extracted deep visual features via convolutional neural networks (CNNs) for food recognition. Such methods ignore the characteristics of food images and are, thus, hard to achieve optimal recognition performance. In contrast to general object recognition, food images typically do not exhibit distinctive spatial arrangement and common semantic patterns. In this paper, we propose a multi-scale multi-view feature aggregation (MSMVFA) scheme for food recognition. MSMVFA can aggregate high-level semantic features, mid-level attribute features, and deep visual features into a unified representation. These three types of features describe the food image from different granularity. Therefore, the aggregated features can capture the semantics of food images with the greatest probability. For that solution, we utilize additional ingredient knowledge to obtain mid-level attribute representation via ingredient-supervised CNNs. High-level semantic features and deep visual features are extracted from class-supervised CNNs. Considering food images do not exhibit distinctive spatial layout in many cases, MSMVFA fuses multi-scale CNN activations for each type of features to make aggregated features more discriminative and invariable to geometrical deformation. Finally, the aggregated features are more robust, comprehensive, and discriminative via two-level fusion, namely multi-scale fusion for each type of features and multi-view aggregation for different types of features. In addition, MSMVFA is general and different deep networks can be easily applied into this scheme. Extensive experiments and evaluations demonstrate that our method achieves state-of-the-art recognition performance on three popular large-scale food benchmark datasets in Top-1 recognition accuracy. Furthermore, we expect this paper will further the agenda of food recognition in the community of image processing and computer vision.","","","10.1109/TIP.2019.2929447","National Natural Science Foundation of China; Natural Science Foundation of Beijing Municipality; National Program for Special Support of Eminent Professionals and National Program for Support of Top-notch Young Professionals; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8779586","Food recognition;ingredient knowledge;feature aggregation;convolutional neural networks","Feature extraction;Visualization;Image recognition;Semantics;Object recognition;Deep learning;Computer vision","computer vision;convolutional neural nets;feature extraction;image classification;image fusion;image representation;learning (artificial intelligence);object recognition","food image;optimal recognition performance;general object recognition;multiscale multiview feature aggregation scheme;food recognition;high-level semantic features;mid-level attribute features;deep visual features;aggregated features;multiscale fusion;state-of-the-art recognition performance;large-scale food benchmark datasets;image processing;computer vision;multiscale multiview deep feature aggregation;multiscale CNN activations","","","53","","","","","IEEE","IEEE Journals"
"Attention GANs: Unsupervised Deep Feature Learning for Aerial Scene Classification","Y. Yu; X. Li; F. Liu","Air Defense and Anti-Missile College, Air Force Engineering University, Xi’an, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Air Defense and Anti-Missile College, Air Force Engineering University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2020","58","1","519","531","With the development of deep learning, supervised feature learning methods have achieved prominent performance in the field of aerial scene classification. However, supervised feature learning methods require a large amount of labeled training data. To address this limitation, in this article, a novel unsupervised deep feature learning method, namely, Attention generative adversarial networks (Attention GANs), is proposed for aerial scene classification. First, Attention GANs integrates the attention mechanism into GANs to enhance the representation power of the discriminator. Then, to obtain contextual information, a context-aggregation-based feature fusion architecture is designed in the discriminator. Furthermore, the generator and discriminator losses are improved on basis of the Relativistic GAN. At the same time, a content loss is formed by using the feature representations from the context-aggregation-based feature fusion architecture. In the experiments, our Attention GANs is evaluated via comprehensive experiments with four publicly available remote sensing scene data sets, i.e., the UC-Merced data set with 21 scene classes, the RSSCN7 data set with 7 scene classes, the AID data set with 30 scene classes, and the NWPU-RESISC45 data set with 45 scene classes. Experimental results demonstrate that our Attention GANs can obtain the best performance compared with the state-of-the-art methods.","","","10.1109/TGRS.2019.2937830","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8842616","Aerial scene classification;attention mechanism;context aggregation;generative adversarial networks (GANs);unsupervised deep feature learning","Learning systems;Gallium nitride;Feature extraction;Generators;Task analysis;Remote sensing;Generative adversarial networks","","","","","77","IEEE","","","","IEEE","IEEE Journals"
"Distilling Before Refine: Spatio-Temporal Transfer Learning for Mapping Irrigated Areas Using Sentinel-1 Time Series","H. Bazzi; D. Ienco; N. Baghdadi; M. Zribi; V. Demarez","UMR-TETIS Laboratory, INRAE, University of Montpellier, 34090 Montpellier, France (e-mail: hassan.bazzi@irstea.fr).; LIRMM laboratory, 34090 Montpellier, France.; UMR-TETIS Laboratory, INRAE, University of Montpellier, 34090 Montpellier, France.; CESBIO, Universite Paul Sabatier, 31330 Toulouse, France.; CESBIO, Universite Paul Sabatier, 31330 Toulouse, France.","IEEE Geoscience and Remote Sensing Letters","","2020","PP","99","1","5","This letter proposes a deep learning model to deal with the spatial transfer challenge for the mapping of irrigated areas through the analysis of Sentinel-1 data. First, a convolutional neural network (CNN) model called ``Teacher Model'' is trained on a source geographical area characterized by a huge volume of samples. Then, this model is transferred from the source area to the target area characterized by a limited number of samples. The transfer learning framework is based on a distill and refine strategy, in which the teacher model is first distilled into a student model and, successively, refined by data samples coming from the target geographical area. The proposed strategy is compared with different approaches including a random forest (RF) classifier trained on the target data set and a CNN trained on the source data set and directly applied on the target area as well as several CNN classifiers trained on the target data set. The evaluation of the performed transfer strategy shows that the ``distill and refine'' framework obtains the best performance compared with other competing approaches. The obtained findings represent a first step toward the understanding of the spatial transferability of deep learning models in the Earth observation domain.","","","10.1109/LGRS.2019.2960625","French National Research Agency under the Investments for the Future Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951247","Deep learning;knowledge distillation;satellite image time series;Sentinel-1 (S1);transfer learning","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Deep Non-Local Kalman Network for Video Compression Artifact Reduction","G. Lu; X. Zhang; W. Ouyang; D. Xu; L. Chen; Z. Gao","Shanghai Key Laboratory of Digital Media Processing and Transmission, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Digital Media Processing and Transmission, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; SenseTime Computer Vision Research Group, School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; Shanghai Key Laboratory of Digital Media Processing and Transmission, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Digital Media Processing and Transmission, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Image Processing","","2020","29","","1725","1737","Video compression algorithms are widely used to reduce the huge size of video data, but they also introduce unpleasant visual artifacts due to the lossy compression. In order to improve the quality of the compressed videos, we proposed a deep non-local Kalman network for compression artifact reduction. Specifically, the video restoration is modeled as a Kalman filtering procedure and the decoded frames can be restored from the proposed deep Kalman model. Instead of using the noisy previous decoded frames as temporal information, the less noisy previous restored frame is employed in a recursive way, which provides the potential to generate high quality restored frames. In the proposed framework, several deep neural networks are utilized to estimate the corresponding states in the Kalman filter and integrated together in the deep Kalman filtering network. More importantly, we also exploit the non-local prior information by incorporating the spatial and temporal non-local networks for better restoration. Our approach takes the advantages of both the model-based methods and learning-based methods, by combining the recursive nature of the Kalman model and powerful representation ability of neural networks. Extensive experimental results on the Vimeo-90k and HEVC benchmark datasets demonstrate the effectiveness of our proposed method.","","","10.1109/TIP.2019.2943214","National Natural Science Foundation of China; Natural Science Foundation of Shanghai; Chinese National Key S&T Special Program; Shanghai Key Laboratory of Digital Media Processing and Transmission; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852849","Video compression artifact reduction;deep neural network;Kalman model;recursive filtering;video restoration","Image restoration;Kalman filters;Image coding;Task analysis;Neural networks;Optical imaging;Video compression","data compression;decoding;image denoising;image filtering;image representation;image restoration;Kalman filters;learning (artificial intelligence);neural nets;recursive filters;video coding","HEVC benchmark datasets;Vimeo-90k;deep neural networks;high quality restored frames;noisy previous restored frame;noisy previous decoded frames;deep Kalman model;Kalman filtering procedure;video restoration;lossy compression;visual artifacts;video data;video compression algorithms;video compression artifact reduction;deep nonlocal Kalman network;representation ability;nonlocal networks;nonlocal prior information;deep Kalman filtering network","","","69","IEEE","","","","IEEE","IEEE Journals"
"Learning Modality-Specific Representations for Visible-Infrared Person Re-Identification","Z. Feng; J. Lai; X. Xie","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Image Processing","","2020","29","","579","590","Traditional person re-identification (re-id) methods perform poorly under changing illuminations. This situation can be addressed by using dual-cameras that capture visible images in a bright environment and infrared images in a dark environment. Yet, this scheme needs to solve the visible-infrared matching issue, which is largely under-studied. Matching pedestrians across heterogeneous modalities is extremely challenging because of different visual characteristics. In this paper, we propose a novel framework that employs modality-specific networks to tackle with the heterogeneous matching problem. The proposed framework utilizes the modality-related information and extracts modality-specific representations (MSR) by constructing an individual network for each modality. In addition, a cross-modality Euclidean constraint is introduced to narrow the gap between different networks. We also integrate the modality-shared layers into modality-specific networks to extract shareable information and use a modality-shared identity loss to facilitate the extraction of modality-invariant features. Then a modality-specific discriminant metric is learned for each domain to strengthen the discriminative power of MSR. Eventually, we use a view classifier to learn view information. The experiments demonstrate that the MSR effectively improves the performance of deep networks on VI-REID and remarkably outperforms the state-of-the-art methods.","","","10.1109/TIP.2019.2928126","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765608","Visible-infrared re-identification;deep learning;modality-specific network;cross-modality constraint","Feature extraction;Measurement;Cameras;Lighting;Data mining;Task analysis;Visualization","cameras;feature extraction;image classification;image matching;image representation;infrared imaging;learning (artificial intelligence);object detection;pedestrians","dark environment;modality-specific networks;heterogeneous matching problem;cross-modality Euclidean constraint;modality-shared layers;modality-shared identity loss;visible-infrared person re-identification;learning modality-specific representations;dual-cameras;visible images;infrared images;visible-infrared matching;pedestrian matching;visual characteristics;modality-specific discriminant metric;view classifier;view information learning;deep networks;VI-REID","","","63","","","","","IEEE","IEEE Journals"
"Deep Neural Network Regression for Automated Retinal Layer Segmentation in Optical Coherence Tomography Images","L. Ngo; J. Cha; J. Han","Brain and Cognitive Engineering Department, Korea University, Seoul, South Korea; Sheikh Zayed Institute for Pediatric Surgical Innovation, Children’s National Health System and Department of Pediatrics, George Washington University School of Medicine and Health Sciences, Washington, DC, USA; Brain and Cognitive Engineering Department, Korea University, Seoul, South Korea","IEEE Transactions on Image Processing","","2020","29","","303","312","Segmenting the retinal layers in optical coherence tomography (OCT) images helps to quantify the layer information in early diagnosis of retinal diseases, which are the main cause of permanent blindness. Thus, the segmentation process plays a critical role in preventing vision impairment. However, because there is a lack of practical automated techniques, expert ophthalmologists still have to manually segment the retinal layers. In this paper, we propose an automated segmentation method for OCT images based on a feature-learning regression network without human bias. The proposed deep neural network regression takes the intensity, gradient, and adaptive normalized intensity score (ANIS) of an image segment as features for learning, and then predicts the corresponding retinal boundary pixel. Reformulating the segmentation as a regression problem obviates the need for a huge dataset and reduces the complexity significantly, as shown in the analysis of computational complexity given here. In addition, assisted by ANIS, the method operates robustly on OCT images containing intensity variances, low-contrast regions, speckle noise, and blood vessels, yet remains accurate and time-efficient. In the evaluation of the method conducted using 114 images, the processing time was approximately 10.596 s per image for identifying eight boundaries, and the training phase for each boundary line took only 30 s. Further, the Dice similarity coefficient used for assessing accuracy gave a computed value of approximately 0.966. The absolute pixel distance of manual and automatic segmentation using the proposed scheme was 0.612, which is less than a one-pixel difference, on average.","","","10.1109/TIP.2019.2931461","National Research Foundation of Korea; LG Yonam Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784409","Artificial intelligence;biomedical optical imaging;image segmentation;neural network;optical coherence tomography","Image segmentation;Retina;Training;Image edge detection;Deep learning;Computational complexity;Neural networks","biomedical optical imaging;blood vessels;diseases;eye;image segmentation;learning (artificial intelligence);medical image processing;neural nets;optical tomography;regression analysis","blood vessels;retinal boundary pixel;image segmentation;automatic segmentation;ANIS;adaptive normalized intensity score;feature-learning regression network;OCT images;automated segmentation method;segmentation process;retinal diseases;optical coherence tomography images;automated retinal layer segmentation;deep neural network regression","","","54","","","","","IEEE","IEEE Journals"
"Learning Representations With Local and Global Geometries Preserved for Machine Fault Diagnosis","Y. Li; C. K. L. Lekamalage; T. Liu; P. Chen; G. Huang","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Delta Electronics, Taipei, Taiwan, R.O.C.; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Industrial Electronics","","2020","67","3","2360","2370","Recently, deep learning-based representation learning methods have attracted increasing attention in machine fault diagnosis. However, few existing methods consider the geometry of data samples. In this paper, we propose a novel method to obtain representations that preserve the geometry of input data. More specifically, we formulate two cost functions to preserve the local and global geometries of input data, respectively and another cost function to reconstruct the input data. Furthermore, to simplify the training process, we formulate a discrimination cost function based on the label information. By jointly optimizing all cost functions, the method can efficiently learn discriminative representations with the local and global geometry of input data preserved. Furthermore, the proposed method can obtain hierarchical representations without any additional tuning step. On two benchmark datasets, the proposed method demonstrates better fault classification performance and shorter training and test time. Therefore, it is an efficient tool to provide accurate information about machine conditions for making maintenance decision and saving costs.","","","10.1109/TIE.2019.2905830","EEE-Delta Joint Laboratory on Internet of Things; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672911","Autoencoder (AE);machine fault diagnosis;neural networks;representation learning","Geometry;Cost function;Feature extraction;Fault diagnosis;Training;Matrix decomposition;Learning systems","condition monitoring;data structures;fault diagnosis;learning (artificial intelligence);mechanical engineering computing;neural nets;pattern classification;preventive maintenance","machine fault diagnosis;local geometry;global geometry;discrimination cost function;discriminative representations;fault classification;deep learning-based representation learning;input data geometry;neural networks;autoencoder;machine conditions;predictive maintenance","","1","45","Traditional","","","","IEEE","IEEE Journals"
"Exploring Molecular Descriptors and Fingerprints to Predict mTOR Kinase Inhibitors using Machine Learning Techniques","C. Kumari; M. Abulaish; N. Subbarao","Department of Computer Science, Jamia Millia Islamia, 28849 New Delhi, Delhi India (e-mail: k.chetna@gmail.com); Department of Computer Science, South Asian University, 232442 New Delhi, Delhi India (e-mail: abulaish@ieee.org); School of Computational and Integrative Biology, Jawaharlal Nehru University, 28754 New Delhi, Delhi India (e-mail: nsrao@jnu.ac.in)","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2020","PP","99","1","1","Mammalian Target of Rapamycin (mTOR) is a Ser/Thr protein kinase, and its role is integral to the autophagy pathway in cancer. Targeting mTOR for therapeutic interventions in cancer through autophagy pathway is challenging due to the dual roles of autophagy in tumor progression. The architecture of mTOR reveals two complexes - mTORC1 and mTORC2, each having multiple protein subunits. mTOR kinase inhibitors target the structurally and functionally similar catalytic subunits of both mTORC1 and mTORC2. In this paper, we have explored two different categories of molecular features - descriptors and fingerprints for developing predictive models using machine learning techniques. Random Forest variable importance measures and autoencoders are used to identify molecular descriptors and fingerprints, respectively. We have built various predictive models using identified features and their combination for predicting mTOR kinase inhibitors. Finally, the best model based on the Mathew correlation co-efficient value over the validation dataset is selected for screening kinase SARfari bioactivity dataset. In this study, we have identified twenty best performing descriptors for predicting mTOR kinase inhibitors. To the best of our knowledge, it is the first study on integrating traditional machine learning and deep learning-based approaches for feature extraction to predict mTOR kinase inhibitors.","","","10.1109/TCBB.2020.2964203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950239","Drug Discovery;Kinase;mTOR;Autophagy;Molecular Descriptor;Fingerprints;Machine Learning;Deep Learning","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"An End-to-End Attack on Text CAPTCHAs","Y. Zi; H. Gao; Z. Cheng; Y. Liu","Institute of Software Engineering, Xidian University, Xi’an, China; Institute of Software Engineering, Xidian University, Xi’an, China; Institute of Software Engineering, Xidian University, Xi’an, China; Institute of Software Engineering, Xidian University, Xi’an, China","IEEE Transactions on Information Forensics and Security","","2020","15","","753","766","Text-based CAPTCHAs are the most widely used CAPTCHA scheme. Most text-based CAPTCHAs have been cracked. However, previous works have mostly relied on a series of preprocessing steps to attack text CAPTCHAs, which was complicated and inefficient. In this paper, we introduce a simple, generic, and effective end-to-end attack on text CAPTCHAs without any preprocessing. Through a convolutional neural network and an attention-based recurrent neural network, our attack broke a wide range of real-world text CAPTCHAs that are deployed by the top 50 most popular websites ranked by Alexa.com. In addition, this paper comprehensively analyzed the security of most resistance mechanisms of text-based CAPTCHAs through experiments. Experimental results prove that the anti-segmentation principle can be completely broken under deep learning attacks without any segmentation or preprocessing steps in contrast to commonly held beliefs.","","","10.1109/TIFS.2019.2928622","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762148","CAPTCHA;text-based;security;CNN;RNN;attention;deep learning","CAPTCHAs;Security;Resistance;Feature extraction;Deep learning;Distance measurement;Noise measurement","convolutional neural nets;learning (artificial intelligence);recurrent neural nets;security of data;text analysis;Web sites","attention-based recurrent neural network;real-world text CAPTCHAs;text-based CAPTCHAs;end-to-end attack;convolutional neural network;Web sites;security;anti-segmentation principle;deep learning attacks","","","35","","","","","IEEE","IEEE Journals"
"InSituNet: Deep Image Synthesis for Parameter Space Exploration of Ensemble Simulations","W. He; J. Wang; H. Guo; K. Wang; H. Shen; M. Raj; Y. S. G. Nashed; T. Peterka","Department of Computer Science and EngineeringThe Ohio State University; Department of Computer Science and EngineeringThe Ohio State University; Mathematics and Computer Science DivisionArgonne National Laboratory; Department of Computer Science and EngineeringThe Ohio State University; Department of Computer Science and EngineeringThe Ohio State University; Mathematics and Computer Science DivisionArgonne National Laboratory; Mathematics and Computer Science DivisionArgonne National Laboratory; Mathematics and Computer Science DivisionArgonne National Laboratory","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","23","33","We propose InSituNet, a deep learning based surrogate model to support parameter space exploration for ensemble simulations that are visualized in situ. In situ visualization, generating visualizations at simulation time, is becoming prevalent in handling large-scale simulations because of the I/O and storage constraints. However, in situ visualization approaches limit the flexibility of post-hoc exploration because the raw simulation data are no longer available. Although multiple image-based approaches have been proposed to mitigate this limitation, those approaches lack the ability to explore the simulation parameters. Our approach allows flexible exploration of parameter space for large-scale ensemble simulations by taking advantage of the recent advances in deep learning. Specifically, we design InSituNet as a convolutional regression model to learn the mapping from the simulation and visualization parameters to the visualization results. With the trained model, users can generate new images for different simulation parameters under various visualization settings, which enables in-depth analysis of the underlying ensemble simulations. We demonstrate the effectiveness of InSituNet in combustion, cosmology, and ocean simulations through quantitative and qualitative evaluations.","","","10.1109/TVCG.2019.2934312","US Department of Energy Los Alamos National Laboratory; UT-Battelle LLC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805426","In situ visualization;ensemble visualization;parameter space exploration;deep learning;image synthesis","Data visualization;Data models;Visualization;Analytical models;Space exploration;Deep learning;Image synthesis","","","","","72","","","","","IEEE","IEEE Journals"
"Comparison of Breast MRI Tumor Classification Using Human-Engineered Radiomics, Transfer Learning From Deep Convolutional Neural Networks, and Fusion Method","H. M. Whitney; H. Li; Y. Ji; P. Liu; M. L. Giger","Department of Radiology, The University of Chicago, Chicago, IL, USA; Department of Radiology, The University of Chicago, Chicago, IL, USA; Department of Breast Imaging, Tianjin Medical University Cancer Institute and Hospital, National Clinical Research Center for Cancer, Tianjin Medical University, Tianjin, China; Department of Breast Imaging, Tianjin Medical University Cancer Institute and Hospital, National Clinical Research Center for Cancer, Tianjin Medical University, Tianjin, China; Department of Radiology, The University of Chicago, Chicago, IL, USA","Proceedings of the IEEE","","2020","108","1","163","177","Digital image-based signatures of breast tumors may ultimately contribute to the design of patient-specific breast cancer diagnostics and treatments. Beyond traditional human-engineered computer vision methods, tumor classification methods using transfer learning from deep convolutional neural networks (CNNs) are actively under development. This article will first discuss our progress in using CNN-based transfer learning to characterize breast tumors for various diagnostic, prognostic, or predictive image-based tasks across multiple imaging modalities, including mammography, digital breast tomosynthesis, ultrasound (US), and magnetic resonance imaging (MRI), compared to both human-engineered feature-based radiomics and fusion classifiers created through combination of such features. Second, a new study is presented that reports on a comprehensive comparison of the classification performances of features derived from human-engineered radiomic features, CNN transfer learning, and fusion classifiers for breast lesions imaged with MRI. These studies demonstrate the utility of transfer learning for computer-aided diagnosis and highlight the synergistic improvement in classification performance using fusion classifiers.","","","10.1109/JPROC.2019.2950187","National Cancer Institute; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8908758","Breast cancer;computer-aided diagnosis (CADx);deep learning;dynamic contrast-enhanced (DCE)-magnetic resonance imaging (MRI);radiomics;transfer learning","Feature extraction;Lesions;Biomedical imaging;Breast cancer;Cancer;Magnetic resonance imaging","","","","","40","IEEE","","","","IEEE","IEEE Journals"
"Maneuver Decision of UAV in Short-Range Air Combat Based on Deep Reinforcement Learning","Q. Yang; J. Zhang; G. Shi; J. Hu; Y. Wu","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","IEEE Access","","2020","8","","363","378","With the development of artificial intelligence and integrated sensor technologies, unmanned aerial vehicles (UAVs) are more and more applied in the air combats. A bottleneck that constrains the capability of UAVs against manned vehicles is the autonomous maneuver decision, which is a very challenging problem in the short-range air combat undergoing highly dynamic and uncertain maneuvers of enemies. In this paper, an autonomous maneuver decision model is proposed for the UAV short-range air combat based on reinforcement learning, which mainly includes the aircraft motion model, one-to-one short-range air combat evaluation model and the maneuver decision model based on deep Q network (DQN). However, such model includes a high dimensional state and action space which requires huge computation load for DQN training using traditional methods. Then, a phased training method, called “basic-confrontation”, which is based on the idea that human beings gradually learn from simple to complex is proposed to help reduce the training time while getting suboptimal but efficient results. Finally, one-to-one short-range air combats are simulated under different target maneuver policies. Simulation results show that the proposed maneuver decision model and training method can help the UAV achieve autonomous decision in the air combats and obtain an effective decision policy to defeat the opponent.","","","10.1109/ACCESS.2019.2961426","Aeronautical Science Foundation of China; National Natural Science Foundation of China; Natural Science Foundation of Shaanxi Province; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938773","Deep reinforcement learning;maneuver decision;independent decision;deep Q network;network training","","","","","","25","CCBY","","","","IEEE","IEEE Journals"
"Sparse deep nonnegative matrix factorization","Z. Guo; S. Zhang","Academy of Mathematics and Systems Science, Chinese Academy of Sciences (CAS), Beijing 100190, China; NCMIS, CEMS, RCSDS, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China; School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing 100049, China","Big Data Mining and Analytics","","2020","3","1","13","28","Nonnegative Matrix Factorization (NMF) is a powerful technique to perform dimension reduction and pattern recognition through single-layer data representation learning. However, deep learning networks, with their carefully designed hierarchical structure, can combine hidden features to form more representative features for pattern recognition. In this paper, we proposed sparse deep NMF models to analyze complex data for more accurate classification and better feature interpretation. Such models are designed to learn localized features or generate more discriminative representations for samples in distinct classes by imposing Li-norm penalty on the columns of certain factors. By extending a one-layer model into a multilayer model with sparsity, we provided a hierarchical way to analyze big data and intuitively extract hidden features due to nonnegativity. We adopted the Nesterov's accelerated gradient algorithm to accelerate the computing process. We also analyzed the computing complexity of our frameworks to demonstrate their efficiency. To improve the performance of dealing with linearly inseparable data, we also considered to incorporate popular nonlinearfunctions into these frameworks and explored their performance. We applied our models using two benchmarking image datasets, and the results showed that our models can achieve competitive or better classification performance and produce intuitive interpretations compared with the typical NMF and competing multilayer models.","","","10.26599/BDMA.2019.9020020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935092","sparse Nonnegative Matrix Factorization (NMF);deep learning;Nesterov's accelerated gradient algorithm","","","","","","","","","","","TUP","TUP Journals"
"Deep In-Memory Architectures for Machine Learning-Accuracy Versus Efficiency Trade-Offs","M. Kang; Y. Kim; A. D. Patil; N. R. Shanbhag","IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598 USA.; Western Digital Research, Milpitas, CA 95035 USA (e-mail: yongjune.kim@wdc.com).; Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, Urbana, IL 61801 USA.; Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, Urbana, IL 61801 USA.","IEEE Transactions on Circuits and Systems I: Regular Papers","","2020","PP","99","1","13","In-memory architectures, in particular, the deep in-memory architecture (DIMA) has emerged as an attractive alternative to the traditional von Neumann (digital) architecture for realizing energy and latency-efficient machine learning systems in silicon. Multiple DIMA integrated circuit (IC) prototypes have demonstrated energy-delay product (EDP) gains of up to 100x over a digital architecture. These EDP gains were achieved minimal or sometimes no loss in decision-making accuracy which is surprising given its intrinsic analog mixed-signal nature. This paper establishes models and methods to understand the fundamental energy-delay and accuracy trade-offs underlying DIMA by: 1) presenting silicon-validated energy, delay, and accuracy models; and 2) employing these to quantify DIMA's decision-level accuracy and to identify the most effective design parameters to maximize its EDP gains at a given level of accuracy. For example, it is shown that: 1) DIMA has the potential to realize between 21x-to-1365x gains; 2) its energy-per-decision is approximately 10x lower at the same decision-making accuracy under most conditions; 3) its accuracy can always be improved by increasing the input vector dimension and/or by increasing the bitline swing; and 4) unlike the digital architecture, there are quantifiable conditions under which DIMA's accuracy is fundamentally limited due to noise.","","","10.1109/TCSI.2019.2960841","Air Force Research Laboratory AFRL and Defense Advanced Research Projects Agency DARPA through the Foundations Required for Novel Compute FRANC Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950291","In-memory computing;analog processing;machine learning;processor;accelerator.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A New Online Detection Approach for Rolling Bearing Incipient Fault via Self-Adaptive Deep Feature Matching","W. Mao; J. Chen; X. Liang; X. Zhang","School of Computer and Information Engineering, Henan Normal University, Xinxiang, China; School of Computer and Information Engineering, Henan Normal University, Xinxiang, China; Department of Mechanical Engineering, University of Manitoba, Winnipeg, MB, Canada; School of Computer and Information Engineering, Henan Normal University, Xinxiang, China","IEEE Transactions on Instrumentation and Measurement","","2020","69","2","443","456","This paper presents a new online detection approach for rolling bearing’s incipient fault based on self-adaptive deep feature matching (SDFM). This approach includes offline and online stages. At the offline stage, a new health state assessment algorithm is first proposed based on singular value decomposition (SVD) and Kurtosis criterion. Based on the assessment results, a kind of deep learning algorithm, i.e., stacked denoising autoencoder (SDAE), is introduced to extract the common deep features of normal state and early fault state. Support vector data description (SVDD) is applied to establish the offline detection model using the obtained features. At the online stage, a self-adaptive matching strategy with 1-Dimensional anchor is proposed. By utilizing the SDAE model established at offline stage, this strategy can extract more representative deep features of the target bearing via generating various proposal fragments and then determining the fault occurrence time in a self-adaptive way by feeding the online features into the SVDD model. Experiments run on the bearing data set of IEEE prognostic and health management (PHM) Challenge 2012. The results show the proposed approach has good detection performance in real time and much lower false alarm rate, with no need to acquire fault characteristic frequency in advance.","","","10.1109/TIM.2019.2903699","National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676259","Deep learning;health state assessment;incipient fault detection;online detection;stacked denoising autoencoder (SDAE);support vector data description (SVDD)","Feature extraction;Fault detection;Deep learning;Fault diagnosis;Support vector machines;Employee welfare;Adaptation models","","","","","38","IEEE","","","","IEEE","IEEE Journals"
"Face Sketch Synthesis in the Wild via Deep Patch Representation-Based Probabilistic Graphical Model","C. Peng; N. Wang; J. Li; X. Gao","State Key Laboratory of Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; Video and Image Processing System Laboratory, School of Electronic Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Information Forensics and Security","","2020","15","","172","183","This paper considers the problem of face sketch synthesis in the wild, which transforms a face photo into a face sketch. Face sketch synthesis is widely applied in law enforcement as well as digital entertainment fields. However, the existing methods either focus on hand-crafted techniques where prior human experience is relied on or adopt deep learning techniques as an end-to-end framework, where facial details cannot be well represented. In this paper, we propose a novel approach for face sketch synthesis in the wild via a deep patch representation-based probabilistic graphical model (DeepPGM). A Siamese network is constructed to extract deep patch representation from a raw facial patch, where the representative detail information for robust face sketch synthesis can be exploited. The generated deep patch representation and facial image patches are then optimally combined through a probabilistic graphical model. The proposed DeepPGM approach not only outperforms the state-of-the-art on public face sketch datasets but also can cope with forensic photos in the wild conditions, including varying lightings, poses, occlusions, skin colors, and ethnic origins. The superiority of the proposed method is demonstrated by extensive experiments on two public face sketch datasets and real-world forensic photos in the wild.","","","10.1109/TIFS.2019.2916633","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Natural Science Basic Research Plan in Shaanxi Province of China; National Key Research and Development Program of China; Key Industrial Innovation Chain in Industrial Domain; National High-Level Talents Special Support Program of China; Young Elite Scientists Sponsorship Program by CAST; Young Talent fund of University Association for Science and Technology in Shaanxi, China; CCF-Tencent Open Fund; China 111 Project; China Postdoctoral Science Foundation; Joint fund of ministry of education for equipment pre-research; Xidian University-Intellifusion Joint Innovation Laboratory of Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8714032","Face sketch synthesis;probabilistic graphical model (PGM);deep representation;forensics","Face;Training;Probabilistic logic;Graphical models;Forensics;Task analysis;Transforms","face recognition;image forensics;image representation;learning (artificial intelligence);neural nets;pose estimation;probability","face photo;robust face sketch synthesis;public face sketch datasets;law enforcement;digital entertainment fields;deep learning;DeepPGM;end-to-end framework;Siamese network;facial image patches;deep patch representation;probabilistic graphical model;forensic photos","","","50","","","","","IEEE","IEEE Journals"
"Machine Learning for Resource Management in Cellular and IoT Networks: Potentials, Current Solutions, and Open Challenges","F. Hussain; S. A. Hassan; R. Hussain; E. Hossain","Royal Bank of Canada, Toronto, Canada.; School of Electrical Engineering and Computer Science (SEECS), National University of Sciences and Technology (NUST), Islamabad, Pakistan.; Networks and Blockchain Laboratory, Institute of Information Security and Cyber-Physical System, Innopolis University, Innopolis, Russia.; Department of Electrical and Computer Engineering at University of Manitoba, Winnipeg, Canada.","IEEE Communications Surveys & Tutorials","","2020","PP","99","1","1","Internet-of-Things (IoT) refers to a massively heterogeneous network formed through smart devices connected to the Internet. In the wake of disruptive IoT with a huge amount and variety of data, Machine Learning (ML) and Deep Learning (DL) mechanisms will play a pivotal role to bring intelligence to the IoT networks. Among other aspects, ML and DL can play an essential role in addressing the challenges of resource management in large-scale IoT networks. In this article, we conduct a systematic and in-depth survey of the ML-and DL-based resource management mechanisms in cellular wireless and IoT networks. We start with the challenges of resource management in cellular IoT and low-power IoT networks, review the traditional resource management mechanisms for IoT networks, and motivate the use of ML and DL techniques for resource management in these networks. Then, we provide a comprehensive survey of the existing ML-and DL-based resource allocation techniques in wireless IoT networks and also techniques specifically designed for HetNets, MIMO and D2D communications, and NOMA networks. To this end, we also identify the future research directions in using ML and DL for resource allocation and management in IoT networks.","","","10.1109/COMST.2020.2964534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951180","Internet-of-Things (IoT);Wireless IoT;Machine Learning;Deep Learning;Resource Allocation;Resource Management;D2D;MIMO;HetNets;NOMA.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Deep Spatial and Temporal Network for Robust Visual Object Tracking","Z. Teng; J. Xing; Q. Wang; B. Zhang; J. Fan","School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA","IEEE Transactions on Image Processing","","2020","29","","1762","1775","There are two key components that can be leveraged for visual tracking: (a) object appearances; and (b) object motions. Many existing techniques have recently employed deep learning to enhance visual tracking due to its superior representation power and strong learning ability, where most of them employed object appearances but few of them exploited object motions. In this work, a deep spatial and temporal network (DSTN) is developed for visual tracking by explicitly exploiting both the object representations from each frame and their dynamics along multiple frames in a video, such that it can seamlessly integrate the object appearances with their motions to produce compact object appearances and capture their temporal variations effectively. Our DSTN method, which is deployed into a tracking pipeline in a coarse-to-fine form, can perceive the subtle differences on spatial and temporal variations of the target (object being tracked), and thus it benefits from both off-line training and online fine-tuning. We have also conducted our experiments over four largest tracking benchmarks, including OTB-2013, OTB-2015, VOT2015, and VOT2017, and our experimental results have demonstrated that our DSTN method can achieve competitive performance as compared with the state-of-the-art techniques. The source code, trained models, and all the experimental results of this work will be made public available to facilitate further studies on this problem.","","","10.1109/TIP.2019.2942502","National Natural Science Foundation of China; Northwestern Polytechnical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8848866","Visual tracking;deep network;spatial-temporal LSTM","Target tracking;Visualization;Biological system modeling;Correlation;Training;Benchmark testing","image motion analysis;image representation;learning (artificial intelligence);neural nets;object tracking;video signal processing","robust visual object tracking;visual tracking;deep learning;object representations;compact object appearances;DSTN method;tracking pipeline;object motions;deep spatial and temporal network;video frames","","","61","IEEE","","","","IEEE","IEEE Journals"
"Hyperspectral Image Denoising via Matrix Factorization and Deep Prior Regularization","B. Lin; X. Tao; J. Lu","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","","2020","29","","565","578","Deep learning has been successfully introduced for 2D-image denoising, but it is still unsatisfactory for hyperspectral image (HSI) denoising due to the unacceptable computational complexity of the end-to-end training process and the difficulty of building a universal 3D-image training dataset. In this paper, instead of developing an end-to-end deep learning denoising network, we propose an HSI denoising framework for the removal of mixed Gaussian impulse noise, in which the denoising problem is modeled as a convolutional neural network (CNN) constrained non-negative matrix factorization problem. Using the proximal alternating linearized minimization, the optimization can be divided into three steps: the update of the spectral matrix, the update of the abundance matrix, and the estimation of the sparse noise. Then, we design the CNN architecture and proposed two training schemes, which can allow the CNN to be trained with a 2D-image dataset. Compared with the state-of-the-art denoising methods, the proposed method has a relatively good performance on the removal of the Gaussian and mixed Gaussian impulse noises. More importantly, the proposed model can be only trained once by a 2D-image dataset but can be used to denoise HSIs with different numbers of channel bands.","","","10.1109/TIP.2019.2928627","National Basic Research Project of China (973); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8767025","Hyperspectral image denoising;nonnegative matrix factorization (NMF);deep prior regularization (DPR);convolutional neural networks (CNN)","Noise reduction;Training;Sparse matrices;Optimization;Hyperspectral imaging;Computational modeling;Image denoising","cellular neural nets;Gaussian noise;hyperspectral imaging;image denoising;impulse noise;learning (artificial intelligence);matrix decomposition;minimisation","hyperspectral image denoising;matrix factorization;deep prior regularization;deep learning;computational complexity;end-to-end training process;convolutional neural network;proximal alternating linearized minimization;optimization;abundance matrix;spectral matrix;sparse noise estimation;training schemes;channel bands;mixed Gaussian impulse noise removal","","","53","","","","","IEEE","IEEE Journals"
"Supervised Deep Sparse Coding Networks for Image Classification","X. Sun; N. M. Nasrabadi; T. D. Tran","Johns Hopkins University, Baltimore, MD, USA; Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA","IEEE Transactions on Image Processing","","2020","29","","405","418","In this paper, we propose a novel deep sparse coding network (SCN) capable of efficiently adapting its own regularization parameters for a given application. The network is trained end-to-end with a supervised task-driven learning algorithm via error backpropagation. During training, the network learns both the dictionaries and the regularization parameters of each sparse coding layer so that the reconstructive dictionaries are smoothly transformed into increasingly discriminative representations. In addition, the adaptive regularization also offers the network more flexibility to adjust sparsity levels. Furthermore, we have devised a sparse coding layer utilizing a “skinny” dictionary. Integral to computational efficiency, these skinny dictionaries compress the high-dimensional sparse codes into lower dimensional structures. The adaptivity and discriminability of our 15-layer SCN are demonstrated on six benchmark datasets, namely Cifar-10, Cifar-100, STL-10, SVHN, MNIST, and ImageNet, most of which are considered difficult for sparse coding models. Experimental results show that our architecture overwhelmingly outperforms traditional one-layer sparse coding architectures while using much fewer parameters. Moreover, our multilayer architecture exploits the benefits of depth with sparse coding's characteristic ability to operate on smaller datasets. In such data-constrained scenarios, our technique demonstrates a highly competitive performance compared with the deep neural networks.","","","10.1109/TIP.2019.2928121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765602","Image classification;sparse representation;dictionary learning;image analysis;image recognition","Image coding;Encoding;Dictionaries;Nonhomogeneous media;Machine learning;Training;Image reconstruction","computational complexity;image classification;image coding;learning (artificial intelligence);neural nets","supervised deep sparse coding networks;image classification;adaptive regularization parameters;task-driven learning algorithm;error backpropagation;sparse coding layer;discriminative representations;computational efficiency;benchmark datasets;Cifar-10;Cifar-100;STL-10;SVHN;MNIST;ImageNet","","","69","","","","","IEEE","IEEE Journals"
"Robust unsupervised discriminative dependency parsing","Y. Jiang; J. Cai; K. Tu","School of Information Science and Technology, ShanghaiTech University, Shanghai 201210, China; Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 200050; University of Chinese Academy of Sciences, Beijing 100049, China; School of Information Science and Technology, ShanghaiTech University, Shanghai 201210, China; School of Information Science and Technology, ShanghaiTech University, Shanghai 201210, China","Tsinghua Science and Technology","","2020","25","2","192","202","Discriminative approaches have shown their effectiveness in unsupervised dependency parsing. However, due to their strong representational power, discriminative approaches tend to quickly converge to poor local optima during unsupervised training. In this paper, we tackle this problem by drawing inspiration from robust deep learning techniques. Specifically, we propose robust unsupervised discriminative dependency parsing, a framework that integrates the concepts of denoising autoencoders and conditional random field autoencoders. Within this framework, we propose two types of sentence corruption mechanisms as well as a posterior regularization method for robust training. We tested our methods on eight languages and the results show that our methods lead to significant improvements over previous work.","","","10.26599/TST.2018.9010145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8821513","unsupervised learning;dependency parsing;autoencoders","Training;Noise reduction;Unsupervised learning;Task analysis;Decoding;Support vector machines;Deep learning","grammars;unsupervised learning","robust unsupervised discriminative dependency parsing;deep learning techniques;random field autoencoders","","","","","","","","TUP","TUP Journals"
"Data Augmentation for Deep Learning-Based Radio Modulation Classification","L. Huang; W. Pan; Y. Zhang; L. Qian; N. Gao; Y. Wu","College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China","IEEE Access","","2020","8","","1498","1506","Deep learning has recently been applied to automatically classify the modulation categories of received radio signals without manual experience. However, training deep learning models requires massive volume of data. An insufficient training data will cause serious overfitting problem and degrade the classification accuracy. To cope with small dataset, data augmentation has been widely used in image processing to expand the dataset and improve the robustness of deep learning models. However, in wireless communication areas, the effect of different data augmentation methods on radio modulation classification has not been studied yet. In this paper, we evaluate different data augmentation methods via a state-of-the-art deep learning-based modulation classifier. Based on the characteristics of modulated signals, three augmentation methods are considered, i.e., rotation, flip, and Gaussian noise, which can be applied in both training phase and inference phase of the deep learning-based classifier. Numerical results show that all three augmentation methods can improve the classification accuracy. Among which, the rotation augmentation method outperforms the flip method, both of which achieve higher classification accuracy than the Gaussian noise method. Given only 12.5% of training dataset, a joint rotation and flip augmentation policy can achieve even higher classification accuracy than the baseline with initial 100% training dataset without augmentation. Furthermore, with data augmentation, radio modulation categories can be successfully classified using shorter radio samples, leading to a simplified deep learning model and a shorter classification response time.","","","10.1109/ACCESS.2019.2960775","National Natural Science Foundation of China; National Natural Science Foundation of China; Southeast University; Research Grant of University of Macau; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936957","Data augmentation;deep learning;modulation classification;wireless communication","","","","","","37","CCBY","","","","IEEE","IEEE Journals"
"A Comparative Review of Recent Kinect-Based Action Recognition Algorithms","L. Wang; D. Q. Huynh; P. Koniusz","Department of Computer Science and Software Engineering, The University of Western Australia (UWA), Crawley, WA, Australia; Department of Computer Science and Software Engineering, The University of Western Australia, Crawley, WA, Australia; Machine Learning Research Group (MLRG), Data61/CSIRO (NICTA), Canberra, ACT, Australia","IEEE Transactions on Image Processing","","2020","29","","15","28","Video-based human action recognition is currently one of the most active research areas in computer vision. Various research studies indicate that the performance of action recognition is highly dependent on the type of features being extracted and how the actions are represented. Since the release of the Kinect camera, a large number of Kinect-based human action recognition techniques have been proposed in the literature. However, there still does not exist a thorough comparison of these Kinect-based techniques under the grouping of feature types, such as handcrafted versus deep learning features and depth-based versus skeleton-based features. In this paper, we analyze and compare 10 recent Kinect-based algorithms for both cross-subject action recognition and cross-view action recognition using six benchmark datasets. In addition, we have implemented and improved some of these techniques and included their variants in the comparison. Our experiments show that the majority of methods perform better on cross-subject action recognition than cross-view action recognition, that the skeleton-based features are more robust for cross-view recognition than the depth-based features, and that the deep learning features are suitable for large datasets.","","","10.1109/TIP.2019.2925285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8753686","Human action recognition;Kinect-based algorithms;cross-view action recognition;3D action analysis","Feature extraction;Deep learning;Three-dimensional displays;Australia;Skeleton;Streaming media;Image recognition","cameras;computer vision;feature extraction;image motion analysis;image recognition;image representation;learning (artificial intelligence);object recognition;video signal processing","video-based human action recognition;Kinect camera;Kinect-based human action recognition techniques;deep learning features;skeleton-based features;cross-subject action recognition;cross-view action recognition;depth-based features;Kinect-based action recognition algorithms;computer vision","","","86","","","","","IEEE","IEEE Journals"
"An Incremental Self-Labeling Strategy for Semi-supervised Deep Learning Based on Generative Adversarial Networks","X. Wei; X. Wei; W. Xing; S. Lu; W. Lu","School of Software Engineering, Beijing Jiaotong University, 100044, China.; School of Software Engineering, Beijing Jiaotong University, 100044, China.; School of Software Engineering, Beijing Jiaotong University, 100044, China.; Beijing Key Laboratory of Security and Privacy in Intelligent Transportation, Beijing Jiaotong University, 3 Shangyuancun, Beijing 100044, China.; School of Software Engineering, Beijing Jiaotong University, 100044, China.","IEEE Access","","2020","PP","99","1","1","The recent success of deep neural networks is attributed in part to large-scale well-labeled training data. However, with the ever-increasing size of modern datasets, combined with the difficulty of obtaining label information, semi-supervised learning (SSL) has become one of the most remarkable issues in data analysis. In this paper, we propose an Incremental Self-Labeling strategy for SSL based on Generative Adversarial Nets (ISL-GAN), which functions by constantly assigning unlabeled data with virtual labels for promoting the training process. Specifically, during the virtual labeling process, we introduce a temporal-based self-labeling strategy for safe and stable data labeling. Then, to dynamically assign more virtual labels to data during the training, we conduct a phased incremental label screening and updating strategy. Finally, to balance the contribution of samples with different loss during the training process, we further introduce the Balance factor Term (BT). The experimental results show that the proposed method gives rise to state-of-the-art semi-supervised learning results for the MNIST, CIFAR-10, and SVHN datasets. Particularly, our model performs well with fewer labeled conditions. With a dataset of only 1,000 labeled CIFAR-10 images with CONV-Large Net, a test error of 11.2% can be achieved, and nearly the same performance with a 3.5% test error can be achieved with both 500 and 1,000 image-labeled SVHN datasets.","","","10.1109/ACCESS.2020.2964315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950402","Deep learning;Semi-supervised learning;Generative adversarial networks;Self-labeling","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Bi-Modal Learning with Channel-Wise Attention for Multi-Label Image Classification","P. Li; P. Chen; Y. Xie; D. Zhang","School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing 100083, China and Beijing Key Laboratory of Knowledge Engineering for Materials Science, Beijing 100083, China.; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing 100083, China and Beijing Key Laboratory of Knowledge Engineering for Materials Science, Beijing 100083, China.; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing 100083, China and Beijing Key Laboratory of Knowledge Engineering for Materials Science, Beijing 100083, China.; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing 100083, China and Beijing Key Laboratory of Knowledge Engineering for Materials Science, Beijing 100083, China.","IEEE Access","","2020","PP","99","1","1","Multi-label image classification is more in line with the real-world applications. This problem is difficult due to the the fact that complex label space makes it hard to get label-level attention regions and deal with semantic relationships among labels. Common deep network-based methods utilize CNN to extract features and consider the labels as a sequence or a graph, thus handling the label correlations with RNN or graph-theoretical algorithms. In this paper, we propose a novel CNN-RNN-based model, bi-modal multi-label learning(BMML) framework. Firstly, an improved channel-wise attention mechanism is presented to propose regional attention maps and connect them to relative labels. After that, based on the assumption that objects in a semantic scene always have high-level relevance among visual and textual corpus, we further embed the labels through different pre-trained language models and determine the label sequence in a “semantic space” constructed on large-scale textual data, thereby handling the labels in their semantic context. In addition, a cross-modal feature aligning module is introduced in BMML framework. Experimental results show that BMML is able to achieve better accuracies then those mainstream multi-label classification methods on several benchmark data sets.","","","10.1109/ACCESS.2020.2964599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951081","Multi-label Image Classification;Multi-Modal Learning;Label Embedding;Deep Learning;Attention","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"A Deep Learning Reconstruction Framework for Differential Phase-Contrast Computed Tomography With Incomplete Data","J. Fu; J. Dong; F. Zhao","Research Center of Digital Radiation Imaging, Beihang University, Beijing, China; Research Center of Digital Radiation Imaging, Beihang University, Beijing, China; Hongxia Chemical Co., Ltd., Hohhot, China","IEEE Transactions on Image Processing","","2020","29","1","2190","2202","Differential phase-contrast computed tomography (DPC-CT) is a powerful analysis tool for soft-tissue and low-atomic-number samples. Limited by the implementation conditions, DPC-CT with incomplete projections happens quite often. Conventional reconstruction algorithms face difficulty when given incomplete data. They usually involve complicated parameter selection operations, which are also sensitive to noise and are time-consuming. In this paper, we report a new deep learning reconstruction framework for incomplete data DPC-CT. It involves the tight coupling of the deep learning neural network and DPC-CT reconstruction algorithm in the domain of DPC projection sinograms. The estimated result is not an artifact caused by the incomplete data, but a complete phase-contrast projection sinogram. After training, this framework is determined and can be used to reconstruct the final DPC-CT images for a given incomplete projection sinogram. Taking the sparse-view, limited-view and missing-view DPC-CT as examples, this framework is validated and demonstrated with synthetic and experimental data sets. Compared with other methods, our framework can achieve the best imaging quality at a faster speed and with fewer parameters. This work supports the application of the state-of-the-art deep learning theory in the field of DPC-CT.","","","10.1109/TIP.2019.2947790","National Science and Technology Major Project of China; National Natural Science Foundation of China; Joint Fund of Research utilizing Large-scale Scientific Facilities; National Cancer Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8879671","Image reconstruction;tomography;computed tomography (CT);biomedical signal processing;biomedical imaging;reconstruction algorithms","Image reconstruction;Computed tomography;Deep learning;Neural networks;Feature extraction;Absorption","","","","","62","IEEE","","","","IEEE","IEEE Journals"
"Approximate Fisher Information Matrix to Characterize the Training of Deep Neural Networks","Z. Liao; T. Drummond; I. Reid; G. Carneiro","Australian Centre for Robotic Vision, University of Adelaide, Adelaide, SA, Australia; Australian Centre for Robotic Vision, Monash University, Clayton, VIC, Australia; Australian Centre for Robotic Vision, University of Adelaide, Adelaide, SA, Australia; Australian Centre for Robotic Vision, University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2020","42","1","15","26","In this paper, we introduce a novel methodology for characterizing the performance of deep learning networks (ResNets and DenseNet) with respect to training convergence and generalization as a function of mini-batch size and learning rate for image classification. This methodology is based on novel measurements derived from the eigenvalues of the approximate Fisher information matrix, which can be efficiently computed even for high capacity deep models. Our proposed measurements can help practitioners to monitor and control the training process (by actively tuning the mini-batch size and learning rate) to allow for good training convergence and generalization. Furthermore, the proposed measurements also allow us to show that it is possible to optimize the training process with a new dynamic sampling training approach that continuously and automatically change the mini-batch size and learning rate during the training process. Finally, we show that the proposed dynamic sampling training approach has a faster training time and a competitive classification accuracy compared to the current state of the art.","","","10.1109/TPAMI.2018.2876413","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493265","Machine learning;deep learning, neural networks;stochastic gradient descent;Fisher information matrix;neural network training characterisation","Training;Machine learning;Neural networks;Computational modeling;Convergence;Linear programming;Testing","","","","","45","IEEE","","","","IEEE","IEEE Journals"
"Source Model Selection for Deep Learning in the Time Series Domain","A. Meiseles; L. Rokach","Department of Software and Information System Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel. (e-mail: amielm@post.bgu.ac.il); Department of Software and Information System Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel.","IEEE Access","","2020","PP","99","1","1","Transfer Learning aims to transfer knowledge from a source task to a target task. We focus on a situation when there is a large number of available source models, and we are interested in choosing a single source model that can maximize the predictive performance in the target domain. Existing methods compute some form of ""similarity"" between the source task data and the target task data. They then select the most similar source task and use the model trained on it for transfer learning. Previous methods do not account for the fact that it is the model parameters that are transferred rather than the data. Therefore, the ""similarity"" of the source data does not directly influence transfer learning performance. In addition, we would like the possibility of confidently selecting a source model even when the data it was trained on is not available, for example, due to privacy or copyright constraints. We propose to use the truncated source models as encoders for the target data.We then select a source model based on how well it clusters the target data in the latent encoding space, which we calculate using the Mean Silhouette Coefficient. We prove that if the encodings achieve a Mean Silhouette Coefficient of 1, optimal classification can be achieved using just the final layer of the target network. We evaluate our method using the University of California, Riverside (UCR) time series archive and show that the proposed method achieves comparable results to previous work, without using the source data.","","","10.1109/ACCESS.2019.2963742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949507","deep learning;model selection;time series;transfer learning","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"KOLLECTOR: Detecting Fraudulent Activities on Mobile Devices Using Deep Learning","L. Sun; B. Cao; J. Wang; W. Srisa-an; P. Yu; A. D. Leow; S. Checkoway","Computer Science, UIC, 14681 Chicago, Illinois United States (e-mail: lsun29@uic.edu); Computer Science, UIC, Chicago, Illinois United States (e-mail: caobokai@uic.edu); College of System Engineering, National University of Defense Technology, Changsha, Hunan China (e-mail: wangji@nudt.edu.cn); Computer Science & Engineering, University of Nebraska-Lincoln, Lincoln, Nebraska United States 68588 (e-mail: witty@cse.unl.edu); Computer Science, UIC, Chicago, Illinois United States 60607 (e-mail: psyu@uic.edu); School of Public Health / Psychiatric Institute, University of Illinois at Chicago, 14681 Chicago, Illinois United States (e-mail: aleow@uic.edu); Computer Science, UIC, 14681 Chicago, Illinois United States (e-mail: sfc@uic.edu)","IEEE Transactions on Mobile Computing","","2020","PP","99","1","1","With the rapid growth in smartphone usage, preventing leakage of personal information and privacy has become a challenging task. One major consequence of such leakage is impersonation. This type of illegal usage is nearly impossible to prevent as existing preventive mechanisms (e.g., passcode and fingerprinting), are not capable of continuously monitoring usage and determining whether the user is authorized. Once unauthorized users can defeat the initial protection mechanisms, they would have full access to the devices including using stored passwords to access high-value websites. We present KOLLECTOR, a new framework to detect impersonation based on a multi-view bagging deep learning approach to capture sequential tapping information on the smart-phone's keyboard. We construct a sequential-tapping biometrics model to continuously authenticate the user while typing. We empirically evaluated our system using real-world phone usage sessions from 26 users over eight weeks. We then compared our model against commonly used shallow machine techniques and find that our system performs better than other approaches and can achieve an 8.42% equal error rate, a 94.24% accuracy and a 94.41% H-mean using only the accelerometer and only five keyboard taps. We also experiment with using only three keyboard taps and find that the system still yields high accuracy while giving additional opportunities to make more decisions that can result in more accurate final decisions.","","","10.1109/TMC.2020.2964226","National Science Foundation; Scientific Research Project of National University of Defense Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950079","Mobile;Authorization;Privacy;Deep learning;Multi view learning","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Deep Learning-Based Picture-Wise Just Noticeable Distortion Prediction Model for Image Compression","H. Liu; Y. Zhang; H. Zhang; C. Fan; S. Kwong; C. -. J. Kuo; X. Fan","School of Computer Science and Engineering, Central South University, Changsha, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA; School of Information Technology and Management, Hunan University of Finance and Economics, Changsha, China","IEEE Transactions on Image Processing","","2020","29","","641","656","Picture Wise Just Noticeable Difference (PW-JND), which accounts for the minimum difference of a picture that human visual system can perceive, can be widely used in perception-oriented image and video processing. However, the conventional Just Noticeable Difference (JND) models calculate the JND threshold for each pixel or sub-band separately, which may not reflect the total masking effect of a picture accurately. In this paper, we propose a deep learning based PW-JND prediction model for image compression. Firstly, we formulate the task of predicting PW-JND as a multi-class classification problem, and propose a framework to transform the multi-class classification problem to a binary classification problem solved by just one binary classifier. Secondly, we construct a deep learning based binary classifier named perceptually lossy/lossless predictor which can predict whether an image is perceptually lossy to another or not. Finally, we propose a sliding window based search strategy to predict PW-JND based on the prediction results of the perceptually lossy/lossless predictor. Experimental results show that the mean accuracy of the perceptually lossy/lossless predictor reaches 92%, and the absolute prediction error of the proposed PW-JND model is 0.79 dB on average, which show the superiority of the proposed PW-JND model to the conventional JND models.","","","10.1109/TIP.2019.2933743","National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; Key Project for Guangdong Provincial Science and Technology Development; Shenzhen International Collaborative Research Project; Shenzhen Science and Technology Program; Guangdong International Science and Technology Cooperative Research Project; Chinese Academy of Sciences; Shenzhen Science and Technology Plan Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8796396","Just noticeable distortion;convolutional neural network;visual perception;image quality assessment","Image coding;Predictive models;Streaming media;Adaptation models;Bit rate;Distortion;Visualization","data compression;error statistics;image coding;learning (artificial intelligence);neural nets","deep learning-based picture-wise just noticeable distortion prediction;image compression;video processing;Just Noticeable Difference models;total masking effect;multiclass classification problem transformation;binary classification problem;perceptually lossy-lossless predictor;sliding window search strategy;absolute prediction error","","1","37","","","","","IEEE","IEEE Journals"
"Semi-active Suspension Control Based on Deep Reinforcement Learning","L. Ming; L. Yibin; R. Xuewen; Z. Shuaishuai; Y. Yanfang","College of Control Science and Engineering, Shandong University, Jinan, 250061, China and Department of Electrical Information, Shandong University of Science and Technology, Jinan, 250031, China.; College of Control Science and Engineering, Shandong University, Jinan, 250061, China.; College of Control Science and Engineering, Shandong University, Jinan, 250061, China.; Department of Electrical Information, Shandong University of Science and Technology, Jinan, 250031, China.; Department of Electrical Information, Shandong University of Science and Technology, Jinan, 250031, China.","IEEE Access","","2020","PP","99","1","1","The performance of vehicle body vibration and ride comfort of active or semi-active suspension with proper control is better than that with passive suspension. The key to achieve good control effect is that the suspension control system should have strong real-time learning ability according to changes in the road surface and suspension parameters. In the control strategies adopted by previous researchers, the classical neural network controller has some learning ability, but it is mainly based on offline learning with a large number of samples. In this paper, the deep reinforcement learning strategy is used to solve the above problems. Aiming at the continuity of state space and execution action in vehicle active suspension system, the control of the semi-active suspension is realized by using improved DDPG (Deep Deterministic Policy Gradient) algorithm. To overcome the shortcoming of low efficiency of this algorithm in the initial stage of learning, the DDPG algorithm is improved and using empirical samples in the learning method is proposed. Based on Mujoco, the physical model of semi-active suspension is established, and its dynamic characteristics are analyzed under the condition of various road level and vehicle speed. The simulation results show that compared with the passive suspension, the semi-active suspension based on improved DDPG algorithm with learning method using experienced samples can better adapt to various road level, more effectively reduce the vertical acceleration of the vehicle body and the dynamic deflection of the suspension, and further improve the ride comfort.","","","10.1109/ACCESS.2020.2964116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950280","Semi-active Suspension;Deep Reinforcement Learning;DDPG;Experienced Samples","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Using Channel-Wise Attention for Deep CNN Based Real-Time Semantic Segmentation With Class-Aware Edge Information","H. Han; Y. Chen; P. Hsiao; L. Fu","Department of Computer Science and Information Engineering, National Taiwan University, Taipei 10617, Taiwan.; Department of Computer Science and Information Engineering, National Taiwan University, Taipei 10617, Taiwan (e-mail: r05922084@ntu.edu.tw).; Department of Electrical Engineering, National University of Kaohsiung, Kaohsiung 811, Taiwan.; Department of Computer Science and Information Engineering, National Taiwan University, Taipei 10617, Taiwan.","IEEE Transactions on Intelligent Transportation Systems","","2020","PP","99","1","11","Advanced Driver Assistance Systems (ADAS) consists of two basic functions. One is the object detection for preventing vehicles from hitting pedestrians or other obstacles. The other is image segmentation for recognizing drivable areas and guiding the vehicle forward. For the latter, unlike those traditional image segmentation methods, image semantic segmentation based on deep learning architecture can handle the irregularly shaped road areas better, guiding a vehicle to drive in a more complex environment. With the popularity of Convolution Neural Networks (CNNs) in recent year, the traditional hand-crafted features methods have shown to be outperformed. However, deep CNN models are difficult to implement on vehicle application because the severe cost of time for complex processing. Although some proposed methods, such as Efficient neural network (Enet), achieved higher speed by removing some layers, it also led to the decrease of segmentation accuracy. In this research work, we propose a novel semantic segmentation network, Edgenet, which contains a class-aware edge loss module and a channel-wise attention mechanism, aiming to improve the accuracy with no harm to inference speed. We evaluate Edgenet on Cityscapes dataset, which is the most challenging and authoritative on-road semantic segmentation dataset. The results show that our proposed method can achieve over 70% mean IOU on Cityscapes test set and run at over 30 FPS in a single GTX Titan X (Maxwell) GPU.","","","10.1109/TITS.2019.2962094","Ministry of Science and Technology MOST Taiwan; Center for AI and Advanced Robotics National Taiwan University; Joint Research Center for AI Technology and All Vista Healthcare; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951276","Deep learning;convolution neural networks;real-time semantic segmentation;edge information.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A Deep Evaluator for Image Retargeting Quality by Geometrical and Contextual Interaction","B. Jiang; J. Yang; Q. Meng; B. Li; W. Lu","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Department of Computer Science, Loughborough University, Loughborough, U.K.; Department of Computer Science, Loughborough University, Loughborough, U.K.; School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Cybernetics","","2020","50","1","87","99","An image is compressed or stretched during the multidevice displaying, which will have a very big impact on perception quality. In order to solve this problem, a variety of image retargeting methods have been proposed for the retargeting process. However, how to evaluate the results of different image retargeting is a very critical issue. In various application systems, the subjective evaluation method cannot be applied on a large scale. So we put this problem in the accurate objective-quality evaluation. Currently, most of the image retargeting quality assessment algorithms use simple regression methods as the last step to obtain the evaluation result, which are not corresponding with the perception simulation in the human vision system (HVS). In this paper, a deep quality evaluator for image retargeting based on the segmented stacked AutoEnCoder (SAE) is proposed. Through the help of regularization, the designed deep learning framework can solve the overfitting problem. The main contributions in this framework are to simulate the perception of retargeted images in HVS. Especially, it trains two separated SAE models based on geometrical shape and content matching. Then, the weighting schemes can be used to combine the obtained scores from two models. Experimental results in three well-known databases show that our method can achieve better performance than traditional methods in evaluating different image retargeting results.","","","10.1109/TCYB.2018.2864158","National Natural Science Foundation of China; Natural Science Foundation of Tianjin City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453851","Content match;geometrical shape;image retargeting quality assessment;perception simulation;segmented stacked AutoEnCoder (SAE)","Shape;Databases;Neural networks;Quality assessment;Visualization;Machine learning;Image segmentation","feature extraction;geometry;image enhancement;image resolution;learning (artificial intelligence);neural nets;visual perception","geometrical shape;content matching;contextual interaction;multidevice displaying;perception quality;image retargeting methods;retargeting process;image retargeting quality assessment algorithms;perception simulation;human vision system;deep quality evaluator;deep learning framework;objective-quality evaluation","","8","57","","","","","IEEE","IEEE Journals"
"Apache Spark Accelerated Deep Learning Inference for Large Scale Satellite Image Analytics","D. Lunga; J. Gerrand; L. Yang; C. Layton; R. Stewart","National Security Sciences Directorate, Oak Ridge National Laboratory, Oak Ridge, TN 37830 USA (e-mail: lungadd@ornl.gov).; National Security Sciences Directorate, Oak Ridge National Laboratory, Oak Ridge, TN 37830 USA (e-mail: gerrand.jonathan@gmail.com).; National Security Sciences Directorate, Oak Ridge National Laboratory, Oak Ridge, TN 37830 USA (e-mail: yangh@ornl.gov).; National Security Sciences Directorate, Oak Ridge National Laboratory, Oak Ridge, TN 37830 USA (e-mail: laytoncc@ornl.gov).; National Security Sciences Directorate, Oak Ridge National Laboratory, Oak Ridge, TN 37830 USA (e-mail: stewartrn@ornl.gov).","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2020","PP","99","1","13","The shear volumes of data generated from earth observation and remote sensing technologies continue to make major impact; leaping key geospatial applications into the dual data and compute-intensive era. As a consequence, this rapid advancement poses new computational and data processing challenges. We implement a novel remote sensing data flow (RESFlow) for advancing machine learning to compute with massive amounts of remotely sensed imagery. The core contribution is partitioning massive amounts of data into homogeneous distributions for fitting simple models. RESFlow takes advantage of Apache Spark and the availability of modern computing hardware to harness the acceleration of deep learning inference on expansive remote sensing imagery. The framework incorporates a strategy to optimize resource utilization across multiple executors assigned to a single worker. We showcase its deployment in both computationally and data-intensive workloads for pixel-level labeling tasks. The pipeline invokes deep learning inference at three stages; during deep feature extraction, deep metric mapping, and deep semantic segmentation. The tasks impose compute-intensive and GPU resource sharing challenges motivating for a parallelized pipeline for all execution steps. To address the problem of hardware resource contention, our containerized workflow further incorporates a novel GPU checkout routine and the ticketing system across multiple workers. The workflow is demonstrated with NVIDIA DGX accelerated platforms and offers appreciable compute speed-ups for deep learning inference on pixel labeling workloads; processing 21 028 TB of imagery data and delivering output maps at area rate of 5.245 sq.km/s, amounting to 453 168 sq.km/day—reducing a 28 day workload to 21 h.","","","10.1109/JSTARS.2019.2959707","National Security Sciences Directorate Oak Ridge National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949817","Big data applications;high performance computing;image classification;inference mechanisms;machine learning;supervised learning","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"MRI Based Radiomics Approach with Deep Learning for Prediction of Vessel Invasion in Early-Stage Cervical Cancer","X. Jiang; J. Li; Y. Kan; T. Yu; S. Chang; X. Sha; H. Zheng; Y. Luo; S. Wang","School of Fundamental Sciences, China Medical University, 26488 Shenyang, Liaoning China (e-mail: xrjiang@cmu.edu.cn); School of Fundamental Sciences, China Medical University, 26488 Shenyang, Liaoning China (e-mail: 1061666922@qq.com); Liaoning Cancer Institute and Hospital, 74665 Shenyang, Liaoning China (e-mail: kanyy.love@163.com); Liaoning Cancer Institute and Hospital, 74665 Shenyang, Liaoning China (e-mail: dryutao@hotmail.com); School of Fundamental Sciences, China Medical University, 26488 Shenyang, Liaoning China (e-mail: sjchang@cmu.edu.cn); School of Fundamental Sciences, China Medical University, 26488 Shenyang, Liaoning China (e-mail: xzsha@cmu.edu.cn); Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, 85411 Shenzhen, Guangdong China (e-mail: hr.zheng@siat.ac.cn); Liaoning Cancer Institute and Hospital, 74665 Shenyang, Liaoning China (e-mail: luoyahong8888@hotmail.com); Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, 85411 Shenzhen, Shenzhen China (e-mail: ss.wang@siat.ac.cn)","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2020","PP","99","1","1","This study aims to build deep learning-based radiomic methods in differentiating vessel invasion from non-vessel invasion in cervical cancer with multi-parametric MRI data. A set of 1,070 dynamic T1 contrast-enhanced (DCE-T1) and 986 T2 weighted imaging (T2WI) MRI images from 167 early-stage cervical cancer patients (January 2014 - August 2018) were used to train and validate deep learning models. Predictive performances were evaluated using receiver operating characteristic (ROC) curve and confusion matrix analysis, with the DCE-T1 showing more discriminative results than T2WI MRI. By adopting an attention ensemble learning strategy that integrates both MRI sequences, the highest average area was obtained under the ROC curve (AUC) of 0.911 (Sensitivity = 0.881 and Specificity = 0.752). The superior performances in this study, when compared to existing radiomic methods, indicate that a wealth of deep learning-based radiomics could be developed to aid radiologists in preoperatively predicting vessel invasion in cervical cancer patients.","","","10.1109/TCBB.2019.2963867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949699","Radiomics;deep learning;vessel invasion;cervical cancer;MRI","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"LassoNet: Deep Lasso-Selection of 3D Point Clouds","Z. Chen; W. Zeng; Z. Yang; L. Yu; C. Fu; H. Qu","Hong Kong University of Science and Technology; Shenzhen Institutes of Advanced TechnologyChinese Academy of Sciences; Shenzhen Institutes of Advanced TechnologyChinese Academy of Sciences; University of Groningen; Chinese University of Hong Kong; Hong Kong University of Science and Technology","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","195","204","Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io","","","10.1109/TVCG.2019.2934332","National Natural Science Foundation of China; MSRA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805456","Point Clouds;Lasso Selection;Deep Learning","Three-dimensional displays;Two dimensional displays;Deep learning;Neural networks;Shape;Task analysis;Visualization","","","","","43","","","","","IEEE","IEEE Journals"
"Few-Shot Learning With Geometric Constraints","H. Jung; S. Lee","Department of Brain and Cognitive Engineering, Korea University, Seoul 02841, South Korea.; Department of Artificial Intelligence, Korea University, Seoul 02841, South Korea (e-mail: sw.lee@korea.ac.kr).","IEEE Transactions on Neural Networks and Learning Systems","","2020","PP","99","1","13","In this article, we consider the problem of few-shot learning for classification. We assume a network trained for base categories with a large number of training examples, and we aim to add novel categories to it that have only a few, e.g., one or five, training examples. This is a challenging scenario because: 1) high performance is required in both the base and novel categories; and 2) training the network for the new categories with a few training examples can contaminate the feature space trained well for the base categories. To address these challenges, we propose two geometric constraints to fine-tune the network with a few training examples. The first constraint enables features of the novel categories to cluster near the category weights, and the second maintains the weights of the novel categories far from the weights of the base categories. By applying the proposed constraints, we extract discriminative features for the novel categories while preserving the feature space learned for the base categories. Using public data sets for few-shot learning that are subsets of ImageNet, we demonstrate that the proposed method outperforms prevalent methods by a large margin.","","","10.1109/TNNLS.2019.2957187","Institute of Information and Communications Technology Planning and Evaluation IITP Grant funded by the Korea Government MSIT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8948295","Deep learning;few-shot learning;geometric constraint;image recognition;neural network.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A Deep Generative Model for Graph Layout","O. Kwon; K. Ma","University of California, Davis; University of California, Davis","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","665","675","Different layouts can characterize different aspects of the same graph. Finding a “good” layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.","","","10.1109/TVCG.2019.2934396","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805452","Graph;network;visualization;layout;machine learning;deep learning;neural network;generative model;autoencoder","Layout;Training;Visualization;Task analysis;Data visualization;Machine learning;Data models","","","","","87","","","","","IEEE","IEEE Journals"
"Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations","F. Hohman; H. Park; C. Robinson; D. H. Polo Chau","Georgia Tech.; Georgia Tech.; Georgia Tech.; Georgia Tech.","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","1096","1106","Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.","","","10.1109/TVCG.2019.2934659","NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807294","Deep learning interpretability;visual analytics;scalable summarization;attribution graph","Neurons;Biological neural networks;Feature extraction;Data visualization;Computational modeling;Predictive models;Visualization","","","","","60","","","","","IEEE","IEEE Journals"
"GenerativeMap: Visualization and Exploration of Dynamic Density Maps via Generative Learning Model","C. Chen; C. Wang; X. Bai; P. Zhang; C. Li","School of Computer Science and TechnologyEast China Normal University; School of Computer Science and TechnologyEast China Normal University; School of Computer Science and TechnologyEast China Normal University; School of Computer Science and TechnologyEast China Normal University; School of Computer Science and TechnologyEast China Normal University","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","216","226","The density map is widely used for data sampling, time-varying detection, ensemble representation, etc. The visualization of dynamic evolution is a challenging task when exploring spatiotemporal data. Many approaches have been provided to explore the variation of data patterns over time, which commonly need multiple parameters and preprocessing works. Image generation is a well-known topic in deep learning, and a variety of generating models have been promoted in recent years. In this paper, we introduce a general pipeline called GenerativeMap to extract dynamics of density maps by generating interpolation information. First, a trained generative model comprises an important part of our approach, which can generate nonlinear and natural results by implementing a few parameters. Second, a visual presentation is proposed to show the density change, which is combined with the level of detail and blue noise sampling for a better visual effect. Third, for dynamic visualization of large-scale density maps, we extend this approach to show the evolution in regions of interest, which costs less to overcome the drawback of the learning-based generative model. We demonstrate our method on different types of cases, and we evaluate and compare the approach from multiple aspects. The results help identify the effectiveness of our approach and confirm its applicability in different scenarios.","","","10.1109/TVCG.2019.2934806","NSFC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807296","Density map;deep learning;spatiotemporal data;generative model","Data visualization;Gallium nitride;Spatiotemporal phenomena;Pipelines;Task analysis;Interpolation;Deep learning","","","","","51","","","","","IEEE","IEEE Journals"
"Image Reconstruction: From Sparsity to Data-Adaptive Methods and Machine Learning","S. Ravishankar; J. C. Ye; J. A. Fessler","Departments of Computational Mathematics, Science and Engineering, and Biomedical Engineering, Michigan State University, East Lansing, MI, USA; Department of Bio and Brain Engineering, Department of Mathematical Sciences, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA","Proceedings of the IEEE","","2020","108","1","86","109","The field of medical image reconstruction has seen roughly four types of methods. The first type tended to be analytical methods, such as filtered backprojection (FBP) for X-ray computed tomography (CT) and the inverse Fourier transform for magnetic resonance imaging (MRI), based on simple mathematical models for the imaging systems. These methods are typically fast, but have suboptimal properties such as poor resolution-noise tradeoff for CT. A second type is iterative reconstruction methods based on more complete models for the imaging system physics and, where appropriate, models for the sensor statistics. These iterative methods improved image quality by reducing noise and artifacts. The U.S. Food and Drug Administration (FDA)-approved methods among these have been based on relatively simple regularization models. A third type of methods has been designed to accommodate modified data acquisition methods, such as reduced sampling in MRI and CT to reduce scan time or radiation dose. These methods typically involve mathematical image models involving assumptions such as sparsity or low rank. A fourth type of methods replaces mathematically designed models of signals and systems with data-driven or adaptive models inspired by the field of machine learning. This article focuses on the two most recent trends in medical image reconstruction: methods based on sparsity or low-rank models and data-driven methods based on machine learning techniques.","","","10.1109/JPROC.2019.2936204","National Research Foundation of Korea; National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844696","Compressed sensing (CS);deep learning;dictionary learning (DL);efficient algorithms;image reconstruction;machine learning;magnetic resonance imaging (MRI);multilayer models;nonconvex optimization;positron emission tomography (PET);single-photon emission computed tomography (SPECT);sparse and low-rank models;structured models;transform learning;X-ray computed tomography (CT)","Image reconstruction;Computed tomography;Mathematical model;Magnetic resonance imaging;Machine learning;X-ray imaging;Data models","","","","1","216","IEEE","","","","IEEE","IEEE Journals"
"Deep-Learning-Based Neural Network Training for State Estimation Enhancement: Application to Attitude Estimation","M. K. Al-Sharman; Y. Zweiri; M. A. K. Jaradat; R. Al-Husari; D. Gan; L. D. Seneviratne","Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; Faculty of Science, Engineering and Computing, Kingston University London, London, U.K.; Department of Mechanical Engineering, American University of Sharjah, Sharjah, UAE; Khalifa University Center for Autonomous Robotic System, Khalifa University of Science and Technology, Abu Dhabi, UAE; Department of Mechanical Engineering, Khalifa University of Science and Technology, Abu Dhabi, UAE; Khalifa University Center for Autonomous Robotic System, Khalifa University of Science and Technology, Abu Dhabi, UAE","IEEE Transactions on Instrumentation and Measurement","","2020","69","1","24","34","Achieving precise state estimation is needed for the unmanned aerial vehicle to perform a successful flight with a high degree of stability. Nonetheless, obtaining accurate state estimation is considered challenging due to the inaccuracies associated with the measurements of the onboard commercial-off-the-shelf inertial measurement unit. The immense vibration of the vehicle’s rotors makes these measurements suffer from issues like large drifts, biases, and immense unpredictable noise sequences. These issues cannot be significantly tackled using classical estimators, and an accurate sensor fusion technique needs to be developed. In this paper, a deep learning (DL) framework is developed to enhance the performance of the state estimator. A deep neural network (DNN) is trained using a deep-learning-based technique to identify the associated measurement noise models and filter them out. The dropout technique is adopted for training DNN to avoid overfitting and reduce the complexity of nets computations. Compared to the classical estimation results, the proposed DL technique demonstrates capabilities in identifying the measurement’s noise characteristics. As an example, an enhancement in estimating the attitude states at near hover is proven using this approach. Furthermore, an actual hover flight was performed to validate the proposed estimation enhancement method.","","","10.1109/TIM.2019.2895495","Khalifa University of Science, Technology and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643440","Attitude determination;deep learning (DL);dropout method;multirotor unmanned aerial vehicle;state estimation","State estimation;Unmanned aerial vehicles;Kalman filters;Vehicle dynamics;Neural networks;Noise measurement","","","","","55","IEEE","","","","IEEE","IEEE Journals"
"Deep Neural Network Acceleration Based on Low-Rank Approximated Channel Pruning","Z. Chen; Z. Chen; J. Lin; S. Liu; W. Li","City University of Hong Kong (CityU), Hong Kong, and also with the University of Science and Technology of China (USTC), Hefei 230026, China.; CAS Key Laboratory of Technology in Geo-spatial Information Processing and Application System, University of Science and Technology of China, Hefei 230026, China (e-mail:chenzhibo@ustc.edu.cn).; CAS Key Laboratory of Technology in Geo-spatial Information Processing and Application System, University of Science and Technology of China, Hefei 230026, China.; CAS Key Laboratory of Technology in Geo-spatial Information Processing and Application System, University of Science and Technology of China, Hefei 230026, China.; CAS Key Laboratory of Technology in Geo-spatial Information Processing and Application System, University of Science and Technology of China, Hefei 230026, China.","IEEE Transactions on Circuits and Systems I: Regular Papers","","2020","PP","99","1","18","Acceleration and compression on deep Convolutional Neural Networks (CNNs) have become a critical problem to develop intelligence on resource-constrained devices. Previous channel pruning can be easily deployed and accelerated without specialized hardware and software. However, weight-level redundancy is not well explored in channel pruning, which results in a relatively low compression ratio. In this work, we propose a Low-rank Approximated channel Pruning (LAP) framework to tackle this problem with two targeted steps. First, we utilize low-rank approximation to eliminate the redundancy within filter. This step achieves acceleration, especially in shallow layers, and also converts filters into smaller compact ones. Then, we apply channel pruning on the approximated network in a global way and obtain further benefits, especially in deep layers. In addition, we propose a spectral norm based indicator to coordinate these two steps better. Moreover, inspired by the integral idea adopted in video coding, we propose an evaluator based on Integral of Decay Curve (IDC) to judge the efficiency of various acceleration and compression algorithms. Ablation experiments and IDC evaluator prove that LAP can significantly improve channel pruning. To further demonstrate the hardware compatibility, the network produced by LAP obtains impressive speedup efficiency on the FPGA.","","","10.1109/TCSI.2019.2958937","National Natural Science Foundation of China; National Basic Research Program of China (973 Program); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8948329","Deep learning;network acceleration;channel pruning;low-rank approximation;efficiency evaluation;hardware resources.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Gated and Axis-Concentrated Localization Network for Remote Sensing Object Detection","X. Lu; Y. Zhang; Y. Yuan; Y. Feng","Key Laboratory of Spectral Imaging Technology CAS, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China; Key Laboratory of Spectral Imaging Technology CAS, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China; Center for Optical Imagery Analysis and Learning, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Key Laboratory of Spectral Imaging Technology CAS, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2020","58","1","179","192","In the multicategory object detection task of high-resolution remote sensing images, small objects are always difficult to detect. This happens because the influence of location deviation on small object detection is greater than on large object detection. The reason is that, with the same intersection decrease between a predicted box and a true box, Intersection over Union (IoU) of small objects drops more than those of large objects. In order to address this challenge, we propose a new localization model to improve the location accuracy of small objects. This model is composed of two parts. First, a global feature gating process is proposed to implement a channel attention mechanism on local feature learning. This process takes full advantages of global features’ abundant semantics and local features’ spatial details. In this case, more effective information is selected for small object detection. Second, an axis-concentrated prediction (ACP) process is adopted to project convolutional feature maps into different spatial directions, so as to avoid interference between coordinate axes and improve the location accuracy. Then, coordinate prediction is implemented with a regression layer using the learned object representation. In our experiments, we explore the relationship between the detection accuracy and the object scale, and the results show that the performance improvements of small objects are distinct using our method. Compared with the classical deep learning detection models, the proposed gated axis-concentrated localization network (GACL Net) has the characteristic of focusing on small objects.","","","10.1109/TGRS.2019.2935177","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; National Natural Science Foundation for Distinguished Young Scholars; State Key Program of National Natural Science of China; Chinese Academy of Sciences; Chinese Academy of Sciences; State Key Laboratory of Transient Optics and Photonics; Chinese Academy of Sciences; Xi’an Postdoctoral Innovation Base Scientific Research Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827601","Deep learning;gated axis-concentrated localization network (GACL Net);localization;remote sensing;small object detection","Object detection;Feature extraction;Remote sensing;Deep learning;Detectors;Logic gates;Semantics","","","","","46","IEEE","","","","IEEE","IEEE Journals"
"Deep Tone Mapping Operator for High Dynamic Range Images","A. Rana; P. Singh; G. Valenzise; F. Dufaux; N. Komodakis; A. Smolic","V-SENSE, Trinity College Dublin, Dublin, Ireland; LIGM/IMAGINE, École des Ponts ParisTech, Université Paris-Est, Champs-sur-Marne, France; Laboratoire des Signaux et Systèmes, CNRS, CentraleSupélec, Université Paris-Sud, Orsay, France; Laboratoire des Signaux et Systèmes, CNRS, CentraleSupélec, Université Paris-Sud, Orsay, France; LIGM/IMAGINE, École des Ponts ParisTech, Université Paris-Est, Champs-sur-Marne, France; V-SENSE, Trinity College Dublin, Dublin, Ireland","IEEE Transactions on Image Processing","","2020","29","","1285","1298","A computationally fast tone mapping operator (TMO) that can quickly adapt to a wide spectrum of high dynamic range (HDR) content is quintessential for visualization on varied low dynamic range (LDR) output devices such as movie screens or standard displays. Existing TMOs can successfully tone-map only a limited number of HDR content and require an extensive parameter tuning to yield the best subjective-quality tone-mapped output. In this paper, we address this problem by proposing a fast, parameter-free and scene-adaptable deep tone mapping operator (DeepTMO) that yields a high-resolution and high-subjective quality tone mapped output. Based on conditional generative adversarial network (cGAN), DeepTMO not only learns to adapt to vast scenic-content (e.g., outdoor, indoor, human, structures, etc.) but also tackles the HDR related scene-specific challenges such as contrast and brightness, while preserving the fine-grained details. We explore 4 possible combinations of Generator-Discriminator architectural designs to specifically address some prominent issues in HDR related deep-learning frameworks like blurring, tiling patterns and saturation artifacts. By exploring different influences of scales, loss-functions and normalization layers under a cGAN setting, we conclude with adopting a multi-scale model for our task. To further leverage on the large-scale availability of unlabeled HDR data, we train our network by generating targets using an objective HDR quality metric, namely Tone Mapping Image Quality Index (TMQI). We demonstrate results both quantitatively and qualitatively, and showcase that our DeepTMO generates high-resolution, high-quality output images over a large spectrum of real-world scenes. Finally, we evaluate the perceived quality of our results by conducting a pair-wise subjective study which confirms the versatility of our method.","","","10.1109/TIP.2019.2936649","Science Foundation Ireland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8822603","High dyanmic range images;tone mapping;generative adversarial networks","Dynamic range;Generative adversarial networks;Generators;Task analysis;Imaging;Indexes","image colour analysis;image denoising;image enhancement;image resolution;learning (artificial intelligence);neural nets","pair-wise subjective study;real-world scenes;large-scale availability;saturation artifact;tiling patterns;HDR related scen;high dynamic range images;deep tone mapping operator;generator-discriminator architectural designs;tone mapping image quality index;low dynamic range output devices;scene-adaptable deep tone mapping operator;subjective-quality tone-mapped output;HDR content;high dynamic range content;perceived quality;high-quality output images;objective HDR quality metric;unlabeled HDR data;deep-learning frameworks;DeepTMO;conditional generative adversarial network;high-subjective quality tone","","","68","","","","","IEEE","IEEE Journals"
"Using Bayesian Deep Learning to Capture Uncertainty for Residential Net Load Forecasting","M. Sun; T. Zhang; Y. Wang; G. Strbac; C. Kang","Department of Electrical & Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical & Electronic Engineering, Imperial College London, London, U.K.; Power Systems Laboratory, ETH Zurich, Zurich, Switzerland; Department of Electrical & Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical Engineering, State Key Lab of Power Systems, Tsinghua University, Beijing, China","IEEE Transactions on Power Systems","","2020","35","1","188","201","Decarbonization of electricity systems drives significant and continued investments in distributed energy sources to support the cost-effective transition to low-carbon energy systems. However, the rapid integration of distributed photovoltaic (PV) generation presents great challenges in obtaining reliable and secure grid operations because of its limited visibility and intermittent nature. Under this reality, net load forecasting is facing unprecedented difficulty in answering the following question: How can we accurately predict the net load while capturing the massive uncertainties arising from distributed PV generation and load, especially in the context of high PV penetration? This paper proposes a novel probabilistic day-ahead net load forecasting method to capture both epistemic uncertainty and aleatoric uncertainty using Bayesian deep learning, which is a new field that combines Bayesian probability theory and deep learning. The proposed methodological framework employs clustering in subprofiles and considers residential rooftop PV outputs as input features to enhance the performance of aggregated net load forecasting. Numerical experiments have been carried out based on fine-grained smart meter data from the Australian grid with separately recorded measurements of rooftop PV generation and loads. The results demonstrate the superior performance of the proposed scheme compared with a series of state-of-the-art methods and indicate the importance and effectiveness of subprofile clustering and high PV visibility.","","","10.1109/TPWRS.2019.2924294","National Natural Science Foundation of China; Tsinghua University Initiative Scientific Research Program; European Union's Horizon 2020 Research and Innovation Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8743433","Probabilistic net load forecasting;distributed PV generation;Bayesian deep learning;clustering;long short-term memory","","","","","1","60","IEEE","","","","IEEE","IEEE Journals"
"Large-Scale Mobile App Identification Using Deep Learning","S. Rezaei; B. Kroencke; X. Liu","Computer Science Department, University of California at Davis, Davis, CA, USA; Computer Science Department, University of California at Davis, Davis, CA, USA; Computer Science Department, University of California at Davis, Davis, CA, USA","IEEE Access","","2020","8","","348","362","Many network services and tools (e.g. network monitors, malware-detection systems, routing and billing policy enforcement modules in ISPs) depend on identifying the type of traffic that passes through the network. With the widespread use of mobile devices, the vast diversity of mobile apps, and the massive adoption of encryption protocols (such as TLS), large-scale encrypted traffic classification becomes increasingly difficult. In this paper, we propose a deep learning model for mobile app identification that works even with encrypted traffic. The proposed model only needs the payload of the first few packets for classification, and, hence, it is suitable even for applications that rely on early prediction, such as routing and QoS provisioning. The deep model achieves between 84% to 98% accuracy for the identification of 80 popular apps. We also perform occlusion analysis to bring insight into what data is leaked from SSL/TLS protocol that allows accurate app identification. Moreover, our traffic analysis shows that many apps generate not only app-specific traffic, but also numerous ambiguous flows. Ambiguous flows are flows generated by common functionality modules, such as advertisement and traffic analytics. Because such flows are common among many different apps, identifying the source app that generates ambiguous flows is challenging. To address this challenge, we propose a CNN+LSTM model that uses adjacent flows to learn the order and pattern of multiple flows, to better identify the app that generates them. We show that such flow association considerably improves the accuracy, particularly for ambiguous flows. Furthermore, we show that our approach is robust to mixed traffic scenarios where some unrelated flows may appear in adjacent flows. To the best of our knowledge, this is the first work that identifies the source app for ambiguous flows.","","","10.1109/ACCESS.2019.2962018","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941027","Convolutional neural network;deep learning;flow association;mobile app identification;occlusion analysis;recurrent neural network;smartphone app fingerprinting;traffic classification","","","","","","56","CCBY","","","","IEEE","IEEE Journals"
"Visual Interaction with Deep Learning Models through Collaborative Semantic Inference","S. Gehrmann; H. Strobelt; R. Krüger; H. Pfister; A. M. Rush","Harvard NLP group; IBM Research Cambridge; Harvard Visual Computing group; Harvard Visual Computing group; Harvard NLP group","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","884","894","Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.","","","10.1109/TVCG.2019.2934595","NIH; AWS; Google Faculty Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805457","Human-Computer Collaboration;Deep Learning;Neural Networks;Interaction Design;Human-Centered Design","Visualization;Collaboration;Semantics;Tools;Analytical models;Cognition;Predictive models","","","","","92","","","","","IEEE","IEEE Journals"
"Local Regression Ranking for Saliency Detection","Y. Zhang; S. Zhang; P. Zhang; H. Song; X. Zhang","Physics Electronic Engineering College, Nanyang Normal University, Nanyang, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Physics Electronic Engineering College, Nanyang Normal University, Nanyang, China; Physics Electronic Engineering College, Nanyang Normal University, Nanyang, China; Computer and Information Technology College, Nanyang Normal University, Nanyang, China","IEEE Transactions on Image Processing","","2020","29","","1536","1547","Saliency detection is an important and challenging research topic due to the variety and complex of the background and saliency regions. In this paper, we present a novel unsupervised saliency detection approach by exploiting a learning-based ranking framework. First, the local linear regression model is adopted to simulate the local manifold structure of every image element, which is approximately linear. Using the background queries from the boundary prior, we construct a unified objective function to globally minimize all the errors of the local models for the whole image element points. The Laplacian matrix is learned via optimizing the unified objective function. Low-level image features as well as high-level semantic information extracted from deep neural networks are used for the Laplacian matrix learning. Based on the learnt Laplacian matrix, the saliency of the image element is measured as the relevance ranking to the background queries. The foreground queries are obtained from the background-based saliency and the relevance ranking to the foreground queries is calculated in the same way as the background-based saliency. Second, we calculate an enhanced similarity matrix by fusing two different-level deep feature metrics through cross diffusion. A propagation algorithm uses this enhanced similarity matrix to better exploit the intrinsic relevance of similar regions and improve the saliency ranking results effectively. Results on four benchmark datasets with pixel-wise accurate labelling demonstrate that the proposed unsupervised method shows better performance compared with the newest state-of-the-art methods and is competitive with deep learning-based methods.","","","10.1109/TIP.2019.2942796","National Natural Science Foundation of China; Scientific and Technological Project in Henan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8850321","Learning-based ranking;saliency detection;local linear regression;global calibration;metric fusion;saliency propagation","Feature extraction;Saliency detection;Laplace equations;Extraterrestrial measurements;Neural networks;Kernel","image fusion;image retrieval;neural nets;object detection;regression analysis;unsupervised learning","background queries;Laplacian matrix learning;foreground queries;deep learning;local regression ranking;unsupervised saliency detection;local linear regression model;local manifold structure;learning-based ranking;deep feature metric fusion","","","48","","","","","IEEE","IEEE Journals"
"Analysis of Diffractive Optical Neural Networks and Their Integration With Electronic Neural Networks","D. Mengu; Y. Luo; Y. Rivenson; A. Ozcan","Electrical and Computer Engineering Department, Bioengineering Department, University of California, Los Angeles, CA, USA; Electrical and Computer Engineering Department, Bioengineering Department, University of California, Los Angeles, CA, USA; Electrical and Computer Engineering Department, Bioengineering Department, University of California, Los Angeles, CA, USA; Electrical and Computer Engineering Department, Bioengineering Department, University of California, Los Angeles, CA, USA","IEEE Journal of Selected Topics in Quantum Electronics","","2020","26","1","1","14","Optical machine learning offers advantages in terms of power efficiency, scalability, and computation speed. Recently, an optical machine learning method based on diffractive deep neural networks (D2NNs) has been introduced to execute a function as the input light diffracts through passive layers, designed by deep learning using a computer. Here, we introduce improvements to D2NNs by changing the training loss function and reducing the impact of vanishing gradients in the error back-propagation step. Using five phase-only diffractive layers, we numerically achieved a classification accuracy of 97.18% and 89.13% for optical recognition of handwritten digits and fashion products, respectively; using both phase and amplitude modulation (complex-valued) at each layer, our inference performance improved to 97.81% and 89.32%, respectively. Furthermore, we report the integration of D2NNs with electronic neural networks to create hybrid classifiers that significantly reduce the number of input pixels into an electronic network using an ultra-compact front-end D2NN with a layer-to-layer distance of a few wavelengths, also reducing the complexity of the successive electronic network. Using a five-layer phase-only D2NN jointly optimized with a single fully connected electronic layer, we achieved a classification accuracy of 98.71% and 90.04% for the recognition of handwritten digits and fashion products, respectively. Moreover, the input to the electronic network was compressed by >7.8 times down to 10 × 10 pixels. Beyond creating low-power and high-frame rate machine learning platforms, D2NN-based hybrid neural networks will find applications in smart optical imager and sensor design.","","","10.1109/JSTQE.2019.2921376","European Union's Horizon 2020 Research and Innovation Programme under the Marie Skłodowska-Curie; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8732486","All-optical neural networks;deep learning;hybrid neural networks;optical computing;optical networks;opto-electronic neural networks","Optical fiber networks;Optical diffraction;Biological neural networks;Optical imaging;Training;Optical computing;Adaptive optics","backpropagation;handwritten character recognition;image classification;optical neural nets","diffractive optical neural networks;electronic neural networks;optical machine learning;diffractive deep neural networks;deep learning;phase-only diffractive layers;optical recognition;handwritten digits;fashion products;high-frame rate machine learning platforms;error back-propagation;D2NN-based hybrid neural networks;amplitude modulation;phase modulation","","2","43","OAPA","","","","IEEE","IEEE Journals"
"Deep Learning of Imaging Phenotype and Genotype for Predicting Overall Survival Time of Glioblastoma Patients","Z. Tang; Y. Xu; L. Jin; A. Aibaidula; J. Lu; Z. Jiao; J. Wu; H. Zhang; D. Shen","Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing 100191, China and Department of Radiology and BRIC, University of North Carolina at Chapel Hill, NC 27599, USA.; Zhejiang Provincial People's Hospital, Hangzhou, Zhejiang, China and Hangzhou Medical College, Hangzhou, Zhejiang, China.; Glioma Surgery Division, Neurologic Surgery Department of Huashan Hospital, Shanghai 200040, China.; Glioma Surgery Division, Neurologic Surgery Department of Huashan Hospital, Shanghai 200040, China.; Glioma Surgery Division, Neurologic Surgery Department of Huashan Hospital, Shanghai 200040, China.; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, NC 27599, USA.; Glioma Surgery Division, Neurologic Surgery Department of Huashan Hospital, Shanghai 200040, China and Brain Function Laboratory, Neurosurgical Institute of Fudan University, Shanghai 201100, China and Institute of Brain-Intelligence Technology, Zhangjiang Lab, Shanghai 200135, China.; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, NC 27599, USA.; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, NC 27599, USA and Department of Brain and Cognitive Engineering, Korea University, Seoul 02841, Republic of Korea.","IEEE Transactions on Medical Imaging","","2020","PP","99","1","1","Glioblastoma (GBM) is the most common and deadly malignant brain tumor. For personalized treatment, an accurate pre-operative prognosis for GBM patients is highly desired. Recently, many machine learning-based methods have been adopted to predict overall survival (OS) time based on the pre-operative mono- or multi-modal imaging phenotype. The genotypic information of GBM has been proven to be strongly indicative of the prognosis; however, this has not been considered in the existing imaging-based OS prediction methods. The main reason is that the tumor genotype is unavailable pre-operatively unless deriving from craniotomy. In this paper, we propose a new deep learning-based OS prediction method for GBM patients, which can derive tumor genotype-related features from pre-operative multimodal magnetic resonance imaging (MRI) brain data and feed them to OS prediction. Specifically, we propose a multi-task convolutional neural network (CNN) to accomplish both tumor genotype and OS prediction tasks jointly. As the network can benefit from learning tumor genotype-related features for genotype prediction, the accuracy of predicting OS time can be prominently improved. In the experiments, multimodal MRI brain dataset of 120 GBM patients, with as many as four different genotypic/molecular biomarkers, are used to evaluate our method. Our method achieves the highest OS prediction accuracy compared to other state-of-the-art methods.","","","10.1109/TMI.2020.2964310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950332","Glioblastoma;overall survival;prognosis;genotype;molecular;multi-task;deep learning","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Urban Intelligence with Deep Edges","G. White; S. Clarke","School of Computer Science and Statistics, Trinity College Dublin Dublin, Ireland. (e-mail: whiteg5@scss.tcd.ie); School of Computer Science and Statistics, Trinity College Dublin Dublin, Ireland.","IEEE Access","","2020","PP","99","1","1","With the increased accuracy available from state of the art deep learning models and new embedded devices at the edge of the network capable of running and updating these models there is potential for urban intelligence at the edge of the network. The physical proximity of these edge devices will allow for intelligent reasoning one hop away from data generation. This will allow a range of modern urban reasoning applications that require reduced latency and jitter such as remote surgery, vehicle collision detection and augmented reality. The traffic flow from IoT devices to the cloud will also be reduced as with the increased accuracy from deep learning models only a subset of the data will need to be reported after a first pass analysis. However, the training time of deep learning models can be long, taking weeks on multiple desktop GPUs for large datasets. In this paper we show how transfer learning can be used to update the last layers of pre-trained models at the edge of the network, dramatically reducing the training time and allowing the model to perform new tasks without data ever having to be sent to the cloud. This will also improve the users’ privacy, which is a key requirement for urban intelligence applications with the introduction of GDPR. We compare our approach to alternative IoT urban intelligence architectures such as cloud-based architectures and deep learning algorithms trained only on local data.","","","10.1109/ACCESS.2020.2963912","Science Foundation Ireland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949510","Edge Computing;Transfer Learning;Deep Learning;Urban Intelligence;IoT;QoS","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Video Saliency Prediction Using Spatiotemporal Residual Attentive Networks","Q. Lai; W. Wang; H. Sun; J. Shen","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Image Processing","","2020","29","","1113","1126","This paper proposes a novel residual attentive learning network architecture for predicting dynamic eye-fixation maps. The proposed model emphasizes two essential issues, i.e., effective spatiotemporal feature integration and multi-scale saliency learning. For the first problem, appearance and motion streams are tightly coupled via dense residual cross connections, which integrate appearance information with multi-layer, comprehensive motion features in a residual and dense way. Beyond traditional two-stream models learning appearance and motion features separately, such design allows early, multi-path information exchange between different domains, leading to a unified and powerful spatiotemporal learning architecture. For the second one, we propose a composite attention mechanism that learns multi-scale local attentions and global attention priors end-to-end. It is used for enhancing the fused spatiotemporal features via emphasizing important features in multi-scales. A lightweight convolutional Gated Recurrent Unit (convGRU), which is flexible for small training data situation, is used for long-term temporal characteristics modeling. Extensive experiments over four benchmark datasets clearly demonstrate the advantage of the proposed video saliency model over other competitors and the effectiveness of each component of our network. Our code and all the results will be available at https://github.com/ashleylqx/STRA-Net.","","","10.1109/TIP.2019.2936112","Natural Science Foundation of Beijing Municipality; National Natural Science Foundation of China; CCF-Tencent Open Fund; Zhijiang Lab’s International Talent Fund for Young Professionals; Fok Ying-Tong Education Foundation for Young Teachers; Beijing Municipal Commission of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811731","Dynamic eye-fixation prediction;residual attentive learning;attention mechanism;deep learning;video saliency","Visualization;Computational modeling;Spatiotemporal phenomena;Dynamics;Task analysis;Data models;Predictive models","feature extraction;image fusion;image motion analysis;learning (artificial intelligence);spatiotemporal phenomena;video signal processing","motion streams;dense residual cross connections;multipath information exchange;unified learning architecture;video saliency prediction;dynamic eye-fixation maps;spatiotemporal feature integration;spatiotemporal residual attentive networks;residual attentive learning network architecture;spatiotemporal learning architecture;lightweight convolutional gated recurrent unit;convGRU","","","93","","","","","IEEE","IEEE Journals"
"Web3D learning framework for 3D shape retrieval based on hybrid convolutional neural networks","W. Zhou; J. Jia; C. Huang; Y. Cheng","School of Computer and Information, Anhui Normal University, Wuhu 241002, China; School of Software Engineering, Tongji University, Shanghai 201804, China; College of Electronics and Information Engineering, Tongji University, Shanghai 201804, China; School of Engineering and Computer Science, University of Hull, Hull, HU6 7RX, UK","Tsinghua Science and Technology","","2020","25","1","93","102","With the rapid development of Web3D technologies, sketch-based model retrieval has become an increasingly important challenge, while the application of Virtual Reality and 3D technologies has made shape retrieval of furniture over a web browser feasible. In this paper, we propose a learning framework for shape retrieval based on two Siamese VGG-16 Convolutional Neural Networks (CNNs), and a CNN-based hybrid learning algorithm to select the best view for a shape. In this algorithm, the AlexNet and VGG-16 CNN architectures are used to perform classification tasks and to extract features, respectively. In addition, a feature fusion method is used to measure the similarity relation of the output features from the two Siamese networks. The proposed framework can provide new alternatives for furniture retrieval in the Web3D environment. The primary innovation is in the employment of deep learning methods to solve the challenge of obtaining the best view of 3D furniture, and to address cross-domain feature learning problems. We conduct an experiment to verify the feasibility of the framework and the results show our approach to be superior in comparison to many mainstream state-of-the-art approaches.","","","10.26599/TST.2018.9010113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768209","Web3D;sketch-based model retrieval;Convolutional Neural Networks (CNNs);best view;cross-domain","Shape;Three-dimensional displays;Solid modeling;Feature extraction;Convolutional neural networks;Task analysis;Training","convolutional neural nets;feature extraction;furniture;image classification;image fusion;image retrieval;Internet;learning (artificial intelligence)","Web3D technologies;sketch-based model retrieval;CNN-based hybrid learning algorithm;VGG-16 CNN architectures;feature fusion method;furniture retrieval;Web3D environment;deep learning methods;hybrid convolutional neural networks;Web3D learning framework;3D shape retrieval;Siamese VGG-16 convolutional neural networks;AlexNet;classification tasks;feature extraction;cross-domain feature learning problems","","1","","","","","","TUP","TUP Journals"
"Learning Nonclassical Receptive Field Modulation for Contour Detection","Q. Tang; N. Sang; H. Liu","School of Biomedical Engineering, South-Central University for Nationalities, Wuhan, China; National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; Key Laboratory of Congnitive Science, State Ethnic Affairs Commission, South-Central University for Nationalities, Wuhan, China","IEEE Transactions on Image Processing","","2020","29","","1192","1203","This work develops a biologically inspired neural network for contour detection in natural images by combining the nonclassical receptive field modulation mechanism with a deep learning framework. The input image is first convolved with the local feature detectors to produce the classical receptive field responses, and then a corresponding modulatory kernel is constructed for each feature map to model the nonclassical receptive field modulation behaviors. The modulatory effects can activate a larger cortical area and thus allow cortical neurons to integrate a broader range of visual information to recognize complex cases. Additionally, to characterize spatial structures at various scales, a multiresolution technique is used to represent visual field information from fine to coarse. Different scale responses are combined to estimate the contour probability. Our method achieves state-of-the-art results among all biologically inspired contour detection models. This study provides a method for improving visual modeling of contour detection and inspires new ideas for integrating more brain cognitive mechanisms into deep neural networks.","","","10.1109/TIP.2019.2940690","National Natural Science Foundation of China; Northwestern Polytechnical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839751","Contour detection;nonclassical receptive field;modulation mechanism;multiresolution","Modulation;Visualization;Detectors;Feature extraction;Neurons;Image edge detection;Kernel","brain;cognition;convolutional neural nets;edge detection;image segmentation;learning (artificial intelligence);probability","deep learning;local feature detectors;nonclassical receptive field modulation learning;natural images;deep neural networks;visual modeling;contour detection;contour probability;visual field information;cortical neurons;modulatory effects","","","75","","","","","IEEE","IEEE Journals"
"Massive MIMO Channel Estimation with an Untrained Deep Neural Network","E. Balevi; A. Doshi; J. G. Andrews","Department of Electrical and Computer Engineering at The University of Texas at Austin, Austin, TX 78712 USA.; Department of Electrical and Computer Engineering at The University of Texas at Austin, Austin, TX 78712 USA.; Department of Electrical and Computer Engineering at The University of Texas at Austin, Austin, TX 78712 USA.","IEEE Transactions on Wireless Communications","","2020","PP","99","1","1","This paper proposes a deep learning-based channel estimation method for multi-cell interference-limited massive MIMO systems, in which base stations equipped with a large number of antennas serve multiple single-antenna users. The proposed estimator employs a specially designed deep neural network (DNN) based on the deep image prior (DIP) network to first denoise the received signal, followed by conventional least-squares (LS) estimation. We analytically prove that our LS-type deep channel estimator can approach minimum mean square error (MMSE) estimator performance for high-dimensional signals, while avoiding complex channel inversions and knowledge of the channel covariance matrix. This analytical result, while asymptotic, is observed in simulations to be operational for just 64 antennas and 64 subcarriers per OFDM symbol. The proposed method also does not require any training and utilizes several orders of magnitude fewer parameters than conventional DNNs. The proposed deep channel estimator is also robust to pilot contamination and can even completely eliminate it under certain conditions.","","","10.1109/TWC.2019.2962474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949757","Deep learning;channel estimation;massive MIMO;OFDM;deep image prior","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Deep Heterogeneous Hashing for Face Video Retrieval","S. Qiao; R. Wang; S. Shan; X. Chen","Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China","IEEE Transactions on Image Processing","","2020","29","","1299","1312","Retrieving videos of a particular person with face image as query via hashing technique has many important applications. While face images are typically represented as vectors in Euclidean space, characterizing face videos with some robust set modeling techniques (e.g. covariance matrices as exploited in this study, which reside on Riemannian manifold), has recently shown appealing advantages. This hence results in a thorny heterogeneous spaces matching problem. Moreover, hashing with handcrafted features as done in many existing works is clearly inadequate to achieve desirable performance for this task. To address such problems, we present an end-to-end Deep Heterogeneous Hashing (DHH) method that integrates three stages including image feature learning, video modeling, and heterogeneous hashing in a single framework, to learn unified binary codes for both face images and videos. To tackle the key challenge of hashing on manifold, a well-studied Riemannian kernel mapping is employed to project data (i.e. covariance matrices) into Euclidean space and thus enables to embed the two heterogeneous representations into a common Hamming space, where both intra-space discriminability and inter-space compatibility are considered. To perform network optimization, the gradient of the kernel mapping is innovatively derived via structured matrix backpropagation in a theoretically principled way. Experiments on three challenging datasets show that our method achieves quite competitive performance compared with existing hashing methods.","","","10.1109/TIP.2019.2940683","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Chinese Academy of Sciences; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839744","Face video retrieval;deep heterogeneous hashing;Riemannian kernel mapping;structured matrix backpropagation","Face;Covariance matrices;Task analysis;Binary codes;Kernel;Manifolds;Feature extraction","binary codes;covariance matrices;face recognition;feature extraction;image matching;image representation;learning (artificial intelligence);matrix algebra;video retrieval;video signal processing","end-to-end deep heterogeneous hashing method;hashing methods;inter-space compatibility;intra-space discriminability;common Hamming space;heterogeneous representations;Riemannian kernel mapping;video modeling;image feature learning;thorny heterogeneous spaces matching problem;Riemannian manifold;covariance matrices;robust set modeling techniques;Euclidean space;face image;face video retrieval","","1","76","","","","","IEEE","IEEE Journals"
"explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning","T. Spinner; U. Schlegel; H. Schäfer; M. El-Assady","University of Konstanz; University of Konstanz; University of Konstanz; University of Konstanz","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","1064","1074","We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.","","","10.1109/TVCG.2019.2934629","European Union's Horizon 2020 research and innovation programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807299","Explainable AI;Interactive Machine Learning;Deep Learning;Visual Analytics;Interpretability;Explainability","Data models;Analytical models;Computational modeling;Pipelines;Machine learning;Monitoring","","","","1","86","","","","","IEEE","IEEE Journals"
"New Flexible Multiple-Precision Multiply-Accumulate Unit for Deep Neural Network Training and Inference","H. Zhang; D. Chen; S. Ko","Department of Electrical and Computer Engineering, University of Saskatchewan, Saskatoon, Saskatchewan, Canada; Intel Corporation, San Jose, CA, USA; Department of Electrical and Computer Engineering, University of Saskatchewan, Saskatoon, Saskatchewan, Canada","IEEE Transactions on Computers","","2020","69","1","26","38","In this paper, a new flexible multiple-precision multiply-accumulate (MAC) unit is proposed for deep neural network training and inference. The proposed MAC unit supports both fixed-point operations and floating-point operations. For floating-point format, the proposed unit supports one 16-bit MAC operation or sum of two 8-bit multiplications plus a 16-bit addend. To make the proposed MAC unit more versatile, the bit-width of exponent and mantissa can be flexibly exchanged. By setting the bit-width of exponent to zero, the proposed MAC unit also supports fixed-point operations. For fixed-point format, the proposed unit supports one 16-bit MAC or sum of two 8-bit multiplications plus a 16-bit addend. Moreover, the proposed unit can be further divided to support sum of four 4-bit multiplications plus a 16-bit addend. At the lowest precision, the proposed MAC unit supports accumulating of eight 1-bit logic AND operations to enable the support of binary neural networks. Compared to the standard 16-bit half-precision MAC unit, the proposed MAC unit provides more flexibility with only 21.8 percent area overhead. Compared to a standard 32-bit single-precision MAC unit, the proposed MAC unit requires much less hardware cost but still provides 8-bit exponent in the numerical format to maintain large dynamic range for deep learning computing.","","","10.1109/TC.2019.2936192","Natural Sciences and Engineering Research Council of Canada; University of Saskatchewan; R&D program of MOTIE/KEIT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825551","Multiply-accumulate unit;multiple-precision arithmetic;flexible precision arithmetic;deep neural network computing;computer arithmetic","Neural networks;Standards;Deep learning;Training;Hardware;Adders;Pipelines","","","","","41","IEEE","","","","IEEE","IEEE Journals"
"Aggregation Signature for Small Object Tracking","C. Liu; W. Ding; J. Yang; V. Murino; B. Zhang; J. Han; G. Guo","School of Electrical and Information Engineering, Beihang University, Beijing, China; Unmanned System Research Institute, Beihang University, Beijing, China; School of Computer Science, University of Birmingham, Birmingham, U.K.; Department of Computer Science, University of Verona, Verona, Italy; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; WMG Data Science Group, University of Warwick, Coventry, U.K.; Baidu Research and National Engineering Laboratory for Deep Learning Technology and Application, Institute of Deep Learning, Beijing, China","IEEE Transactions on Image Processing","","2020","29","","1738","1747","Small object tracking becomes an increasingly important task, which however has been largely unexplored in computer vision. The great challenges stem from the facts that: 1) small objects show extreme vague and variable appearances, and 2) they tend to be lost easier as compared to normal-sized ones due to the shaking of lens. In this paper, we propose a novel aggregation signature suitable for small object tracking, especially aiming for the challenge of sudden and large drift. We make three-fold contributions in this work. First, technically, we propose a new descriptor, named aggregation signature, based on saliency, able to represent highly distinctive features for small objects. Second, theoretically, we prove that the proposed signature matches the foreground object more accurately with a high probability. Third, experimentally, the aggregation signature achieves a high performance on multiple datasets, outperforming the state-of-the-art methods by large margins. Moreover, we contribute with two newly collected benchmark datasets, i.e., small90 and small112, for visually small object tracking. The datasets will be available in https://github.com/bczhangbczhang/.","","","10.1109/TIP.2019.2940477","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Shenzhen Science and Technology Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839730","Aggregation signature;small object tracking;saliency;image signature","Discrete cosine transforms;Target tracking;Object tracking;Silicon;Image reconstruction","digital signatures;image representation;object detection;object tracking;probability","small object tracking;novel aggregation signature;foreground object;aggregation signature;image signature;saliency;probability","","","45","IEEE","","","","IEEE","IEEE Journals"
"Development of an Efficient Driving Strategy for Connected and Automated Vehicles at Signalized Intersections: A Reinforcement Learning Approach","M. Zhou; Y. Yu; X. Qu","Tencent Holdings Limited, Shenzhen, China; School of Civil and Environmental Engineering, University of Technology Sydney, Sydney, Australia; Department of Architecture and Civil Engineering, Chalmers University of Technology, Gothenburg, Sweden","IEEE Transactions on Intelligent Transportation Systems","","2020","21","1","433","443","The concept of Connected and Automated Vehicles (CAVs) enables instant traffic information to be shared among vehicle networks. With this newly proposed concept, a vehicle’s driving behaviour will no longer be solely based on the driver’s limited and incomplete observation. By taking advantages of the shared information, driving behaviours of CAVs can be improved greatly to a more responsible, accurate and efficient level. This study proposed a reinforcement-learning-based car following model for CAVs in order to obtain an appropriate driving behaviour to improve travel efficiency, fuel consumption and safety at signalized intersections in real-time. The result shows that by specifying an effective reward function, a controller can be learned and works well under different traffic demands as well as traffic light cycles with different durations. This study reveals a great potential of emerging reinforcement learning technologies in transport research and applications.","","","10.1109/TITS.2019.2942014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8848852","Neural network;reinforcement learning;car-following;intersection;traffic light;machine learning;deep deterministic policy gradient;traffic oscillation","Oscillators;Trajectory;Reinforcement learning;Vehicles;Real-time systems;Optimization;Training","","","","1","63","IEEE","","","","IEEE","IEEE Journals"
"Prediction of Safe-Zone Evolution in Poisonous Environment Based on Deep Learning","J. Xv; Z. Hou","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Access","","2020","8","","649","659","Precise online identification of safe-zone evolution has been long desired in the context of indoor chemical attack events. Computational fluid dynamics (CFD) technology can provide great accuracy but is incapable of applying on online predictions of a large-scale fluid system due to the enormous computational costs to date. In this paper, we propose a Multi-Step Spatial-Temporal Situational-Awareness Network (MSSTP-SA Net) based on deep learning algorithms for rapid online estimation of concentration field. The dataset is firstly created by CFD simulation considering different poisonous gas release and decontamination scenarios in pre-specific domain with various combinations of airflow conditions and source parameters. Corresponding experiments are also provided for validations. The test experiments of the trained network suggest that the evolution of concentration field’s distribution can be predicted faithfully in millisecond computation time costs. We hope this approach to be highly useful in most chemical attack scenarios to reduce casualties.","","","10.1109/ACCESS.2019.2960451","National Natural Science Foundation of China; Scientific and Innovative Action Plan of Shanghai; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935332","Chemical attack;CFD;deep learning;safe-zone","","","","","","38","CCBY","","","","IEEE","IEEE Journals"
"Radon Inversion via Deep Learning","J. He; Y. Wang; J. Ma","School of Biomedical Engineering, Southern Medical University, Guangzhou 510515, China and Guangzhou key Laboratory of Medical Radiation Imaging and Detection Technology, Southern Medical University, Guangzhou 510515, China.; School of Biomedical Engineering, Southern Medical University, Guangzhou 510515, China and Guangzhou key Laboratory of Medical Radiation Imaging and Detection Technology, Southern Medical University, Guangzhou 510515, China.; School of Biomedical Engineering, Southern Medical University, Guangzhou 510515, China and Guangzhou key Laboratory of Medical Radiation Imaging and Detection Technology, Southern Medical University, Guangzhou 510515, China.","IEEE Transactions on Medical Imaging","","2020","PP","99","1","1","The Radon transform is widely used in physical and life sciences, and one of its major applications is in medical X-ray computed tomography (CT), which is significantly important in disease screening and diagnosis. In this paper, we propose a novel reconstruction framework for Radon inversion with deep learning (DL) techniques. For simplicity, the proposed framework is denoted as iRadonMAP, i.e., inverse Radon transform approximation. Specifically, we construct an interpretable neural network that contains three dedicated components. The first component is a fully connected filtering (FCF) layer along the rotation angle direction in the sinogram domain, and the second one is a sinusoidal back-projection (SBP) layer, which back-projects the filtered sinogram data into the spatial domain. Next, a common network structure is added to further improve the overall performance. iRadonMAP is first pretrained on a large number of generic images from the ImageNet database and then fine-tuned with clinical patient data. The experimental results demonstrate the feasibility of the proposed iRadonMAP framework for Radon inversion.","","","10.1109/TMI.2020.2964266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950464","Radon transform;Radon inversion;computed tomography;image reconstruction;deep learning","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Fuzzy Deep Neural Learning Based on Goodman and Kruskal's Gamma for Search Engine Optimization","S. Jayaraman; M. Ramachandran; R. Patan; M. Daneshmand; A. H. Gandomi","SASTRA Deemed University, Thanjavur, Tamil Nadu India (e-mail: lalgudisethu@gmail.com); School of Computing, SASTRA Deemed University, Thanjavur, Tamil Nadu India (e-mail: srmanimt75@gmail.com); School of Computer science Engineering, VIT University, 30026 Vellore, Tamil Nadu India 632014 (e-mail: prizwan5@gmail.com); Stevens Institute of Technology, Hoboken, New Jersey United States (e-mail: mdaneshm@stevens.edu); Faculty of Engineering & Information Technology, University of Technology Sydney, 1994 Sydney, New South Wales Australia (e-mail: a.h.gandomi@gmail.com)","IEEE Transactions on Big Data","","2020","PP","99","1","1","Search engine optimization (SEO) is a significant problem for enhancing a website's visibility with search engine results. SEO issues, such as Site Popularity, Content Quality, Keyword Density, and Publicity, were not considered during the search engine optimization process. Therefore, the retrieval rate of the existing techniques is inadequate. In this study, Triangular Fuzzy Deep Structured Learning-Based Predictive Page Ranking (TFDSL-PPR) Technique is proposed to solve these limitations. First, the TFDSL-PPR technique takes a number of user queries as input in the input layer, and then it employs four hidden layers in order to deeply analyze the web pages based on an input query. The first hidden layer determines the keywords from the user query. The second hidden layer measures the site popularity, content quality, keyword density and publicity of all web pages in the search engine. It then accomplishes Goodman and Kruskal's Gamma Predictive Ranking process in the third hidden layer, where it ranks the web pages by considering their similarities. The proposed TFDSL-PPR technique is applied to the ClueWeb09 Dataset with respect to a variety of user queries. The results are benchmarked by existing methods based on several metrics such as retrieval rate, time, and false-positive rate.","","","10.1109/TBDATA.2020.2963982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949739","Deep Structured Learning;Filtering;Ranking;Search Engine;Site Popularity;Content Quality;Keyword Density;Publicity;Web pages","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"ARTDL: Adaptive Random Testing for Deep Learning Systems","M. Yan; L. Wang; A. Fei","School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Access","","2020","8","","3055","3064","With recent breakthroughs in Deep Learning (DL), DL systems are increasingly deployed in safety-critical fields. Hence, some software testing methods are required to ensure the reliability and safety of DL systems. Since the rules of DL systems are inferred from training data, it is difficult to know the implementation rules about each behavior of DL systems. At the same time, Random Testing (RT) is a popular testing method and the knowledge about software implementation is not needed when we use RT. Therefore, RT is very suitable for the testing of DL systems. And the existing mechanisms for testing DL systems also depend heavily on RT by the labeled test data. In order to increase the effectiveness of RT for DL systems, we design, implement and evaluate the Adaptive Random Testing for DL systems (ARTDL), which is the first Adaptive Random Testing (ART) method to improve the effectiveness of RT for DL systems. ARTDL refers to the idea of ART. That is, fewer test cases are needed to detect failures by selecting the test case with the furthest distance from non-failure-causing test cases. Firstly, we propose the Feature-based Euclidean Distance (FED) as the distance metric that can be used to measure the difference between failure-causing inputs and non-failure-causing inputs. Secondly, we verify the availability of FED by presenting the failure pattern of DL models. Finally, we design ARTDL algorithm to generate the test cases that are more likely to cause failures based on the FED. We implement ARTDL to test top performing DL models in the field of image classification and automatic driving. The results show that, on average, the number of test cases used to find the first bug is reduced by 62.74% through ARTDL, compared with RT.","","","10.1109/ACCESS.2019.2962695","National Natural Science Foundation of China; Nova; Fundamental Research Funds for the Central Universities; Natural Science Foundation of Beijing Municipality; BUPT Excellent Ph.D. Students Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944083","Deep learning testing;adaptive random testing;distance metric;metamorphic testing","","","","","","52","CCBY","","","","IEEE","IEEE Journals"
"Smart Power Control for Quality-Driven Multi-User Video Transmissions: A Deep Reinforcement Learning Approach","T. Zhang; S. Mao","Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA","IEEE Access","","2020","8","","611","622","Device-to-device (D2D) communications have been regarded as a promising technology to meet the dramatically increasing video data demand in the 5G network. In this paper, we consider the power control problem in a multi-user video transmission system. Due to the non-convex nature of the optimization problem, it is challenging to obtain an optimal strategy. In addition, many existing solutions require instantaneous channel state information (CSI) for each link, which is hard to obtain in resource-limited wireless networks. We developed a multi-agent deep reinforcement learning-based power control method, where each agent adaptively controls its transmit power based on the observed local states. The proposed method aims to maximize the average quality of received videos of all users while satisfying the quality requirement of each user. After off-line training, the method can be distributedly implemented such that all the users can achieve their target state from any initial state. Compared with conventional optimization based approach, the proposed method is model-free, does not require CSI, and is scalable to large networks.","","","10.1109/ACCESS.2019.2961914","National Science Foundation; Auburn University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8939427","Multi-user video transmission;multi-agent deep reinforcement learning;power control;quality of experience","","","","","","46","CCBY","","","","IEEE","IEEE Journals"
"Minimum Throughput Maximization for Multi-UAV enabled WPCN: A Deep Reinforcement Learning Method","J. Tang; J. Song; J. Ou; J. Luo; X. Zhang; K. Wong","School of Electronic of Information Engineering, South China University of Technology, Guangzhou 510641, China.; School of Electronic of Information Engineering, South China University of Technology, Guangzhou 510641, China.; School of Electronic of Information Engineering, South China University of Technology, Guangzhou 510641, China.; School of Electronic of Information Engineering, South China University of Technology, Guangzhou 510641, China.; School of Electronic of Information Engineering, South China University of Technology, Guangzhou 510641, China.; Department of Electronic and Electrical Engineering, University College London, London WC1E 7JE, United Kingdom.","IEEE Access","","2020","PP","99","1","1","This paper investigates joint unmanned aerial vehicle (UAV) trajectory planning and time resource allocation for minimum throughput maximization in a multiple UAV-enabled wireless powered communication network (WPCN). In particular, the UAVs perform as base stations (BS) to broadcast energy signals in the downlink to charge IoT devices, while the IoT devices send their independent information in the uplink by utilizing the collected energy. The formulated throughput optimization problem which involves joint optimization of 3D path design and channel resource assignment with the constraint of flight speed of UAVs and uplink transmit power of IoT devices, is not convex and thus is extremely difficult to solve directly. We take advantage of the multi-agent deep Q learning (DQL) strategy and propose a novel algorithm to tackle this problem. Simulation results indicate that the proposed DQL-based algorithm significantly improve performance gain in terms of minimum throughput maximization compared with the conventional WPCN scheme.","","","10.1109/ACCESS.2020.2964042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950047","Unmanned aerial vehicle (UAV);wireless powered communication network (WPCN);Internet of Things (IoT);trajectory design;deep reinforcement learning (DRL)","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Maximal overlap discrete wavelet transform and deep learning for robust denoising and detection of power quality disturbance","F. Xiao; T. Lu; M. Wu; Q. Ai","College of Electrical Engineering, Beijing Jiao Tong University, Beijing 100044, People's Republic of China; John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138, USA; College of Electrical Engineering, Beijing Jiao Tong University, Beijing 100044, People's Republic of China; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai 200240, People's Republic of China","IET Generation, Transmission & Distribution","","2020","14","1","140","147","This study presents a new technique for power quality (PQ) disturbance detection. The technique focuses on voltage sags and interruptions that are related to various faults, i.e. transmission line, feeder, and transformer faults. A maximal overlap discrete wavelet transform-based PQ detection algorithm is proposed to provide accurate points of disturbance initiation and recovery. The proposed PQ detection algorithm is robust even without a detection threshold and independent of the sampling frequency of PQ recording. In consideration of the presence of noise conditions, the preprocessed PQ waveforms are converted into 2D binary vectors using space vector transformation. Then, an improved stacked sparse denoising autoencoder combined with supervised backpropagation training is proposed as a robust classifier. Results show that the proposed method is suitable for detecting various types of PQ disturbances and possesses high recognition accuracy despite insufficient training samples.","","","10.1049/iet-gtd.2019.1121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8948568","","","neural nets;signal denoising;power system harmonics;learning (artificial intelligence);power supply quality;power engineering computing;feature extraction;backpropagation;power system faults;discrete wavelet transforms","robust classifier;PQ disturbances;deep learning;robust denoising;power quality disturbance detection;voltage sags;transformer faults;maximal overlap discrete wavelet transform;PQ detection algorithm;disturbance initiation;detection threshold;PQ recording;preprocessed PQ waveforms;space vector transformation;improved stacked sparse denoising autoencoder","","","37","","","","","IET","IET Journals"
"CT Super-Resolution GAN Constrained by the Identical, Residual, and Cycle Learning Ensemble (GAN-CIRCLE)","C. You; G. Li; Y. Zhang; X. Zhang; H. Shan; M. Li; S. Ju; Z. Zhao; Z. Zhang; W. Cong; M. W. Vannier; P. K. Saha; E. A. Hoffman; G. Wang","Departments of Bioengineering and Electrical Engineering, Stanford University, Stanford, CA, USA; Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA; College of Computer Science, Sichuan University, Chengdu, China; Department of Electrical and Computer Engineering, University of Iowa, Iowa City, IA, USA; Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA; Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA; Department of Radiology, Jiangsu Key Laboratory of Molecular and Functional Imaging, Zhongda Hospital, Medical School, Southeast University, Nanjing, China; Department of Radiology, Jiangsu Key Laboratory of Molecular and Functional Imaging, Zhongda Hospital, Medical School, Southeast University, Nanjing, China; Department of Radiology, Wuxi No.2 People’s Hospital, Wuxi, China; Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA; Department of Radiology, University of Chicago, Chicago, IL, USA; Department of Electrical and Computer Engineering and Radiology, University of Iowa, Iowa City, IA, USA; Department of Radiology and Biomedical Engineering, University of Iowa, Iowa City, IA, USA; Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA","IEEE Transactions on Medical Imaging","","2020","39","1","188","203","In this paper, we present a semi-supervised deep learning approach to accurately recover high-resolution (HR) CT images from low-resolution (LR) counterparts. Specifically, with the generative adversarial network (GAN) as the building block, we enforce the cycle-consistency in terms of the Wasserstein distance to establish a nonlinear end-to-end mapping from noisy LR input images to denoised and deblurred HR outputs. We also include the joint constraints in the loss function to facilitate structural preservation. In this process, we incorporate deep convolutional neural network (CNN), residual learning, and network in network techniques for feature extraction and restoration. In contrast to the current trend of increasing network depth and complexity to boost the imaging performance, we apply a parallel  ${1}\times {1}$  CNN to compress the output of the hidden layer and optimize the number of layers and the number of filters for each convolutional layer. The quantitative and qualitative evaluative results demonstrate that our proposed model is accurate, efficient and robust for super-resolution (SR) image restoration from noisy LR input images. In particular, we validate our composite SR networks on three large-scale CT datasets, and obtain promising results as compared to the other state-of-the-art methods.","","","10.1109/TMI.2019.2922960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736838","Computed tomography (CT);super-resolution;noise reduction;deep learning;adversarial learning;residual learning","Computed tomography;Gallium nitride;Image resolution;Generative adversarial networks;Image reconstruction;Training","","","","","87","IEEE","","","","IEEE","IEEE Journals"
"Hierarchical Recurrent Deep Fusion Using Adaptive Clip Summarization for Sign Language Translation","D. Guo; W. Zhou; A. Li; H. Li; M. Wang","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; EEIS Department, University of Science and Technology of China, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; EEIS Department, University of Science and Technology of China, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China","IEEE Transactions on Image Processing","","2020","29","","1575","1590","Vision-based sign language translation (SLT) is a challenging task due to the complicated variations of facial expressions, gestures, and articulated poses involved in sign linguistics. As a weakly supervised sequence-to-sequence learning problem, in SLT there are usually no exact temporal boundaries of actions. To adequately explore temporal hints in videos, we propose a novel framework named Hierarchical deep Recurrent Fusion (HRF). Aiming at modeling discriminative action patterns, in HRF we design an adaptive temporal encoder to capture crucial RGB visemes and skeleton signees. Specifically, RGB visemes and skeleton signees are learned by the same scheme named Adaptive Clip Summarization (ACS), respectively. ACS consists of three key modules, i.e., variable-length clip mining, adaptive temporal pooling, and attention-aware weighting. Besides, based on unaligned action patterns (RGB visemes and skeleton signees), a query-adaptive decoding fusion is proposed to translate the target sentence. Extensive experiments demonstrate the effectiveness of the proposed HRF framework.","","","10.1109/TIP.2019.2941267","National Natural Science Foundation of China; National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846585","Sign language translation;hierarchical adaptive temporal network;adaptive clip summarization;temporal pooling;score fusion","Videos;Visualization;Semantics;Three-dimensional displays;Skeleton;Assistive technology;Gesture recognition","face recognition;image colour analysis;image fusion;image thinning;language translation;recurrent neural nets;sign language recognition;supervised learning;video signal processing","RGB visemes;weakly supervised sequence-to-sequence learning problem;hierarchical recurrent deep fusion;adaptive clip summarization;hierarchical deep recurrent fusion;HRF framework;query-adaptive decoding fusion;unaligned action patterns;adaptive temporal pooling;variable-length clip mining;ACS;skeleton signees;adaptive temporal encoder;discriminative action patterns;sign linguistics;articulated poses;facial expressions;SLT;vision-based sign language translation","","","62","","","","","IEEE","IEEE Journals"
"A Conclusive Analysis of the Finite-Time Behavior of the Discretized Pursuit Learning Automaton","X. Zhang; L. Jiao; B. J. Oommen; O. Granmo","Centre for Artificial Intelligence Research, University of Agder, Grimstad, Norway; Department of ICT, University of Agder, Grimstad, Norway; Department of ICT, University of Agder, Grimstad, Norway; Department of ICT, University of Agder, Grimstad, Norway","IEEE Transactions on Neural Networks and Learning Systems","","2020","31","1","284","294","This paper deals with the finite-time analysis (FTA) of learning automata (LA), which is a topic for which very little work has been reported in the literature. This is as opposed to the asymptotic steady-state analysis for which there are, probably, scores of papers. As clarified later, unarguably, the FTA of Markov chains, in general, and of LA, in particular, is far more complex than the asymptotic steady-state analysis. Such an FTA provides rigid bounds for the time required for the LA to attain to a given convergence accuracy. We concentrate on the FTA of the Discretized Pursuit Automaton (DPA), which is probably one of the fastest and most accurate reported LA. Although such an analysis was carried out many years ago, we record that the previous work is flawed. More specifically, in all brevity, the flaw lies in the wrongly “derived” monotonic behavior of the LA after a certain number of iterations. Rather, we claim that the property should be invoked is the submartingale property. This renders the proof to be much more involved and deep. In this paper, we rectify the flaw and reestablish the FTA based on such a submartingale phenomenon. More importantly, from the derived analysis, we are able to discover and clarify, for the first time, the underlying dilemma between the DPA’s exploitation and exploration properties. We also nontrivially confirm the existence of the optimal learning rate, which yields a better comprehension of the DPA itself.","","","10.1109/TNNLS.2019.2900639","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8671461","Discretized pursuit automaton (DPA);finite-time analysis (FTA);learning automaton;pursuit algorithms (PAs)","Convergence;Markov processes;Learning automata;Eigenvalues and eigenfunctions;Pursuit algorithms;Maximum likelihood estimation","","","","","37","IEEE","","","","IEEE","IEEE Journals"
"MRI Super-Resolution with Ensemble Learning and Complementary Priors","Q. Lyu; H. Shan; G. Wang","Department of Biomedical Engineering, Rensselaer Polytechnic Institute, 8024 Troy, New York United States (e-mail: lyuq@rpi.edu); Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, New York United States 12180 (e-mail: shanh@rpi.edu); Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, New York United States 12180 (e-mail: wangg6@rpi.edu)","IEEE Transactions on Computational Imaging","","2020","PP","99","1","1","Magnetic resonance imaging (MRI) is a widely used medical imaging modality. However, due to the limitations in hardware, scan time, and throughput, it is often clinically challenging to obtain high-quality MR images. The super-resolution approach is potentially promising to improve MR image quality without any hardware upgrade. In this paper, we propose an ensemble learning and deep learning framework for MR image super-resolution. In our study, we first enlarged low resolution images using 5 commonly used super-resolution algorithms and obtained differentially enlarged image datasets with complementary priors. Then, a generative adversarial network (GAN) is trained with each dataset to generate super-resolution MR images. Finally, another GAN is used for ensemble learning that synergizes the outputs of GANs into the final MR super-resolution images. According to our results, the ensemble learning results outperform any single GAN output component. Compared with some state-of-the-art deep learning-based super-resolution methods, our approach is advantageous in suppressing artifacts and keeping more image details.","","","10.1109/TCI.2020.2964201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950304","Deep learning;ensemble learning;generative adversarial network (GAN);magnetic resonance imaging (MRI);super-resolution","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Prediction of Taxi Destinations Using a Novel Data Embedding Method and Ensemble Learning","X. Zhang; Z. Zhao; Y. Zheng; J. Li","Advanced Analytics Institute, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; Advanced Analytics Institute, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; Advanced Analytics Institute, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; Advanced Analytics Institute, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Intelligent Transportation Systems","","2020","21","1","68","78","The accurate and timely destination prediction of taxis is of great importance for location-based service applications. Over the last few decades, the popularization of vehicle navigation systems has brought the era of big data to the taxi industry. Existing destination prediction approaches are mainly based on various Markov chain models or trip matching ideas, which require geographical information and may encounter the problem of data sparsity. Other machine learning prediction models are still unsatisfactory in providing favorable results. In this paper, first, we propose use of a novel and efficient data embedding method for time-related feature pre-processing. The key idea behind this is to embed the data into a two-dimensional space before feature selection. Second, we propose use of a novel data-driven ensemble learning approach for destination prediction. This approach combines the respective superiorities of support vector regression and deep learning at different segments of the whole trajectory. Our experiments are conducted on two real data sets to demonstrate that the proposed ensemble learning model can get superior performance for taxi destination prediction. Comparisons also confirm the effectiveness of the proposed data embedding method in the deep learning model.","","","10.1109/TITS.2018.2888587","China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8604118","Taxi;destination prediction;support vector regression (SVR);deep learning;ensemble learning","Public transportation;Trajectory;Hidden Markov models;Predictive models;Markov processes;Global Positioning System","","","","1","39","IEEE","","","","IEEE","IEEE Journals"
"Towards Automated Infographic Design: Deep Learning-based Auto-Extraction of Extensible Timeline","Z. Chen; Y. Wang; Q. Wang; Y. Wang; H. Qu","Hong Kong University of Science and Technology; Microsoft Research; Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; Hong Kong University of Science and Technology","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","917","926","Designers need to consider not only perceptual effectiveness but also visual styles when creating an infographic. This process can be difficult and time consuming for professional designers, not to mention non-expert users, leading to the demand for automated infographics design. As a first step, we focus on timeline infographics, which have been widely used for centuries. We contribute an end-to-end approach that automatically extracts an extensible timeline template from a bitmap image. Our approach adopts a deconstruction and reconstruction paradigm. At the deconstruction stage, we propose a multi-task deep neural network that simultaneously parses two kinds of information from a bitmap timeline: 1) the global information, i.e., the representation, scale, layout, and orientation of the timeline, and 2) the local information, i.e., the location, category, and pixels of each visual element on the timeline. At the reconstruction stage, we propose a pipeline with three techniques, i.e., Non-Maximum Merging, Redundancy Recover, and DL GrabCut, to extract an extensible template from the infographic, by utilizing the deconstruction results. To evaluate the effectiveness of our approach, we synthesize a timeline dataset (4296 images) and collect a real-world timeline dataset (393 images) from the Internet. We first report quantitative evaluation results of our approach over the two datasets. Then, we present examples of automatically extracted templates and timelines automatically generated based on these templates to qualitatively demonstrate the performance. The results confirm that our approach can effectively extract extensible templates from real-world timeline infographics.","","","10.1109/TVCG.2019.2934810","MSRA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807266","Automated Infographic Design;Deep Learning-based Approach;Timeline Infographics;Multi-task Model","Visualization;Data visualization;Data mining;Image reconstruction;Pipelines;Image coding;Object detection","","","","","56","","","","","IEEE","IEEE Journals"
"Intelligent Detection for Tunnel Shotcrete Spray Using Deep Learning and LiDAR","L. Chun-Lei; S. Hao; L. Chun-Lai; L. Jin-Yang","College of Mechanical and Electrical Engineering, Central South University, Changsha, China; College of Mechanical and Electrical Engineering, Central South University, Changsha, China; College of Mechanical and Electrical Engineering, Central South University, Changsha, China; College of Mechanical and Electrical Engineering, Central South University, Changsha, China","IEEE Access","","2020","8","","1755","1766","Shotcrete spray is an indispensable process in tunnel construction. At present, the construction of tunnels in China is mainly depend on labor or mobile concrete sprayer, which has lots problems like time-consuming, low precision, and labor intensive. An intelligent detection method for tunnel shotcrete spraying is proposed in this article. There are two main issues need to be solved, one is the modeling of tunnel in real-time to monitor the thickness of shotcrete and other is the detection of spraying area in the tunnel. The LiDAR can obtain a 3D model of tunnel after performing necessary preprocess on it in real-time. On the other hand, the spraying areas are usually divided by arches in the tunnel, so we can detect the position of arches to determine the spraying areas. Inspired by the YOLO algorithm, we proposed a novel neural network structure to detect the approximate bounding boxes of the arches and a line-detection algorithm is used to determine the final positions of the spraying area in the image. The size of the weight file of our neural network is only 2.57 MB after the use of some deep compression tricks, which means our model is device friendly. After that, the object detection results in the image will be projected to the point cloud data. The experimental results suggest that our method performed well in the detection for tunnel shotcrete spraying, and the mAP for spraying area detection was found to be 91.4%.","","","10.1109/ACCESS.2019.2962496","Gengli Engineering Machinery Equipment co., Ltd., Henan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943432","3D point cloud;deep learning;object detection;tunnel shotcrete","","","","","","56","CCBY","","","","IEEE","IEEE Journals"
"D2N4: A Discriminative Deep Nearest Neighbor Neural Network for Few-Shot Space Target Recognition","X. Yang; X. Nan; B. Song","State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an 710071, China.; State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an 710071, China.; State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an 710071, China (e-mail: bsong@mail.xidian.edu.cn).","IEEE Transactions on Geoscience and Remote Sensing","","2020","PP","99","1","10","With the rapid development of space exploration worldwide, there is a sudden increase in the type and number of spacecraft, thus leading to a more complex space environment. To enhance the ability of space situational awareness, the most important step is to effectively recognize space targets of interests from various spacecraft and debris. Traditional space target recognition approaches adopt manual feature extraction with limited data, resulting in a semantic gap between low-level visual features and high-level semantic representation. Although deep learning models alleviate this problem with a unified framework for combined learning feature extraction and classification simultaneously, it is easy to overfit and leads to poor generalization results when faced with a situation of small examples. To address these issues, we present an end-to-end few-shot deep learning framework for space target recognition, i.e., discriminative deep nearest neighbor neural network (D2N4). Our D2N4 aims to improve the discriminability of the deeply learned features with mainly two strategies. On the one hand, we add an intraclass compactness principle by introducing center loss to efficiently pull deep features of the same classes to their centers and, thus overcoming significant intraclass variation of space target. On the other hand, we introduce the global pooling information for each deep local descriptor to reduce interference from local background noise, thus enhancing the model robustness. In practice, under the joint supervision of soft-max loss and center loss, the deep embedding module and image-to-class metric module are trained in an end-to-end way. Extensive experiments on the space target data set BUAA-SID-share1.0 demonstrate that our simple and effective approach outperforms previous space target recognition methods and is more efficient than recent few-shot approaches. In addition, the proposed framework is equally applicable to natural images and achieves state-of-the-art performance on data sets CUB-200-2010, Stanford Dogs, and Stanford Cars.","","","10.1109/TGRS.2019.2959838","National Natural Science Foundation of China; Fundamental Research Funds of Ministry of Education and China Mobile; National Natural Science Foundation of Shaanxi Province; Fundamental Research Funds for the Central Universities; Young Talent fund of University Association for Science and Technology in Shaanxi China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949701","Discriminative feature;few-shot learning;metric learning;space target recognition;transfer learning.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Exploring Multi-Level Attention and Semantic Relationship for Remote Sensing Image Captioning","Z. Yuan; X. Li; Q. Wang","School of Computer Science, Center for Optical Imagery Analysis and Learning, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Center for Optical Imagery Analysis and Learning, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Center for Optical Imagery Analysis and Learning, Northwestern Polytechnical University, Xi’an, China","IEEE Access","","2020","8","","2608","2620","Remote sensing image captioning, which aims to understand high-level semantic information and interactions of different ground objects, is a new emerging research topic in recent years. Though image captioning has developed rapidly with convolutional neural networks (CNNs) and recurrent neural networks (RNNs), the image captioning task for remote sensing images still suffers from two main limitations. One limitation is that the scales of objects in remote sensing images vary dramatically, which makes it difficult to obtain an effective image representation. Another limitation is that the visual relationship in remote sensing images is still underused, which should have great potential to improve the final performance. In order to deal with these two limitations, an effective framework for captioning the remote sensing image is proposed in this paper. The framework is based on multi-level attention and multi-label attribute graph convolution. Specifically, the proposed multi-level attention module can adaptively focus not only on specific spatial features, but also on features of specific scales. Moreover, the designed attribute graph convolution module can employ the attribute-graph to learn more effective attribute features for image captioning. Extensive experiments are conducted and the proposed method achieves superior performance on UCM-captions, Sydney-captions and RSICD dataset.","","","10.1109/ACCESS.2019.2962195","National Key Research and Development Program of China; National Natural Science Foundation of China; Project of Special Zone for National Defense Science and Technology Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943170","Remote sensing image;image captioning;deep learning;graph convolutional networks (GCNs);semantic understanding","","","","","","51","CCBY","","","","IEEE","IEEE Journals"
"Deep Residual Correction Network for Partial Domain Adaptation","S. Li; C. H. Liu; Q. Lin; Q. Wen; L. Su; G. Huang; Z. Ding","School of Computer Science and Technology, Beijing Institute of Technology, 47833 Beijing, Beijing China (e-mail: 6120180107@bit.edu.cn); School of Software, Beijing Institute of Technology, 47833 Beijing, Beijing China 100081 (e-mail: liuchi02@gmail.com); School of Computer Science and Technology, Beijing Institute of Technology, 47833 Beijing, Beijing China (e-mail: linqiuxia1995@126.com); School of Computer Science and Technology, Beijing Institute of Technology, 47833 Beijing, Beijing China (e-mail: wenqijay@gmail.com); School of Computer Science and Technology, Beijing Institute of Technology, 47833 Beijing, Beijing China (e-mail: sulimin@bit.edu.cn); Department of Computer Science, Cornell University, 5922 Ithaca, New York United States 14853-0001 (e-mail: gaohuang@tsinghua.edu.cn); Department of Computer, Information and Technology, Indiana University-Purdue University Indianapolis, Indianapolis, Indiana United States (e-mail: zd2@iu.edu)","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2020","PP","99","1","1","Deep domain adaptation methods have achieved appealing performance by learning transferable representations from a well-labeled source domain to a different but related unlabeled target domain. Most existing works assume source and target data share the identical label space, which is often difficult to be satisfied in many real-world applications. There is a more practical scenario called partial domain adaptation, where the target label space is a subset of the source label space. In this case, reinforcing the positive effects of the most relevant source subclasses and reducing the negative impacts of irrelevant source subclasses are crucial. This paper proposes an efficiently-implemented Deep Residual Correction Network by plugging one residual block into the source network, which effectively enhances the adaptation from source to target and explicitly weakens the influence from the irrelevant source classes. Moreover, we design a weighted class-wise domain alignment loss to couple two domains by matching the feature distributions of shared classes between source and target. Comprehensive experiments on partial, traditional and fine-grained cross-domain visual recognition demonstrate that DRCN is superior to the competitive deep domain adaptation approaches.","","","10.1109/TPAMI.2020.2964173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951442","Deep Transfer Leaning;Partial Domain adaptation;Maximum Mean Discrepancy;Fine-grained Visual Recognition","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Visually Guided Picking Control of an Omnidirectional Mobile Manipulator Based on End-to-End Multi-Task Imitation Learning","C. Tsai; Y. Chou; C. Wong; Y. Lai; C. Huang","Department of Electrical and Computer Engineering, Tamkang University, New Taipei City, Taiwan; Department of Electrical and Computer Engineering, Tamkang University, New Taipei City, Taiwan; Department of Electrical and Computer Engineering, Tamkang University, New Taipei City, Taiwan; Department of Electrical and Computer Engineering, Tamkang University, New Taipei City, Taiwan; Department of Electrical and Computer Engineering, Tamkang University, New Taipei City, Taiwan","IEEE Access","","2020","8","","1882","1891","In this paper, a novel deep convolutional neural network (CNN) based high-level multi-task control architecture is proposed to address the visual guide-and-pick control problem of an omnidirectional mobile manipulator platform based on deep learning technology. The proposed mobile manipulator control system only uses a stereo camera as a sensing device to accomplish the visual guide-and-pick control task. After the stereo camera captures the stereo image of the scene, the proposed CNN-based high-level multi-task controller can directly predict the best motion guidance and picking action of the omnidirectional mobile manipulator by using the captured stereo image. In order to collect the training dataset, we manually controlled the mobile manipulator to navigate in an indoor environment for approaching and picking up an object-of-interest (OOI). In the meantime, we recorded all of the captured stereo images and the corresponding control commands of the robot during the manual teaching stage. In the training stage, we employed the end-to-end multi-task imitation learning technique to train the proposed CNN model by learning the desired motion and picking control strategies from prior expert demonstrations for visually guiding the mobile platform and then visually picking up the OOI. Experimental results show that the proposed visually guided picking control system achieves a picking success rate of about 78.2% on average.","","","10.1109/ACCESS.2019.2962335","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943173","Omnidirectional mobile manipulator;visually guided picking control;deep learning;multi-task imitation learning;end-to-end control","","","","","","35","CCBY","","","","IEEE","IEEE Journals"
"NetFlow Monitoring and Cyberattack Detection Using Deep Learning with Ceph","C. Yang; J. Liu; E. Kristiani; M. Liu; I. You; G. Pau","Department of Computer Science, Tunghai University, No.1727, Sec.4, Taiwan Boulevard, Xitun District, Taichung City, Taiwan (R.O.C.) 40704.; Department of Computer Science, Tunghai University, No.1727, Sec.4, Taiwan Boulevard, Xitun District, Taichung City, Taiwan (R.O.C.) 40704.; Department of Industrial Engineering and Enterprise Information, Tunghai University, Taichung City, Taiwan, (R.O.C.) 40704 and Department of Informatics, Krida Wacana Christian University, Jakarta, Indonesia 11470.; Department of Computer Science, Tunghai University, No.1727, Sec.4, Taiwan Boulevard, Xitun District, Taichung City, Taiwan (R.O.C.) 40704.; Department of Information Security Engineering Soonchunhyang University 646 Eupnaeri, Sinchangmyeon Asansi, Chungcheongnamdo, Republic of Korea 31538. (e-mail: ilsunu@gmail.com); Faculty of Engineering and Architecture, Kore University of Enna, 94100 Enna - Italy.","IEEE Access","","2020","PP","99","1","1","Figuring the network’s hidden abnormal behavior can reduce network vulnerability. This paper presents a detailed architecture in which the collected log data of the network can be processed and analyzed. We process and integrate on-campus network information from every router and store the integrated NetFlow log data. Ceph is used as an open-source distributed storage platform that offers high efficiency, high reliability, scalability, and preliminary preprocessing of raw data with Python, removing redundant areas and unification. In the subanalysis, we discover the anomaly event and absolute flow by three times of standard deviation rule. Keras has been used to classify in-time data collected via a cyber-attack and to construct an automatic identifier template through the Recurring Neural Network (RNN) test. The identification accuracy of the optimization model is around 98% in attack detection. Finally, in the MySQL server, the results of the real-time evaluation can be obtained, and the results of the assessment can be displayed via ECharts.","","","10.1109/ACCESS.2019.2963716","Soonchunhyang University; The Ministry of Science and Technology Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949457","Data Storage;Ceph;Deep Learning;Cyberattack;NetFlow Log","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Learning Sparse and Identity-Preserved Hidden Attributes for Person Re-Identification","Z. Wang; J. Jiang; Y. Wu; M. Ye; X. Bai; S. Satoh","Digital Content and Media Sciences Research Division, National Institute of Informatics, Tokyo, Japan; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; International Collaborative Laboratory for Robotics Vision, Institute for Research Initiatives, Nara Institute of Science and Technology, Nara, Japan; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Digital Content and Media Sciences Research Division, National Institute of Informatics, Tokyo, Japan","IEEE Transactions on Image Processing","","2020","29","1","2013","2025","Person re-identification (Re-ID) aims at matching person images captured in non-overlapping camera views. To represent person appearance, low-level visual features are sensitive to environmental changes, while high-level semantic attributes, such as “short-hair” or “long-hair”, are relatively stable. Hence, researches have started to design semantic attributes to reduce the visual ambiguity. However, to train a prediction model for semantic attributes, it requires plenty of annotations, which are hard to obtain in practical large-scale applications. To alleviate the reliance on annotation efforts, we propose to incrementally generate Deep Hidden Attribute (DHA) based on baseline deep network for newly uncovered annotations. In particular, we propose an auto-encoder model that can be plugged into any deep network to mine latent information in an unsupervised manner. To optimize the effectiveness of DHA, we reform the auto-encoder model with additional orthogonal generation module, along with identity-preserving and sparsity constraints. 1) Orthogonally generating: In order to make DHAs different from each other, Singular Vector Decomposition (SVD) is introduced to generate DHAs orthogonally. 2) Identity-preserving constraint: The generated DHAs should be distinct for telling different persons, so we associate DHAs with person identities. 3) Sparsity constraint: To enhance the discriminability of DHAs, we also introduce the sparsity constraint to restrict the number of effective DHAs for each person. Experiments conducted on public datasets have validated the effectiveness of the proposed network. On two large-scale datasets, i.e., Market-1501 and DukeMTMC-reID, the proposed method outperforms the state-of-the-art methods.","","","10.1109/TIP.2019.2946975","JST CREST; Grant-in-Aid for JSPS Fellows; Microsoft Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8874954","Person re-identification;attribute learning;generation;discrimination","Semantics;Deep learning;Visualization;Feature extraction;Image reconstruction;Clothing;Training","","","","","59","CCBY","","","","IEEE","IEEE Journals"
"Stroke Sequence-Dependent Deep Convolutional Neural Network for Online Handwritten Chinese Character Recognition","X. Liu; B. Hu; Q. Chen; X. Wu; J. You","Shenzhen Chinese Calligraphy Digital Simulation Engineering Laboratory, Harbin Institute of Technology, Shenzhen, China, and also with the Peng Cheng Laboratory, Shenzhen 518055, China.; Shenzhen Chinese Calligraphy Digital Simulation Engineering Laboratory, Harbin Institute of Technology, Shenzhen, China, and also with the Peng Cheng Laboratory, Shenzhen 518055, China.; Shenzhen Chinese Calligraphy Digital Simulation Engineering Laboratory, Harbin Institute of Technology, Shenzhen, China, and also with the Peng Cheng Laboratory, Shenzhen 518055, China (e-mail: qingcai.chen@hit.edu.cn).; Shenzhen Chinese Calligraphy Digital Simulation Engineering Laboratory, Harbin Institute of Technology, Shenzhen, China, and also with the Peng Cheng Laboratory, Shenzhen 518055, China.; Shenzhen Chinese Calligraphy Digital Simulation Engineering Laboratory, Harbin Institute of Technology, Shenzhen, China, and also with the Peng Cheng Laboratory, Shenzhen 518055, China.","IEEE Transactions on Neural Networks and Learning Systems","","2020","PP","99","1","12","We propose a novel model, called stroke sequence-dependent deep convolutional neural network (SSDCNN), which uses the stroke sequence information and eight-directional features of Chinese characters for online handwritten Chinese character recognition (OLHCCR). SSDCNN learns the representation of OLHCCs by incorporating the natural sequence information of the strokes. Furthermore, it naturally incorporates the eight-directional features. First, SSDCNN inputs the stroke sequence and transforms it into stacks of feature maps following the writing order of the strokes. Second, the fixed-length, stroke sequence-dependent representations of OLHCC are derived through convolutional, residual, and max-pooling operations. Third, the stroke sequence-dependent representation is combined with the eight-directional features via a number of fully connected neural network layers. Finally, the Chinese characters are recognized using a softmax classifier. The SSDCNN is trained in two stages: 1) the whole architecture is pretrained using the training data until the performance converges to an acceptable degree. 2) The stroke sequence-dependent representation is combined with the eight-directional features by a fully connected neural network and a softmax layer for further training. The model was experimentally evaluated on the OLHCCR competition tasks of International Conference on Document Analysis and Recognition (ICDAR) 2013. The recognition error was a maximum 58.28% lower in SSDCNN than in a model using the eight-directional features alone (5.13% versus 2.14%). Owing to its high accuracy (97.86%), the proposed SSDCNN reduced the recognition error by approximately 18.0% as compared with that of the winning system in the ICDAR 2013 competition. SSDCNN integrated with an adaptation mechanism, called the SSDCNN+Adapt model, and reached a new state-of-the-art (SOTA) standard with an accuracy of 97.94%. The SSDCNN exploits the stroke sequence information to learn high-quality OLHCC representations. Moreover, the learned representation and the classical eight-directional features complement each other within the SSDCNN architecture.","","","10.1109/TNNLS.2019.2956965","National Natural Science Foundation of China; Strategic Emerging Industry Development Special Funds of Shenzhen; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949703","Deep convolutional neural;online handwritten Chinese character recognition (OLHCCR);stroke sequence-dependent representation.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"HSI-BERT: Hyperspectral Image Classification Using the Bidirectional Encoder Representation From Transformers","J. He; L. Zhao; H. Yang; M. Zhang; W. Li","College of Mathematics and Physics, Beijing University of Chemical Technology, Beijing, China; College of Mathematics and Physics, Beijing University of Chemical Technology, Beijing, China; Center of Information, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2020","58","1","165","178","Deep learning methods have been widely used in hyperspectral image classification and have achieved state-of-the-art performance. Nonetheless, the existing deep learning methods are restricted by a limited receptive field, inflexibility, and difficult generalization problems in hyperspectral image classification. To solve these problems, we propose HSI-BERT, where BERT stands for bidirectional encoder representations from transformers and HSI stands for hyperspectral imagery. The proposed HSI-BERT has a global receptive field that captures the global dependence among pixels regardless of their spatial distance. HSI-BERT is very flexible and enables the flexible and dynamic input regions. Furthermore, HSI-BERT has good generalization ability because the jointly trained HSI-BERT can be generalized from regions with different shapes without retraining. HSI-BERT is primarily built on a multihead self-attention (MHSA) mechanism in an MHSA layer. Moreover, several attentions are learned by different heads, and each head of the MHSA layer encodes the semantic context-aware representation to obtain discriminative features. Because all head-encoded features are merged, the resulting features exhibit spatial–spectral information that is essential for accurate pixel-level classification. Quantitative and qualitative results demonstrate that HSI-BERT outperforms any other CNN-based model in terms of both classification accuracy and computational time and achieves state-of-the-art performance on three widely used hyperspectral image data sets.","","","10.1109/TGRS.2019.2934760","National Natural Science Foundation of China; Natural Science Foundation of Beijing Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824217","Deep learning;hyperspectral image;image classification;multihead self-attention (MHSA);pattern recognition","Feature extraction;Bit error rate;Hyperspectral imaging;Shape;Deep learning;Kernel","","","","","45","IEEE","","","","IEEE","IEEE Journals"
"Multiple Cycle-in-Cycle Generative Adversarial Networks for Unsupervised Image Super-Resolution","Y. Zhang; S. Liu; C. Dong; X. Zhang; Y. Yuan","Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Department of Automation, Tsinghua University, Beijing, China; Shenzhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; School of Computer Science and Technology, University of the Chinese Academy of Sciences, Beijing, China; Guangdong Key Laboratory of Intelligent Information Processing and the Shenzhen Key Laboratory of Media Security, College of Information Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Image Processing","","2020","29","","1101","1112","With the help of convolutional neural networks (CNN), the single image super-resolution problem has been widely studied. Most of these CNN based methods focus on learning a model to map a low-resolution (LR) image to a highresolution (HR) image, where the LR image is downsampled from the HR image with a known model. However, in a more general case when the process of the down-sampling is unknown and the LR input is degraded by noises and blurring, it is difficult to acquire the LR and HR image pairs for traditional supervised learning. Inspired by the recent unsupervised imagestyle translation applications using unpaired data, we propose a multiple Cycle-in-Cycle network structure to deal with the more general case using multiple generative adversarial networks (GAN) as the basis components. The first network cycle aims at mapping the noisy and blurry LR input to a noise-free LR space, then a new cycle with a well-trained x2 network model is orderly introduced to super-resolve the intermediate output of the former cycle. The number of total cycles depends on the different up-sampling factors (x2, x4, x8). Finally, all modules are trained in an end-to-end manner to get the desired HR output. Quantitative indexes and qualitative results show that our proposed method achieves comparable performance with the state-of-the-art supervised models.","","","10.1109/TIP.2019.2938347","National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; Shenzhen Fundamental Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825849","Super resolution;unsupervised learning;generative adversarial networks","Training;Kernel;Degradation;Interpolation;Deep learning","convolutional neural nets;image resolution;image restoration;learning (artificial intelligence)","image blurring;supervised learning;multiple generative adversarial networks;noise-free LR space;cycle-in-cycle generative adversarial networks;unsupervised image super-resolution;convolutional neural networks;multiple cycle-in-cycle network structure","","","51","","","","","IEEE","IEEE Journals"
"Semi-Supervised Encrypted Traffic Classification With Deep Convolutional Generative Adversarial Networks","A. S. Iliyasu; H. Deng","Department of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Department of Computer Science and Engineering, South China University of Technology, Guangzhou, China","IEEE Access","","2020","8","","118","126","Network traffic classification serves as a building block for important tasks such as security and quality of service management. The field has been studied for a long time, with many techniques such as classical machine learning and deep learning methods currently available. However, the emergence of stronger encryption protocols has led to the rise of new challenges. One of the challenges is capturing and labeling a large amount of encrypted traffic data especially for training deep learning classifiers, as current techniques rely on deep packet inspection tools (DPI) which perform poorly on encrypted traffic. In this paper, we propose a semi-supervised learning approach using Deep Convolutional Generative Adversarial Network (DCGAN). The basic idea is to utilize the samples generated by DCGAN generators as well as unlabeled data to improve the performance of a classifier trained on a few labeled samples. Thus, alleviating the difficulties associated with large dataset collecting and labeling. To demonstrate the efficacy of our approach, we evaluated our model using a self-collected dataset of the recently established QUIC protocol as well as publicly available ISCX VPN-NonVPN dataset. Our approach is able to achieve 89% and 78% accuracy with a very small number of labeled samples (just 10% of the dataset) on both QUIC and ISCX VPN-NonVPN datasets respectively.","","","10.1109/ACCESS.2019.2962106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941140","Deep convolutional generative adversarial network;encrypted traffic classification;semi-supervised learning","","","","","","23","CCBY","","","","IEEE","IEEE Journals"
"Object Re-identification via Joint Quadruple Decorrelation Directional Deep Networks in Smart Transportation","J. Zhu; J. Huang; H. Zeng; X. Ye; B. Li; Z. Lei; L. Zheng","College of Engineering, Huaqiao University, Quanzhou, 362021, China.; Shanghai Institute of Micro-system and Information Technology, Chinese Academy of Sciences, China.; College of Information Science and Engineering, Huaqiao University, Xiamen, 361021, China.; Shanghai Institute of Micro-system and Information Technology, Chinese Academy of Sciences, China.; Shanghai Institute of Micro-system and Information Technology, Chinese Academy of Sciences, China.; Institute of Automation, Chinese Academy of Sciences, China.; College of Engineering, Huaqiao University, Quanzhou, 362021, China.","IEEE Internet of Things Journal","","2020","PP","99","1","1","Object re-identification with the goal of matching pedestrian or vehicle images captured from different camera viewpoints is of considerable significance to public security. Quadruple directional deep learning features (QD-DLFs) can comprehensively describe object images. However, the correlation among QD-DLFs is an unavoidable problem, since QD-DLFs are learned with quadruple independent directional deep networks driven with the same training data, and each network holds the same basic deep feature learning architecture (BDFLA). The correlation among QD-DLFs is harmful to the complementarity of QD-DLFs, restricting the object re-identification performance. For that, we propose joint quadruple decorrelation directional deep networks (JQDNs) to reduce the correlation among the learned QD-DLFs. In order to jointly train JQDNs, besides the softmax loss functions, a parameter correlation cost function is proposed to indirectly reduce the correlation among QD-DLFs by enlarging the dissimilarity among the parameters of JQDNs. Extensive experiments on three publicly available large-scale datasets demonstrate that the proposed JQDNs approach is superior to multiple state-of-the-art object re-identification methods.","","","10.1109/JIOT.2020.2963996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950171","Smart transportation;Deep learning;Vehicle re-identification;Pedestrian re-identification.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Precious Metal Price Prediction Based on Deep Regularization Self-Attention Regression","J. Zhou; Z. He; Y. N. Song; H. Wang; X. Yang; W. Lian; H. Dai","Faculty of Information Technology, Macau University of Science and Technology, Macao, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; School of Business, Macau University of Science and Technology, Macao, China; Department of Computer Science, Norwegian University of Science and Technology, Gjøvik, Norway; Institute of Modern Economics and Management, Zhejiang Yuexiu University of Foreign Languages, Shaoxing, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; Faculty of Information Technology, Macau University of Science and Technology, Macao, China","IEEE Access","","2020","8","","2178","2187","It is non-trivial to predict the prices of precious metals since a number of factors can affect the fluctuations of precious metal prices. Either parametric models or machine learning models cannot accurately forecast the precious metal prices. Though deep learning approaches show their strengths in extracting key features from complicated data, they have the limitations of learning localization and losing some temporal and spatial features. The recent advances in attention mechanisms bring the opportunities to overcome the limitation of deep learning models. In this paper, we originally propose a Regularization Self-Attention Regression Model for precious metal price prediction. In particular, the proposed RSAR model consists of convolutional neural network (CNN) component and Long Short-Term Memory Neural Networks (LSTM) component. Integrating with self-attention mechanism, this model can extract both spatial and temporal features from precious metal price data. Meanwhile, the proper configuration of regularization functions can also lead to the further performance improvement. Extensive experiments on realistic precious metal price dataset show that our proposed approach outperforms other conventional machine learning and deep learning methods.","","","10.1109/ACCESS.2019.2962202","National Natural Science Foundation of China; NSFC-Guangdong Joint Fund; Science and Technology Planning Project of Guangdong Province; Science and Technology Program of Guangzhou; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943215","Long short-term memory;convolutional neural network;attention mechanism;financial data analysis;deep learning","","","","","","35","CCBY","","","","IEEE","IEEE Journals"
"A cloud-based framework for machine learning workloads and applications","Á. L. García; J. M. De Lucas; M. Antonacci; W. Z. Castell; M. David; M. Hardt; L. L. Iglesias; G. Moltó; M. Plociennik; V. Tran; A. S. Alic; M. Caballer; I. C. Plasencia; A. Costantini; S. Dlugolinsky; D. C. Duma; G. Donvito; J. Gomes; I. H. Cacha; K. Ito; V. Y. Kozlov; G. Nguyen; P. O. Fernández; Z. Šustr; P. Wolniewicz","IFCA (CSIC-UC), Santander, Spain.; IFCA (CSIC-UC), Santander, Spain.; INFN Bari, Bari, Italy.; Helmholtz Zentrum München, Deutsches Forschungszentrum für Gesundheit and Department of Mathematics, Technische Universität München, Germany.; Laboratory of Instrumentation and Experimental Particle Physics (LIP), Lisbon, Portugal.; Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany.; IFCA (CSIC-UC), Santander, Spain.; Instituto de Instrumentación para Imagen Molecular (I3M), CSIC - Universitat Politècnica de València, Valencia, Spain.; INFN CNAF, Bologna, Italy.; Institute of Informatics, Slovak Academy of Sciences (IISAS), Bratislava, Slovakia.; Instituto de Instrumentación para Imagen Molecular (I3M), CSIC - Universitat Politècnica de València, Valencia, Spain.; Instituto de Instrumentación para Imagen Molecular (I3M), CSIC - Universitat Politècnica de València, Valencia, Spain.; IFCA (CSIC-UC), Santander, Spain.; INFN CNAF, Bologna, Italy.; Institute of Informatics, Slovak Academy of Sciences (IISAS), Bratislava, Slovakia.; INFN CNAF, Bologna, Italy.; INFN Bari, Bari, Italy.; Laboratory of Instrumentation and Experimental Particle Physics (LIP), Lisbon, Portugal.; IFCA (CSIC-UC), Santander, Spain.; Helmholtz Zentrum München, Deutsches Forschungszentrum für Gesundheit.; Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany.; Institute of Informatics, Slovak Academy of Sciences (IISAS), Bratislava, Slovakia.; IFCA (CSIC-UC), Santander, Spain.; CESNET, Prague, Czech Republic.; INFN CNAF, Bologna, Italy.","IEEE Access","","2020","PP","99","1","1","In this paper we propose a distributed architecture to provide machine learning practitioners with a set of tools and cloud services that cover the whole machine learning development cycle: ranging from the models creation, training, validation and testing to the models serving as a service, sharing and publication. In such respect, the DEEP-Hybrid-DataCloud framework allows transparent access to existing e-Infrastructures, effectively exploiting distributed resources for the most compute-intensive tasks coming from the machine learning development cycle. Moreover, it provides scientists with a set of Cloud-oriented services to make their models publicly available, by adopting a serverless architecture and a DevOps approach, allowing an easy share, publish and deploy of the developed models.","","","10.1109/ACCESS.2020.2964386","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950411","Cloud Computing;Computers and information processing;Distributed Computing;Machine Learning;Deep Learning;Serverless architectures","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"A Novel Application of Deep Belief Networks in Learning Partial Discharge Patterns for Classifying Corona, Surface, and Internal Discharges","M. Karimi; M. Majidi; H. MirSaeedi; M. M. Arefi; M. Oskuoee","Department of Control Engineering, School of Electrical Engineering, Shahid Beheshti University, Tehran, Iran; Department of Electrical and Biomedical Engineering, University of Nevada, Reno, Reno, NV, USA; Department of Power Engineering, School of Electrical and Computer Engineering, University of Tehran, Tehran, Iran; Department of Power and Control Engineering, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran; High Voltage Department, Niroo Research Institute, Tehran, Iran","IEEE Transactions on Industrial Electronics","","2020","67","4","3277","3287","This paper introduces a new application of deep belief network (DBN) as an emerging artificial neural network for recognizing and classifying different partial discharge (PD) patterns. Phase resolved PD (PRPD) technique with different window intervals is used to manipulate three PD types, including corona, surface, and internal discharges measured in a high-voltage lab. Four approaches are proposed for extracting features from the raw measured data. In the first approach, the DBN is used as both a feature extractor and a PD classifier. The other three approaches extract discriminatory features using statistical and vector-norm-based operators to train a DBN classifier. The impact of the various phase windows through the PRPD method on the performance of the trained classifier is evaluated to obtain the best window interval. It is shown that the deep architectures are capable of learning important distinguishable features from PD data without any data preprocessing. This eliminates time-consuming feature extraction processes that produce the handcrafted features. Based on a comparison analysis, when the input data are corrupted by noise levels or no feature extraction technique is used to preprocess the data, the proposed approach outperforms other techniques, such as artificial neural networks, fuzzy logic classifiers, and support vector machines.","","","10.1109/TIE.2019.2908580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8683990","Deep belief networks (DBNs);partial discharge (PD);pattern recognition;statistical moments;vector norms","","","","","","43","IEEE","","","","IEEE","IEEE Journals"
"Hybrid-Model-Based Intelligent Optimization of Ironmaking Process","H. Zhou; H. Zhang; C. Yang","State Key Laboratory of Industrial Control Technology, College of Control Science and Engineering, Zhejiang University, Hangzhou, P.R. China; Guangxi Liuzhou Iron and Steel Group Company Ltd., Liuzhou, P.R. China; State Key Laboratory of Industrial Control Technology, College of Control Science and Engineering, Zhejiang University, Hangzhou, P.R. China","IEEE Transactions on Industrial Electronics","","2020","67","3","2469","2479","Due to the limits on market requirements, material conditions, and production situations in manufacturing process, conventional optimization approaches are difficult to obtain optimal economical and technical indices with physical constraints. To optimize several conflicting objects such as production rate, economic benefits, and gas emission, a hybrid-model-based intelligent optimization method that consists of an improved genetic algorithm and derived deep learning is put forward in this paper. Integration of the hybrid model has made modeling and optimizing an indivisible whole, in which the fitness of the genetic algorithm comes from deep neural networks by weighted sum of the output variables that correspond to the input solutions. The recurrent neural network (RNN) with disposition-gated recurrent unit (dGRU) is applied to capture the dynamics of blast furnace by training the model over datasets recorded in the production scene. Meanwhile, the self-adaptive population genetic algorithm (SAPGA) with a varied population size depending on the fitness distribution is used to locate the optimal solutions under current working conditions. The hybrid intelligent optimization model, validated by both numerical tests and practical data, has been running in an ironmaking plant for one year. It has proved to be successful in meeting industry demands by optimizing multiproduction indices simultaneously.","","","10.1109/TIE.2019.2903770","National Natural Science Foundation of China; National High Technology Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667016","Blast furnace ironmaking;hybrid model;intelligent optimization;production indices","Optimization;Logic gates;Production;Genetic algorithms;Deep learning;Iron;Sociology","blast furnaces;genetic algorithms;industrial plants;metallurgical industries;optimisation;production engineering computing;recurrent neural nets","self-adaptive population genetic algorithm;optimal solutions;hybrid intelligent optimization model;ironmaking process;market requirements;material conditions;conventional optimization approaches;optimal economical indices;technical indices;physical constraints;economic benefits;hybrid-model-based intelligent optimization method;deep learning;hybrid model;deep neural networks;recurrent neural network;disposition-gated recurrent unit;SAPGA;RNN;fitness distribution;ironmaking plant;multiproduction indices simultaneously;dGRU;blast furnace","","","38","Traditional","","","","IEEE","IEEE Journals"
"Attention-Based Response Generation Using Parallel Double Q-Learning for Dialog Policy Decision in a Conversational System","M. Su; C. Wu; L. Chen","Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2020","28","","131","143","This article proposes an approach to response generation using a Parallel Double Q-learning algorithm for dialog policy decision in a conversational system. First, a new semantic representation of the user's input sentence is presented by using the CKIP parser to derive the semantic dependency sequence of the input sentence. Then, a Gated Recurrent Unit-based Autoencoder is used to obtain the user's turn representation as well as context representation. A Parallel Double Q-learning algorithm with a Deep Neural Network (PD-DQN), combining two Double DQNs in parallel for the contextual and semantic information in the user's message, respectively, are proposed to determine the dialog act. Finally, the user's input and the determined dialog act are fed to an attention-based Transformer model to generate the response template. With the generated response template, the semantic slots are filled with their corresponding values to obtain the final sentence response. This article collects a multi-turn conversation database consisting of 4186 turns in the travel domain and 447 chitchat question-answer pairs as the evaluation corpus. Five-fold cross validation is employed for performance evaluation. Experimental results show that the proposed approach based on semantic dependency for intent detection increases the accuracy by 4.3%. For dialog policy decision, the PD-DQN achieves 87.57% task success rate, which is 13.9% higher than the baseline Double DQN (73.67%). Finally, using the attention-based Transformer for response template generation obtains a Bleu score of 13.6, improved by 1.5 compared to the Sequence-to-Sequence model. In subjective evaluation, both the dialog policy and sentence generation model achieve a higher appropriateness and grammatical correctness scores than the baseline system.","","","10.1109/TASLP.2019.2949687","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8883052","Attention mechanism;semantic dependency;deep reinforcement learning;conversational system","Semantics;Task analysis;Neural networks;Logic gates;Databases;Generators;Reinforcement learning","","","","","47","IEEE","","","","IEEE","IEEE Journals"
"Correlation Filter Selection for Visual Tracking Using Reinforcement Learning","Y. Xie; J. Xiao; K. Huang; J. Thiyagalingam; Y. Zhao","Department of Electrical and Electronic Engineering, Xi’an Jiaotong-Liverpool University, Suzhou, China; Department of Electrical and Electronic Engineering, Xi’an Jiaotong-Liverpool University, Suzhou, China; Department of Electrical and Electronic Engineering, Xi’an Jiaotong-Liverpool University, Suzhou, China; Rutherford Appleton Laboratory, Science and Technologies Facilities Council, Didcot, U.K.; Institute of Information Science, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2020","30","1","192","204","Correlation filter has been proven to be an effective tool for a number of approaches in visual tracking, particularly for seeking a good balance between tracking accuracy and speed. However, correlation filter-based models are susceptible to wrong updates stemming from inaccurate tracking results. To date, very little effort has been devoted towards handling the correlation filter update problem. In this paper, we propose a novel approach to address the correlation filter update problem. In our approach, we update and maintain multiple correlation filter models in parallel, and we use deep reinforcement learning for the selection of an optimal correlation filter model among them. To facilitate the decision process in an efficient manner, we propose a decision-net to deal with target appearance modeling, which is trained through hundreds of challenging videos using proximal policy optimization and a lightweight learning network. An exhaustive evaluation of the proposed approach on the OTB100 and OTB2013 benchmarks shows that the approach is effective enough to achieve the average success rate of 62.3% and the average precision score of 81.2%, both exceeding the performance of traditional correlation filter-based trackers.","","","10.1109/TCSVT.2018.2889488","National Key Research and Development of China; National Natural Science Foundation of China; Key Program Special Fund in XJTLU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8587196","Correlation filter;visual tracking;reinforcement learning;model selection;deep learning","Target tracking;Visualization;Biological system modeling;Correlation;Feature extraction","","","","","51","IEEE","","","","IEEE","IEEE Journals"
"Weakly Supervised Representation Learning for Audio-Visual Scene Analysis","S. Parekh; S. Essid; A. Ozerov; N. Q. K. Duong; P. Pérez; G. Richard","Telecom Paris, Paris, France; Telecom Paris, Paris, France; InterDigital, Cesson Sevigne, France; InterDigital, Cesson Sevigne, France; Valeo.ai, 75008 Paris, France; Telecom Paris, Paris, France","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2020","28","","416","428","Audio-visual (AV) representation learning is an important task from the perspective of designing machines with the ability to understand complex events. To this end, we propose a novel multimodal framework that instantiates multiple instance learning. Specifically, we develop methods that identify events and localize corresponding AV cues in unconstrained videos. Importantly, this is done using weak labels where only video-level event labels are known without any information about their location in time. We show that the learnt representations are useful for performing several tasks such as event/object classification, audio event detection, audio source separation and visual object localization. An important feature of our method is its capacity to learn from unsynchronized audio-visual events. We also demonstrate our framework's ability to separate out the audio source of interest through a novel use of nonnegative matrix factorization. State-of-the-art classification results, with a F1-score of 65.0, are achieved on DCASE 2017 smart cars challenge data with promising generalization to diverse object types such as musical instruments. Visualizations of localized visual regions and audio segments substantiate our system's efficacy, especially when dealing with noisy situations where modality-specific cues appear asynchronously.","","","10.1109/TASLP.2019.2957889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8926380","Multimodal classification;sound event detection;object localization;multiple instance learning;deep learning;audio-visual fusion","Visualization;Task analysis;Videos;Proposals;Feature extraction;Event detection;Source separation","","","","","74","IEEE","","","","IEEE","IEEE Journals"
"Unsupervised Adversarial Domain Adaptation for Micro-Doppler Based Human Activity Classification","H. Du; T. Jin; Y. Song; Y. Dai","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; Science and Technology on Near-Surface Detection Laboratory, Wuxi, China; Science and Technology on Near-Surface Detection Laboratory, Wuxi, China","IEEE Geoscience and Remote Sensing Letters","","2020","17","1","62","66","The fundamental difficulties in the supervised deep learning algorithm are obtaining large-scale labeled data and generalizing the trained model to a new environment. In this letter, we propose an unsupervised domain adaption method for human activity classification using micro-Doppler signatures. We study on how to classify micro-Doppler signatures in a new domain using only labeled samples from a different domain, mainly focus on simulation-to-real-world deep domain adaptation. First, we use motion capture (MOCAP) database to generate simulated micro-Doppler data to train the convolutional neural network (CNN). Then, considering the difference between simulation and real-world domain distributions, we introduce a domain discriminator to pit against the feature extractor part of the CNN. Through this adversarial process, like the generative adversarial network, the CNN trained on the simulation domain is able to generalize to the real-world domain. Experiment results show that the proposed method achieves over 84.02% accuracy in real-world micro-Doppler classification, which outperforms nearly 16% in CNN trained on the annotated simulation without domain adaptation and performs better than the existing domain adaptation methods.","","","10.1109/LGRS.2019.2917301","Foundation of Science and Technology on Near-Surface Detection Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733025","Deep learning;human activity classification;micro-Doppler effect;unsupervised domain adaptation","Feature extraction;Adaptation models;Radar;Databases;Neural networks;Training;Deep learning","","","","","30","IEEE","","","","IEEE","IEEE Journals"
"Fast Collective Activity Recognition Under Weak Supervision","P. Zhang; Y. Tang; J. Hu; W. Zheng","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Image Processing","","2020","29","","29","43","Collective activity recognition, which tells what activity a group of people is performing, is a cutting-edge research topic in computer vision. Different from action performed by individuals, collective activity needs to consider the complex interactions among different people. However, most previous works require exhaustive annotations such as accurate label information of individual actions, pairwise interactions, and poses, which could not be easily available in practice. Moreover, most of them treat human detection as a decoupled task before collective activity recognition and leverage all detected persons. This not only ignores the mutual relation between the two tasks, which makes it hard for filtering out irrelevant people, but also probably increases the computation burden when reasoning the collective activities. In this paper, we propose a fast weakly supervised deep learning architecture for collective activity recognition. For fast inference, we propose to make the actor detection and weakly supervised collective activity reasoning collaborate in an end-to-end framework by sharing convolutional layers between them. The joint learning makes the two tasks united and reinforced each other, so that it is more effective to filter out the outliers who are not involved in the activity. For the weakly supervised learning, we propose a latent embedding scheme for mining person-group interactive relationship to get rid of the use of any pairwise relation between people and the individual action labels as well. The experimental results show that the proposed framework achieves comparable or even better performance as compared to the state-of-the-art on three datasets. Our joint modelling reasons collective activities at the speed of 22.65 fps, which is the fastest ever known and substantially makes collective activity recognition more towards real-time applications.","","","10.1109/TIP.2019.2918725","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Guangdong Province Science and Technology Innovation Leading Talents; Guangdong Project; Guangzhou Research Project; Royal Society; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8726314","Fast collective activity recognition;weakly supervised learning;joint learning","Activity recognition;Task analysis;Detectors;Feature extraction;Training;Real-time systems","computer vision;data mining;image recognition;inference mechanisms;learning (artificial intelligence)","fast collective activity recognition;fast weakly supervised deep learning architecture;weakly supervised collective activity reasoning collaborate","","","39","","","","","IEEE","IEEE Journals"
"TSR-TVD: Temporal Super-Resolution for Time-Varying Data Analysis and Visualization","J. Han; C. Wang","Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","205","215","We present TSR-TVD, a novel deep learning framework that generates temporal super-resolution (TSR) of time-varying data (TVD) using adversarial learning. TSR-TVD is the first work that applies the recurrent generative network (RGN), a combination of the recurrent neural network (RNN) and generative adversarial network (GAN), to generate temporal high-resolution volume sequences from low-resolution ones. The design of TSR-TVD includes a generator and a discriminator. The generator takes a pair of volumes as input and outputs the synthesized intermediate volume sequence through forward and backward predictions. The discriminator takes the synthesized intermediate volumes as input and produces a score indicating the realness of the volumes. Our method handles multivariate data as well where the trained network from one variable is applied to generate TSR for another variable. To demonstrate the effectiveness of TSR-TVD, we show quantitative and qualitative results with several time-varying multivariate data sets and compare our method against standard linear interpolation and solutions solely based on RNN or CNN.","","","10.1109/TVCG.2019.2934255","U.S. National Science Foundation; NVIDIA GPU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802285","Time-varying data visualization;super-resolution;deep learning;recurrent generative network","Gallium nitride;Data visualization;Deep learning;Spatial resolution;Training;Generators;Generative adversarial networks","","","","","62","","","","","IEEE","IEEE Journals"
"Explainable Anatomical Shape Analysis through Deep Hierarchical Generative Models","C. Biffi; J. J. Cerrolaza; G. Tarroni; W. Bai; A. De Marvao; O. Oktay; C. Ledig; L. Le Folgoc; K. Kamnitsas; G. Doumou; J. Duan; S. K. Prasad; S. A. Cook; D. P. O’Regan; D. Rueckert","Department of Computing, Imperial College London.; Department of Computing, Imperial College London.; Department of Computing, Imperial College London.; Department of Computing, Imperial College London.; MRC London Institute of Medical Sciences, Faculty of Medicine, Imperial College London.; Department of Computing, Imperial College London.; Department of Computing, Imperial College London.; Department of Computing, Imperial College London.; Department of Computing, Imperial College London.; MRC London Institute of Medical Sciences, Faculty of Medicine, Imperial College London.; Department of Computing, Imperial College London.; National Heart and Lung Institute, Imperial College London.; MRC London Institute of Medical Sciences, Faculty of Medicine, Imperial College London.; MRC London Institute of Medical Sciences, Faculty of Medicine, Imperial College London.; Department of Computing, Imperial College London.","IEEE Transactions on Medical Imaging","","2020","PP","99","1","1","Quantification of anatomical shape changes currently relies on scalar global indexes which are largely insensitive to regional or asymmetric modifications. Accurate assessment of pathology-driven anatomical remodeling is a crucial step for the diagnosis and treatment of many conditions. Deep learning approaches have recently achieved wide success in the analysis of medical images, but they lack interpretability in the feature extraction and decision processes. In this work, we propose a new interpretable deep learning model for shape analysis. In particular, we exploit deep generative networks to model a population of anatomical segmentations through a hierarchy of conditional latent variables. At the highest level of this hierarchy, a two-dimensional latent space is simultaneously optimised to discriminate distinct clinical conditions, enabling the direct visualisation of the classification space. Moreover, the anatomical variability encoded by this discriminative latent space can be visualised in the segmentation space thanks to the generative properties of the model, making the classification task transparent. This approach yielded high accuracy in the categorisation of healthy and remodelled left ventricles when tested on unseen segmentations from our own multi-centre dataset as well as in an external validation set, and on hippocampi from healthy controls and patients with Alzheimer’s disease when tested on ADNI data. More importantly, it enabled the visualisation in three-dimensions of both global and regional anatomical features which better discriminate between the conditions under exam. The proposed approach scales effectively to large populations, facilitating highthroughput analysis of normal anatomy and pathology in largescale studies of volumetric imaging.","","","10.1109/TMI.2020.2964499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950467","Shape Analysis;Explainable Deep Learning;Generative Modeling;MRI","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Falls Risk Classification of Older Adults Using Deep Neural Networks and Transfer Learning","M. Martinez; P. L. De Leon","Sandia National Laboratories, Albuquerque, NM, USA; Klipsch School of Electrical and Computer Engineering, New Mexico State University, Las Cruces, NM, USA","IEEE Journal of Biomedical and Health Informatics","","2020","24","1","144","150","Prior research in falls risk classification using inertial sensors has relied on the use of engineered features, which has resulted in a feature space containing hundreds of features that are likely redundant and possibly irrelevant. In this paper, we propose using fully convolutional neural networks (FCNNs) to classify older adults at low or high risk of falling using inertial sensor data collected from a smartphone. Due to the limited nature of older adult inertial gait datasets, we first pre-train the FCNN models using a publicly available dataset for pedestrian activity recognition. Then via transfer learning, we train the network for falls risk classification. We show that via transfer learning, our falls risk classifier obtains an area under the receiver operating characteristic curve of 93.3%, which is 10.6% higher than the equivalent model trained without the use of transfer learning. Additionally, we show that our method outperforms other standard machine learning classifiers trained on features developed in prior research.","","","10.1109/JBHI.2019.2906499","National Technology and Engineering Solutions of Sandia, LLC; U.S. Department of Energy's National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675368","Multi-layer neural networks;machine learning;accelerometers;gyroscopes","Sensors;Biomedical measurement;Informatics;Time series analysis;Kernel;Biological system modeling;Data models","","","","","49","IEEE","","","","IEEE","IEEE Journals"
"Adaptive Discriminative Deep Correlation Filter for Visual Object Tracking","Z. Han; P. Wang; Q. Ye","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2020","30","1","155","166","Correlation filter trackers building on deep convolution neural networks (CNNs) contribute efficient visual object trackers but remain challenged with severe target appearance variations. The reason for this is that CNNs trained for image classification tasks are less discriminative to the dynamic variations of targets and backgrounds. In this paper, we propose an adaptive discriminative deep correlation filter (adaDDCF), which, by incorporating discriminative feature fine-tuning with adaptive appearance modeling, pursues stable object tracking in complex backgrounds. In adaDDCF, a convolutional Fisher discriminative analysis (FDA) layer is implemented for positive and negative instance mining and scene-specific feature learning. A correlation layer is then embedded to learn the correlation response of consecutive frames for target appearance modeling. With an online learning procedure using forward–backward propagation, the FDA layer and the correlation layer are effectively coupled, leading to effective and discriminative fine-tuning for the proposed tracker, which consequently alleviates the target drifting problem. Extensive experiments on the challenging benchmarks OTB2013, OTB2015, and OTB50 demonstrate that the proposed adaDDCF tracker outperforms many state-of-the-art trackers.","","","10.1109/TCSVT.2018.2888492","National Natural Science Foundation of China; Huawei Innovation Research Program (HIRP); Beijing Municipal Science and Technology Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8580581","Object tracking;correlation filter;deep learning;discriminative fine-tuning","Correlation;Target tracking;Adaptation models;Visualization;Training;Object tracking;Task analysis","","","","1","37","IEEE","","","","IEEE","IEEE Journals"
"Photonics Inverse Design: Pairing Deep Neural Networks With Evolutionary Algorithms","R. S. Hegde","Department of Electrical Engineering, Indian Institute of Technology, Gandhinagar, India","IEEE Journal of Selected Topics in Quantum Electronics","","2020","26","1","1","8","Deep Neural Networks (DNN) have shown early promise for inverse design with their ability to arrive at working designs much faster than conventional optimization techniques. Current approaches, however, require complicated workflows involving training more than one DNN to address the problem of non-uniqueness in the inversion and the emphasis on speed has overshadowed the far more important consideration of solution optimality. We propose and demonstrate a simplified workflow that pairs forward-model DNN with evolutionary algorithms which are widely used for inverse gg design. Our evolutionary search in forward-model space is global and exploits the massive parallelism of modern GPUs for a speedy inversion. We propose a hybrid approach where the DNN is used only for preselection and initialization that is more effective at optimization than a standalone DNN and performs nearly as well as a vanilla evolutionary search with a significantly reduced function evaluation budget. We finally show the utility of an iterative procedure for building the training dataset which further boosts the effectiveness of this approach.","","","10.1109/JSTQE.2019.2933796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8790648","Optical system design;optics and lens design;thin films;deep learning;artificial intelligence;evolutionary optimization","Optimization;Sociology;Statistics;Evolutionary computation;Photonics;Neural networks;Python","genetic algorithms;neural nets","inverse gg design;forward-model space;vanilla evolutionary search;photonics inverse design;evolutionary algorithms;conventional optimization techniques;deep neural networks;forward-model DNN","","","36","Traditional","","","","IEEE","IEEE Journals"
"Flood Forecasting System based on Integrated Big and Crowdsource Data by using Machine Learning Techniques","S. Puttinaovarat; P. Horkaew","Faculty of Science and Industrial Technology, Prince of Songkla University, Surat Thani Campus, Surat Thani, Thailand, 84000. (e-mail: supattra.p@psu.ac.th); School of Computer Engineering, Institute of Engineering, Suranaree University of Technology, Nakhon Ratchasima, Thailand, 30000.","IEEE Access","","2020","PP","99","1","1","Flood is one of the most disruptive natural hazards, responsible for loss of lives and damage to properties. A number of cities are subject to monsoons influences and hence face the disaster almost every year. Early notification of flood incident could benefit the authorities and public to devise both short and long terms preventive measures, to prepare evacuation and rescue mission, and to relieve the flood victims. Geo-graphical locations of affected areas and respective severities, for instances, are among the key determinants in most flood administration. Thus far, an effective means of anticipating flood in advance remains lacking. Existing tools were typically based on manually input and prepared data. The processes were tedious and thus prohibitive for real-time and early forecasts. Furthermore, these tools did not fully exploit more comprehen-sive information available in current big data platforms. Therefore, this paper proposes a novel flood fore-casting system based on fusing meteorological, hydrological, geospatial, and crowdsource big data in an adaptive machine learning framework. Data intelligence was driven by state-of-the-art learning strategies. Subjective and objective evaluations indicated that the developed system was able to forecast flood incidents, happening in specific areas and time frames. It was also later revealed by benchmarking experiments that the system configured with an MLP ANN gave the most effective prediction, with correct percentage, Kappa, MAE and RMSE of 97.93, 0.89, 0.01 and 0.10, respectively.","","","10.1109/ACCESS.2019.2963819","the Agricultural Research Development Agency Public Organization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949502","Flood Forecasting System;Big Data;Machine Learning;Crowdsource;Deep Learning","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Convolutional Neural Network-Based Transfer Learning for Optical Aerial Images Change Detection","J. Liu; K. Chen; G. Xu; X. Sun; M. Yan; W. Diao; H. Han","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2020","17","1","127","131","Considering the lack of labeled training data sets for the supervised change detection task, in this letter, we try to relieve this problem by proposing a convolutional neural network (CNN)-based change detection method with a newly designed loss function to achieve transfer learning among different data sets. To reach this goal, we first pretrain a U-Net model on an open source data set by taking advantages of the relatively sufficient training data used for the supervised semantic segmentation task. Then, we minimize a skillfully designed loss function to combine the high-level features extracted from the pretrained model and the semantic information contained in the change detection data set, by which a transfer learning is achieved. Third, we compute the distance between the feature vectors obtained from the above step and produce a difference map. Finally, a simple clustering method used on the difference map can even obtain satisfied change map. Experiments carried out on typical optical aerial image data sets validate that the proposed approach compares favorably to the state-of-the-art unsupervised methods.","","","10.1109/LGRS.2019.2916601","National Natural Science Foundation of China; Dongguan Science and Technology Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733847","Change detection;convolutional neural network 20 (CNN);deep learning;optical aerial image;transfer learning","Feature extraction;Semantics;Data models;Training;Remote sensing;Optical imaging;Training data","","","","","16","IEEE","","","","IEEE","IEEE Journals"
"Multi-view learning for subsurface defect detection in composite products: a challenge on thermographic data analysis","H. Wu; K. Zheng; S. Sfarra; Y. Liu; Y. Yao","Department of Electrical Engineering, National Taiwan University, 33561 Taipei Taiwan 10617 (e-mail: wuhaibinchn@outlook.com); School of Food and Biological Engineering, Jiangsu University, 12676 Zhenjiang, Jiangsu China 212013 (e-mail: beihai722@126.com); Universita degli Studi dell'Aquila, 9303 L'Aquila, Abruzzo Italy 67100 (e-mail: stefano.sfarra@univaq.it); Zhejiang University of Technology, 12624 Hangzhou China 310014 (e-mail: yliuzju@zjut.edu.cn); Department of Chemical Engineering, National Tsing Hua University, Hsinchu Taiwan 30013 (e-mail: yyao@mx.nthu.edu.tw)","IEEE Transactions on Industrial Informatics","","2020","PP","99","1","1","Nondestructive testing (NDT) is an economical way of detecting subsurface defects in composite products. Infrared thermography serves as a popular NDT method due to its high efficiency and low cost. However, defect identification by directly visualizing thermal images is difficult owing to the nonuniform background and noise. Recently, data analysis methods have been introduced to thermal image processing, including principal component analysis (PCA) which is known for its good performance in dimensionality reduction, feature extraction, and noise reduction. However, most of these methods can only extract linear features. In this study, a multi-view learning-based autoencoder, which can process not only nonlinear features but also sequential attributes, is utilized in thermographic data analysis. After extracting the low-dimensional features by multi-view learning, a background elimination step is conducted to highlight the locations and shapes of the defects. The experimental results demonstrate the feasibility of the proposed method.","","","10.1109/TII.2019.2963795","Ministry of Science and Technology Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949715","Multi-view Learning;Autoencoder;Deep Learning;Nondestructive Testing;Feature Extraction;Thermographic Data Analysis","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Combining of Multiple Deep Networks via Ensemble Generalization Loss, based on MRI Images, for Alzheimer's Disease Classification","J. Y. Choi; B. Lee","Division of Computer & Electronic Systems Engineering, Hankuk University of Foreign Studies - Global Campus, 65466 Yongin, Gyeonggi-do Korea (the Republic of) 17579 (e-mail: jychoi@hufs.ac.kr); Department of Infomation and Communications, Chosun University, 34932 Gwangju Korea (the Republic of) 61452 (e-mail: bslee@chosun.ac.kr)","IEEE Signal Processing Letters","","2020","PP","99","1","1","This letter proposes a novel way of using an ensemble of multiple deep convolutional neural networks (DCNNs) for Alzheimer's disease classification, based on magnetic resonance imaging (MRI) images. To create this ensemble of DCNNs, we propose to combine the use of multiple MRI projections (as input) with that of different DCNN architectures to increase the deep ensemble diversity. In particular, to find the optimal fusion weights of the DCNN members, we designed a novel deep ensemble generalization loss, which accounts for interaction and cooperation during the optimal weight search. The optimization framework, equipped with our ensemble generalization loss, was formulated and solved using the sequential quadratic programming. Through this method, we achieved optimal DCNN weights (i.e., a high generalization performance). The experimental results showed that our proposed DCNN ensemble outperforms current deep learning-based methods: it is able to produce state-of-the-art results on the Alzheimer's disease neuroimaging initiative (ADNI) dataset.","","","10.1109/LSP.2020.2964161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950078","Alzheimers disease classification;ensemble deep learning;generalization loss","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Truth Inference with a Deep Clustering-based Aggregation Model","L. Yin; Y. Liu; W. Zhang; Y. Yu","Shanghai Jiao Tong University, Dongchuan Road 800, Shanghai, China.; Shanghai Jiao Tong University, Dongchuan Road 800, Shanghai, China.; Shanghai Jiao Tong University, Dongchuan Road 800, Shanghai, China.; Shanghai Jiao Tong University, Dongchuan Road 800, Shanghai, China.","IEEE Access","","2020","PP","99","1","1","Traditional truth inference algorithms take multiple source labels as input and infer true labels for objects. Besides source labels, object features have been introduced in inference algorithms to achieve superior performance. A typical algorithm such as learning from crowds learns a classification model with the guide of inferred true labels where true labels are inferred from source labels. However, the main shortcoming exists in current algorithms and limits their inference performance: label noise. Since source labels from real-world data are noisy, a classifier is likely to be misguided to learn an imprecise decision boundary. In this paper, we propose a deep clustering-based aggregation model (DCAM) to overcome the shortcoming. DCAM introduces clustering for object features to form fine-grained clusters, where objects in the same cluster are supposed to have similar labels. DCAM exploits a cluster label distribution to represent the labeling information of all objects in the corresponding cluster to overcome the problem of label noise. To implement the idea of clustering-based truth inference, DCAM integrates source label generation and deep clustering in a unified framework by utilizing maximum a posteriori (MAP) estimation. Therefore, the proposed model is a novel approach for truth inference with object features. Experimental results on eight real-world inference tasks show that DCAM has a significant improvement of inference accuracy over the state-of-the-art truth inference algorithms. We further discuss the effect of cluster numbers, the quality of clustering, and illustrate the learned embeddings to support the effectiveness of DCAM.","","","10.1109/ACCESS.2020.2964484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950460","Crowdsourcing;truth inference;clustering methods;neural networks;unsupervised learning;machine learning","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Biometric Presentation Attack Detection: Beyond the Visible Spectrum","R. Tolosana; M. Gomez-Barrero; C. Busch; J. Ortega-Garcia","Biometrics and Data Pattern Analytics (BiDA) Lab, Universidad Autonoma de Madrid, Madrid, Spain; Da/sec—Biometrics and Internet Security Research Group, Hochschule Darmstadt, Darmstadt, Germany; Da/sec—Biometrics and Internet Security Research Group, Hochschule Darmstadt, Darmstadt, Germany; Biometrics and Data Pattern Analytics (BiDA) Lab, Universidad Autonoma de Madrid, Madrid, Spain","IEEE Transactions on Information Forensics and Security","","2020","15","","1261","1275","The increased need for unattended authentication in multiple scenarios has motivated a wide deployment of biometric systems in the last few years. This has in turn led to the disclosure of security concerns specifically related to biometric systems. Among them, presentation attacks (PAs, i.e., attempts to log into the system with a fake biometric characteristic or presentation attack instrument) pose a severe threat to the security of the system: any person could eventually fabricate or order a gummy finger or face mask to impersonate someone else. In this context, we present a novel fingerprint presentation attack detection (PAD) scheme based on  $i$ ) a new capture device able to acquire images within the short wave infrared (SWIR) spectrum, and  $ii$ ) an in-depth analysis of several state-of-the-art techniques based on both handcrafted and deep learning features. The approach is evaluated on a database comprising over 4700 samples, stemming from 562 different subjects and 35 different presentation attack instrument (PAI) species. The results show the soundness of the proposed approach with a detection equal error rate (D-EER) as low as 1.35% even in a realistic scenario where five different PAI species are considered only for testing purposes (i.e., unknown attacks).","","","10.1109/TIFS.2019.2934867","Office of the Director of National Intelligence; German Federal Ministry of Education and Research (BMBF); Hessen State Ministry for Higher Education, Research and the Arts (HMWK) within the National Research Centre for Applied Cybersecurity (CRISP); BIBECA (MINECO/FEDER); Bio-Guard (Ayudas Fundacion BBVA a Equipos de Investigacion Cientfica 2017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794818","Biometrics;presentation attack detection;deep learning;CNN;SWIR;fingerprint","Databases;Skin;Deep learning;Feature extraction;Instruments;Face;Support vector machines","","","","1","82","CCBY","","","","IEEE","IEEE Journals"
"A Copy-Proof Scheme Based on the Spectral and Spatial Barcoding Channel Models","C. Chen; M. Li; A. Ferreira; J. Huang; R. Cai","Guangdong Key Laboratory of Intelligent Information Processing and Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, China; Tencent Holdings Ltd., Shenzhen, China; Department of Mathematics and Informatics, University of Cagliari, Cagliari, Italy; Guangdong Key Laboratory of Intelligent Information Processing and Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, China; Rapid-Rich Object Search Lab, Nanyang Technological University, Singapore","IEEE Transactions on Information Forensics and Security","","2020","15","","1056","1071","The traditional two-dimensional (2D) barcode has been employed in anti-counterfeiting systems as a storage media for serial numbers. However, an attack can be initiated by simply copying the 2D barcode and attaching it to a counterfeit product. In this paper, we aim at proposing an authentication scheme with a mobile imaging device for a 2D barcode. This work presents a competitive solution among the 2D barcode authentication schemes that have been verified under mobile imaging conditions. The proposed copy-proof scheme is composed of two sets of features which are extracted by exploiting the characteristics of barcoding channel models. The proposed features identify the intrinsic differences between genuine and counterfeit barcode images in the frequency and spatial domains. An efficient two-stage barcode authentication framework is then proposed by combining the two sets of features in a cascading manner. To evaluate the practicality of the proposed authentication scheme, four databases with different devices (printers, scanners, mobile cameras), barcode sizes, and barcode designs are considered in the experiments. By comparing with the existing texture descriptors and some deep learning-based approaches, it is shown that the proposed scheme has a higher authentication accuracy under various conditions, such as cross-database, cross-size and cross-pattern experiments which study the generalities of a pre-trained model towards challenging conditions commonly found in real-world scenarios. Last but not least, the proposed scheme has been evaluated under some state-of-the-art attack scenarios where the attacker employs several realizations of genuine patterns or the deep learning-based technique to produce a counterfeit copy. The source code and data for producing the results in our experiments are available at https://bit.ly/2FOlJH7.","","","10.1109/TIFS.2019.2934861","National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; Guangdong Province Research and Development Plan in Key Areas; Science and Technology Innovation Commission of Shenzhen, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794824","Copy-proof;2D barcode;Fourier domain;local binary pattern","Two dimensional displays;Authentication;Channel models;Imaging;Printing;Databases","bar codes;feature extraction;image coding;image texture;learning (artificial intelligence)","higher authentication accuracy;cross-pattern experiments;state-of-the-art attack scenarios;deep learning-based technique;counterfeit copy;copy-proof scheme;two-dimensional barcode;anti-counterfeiting systems;storage media;serial numbers;counterfeit product;authentication scheme;mobile imaging device;2D barcode authentication schemes;mobile imaging conditions;barcoding channel models;genuine images;counterfeit barcode images;spatial domains;two-stage barcode authentication framework;mobile cameras;barcode sizes;barcode designs;deep learning-based approaches","","","52","","","","","IEEE","IEEE Journals"
"Neural Machine Translation with Deep Attention","B. Zhang; D. Xiong; J. Su","Software School, Xiamen University, Xiamen, Fujian, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Software School, Xiamen University, Xiamen, Fujian, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2020","42","1","154","163","Deepening neural models has been proven very successful in improving the model's capacity when solving complex learning tasks, such as the machine translation task. Previous efforts on deep neural machine translation mainly focus on the encoder and the decoder, while little on the attention mechanism. However, the attention mechanism is of vital importance to induce the translation correspondence between different languages where shallow neural networks are relatively insufficient, especially when the encoder and decoder are deep. In this paper, we propose a deep attention model (DeepAtt). Based on the low-level attention information, DeepAtt is capable of automatically determining what should be passed or suppressed from the corresponding encoder layer so as to make the distributed representation appropriate for high-level attention and translation. We conduct experiments on NIST Chinese-English, WMT English-German, and WMT English-French translation tasks, where, with five attention layers, DeepAtt yields very competitive performance against the state-of-the-art results. We empirically find that with an adequate increase of attention layers, DeepAtt tends to produce more accurate attention weights. An in-depth analysis on the translation of important context words further reveals that DeepAtt significantly improves the faithfulness of system translations.","","","10.1109/TPAMI.2018.2876404","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; National Language Committee of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493282","Deep attention network;neural machine translation (NMT);attention-based sequence-to-sequence learning;natural language processing","Decoding;Task analysis;Semantics;NIST;Encoding;Neural networks;Analytical models","","","","2","32","IEEE","","","","IEEE","IEEE Journals"
"Discriminative Transfer Feature and Label Consistency for Cross-Domain Image Classification","S. Li; C. H. Liu; L. Su; B. Xie; Z. Ding; C. L. P. Chen; D. Wu","School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China.; School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China (e-mail: liuchi02@gmail.com).; School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China.; School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China.; Department of Computer, Information and Technology, Indiana University-Purdue University Indianapolis, Indianapolis, IN 46202 USA.; Faculty of Science and Technology, University of Macau, Macau 999078, China.; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611-6130 USA.","IEEE Transactions on Neural Networks and Learning Systems","","2020","PP","99","1","15","Visual domain adaptation aims to seek an effective transferable model for unlabeled target images by benefiting from the well-labeled source images following different distributions. Many recent efforts focus on extracting domain-invariant image representations via exploring target pseudo labels, predicted by the source classifier, to further mitigate the conditional distribution shift across domains. However, two essential factors are overlooked by most existing methods: 1) the learned transferable features should be not only domain invariant but also category discriminative; and 2) the target pseudo label is a two-edged sword to cross-domain alignment. In other words, the wrongly predicted target labels may hinder the class-wise domain matching. In this article, to address these two issues simultaneously, we propose a discriminative transfer feature and label consistency (DTLC) approach for visual domain adaptation problems, which can naturally unify cross-domain alignment with discriminative information preserved and label consistency of source and target data into one framework. To be specific, DTLC first incorporates class discriminative information by penalizing the maximum distance of data pair in the same class and the minimum distance of data pair sharing the different labels for each data into the distribution alignment of both domains. The target pseudo labels are then refined based on the label consistency within the domains. Thus, the transfer feature learning and coarse-to-fine target labels would be coupled to benefit each other in an iterative way. Comprehensive experiments on several visual cross-domain benchmarks verify that DTLC can gain remarkable margins over state-of-the-art (SOTA) nondeep visual domain adaptation methods and even be comparable to competitive deep domain adaptation ones.","","","10.1109/TNNLS.2019.2958152","National Natural Science Foundation of China; National Key Research and Development Plan of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951259","Cross-domain image classification;discriminative transfer feature learning;label consistency;visual domain adaptation.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Co-Learning Feature Fusion Maps From PET-CT Images of Lung Cancer","A. Kumar; M. Fulham; D. Feng; J. Kim","School of Computer Science, The University of Sydney, Sydney, NSW, Australia; Department of Molecular Imaging, Royal Prince Alfred Hospital, Sydney, NSW, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Medical Imaging","","2020","39","1","204","217","The analysis of multi-modality positron emission tomography and computed tomography (PET-CT) images for computer-aided diagnosis applications (e.g., detection and segmentation) requires combining the sensitivity of PET to detect abnormal regions with anatomical localization from CT. Current methods for PET-CT image analysis either process the modalities separately or fuse information from each modality based on knowledge about the image analysis task. These methods generally do not consider the spatially varying visual characteristics that encode different information across different modalities, which have different priorities at different locations. For example, a high abnormal PET uptake in the lungs is more meaningful for tumor detection than physiological PET uptake in the heart. Our aim is to improve the fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. Our CNN first encodes modality-specific features and then uses them to derive a spatially varying fusion map that quantifies the relative importance of each modality’s feature across different spatial locations. These fusion maps are then multiplied with the modality-specific feature maps to obtain a representation of the complementary multi-modality information at different locations, which can then be used for image analysis. We evaluated the ability of our CNN to detect and segment multiple regions (lungs, mediastinum, and tumors) with different fusion requirements using a dataset of PET-CT images of lung cancer. We compared our method to baseline techniques for multi-modality image fusion (fused inputs (FSs), multi-branch (MB) techniques, and multi-channel (MC) techniques) and segmentation. Our findings show that our CNN had a significantly higher foreground detection accuracy (99.29%,  ${p} < {0.05}$ ) than the fusion baselines (FS: 99.00%, MB: 99.08%, and TC: 98.92%) and a significantly higher Dice score (63.85%) than the recent PET-CT tumor segmentation methods.","","","10.1109/TMI.2019.2923601","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8737963","Multi-modality imaging;deep learning;fusion learning;PET-CT","Computed tomography;Tumors;Lung;Image segmentation;Biomedical imaging;Cancer","","","","","85","IEEE","","","","IEEE","IEEE Journals"
"Breast Cancer Image Classification via Multi-network Features and Dual-network Orthogonal Low-rank Learning","Y. Wang; B. Lei; A. Elazab; E. Tan; W. Wang; F. Huang; X. Gong; T. Wang","National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, 518060 China.; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, 518060 China.; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, 518060 China and Computer Science Department, Misr Higher Institute of Commerce and computers, Mansoura, 35516 Egypt.; School of Electric and Electronical Engineering, Nanyang Technological University, Singapore, 639798 Singapore.; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, 518060 China.; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, 518060 China.; Affiliated Hospital of Shenzhen University, Shenzhen University, Shenzhen Second People’s Hospital, Shenzhen 530031, China.; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, 518060 China.","IEEE Access","","2020","PP","99","1","1","Histopathological image analysis is an important technique for early diagnosis and detection of breast cancer in clinical practice. However, it has limited efficiency and thus the detection of breast cancer is still an open issue in medical image analysis. To improve the early diagnostic accuracy of breast cancer and reduce the workload of doctors, we devise a classification framework based on histology images by combining deep learning with machine learning methodologies in this paper. Specifically, we devise a multi-network feature extraction model by using pre-trained deep convolution neural networks (DCNNs), develop an effective feature dimension reduction method and train an ensemble support vector machine (E-SVM). First, we preprocess the histological images via scale transformation and color enhancement methods. Second, the multi-network features are extracted by using four pre-trained DCNNs (e.g., DenseNet-121, ResNet-50, multi-level InceptionV3, and multi-level VGG-16). Third, a feature selection method via dual-network orthogonal low-rank learning (DOLL) is further developed for performance boosting and overfitting alleviation. Finally, an E-SVM is trained via fused features and voting strategy to perform the classification task, which classifies the images into four classes (i.e., benign, in situ carcinomas, invasive carcinomas, and normal). We evaluate the proposed method on the public ICIAR 2018 Challenge dataset of histology images of breast cancer and achieve a high classification accuracy of 97.70%. Experimental results show that our method can achieve quite promising performance and outperform state-of-the-art methods.","","","10.1109/ACCESS.2020.2964276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950452","Breast cancer image classification;Deep convolutional neural network;Multi-network features;Low-rank learning;Ensemble support vector machine","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"5G Vehicular Network Resource Management for Improving Radio Access through Machine Learning","S. K. Tayaba; H. A. Khattak; A. Almogren; M. A. Shah; I. U. Din; I. Alkhalifa; M. Guizani","Department of Computer Science, COMSATS University Islamabad, Islamabad 44550, Pakistan.; Department of Computer Science, COMSATS University Islamabad, Islamabad 44550, Pakistan.; Chair of Cyber Security, Department of Computer Science, College of Computer and Information Sciences, King Saud University, Riyadh 11633, Saudi Arabia.; Department of Computer Science, COMSATS University Islamabad, Islamabad 44550, Pakistan.; Department of Information Technology, The University of Haripur, Haripur, 22620, Pakistan.; Chair of Cyber Security, Department of Computer Science, College of Computer and Information Sciences, King Saud University, Riyadh 11633, Saudi Arabia.; Computer Science and Engineering Department, Qatar University, Doha, 2713, Qatar.","IEEE Access","","2020","PP","99","1","1","The current cellular technology and vehicular networks cannot satisfy the mighty strides of vehicular network demands. Resource management has become a complex and challenging objective to gain expected outcomes in a vehicular environment. The 5G cellular network promises to provide ultra-high-speed, reduced delay, and reliable communications. The development of new technologies such as the network function virtualization (NFV) and software defined networking (SDN) are critical enabling technologies leveraging 5G. The SDN-based 5G network can provide an excellent platform for autonomous vehicles because SDN offers open programmability and flexibility for new services incorporation. This separation of control and data planes enables centralized and efficient management of resources in a very optimized and secure manner by having a global overview of the whole network. The SDN also provides flexibility in communication administration and resource management, which are of critical importance when considering the ad-hoc nature of vehicular network infrastructures, in terms of safety, privacy, and security, in vehicular network environments. In addition, it promises the overall improved performance. In this paper, we propose a flow-based policy framework on the basis of two tiers virtualization for vehicular networks using SDNs. The vehicle to vehicle (V2V) communication is quite possible with wireless virtualization where different radio resources are allocated to V2V communications based on the flow classification, i.e., safety-related flow or non-safety flows, and the controller is responsible for managing the overall vehicular environment and V2X communications. The motivation behind this study is to implement a machine learning-enabled architecture to cater the sophisticated demands of modern vehicular Internet infrastructures. The inclination towards robust communications in 5G-enabled networks has made it somewhat tricky to manage network slicing efficiently. This paper also presents a proof of concept for leveraging machine learning-enabled resource classification and management through experimental evaluation of special-purpose testbed established in custom mininet setup. Furthermore, the results have been evaluated using Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Deep Neural Network (DNN). While concluding the paper, it is shown that the LSTM has outperformed the rest of classification techniques with promising results.","","","10.1109/ACCESS.2020.2964697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951149","Future Internet Architectures;Machine Learning;Network Reliability;Privacy;Resource Management;Security;Software Defined Networks;Vehicular Networks","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Deep imitation learning for autonomous vehicles based on convolutional neural networks","P. M. Kebria; A. Khosravi; S. M. Salaken; S. Nahavandi","Institute for Intelligent Systems Research and Innovation, Deakin University, Waurn Ponds, VIC 3216, Australia; Institute for Intelligent Systems Research and Innovation, Deakin University, Waurn Ponds, VIC 3216, Australia; Institute for Intelligent Systems Research and Innovation, Deakin University, Waurn Ponds, VIC 3216, Australia; Institute for Intelligent Systems Research and Innovation, Deakin University, Waurn Ponds, VIC 3216, Australia","IEEE/CAA Journal of Automatica Sinica","","2020","7","1","82","95","Providing autonomous systems with an effective quantity and quality of information from a desired task is challenging. In particular, autonomous vehicles, must have a reliable vision of their workspace to robustly accomplish driving functions. Speaking of machine vision, deep learning techniques, and specifically convolutional neural networks, have been proven to be the state of the art technology in the field. As these networks typically involve millions of parameters and elements, designing an optimal architecture for deep learning structures is a difficult task which is globally under investigation by researchers. This study experimentally evaluates the impact of three major architectural properties of convolutional networks, including the number of layers, filters, and filter size on their performance. In this study, several models with different properties are developed, equally trained, and then applied to an autonomous car in a realistic simulation environment. A new ensemble approach is also proposed to calculate and update weights for the models regarding their mean squared error values. Based on design properties, performance results are reported and compared for further investigations. Surprisingly, the number of filters itself does not largely affect the performance efficiency. As a result, proper allocation of filters with different kernel sizes through the layers introduces a considerable improvement in the performance. Achievements of this study will provide the researchers with a clear clue and direction in designing optimal network architectures for deep learning purposes.","","","10.1109/JAS.2019.1911825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945486","","","","","","","","","","","","IEEE","IEEE Journals"
"An Embedding Cost Learning Framework Using GAN","J. Yang; D. Ruan; J. Huang; X. Kang; Y. Shi","Guangdong Key Laboratory of Information Security, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory of Information Security, Sun Yat-sen University, Guangzhou, China; Shenzhen Key Laboratory of Media Security, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Information Security, Sun Yat-sen University, Guangzhou, China; Department of ECE, New Jersey Institute of Technology, Newark, NJ, USA","IEEE Transactions on Information Forensics and Security","","2020","15","","839","851","Successful adaptive steganography has mainly focused on embedding the payload while minimizing an appropriately defined distortion function. The application of deep learning to steganalysis has greatly challenged present adaptive steganographic methods, but has also shown the potential for the improvement of steganography. This paper proposes a distortion function generating a framework for steganography. It has three modules: a generator with a U-Net architecture to translate a cover image into an embedding change probability map, a no-pre-training-required double-tanh function to approximate the optimal embedding simulator while preserving gradient norm during backpropagation in the adversarial training, and an enhanced steganalyzer based on a convolution neural network together with multiple high pass filters as the discriminator. Extensive experimental results on different datasets have shown that the proposed framework outperforms the current state-of-the-art steganographic schemes. Moreover, the adversarial training time is reduced dramatically compared with the GAN-based automatic steganographic distortion learning framework (ASDL-GAN).","","","10.1109/TIFS.2019.2922229","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735922","Adaptive steganography;steganalysis;generative adversarial networks (GAN)","Generators;Training;Distortion;Gallium nitride;Payloads;Feature extraction;Computer architecture","backpropagation;distortion;gradient methods;high-pass filters;image coding;image filtering;neural net architecture;probability;steganography","deep learning;steganalysis;adaptive steganographic methods;U-Net architecture;embedding change probability map;no-pre-training-required double-tanh function;gradient norm;convolution neural network;multiple high pass filters;adversarial training time;GAN-based automatic steganographic distortion learning framework;ASDL-GAN;embedding cost learning framework;adaptive steganography;distortion function;optimal embedding simulator;steganographic schemes;backpropagation;enhanced steganalyzer;discriminator;adversarial training;cover image translation","","2","36","","","","","IEEE","IEEE Journals"
"Deep Residual Inception Encoder–Decoder Network for Medical Imaging Synthesis","F. Gao; T. Wu; X. Chu; H. Yoon; Y. Xu; B. Patel","Industrial Engineering Program, School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA; Industrial Engineering Program, School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA; College of Management, Institute of Big Data Intelligent Management and Decision, Shenzhen University, Shenzhen, China; Industrial Engineering Program, School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA; Industrial Engineering Program, School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA; Department of Radiology, Mayo Clinic in Arizona, Scottsdale, AZ, USA","IEEE Journal of Biomedical and Health Informatics","","2020","24","1","39","49","Image synthesis is a novel solution in precision medicine for scenarios where important medical imaging is not otherwise available. The convolutional neural network (CNN) is an ideal model for this task because of its powerful learning capabilities through the large number of layers and trainable parameters. In this research, we propose a new architecture of residual inception encoder–decoder neural network (RIED-Net) to learn the nonlinear mapping between the input images and targeting output images. To evaluate the validity of the proposed approach, it is compared with two models from the literature: synthetic CT deep convolutional neural network (sCT-DCNN) and shallow CNN, using both an institutional mammogram dataset from Mayo Clinic Arizona and a public neuroimaging dataset from the Alzheimer's Disease Neuroimaging Initiative. Experimental results show that the proposed RIED-Net outperforms the two models on both datasets significantly in terms of structural similarity index, mean absolute percent error, and peak signal-to-noise ratio.","","","10.1109/JBHI.2019.2912659","Alzheimer's Disease Neuroimaging Initiative (ADNI); National Institutes of Health; DOD ADNI; U.S. Department of Defense; National Institute on Aging; National Institute of Biomedical Imaging and Bioengineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695110","Deep learning;image synthesis;inception;medical imaging and residual net","Image segmentation;Biomedical imaging;Task analysis;Image generation;Magnetic resonance imaging;Tumors","","","","1","53","IEEE","","","","IEEE","IEEE Journals"
"Augmenting Recurrent Neural Networks Resilience by Dropout","D. Bacciu; F. Crecchi","Dipartimento di Informatica, Università di Pisa, Pisa, Italy; Dipartimento di Informatica, Università di Pisa, Pisa, Italy","IEEE Transactions on Neural Networks and Learning Systems","","2020","31","1","345","351","This brief discusses the simple idea that dropout regularization can be used to efficiently induce resiliency to missing inputs at prediction time in a generic neural network. We show how the approach can be effective on tasks where imputation strategies often fail, namely, involving recurrent neural networks and scenarios where whole sequences of input observations are missing. The experimental analysis provides an assessment of the accuracy–resiliency tradeoff in multiple recurrent models, including reservoir computing methods, and comprising real-world ambient intelligence and biomedical time series.","","","10.1109/TNNLS.2019.2899744","Italian Ministry of Education, University, and Research through Project SIR 2014 LIST-IT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668686","Deep learning;dependable machine learning;dropout;missing inputs;recurrent neural networks (RNNs)","Training;Biological neural networks;Neurons;Recurrent neural networks;Resilience;Data models;Computational modeling","","","","1","34","IEEE","","","","IEEE","IEEE Journals"
"Attention-Based Two-Stream Convolutional Networks for Face Spoofing Detection","H. Chen; G. Hu; Z. Lei; Y. Chen; N. M. Robertson; S. Z. Li","Zhejiang Provincial Key Laboratory for Network Multimedia Technologies, Zhejiang University, Hangzhou, China; School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K.; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Zhejiang Provincial Key Laboratory for Network Multimedia Technologies, Zhejiang University, Hangzhou, China; School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K.; Chinese Academy of Sciences, Institute of Automation, Beijing, China","IEEE Transactions on Information Forensics and Security","","2020","15","","578","593","Since the human face preserves the richest information for recognizing individuals, face recognition has been widely investigated and achieved great success in various applications in the past decades. However, face spoofing attacks (e.g., face video replay attack) remain a threat to modern face recognition systems. Though many effective methods have been proposed for anti-spoofing, we find that the performance of many existing methods is degraded by illuminations. It motivates us to develop illumination-invariant methods for anti-spoofing. In this paper, we propose a two-stream convolutional neural network (TSCNN), which works on two complementary spaces: RGB space (original imaging space) and multi-scale retinex (MSR) space (illumination-invariant space). Specifically, the RGB space contains the detailed facial textures, yet it is sensitive to illumination; MSR is invariant to illumination, yet it contains less detailed facial information. In addition, the MSR images can effectively capture the high-frequency information, which is discriminative for face spoofing detection. Images from two spaces are fed to the TSCNN to learn the discriminative features for anti-spoofing. To effectively fuse the features from two sources (RGB and MSR), we propose an attention-based fusion method, which can effectively capture the complementarity of two features. We evaluate the proposed framework on various databases, i.e., CASIA-FASD, REPLAY-ATTACK, and OULU, and achieve very competitive performance. To further verify the generalization capacity of the proposed strategies, we conduct cross-database experiments, and the results show the great effectiveness of our method.","","","10.1109/TIFS.2019.2922241","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8737949","Face spoofing;multi-scale retinex;deep learning;attention model;feature fusion","Face;Feature extraction;Lighting;Face recognition;Fuses;Deep learning;Three-dimensional displays","convolutional neural nets;face recognition;feature extraction;image colour analysis;image texture","face spoofing detection;human face;face spoofing attacks;face video replay attack;modern face recognition systems;anti-spoofing;illumination-invariant methods;two-stream convolutional neural network;complementary spaces;RGB space;original imaging space;multiscale retinex space;illumination-invariant space;detailed facial textures;MSR images;high-frequency information;attention-based fusion method;REPLAY-ATTACK;facial information;attention-based two-stream convolutional networks","","","66","","","","","IEEE","IEEE Journals"
"Learning Long-Term Temporal Features With Deep Neural Networks for Human Action Recognition","S. Yu; L. Xie; L. Liu; D. Xia","School of Information Science and Engineering and Provincial Demonstration Software Institute, Shaoguan University, Shaoguan, China; School of Information Science and Engineering and Provincial Demonstration Software Institute, Shaoguan University, Shaoguan, China; School of Information Science and Engineering and Provincial Demonstration Software Institute, Shaoguan University, Shaoguan, China; School of Big Data and Computer Science, Guizhou Normal University, Guiyang, China","IEEE Access","","2020","8","","1840","1850","One of challenging tasks in the field of artificial intelligence is the human action recognition. In this paper, we propose a novel long-term temporal feature learning architecture for recognizing human action in video, named Pseudo Recurrent Residual Neural Networks (P-RRNNs), which exploits the recurrent architecture and composes each in different connection among units. Two-stream CNNs model (GoogLeNet) is employed for extracting local temporal and spatial features respectively. The local spatial and temporal features are then integrated into global long-term temporal features by using our proposed two-stream P-RRNNs. Finally, the Softmax layer fuses the outputs of two-stream P-RRNNs for action recognition. The experimental results on two standard databases UCF101 and HMDB51 demonstrate the outstanding performance of proposed method based on architectures for human action recognition.","","","10.1109/ACCESS.2019.2962284","National Natural Science Foundation of China; Shaoguan Science and Technology Plan Project; Shaoguan University; Shaoguan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943218","Action recognition;residual learning;recurrent neural networks;long short-term memory (LSTM)","","","","","","86","CCBY","","","","IEEE","IEEE Journals"
"Personalizing Activity Recognition Models with Quantifying Different Types of Uncertainty Using Wearable Sensors","A. Akbari; R. Jafari","Biomedical Engineering, Texas A&M University, College Station, Texas United States 77843-3120 (e-mail: aliakbari@tamu.edu); Texas A&M University, College Station, Texas United States 77843 (e-mail: rjafari@tamu.edu)","IEEE Transactions on Biomedical Engineering","","2020","PP","99","1","1","Recognizing activities of daily living (ADL) provides vital contextual information that enhances the effectiveness of various mobile health and wellness applications. Development of wearable motion sensors along with machine learning algorithms offer a great opportunity for ADL recognition. However, the performance of the ADL recognition systems may significantly degrade when they are used by a new user due to inter-subject variability. This issue limits the usability of these systems. In this paper, we propose a deep learning assisted personalization framework for ADL recognition with the aim to maximize the personalization performance while minimizing solicitation of inputs or labels from the user to reduce user's burden. The proposed framework consists of unsupervised retraining of automatic feature extraction layers and supervised fine-tuning of classification layers through a novel active learning model based on a given model's uncertainty. We design a Bayesian deep convolutional neural network with stochastic latent variables that allows us to estimate both aleatoric (data-dependent) and epistemic (model-dependent) uncertainties in recognition task. In this study, for the first time, we show how distinguishing between the two aforementioned sources of uncertainty leads to more effective active learning. The experimental results show that our proposed method improves the accuracy of ADL recognition on a new user by 25% on average compared to the case of using a model for a new user with no personalization with an average final accuracy of 89.2%. Moreover, our method achieves higher personalization accuracy while significantly reducing user's burden in terms of soliciting inputs and labels compared to other methods.","","","10.1109/TBME.2019.2963816","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949726","Activity recognition;Personalization;Wearable sensors;Deep learning;Active learning;Uncertainty quantification;Unsupervised learning","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Online Fault Diagnosis Method Based on Transfer Convolutional Neural Networks","G. Xu; M. Liu; Z. Jiang; W. Shen; C. Huang","School of Electronics and Information Engineering, Tongji University, Shanghai, China; School of Electronics and Information Engineering, Tongji University, Shanghai, China; School of Electronics and Information Engineering, Tongji University, Shanghai, China; Key Laboratory of Embedded System and Service Computing, Tongji University, Shanghai, China; School of Electronics and Information Engineering, Tongji University, Shanghai, China","IEEE Transactions on Instrumentation and Measurement","","2020","69","2","509","520","Fault detection and diagnosis (FDD) is crucial for stable, reliable, and safe operation of industrial equipment. In recent years, deep learning models have been widely used in data-driven FDD methods because of their automatic feature learning capability. In general, these models are trained on historical sensor data, and therefore, it is very difficult to meet the real-time requirement of online FDD applications. Since transfer learning can solve different but similar problems in the target domain efficiently and effectively with the knowledge learned from the source domain, this paper proposes an online fault diagnosis method based on a deep transfer convolutional neural network (TCNN) framework. The TCNN framework is made up of an online CNN based on LeNet-5 and several offline CNNs with a shallow structure. First, time-domain signal data are converted into images that contain abundant fault information and are suitable as the input of CNN. Then, the online CNN is constructed to automatically extract representative features from the converted images and classify faults. Finally, in order to improve the real-time performance of the online CNN, several offline CNNs are also constructed and pretrained on related data sets. By directly transferring the shallow layers of the trained offline CNNs to the online CNN, the online CNN can significantly improve the real-time performance and successfully address the issue of achieving the desired diagnostic accuracy within limited training time. The proposed method is validated on two bearing data sets and one pump data set, respectively. The prediction accuracy of the proposed method using three data sets are 99.88%, 99.13%, and 99.98%, respectively. The experimental results also indicate that the improvement of accuracy is 19.21% for the motor bearing case, 29.82% for the rolling mill bearing case, and 33.26% for the pump case during the early stage of learning.","","","10.1109/TIM.2019.2902003","National Natural Science Foundation of China; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672123","Convolutional neural network (CNN);online fault diagnosis;real-time performance;signal-to-image conversion;transfer learning","Fault diagnosis;Feature extraction;Deep learning;Circuit faults;Real-time systems;Training;Data models","","","","3","45","IEEE","","","","IEEE","IEEE Journals"
"An Intelligent Deep Feature Learning Method With Improved Activation Functions for Machine Fault Diagnosis","W. You; C. Shen; D. Wang; L. Chen; X. Jiang; Z. Zhu","School of Rail Transportation, Soochow University, Suzhou, China; School of Rail Transportation, Soochow University, Suzhou, China; The State Key Laboratory of Mechanical Systems and Vibration, Shanghai Jiao Tong University, Shanghai, China; School of Rail Transportation, Soochow University, Suzhou, China; School of Rail Transportation, Soochow University, Suzhou, China; School of Rail Transportation, Soochow University, Suzhou, China","IEEE Access","","2020","8","","1975","1985","Rotating machinery has been developed with high complexity and precision, and bearings and gears are crucial components in the machinery system. Deep learning has attracted considerable attention from researchers in this area. The convolutional neural network (CNN) is a typical deep learning model that has a strong capability for automatically extracting features from raw data. This capability minimizes dependence on expert knowledge during feature extraction and selection. In CNN, hyperparameters, such as activation functions, can directly influence the performance of the model. In this study, the improved rectified linear units (ReLU)-CNNs are proposed for machinery fault diagnosis. The model’s input are raw vibration signals without feature extraction and selection. It is experimentally validated for fault diagnosis using bearing and gearbox datasets. Results show that the proposed method can obtain satisfactory accuracy with enhanced convergence speed. For both datasets, the proposed method gives better diagnosis accrues than the other compared models. The proposed model can take advantages of standard ReLU-CNN, and these advantages can overcome traditional activation functions’ vanishing gradient problems. Meanwhile, the improved ReLU-CNN has a new property that makes it perform better than the standard ReLU-CNN.","","","10.1109/ACCESS.2019.2962734","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Suzhou Prospective Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944015","Rotating machinery;convolutional neural network;fault diagnosis;activation function","","","","","","33","CCBY","","","","IEEE","IEEE Journals"
"ProtoSteer: Steering Deep Sequence Model with Prototypes","Y. Ming; P. Xu; F. Cheng; H. Qu; L. Ren","Hong Kong University of Science and Technology; Bosch Research North America; Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; Bosch Research North America","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","238","248","Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy.","","","10.1109/TVCG.2019.2934267","Hong Kong TRS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827944","Sequence Data;Explainable Artificial Intelligence (XAI);Recurrent Neural Networks (RNNs);Prototype Learning","Prototypes;Data visualization;Data models;Machine learning;Computational modeling;Predictive models;Task analysis","","","","","51","","","","","IEEE","IEEE Journals"
"When Your Robot Breaks: Active Learning During Plant Failure","M. L. Schrum; M. C. Gombolay","Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA; Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Robotics and Automation Letters","","2020","5","2","438","445","Detecting and adapting to catastrophic failures in robotic systems requires a robot to learn its new dynamics quickly and safely to best accomplish its goals. To address this challenging problem, we propose probabilistically-safe, online learning techniques to infer the altered dynamics of a robot at the moment a failure (e.g., physical damage) occurs. We combine model predictive control and active learning within a chance-constrained optimization framework to safely and efficiently learn the new plant model of the robot. We leverage a neural network for function approximation in learning the latent dynamics of the robot under failure conditions. Our framework generalizes to various damage conditions while being computationally light-weight to advance real-time deployment. We empirically validate within a virtual environment that we can regain control of a severely damaged aircraft in seconds and require only 0.1 seconds to find safe, information-rich trajectories, outperforming state-of-the-art approaches.","","","10.1109/LRA.2019.2961598","NSF Accessibility, Rehabilitation and Movement Science Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938725","Aerial systems: mechanics and control;autonomous agents;deep learning in robotics and automation","","","","","","30","IEEE","","","","IEEE","IEEE Journals"
"A Fine-Grained Adversarial Network Method for Cross-Domain Industrial Fault Diagnosis","Z. Chai; C. Zhao","State Key Laboratory of Industrial Control Technology, College of Control Science and Engineering, Zhejiang University, Hangzhou 310027, China.; State Key Laboratory of Industrial Control Technology, College of Control Science and Engineering, Zhejiang University, Hangzhou 310027, China (e-mail: chhzhao@zju.edu.cn).","IEEE Transactions on Automation Science and Engineering","","2020","PP","99","1","11","While machine-learning techniques have been widely used in smart industrial fault diagnosis, there is a major assumption that the source domain data (where the diagnosis model is trained) and the future target data (where the model is applied) must have the same distribution. However, this assumption may not hold in real industrial applications due to the changing operating conditions or mechanical wear. Recent advances have embedded the adversarial-learning mechanism into deep neural networks to reduce the distribution discrepancy between different domains to learn domain-invariant features and perform fault diagnosis. However, they only aligned the distributions of domains and neglected the fault-discriminative structure underlying the target domain, which leads to a decline in the diagnostic performance. In this article, a new method termed the fine-grained adversarial network-based domain adaptation (FANDA) is proposed to address the cross-domain industrial fault diagnosis problem. Different from the existing domain adversarial adaptation methods considering the domain discrepancy only, the features in FANDA are learned by competing against multiple-domain discriminators, which enable both a global alignment for two domains and a fine-grained alignment for each fault class across two domains. Thus, the fault-discriminative structure underlying two domains can be preserved in the adaptation process and the fault classification ability learned on the source domain can remain effective on the target data. Experiments on a mechanical bearing case and an industrial three-phase flow process case demonstrate the effectiveness of the proposed method.","","","10.1109/TASE.2019.2957232","Zhejiang Key Research and Development Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950281","Adversarial learning;deep neural network (DNN);domain adaptation;fault diagnosis.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Monocular Vision Aided Depth Map from RGB Images to Estimate of Localization and Support to Navigation of Mobile Robots","S. P. P. Da Silva; J. S. Almeida; E. F. Ohata; J. J. P. C. Rodrigues; V. H. C. De Albuquerque; P. P. R. Filho","Programa de Pós-Graduação em Engenharia de Teleinformática, Universidade Federal do Ceará, Fortaleza/CE, Brazil and Laboratório de Processamento de Imagens, Sinais e Computação Aplicada, Instituto Federal de Educação, Ciência e Tecnologia do Ceará, Fortaleza/CE, Brazil.; Programa de Pós-Graduação em Engenharia Elétrica, Universidade Federal do Ceará, Fortaleza/CE, Brazil and Laboratório de Processamento de Imagens, Sinais e Computação Aplicada, Instituto Federal de Educação, Ciência e Tecnologia do Ceará, Fortaleza/CE, Brazil.; Programa de Pós-Graduação em Engenharia de Teleinformática, Universidade Federal do Ceará, Fortaleza/CE, Brazil and Laboratório de Processamento de Imagens, Sinais e Computação Aplicada, Instituto Federal de Educação, Ciência e Tecnologia do Ceará, Fortaleza/CE, Brazil.; Universidade Federal do Piauí, Teresina/PI, Brazil and Instituto de Telecomunicações, Portugal.; Universidade de Fortaleza, Fortaleza/CE, Brazil.; Programa de Pós-Graduação em Engenharia de Teleinformática, Universidade Federal do Ceará, Fortaleza/CE, Brazil and Laboratório de Processamento de Imagens, Sinais e Computação Aplicada, Instituto Federal de Educação, Ciência e Tecnologia do Ceará, Fortaleza/CE, Brazil and Programa de Pós-Graduação em Engenharia Elétrica, Universidade Federal do Ceará, Fortaleza/CE, Brazil.","IEEE Sensors Journal","","2020","PP","99","1","1","Localization is one of the most challenging requirements needed for a mobile robots. Successful localization represents success in meeting the other principal requirements, such as perception and navigation. This article proposes a new approach for the localization of autonomous mobile robots using a Kinect sensor and the concept of Transfer Learning linked to Convolutional Neural Networks (CNNs). The images acquired from the sensor are applied in a mosaic form, which consists of the three color channels provided along with the depth information of the environment. Topological maps were used for indoor environment localization. The proposed computer vision system employed the Bayesian Classifier, k-Nearest Neighbor, Random Forest, Multi-layer Perceptron, and Support Vector Machine as the classifiers. The main focus of this work is the use of a unique configuration of RGB-D images transformed into a mosaic image, combined with the descriptive power of CNNs, in order to estimate the location of a mobile robot in an indoor environments. The results show that the proposed approach proved to be a convincing method for the tasks of localization and supporting the navigation of mobile robots; the results achieved 100% in Accuracy and in the F1-Score. The values obtained for the processing times were also suitable for a computer vision system, with 31.885ms, 0.040s, and 0.044s for the extraction, training, and classification stages, respectively. All these results were from the RGB-D mosaic images. The data demonstrated the relevance of the proposed work, providing a successful localization and, consequently, a successful navigation for mobile robots.","","","10.1109/JSEN.2020.2964735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951150","Mobile Robot Localization;Deep Transfer Learning;Convolutional Neural Network;RGB-D Mosaic Images;Machine Learning;Topological Maps;Indoor Environment;Computer Vision","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Coarse-to-Fine Semantic Segmentation From Image-Level Labels","L. Jing; Y. Chen; Y. Tian","Department of Computer Science, The Graduate Center, The City University of New York, New York, NY, USA; Department of Electrical Engineering, The City College of New York, The City University of New York, Xi’an, NY, China; Department of Computer Science, The Graduate Center, The City University of New York, New York, NY, USA","IEEE Transactions on Image Processing","","2020","29","","225","236","Deep neural network-based semantic segmentation generally requires large-scale cost extensive annotations for training to obtain better performance. To avoid pixel-wise segmentation annotations that are needed for most methods, recently some researchers attempted to use object-level labels (e.g., bounding boxes) or image-level labels (e.g., image categories). In this paper, we propose a novel recursive coarse-to-fine semantic segmentation framework based on only image-level category labels. For each image, an initial coarse mask is first generated by a convolutional neural network-based unsupervised foreground segmentation model and then is enhanced by a graph model. The enhanced coarse mask is fed to a fully convolutional neural network to be recursively refined. Unlike the existing image-level label-based semantic segmentation methods, which require labeling of all categories for images that contain multiple types of objects, our framework only needs one label for each image and can handle images that contain multi-category objects. Only trained on ImageNet, our framework achieves comparable performance on the PASCAL VOC dataset with other image-level label-based state-of-the-art methods of semantic segmentation. Furthermore, our framework can be easily extended to foreground object segmentation task and achieves comparable performance with the state-of-the-art supervised methods on the Internet object dataset.","","","10.1109/TIP.2019.2926748","National Science Foundation; Chinese Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8760555","Weakly supervised learning;semantic segmentation;foreground object segmentation;convolutional neural network;deep learning","Image segmentation;Semantics;Task analysis;Convolutional neural networks;Object segmentation;Urban areas;Training","convolutional neural nets;graph theory;image classification;image segmentation;learning (artificial intelligence);object detection","image-level labels;neural network-based semantic segmentation;large-scale cost extensive annotations;pixel-wise segmentation annotations;object-level labels;coarse-to-fine semantic segmentation framework;image-level category labels;initial coarse mask;convolutional neural network-based unsupervised foreground segmentation model;fully convolutional neural network;multicategory objects;image-level label-based state-of-the-art methods;foreground object segmentation task;image-level label-based semantic segmentation methods;PASCAL VOC dataset;ImageNet framework;Internet object dataset","","","55","","","","","IEEE","IEEE Journals"
"DSSNet: A Simple Dilated Semantic Segmentation Network for Hyperspectral Imagery Classification","B. Pan; X. Xu; Z. Shi; N. Zhang; H. Luo; X. Lan","School of Statistics and Data Science, Nankai University, Tianjin 300350, China.; College of Computer Science, Nankai University, Tianjin 300350, China (e-mail: xuxia@nankai.edu.cn).; Image Processing Center, School of Astronautics, Beihang University, Beijing 100191, China.; Shanghai Aerospace Electronic Technology Institute, Shanghai 201109, China.; Shanghai Aerospace Electronic Technology Institute, Shanghai 201109, China.; Shanghai Aerospace Electronic Technology Institute, Shanghai 201109, China.","IEEE Geoscience and Remote Sensing Letters","","2020","PP","99","1","5","Deep learning-based methods have presented a promising performance in the task of hyperspectral imagery classification (HSIC). However, recent methods usually are considered HSIC as a patchwise image classification problem and addressed it by giving a single label to the patch surrounding a pixel. In this letter, we propose a new semantic segmentation network that can directly label each pixel in an end-to-end manner. Compared with patchwise models, our method can significantly improve training effectiveness and reduce some manual parameters. Another challenge in HSIC is that the spatial resolution of hyperspectral imagery is relatively low; in that case, the pooling operation may result in resolution and coverage loss. To address this issue, we introduce dilated convolution to our model and construct a dilated semantic segmentation network (DSSNet). Different from some existing works, DSSNet is specially designed for HSIC without complicated architecture, and no pretrained models are required. The joint spatial-spectral information can be extracted via an end-to-end manner and, thus, avoid various preprocessing or postprocessing operations. Experiments on two public data sets have demonstrated the effectiveness of our improvements compared with some of the latest deep learning-based HSIC models.","","","10.1109/LGRS.2019.2960528","National Key R and D Program of China; National Natural Science Foundation of China; Beijing Natural Science Foundation; Shanghai Association for Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950204","Deep learning;dilated convolution;hyperspectral imagery classification (HSIC)","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Application of Machine Learning to Evaluate Insulator Surface Erosion","A. Ibrahim; A. Dalbah; A. Abualsaud; U. Tariq; A. El-Hag","Department of Electrical Engineering, College of Engineering, American University of Sharjah, Sharjah, United Arab Emirates; Department of Electrical Engineering, College of Engineering, American University of Sharjah, Sharjah, United Arab Emirates; Department of Electrical Engineering, College of Engineering, American University of Sharjah, Sharjah, United Arab Emirates; Department of Electrical Engineering, College of Engineering, American University of Sharjah, Sharjah, United Arab Emirates; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Instrumentation and Measurement","","2020","69","2","314","316","This article proposes a new automated inspection system that can estimate erosion in silicone rubber (SIR) samples using a computer vision-based method. In this work, we used SIR samples that were damaged under laboratory conditions. The proposed work is expected to classify SIR samples into one of three classes based on the degree of erosion following the IEC-60587 standard in defining failed samples. We use various preprocessing and feature extraction methods and classify using the artificial neural network (ANN) and deep convolutional neural network (CNN). We compare their performance and find that the best results were achieved using a deep CNN architecture. This work serves as a proof of concept and can be further extended to outdoor on-field test cases.","","","10.1109/TIM.2019.2956300","American University of Sharjah; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8915809","Artificial neural network (ANN);convolutional neural network (CNN);image processing;machine learning (ML)","Insulators;Feature extraction;Inspection;Polymers;Gray-scale;Artificial neural networks;Testing","","","","","12","IEEE","","","","IEEE","IEEE Journals"
"Deep Ensemble Object Tracking Based on Temporal and Spatial Networks","Z. Hu; H. Chen; G. Li","School of Electronic & Information Engineering, Nanjing University of Information Science & Technology, Nanjing 210044, China and Jiangsu Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science & Technology, Nanjing 210044, China.; School of Electronic & Information Engineering, Nanjing University of Information Science & Technology, Nanjing 210044, China.; School of Electronic & Information Engineering, Nanjing University of Information Science & Technology, Nanjing 210044, China.","IEEE Access","","2020","PP","99","1","1","In recent years, correlation filtering and deep learning have achieved good performance in object tracking. Correlation filtering is an efficient and real-time method because its formula provides a fast solution in the Fourier domain, but it does not benefit from end-to-end training. Although deep learning is an effective method for learning object representations, training deep networks online with one or a few examples is challenging. To address these problems, we propose a deep ensemble object tracking algorithm that fuses temporal and spatial information to improve algorithm precision and robustness. The framework of our algorithm includes four aspects: feature extraction, a baseline network, a branch network and adaptive ensemble learning. Feature extraction extracts the general object representation. The baseline network integrates feature extraction and a correlation filtering algorithm into a convolutional neural network for end-to-end training. The branch network is composed of a temporal network and a spatial network. The temporal and spatial networks capture the object temporal and spatial information and further refine the object position. Our algorithm only needs an initial frame to train all networks. Adaptive ensemble learning compensates for the object information deficiency and improves tracking accuracy. Many experiments on tracking benchmark datasets demonstrate that our algorithm performs favourably compared with state-of-the-art tracking algorithms.","","","10.1109/ACCESS.2020.2964100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950038","Object tracking;Feature extraction;Baseline network;Temporal and spatial networks;Adaptive ensemble learning","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"DeepOrganNet: On-the-Fly Reconstruction and Visualization of 3D / 4D Lung Models from Single-View Projections by Deep Deformation Network","Y. Wang; Z. Zhong; J. Hua","Department of Computer Science, Wayne State University, Detroit, MI; Department of Computer Science, Wayne State University, Detroit, MI; Department of Computer Science, Wayne State University, Detroit, MI","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","960","970","This paper introduces a deep neural network based method, i.e., DeepOrganNet, to generate and visualize fully high-fidelity 3D / 4D organ geometric models from single-view medical images with complicated background in real time. Traditional 3D / 4D medical image reconstruction requires near hundreds of projections, which cost insufferable computational time and deliver undesirable high imaging / radiation dose to human subjects. Moreover, it always needs further notorious processes to segment or extract the accurate 3D organ models subsequently. The computational time and imaging dose can be reduced by decreasing the number of projections, but the reconstructed image quality is degraded accordingly. To our knowledge, there is no method directly and explicitly reconstructing multiple 3D organ meshes from a single 2D medical grayscale image on the fly. Given single-view 2D medical images, e.g., 3D / 4D-CT projections or X-ray images, our end-to-end DeepOrganNet framework can efficiently and effectively reconstruct 3D / 4D lung models with a variety of geometric shapes by learning the smooth deformation fields from multiple templates based on a trivariate tensor-product deformation technique, leveraging an informative latent descriptor extracted from input 2D images. The proposed method can guarantee to generate high-quality and high-fidelity manifold meshes for 3D / 4D lung models; while, all current deep learning based approaches on the shape reconstruction from a single image cannot. The major contributions of this work are to accurately reconstruct the 3D organ shapes from 2D single-view projection, significantly improve the procedure time to allow on-the-fly visualization, and dramatically reduce the imaging dose for human subjects. Experimental results are evaluated and compared with the traditional reconstruction method and the state-of-the-art in deep learning, by using extensive 3D and 4D examples, including both synthetic phantom and real patient datasets. The efficiency of the proposed method shows that it only needs several milliseconds to generate organ meshes with 10K vertices, which has great potential to be used in real-time image guided radiation therapy (IGRT).","","","10.1109/TVCG.2019.2934369","NSF; Wayne State University Subaward; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809843","Deep deformation network;organ meshes;3D / 4D shapes;2D projections;single-view","Three-dimensional displays;Image reconstruction;Shape;Strain;Solid modeling;Biomedical imaging;Two dimensional displays","","","","","54","","","","","IEEE","IEEE Journals"
"An Energy-Efficient Deep Convolutional Neural Network Inference Processor With Enhanced Output Stationary Dataflow in 65-nm CMOS","J. Sim; S. Lee; L. Kim","School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2020","28","1","87","100","We propose a deep convolutional neural network (CNN) inference processor based on a novel enhanced output stationary (EOS) dataflow. Based on the observation that some activations are commonly used in two successive convolutions, the EOS dataflow employs dedicated register files (RFs) for storing such reused activation data to eliminate redundant memory accesses for highly energy-consuming SRAM banks. In addition, processing elements (PEs) are split into multiple small groups such that each group covers a tile of input activation map to increase the usability of activation RFs (ARFs). The processor has two different voltage/frequency domains. The computation domain with 512 PEs operates at near-threshold voltage (NTV) (0.4 V) and 60-MHz frequency to increase energy efficiency, while the rest of the processors including 848-KB SRAMs run at 0.7 V and 120-MHz frequency to increase both on-chip and off-chip memory bandwidths. The measurement results show that our processor is capable of running AlexNet at 831 GOPS/W, VGG-16 at 1151 GOPS/W, ResNet-18 at 1004 GOPS/W, and MobileNet at 948 GOPS/W energy efficiency.","","","10.1109/TVLSI.2019.2935251","National Research Foundation of Korea; Korean Government (MSIP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8822636","Convolutional neural network (CNN);dataflow;deep learning;energy-efficient processor;near-threshold voltage (NTV)","Earth Observing System;Radio frequency;Energy consumption;System-on-chip;Memory management;Registers;Random access memory","","","","","37","IEEE","","","","IEEE","IEEE Journals"
"Text Detection and Recognition for Images of Medical Laboratory Reports With a Deep Learning Approach","W. Xue; Q. Li; Q. Xue","Beijing Key Laboratory of Transportation Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Transportation Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Department of Burn and Plastic Surgery, The Fifth People’s Hospital of Datong, Datong, China","IEEE Access","","2020","8","","407","416","The adoption of electronic health records (EHRs) is an important step in the development of modern medicine. However, complete health records are not often available during treatment because of the functional problem of the EHR system or information barriers. This paper presents a deep-learning-based approach for textual information extraction from images of medical laboratory reports, which may help physicians solve the data-sharing problem. The approach consists of two modules: text detection and recognition. In text detection, a patch-based training strategy is applied, which can achieve the recall of 99.5% in the experiments. For text recognition, a concatenation structure is designed to combine the features from both shallow and deep layers in neural networks. The experimental results demonstrate that the text recognizer in our approach can improve the accuracy of multi-lingual text recognition. The approach will be beneficial for integrating historical health records and engaging patients in their own health care.","","","10.1109/ACCESS.2019.2961964","CERNET Innovation Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941040","Medical laboratory reports;textual information extraction;text detection;text recognition","","","","","","52","CCBY","","","","IEEE","IEEE Journals"
"Deep Private-Feature Extraction","S. A. Osia; A. Taheri; A. S. Shamsabadi; K. Katevas; H. Haddadi; H. R. Rabiee","Department of Computer Engineering, Advanced ICT Innovation Center, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Advanced ICT Innovation Center, Sharif University of Technology, Tehran, Iran; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, United Kingdom; Dyson School of Design Engineering, Imperial College London, London, United Kingdom; Dyson School of Design Engineering, Imperial College London, London, United Kingdom; Department of Computer Engineering, Advanced ICT Innovation Center, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Knowledge and Data Engineering","","2020","32","1","54","66","We present and evaluate Deep Private-Feature Extractor (DPFE), a deep model which is trained and evaluated based on information theoretic constraints. Using the selective exchange of information between a user's device and a service provider, DPFE enables the user to prevent certain sensitive information from being shared with a service provider, while allowing them to extract approved information using their model. We introduce and utilize the log-rank privacy, a novel measure to assess the effectiveness of DPFE in removing sensitive information and compare different models based on their accuracy-privacy trade-off. We then implement and evaluate the performance of DPFE on smartphones to understand its complexity, resource demands, and efficiency trade-offs. Our results on benchmark image datasets demonstrate that under moderate resource utilization, DPFE can achieve high accuracy for primary tasks while preserving the privacy of sensitive information.","","","10.1109/TKDE.2018.2878698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8515092","Feature extraction;privacy;information theory;deep learning","Data privacy;Feature extraction;Privacy;Data models;Task analysis;Training","","","","","59","IEEE","","","","IEEE","IEEE Journals"
"Pictionary-Style Word Guessing on Hand-Drawn Object Sketches: Dataset, Analysis and Deep Network Models","R. K. Sarvadevabhatla; S. Surya; T. Mittal; R. V. Babu","Department of Computational and Data Sciences, Video Analytics Lab, Indian Institute of Science, Bangalore, India; Department of Computational and Data Sciences, Video Analytics Lab, Indian Institute of Science, Bangalore, India; Department of Computational and Data Sciences, Video Analytics Lab, Indian Institute of Science, Bangalore, India; Department of Computational and Data Sciences, Video Analytics Lab, Indian Institute of Science, Bangalore, India","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2020","42","1","221","231","The ability of intelligent agents to play games in human-like fashion is popularly considered a benchmark of progress in Artificial Intelligence. In our work, we introduce the first computational model aimed at Pictionary, the popular word-guessing social game. We first introduce Sketch-QA, a guessing task. Styled after Pictionary, Sketch-QA uses incrementally accumulated sketch stroke sequences as visual data. Sketch-QA involves asking a fixed question (“What object is being drawn?”) and gathering open-ended guess-words from human guessers. We analyze the resulting dataset and present many interesting findings therein. To mimic Pictionary-style guessing, we propose a deep neural model which generates guess-words in response to temporally evolving human-drawn object sketches. Our model even makes human-like mistakes while guessing, thus amplifying the human mimicry factor. We evaluate our model on the large-scale guess-word dataset generated via Sketch-QA task and compare with various baselines. We also conduct a Visual Turing Test to obtain human impressions of the guess-words generated by humans and our model. Experimental results demonstrate the promise of our approach for Pictionary and similarly themed games.","","","10.1109/TPAMI.2018.2877996","Qualcomm Innovation Fellowship 2016; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8509167","Deep learning;pictionary;games;sketch;visual question answering","Games;Computational modeling;Visualization;Task analysis;Knowledge discovery;Robots","","","","","52","IEEE","","","","IEEE","IEEE Journals"
"PCL: Proposal Cluster Learning for Weakly Supervised Object Detection","P. Tang; X. Wang; S. Bai; W. Shen; X. Bai; W. Liu; A. Yuille","School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai Institute for Advanced Communication and Data Science, School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Departments of Cognitive Science and Computer Science, Johns Hopkins University, Baltimore, MD, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2020","42","1","176","191","Weakly Supervised Object Detection (WSOD), using only image-level annotations to train object detectors, is of growing importance in object recognition. In this paper, we propose a novel deep network for WSOD. Unlike previous networks that transfer the object detection problem to an image classification problem using Multiple Instance Learning (MIL), our strategy generates proposal clusters to learn refined instance classifiers by an iterative process. The proposals in the same cluster are spatially adjacent and associated with the same object. This prevents the network from concentrating too much on parts of objects instead of whole objects. We first show that instances can be assigned object or background labels directly based on proposal clusters for instance classifier refinement, and then show that treating each cluster as a small new bag yields fewer ambiguities than the directly assigning label method. The iterative instance classifier refinement is implemented online using multiple streams in convolutional neural networks, where the first is an MIL network and the others are for instance classifier refinement supervised by the preceding one. Experiments are conducted on the PASCAL VOC, ImageNet detection, and MS-COCO benchmarks for WSOD. Results show that our method outperforms the previous state of the art significantly.","","","10.1109/TPAMI.2018.2876304","National Natural Science Foundation of China; ONR; Hubei Scientific and Technical Innovation Key Project; HUST Academic Frontier Youth Team; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493315","Object detection;weakly supervised learning;convolutional neural network;multiple instance learning;proposal cluster","Proposals;Training;Streaming media;Detectors;Object detection;Electronic mail;Convolutional neural networks","","","","6","61","IEEE","","","","IEEE","IEEE Journals"
"Learning Temporal and Spatial Correlations Jointly: A Unified Framework for Wind Speed Prediction","Q. Zhu; J. Chen; D. Shi; L. Zhu; X. Bai; X. Duan; Y. Liu","State Key Laboratory of Advanced Electromagnetic Engineering and Technology, Huahzong University of Science and Technology, Wuhan, China; State Key Laboratory of Advanced Electromagnetic Engineering and Technology, Huahzong University of Science and Technology, Wuhan, China; State Key Laboratory of Advanced Electromagnetic Engineering and Technology, Huahzong University of Science and Technology, Wuhan, China; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN, USA; Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Advanced Electromagnetic Engineering and Technology, Huahzong University of Science and Technology, Wuhan, China; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN, USA","IEEE Transactions on Sustainable Energy","","2020","11","1","509","523","Leveraging both temporal and spatial correlations to predict wind speed remains one of the most challenging and less studied areas of wind speed prediction. In this paper, the problem of predicting wind speeds for multiple sites is investigated by using the spatio-temporal correlation. We proposed a deep architecture termed predictive spatio-temporal network (PSTN), which is a unified framework integrating a convolutional neural network (CNN) and a long short-term memory (LSTM). Initially, the spatial features are extracted from the spatial wind speed matrices by the CNN at the bottom of the model. Then, the LSTM captures the temporal dependencies among the spatial features extracted from contiguous time points. Finally, the predicted wind speeds are given by the last state of the top layer of the LSTM, which are generated by using the spatial features and temporal dependencies. Though composed of two kinds of architectures, PSTN is trained with one loss function in an end-to-end manner, which can learn temporal and spatial correlations jointly. Experiments for short-term predictions are conducted on real-world data, whose results demonstrate that PSTN outperforms prior methods.","","","10.1109/TSTE.2019.2897136","National Key Research Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633392","Convolutional neural networks (CNN);deep learning;spatial and temporal correlations;wind speed prediction","Wind speed;Correlation;Time series analysis;Communication networks;Wind farms;Feature extraction;Wind turbines","","","","","64","IEEE","","","","IEEE","IEEE Journals"
"User Authentication Based on Mouse Dynamics Using Deep Neural Networks: A Comprehensive Study","P. Chong; Y. Elovici; A. Binder","Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore; ST Engineering Electronics-SUTD Cyber Security Laboratory, Singapore University of Technology and Design, Singapore; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore","IEEE Transactions on Information Forensics and Security","","2020","15","","1086","1101","Recently conducted research demonstrated the potential use of mouse dynamics as a behavioral biometric for user authentication systems. However, the state-of-the-art methods in this field rely on classical machine learning methods that necessitate the design of hand crafted mouse features for feature extraction. To simplify the feature extraction process, we leverage various deep learning architectures for mouse movement sequences classification, including convolutional networks, recurrent networks, and a hybrid model which combines convolutional and recurrent layers. It is known that the training of these networks with random initialization of weights on small datasets will produce models that perform poorly. Therefore, we consider a two-dimensional convolutional neural network that allows transfer learning, which is a domain adaptation technique effective for learning on small datasets. Although employing such architecture may seem counterintuitive, since the temporal information is discarded from the input data, the architecture has outperformed all the other deep architectures investigated, as well as a classical machine learning method. In order to understand the features learned, we adopt the layer-wise relevance propagation (LRP) algorithm to compute relevance scores for each part of the mouse curves. In addition, the models are measured for their usability and effectiveness in realistic scenarios.","","","10.1109/TIFS.2019.2930429","ST Engineering Electronics-SUTD Cyber Security Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768407","Behavioral biometrics;CNN;LRP;LSTM;mouse dynamics;weighted learning","Mice;Authentication;Feature extraction;Computer architecture;Training;Support vector machines;Biological system modeling","","","","","29","IEEE","","","","IEEE","IEEE Journals"
"Spatio-Temporal Representation with Deep Neural Recurrent Network in MIMO CSI Feedback","X. Li; H. Wu","Center for Applied Mathematics, Tianjin University, Tianjin 300072, China.; Center for Applied Mathematics, Tianjin University, Tianjin 300072, China.","IEEE Wireless Communications Letters","","2020","PP","99","1","1","In multiple-input multiple-output (MIMO) systems, it is crucial of utilizing the available channel state information (CSI) at the transmitter for precoding to improve the performance of frequency division duplex (FDD) networks. One of the main challenges is to compress a large amount of CSI in CSI feedback transmission in massive MIMO systems. In this paper, we propose a deep learning (DL)-based approach that uses a deep recurrent neural network (RNN) to learn temporal correlation and adopts depthwise separable convolution to shrink the model. The feature extraction module is also elaborately devised by studying decoupled spatio-temporal feature representations in different structures. Experimental results demonstrate that the proposed approach outperforms existing DL-based methods in terms of recovery quality and accuracy, which can also achieve remarkable robustness at low compression ratio (CR).","","","10.1109/LWC.2020.2964550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951228","MIMO;CSI Feedback;FDD;Recurrent Neural Network;Spatio-Temporal Feature.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A Training-Efficient Hybrid-Structured Deep Neural Network With Reconfigurable Memristive Synapses","K. Bai; Q. An; L. Liu; Y. Yi","Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2020","28","1","62","75","The continued success in the development of neuromorphic computing has immensely pushed today’s artificial intelligence forward. Deep neural networks (DNNs), a brainlike machine learning architecture, rely on the intensive vector–matrix computation with extraordinary performance in data-extensive applications. Recently, the nonvolatile memory (NVM) crossbar array uniquely has unvailed its intrinsic vector–matrix computation with parallel computing capability in neural network designs. In this article, we design and fabricate a hybrid-structured DNN (hybrid-DNN), combining both depth-in-space (spatial) and depth-in-time (temporal) deep learning characteristics. Our hybrid-DNN employs memristive synapses working in a hierarchical information processing fashion and delay-based spiking neural network (SNN) modules as the readout layer. Our fabricated prototype in 130-nm CMOS technology along with experimental results demonstrates its high computing parallelism and energy efficiency with low hardware implementation cost, making the designed system a candidate for low-power embedded applications. From chaotic time-series forecasting benchmarks, our hybrid-DNN exhibits  $1.16\times $ – $13.77\times $  reduction on the prediction error compared to the state-of-the-art DNN designs. Moreover, our hybrid-DNN records 99.03% and 99.63% testing accuracy on the handwritten digit classification and the spoken digit recognition tasks, respectively.","","","10.1109/TVLSI.2019.2942267","Air Force Research Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866734","Chaotic time-series forecasting;deep neural network (DNN);delay feedback system;hybrid neural network;image classification;memristor;reservoir computing;speech recognition","Biological neural networks;Reservoirs;Memristors;Training;Computer architecture;Synapses;Nonvolatile memory","","","","","60","IEEE","","","","IEEE","IEEE Journals"
"Deep Salient Object Detection With Contextual Information Guidance","Y. Liu; J. Han; Q. Zhang; C. Shan","Key Laboratory of Electronic Equipment Structure Design, Ministry of Education, Xidian University, Xi’an, China; WMG Data Science, University of Warwick, Coventry, U.K.; Key Laboratory of Electronic Equipment Structure Design, Ministry of Education, Xidian University, Xi’an, China; Philips Research, Eindhoven, The Netherlands","IEEE Transactions on Image Processing","","2020","29","","360","374","Integration of multi-level contextual information, such as feature maps and side outputs, is crucial for Convolutional Neural Networks (CNNs)-based salient object detection. However, most existing methods either simply concatenate multi-level feature maps or calculate element-wise addition of multi-level side outputs, thus failing to take full advantages of them. In this paper, we propose a new strategy for guiding multi-level contextual information integration, where feature maps and side outputs across layers are fully engaged. Specifically, shallower-level feature maps are guided by the deeper-level side outputs to learn more accurate properties of the salient object. In turn, the deeper-level side outputs can be propagated to high-resolution versions with spatial details complemented by means of shallower-level feature maps. Moreover, a group convolution module is proposed with the aim to achieve high-discriminative feature maps, in which the backbone feature maps are divided into a number of groups and then the convolution is applied to the channels of backbone feature maps within each group. Eventually, the group convolution module is incorporated in the guidance module to further promote the guidance role. Experiments on three public benchmark datasets verify the effectiveness and superiority of the proposed method over the state-of-the-art methods.","","","10.1109/TIP.2019.2930906","National Natural Science Foundation of China; Science Foundation of Science and Technology on Complex System Control and Intelligent Agent Cooperative Laboratory; China Scholarship Council; Xidian University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8782147","Salient object detection;convolutional neural networks (CNNs);group convolution;multi-level contextual information integration","Feature extraction;Object detection;Convolution;Semantics;Saliency detection;Neural networks;Object recognition","convolutional neural nets;learning (artificial intelligence);object detection","deep salient object detection;contextual information guidance;multilevel feature maps;multilevel side outputs;multilevel contextual information integration;shallower-level feature maps;deeper-level side outputs;group convolution module;high-discriminative feature maps;backbone feature maps;convolutional neural network-based salient object detection;public benchmark datasets","","","77","","","","","IEEE","IEEE Journals"
"Deep Learning Research and Development Platform: Characterizing and Scheduling with QoS Guarantees on GPU Clusters","Z. Chen; W. Quan; M. Wen; J. Fang; J. Yu; C. Zhang; L. Luo","Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China","IEEE Transactions on Parallel and Distributed Systems","","2020","31","1","34","50","Deep learning (DL) has been widely adopted in various domains of artificial intelligence (AI), achieving dramatic developments in industry and academia. Besides giant AI companies, numerous small and medium-sized enterprises, institutes, and universities (EIUs) have focused on the research and development (R&D) of DL. Considering the high cost of datacenters and high performance computing (HPC) systems, EIUs prefer adopting off-the-shelf GPU clusters as a DL R&D platform for multiple users and developers to process diverse DL workloads. In such scenarios, the scheduling of multiple DL tasks on a shared GPU cluster is both significant and challenging in terms of efficiently utilizing limited resources. Existing schedulers cannot predict the resource requirements of diverse DL workloads, leading to the under-utilization of computing resources and a decline in user satisfaction. This paper proposes GENIE, a QoS-aware dynamic scheduling framework for a shared GPU cluster, which achieves users’ QoS guarantee and high system utilization. In accordance with an exhaustive characterization, GENIE analyzes the key factors that affect the performance of DL tasks and proposes a prediction model derived from lightweight profiling to estimate the processing rate and response latency for diverse DL workloads. Based on the prediction models, we propose a QoS-aware scheduling algorithm to identify the best placements for DL tasks and schedule them on the shared cluster. Experiments on a GPU cluster and large-scale simulations demonstrate that GENIE achieves a QoS-guarantee percentage improvement of up to 67.4 percent and a makespan reduction of up to 28.2 percent, compared to other baseline schedulers.","","","10.1109/TPDS.2019.2931558","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; NUDT Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778770","DL research and development platform;characterizing;scheduling;QoS-aware;GPU clusters","Task analysis;Graphics processing units;Research and development;Quality of service;Job shop scheduling;Training;Predictive models","","","","","42","IEEE","","","","IEEE","IEEE Journals"
"Generative Localization With Uncertainty Estimation Through Video-CT Data for Bronchoscopic Biopsy","C. Zhao; M. Shen; L. Sun; G. Yang","Hamlyn Centre, Imperial College London, London, U.K.; Hamlyn Centre, Imperial College London, London, U.K.; Oxford Robotics Institute, University of Oxford, Oxford, U.K.; Hamlyn Centre, Imperial College London, London, U.K.","IEEE Robotics and Automation Letters","","2020","5","1","258","265","Robot-assisted endobronchial intervention requires accurate localization based on both intra- and pre-operative data. Most existing methods achieve this by registering 2D videos with 3D CT models according to a defined similarity metric with local features. Instead, we formulate the bronchoscopic localization as a learning-based global localisation using deep neural networks. The proposed network consists of two generative architectures and one auxiliary learning component. The cycle generative architecture bridges the domain variance between the real bronchoscopic videos and virtual views derived from pre-operative CT data so that the proposed approach can be trained through a large number of generated virtual images but deployed through real images. The auxiliary learning architecture leverages complementary relative pose regression to constrain the search space, ensuring consistent global pose predictions. Most importantly, the uncertainty of each global pose is obtained through variational inference by sampling within the learned underlying probability distribution. Detailed validation results demonstrate the localization accuracy with reasonable uncertainty achieved and its potential clinical value. A demonstration video demo can be found on the website https://youtu.be/ci9LMY49aF8.","","","10.1109/LRA.2019.2955941","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8913461","Computer vision for medical robotics;medical robots and systems;localization;visual learning;deep learning in robotics and automation","Computed tomography;Uncertainty;Estimation;Videos;Cameras;Three-dimensional displays;Training","","","","","22","IEEE","","","","IEEE","IEEE Journals"
"Hierarchical Weakly Supervised Learning for Residential Area Semantic Segmentation in Remote Sensing Images","L. Zhang; J. Ma; X. Lv; D. Chen","College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2020","17","1","117","121","Residential-area segmentation is one of the most fundamental tasks in the field of remote sensing. Recently, fully supervised convolutional neural network (CNN)-based methods have shown superiority in the field of semantic segmentation. However, a serious problem for those CNN-based methods is that pixel-level annotations are expensive and laborious. In this study, a novel hierarchical weakly supervised learning (HWSL) method is proposed to realize pixel-level semantic segmentation in remote sensing images. First, a weakly supervised hierarchical saliency analysis is proposed to capture a sequence of class-specific hierarchical saliency maps by computing the gradient maps with respect to the middle layers of the CNN. Then, superpixels and low-rank matrix recovery are introduced to highlight the common salient areas and fuse class-specific saliency maps with adaptive weights. Finally, a subtraction operation between class-specific saliency maps is conducted to generate hierarchical residual saliency maps and fulfill residential-area segmentation. Comprehensive evaluations with two remote sensing data sets and comparison with seven methods validate the superiority of the proposed HWSL model.","","","10.1109/LGRS.2019.2914490","National Natural Science Foundation of China; National Natural Science Foundation of China; BNU Interdisciplinary Research Foundation for the First-Year Doctoral Candidates; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8720000","Deep learning;remote sensing;saliency analysis;semantic segmentation;weakly supervised","Image segmentation;Semantics;Remote sensing;Task analysis;Image color analysis;Feature extraction;Fuses","","","","","13","IEEE","","","","IEEE","IEEE Journals"
"Automatic Elevator Button Localization Using a Combined Detecting and Tracking Framework for Multi-Story Navigation","S. Jiang; W. Yao; M. Wong; M. Hang; Z. Hong; E. Kim; S. Joo; T. Kuc","College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong; School of Mechanical Engineering and Automation, Beihang University, Beijing, China; College of Information Technology, Shanghai Ocean University, Shanghai, China; College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Access","","2020","8","","1118","1134","Simultaneous localization and mapping (SLAM) is an important function for service robots to self-navigate modernized buildings. However, only a few existing applications allow them to automatically move between stories through elevator. Some approaches have accomplished with the aid of hardware; however, this study shows that computer vision can be a promising alternative for button localization. In this paper, we proposed a real-time multi-story SLAM system which overcomes the problem of detecting elevator buttons using a localization framework that combines tracking and detecting approaches. A two-stage deep neural network initially locates the original positions of the target buttons, and a part-based tracker follows the target buttons in real-time. A positive-negative classifier and deep learning neural network (particular for button shape detection) modify the tracker’s output in every frame. To allow the robot to self-navigate, a 2D grid mapping approach was used for the localization and mapping. Then, when the robot navigates a floor, the A* algorithm generates the shortest path. In the experiment, two dynamic scenes (which include common elevator button localization challenges) were used to evaluate the efficiency of our approach, and compared it with other state-of-the-art methods. Our approach was also tested on a prototype robot system to assesses how well it can navigate a multi-story building. The results show that our method could overcome the common background challenges that occur inside an elevator, and in doing so, it enables the mobile robot to autonomously navigate a multi-story building.","","","10.1109/ACCESS.2019.2958092","Korea Evaluation Institute of Industrial Technology; Ministry of Trade, Industry and Energy; Shanghai Rising-Star Program; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8926334","Elevator button localization;multi-story navigation;object detection;visual tracking;deep learning","Elevators;Target tracking;Simultaneous localization and mapping;Navigation;Task analysis;Machine learning","","","","","34","CCBY","","","","IEEE","IEEE Journals"
"Pilot-Assisted MIMO-V-OFDM systems: Compressed Sensing and Deep Learning Approaches","W. Zhang; X. Gao; Z. Li; Y. Shi","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China.; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China.; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China.; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China.","IEEE Access","","2020","PP","99","1","1","In this paper, we investigate the channel estimation and decoding methods exploiting the channel sparsity in pilot-assisted Multiple-Input Multiple-Output (MIMO) Vector Orthogonal Frequency Division Multiplexing (V-OFDM) systems. Based on the sparse multipath channels, we utilize orthogonal and non-orthogonal pilot schemes to design the compressed sensing (CS) measurement process. For the optimization of the sensing matrix, we discuss the influence of pilot search algorithms and evaluation criteria and propose a particle swarm optimization (PSO) based pilot search algorithm with the simplified evaluation criterion to improve the pilot design procedure. Meanwhile, the effect of pilot insertion on the Peak-to-Average Power Ratio (PAPR) is reduced by a particular precoding matrix method without affecting the decoding complexity. Simulation data are used to evaluate the classical sparsity adaptive matching (SAMP) algorithms and the proposed Variable Threshold SAMP (VTSAMP) algorithm, and the results show that the improved method has higher channel estimation accuracy with unknown sparsity. On the other hand, to overcome the complexity of CS-based decoding, we design the fully connected Deep Neural Network (FC-DNN) decoders, which combine the results of channel estimation results with the prevalent neural network technology. We observe that when the sparse channels are estimated accurately by CS methods, the proposed FC-DNN can achieve the same performance as the high-precision linear decoder by using the time-domain pilots and channel estimation results.","","","10.1109/ACCESS.2020.2964046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950181","MIMO;OFDM;Compressed sensing;Channel estimation;Peak to average power ratio;Decoding;Neural networks","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Neuromemristive Circuits for Edge Computing: A Review","O. Krestinskaya; A. P. James; L. O. Chua","Electrical and Computer Engineering Department, Nazarbayev University, Astana, Kazakhstan; Electrical and Computer Engineering Department, Nazarbayev University, Astana, Kazakhstan; Electrical Engineering and Computer Sciences Department, University of California at Berkeley, Berkeley, CA, USA","IEEE Transactions on Neural Networks and Learning Systems","","2020","31","1","4","23","The volume, veracity, variability, and velocity of data produced from the ever increasing network of sensors connected to Internet pose challenges for power management, scalability, and sustainability of cloud computing infrastructure. Increasing the data processing capability of edge computing devices at lower power requirements can reduce several overheads for cloud computing solutions. This paper provides the review of neuromorphic CMOS-memristive architectures that can be integrated into edge computing devices. We discuss why the neuromorphic architectures are useful for edge devices and show the advantages, drawbacks, and open problems in the field of neuromemristive circuits for edge computing.","","","10.1109/TNNLS.2019.2899262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667457","Cellular neural network (CeNN);convolutional neural network (CNN);deep learning neural network;hierarchical temporal memory (HTM);long short-term memory (LSTM);memristor circuits;memristors;neural networks;spiking neural networks (SNNs)","Computer architecture;Edge computing;Neuromorphics;Hardware;Memristors;Cloud computing;Data processing","","","","2","173","IEEE","","","","IEEE","IEEE Journals"
"Context-Aware Deep Spatiotemporal Network for Hand Pose Estimation From Depth Images","Y. Wu; W. Ji; X. Li; G. Wang; J. Yin; F. Wu","College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; AI Labs, Alibaba Group, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China","IEEE Transactions on Cybernetics","","2020","50","2","787","797","As a fundamental and challenging problem in computer vision, hand pose estimation aims to estimate the hand joint locations from depth images. Typically, the problems are modeled as learning a mapping function from images to hand joint coordinates in a data-driven manner. In this paper, we propose a context-aware deep spatiotemporal network, a novel method to jointly model the spatiotemporal properties for hand pose estimation. Our proposed network is able to learn the representations of the spatial information and the temporal structure from the image sequences. Moreover, by adopting the adaptive fusion method, the model is capable of dynamically weighting different predictions to lay emphasis on sufficient context. Our method is examined on two common benchmarks, the experimental results demonstrate that our proposed approach achieves the best or the second-best performance with the state-of-the-art methods and runs in 60 fps.","","","10.1109/TCYB.2018.2873733","National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; National Basic Research Program of China (973 Program); Key Research and Development Program of Zhejiang Province; Zhejiang University; Alibaba-Zhejiang University Joint Institute of Frontier Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8563043","Adaptive fusion;context-aware deep spatiotemporal network (CADSTN);hand pose estimation","Feature extraction;Pose estimation;Spatiotemporal phenomena;Image sequences;Context modeling;Adaptation models;Data mining","","","","","47","IEEE","","","","IEEE","IEEE Journals"
"Learning Discriminative Embedding for Hyperspectral Image Clustering Based on Set-to-Set and Sample-to-Sample Distances","Y. Qin; L. Bruzzone; B. Li","College of Electronic Science, National University of Defense Technology, Changsha, China; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; College of Electronic Science, National University of Defense Technology, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","","2020","58","1","473","485","Recently, deep learning techniques have been introduced to address hyperspectral image (HSI) classification problems and have achieved the state-of-the-art performances. In this article, we propose a novel clustering algorithm for HSI based on learning embedding using the set-to-set and sample-to-sample distances (LSSDs). This technique consists of four main components: 1) oversegmentation; 2) generation of set-to-set and sample-to-sample distances; 3) learning embedding by training a siamese network; and 4) density-based spectral clustering. First, the HSI is oversegmented into superpixels by using the entropy rate superpixel (ERS) algorithm. Second, the set-to-set distances are obtained by representing the segmented sets of samples as affine hull (AH) models, whereas the sample-to-sample distances are computed by employing the local covariance matrix representation (LCMR) method. Third, sample pairs with the smallest and largest similarities are extracted according to the two distances. Then, these pairs are fed into the siamese multilayer perceptron (MLP) network and discriminative embeddings are learned by training the network with contrastive loss. Finally, density-based spectral clustering is applied to the deep embedding to obtain clustering results. Experimental results on three real HSIs demonstrate that the proposed method can achieve better performance than the considered baseline methods.","","","10.1109/TGRS.2019.2937204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839737","Affine hull (AH);clustering;distance learning;hyperspectral image (HSI);local covariance matrix representation (LCMR);remote sensing;sample-to-sample distance;set-to-set distance;siamese network","Clustering algorithms;Clustering methods;Training;Hyperspectral imaging;Computational modeling;Feature extraction","","","","","49","IEEE","","","","IEEE","IEEE Journals"
"Dual Intercommunication Network: Enabling Interhemispheric Communications in Hemisphere-Inspired ANNs","Y. Wu","School of Computer Science, Fudan University, Shanghai, China","IEEE Access","","2020","8","","526","534","The human brain has been a main source of inspiration for designing deep learning models. Recently, inspired by the specialized functions of two cerebral hemispheres in processing low and high spatial frequency information, some dual-path neural networks with global and local branches have been proposed to deal with both coarse- and fine-grained visual tasks simultaneously. However, in existing works, the interhemispheric communication mechanism, which is responded by the corpus callosum, the largest white matter structure in the human brain that connecting the left and right cerebral hemispheres, is still not fully explored and exploited. This paper aims to explore how the corpus callosum can inspire us to enable transfer and integration of information between global and local branches in hemisphere-inspired artificial neural networks, such that one branch can leverage the other’s learned knowledge and benefit each other. To this end, we propose a gated intercommunication unit to selectively transfer useful knowledge between the two branches via attention mechanisms to alleviate the negative transfer. Experiments on sb-MNIST and two pedestrian attribute datasets show that the proposed method outperforms the compared ones in most cases.","","","10.1109/ACCESS.2019.2961933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941050","Attribute recognition;computer vision;deep learning;multi-task learning","","","","","","53","CCBY","","","","IEEE","IEEE Journals"
"Automatic Dataset Expansion With Structured Feature Learning for Human Lying Pose Detection","D. Xia; L. Zhao; F. Guo; X. Chen","School of Big Data and Computer Science, Guizhou Normal University, Guiyang, China; Engineering Laboratory for Applied Technology of Big Data in Education, Guizhou Normal University, Guiyang, China; School of Big Data and Computer Science, Guizhou Normal University, Guiyang, China; School of Big Data and Computer Science, Guizhou Normal University, Guiyang, China","IEEE Access","","2020","8","","1080","1090","In this study, we developed a framework to localize human lying poses by a camera positioned above. Our framework is motivated by the fact that detecting lying poses is fundamentally more difficult than detecting pedestrians or localizing nondeformable objects such as cars, roads, and buildings due to the large number of poses, orientations, and scales that a human lying on the ground can take. An important problem with lying pose detection is the training dataset, which hardly accounts for each possible body configuration. As a solution, we propose a geometric expansion procedure that uses a virtual camera to increase the number of training images. We also use a Gibbs sampler to generate more training samples in the feature space on which the system can train its model. Once the training is completed, detection is performed on a multiscale and multirotational space. Because our framework accommodates a variety of object detection systems, we report the results for the Faster R-CNN, FPN, and RefineDet models. The results show that using automatic dataset expansion models systematically improves the results.","","","10.1109/ACCESS.2019.2962100","National Natural Science Foundation of China; Guizhou Science and Technology Department; Guizhou Normal University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941100","Human lying pose detection;automatic dataset expansion;perspective transformation;gibbs sampling;deep learning","","","","","","50","CCBY","","","","IEEE","IEEE Journals"
"PWStableNet: Learning Pixel-wise Warping Maps for Video Stabilization","M. Zhao; Q. Ling","Dept. of Automation, University of Science and Technology of China, Hefei, Anhui 230027, P. R. China.; Dept. of Automation, University of Science and Technology of China, Hefei, Anhui 230027, P. R. China.","IEEE Transactions on Image Processing","","2020","PP","99","1","1","As the videos captured by hand-held cameras are often perturbed by high-frequency jitters, stabilization of these videos is an essential task. Many video stabilization methods have been proposed to stabilize shaky videos. However, most methods estimate one global homography or several homographies based on fixed meshes to warp the shaky frames into their stabilized views. Due to the existence of parallax, such single or a few homographies can not well handle the depth variation. In contrast to these traditional methods, we propose a novel video stabilization network, called PWStableNet, which comes up pixel-wise warping maps, i.e., potentially different warping for different pixels, and stabilizes each pixel to its stabilized view. To our best knowledge, this is the first deep learning based pixel-wise video stabilization. The proposed method is built upon a multi-stage cascade encoder-decoder architecture and learns pixel-wise warping maps from consecutive unstable frames. Inter-stage connections are also introduced to add feature maps of a former stage to the corresponding feature maps at a latter stage, which enables the latter stage to learn the residual from the feature maps of former stages. This cascade architecture can produce more precise warping maps at latter stages. To ensure the correct learning of pixel-wise warping maps, we use a well-designed loss function to guide the training procedure of the proposed PWStableNet. The proposed stabilization method achieves comparable performance with traditional methods, but stronger robustness and much faster processing speed. Moreover, the proposed stabilization method outperforms some typical CNN-based stabilization methods, especially in videos with strong parallax. Codes will be provided at https://github.com/mindazhao/pix-pix-warping-video-stabilization.","","","10.1109/TIP.2019.2963380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951447","Video stabilization;pixel-wise warping;cascade networks","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Semi-Supervised Learning for Fine-Grained Classification With Self-Training","O. T. Nartey; G. Yang; J. Wu; S. K. Asare","Big Data Research Center, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Big Data Research Center, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Guangxi Key Laboratory of Hybrid Computation and IC Design Analysis, Guangxi University for Nationalities, Nanning, China; School of Electronic Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Access","","2020","8","","2109","2121","Semi-supervised learning is a machine learning approach that tackles the challenge of having a large set of unlabeled data and few labeled ones. In this paper we adopt a semi-supervised self-training method to increase the amount of training data, prevent overfitting and improve the performance of deep models by proposing a novel selection algorithm that prevents mistake reinforcement which is a common thing in conventional self-training models. The model leverages, unlabeled data and specifically, after each training, we first generate pseudo-labels on the unlabeled set to be added to the labeled training samples. Next, we select the top- $k$  most-confident pseudo-labeled images from each unlabeled class with their pseudo-labels and update the training data, and retrain the network on the updated training data. The method improves the accuracy in two-fold; bridging the gap in the appearance of visual objects, and enlarging the training set to meet the demands of deep models. We demonstrated the effectiveness of the model by conducting experiments on four state-of-the-art fine-grained datasets, which include Stanford Dogs, Stanford Cars, 102-Oxford flowers, and CUB-200-2011. We further evaluated the model on some coarse-grain data. Experimental results clearly show that our proposed framework has better performance than some previous works on the same data; the model obtained higher classification accuracy than most of the supervised learning models.","","","10.1109/ACCESS.2019.2962258","National Natural Science Foundation of China; Science and Technology Program of Guangxi; Science and Technology Major Project of Guangxi; special Fund for Scientific and Technological Bases and Talents of Gungxi; Special Fund for Bagui Scholars of Guangxi; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943213","Fine-grained classification;pseudo-labels;self-training;semi-supervised learning","","","","","","52","CCBY","","","","IEEE","IEEE Journals"
"One-Dimensional Deep Attention Convolution Network (ODACN) for Signals Classification","S. Yang; C. Yang; D. Feng; X. Hao; M. Wang","School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; School of Aerospace Science and Technology, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Access","","2020","8","","2804","2812","Handcraft features are commonly used for signal classification, which is a time-consuming feature engineering. In order to develop a general and robust feature learning method for radio signals, a novel One-dimensional Deep Attention Convolution Network (ODACN) is proposed to automatically extract discriminative features and classify various kinds of signals. First, one-dimensional (1-D) sparse filters are designed to learn hierarchical features of raw signals. Second, an attention layer is constructed to weight and assemble feature maps, to derive more context-relevant representation. By using simple 1-D filtering, ODACN is characteristic of less parameters and lower computation complexity than traditional Convolutional Neural Networks (CNNs). Moreover, feature attention can mimic a succession of partial glimpses of humans and focus on context parts of signals, thus helps in recognizing signals even at low Signal-to-Noise Ratio (SNR). Some experiments are taken to classify 31 kinds of signals with different modulation and channel coding types, and the results show that ODACN can achieve accurate classification of very similar signals, without any prior knowledge and manual operation.","","","10.1109/ACCESS.2019.2958131","National Natural Science Foundation of China; Equipment pre-research project of the 13th Five-Years Plan; Major Research Plan in Shaanxi Province of China; Foundation of the State Key Laboratory of CEMEE; Science Basis Research Program in Shaanxi Province of China; Key Scientific and Technological Innovation Team of Shaanxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8926472","Signal classification;feature learning;one-dimensional convolution neural network;attention layer","Feature extraction;Convolution;Modulation;Neurons;Pattern classification;Transforms;Filtering","","","","","41","CCBY","","","","IEEE","IEEE Journals"
"A Novel Lane-Changing Decision Model for Autonomous Vehicles Based on Deep Autoencoder Network and XGBoost","X. Gu; Y. Han; J. Yu","Key Laboratory of High Efficiency and Clean Mechanical Manufacture, Shandong University, Ministry of Education, Jinan 250061, China and School of Mechanical Engineering, Shandong University, Jinan 250061, China.; Key Laboratory of High Efficiency and Clean Mechanical Manufacture, Shandong University, Ministry of Education, Jinan 250061, China and School of Mechanical Engineering, Shandong University, Jinan 250061, China.; Key Laboratory of High Efficiency and Clean Mechanical Manufacture, Shandong University, Ministry of Education, Jinan 250061, China and School of Mechanical Engineering, Shandong University, Jinan 250061, China.","IEEE Access","","2020","PP","99","1","1","Lane-changing (LC) is a critical task for autonomous driving, especially in complex dynamic environments. Numerous automatic LC algorithms have been proposed. This topic, however, has not been sufficiently addressed in existing on-road manoeuvre decision methods. Therefore, this paper presents a novel LC decision (LCD) model that gives autonomous vehicles the ability to make human-like decisions. This method combines a deep autoencoder (DAE) network with the XGBoost algorithm. First, a DAE is utilized to build a robust multivariate reconstruction model using time series data from multiple sensors; then, the reconstruction error of the DAE trained with normal data is analysed for LC identification (LCI) and training data extraction. Then, to address the multi-parametric and nonlinear problem of the autonomous LC decision-making process, an XGBoost algorithm with Bayesian parameter optimization is adopted. Meanwhile, to fully train our learning model with large-scale datasets, we proposed an online training strategy that updates the model parameters with data batches. The experimental results illustrate that the DAE-based LCI model is able to accurately identify the LC behaviour of vehicles. Furthermore, with the same input features, the proposed XGBoost-based LCD model achieves better performance than other popular approaches. Moreover, a simulation experiment is performed to verify the effectiveness of the decision model.","","","10.1109/ACCESS.2020.2964294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950329","Autonomous vehicle;lane-changing identification;lane-changing decision-making;deep autoencoder network;XGBoost","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Model-Free Tracker for Multiple Objects Using Joint Appearance and Motion Inference","C. Liu; R. Yao; S. H. Rezatofighi; I. Reid; Q. Shi","School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Image Processing","","2020","29","","277","288","Model-free tracking is a widely accepted approach to track an arbitrary object in a video using a single frame annotation with no further prior knowledge about the object of interest. Extending this problem to track multiple objects is really challenging because: 1) the tracker is not aware of the objects' type while trying to distinguish them from background (detection task) and 2) the tracker needs to distinguish one object from other potentially similar objects (data association task) to generate stable trajectories. In order to track multiple arbitrary objects, most existing model-free tracking approaches rely on tracking each target individually by updating their appearance model independently. Therefore, in this scenario they often fail to perform well due to confusion between the appearance of similar objects, their sudden appearance changes and occlusion. To tackle this problem, we propose to use both appearance and motion models, and to learn those jointly using graphical models and the deep neural networks features. We introduce an indicator variable to predict sudden appearance change and/or occlusion. When these happen, our model does not update the appearance model thus avoiding using the background and/or incorrect object to update the appearance of the object of interest mistakenly, and relies on our motion model to track. Moreover, we consider the correlation among all targets, and seek the joint optimal locations for all targets simultaneously as a graphical model inference problem. We learn the joint parameters for both appearance model and motion model in an online fashion under the framework of LaRank. Experiment results show that our method achieved superior performance compared to the competitive methods.","","","10.1109/TIP.2019.2928123","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; Six Talent Peaks Project in Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765607","Model-free tracking;multiple objects tracking;joint learning;joint optimal inference;graphical models","Target tracking;Visualization;Adaptation models;Feature extraction;Task analysis;Object tracking","inference mechanisms;learning (artificial intelligence);motion estimation;neural nets;object recognition;object tracking;video signal processing","multiple objects model-free tracker;joint appearance;motion inference;stable trajectories generation;sudden appearance changes;graphical models;deep neural networks;targets correlation;joint optimal locations;graphical model inference problem;LaRank framework","","","50","","","","","IEEE","IEEE Journals"
"On Kernel Method–Based Connectionist Models and Supervised Deep Learning Without Backpropagation","S. Duan; S. Yu; Y. Chen; J. C. Principe","Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611, U.S.A.; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611, U.S.A.; Department of Mathematics, University of Florida, Gainesville, FL 32611, U.S.A.; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611, U.S.A.","Neural Computation","","2020","32","1","97","135","We propose a novel family of connectionist models based on kernel machines and consider the problem of learning layer by layer a compositional hypothesis class (i.e., a feedforward, multilayer architecture) in a supervised setting. In terms of the models, we present a principled method to “kernelize” (partly or completely) any neural network (NN). With this method, we obtain a counterpart of any given NN that is powered by kernel machines instead of neurons. In terms of learning, when learning a feedforward deep architecture in a supervised setting, one needs to train all the components simultaneously using backpropagation (BP) since there are no explicit targets for the hidden layers (Rumelhart, Hinton, & Williams, 1986). We consider without loss of generality the two-layer case and present a general framework that explicitly characterizes a target for the hidden layer that is optimal for minimizing the objective function of the network. This characterization then makes possible a purely greedy training scheme that learns one layer at a time, starting from the input layer. We provide instantiations of the abstract framework under certain architectures and objective functions. Based on these instantiations, we present a layer-wise training algorithm for an l-layer feedforward network for classification, where l≥2 can be arbitrary. This algorithm can be given an intuitive geometric interpretation that makes the learning dynamics transparent. Empirical results are provided to complement our theory. We show that the kernelized networks, trained layer-wise, compare favorably with classical kernel machines as well as other connectionist models trained by BP. We also visualize the inner workings of the greedy kernelized models to validate our claim on the transparency of the layer-wise algorithm.","","","10.1162/neco_a_01250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8937117","","","","","","","","Traditional","","","","MITP",""
"Sarcasm Detection Using Multi-Head Attention Based Bidirectional LSTM","A. Kumar; V. T. Narapareddy; V. A. Srikanth; A. Malapati; L. B. M. Neti","Birla Institute of Technology & Science, Pilani-Hyderabad, Hyderabad 500078, India. (e-mail: avinash.akumar@gmail.com); Birla Institute of Technology & Science, Pilani-Hyderabad, Hyderabad 500078, India.; Birla Institute of Technology & Science, Pilani-Hyderabad, Hyderabad 500078, India.; Birla Institute of Technology & Science, Pilani-Hyderabad, Hyderabad 500078, India.; Birla Institute of Technology & Science, Pilani-Hyderabad, Hyderabad 500078, India.","IEEE Access","","2020","PP","99","1","1","Sarcasm is often used to express a negative opinion using positive or intensified positive words in social media. This intentional ambiguity makes sarcasm detection, an important task of sentiment analysis. Sarcasm detection is considered a binary classification problem wherein both feature-rich traditional models and deep learning models have been successfully built to predict sarcastic comments. In previous research works, models have been built using lexical, semantic and pragmatic features. We extract the most significant features and build a feature-rich SVM that outperforms these models. In this paper, we introduce a multi-head attention-based bidirectional long-short memory (MHA-BiLSTM) network to detect sarcastic comments in a given corpus. The experiment results reveal that a multi-head attention mechanism enhances the performance of BiLSTM, and it performs better than feature-rich SVM models.","","","10.1109/ACCESS.2019.2963630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949523","Sarcasm detection;deep learning;self-attention;machine learning;social data","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Representation Learning of Knowledge Graphs with Entity Attributes","Z. Zhang; L. Cao; X. Chen; W. Tang; Z. Xu; Y. Meng","Command & Control Engineering College, Army Engineering University of PLA, Nanjing, CO 210000 China and PLA Unit 73671, Lu’an, CO 237000 China.; Command & Control Engineering College, Army Engineering University of PLA, Nanjing, CO 210000 China.; Command & Control Engineering College, Army Engineering University of PLA, Nanjing, CO 210000 China.; Command & Control Engineering College, Army Engineering University of PLA, Nanjing, CO 210000 China.; Command & Control Engineering College, Army Engineering University of PLA, Nanjing, CO 210000 China.; PLA Unit 31121, Nanjing, CO 210000 China.","IEEE Access","","2020","PP","99","1","1","Most of the existing knowledge representation learning methods project the entities and relations represented by symbols in the knowledge graph into the low-dimensional vector space from the perspective of the structure and semantics of triples, and express the complex relations between entities and relations with dense low-dimensional vectors. However, triples in the knowledge graph not only contain relation triples, but also contain a large number of attribute triples. Existing knowledge representation methods often confuse these two kinds of triples and pay little attention to the semantic information contained in attributes and attribute values. In this paper, a novel representation learning method which makes use of the attribute information of entities is proposed. Specifically, deep convolutional neural network model is used to encode attribute information of entities, and both attribute information and triple structure information are utilized to learn knowledge representation, and then generate attribute-based representation of entities. The knowledge graph completion task was used to evaluate this method, and the experimental results on open data sets FB15K and FB24k showed that the attribute-embodied knowledge representation learning model outperforms the other baselines.","","","10.1109/ACCESS.2020.2963990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950182","attribute;knowledge graph;representation learning","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Improved Robust Video Saliency Detection Based on Long-Term Spatial-Temporal Information","C. Chen; G. Wang; C. Peng; X. Zhang; H. Qin","College of Computer Science and Technology, Qingdao University, Qingdao, China; College of Computer Science and Technology, Qingdao University, Qingdao, China; College of Computer Science and Technology, Qingdao University, Qingdao, China; College of Computer Science and Technology, Qingdao University, Qingdao, China; Computer Science Department, Stony Brook University, Stony Brook, NY, USA","IEEE Transactions on Image Processing","","2020","29","","1090","1100","This paper proposes to utilize supervised deep convolutional neural networks to take full advantage of the long-term spatial-temporal information in order to improve the video saliency detection performance. The conventional methods, which use the temporally neighbored frames solely, could easily encounter transient failure cases when the spatial-temporal saliency clues are less-trustworthy for a long period. To tackle the aforementioned limitation, we plan to identify those beyond-scope frames with trustworthy long-term saliency clues first and then align it with the current problem domain for an improved video saliency detection.","","","10.1109/TIP.2019.2934350","National Natural Science Foundation of China; Natural Science Foundation of Shandong Province; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811767","Video saliency detection;spatial-temporal saliency consistency;low-level saliency clues;long-term information revealing","Saliency detection;Deep learning;Quality assessment;Trajectory;Computational modeling;Color;Training","convolutional neural nets;feature extraction;image motion analysis;image representation;object detection;video signal processing","video saliency detection;long-term spatial-temporal information;supervised deep convolutional neural networks;temporally neighbored frames;spatial-temporal saliency clues;long-term saliency clues","","","68","","","","","IEEE","IEEE Journals"
"Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk","P. Hand; V. Voroninski","Department of Mathematics, Northeastern University, Boston, MA, USA; Helm.ai, Menlo Park, CA, USA","IEEE Transactions on Information Theory","","2020","66","1","401","418","We examine the theoretical properties of enforcing priors provided by generative deep neural networks via empirical risk minimization. In particular we consider two models, one in which the task is to invert a generative neural network given access to its last layer and another in which the task is to invert a generative neural network given only compressive linear observations of its last layer. We establish that in both cases, in suitable regimes of network layer sizes and a randomness assumption on the network weights, that the non-convex objective function given by empirical risk minimization does not have any spurious stationary points. That is, we establish that with high probability, at any point away from small neighborhoods around two scalar multiples of the desired solution, there is a descent direction. Hence, there are no local minima, saddle points, or other stationary points outside these neighborhoods. These results constitute the first theoretical guarantees which establish the favorable global geometry of these non-convex optimization problems, and they bridge the gap between the empirical success of enforcing deep generative priors and a rigorous understanding of non-linear inverse problems.","","","10.1109/TIT.2019.2935447","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8801854","Information theory;learning;optimization;probability;generative models","","","","","1","78","IEEE","","","","IEEE","IEEE Journals"
"Bathymetry Model Based on Spectral and Spatial Multifeatures of Remote Sensing Image","Y. Wang; X. Zhou; C. Li; Y. Chen; L. Yang","Department of Engineering Center, First Institute of Oceanography, Ministry of Natural Resources, Qingdao, China; Department of Engineering Center, First Institute of Oceanography, Ministry of Natural Resources, Qingdao, China; SenseTime Group Limited, Beijing, China; Department of Engineering Center, First Institute of Oceanography, Ministry of Natural Resources, Qingdao, China; Department of Engineering Center, First Institute of Oceanography, Ministry of Natural Resources, Qingdao, China","IEEE Geoscience and Remote Sensing Letters","","2020","17","1","37","41","Multispectral methods for remote sensing image have been widely applied to shallow water bathymetry by researchers. In nonideal conditions, even with the same spectral radiance, the points still have a very wide range of water depths. This means that spectral features alone are insufficient for water bathymetry. Hence, we need to extract other valuable features from a remote sensing image. This letter introduces a spatial feature for water bathymetry using remote sensing images. We propose a model that utilizes a multilayer perceptron (MLP) to integrate the spectral and spatial location features. Experimental results demonstrate that the proposed model yields a substantial performance improvement. The mean relative error is only 8.41%, and the root mean square error is reduced by 34%–68% when compared with three other models. Furthermore, the proposed model addresses well the problems caused by heterogeneous bottom types.","","","10.1109/LGRS.2019.2915122","National Basic Research Program of China (973 Program); Satellite Remote Sensing Mapping Application; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8732587","Bathymetry;multilayer perceptron (MLP);multiple features;remote sensing","Remote sensing;Training;Sea measurements;Neural networks;Feature extraction;Machine learning algorithms;Deep learning","","","","","15","IEEE","","","","IEEE","IEEE Journals"
"Burst Ranking for Blind Multi-Image Deblurring","F. A. G. Peña; P. D. Marrero Fernández; T. Ing Ren; J. de Jesus Gomes Leandro; R. M. Nishihara","Center for Informatics, Federal University of Pernambuco, Recife, Brazil; Center for Informatics, Federal University of Pernambuco, Recife, Brazil; Center for Informatics, Federal University of Pernambuco, Recife, Brazil; Motorola Mobility LLC (a Lenovo Company), São Paulo, Brazil; Motorola Mobility LLC (a Lenovo Company), São Paulo, Brazil","IEEE Transactions on Image Processing","","2020","29","","947","958","We propose a new incremental aggregation algorithm for multi-image deblurring with automatic image selection. The primary motivation is that current burst deblurring methods do not handle well situations in which misalignment or out-of-context frames are present in the burst. These real-life situations result in poor reconstructions or manual selection of the images that are used to deblur. Automatically selecting the best frames within the burst to improve the base reconstruction is challenging because the number of possible images fusions is equal to the power set cardinal. Here, we approach the multi-image deblurring problem as a two steps process. First, we successfully learn a comparison function to rank a burst of images using a deep convolutional neural network. Then, an incremental Fourier burst accumulation with a reconstruction degradation mechanism is applied fusing only less blurred images that are sufficient to maximize the reconstruction quality. Experiments with the proposed algorithm have shown superior results when compared to other similar approaches, outperforming other methods described in the literature in previously described situations. We validate our findings on several synthetic and real datasets.","","","10.1109/TIP.2019.2936073","Fundação de Amparo à Ciência e Tecnologia do Estado de Pernambuco; Motorola; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818649","Blurred images sorting;deep learning;multi-image deblurring","Image reconstruction;Kernel;Degradation;Sorting;Measurement;Training;Indexes","convolutional neural nets;Fourier analysis;image fusion;image reconstruction;image restoration","incremental aggregation algorithm;automatic image selection;deep convolutional neural network;incremental Fourier burst accumulation;reconstruction degradation mechanism;blurred image reconstruction quality;burst deblurring methods;image fusions;blind multiimage deblurring problem","","","34","","","","","IEEE","IEEE Journals"
"Integrating Gray Data Preprocessor and Deep Belief Network for Day-Ahead PV Power Output Forecast","G. W. Chang; H. Lu","Department of Electrical Engineering, National Chung Cheng University, Chia-Yi, Taiwan; Department of Electrical Engineering, National Chung Cheng University, Chia-Yi, Taiwan","IEEE Transactions on Sustainable Energy","","2020","11","1","185","194","Generation output forecasting is a crucial task for planning and sizing of a photovoltaic (PV) power plant. The purpose of this paper is to present an effective model for day-ahead forecasting PV power output of a plant based on deep belief network (DBN) combined with gray theory-based data preprocessor (GT-DBN), where the DBN attempts to learn high-level abstractions in historical PV output data by utilizing hierarchical architectures. Test results obtained by the proposed model are compared with those obtained by other five forecasting methods including autoregressive integrated moving average model, back propagation neural network, radial basis function neural network, support vector regression, and DBN alone. It shows that the proposed model is superior to other models in forecasting accuracy and is suitable for day-ahead PV power output prediction.","","","10.1109/TSTE.2018.2888548","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8580409","Neural networks;power generation planning;renewable energy;supervised learning;time series analysis","Forecasting;Predictive models;Support vector machines;Autoregressive processes;Data models;Training;Neural networks","","","","1","29","IEEE","","","","IEEE","IEEE Journals"
"Degraded Image Semantic Segmentation With Dense-Gram Networks","D. Guo; Y. Pei; K. Zheng; H. Yu; Y. Lu; S. Wang","Department of Computer Science and Engineering, University of South Carolina, Columbia, SC, USA; Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Department of Computer Science and Engineering, University of South Carolina, Columbia, SC, USA; Department of Computer Science, University of Texas-Rio Grande Valley, Edinburg, TX, USA; Department of Computer Science and Engineering, University of South Carolina, Columbia, SC, USA; Department of Computer Science and Engineering, University of South Carolina, Columbia, SC, USA","IEEE Transactions on Image Processing","","2020","29","","782","795","Degraded image semantic segmentation is of great importance in autonomous driving, highway navigation systems, and many other safety-related applications and it was not systematically studied before. In general, image degradations increase the difficulty of semantic segmentation, usually leading to decreased semantic segmentation accuracy. Therefore, performance on the underlying clean images can be treated as an upper bound of degraded image semantic segmentation. While the use of supervised deep learning has substantially improved the state of the art of semantic image segmentation, the gap between the feature distribution learned using the clean images and the feature distribution learned using the degraded images poses a major obstacle in improving the degraded image semantic segmentation performance. The conventional strategies for reducing the gap include: 1) Adding image-restoration based pre-processing modules; 2) Using both clean and the degraded images for training; 3) Fine-tuning the network pre-trained on the clean image. In this paper, we propose a novel Dense-Gram Network to more effectively reduce the gap than the conventional strategies and segment degraded images. Extensive experiments demonstrate that the proposed Dense-Gram Network yields state-of-the-art semantic segmentation performance on degraded images synthesized using PASCAL VOC 2012, SUNRGBD, CamVid, and CityScapes datasets.","","","10.1109/TIP.2019.2936111","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812903","Semantic segmentation;degraded images","Image segmentation;Semantics;Degradation;Training;Motion segmentation;Image restoration;Image texture","feature extraction;image restoration;image segmentation;neural nets;object detection;supervised learning","clean image;image degradations;semantic segmentation accuracy;dense-gram network;safety-related applications;supervised deep learning;feature distribution;degraded image semantic segmentation performance improvement;image-restoration;preprocessing modules;PASCAL VOC 2012 dataset;SUNRGBD dataset;CamVid dataset;CityScapes dataset","","","49","","","","","IEEE","IEEE Journals"
"CAI4CAI: The Rise of Contextual Artificial Intelligence in Computer-Assisted Interventions","T. Vercauteren; M. Unberath; N. Padoy; N. Navab","School of Biomedical Engineering & Imaging Sciences, King’s College London, London, U.K.; Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA; ICube institute, CNRS, IHU Strasbourg, University of Strasbourg, Strasbourg, France; Fakultät für Informatik, Technische Universität München, Munich, Germany","Proceedings of the IEEE","","2020","108","1","198","214","Data-driven computational approaches have evolved to enable extraction of information from medical images with reliability, accuracy, and speed, which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theaters are extremely complex and typically rely on poorly integrated intraoperative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer-assisted interventions, we highlight the crucial need to take the context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer-assisted intervention (CAI4CAI) arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors, and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human–AI actor team; and how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision-making ultimately producing more precise and reliable interventions.","","","10.1109/JPROC.2019.2946993","Royal Academy of Engineering; Wellcome Trust; Engineering and Physical Sciences Research Council; National Institute of Biomedical Imaging and Bioengineering; Johns Hopkins University; Fellowship of the Malone Center for Engineering in Healthcare, Johns Hopkins University; Agence Nationale de la Recherche; BPI France through Project CONDOR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8880624","Artificial intelligence;computer-assisted interventions;context-aware user interface;data fusion;interventional workflow;intraoperative imaging;machine and deep learning;surgical data science;surgical planning;surgical scene understanding","Surgery;Planning;Machine learning;Artificial intelligence;Image segmentation;Biomedical imaging","","","","","176","CCBY","","","","IEEE","IEEE Journals"
"Study on the Satellite Telemetry Data Classification Based on Self-Learning","P. Wan; Y. Zhan; W. Jiang","Space Center, Tsinghua University, Beijing, China; Space Center, Tsinghua University, Beijing, China; Space Center, Tsinghua University, Beijing, China","IEEE Access","","2020","8","","2656","2669","Since great redundancy of telemetry data of spacecraft, telemetry data compression is a good solution for the limited bandwidth and contact wireless links. It is important to obtain accurate data characteristic firstly. State-of-the-art machine learning methods work well on data mining and pattern recognition under conditions of the given test data set, which could be used as the available tools for post-event data processing and analysis, such as trend forecasting and outlier detection, but they have not provided the proper solution from the source on-board. In this paper, four base classes of the telemetry data are suggested and studied through the time series feature and information entropy analysis, then a new on-board lightweight self-learning algorithm named Classification Probability calculation - Window Step optimization (CP-WS) is proposed to obtain the class features and make the decision of each single parameter from the continuous discrete telemetry time series. Simulation results show that, our algorithm correctly classifies the simulation and real mission data into the appropriate base class with advantages of high classification accuracy as 100% and adaptive computational complexity from  $O(L^{2})$  to  $O(L)$ , which could be used in satellite on-board data compression for space-to-ground transmission, especially for the deep space explorers to save important status with less on-board storage space.","","","10.1109/ACCESS.2019.2962235","National Natural Science Foundation of China; Tsinghua University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943135","Telemetry data;self-learning;time series;information entropy;classification;sliding window","","","","","","23","CCBY","","","","IEEE","IEEE Journals"
"Machine Learning Adoption in Blockchain-Based Smart Applications: The Challenges, and a Way Forward","S. Tanwar; Q. Bhatia; P. Patel; A. Kumari; P. K. Singh; W. Hong","Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, India; Department of Computer Science and Engineering, Institute of Technology, Nirma University, Ahmedabad, India; Department of Computer Science and Engineering, Jaypee University of Information Technology, Waknaghat, India; Department of Information Management, Oriental Institute of Technology, New Taipei, Taiwan","IEEE Access","","2020","8","","474","488","In recent years, the emergence of blockchain technology (BT) has become a unique, most disruptive, and trending technology. The decentralized database in BT emphasizes data security and privacy. Also, the consensus mechanism in it makes sure that data is secured and legitimate. Still, it raises new security issues such as majority attack and double-spending. To handle the aforementioned issues, data analytics is required on blockchain based secure data. Analytics on these data raises the importance of arisen technology Machine Learning (ML). ML involves the rational amount of data to make precise decisions. Data reliability and its sharing are very crucial in ML to improve the accuracy of results. The combination of these two technologies (ML and BT) can provide highly precise results. In this paper, we present a detailed study on ML adoption for making BT-based smart applications more resilient against attacks. There are various traditional ML techniques, for instance, Support Vector Machines (SVM), clustering, bagging, and Deep Learning (DL) algorithms such as Convolutional Neural Network (CNN) and Long short-term memory (LSTM) can be used to analyse the attacks on a blockchain-based network. Further, we include how both the technologies can be applied in several smart applications such as Unmanned Aerial Vehicle (UAV), Smart Grid (SG), healthcare, and smart cities. Then, future research issues and challenges are explored. At last, a case study is presented with a conclusion.","","","10.1109/ACCESS.2019.2961372","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938741","Blockchain;machine learning;smart grid;data security and privacy;data analytics;smart applications","","","","","","72","CCBY","","","","IEEE","IEEE Journals"
"An Energy-Efficient FPGA-Based Deconvolutional Neural Networks Accelerator for Single Image Super-Resolution","J. Chang; K. Kang; S. Kang","Department of Electronic Engineering, Sogang University, Seoul, South Korea; Department of Electronic Engineering, Sogang University, Seoul, South Korea; Department of Electronic Engineering, Sogang University, Seoul, South Korea","IEEE Transactions on Circuits and Systems for Video Technology","","2020","30","1","281","295","Convolutional neural networks (CNNs) demonstrate excellent performance in various computer vision applications. In recent years, FPGA-based CNN accelerators have been proposed for optimizing performance and power efficiency. Most accelerators are designed for object detection and recognition algorithms that are performed on low-resolution images. However, real-time image super-resolution (SR) cannot be implemented on a typical accelerator because of the long execution cycles required to generate high-resolution (HR) images, such as those used in ultra-high-definition systems. In this paper, we propose a novel CNN accelerator with efficient parallelization methods for SR applications. First, we propose a new methodology for optimizing the deconvolutional neural networks (DCNNs) used for increasing feature maps. Second, we propose a novel method to optimize CNN dataflow so that the SR algorithm can be driven at low power in display applications. Finally, we quantize and compress a DCNN-based SR algorithm into an optimal model for efficient inference using on-chip memory. We present an energy-efficient architecture for SR and validate our architecture on a mobile panel with quad-high-definition resolution. Our experimental results show that, with the same hardware resources, the proposed DCNN accelerator achieves a throughput up to 108 times greater than that of a conventional DCNN accelerator. In addition, our SR system achieves an energy efficiency of 144.9, 293.0, and 500.2 GOPS/W at SR scale factors of 2, 3, and 4, respectively. Furthermore, we demonstrate that our system can restore HR images to a high quality while greatly reducing the data bit-width and the number of parameters compared with conventional SR algorithms.","","","10.1109/TCSVT.2018.2888898","MSIT (Ministry of Science and ICT), Korea; ITRC (Information Technology Research Center); National Research Foundation of Korea; Korea Government (MSIT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8584497","Accelerator architectures;deep neural networks (DNNs);deep learning;super-resolution;system architecture","","","","","2","44","IEEE","","","","IEEE","IEEE Journals"
"Automatic Diagnosis of Familial Exudative Vitreoretinopathy Using a Fusion Neural Network for Wide-Angle Retinal Images","Y. Ye; J. Mao; L. Liu; S. Zhang; L. Shen; M. Sun","Department of Precision Machinery and Instrumentation, University of Science and Technology of China, Hefei, China; Eye Hospital of Wenzhou Medical University, Wenzhou, China; Department of Precision Machinery and Instrumentation, University of Science and Technology of China, Hefei, China; Department of Precision Machinery and Instrumentation, University of Science and Technology of China, Hefei, China; Eye Hospital of Wenzhou Medical University, Wenzhou, China; Department of Precision Machinery and Instrumentation, University of Science and Technology of China, Hefei, China","IEEE Access","","2020","8","","162","173","Familial exudative vitreoretinopathy (FEVR) is a hereditary disorder that can damage the retina. This retinal damage can lead to vision loss and even blindness in the late stages. Thus, early diagnosis and prevention of the disease’s progression are critical. The purpose of this study was to develop an automated diagnosis system for FEVR based on combining deep learning and domain knowledge. A transfer learning scheme was designed to train a deep convolutional neural network (DCNN) to provide segmentation of the retinal vessels. Based on this vessel segmentation and prior clinical knowledge, the vascular characteristics, including the retinal avascular area, vessel angle, fractal dimension, branching and density of blood vessels, were automatically evaluated. Finally, the diagnosis of FEVR was achieved by a feature fusion neural network. Our method was evaluated on 300 images with 168 healthy and 132 FEVR images. By combining deep features and handcrafted features (extracted vascular characteristics), the proposed method achieved an average F1-score of 0.95, with excellent accuracy (94.34%) and sensitivity (91.43%); the quadratic weighted  $\kappa $  was 0.88 for the diagnosis of FEVR. We demonstrated the effectiveness and robustness of the proposed method using five-fold cross-validation. The proposed automatic diagnosis system can assist doctors for better judgment and make sense of early diagnosis and prevention of the disease’s progression.","","","10.1109/ACCESS.2019.2961418","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938765","FEVR;wide-angle retinal images;deep learning;vascular characteristics;fusion","","","","","","42","CCBY","","","","IEEE","IEEE Journals"
"Structure-Preserving Neural Style Transfer","M. Cheng; X. Liu; J. Wang; S. Lu; Y. Lai; P. L. Rosin","College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; School of Computer Science and Informatics, Cardiff University, Cardiff, U.K.; School of Computer Science and Informatics, Cardiff University, Cardiff, U.K.","IEEE Transactions on Image Processing","","2020","29","","909","920","State-of-the-art neural style transfer methods have demonstrated amazing results by training feed-forward convolutional neural networks or using an iterative optimization strategy. The image representation used in these methods, which contains two components: style representation and content representation, is typically based on high-level features extracted from pre-trained classification networks. Because the classification networks are originally designed for object recognition, the extracted features often focus on the central object and neglect other details. As a result, the style textures tend to scatter over the stylized outputs and disrupt the content structures. To address this issue, we present a novel image stylization method that involves an additional structure representation. Our structure representation, which considers two factors: i) the global structure represented by the depth map and ii) the local structure details represented by the image edges, effectively reflects the spatial distribution of all the components in an image as well as the structure of dominant objects respectively. Experimental results demonstrate that our method achieves an impressive visual effectiveness, which is particularly significant when processing images sensitive to structure distortion, e.g. images containing multiple objects potentially at different depths, or dominant objects with clear structures.","","","10.1109/TIP.2019.2936746","National Natural Science Foundation of China; National Youth Talent Support Program; Natural Science Foundation of Tianjin City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816670","Style transfer;structure preserving;deep learning;neural network;local structure;global structure","Image representation;Loss measurement;Optimization;Feature extraction;Image edge detection;Training;Visualization","convolutional neural nets;feature extraction;image classification;image representation;image texture;iterative methods;learning (artificial intelligence);object recognition;optimisation","image stylization method;structure representation;global structure;local structure details;image edges;dominant objects;multiple objects;clear structures;structure-preserving neural style transfer;iterative optimization strategy;image representation;style representation;content representation;high-level features;pre-trained classification networks;object recognition;central object;style textures;stylized outputs;content structures;feedforward convolutional neural networks training","","","47","CCBY","","","","IEEE","IEEE Journals"
"Selective Audio Adversarial Example in Evasion Attack on Speech Recognition System","H. Kwon; Y. Kim; H. Yoon; D. Choi","School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Electrical Engineering, Korea Military Academy, Seoul, South Korea; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Medical Information, Kongju National University, Gongju, South Korea","IEEE Transactions on Information Forensics and Security","","2020","15","","526","538","Deep neural networks (DNNs) are widely used for image recognition, speech recognition, and other pattern analysis tasks. Despite the success of DNNs, these systems can be exploited by what is termed adversarial examples. An adversarial example, in which a small distortion is added to the input data, can be designed to be misclassified by the DNN while remaining undetected by humans or other systems. Such adversarial examples have been studied mainly in the image domain. Recently, however, studies on adversarial examples have been expanding into the voice domain. For example, when an adversarial example is applied to enemy wiretapping devices (victim classifiers) in a military environment, the enemy device will misinterpret the intended message. In such scenarios, it is necessary that friendly wiretapping devices (protected classifiers) should not be deceived. Therefore, the selective adversarial example concept can be useful in mixed situations, defined as situations in which there is both a classifier to be protected and a classifier to be attacked. In this paper, we propose a selective audio adversarial example with minimum distortion that will be misclassified as the target phrase by a victim classifier but correctly classified as the original phrase by a protected classifier. To generate such examples, a transformation is carried out to minimize the probability of incorrect classification by the protected classifier and that of correct classification by the victim classifier. We conducted experiments targeting the state-of-the-art DeepSpeech voice recognition model using Mozilla Common Voice datasets and the Tensorflow library. They showed that the proposed method can generate a selective audio adversarial example with a 91.67% attack success rate and 85.67% protected classifier accuracy.","","","10.1109/TIFS.2019.2925452","National Research Foundation of Korea; National Research Foundation of Korea; Institute for Information and communications Technology Promotion; Korean Government (MSIT) (2016-0-00173, Security Technologies for Financial Fraud Prevention on Fintech); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747397","Deep neural network (DNN);recursive neural network (RNN);adversarial example;machine learning;speech recognition","Hidden Markov models;Distortion;Mel frequency cepstral coefficient;Speech recognition;Image recognition;Distortion measurement;Neural networks","audio signal processing;distortion;neural nets;probability;security of data;signal classification;speech recognition","selective audio adversarial example;victim classifier;protected classifier;speech recognition system;selective adversarial example concept;evasion attack;deep neural networks;minimum distortion;DeepSpeech voice recognition model;Mozilla common voice datasets;attack success rate;probability","","","49","","","","","IEEE","IEEE Journals"
"Dueling Deep-Q-Network Based Delay-Aware Cache Update Policy for Mobile Users in Fog Radio Access Networks","B. Guo; X. Zhang; Q. Sheng; H. Yang","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, CHN.; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, CHN.; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, CHN.; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, CHN.","IEEE Access","","2020","PP","99","1","1","Fog radio access networks (F-RANs) can effectively alleviate fronthaul loads and reduce content transmission delay by migrating cloud services to the network edge. This paper addresses a cooperative caching scenario in F-RAN, where each mobile user can acquire the requested contents from any one of its associated fog-computing-based access points (F-APs). However, caching disparate contents in different F-APs will lead to different content delivery delays, since mobile users suffer from diverse channel fadings and interferences when they download contents from different F-APs. Considering limited caching storage in each F-AP, diverse user preferences, unpredictable user mobility and time-varying channel states, an average transmission delay minimization problem is formulated. With the aid of dueling deep-Q-network framework, a delay-aware cache update policy is proposed for mobile users in F-RAN. The proposed cache update policy will decide to replace the stored contents in F-APs with the proper contents at each time slot. Compared with first in first out, least recently used and least frequently used caching policies, simulation experiments are performed to evaluate the performance of the proposed algorithm. Simulation results illustrate that the proposed caching policy yields better average hit ratio and lower average transmission delay than other traditional caching policies.","","","10.1109/ACCESS.2020.2964258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950344","Caching;fog radio access network;hit ratio;mobility;reinforcement learning","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Image Super-Resolution Using Capsule Neural Networks","J. Hsu; C. Kuo; D. Chen","Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan.; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan.; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan.","IEEE Access","","2020","PP","99","1","1","CONVOLUTIONAL neural networks (CNNs) have been widely applied in super-resolution (SR) and other image restoration tasks. Recently, Hinton et al. proposed capsule neural networks to resolve the problem of viewpoint variations in image classification tasks. Each capsule is represented as either a vector or a matrix to encode more object information, such as position, size, direction, etc. Instead of detecting specific features, these capsule neural networks search for the most relevant features using an iterative process. Therefore, capsule neural networks require fewer parameters compared to traditional neural networks. Inspired by these advances, we make use of a capsule neural network to exploit more potential features for image SR. In this paper, we develope two frameworks: the Capsule Image Restoration Neural Network (CIRNN) and the Capsule Attention and Reconstruction Neural Network (CARNN), to incorporate capsules into image SR convolutional neural networks. The CIRNN takes advantage of the rich information encoded in the capsules to reconstruct accurate high-resolution images. The CARNN generates SR attention features by utilizing the robust segmentation capability of the capsules. Our experiments show that both frameworks can enhance SR for most testing datasets. The CIRNN performs better than the CARNN and can achieve better performance than other traditional CNN methods with a similar amount of parameters.","","","10.1109/ACCESS.2020.2964292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950449","Super resolution;deep learning;convolutional neural network;capsule neural network","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Needs-Based Product Configurator Design for Mass Customization Using Hierarchical Attention Network","Y. Wang; W. Zhao; W. X. Wan","Department of Supply Chain and Information Management, The Hang Seng University of Hong Kong, Hong Kong (e-mail: yuewang@hsu.edu.hk).; Department of Supply Chain and Information Management, The Hang Seng University of Hong Kong, Hong Kong.; Department of Land Economy, University of Cambridge, Cambridge CB2 1TN, U.K..","IEEE Transactions on Automation Science and Engineering","","2020","PP","99","1","10","Mass customization aims to provide goods and services that meet each individual customer's needs with a level of efficiency close to that of mass production. It is also a viable smart manufacturing strategy for companies that want to gain a competitive advantage in the current business environment. Product configurators are one of the major toolkits enabling mass customization. Existing product configurators require customers to choose from a set of predefined attributes or a list of component alternatives. However, customers may feel confused when configuring products if they do not have the necessary domain knowledge about the product. This article proposes a needs-based configurator mechanism that takes customer needs expressed in natural language as input to generate satisfactory product variants as output. This method leverages online product review data to distill the knowledge of customer preferences and needs, which then maps onto the product attribute specifications. A hierarchical attention network is applied to fully extract the information in the review text, which emphasizes the important keywords and phrases. We have obtained the promising experimental results, and our proposed needs-based configurators could help customers to find satisfactory product configurations with high recall rates.","","","10.1109/TASE.2019.2957136","Hong Kong Research Grant Council through the Faculty Development Scheme FDS Project; Institutional Development Scheme IDS Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950277","Configurator design;deep learning;mass customization.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Holistic Multi-Modal Memory Network for Movie Question Answering","A. Wang; A. T. Luu; C. Foo; H. Zhu; Y. Tay; V. Chandrasekhar","Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Institute for Infocomm Research, A*STAR, Singapore","IEEE Transactions on Image Processing","","2020","29","","489","499","Answering questions using multi-modal context is a challenging problem, as it requires a deep integration of diverse data sources. Existing approaches only consider a subset of all possible interactions among data sources during one attention hop. In this paper, we present a holistic multi-modal memory network (HMMN) framework that fully considers interactions between different input sources (multi-modal context and question) at each hop. In addition, to hone in on relevant information, our framework takes answer choices into consideration during the context retrieval stage. Our HMMN framework effectively integrates information from the multi-modal context, question, and answer choices, enabling more informative context to be retrieved for question answering. Experimental results on the Movie QA and TVQA datasets validate the effectiveness of our HMMN framework. Extensive ablation studies show the importance of holistic reasoning and reveal the contributions of different attention strategies to model performance.","","","10.1109/TIP.2019.2931534","A*STAR Deep Learning 2.0 Program; A*STAR CHEEM; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8786906","Question answering;multi-modal learning;MovieQA","Knowledge discovery;Visualization;Videos;Hidden Markov models;Task analysis;Motion pictures;Semantics","question answering (information retrieval)","holistic multimodal memory network;multimodal context;diverse data sources;multimodal memory network framework;context retrieval stage;HMMN framework;informative context;movie question answering;TVQA datasets;Movie QA datasets","","1","47","","","","","IEEE","IEEE Journals"
"Benchmarking In-Hand Manipulation","S. Cruciani; B. Sundaralingam; K. Hang; V. Kumar; T. Hermans; D. Kragic","RPL, KTH Royal Institute of Technology, Stockholm Sweden 11428 (e-mail: cruciani@kth.se); School of Computing, University of Utah, Salt Lake city, UT United States of America 84103 (e-mail: bala@cs.utah.edu); Mechanical Engineering and Material Science, Yale University, New Haven, Connecticut United States of America 06511 (e-mail: kaiyuh@kth.se); Deep Reinforcement Learning for Robotics, Google-Brain, SanFrancisco, CA United States of America 94110 (e-mail: vikashplus@gmail.com); School of Computing, University of Utah, Salt Lake City, UT United States of America 84112 (e-mail: thermans@cs.utah.edu); Computational Vision and Active Perception, KTH, Stockholm Sweden 10044 (e-mail: dani@kth.se)","IEEE Robotics and Automation Letters","","2020","PP","99","1","1","The purpose of this benchmark is to evaluate the planning and control aspects of robotic in-hand manipulation systems. The goal is to assess the system's ability to change the pose of a hand-held object by either using the fingers, environment or a combination of both. Given an object surface mesh from the YCB data-set, we provide examples of initial and goal states (i.e. static object poses and fingertip locations) for various in-hand manipulation tasks. We further propose metrics that measure the error in reaching the goal state from a specific initial state, which, when aggregated across all tasks, also serves as a measure of the system's in-hand manipulation capability. We provide supporting software, task examples, and evaluation results associated with the benchmark.","","","10.1109/LRA.2020.2964160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950086","Performance Evaluation and Benchmarking;Dexterous Manipulation","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A Low Effort Approach to Structured CNN Design Using PCA","I. Garg; P. Panda; K. Roy","School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; Electrical Engineering Department, Yale University, New Haven, U.K.; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA","IEEE Access","","2020","8","","1347","1360","Deep learning models hold state of the art performance in many fields, yet their design is still based on heuristics or grid search methods that often result in overparametrized networks. This work proposes a method to analyze a trained network and deduce an optimized, compressed architecture that preserves accuracy while keeping computational costs tractable. Model compression is an active field of research that targets the problem of realizing deep learning models in hardware. However, most pruning methodologies tend to be experimental, requiring large compute and time intensive iterations of retraining the entire network. We introduce structure into model design by proposing a single shot analysis of a trained network that serves as a first order, low effort approach to dimensionality reduction, by using PCA (Principal Component Analysis). The proposed method simultaneously analyzes the activations of each layer and considers the dimensionality of the space described by the filters generating these activations. It optimizes the architecture in terms of number of layers, and number of filters per layer without any iterative retraining procedures, making it a viable, low effort technique to design efficient networks. We demonstrate the proposed methodology on AlexNet and VGG style networks on the CIFAR-10, CIFAR-100 and ImageNet datasets, and successfully achieve an optimized architecture with a reduction of up to 3.8X and 9X in the number of operations and parameters respectively, while trading off less than 1% accuracy. We also apply the method to MobileNet, and achieve 1.7X and 3.9X reduction in the number of operations and parameters respectively, while improving accuracy by almost one percentage point.","","","10.1109/ACCESS.2019.2961960","Center for Brain Inspired Computing (C-BRIC), one of the six centers in JUMP; Semiconductor Research Corporation; Defense Advanced Research Projects Agency; Semiconductor Research Corporation; National Science Foundation; Intel Corporation; DoD Vannevar Bush Fellowship; Army Research Laboratory; U.K. Ministry of Defence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941144","CNNs;efficient deep learning;model architecture;model compression;PCA;dimensionality reduction;pruning;network design","","","","","","43","CCBY","","","","IEEE","IEEE Journals"
"Digital Electronics and Analog Photonics for Convolutional Neural Networks (DEAP-CNNs)","V. Bangari; B. A. Marquez; H. Miller; A. N. Tait; M. A. Nahmias; T. F. de Lima; H. Peng; P. R. Prucnal; B. J. Shastri","Department of Physics, Engineering Physics & Astronomy, Queen's University, Kingston, ON, Canada; Department of Physics, Engineering Physics & Astronomy, Queen's University, Kingston, ON, Canada; Department of Physics, Engineering Physics & Astronomy, Queen's University, Kingston, ON, Canada; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA; Department of Physics, Engineering Physics & Astronomy, Queen's University, Kingston, ON, Canada","IEEE Journal of Selected Topics in Quantum Electronics","","2020","26","1","1","13","Convolutional Neural Networks (CNNs) are powerful and highly ubiquitous tools for extracting features from large datasets for applications such as computer vision and natural language processing. However, a convolution is a computationally expensive operation in digital electronics. In contrast, neuromorphic photonic systems, which have experienced a recent surge of interest over the last few years, propose higher bandwidth and energy efficiencies for neural network training and inference. Neuromorphic photonics exploits the advantages of optical electronics, including the ease of analog processing, and busing multiple signals on a single waveguide at the speed of light. Here, we propose a Digital Electronic and Analog Photonic (DEAP) CNN hardware architecture that has potential to be 2.8 to 14 times faster while using almost 25% less energy than current state-of-the-art graphical processing units (GPUs).","","","10.1109/JSTQE.2019.2945540","Natural Sciences and Engineering Research Council of Canada; Queen's Research Initiation Grant; 2019 Queens Postdoctoral Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859364","Deep learning;machine learning;neuromorphic photonics;photonic neural networks;convolutional neural network (CNN)","Photonics;Convolution;Kernel;Silicon;Computer architecture;Neural networks;Optical waveguides","convolutional neural nets;graphics processing units;neural net architecture","graphical processing units;GPU;digital electronic-and-analog photonic CNN hardware architecture;digital electronics-and-analog photonics-for-convolutional neural networks;analog processing;optical electronics;neuromorphic photonics;neural network inference;neural network training;neuromorphic photonic systems;ubiquitous tools;DEAP-CNNs;convolutional neural networks;digital electronics","","","52","IEEE","","","","IEEE","IEEE Journals"
"Joint Decision of Anti-Spoofing and Automatic Speaker Verification by Multi-Task Learning with Contrastive Loss","J. Li; M. Sun; X. Zhang; Y. Wang","Laboratory of Intelligent Information Processing, Army Engineering University, Nanjing, 210007, P. R. China.; Laboratory of Intelligent Information Processing, Army Engineering University, Nanjing, 210007, P. R. China.; Laboratory of Intelligent Information Processing, Army Engineering University, Nanjing, 210007, P. R. China.; Communications Engineering College, Army Engineering University, Nanjing, 210007, P. R. China.","IEEE Access","","2020","PP","99","1","1","Automatic speaker verification (ASV) is an emerging biometric verification technique with more and more applications. However, both verification accuracy and anti-spoofing should be considered carefully before putting ASV into practice, where anti-spoofing is also called replay detection in which voice is recorded, stored and replayed to deceive ASV systems. Cascaded decision of anti-spoofing and ASV is a straightforward solution to tackle the two issues. In this paper, joint decision of anti-spoofing and ASV was investigated in a multi-task learning framework with contrastive loss in order to improve the cascaded decision approach. A modified triplet loss was firstly constructed to supervise deep neural networks to extract embedding vectors containing information of both speaker identity and spoofing. The embedding vectors were subsequently taken as input features by back-end classifiers towards speaker and spoofing classification. The experimental results on both ASVspoof 2017 and ASVspoof 2019 showed that the proposed joint decision approach with triplet loss outperformed the corresponding baselines, a recent work on joint decision with Gaussian back-end fusion and our previous joint decision approach with cross-entropy loss.","","","10.1109/ACCESS.2020.2964048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950160","Anti-spoofing;Speaker Verification;Replay Detection;Multi-task Learning;Triplet Loss","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Scalable knowledge-defined orchestration for hybrid optical–electrical datacenter networks [Invited]","Q. Li; H. Fang; D. Li; J. Peng; J. Kong; W. Lu; Z. Zhu","School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui 230027, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui 230027, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui 230027, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui 230027, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui 230027, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui 230027, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui 230027, China","IEEE/OSA Journal of Optical Communications and Networking","","2020","12","2","A113","A122","To better provision fast-emerging network applications with various quality-of-service demands, datacenter network (DCN) operators need an effective network orchestration scheme that can coordinate IT and bandwidth resources for differentiated services in a timely manner. In this work, we consider a hybrid optical-electrical DCN (HOE-DCN) and study how to achieve scalable knowledge-defined network orchestration (KD-NO) for managing the delay-sensitive and delay-tolerant applications in it. For delay-sensitive applications, we leverage a multi-agent scheme to distribute the tasks of placing virtual machines (VMs) in server racks and routing VM traffic in electrical-optical inter-rack clouds to two cooperative deep reinforcement learning modules, respectively. Then, we utilize a classic-algorithm-based module to provision delay-tolerant applications with the residual resources in the HOE-DCN. We design the operation and coordination procedure of the KD-NO system and build a small HOE-DCN testbed that consists of four server racks to demonstrate its performance experimentally. Experimental results indicate that our KD-NO system can make timely and correct network orchestration decisions and have better convergence performance compared with the existing benchmark.","","","10.1364/JOCN.12.00A113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894303","","Optical switches;Adaptive optics;Optical fiber networks;Servers;Optical packet switching;Bandwidth;Routing","computer centres;DiffServ networks;learning (artificial intelligence);multi-agent systems;optical fibre networks;telecommunication network routing;telecommunication traffic;virtual machines","multiagent scheme;delay-sensitive applications;scalable knowledge-defined network orchestration;optical-electrical DCN;differentiated services;bandwidth resources;effective network orchestration scheme;quality-of-service demands;network applications;optical-electrical datacenter networks;scalable knowledge-defined orchestration;correct network orchestration decisions;timely network orchestration decisions;server racks;coordination procedure;HOE-DCN;residual resources;delay-tolerant applications;classic-algorithm-based module;deep reinforcement learning modules;electrical-optical inter-rack clouds","","","","","","","","IEEE","IEEE Journals"
"A 65-nm Neuromorphic Image Classification Processor With Energy-Efficient Training Through Direct Spike-Only Feedback","J. Park; J. Lee; D. Jeon","Graduate School of Convergence Science and Technology, Seoul National University, Seoul, South Korea; Graduate School of Convergence Science and Technology, Seoul National University, Seoul, South Korea; Graduate School of Convergence Science and Technology, Seoul National University, Seoul, South Korea","IEEE Journal of Solid-State Circuits","","2020","55","1","108","119","Recent advances in neural network (NN) and machine learning algorithms have sparked a wide array of research in specialized hardware, ranging from high-performance NN accelerators for use inside the server systems to energy-efficient edge computing systems. While most of these studies have focused on designing inference engines, implementing the training process of an NN for energy-constrained mobile devices has remained to be a challenge due to the requirement of higher numerical precision. In this article, we aim to build an on-chip learning system that would show highly energy-efficient training for NNs without degradation in the performance for machine learning tasks. To achieve this goal, we adapt and optimize a neuromorphic learning algorithm and propose hardware design techniques to fully exploit the properties of the modifications. We verify that our system achieves energy-efficient training with only 7.5% more energy consumption compared with its highly efficient inference of 236 nJ/image on the handwritten digit [Modified National Institute of Standards and Technology database (MNIST)] images. Moreover, our system achieves 97.83% classification accuracy on the MNIST test data set, which outperforms prior neuromorphic on-chip learning systems and is close to the performance of the conventional method for training deep neural networks (NNs), the backpropagation.","","","10.1109/JSSC.2019.2942367","National Research Foundation of Korea; Korea Institute of Ocean Science and Technology; IC Design Education Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8867974","Computational efficiency;digital integrated circuits;edge computing;image classification;learning systems;multi layer perceptrons;neuromorphics;very large-scale integration","Training;Neurons;Artificial neural networks;Neuromorphics;Hardware;Task analysis;Machine learning algorithms","","","","","37","IEEE","","","","IEEE","IEEE Journals"
"Efficient and Interpretable Deep Blind Image Deblurring Via Algorithm Unrolling","Y. Li; M. Tofighi; J. Geng; V. Monga; Y. C. Eldar","Department of Electrical Engineering, Pennsylvania State University, State College, Pennsylvania United States 16801 (e-mail: liyuelongee@gmail.com); Electrical Engineering, Pennsylvania State University University Park, 311285 University Park, Pennsylvania United States 16802 (e-mail: mo.tofighi@gmail.com); State College, Pennsylvania United States (e-mail: jxg1052@psu.edu); Electrical Engineering, Pennsylvania State University, University Park, Pennsylvania United States 16802 (e-mail: vmonga@engr.psu.edu); Electrical Engineering, Technion-Israel Institute of Technology, Haifa Israel 32000 (e-mail: yonina.eldar@weizmann.ac.il)","IEEE Transactions on Computational Imaging","","2020","PP","99","1","1","Blind image deblurring remains a topic of enduring interest. Learning based approaches, especially those that employ neural networks have emerged to complement traditional model based methods and in many cases achieve vastly enhanced performance. That said, neural network approaches are generally empirically designed and the underlying structures are difficult to interpret. In recent years, a promising technique called algorithm unrolling has been developed that has helped connect iterative algorithms such as those for sparse coding to neural network architectures. In this paper, we propose a neural network architecture based on this idea. We first present an iterative algorithm that may be considered as a generalization of the traditional total-variation regularization method in the gradient domain. We then unroll the algorithm to construct a neural network for image deblurring which we refer to as Deep Unrolling for Blind Deblurring (DUBLID). Key algorithm parameters are learned with the help of training images. Our proposed deep network DUBLID achieves significant practical performance gains while enjoying interpretability at the same time. Extensive experimental results show that DUBLID outperforms many state-of-the-art methods and in addition is computationally faster.","","","10.1109/TCI.2020.2964202","NSF National Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950351","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Robust License Plate Recognition With Shared Adversarial Training Network","S. Zhang; G. Tang; Y. Liu; H. Mao","College of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; College of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; College of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; College of Computer Science, South China University of Technology, Guangzhou, China","IEEE Access","","2020","8","","697","705","Recently, deep learning has greatly promoted the performance of license plate recognition (LPR) by learning robust features from numerous labeled data. However, the large variation of wild license plates across complicated environments and perspectives is still a huge challenge to the robust LPR. To solve the problem, we propose an effective and efficient shared adversarial training network (SATN) in this paper, which can learn the environment-independent and perspective-free semantic features from wild license plates with the prior knowledge of standard stencil-rendered license plates, as standard stencil-rendered license plates are independent of complicated environments and various perspectives. Besides, to correct the features of heavily perspective distorted license plates perfectly, we further propose a novel dual attention transformation (DAT) module in the shared adversarial training network. Comprehensive experiments on AOLP-RP and CCPD benchmarks show that the proposed method outperforms state-of-the-art methods by a large margin on the LPR task.","","","10.1109/ACCESS.2019.2961744","National Natural Science Foundation of China; National Key Research and Development Program of China; Natural Science Foundation of Guangdong Province; Guangdong Intellectual Property Office Project; Guangzhou Science, Technology and Innovation Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8939462","Deep learning;license plate recognition (LPR);dual attention transformation (DAT);shared adversarial training network (SATN)","","","","","","34","CCBY","","","","IEEE","IEEE Journals"
"50 FPS Object-Level Saliency Detection via Maximally Stable Region","X. Huang; Y. Zheng; J. Huang; Y. Zhang","Computer School, Beijing Information Science and Technology University, Beijing, China; WeChat Search Application Department, Tencent Beijing, Beijing, China; Tencent AI Lab, Shenzhen, China; Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","","2020","29","","1384","1396","The human visual system tends to consider saliency of an object as a whole. Some object-level saliency detection methods have been proposed by leveraging object proposals in bounding boxes, and regarding the entire bounding box as one candidate salient region. However, the bounding boxes can not provide exact object position and a lot of pixels in bounding boxes belong to the background. Consequently, background pixels in bounding box also show high saliency. Besides, acquiring object proposals needs high time cost. In order to compute object-level saliency, we consider region growing from some seed superpixels, to find one surrounding region which probably represents the whole object. The desired surrounding region has similar appearance inside and obvious difference with the outside, which is proposed as maximally stable region (MSR) in this paper. In addition, one effective seed superpixel selection strategy is presented to improve speed. MSR based saliency detection is more robust than pixel or superpixel level methods and object proposal based methods. The proposed method significantly outperforms the state-of-the-art unsupervised methods at 50 FPS. Compared with deep learning based methods, we show worse performance, but with about 1200-1600 times faster, which means better trade-off between performance and speed.","","","10.1109/TIP.2019.2941663","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8845759","Saliency detection;salient object detection;maximally stable region;minimum barrier distance;seed selection","Saliency detection;Proposals;Object detection;Graphical models;Visual systems;Deep learning;Visualization","feature extraction;image representation;object detection;object tracking","maximally stable region;object-level saliency detection methods;MSR based saliency detection;object proposal based methods","","","54","","","","","IEEE","IEEE Journals"
"Photonic Multiply-Accumulate Operations for Neural Networks","M. A. Nahmias; T. F. de Lima; A. N. Tait; H. Peng; B. J. Shastri; P. R. Prucnal","Department of Electrical Engineering, Princeton University, Princeton, NJ, USA; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA; Department of Physics, Engineering Physics and Astronomy, Queen's University, Kingston, ON, Canada; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA","IEEE Journal of Selected Topics in Quantum Electronics","","2020","26","1","1","18","It has long been known that photonic communication can alleviate the data movement bottlenecks that plague conventional microelectronic processors. More recently, there has also been interest in its capabilities to implement low precision linear operations, such as matrix multiplications, fast and efficiently. We characterize the performance of photonic and electronic hardware underlying neural network models using multiply-accumulate operations. First, we investigate the limits of analog electronic crossbar arrays and on-chip photonic linear computing systems. Photonic processors are shown to have advantages in the limit of large processor sizes (${>}\text{100}\; \mu$m), large vector sizes ($N > 500)$, and low noise precision (${\leq} 4$ bits). We discuss several proposed tunable photonic MAC systems, and provide a concrete comparison between deep learning and photonic hardware using several empirically-validated device and system models. We show significant potential improvements over digital electronics in energy (${>}10^2$), speed (${>}10^3$), and compute density (${>}10^2$).","","","10.1109/JSTQE.2019.2941485","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844098","Artificial intelligence;neural networks;analog computers;analog processing circuits;optical computing","Photonics;Neural networks;Program processors;Computational modeling;Deep learning;Training;Metals","","","","","127","CCBY","","","","IEEE","IEEE Journals"
"Cross-Camera Person Re-Identification With Body-Guided Attention Network","Y. Xie; Y. Wang; C. Hu; C. Shan; T. Li; Y. Hu","School of Electrical Engineering and Automation, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; School of Electrical Engineering and Automation, Anhui University, Hefei, China; Philips Research, High-Tech Campus 34, Eindhoven, The Netherlands; Institute of Physical Science and Information Technology, Anhui University, Hefei, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China","IEEE Sensors Journal","","2020","20","1","359","368","Various challenges exist throughout person re-identification (ReID) process, including background clutters, illumination variation, pose variation, occlusion, etc. Addressing these problems, this paper explores the incorporation of human attention mechanism in person ReID and proposes an attention-aware model named Body-guided Attention Network (BANet). The proposed attention is based on the body masked images which are obtained by a reliable pixel-level segmentation strategy. To optimize the feature representation learning so as to pay more attention to the discriminative details of human body, BANet is built. It is composed of three attention branches. In order to guide attention learning layer by layer, these branches are applied to the convolution features of different levels. The proposed BANet aims to fully utilize fine-grained information of body region to guide the final process of feature extraction. Extensive experiments on benchmarks including CUHK03, Market1501 and DukeMTMC-reID show that BANet can achieve state-of-the-art performance, which validates the importance of attention mechanism in person ReID.","","","10.1109/JSEN.2019.2942106","National Key R&D Program of China; Science and Technology Foundation of Guangzhou Huangpu Development District; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8843897","Person re-identification;convolutional neural network;body-guided attention network","Feature extraction;Image segmentation;Training;Clutter;Cameras;Deep learning;Reliability","","","","","49","IEEE","","","","IEEE","IEEE Journals"
"Blood Cell Classification Based on Hyperspectral Imaging With Modulated Gabor and CNN","Q. Huang; W. Li; B. Zhang; Q. Li; R. Tao; N. H. Lovell","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Graduate School of Biomedical Engineering, University of New South Wales, Sydney, NSW, Australia","IEEE Journal of Biomedical and Health Informatics","","2020","24","1","160","170","Cell classification, especially that of white blood cells, plays a very important role in the field of diagnosis and control of major diseases. Compared to traditional optical microscopic imaging, hyperspectral imagery, combined with both spatial and spectral information, provides more wealthy information for recognizing cells. In this paper, a novel blood cell classification framework, which combines a modulated Gabor wavelet and deep convolutional neural network (CNN) kernels, named as MGCNN, is proposed based on medical hyperspectral imaging. For each convolutional layer, multi-scale and orientation Gabor operators are taken dot product with initial CNN kernels. The essence is to transform the convolutional kernels into the frequency domain to learn features. By combining characteristics of Gabor wavelets, the features learned by modulated kernels at different frequencies and orientations are more representative and discriminative. Experimental results demonstrate that the proposed model can achieve better classification performance than traditional CNNs and widely used support vector machine approaches, especially as training small-sample-size situations.","","","10.1109/JBHI.2019.2905623","Natural Science Foundation of Beijing Municipality; Beijing Nova Program; Research Fund for Basic Researches in Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668460","Deep learning;medical hyperspectral imagery;blood cell classification;convolutional neural network;Gabor wavelet","Kernel;Feature extraction;Hyperspectral imaging;Biomedical imaging;Frequency modulation;Training","","","","","45","IEEE","","","","IEEE","IEEE Journals"
"Automatic Sleep Staging Employing Convolutional Neural Networks and Cortical Connectivity Images","P. Chriskos; C. A. Frantzidis; P. T. Gkivogkli; P. D. Bamidis; C. Kourtidou-Papadeli","Laboratory of Medical Physics, Medical School, Aristotle University of Thessaloniki, Thessaloniki, Greece; Laboratory of Medical Physics, Medical School, Aristotle University of Thessaloniki, Thessaloniki, Greece; Laboratory of Medical Physics, Medical School, Aristotle University of Thessaloniki, Thessaloniki, Greece; Laboratory of Medical Physics, Medical School, Aristotle University of Thessaloniki, Thessaloniki, Greece; Laboratory of Medical Physics, Medical School, Aristotle University of Thessaloniki, Thessaloniki, Greece","IEEE Transactions on Neural Networks and Learning Systems","","2020","31","1","113","123","Understanding of the neuroscientific sleep mechanisms is associated with mental/cognitive and physical well-being and pathological conditions. A prerequisite for further analysis is the identification of the sleep macroarchitecture through manual sleep staging. Several computer-based approaches have been proposed to extract time and/or frequency-domain features with accuracy ranging from 80% to 95% compared with the golden standard of manual staging. However, their acceptability by the medical community is still suboptimal. Recently, utilizing deep learning methodologies increased the research interest in computer-assisted recognition of sleep stages. Aiming to enhance the arsenal of automatic sleep staging, we propose a novel classification framework based on convolutional neural networks. These receive as input synchronizations features derived from cortical interactions within various electroencephalographic rhythms (delta, theta, alpha, and beta) for specific cortical regions which are critical for the sleep deepening. These functional connectivity metrics are then processed as multidimensional images. We also propose to augment the small portion of sleep onset (N1 stage) through the Synthetic Minority Oversampling Technique in order to deal with the great difference in its duration when compared with the remaining sleep stages. Our results (99.85%) indicate the flexibility of deep learning techniques to learn sleep-related neurophysiological patterns.","","","10.1109/TNNLS.2019.2899781","H2020 Societal Challenges; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667849","Automatic sleep staging;convolutional neural networks (CNNs);default mode network (DMN);functional connectivity features;minority class oversampling technique","Sleep;Feature extraction;Electroencephalography;Biomedical imaging;Convolutional neural networks;Electrooculography","","","","1","58","IEEE","","","","IEEE","IEEE Journals"
"Fall Detection Using Standoff Radar-Based Sensing and Deep Convolutional Neural Network","H. Sadreazami; M. Bolic; S. Rajan","School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada","IEEE Transactions on Circuits and Systems II: Express Briefs","","2020","67","1","197","201","Automatic fall detection using radar aids in better assisted living and smarter health care. In this brief, a novel time series-based method for detecting fall incidents in human daily activities is proposed. A time series in the slow-time is obtained by summing all the range bins corresponding to fast-time of the ultra wideband radar return signals. This time series is used as input to the proposed deep convolutional neural network for automatic feature extraction. In contrast to other existing methods, the proposed fall detection method relies on multi-level feature learning directly from the radar time series signals. In particular, the proposed method utilizes a deep convolutional neural network for automating feature extraction as well as global maximum pooling technique for enhancing model discriminability. The performance of the proposed method is compared with that of the state-of-the-art, such as recurrent neural network, multi-layer perceptron, and dynamic time warping techniques. The results demonstrate that the proposed fall detection method outperforms the other methods in terms of higher accuracy, precision, sensitivity, and specificity values.","","","10.1109/TCSII.2019.2904498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664624","Biomedical signal processing;smart homes;fall detection;convolutional neural network;ultra-wideband radar","Feature extraction;Time series analysis;Convolutional neural networks;Radar detection;Convolution","","","","1","21","IEEE","","","","IEEE","IEEE Journals"
"Dependency-Aware Attention Control for Image Set-Based Face Recognition","X. Liu; Z. Guo; J. You; B. V. K. Vijaya Kumar","Medical School, Harvard University, Boston, MA, USA; Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong.; Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Transactions on Information Forensics and Security","","2020","15","","1501","1512","This paper considers the problem of image set-based face verification and identification. Unlike traditional single sample (an image or a video) setting, this situation assumes the availability of a set of heterogeneous collection of orderless images and videos. The samples can be taken at different check points, different identity documents  $etc$ . The importance of each image is usually considered either equal or based on a quality assessment of that image independent of other images and/or videos in that image set. How to model the relationship of orderless images within a set remains a challenge. We address this problem by formulating it as a Markov Decision Process (MDP) in a latent space. Specifically, we first propose a dependency-aware attention control (DAC) network, which uses actor-critic reinforcement learning for attention decision of each image to exploit the correlations among the unordered images. An off-policy experience replay is introduced to speed up the learning process. Moreover, the DAC is combined with a temporal model for videos using divide and conquer strategies. We also introduce a pose-guided representation (PGR) scheme that can further boost the performance at extreme poses. We propose a parameter-free PGR without the need for training as well as a novel metric learning-based PGR for pose alignment without the need for pose detection in testing stage. Extensive evaluations on IJB-A/B/C, YTF, Celebrity-1000 datasets demonstrate that our method outperforms many state-of-art approaches on the set-based as well as video-based face recognition databases.","","","10.1109/TIFS.2019.2938418","National Natural Science Foundation of China; Shenzhen FRF; HK GRF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8820094","Deep reinforcement learning;actor-critic;face recognition;set-to-set;attention control","","","","","","64","IEEE","","","","IEEE","IEEE Journals"
"Super-Resolution PET Imaging Using Convolutional Neural Networks","T. Song; S. Roy Chowdhury; F. Yang; J. Dutta",", University of Massachusetts Lowell, 14710 Lowell, Massachusetts United States 01854 (e-mail: tzuan_song@student.uml.edu); , University of Massachusetts Lowell, 14710 Lowell, Massachusetts United States (e-mail: samadrita_chowdhury@uml.edu); , University of Massachusetts Lowell, 14710 Lowell, Massachusetts United States (e-mail: fan_yang@student.uml.edu); , University of Massachusetts Lowell, 14710 Lowell, Massachusetts United States (e-mail: Dutta.Joyita@mgh.harvard.edu)","IEEE Transactions on Computational Imaging","","2020","PP","99","1","1","Positron emission tomography (PET) suffers from severe resolution limitations which reduce its quantitative accuracy. In this paper, we present a super-resolution (SR) imaging technique for PET based on convolutional neural networks (CNNs). To facilitate the resolution recovery process, we incorporate high-resolution (HR) anatomical information based on magnetic resonance (MR) imaging. We introduce the spatial location information of the input image patches as additional CNN inputs to accommodate the spatially-variant nature of the blur kernels in PET. We compared the performance of shallow (3-layer) and very deep (20-layer) CNNs with various combinations of the following inputs: low-resolution (LR) PET, radial locations, axial locations, and HR MR. To validate the CNN architectures, we performed both realistic simulation studies using the BrainWeb digital phantom and clinical studies using neuroimaging datasets. For both simulation and clinical studies, the LR PET images were based on the Siemens HR+ scanner. Two different scenarios were examined in simulation: one where the target HR image is the ground-truth phantom image and another where the target HR image is based on the Siemens HRRT scanner — a high-resolution dedicated brain PET scanner. The latter scenario was also examined using clinical neuroimaging datasets. A number of factors affected relative performance of the different CNN designs examined, including network depth, target image quality, and the resemblance between the target and anatomical images. In general, however, all deep CNNs outperformed classical penalized deconvolution and partial volume correction techniques by large margins both qualitatively (e.g., edge and contrast recovery) and quantitatively (as indicated by three metrics: peak signal-to-noise-ratio, structural similarity index, and contrast-to-noise ratio).","","","10.1109/TCI.2020.2964229","National Institute on Aging; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950375","super-resolution;CNN;deep learning;PET/MRI;multimodality imaging;partial volume correction","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Objective ADHD diagnosis using Convolutional Neural Networks over Daily-Life Activity Records","P. Amado-Caballero; P. Casaseca-de-la-Higuera; S. Alberola-Lopez; J. M. Andres-de-Llano; J. A. Lopez-Villalobos; J. R. Garmendia-Leiza; C. Alberola-Lopez","Laboratorio de Procesado de Imagen, Universidad de Valladolid, 16782 Valladolid, Castilla y Leon Spain (e-mail: pamacab@lpi.tel.uva.es); Artificial Intelligence, Visual Communications and Networking Research Centre, University of the West of Scotland, Paisley United Kingdom of Great Britain and Northern Ireland PA1 2BE (e-mail: Pablo.Casaseca@uws.ac.uk); Centro de Salud Jardinillos, SACYL, Palencia Spain (e-mail: salberola56@gmail.com); Complejo Asistencial Universitario de Palencia., SACYL, Palencia Spain (e-mail: jm.andres.dellano@gmail.com); Complejo Asistencial Universitario de Palencia., SACYL, 156300 Palencia Spain (e-mail: villalobos@cop.es); Complejo Asistencial Universitario de Palencia, SACYL, 156300 Palencia Spain (e-mail: garbi69@garbi6.jazztel.es); Escuela Tecnica Superior de Ingenieros de Telecomunicacion, Universidad de Valladolid, Valladolid, Castilla y Leon Spain 47011 (e-mail: caralb@tel.uva.es)","IEEE Journal of Biomedical and Health Informatics","","2020","PP","99","1","1","Attention Deficit/Hyperactivity Disorder (ADHD) is the most common neurobehavioral disorder in children and adolescents. However, its etiology is still unknown, and this hinders the existence of reliable, fast and inexpensive standard diagnostic methods. Objective: This paper proposes an end-to-end methodology for automatic diagnosis of the combined type of ADHD. Methods: Diagnosis is based on the analysis of 24 hour-long activity records using Convolutional Neural Networks to classify spectrograms of activity windows. Results: We achieve up to 97.62% average sensitivity, 99.52% specificity and AUC values over 99%. Overall, our figures overcome those obtained by actigraphy-based methods reported in the literature as well as others based on more expensive (and not so convenient) acquisition methods. Conclusion: These results reinforce the idea that combining deep learning techniques together with actimetry can lead to a robust and efficient system for objective ADHD diagnosis. Significance: Reliance on simple activity measurements leads to an inexpensive and non-invasive objective diagnostic method, which can be easily implemented with daily devices.","","","10.1109/JBHI.2020.2964072","Scottish Funding Council; Instituto de Competitividad Empresarial; European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950177","ADHD;actigraphy;Deep Learning;Convolutional Neural Network (CNN)","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Multidomain Features-Based GA Optimized Artificial Immune System for Bearing Fault Detection","A. Abid; M. T. Khan; M. S. Khan","Department of Mechatronics Engineering, University of Engineering and Technology, Peshawar, Pakistan; Department of Mechatronics Engineering, University of Engineering and Technology, Peshawar, Pakistan; Department of Electrical Engineering, Jalozai Campus, University of Engineering and Technology, Peshawar, Pakistan","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2020","50","1","348","359","This paper proposes a novel multidomain features-based genetic algorithm (GA) optimized artificial immune system (AIS) framework for fault detection in real systems. Different from native real-valued negative selection algorithm (RNSA) that operates in original data space, this algorithm utilizes feature space transformation and diversity factor-based GA for optimized detector distribution in nonself feature space. The proposed framework comprises three stages namely; feature extraction, unsupervised feature selection, and GA optimized AIS. In the first stage, signal processing methods are applied to extract multidomain features (time-domain statistical, frequency domain statistical, and special features) of the system. In the second stage, two unsupervised methods namely,  ${k}$ -NN clustering and pretraining using deep learning neural network are proposed for dominant fault-characterizing feature selection. Finally, in the third stage, the fault-characterizing feature vectors are used for system status categorization (i.e., normal, fault) using selected (fault-characterizing) features-based AIS method. The efficacy of the proposed framework is verified through experiments on motor bearing fault detection using vibration signal. The major accomplishment of the proposed combination of space transformation, feature selection and AIS (anomaly classification) techniques is the alleviation of computational burden on RNSA implementation. Moreover, GA optimized AIS fault diagnosis based on well-established features gives improved detection performance.","","","10.1109/TSMC.2017.2746762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031077","Artificial immune system (AIS);deep learning;fault detection;feature selection;genetic algorithm (GA);negative selection algorithm (NSA)","Feature extraction;Fault detection;Detectors;Artificial intelligence;Genetic algorithms;Anomaly detection;Training","","","","","59","IEEE","","","","IEEE","IEEE Journals"
"Multi-Sensor Fusion in Automated Driving: A Survey","Z. Wang; Y. Wu; Q. Niu","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Access","","2020","8","","2847","2868","With the significant development of practicability in deep learning and the ultra-high-speed information transmission rate of 5G communication technology will overcome the barrier of data transmission on the Internet of Vehicles, automated driving is becoming a pivotal technology affecting the future industry. Sensors are the key to the perception of the outside world in the automated driving system and whose cooperation performance directly determines the safety of automated driving vehicles. In this survey, we mainly discuss the different strategies of multi-sensor fusion in automated driving in recent years. The performance of conventional sensors and the necessity of multi-sensor fusion are analyzed, including radar, LiDAR, camera, ultrasonic, GPS, IMU, and V2X. According to the differences in the latest studies, we divide the fusion strategies into four categories and point out some shortcomings. Sensor fusion is mainly applied for multi-target tracking and environment reconstruction. We discuss the method of establishing a motion model and data association in multi-target tracking. At the end of the paper, we analyzed the deficiencies in the current studies and put forward some suggestions for further improvement in the future. Through this investigation, we hope to analyze the current situation of multi-sensor fusion in the automated driving process and provide more efficient and reliable fusion strategies.","","","10.1109/ACCESS.2019.2962554","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943388","Automated driving;multi-sensor fusion strategy;multi-target tracking;environmental reconstruction;data association;intent analysis;deep learning","","","","","","163","CCBY","","","","IEEE","IEEE Journals"
"Face Hallucination Using Cascaded Super-Resolution and Identity Priors","K. Grm; W. J. Scheirer; V. Štruc","Faculty of Electrical Engineering, University of Ljubljana, Ljubljana, Slovenia; Department of Computer Science and Engineering, University of Notre Dame, South Bend, IN, USA; Faculty of Electrical Engineering, University of Ljubljana, Ljubljana, Slovenia","IEEE Transactions on Image Processing","","2020","29","1","2150","2165","In this paper we address the problem of hallucinating high-resolution facial images from low-resolution inputs at high magnification factors. We approach this task with convolutional neural networks (CNNs) and propose a novel (deep) face hallucination model that incorporates identity priors into the learning procedure. The model consists of two main parts: i) a cascaded super-resolution network that upscales the low-resolution facial images, and ii) an ensemble of face recognition models that act as identity priors for the super-resolution network during training. Different from most competing super-resolution techniques that rely on a single model for upscaling (even with large magnification factors), our network uses a cascade of multiple SR models that progressively upscale the low-resolution images using steps of  $2\times $ . This characteristic allows us to apply supervision signals (target appearances) at different resolutions and incorporate identity constraints at multiple-scales. The proposed C-SRIP model (Cascaded Super Resolution with Identity Priors) is able to upscale (tiny) low-resolution images captured in unconstrained conditions and produce visually convincing results for diverse low-resolution inputs. We rigorously evaluate the proposed model on the Labeled Faces in the Wild (LFW), Helen and CelebA datasets and report superior performance compared to the existing state-of-the-art.","","","10.1109/TIP.2019.2945835","Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866753","Face hallucination;deep learning;CNN;identity","Face;Training;Face recognition;Signal resolution;Task analysis","","","","1","77","CCBY","","","","IEEE","IEEE Journals"
"Addressing the Sim2Real Gap in Robotic 3-D Object Classification","J. Weibel; T. Patten; M. Vincze","Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Wien, Austria; Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Wien, Austria; Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Wien, Austria","IEEE Robotics and Automation Letters","","2020","5","2","407","413","Object classification with 3D data is an essential component of any scene understanding method. It has gained significant interest in a variety of communities, most notably in robotics and computer graphics. While the advent of deep learning has progressed the field of 3D object classification, most work using this data type are solely evaluated on CAD model datasets. Consequently, current work does not address the discrepancies existing between real and artificial data. In this work, we examine this gap in an indoor service robotic context by specifically addressing the problem of classification when transferring from artificial CAD models to real reconstructed objects. This is performed by training on ModelNet (CAD models) and evaluating on ScanNet (objects extracted from reconstructed rooms). We show that standard methods do not perform well in this task. We thus introduce a method that carefully samples object parts that are reproducible under various transformations and hence robust. Using graph convolution to classify the composed graph of parts, our method improves upon the baseline. Code is publicly available at https://rgit.acin.tuwien.ac.at/jb.weibel/cad2real_object_clf.","","","10.1109/LRA.2019.2959497","Austrian Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931599","Object detection;segmentation and categorization;deep learning in robotics and automation;RGB-D perception","","","","","","34","IEEE","","","","IEEE","IEEE Journals"
"Scalable Topological Data Analysis and Visualization for Evaluating Data-Driven Models in Scientific Applications","S. Liu; D. Wang; D. Maljovec; R. Anirudh; J. J. Thiagarajan; S. A. Jacobs; B. C. Van Essen; D. Hysom; J. Yeom; J. Gaffney; L. Peterson; P. B. Robinson; H. Bhatia; V. Pascucci; B. K. Spears; P. Bremer","Lawrence Livermore National Laboratory; SCI InstituteUniversity of Utah; SCI InstituteUniversity of Utah; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory; SCI InstituteUniversity of Utah; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","291","300","With the rapid adoption of machine learning techniques for large-scale applications in science and engineering comes the convergence of two grand challenges in visualization. First, the utilization of black box models (e.g., deep neural networks) calls for advanced techniques in exploring and interpreting model behaviors. Second, the rapid growth in computing has produced enormous datasets that require techniques that can handle millions or more samples. Although some solutions to these interpretability challenges have been proposed, they typically do not scale beyond thousands of samples, nor do they provide the high-level intuition scientists are looking for. Here, we present the first scalable solution to explore and analyze high-dimensional functions often encountered in the scientific data analysis pipeline. By combining a new streaming neighborhood graph construction, the corresponding topology computation, and a novel data aggregation scheme, namely topology aware datacubes, we enable interactive exploration of both the topological and the geometric aspect of high-dimensional data. Following two use cases from high-energy-density (HED) physics and computational biology, we demonstrate how these capabilities have led to crucial new insights in both applications.","","","10.1109/TVCG.2019.2934594","U.S. Department of Energy; Lawrence Livermore National Laboratory; NSF; CGV; ACI; DOE/SciDAC; PSAAP; CCMSC; OAC; Intel Graphics and Visualization; Institutes of XeLLENCE program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8820172","Model Evaluation;Deep Learning;High-Dimensional Space;Topological Data Analysis;Inertial Confinement Fusion","Computational modeling;Data visualization;Data analysis;Analytical models;Topology;Physics;Predictive models","","","","","38","","","","","IEEE","IEEE Journals"
"Real-time CU-net based welding quality inspection algorithm in battery production","H. Zhang; X. Di; Y. Zhang","Harbin Institute of Technology, 47822 Harbin China 150001 (e-mail: 893560790@qq.com); Harbin Institute of Industry, 47822 Harbin China 150001 (e-mail: dixiaoguang@hit.edu.cn); Harbin Institute of Technology, 47822 Harbin China 150001 (e-mail: hitzhangyu@qq.com)","IEEE Transactions on Industrial Electronics","","2020","PP","99","1","1","In the production process of laser welding products, visual inspection is usually employed to rec- ognize welding spot locations and diagnose their quality faults. However, commonly used algorithms fail to succeed in both reliability and computational efficiency, especially when applied to assembly line. In this paper, a method based on deep learning algorithm and traditional computer vision (TCV) algorithm is proposed which achieves quality inspection of laser welding spots in the process of battery production. First, Compressed U-shape network (CU-net) is proposed to extract welding pads and welding spots. Then, a template-based method is proposed to confirm the validity of each welding spot. Finally, TCV heuristic algorithms are proposed to achieve three error detections, i.e, welding pad placed obliquely, electrode tab placed over highly, welding spot welded through. Moreover, we build a Welding Spot Quality Inspection Dataset taken from real assembly line. Compared with other pipelines including U- net, MaskRCNN and PSPNet, CU-net shows a significant su- periority in both processing speed and detection accuracy. The results of template-based method and TCV heuristic algorithms have shown the high computational efficiency and ensured the inspection accuracy. The inference time of whole method is less than 100ms with the implementation on NVIDIA 1060 and INTEL I7-6700.","","","10.1109/TIE.2019.2962421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8948316","Deep Learning;Battery Quality inspection;Welding Spot;Real time","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A Smart Collaborative Routing Protocol for Delay Sensitive Applications in Industrial IoT","M. Zhu; L. Chang; N. Wang; I. You","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing 100044, China.; Network Technology Research Institute of China Unicom, Beijing 100044, China.; Dawning Information Industy (Beijing) Corp. Ltd, Beijing 100193, China.; Department of Information Security Engineering, Soonchunhyang University, Asan 31538, South Korea. (e-mail: ilsunu@gmail.com)","IEEE Access","","2020","PP","99","1","1","In the industrial Internet of things (IIoT), there is always a strong demand for real-time information transfer. Especially when deploying wireless/wired hybrid networks in smart factories, the requirement for low delay interaction is more prominent. Although tree routing protocols have been successfully executed in simple networks, more challenges in transmission speed can be observed in the manufacturing broadband communication system. Motivated by the progresses in deep learning, a smart collaborative routing protocol with low delay and high reliability is proposed to accommodate mixed link scenarios. First, we establish a one-hop delay model to investigate the potential affects of Media Access Control (MAC) layer parameters, which supports the subsequent design. Second, forwarding, maintenance, and efficiency strategies are created to construct the basic functionalities for our routing protocol. Relevant procedures and key approaches are highlighted as well. Third, two sub-protocols are generated and the corresponding implementation steps are described. The experimental results demonstrate that the end-to-end delay can be effectively cut down through comprehensive improvements. Even more sensor nodes and larger network scale are involved, our proposed protocol can still illustrate the advantages comparing with existing solutions within IIoT.","","","10.1109/ACCESS.2019.2963723","Soonchunhyang University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949516","Industrial IoT;Deep learning;Routing protocol;Tree topology;Delay","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"OriNet: Robust 3-D Orientation Estimation With a Single Particular IMU","M. A. Esfahani; H. Wang; K. Wu; S. Yuan","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Robotics and Automation Letters","","2020","5","2","399","406","Estimating the robot's heading is a crucial requirement in odometry systems which are attempting to estimate the movement trajectory of a robot. Small errors in the orientation estimation result in a significant difference between the estimated and real trajectory, and failure of the odometry system. The odometry problem becomes much more complicated for micro flying robots since they cannot carry massive sensors. In this manner, they should benefit from the small size and low-cost sensors, such as IMU, to solve the odometry problem, and industries always look for such solutions. However, IMU suffers from bias and measurement noise, which makes the problem of position and orientation estimation challenging to be solved by a single IMU. While there are numerous studies on the fusion of IMU with other sensors, this study illustrates the power of the first deep learning framework for estimating the full 3D orientation of the flying robots (as yaw, pitch, and roll in quaternion coordinates) accurately with the presence of a single IMU. A particular IMU should be utilized during the training and testing of the proposed system. Besides, a method based on the Genetic Algorithm is introduced to measure the IMU bias in each execution. The results show that the proposed method improved the flying robots’ ability to estimate their orientation displacement by approximately 80% with the presence of a single particular IMU. The proposed approach also outperforms existing solutions that utilize a monocular camera and IMU simultaneously by approximately 30%.","","","10.1109/LRA.2019.2959507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931590","Localization;SLAM;deep learning in robotics and automation;autonomous vehicle navigation","","","","","","19","IEEE","","","","IEEE","IEEE Journals"
"Regional Intelligent Resource Allocation in Mobile Edge Computing based Vehicular Network","G. Wang; F. Xu","Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing 100876, China.; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing 100876, China.","IEEE Access","","2020","PP","99","1","1","The advancement of 5G technology has brought the prosperous development of Internet of Vehicles (IoV). IoV services are not only computational intensive but also extremely sensitive to the delay. As a promising computing paradigm, mobile edge computing (MEC) can be applied to IoV scenarios. However, due to the limited resources of a single MEC server, it is difficult to cope with the suddenly increased computation loads caused by emergencies, or the intensive resource requests from busy regions. Therefore, we propose a novel regional intelligent management vehicular system with dual MEC planes, in which MEC servers in the same region cooperate with each other to achieve resource sharing. We classify computing tasks into different types according to their delay tolerances and focus on the optimization problem of resource allocation for different type tasks. And then, we design a resource allocation algorithm based on deep reinforcement learning, which can adapt to the changeable MEC environment to process high-dimensional data. Simulation results confirm that our proposed scheme is feasible and effective.","","","10.1109/ACCESS.2020.2964018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950186","Internet of Vehicles;Mobile Edge Computing;Deep Reinforcement Learning;Resource Allocation;Delay Tolerance","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Automatic Building Extraction from High-resolution Aerial Imagery via Fully Convolutional Encoder-Decoder Network With Non-local Block","S. Wang; X. Hou; X. Zhao","College of Computer Science and Technology, Jilin University, Changchun 130012, China and Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, Changchun 130012, China.; College of Computer Science and Technology, Jilin University, Changchun 130012, China and Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, Changchun 130012, China.; College of Computer Science and Technology, Jilin University, Changchun 130012, China and Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, Changchun 130012, China.","IEEE Access","","2020","PP","99","1","1","Extracting buildings automatically from high-resolution aerial images is a significant and fundamental task for various practical applications, such as land-use statistics and urban planning. Recently, various methods based on deep learning, especially the fully convolution networks, achieve impressive scores in this challenging semantic segmentation task. However, the lack of global contextual information and the careless upsampling method limit the further improvement of the performance for building extraction task. To simultaneously address these problems, we propose a novel network named Efficient Non-local Residual U-shape Network(ENRU-Net), which is composed of a well designed U-shape encoder-decoder structure and an improved non-local block named asymmetric pyramid non-local block (APNB). The encoder-decoder structure is adopted to extract and restore the feature maps carefully, and APNB could capture global contextual information by utilizing self-attention mechanism. We evaluate the proposed ENRU-Net and compare it with other state-of-the-art models on two widely-used public aerial building imagery datasets: the Massachusetts Buildings Dataset and the WHU Aerial Imagery Dataset. The experiments show that the accuracy of ENRU-Net on these datasets has remarkable improvement against previous state-of-the-art semantic segmentation models, including FCN-8s, U-Net, SegNet and Deeplab v3. The subsequent analysis also indicates that our ENRU-Net has advantages in efficiency for building extraction from high-resolution aerial images.","","","10.1109/ACCESS.2020.2964043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950134","Deep learning;Semantic Segmentation;Fully Convolution Network;Building Extraction;Non-local Method","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Forensic Similarity for Digital Images","O. Mayer; M. C. Stamm","Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA; Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA","IEEE Transactions on Information Forensics and Security","","2020","15","","1331","1346","In this paper, we introduce a new digital image forensics approach called forensic similarity, which determines whether two image patches contain the same forensic trace or different forensic traces. One benefit of this approach is that prior knowledge, e.g., training samples, of a forensic trace is not required to make a forensic similarity decision on it in the future. To do this, we propose a two-part deep-learning system composed of a convolutional neural network-based feature extractor and a three-layer neural network, called the similarity network. This system maps the pairs of image patches to a score indicating whether they contain the same or different forensic traces. We evaluated the system accuracy of determining whether two image patches were captured by the same or different camera model and manipulated by the same or a different editing operation and the same or a different manipulation parameter, given a particular editing operation. Experiments demonstrate applicability to a variety of forensic traces and importantly show efficacy on “unknown” forensic traces that were not used to train the system. Experiments also show that the proposed system significantly improves upon prior art, reducing error rates by more than half. Furthermore, we demonstrated the utility of the forensic similarity approach in two practical applications: forgery detection and localization, and database consistency verification.","","","10.1109/TIFS.2019.2924552","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744262","Multimedia forensics;deep learning;forgery detection","","","","","","39","IEEE","","","","IEEE","IEEE Journals"
"LCSCNet: Linear Compressing-Based Skip-Connecting Network for Image Super-Resolution","W. Yang; X. Zhang; Y. Tian; W. Wang; J. Xue; Q. Liao","Department of Electronic Engineering, Shenzhen Key Lab of Information Science and Technology, Shenzhen Engineering Lab of IS&DRM, Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Department of Electronic Engineering, Shenzhen Key Lab of Information Science and Technology, Shenzhen Engineering Lab of IS&DRM, Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Department of Computer Science, University of Rochester, Rochester, NY, USA; Department of Electronic Engineering, Shenzhen Key Lab of Information Science and Technology, Shenzhen Engineering Lab of IS&DRM, Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Department of Statistical Science, University College London, London, U.K.; Department of Electronic Engineering, Shenzhen Key Lab of Information Science and Technology, Shenzhen Engineering Lab of IS&DRM, Graduate School at Shenzhen, Tsinghua University, Shenzhen, China","IEEE Transactions on Image Processing","","2020","29","","1450","1464","In this paper, we develop a concise but efficient network architecture called linear compressing based skip-connecting network (LCSCNet) for image super-resolution. Compared with two representative network architectures with skip connections, ResNet and DenseNet, a linear compressing layer is designed in LCSCNet for skip connection, which connects former feature maps and distinguishes them from newly-explored feature maps. In this way, the proposed LCSCNet enjoys the merits of the distinguish feature treatment of DenseNet and the parameter-economic form of ResNet. Moreover, to better exploit hierarchical information from both low and high levels of various receptive fields in deep models, inspired by gate units in LSTM, we also propose an adaptive element-wise fusion strategy with multi-supervised training. Experimental results in comparison with state-of-the-art algorithms validate the effectiveness of LCSCNet.","","","10.1109/TIP.2019.2940679","National Natural Science Foundation of China; Special Foundation for the Development of Strategic Emerging Industries of Shenzhen; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852859","Single-image super-resolution;deep convolutional neural networks;skip connection;feature fusion","Training;Neural networks;Image reconstruction;Computer architecture;Image coding;Network architecture","image resolution;learning (artificial intelligence);neural net architecture;recurrent neural nets","image super-resolution;representative network architectures;skip connection;linear compressing layer;LCSCNet;newly-explored feature maps;linear compressing-based skip-connecting network;network architecture;multisupervised training;LSTM","","","60","","","","","IEEE","IEEE Journals"
"Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures","D. Cashman; A. Perer; R. Chang; H. Strobelt","Tufts University, USA; Carnegie Mellon University, USA; Tufts University, USA; MIT IBM Watson AI Lab, USA","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","863","873","The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.","","","10.1109/TVCG.2019.2934261","DARPA; NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827593","visual analytics;neural networks;parameter space exploration","Neural networks;Computer architecture;Tools;Training;Visual analytics;Machine learning","","","","","73","","","","","IEEE","IEEE Journals"
"AddNet: Deep Neural Networks Using FPGA-Optimized Multipliers","J. Faraone; M. Kumm; M. Hardieck; P. Zipf; X. Liu; D. Boland; P. H. W. Leong","School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; Applied Computer Science Faculty, Fulda University of Applied Sciences, Fulda, Germany; Digital Technology Group, University of Kassel, Kassel, Germany; Digital Technology Group, University of Kassel, Kassel, Germany; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2020","28","1","115","128","Low-precision arithmetic operations to accelerate deep-learning applications on field-programmable gate arrays (FPGAs) have been studied extensively, because they offer the potential to save silicon area or increase throughput. However, these benefits come at the cost of a decrease in accuracy. In this article, we demonstrate that reconfigurable constant coefficient multipliers (RCCMs) offer a better alternative for saving the silicon area than utilizing low-precision arithmetic. RCCMs multiply input values by a restricted choice of coefficients using only adders, subtractors, bit shifts, and multiplexers (MUXes), meaning that they can be heavily optimized for FPGAs. We propose a family of RCCMs tailored to FPGA logic elements to ensure their efficient utilization. To minimize information loss from quantization, we then develop novel training techniques that map the possible coefficient representations of the RCCMs to neural network weight parameter distributions. This enables the usage of the RCCMs in hardware, while maintaining high accuracy. We demonstrate the benefits of these techniques using AlexNet, ResNet-18, and ResNet-50 networks. The resulting implementations achieve up to 50% resource savings over traditional 8-bit quantized networks, translating to significant speedups and power savings. Our RCCM with the lowest resource requirements exceeds 6-bit fixed point accuracy, while all other implementations with RCCMs achieve at least similar accuracy to an 8-bit uniformly quantized design, while achieving significant resource savings.","","","10.1109/TVLSI.2019.2939429","Australia–Germany Joint Research Co-operation Scheme; German Academic Exchange Service (DAAD); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8848603","Digital arithmetic;field programmable gate arrays (FPGAs);neural networks;neural network hardware;quantization","Field programmable gate arrays;Quantization (signal);Neural networks;Training;Hardware;Silicon;Throughput","","","","","41","IEEE","","","","IEEE","IEEE Journals"
"Progressively Refined Face Detection Through Semantics-Enriched Representation Learning","Z. Li; X. Tang; X. Wu; J. Liu; R. He","National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing and the CAS Center for Excellence in Brain Science and Intelligence Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Baidu Inc., Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Baidu Inc., Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing and the CAS Center for Excellence in Brain Science and Intelligence Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","","2020","15","","1394","1406","Feature pyramids aim to learn multi-scale representations for detecting faces over various scales. However, they often lack adequate context over different scales, especially when there are many tiny faces in the wild. In this paper, we propose an attention-guided semantically enriched feature aggregation framework to learn a feature pyramid with rich semantics at all scales for face detection. Specifically, high-level abstract features are directly integrated into low-level representations by skip connections to retain as much semantic as possible. In addition, an attention mechanism is employed as a gate to emphasize relevant features and suppress useless features during feature fusion. Inspired by human visual perception of tiny faces, we specially design a deep progressive refined loss (DPRL) to effectively facilitate feature learning. According to the above principles, we design and investigate various feature pyramid frameworks through extensive experiments. Finally, two typical structures named Centralized Attention Feature (CAF) and Distributed Attention Feature (DAF) are proposed for face detection, which are in-place and end-to-end trainable. Extensive experiments across different aggregation architectures on four challenging face detection benchmarks demonstrate the superiority of our framework over state-of-the-art methods.","","","10.1109/TIFS.2019.2941800","State Key Development Program; National Natural Science Foundation of China; Natural Science Foundation of Beijing Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839888","Face detection;object detection","Face;Feature extraction;Face detection;Semantics;Detectors;Visualization;Task analysis","","","","","61","IEEE","","","","IEEE","IEEE Journals"
"Fast Single Image Dehazing Using Saturation Based Transmission Map Estimation","S. E. Kim; T. H. Park; I. K. Eom","Department of Electronics Engineering, Pusan National University, Busan, South Korea; Department of Mechatronics Engineering, Tongmyong University, Busan, South Korea; Department of Electronics Engineering, Pusan National University, Busan, South Korea","IEEE Transactions on Image Processing","","2020","29","","1985","1998","Single image dehazing has been a challenging problem because of its ill-posed nature. For this reason, numerous efforts have been made in the field of haze removal. This paper proposes a simple, fast, and powerful algorithm for haze removal. The medium transmission is derived as a function of the saturation of the scene radiance only, and the saturation of scene radiance is estimated using a simple stretching method. A different medium transmission can be estimated for each pixel because this method does not assume that transmission is constant in a small patch. Furthermore, this paper presents a color veil removing algorithm, which is useful for an image with fine or yellow dust, using the white balance technique. The proposed algorithm requires no training, prior, and refinement process. The simulation results show that the proposed dehazing scheme outperforms state-of-the-art dehazing approaches in terms of both computational complexity and dehazing efficiency.","","","10.1109/TIP.2019.2948279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8882514","Single image dehazing;medium transmission;saturation stretch;white balance","Atmospheric modeling;Image color analysis;Scattering;Filtering;Estimation;Training;Deep learning","estimation theory;image colour analysis;image denoising;image enhancement;image restoration","color veil removing algorithm;single image dehazing;haze removal;scene radiance;stretching method;saturation based transmission map estimation","","","56","IEEE","","","","IEEE","IEEE Journals"
"Visual Tracking via Auto-Encoder Pair Correlation Filter","X. Cheng; Y. Zhang; L. Zhou; Y. Zheng","School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China; School of Information Science and Engineering, Southeast University, Nanjing, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Transactions on Industrial Electronics","","2020","67","4","3288","3297","Robust visual tracking is one of the most challenging problems in computer vision applications. However, the limited training data and the computational complexity have severely affected tracking performance. In this paper, we propose an auto-encoder pair model for visual tracking which is composed of source domain network and target domain network to help a more accurate localization. We adopt the dense circular samples of the object state to increase the number of training samples and prevent model overfitting. Meanwhile, a difference regularization term is also introduced into our framework to penalize the large appearance variations of the object in two domains. The alternating optimization is used to solve the optimization problems. Furthermore, our method alleviates the model update problem and improves the tracking speed by using long-term and short-term updating scheme. In addition, the target domain filter is updated by introducing the updated source domain filter to avoid the object drift. Comprehensive experiments on some challenging benchmarks demonstrate that our approach concurrently improves both tracking accuracy and speed.","","","10.1109/TIE.2019.2913815","National Natural Science Foundation of China; International Cooperation and Exchange Programme; National Natural Science Foundation of China; Equipment Advance Research Foundation Project of China; Nanjing University of Information Science and Technology; State Key Laboratory of Novel Software Technology; State Key Lab of CAD&CG; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8705675","Auto-encoder network;correlation filter (CF);optimization;visual tracking","Visualization;Correlation;Target tracking;Training data;Deep learning;Training;Decoding","","","","1","55","IEEE","","","","IEEE","IEEE Journals"
"Image Super-Resolution as a Defense Against Adversarial Attacks","A. Mustafa; S. H. Khan; M. Hayat; J. Shen; L. Shao","Inception Institute of Artificial Intelligence, UAE; Inception Institute of Artificial Intelligence, UAE; Inception Institute of Artificial Intelligence, UAE; Inception Institute of Artificial Intelligence, UAE; Inception Institute of Artificial Intelligence, UAE","IEEE Transactions on Image Processing","","2020","29","","1711","1724","Convolutional Neural Networks have achieved significant success across multiple computer vision tasks. However, they are vulnerable to carefully crafted, human-imperceptible adversarial noise patterns which constrain their deployment in critical security-sensitive systems. This paper proposes a computationally efficient image enhancement approach that provides a strong defense mechanism to effectively mitigate the effect of such adversarial perturbations. We show that deep image restoration networks learn mapping functions that can bring off-the-manifold adversarial samples onto the natural image manifold, thus restoring classification towards correct classes. A distinguishing feature of our approach is that, in addition to providing robustness against attacks, it simultaneously enhances image quality and retains models performance on clean images. Furthermore, the proposed method does not modify the classifier or requires a separate mechanism to detect adversarial images. The effectiveness of the scheme has been demonstrated through extensive experiments, where it has proven a strong defense in gray-box settings. The proposed scheme is simple and has the following advantages: 1) it does not require any model training or parameter optimization, 2) it complements other existing defense mechanisms, 3) it is agnostic to the attacked model and attack type, and 4) it provides superior performance across all popular attack algorithms. Our codes are publicly available at https://github.com/aamir-mustafa/super-resolution-adversarial-defense.","","","10.1109/TIP.2019.2940533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844865","Adversarial attacks;gray-box setting;CNNs;image super-resolution;image denoising","Image resolution;Perturbation methods;Computational modeling;Manifolds;Transform coding;Robustness;Training","computer vision;convolutional neural nets;image enhancement;image resolution;image restoration;learning (artificial intelligence);security of data","image enhancement approach;popular attack algorithms;attack type;attacked model;defense mechanisms;adversarial images;clean images;models performance;image quality;natural image manifold;off-the-manifold adversarial samples;mapping functions;deep image restoration networks;adversarial perturbations;strong defense mechanism;critical security-sensitive systems;human-imperceptible adversarial noise patterns;multiple computer vision tasks;Convolutional Neural Networks;adversarial attacks;image super-resolution","","","62","IEEE","","","","IEEE","IEEE Journals"
"Neural Architecture Search for Skin Lesion Classification","A. Kwasigroch; M. Grochowski; A. Mikołajczyk","Gdańsk University of Technology, Faculty of Electrical and Control Engineering.; Gdańsk University of Technology, Faculty of Electrical and Control Engineering.; Gdańsk University of Technology, Faculty of Electrical and Control Engineering.","IEEE Access","","2020","PP","99","1","1","Deep neural networks have achieved great success in many domains. However, successful deployment of such systems is determined by proper manual selection of the neural architecture. This is a tedious and time-consuming process that requires expert knowledge. Different tasks need very different architectures to obtain satisfactory results. The group of methods called the neural architecture search (NAS) helps to find effective architecture in an automated manner. In this paper, we present the use of an architecture search framework to solve the medical task of malignant melanoma detection. Unlike many other methods tested on benchmark datasets, we tested it on practical problem, which differs greatly in terms of difficulty in distinguishing between classes, resolution of images, data balance within the classes, and the number of data available. In order to find a suitable network structure, the hill-climbing search strategy was employed along with network morphism operations to explore the search space. The network morphism operations allow for incremental increases in the network size with the use of the previously trained network. This kind of knowledge reusing allows significantly reducing the computational cost. The proposed approach produces structures that achieve similar results to those provided by manually designed structures, at the same time making use of almost 20 times fewer parameters. What is more, the search process lasts on average only 18h on single GPU.","","","10.1109/ACCESS.2020.2964424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950333","deep learning;convolutional neural network;neural architecture search;network morphism;malignant melanoma","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"RGB-T Image Saliency Detection via Collaborative Graph Learning","Z. Tu; T. Xia; C. Li; X. Wang; Y. Ma; J. Tang","Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, China","IEEE Transactions on Multimedia","","2020","22","1","160","173","Image saliency detection is an active research topic in the community of computer vision and multimedia. Fusing complementary RGB and thermal infrared data has been proven to be effective for image saliency detection. In this paper, we propose an effective approach for RGB-T image saliency detection. Our approach relies on a novel collaborative graph learning algorithm. In particular, we take superpixels as graph nodes, and collaboratively use hierarchical deep features to jointly learn graph affinity and node saliency in a unified optimization framework. Moreover, we contribute a more challenging dataset for the purpose of RGB-T image saliency detection, which contains 1000 spatially aligned RGB-T image pairs and their ground truth annotations. Extensive experiments on the public dataset and the newly created dataset suggest that the proposed approach performs favorably against the state-of-the-art RGB-T saliency detection methods.","","","10.1109/TMM.2019.2924578","National Natural Science Foundation of China; National Natural Science Foundation of China; Natural Science Foundation of Anhui Province; Natural Science Foundation of Anhui Higher Education Institution of China; Open Fund for Discipline Construction; Institute of Physical Science and Information Technology; Anhui University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744296","Image saliency detection;RGB-thermal fusion;Collaborative graph;Joint optimization;Benchmark dataset","","","","","","44","IEEE","","","","IEEE","IEEE Journals"
"Sketch Fewer to Recognize More by Learning A Co-regularized Sparse Representation","Y. Qi; Y. Song","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China. (e-mail: qiyg@bupt.edu.cn); SketchX Research Lab, Centre for Vision Speech and Signal Processing (CVSSP), University of Surrey, Guildford, Surrey, GU2 7XH, UK.","IEEE Transactions on Circuits and Systems for Video Technology","","2020","PP","99","1","1","Categorizing free-hand human sketches has profound implications in applications such as human computer interaction and image retrieval. The task is non-trivial due to the iconic nature of sketches, signified by large variances in both appearance and structure when compared with photographs. Despite recent advances made by deep learning methods, the requirement of a large training set is commonly imposed making them impractical for real-world applications where training sketches are cumbersome to obtain – sketches have to be handdrawn one by one other than crawled freely on the Internet. In this work, we aim to delve further into the data scarcity problem of sketch-related research, by proposing a few-shot sketch classification framework. The model is based on a coregularized embedding algorithm where common/shareable parts of learned human sketches are exploited, thereby can embed query sketch into a co-regularized sparse representation space for few-shot classification. A new dataset of 8,000 part-level annotated sketches of 100 categories is also proposed to facilitate future research. Experiment shows that our approach can achieve an 5-way one-shot classification accuracy of 85%, and 20-way one-shot at 51%.","","","10.1109/TCSVT.2019.2963862","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949551","Sketch categorization;Few-shot classification","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Reverse JPEG Compatibility Attack","J. Butora; J. Fridrich","Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY, USA; Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY, USA","IEEE Transactions on Information Forensics and Security","","2020","15","","1444","1454","A novel steganalysis method for JPEG images is introduced that is universal in the sense that it reliably detects any type of steganography as well as small payloads. It is limited to quality factors 99 and 100. The detection statistic is formed from the rounding errors in the spatial domain after decompressing the JPEG image. The attack works whenever, during compression, the discrete cosine transform is applied to integer-valued signal. Reminiscent of the well-established JPEG compatibility steganalysis, we call the new approach the “reverse JPEG compatibility attack.” While the attack is introduced and analyzed under simplifying assumptions using reasoning based on statistical signal detection, the best detection in practice is obtained with machine learning tools. Experiments on diverse datasets of both grayscale and color images, five steganographic schemes, and with a variety of JPEG compressors demonstrate the universality and applicability of this steganalysis method in practice.","","","10.1109/TIFS.2019.2940904","National Science Foundation; Defense Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832246","Steganography;steganalysis;JPEG;quality factor 100;reverse compatibility;rounding errors;deep learning","Transform coding;Discrete cosine transforms;Quantization (signal);Image coding;Gaussian distribution;Detectors;Payloads","","","","","44","IEEE","","","","IEEE","IEEE Journals"
"Structured Medical Pathology Data Hiding Information Association Mining Algorithm Based on Optimized Convolutional Neural Network","X. Li; Y. Wang; G. Liu","Department of Information Engineering, Heilongjiang International University, Harbin, China; Department of Mechanical Engineering, Harbin Institute of Petroleum, Harbin, China; College of Computer Science and Technology, Harbin Engineering University, Harbin, China","IEEE Access","","2020","8","","1443","1452","When using traditional algorithms to mine the association of hiding information in medical pathological data, there are some problems, such as low recognition rate of association and poor accuracy of mining results. Therefore, structured medical pathology data hiding information association mining algorithm based on optimized convolution neural network is proposed. Firstly, an information feature is optimized based on rough set relative classification information entropy and ant colony algorithm and the optimized feature matrix is obtained. The information in the optimized feature matrix is weighted, and the weighted features of hiding information are obtained. Secondly, the hiding information feature matrix is transmitted to the convolution neural network for learning, and the weight of the connection layer is extracted. The importance of the corresponding area of the weight is confirmed by the distribution of the weight value, and the feature average matrix is obtained. According to the matrix, the feature of hiding information data is enhanced. The hiding information in the structured medical pathology data is generalized by using the Gaussian Bell function, and the hiding information generalization processing result is combined with the adjacent matrix in the convolution neural network to construct the hiding information classification model. Finally, the classification standard is defined, the cooperative association of hiding information group is obtained, and the mining of association between hiding information of structured medical pathological data is completed. The experimental results show that the proposed algorithm has good feature optimization effect, and the information association recognition rate is high, the anti-interference ability and accuracy are better than the current related results, the highest recall rate is 99.24%, which is much higher than the traditional algorithm, which shows that the algorithm is effective.","","","10.1109/ACCESS.2019.2960456","National Natural Science Foundation of China; Ministry of Education Science and Technology Development Center Industry-University Research Innovation Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935218","Convolutional neural network;deep learning;medical pathology data;hiding information;association mining","","","","","","51","CCBY","","","","IEEE","IEEE Journals"
"Double-Channel Object Tracking With Position Deviation Suppression","J. Chu; X. Tu; L. Leng; J. Miao","Key Laboratory of Jiangxi Province for Image Processing and Pattern Recognition, Nanchang Hangkong University, Nanchang, China; Key Laboratory of Jiangxi Province for Image Processing and Pattern Recognition, Nanchang Hangkong University, Nanchang, China; Key Laboratory of Jiangxi Province for Image Processing and Pattern Recognition, Nanchang Hangkong University, Nanchang, China; Key Laboratory of Lunar and Deep Space Exploration, National Astronomical Observatories, Chinese Academy of Sciences, Beijing, China","IEEE Access","","2020","8","","856","866","The object tracking methods based on multi-domain convolutional neural network (MDNet) commonly fail to track in the case of background clutter. A novel double-channel object tracking (DCOT) is proposed to solve this problem. The discriminative correlation filter (DCF), which has strong discriminative power of low-level features, is employed for the position deviation suppress of the samples generated from MDNet. Firstly the pre-trained deep network is used to learn and classify the target and background in the video frames. If the tracked position of the DCF is judged to be correct, we delete the target candidate samples with high position deviation from MDNet. The position deviation is measured by the distance between the tracked positions of the DCF and MDNet. Finally, MDNet and DCF are updated with a robust update strategy. The experiments are performed on OTB-100 and VOT-2016. The overlap precision and distance precision of DCOT on OTB-100 are 92.2% and 69.5%, respectively, which are higher than those of MDNet by 1.3% and 1.7%. The results of DCOT in background clutter are higher than those of SANet by 0.2% and 2.8%, respectively. DCOT is also superior to other state-of-the-art trackers on VOT-2016.","","","10.1109/ACCESS.2019.2961778","National Natural Science Foundation of China; China Scholarship Council; Jiangxi Provincial Department of Science and Technology; Construction Project of Advantageous Science and Technology Innovation Team in Jiangxi Province; Open Foundation of Key Laboratory of Jiangxi Province for Image Processing and Pattern Recognition; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8939361","Double-channel object tracking;position deviation suppression;DCF;MDNet","","","","","","43","CCBY","","","","IEEE","IEEE Journals"
"Cooperative Training of Descriptor and Generator Networks","J. Xie; Y. Lu; R. Gao; S. Zhu; Y. N. Wu","Hikvision Research Institute, Santa Clara, CA, USA; Facebook, Menlo Park, CA, USA; Department of Statistics, University of California, Los Angeles, Los Angeles, CA, USA; Department of Statistics, University of California, Los Angeles, Los Angeles, CA, USA; Department of Statistics, University of California, Los Angeles, Los Angeles, CA, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2020","42","1","27","45","This paper studies the cooperative training of two generative models for image modeling and synthesis. Both models are parametrized by convolutional neural networks (ConvNets). The first model is a deep energy-based model, whose energy function is defined by a bottom-up ConvNet, which maps the observed image to the energy. We call it the descriptor network. The second model is a generator network, which is a non-linear version of factor analysis. It is defined by a top-down ConvNet, which maps the latent factors to the observed image. The maximum likelihood learning algorithms of both models involve MCMC sampling such as Langevin dynamics. We observe that the two learning algorithms can be seamlessly interwoven into a cooperative learning algorithm that can train both models simultaneously. Specifically, within each iteration of the cooperative learning algorithm, the generator model generates initial synthesized examples to initialize a finite-step MCMC that samples and trains the energy-based descriptor model. After that, the generator model learns from how the MCMC changes its synthesized examples. That is, the descriptor model teaches the generator model by MCMC, so that the generator model accumulates the MCMC transitions and reproduces them by direct ancestral sampling. We call this scheme MCMC teaching. We show that the cooperative algorithm can learn highly realistic generative models.","","","10.1109/TPAMI.2018.2879081","Hikvision gift fund; NSF DMS; DARPA SIMPLEX; ONR MURI; DARPA ARO; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519332","Deep generative models;Energy-based models;Latent variable models;Bottom-up and top-down convolutional neural networks;Modified contrastive divergence;MCMC teaching","Generators;Training;Computational modeling;Inference algorithms;Heuristic algorithms;Analytical models","","","","1","74","IEEE","","","","IEEE","IEEE Journals"
"Towards Weakly-Supervised Focus Region Detection via Recurrent Constraint Network","W. Zhao; X. Hou; X. Yu; Y. He; H. Lu","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; Institute of Information Fusion, Naval Aviation University, Yantai, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Image Processing","","2020","29","","1356","1367","Recent state-of-the-art methods on focus region detection (FRD) rely on deep convolutional networks trained with costly pixel-level annotations. In this study, we propose a FRD method that achieves competitive accuracies but only uses easily obtained bounding box annotations. Box-level tags provide important cues of focus regions but lose the boundary delineation of the transition area. A recurrent constraint network (RCN) is introduced for this challenge. In our static training, RCN is jointly trained with a fully convolutional network (FCN) through box-level supervision. The RCN can generate a detailed focus map to locate the boundary of the transition area effectively. In our dynamic training, we iterate between fine-tuning FCN and RCN with the generated pixel-level tags and generate finer new pixel-level tags. To boost the performance further, a guided conditional random field is developed to improve the quality of the generated pixel-level tags. To promote further study of the weakly supervised FRD methods, we construct a new dataset called FocusBox, which consists of 5000 challenging images with bounding box-level labels. Experimental results on existing datasets demonstrate that our method not only yields comparable results than fully supervised counterparts but also achieves a faster speed.","","","10.1109/TIP.2019.2942505","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Northwestern Polytechnical University; Dalian University of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8848865","Focus region detection;recurrent constraint network;fully convolutional network;box-level supervision","Training;Task analysis;Object segmentation;Semantics;Image segmentation;Dogs","convolutional neural nets;image classification;image segmentation;learning (artificial intelligence);object detection;recurrent neural nets","recurrent constraint network;state-of-the-art methods;deep convolutional networks;costly pixel-level annotations;FRD method;box annotations;box-level tags;focus regions;boundary delineation;transition area;RCN;static training;fully convolutional network;box-level supervision;detailed focus map;dynamic training;fine-tuning FCN;generated pixel-level tags;weakly supervised FRD methods;box-level labels;fully supervised counterparts;weakly-supervised focus region detection","","","46","","","","","IEEE","IEEE Journals"
"A Deep Convolutional Neural Network With Fuzzy Rough Sets for FER","X. Chen; D. Li; P. Wang; X. Yang","School of Computer, Jiangsu University of Science and Technology, Zhenjiang, China; China Shipbuilding Industry Corporation 723, Yangzhou, China; School of Science, Jiangsu University of Science and Technology, Zhenjiang, China; School of Computer, Jiangsu University of Science and Technology, Zhenjiang, China","IEEE Access","","2020","8","","2772","2779","Existing facial emotion recognition methods do not have high accuracy and are not sufficient practical in real-time applications. We introduce type 2 fuzzy rough sets to develop a Type 2 Fuzzy Rough Convolutional Neural Network, as type 2 fuzzy rough sets form a suitable mathematical tool to characterize uncertainty of classifification. Based on the type 2 fuzzy rough sets theory, we construct an optimization objective for training CNNs by minimizing fuzzy classification uncertainty, and present the defifinition and optimization of type 2 fuzzy rough loss, which can be achieved by better performance. This method could reduce the uncertainty in terms of vagueness and indiscernibility by using type 2 fuzzy rough sets theory and specififically removing noise samples by using CNN from raw data. And finally, compared the proposed method with other feature extraction and learning techniques based on Algorithm Adaption k-Nearest-Neighbors. Experimental results demonstrate that type 2 fuzzy rough sets convolutional neural network could achieve better performances comparing with other methods.","","","10.1109/ACCESS.2019.2960769","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8937504","Convolutional neural network;Type 2 fuzzy rough sets;algorithm adaption k-nearest-neighbors","","","","","","35","CCBY","","","","IEEE","IEEE Journals"
"A semi-supervised attention model for identifying authentic sneakers","Y. Yang; N. Zhu; Y. Wu; J. Cao; D. Zhan; H. Xiong","National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai 200240, China; Alibaba Company, Hangzhou 310000, China; Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai 200240, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; Rutgers University, New York, NJ 07102, USA","Big Data Mining and Analytics","","2020","3","1","29","40","To protect consumers and those who manufacture and sell the products they enjoy, it is important to develop convenient tools to help consumers distinguish an authentic product from a counterfeit one. The advancement of deep learning techniques for fine-grained object recognition creates new possibilities for genuine product identification. In this paper, we develop a Semi-Supervised Attention (SSA) model to work in conjunction with a large-scale multiple-source dataset named YSneaker, which consists of sneakers from various brands and their authentication results, to identify authentic sneakers. Specifically, the SSA model has a self-attention structure for different images of a labeled sneaker and a novel prototypical loss is designed to exploit unlabeled data within the data structure. The model draws on the weighted average of the output feature representations, where the weights are determined by an additional shallow neural network. This allows the SSA model to focus on the most important images of a sneaker for use in identification. A unique feature of the SSA model is its ability to take advantage of unlabeled data, which can help to further minimize the intra-class variation for more discriminative feature embedding. To validate the model, we collect a large number of labeled and unlabeled sneaker images and perform extensive experimental studies. The results show that YSneaker together with the proposed SSA architecture can identify authentic sneakers with a high accuracy rate.","","","10.26599/BDMA.2019.9020017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935093","sneaker identification;fine-grained classification;multi-instance learning;attention mechanism","","","","","","","","","","","TUP","TUP Journals"
"Model-Based and Data-Driven Strategies in Medical Image Computing","D. Rueckert; J. A. Schnabel","Department of Computing, Imperial College London, London, U.K.; School of Biomedical Engineering and Imaging Sciences, King’s College London, London, U.K.","Proceedings of the IEEE","","2020","108","1","110","124","Model-based approaches for image reconstruction, analysis, and interpretation have made significant progress over the past decades. Many of these approaches are based on either mathematical, physical, or biological models. A challenge for these approaches is the modeling of the underlying processes (e.g., the physics of image acquisition or the patho-physiology of a disease) with appropriate levels of detail and realism. With the availability of large amounts of imaging data and machine learning (in particular deep learning) techniques, data-driven approaches have become more widespread for use in different tasks in reconstruction, analysis, and interpretation. These approaches learn statistical models directly from labeled or unlabeled image data and have been shown to be very powerful for extracting clinically useful information from medical imaging. While these data-driven approaches often outperform traditional model-based approaches, their clinical deployment often poses challenges in terms of robustness, generalization ability, and interpretability. In this article, we discuss what developments have motivated the shift from model-based approaches toward data-driven strategies and what potential problems are associated with the move toward purely data-driven approaches, in particular deep learning. We also discuss some of the open challenges for data-driven approaches, e.g., generalization to new unseen data (e.g., transfer learning), robustness to adversarial attacks, and interpretability. Finally, we conclude with a discussion on how these approaches may lead to the development of more closely coupled imaging pipelines that are optimized in an end-to-end fashion.","","","10.1109/JPROC.2019.2943836","Engineering and Physical Sciences Research Council; Wellcome Trust; Wellcome Trust/EPSRC IEH Award; Innovate UK; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8867900","Artificial neural networks;biomedical imaging;image analysis;image classification;image processing;image reconstruction;image registration;image segmentation;machine learning","Biomedical imaging;Biological system modeling;Computational modeling;Mathematical model;Data models;Image reconstruction","","","","","123","IEEE","","","","IEEE","IEEE Journals"
"Target-Specific Siamese Attention Network for Real-Time Object Tracking","K. Thanikasalam; C. Fookes; S. Sridharan; A. Ramanan; A. Pinidiyaarachchi","Department of Physical Science, University of Jaffna at Vavuniya, Vavuniya, Sri Lanka; SAIVT Lab, Queensland University of Technology (QUT), Brisbane, QLD, Australia; SAIVT Lab, Queensland University of Technology (QUT), Brisbane, QLD, Australia; Department of Computer Science, University of Jaffna, Jaffna, Sri Lanka; Department of Statistics and Computer Science, University of Peradeniya, Peradeniya, Sri Lanka","IEEE Transactions on Information Forensics and Security","","2020","15","","1276","1289","Deep similarity trackers are able to track above real-time speed. However, their accuracy is considerably lower than deep classification based trackers since they avoid valuable online cues. To feed the target-specific information for real-time object tracking, we propose a novel Siamese attention network. Different types of attention mechanisms are used to capture different contexts of target information and then learned knowledge is used to feed target cues at different representation levels of similarity tracking. In addition, an online learning mechanism is employed to utilise the available target-specific data. The proposed tracker reduces the impact of noise in the target template and improves the accuracy of similarity tracking by feeding target cues into the similarity search. Extensive evaluation performed on OTB-2013/50/100 and VOT2018 benchmark datasets demonstrate the proposed tracker outperforms state-of-the-art approaches while maintaining real-time tracking speed.","","","10.1109/TIFS.2019.2935871","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8804242","Visual object tracking;deep neural networks;Siamese network;attention network","","","","","1","66","IEEE","","","","IEEE","IEEE Journals"
"Noiseprint: A CNN-Based Camera Model Fingerprint","D. Cozzolino; L. Verdoliva","Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione (DIETI), Università degli studi di Napoli Federico II, Naples, Italy; Dipartimento di Ingegneria Industriale (DII), Università degli studi di Napoli Federico II, Naples, Italy","IEEE Transactions on Information Forensics and Security","","2020","15","","144","159","Forensic analyses of digital images rely heavily on the traces of in-camera and out-camera processes left on the acquired images. Such traces represent a sort of camera fingerprint. If one is able to recover them, by suppressing the high-level scene content and other disturbances, a number of forensic tasks can be easily accomplished. A notable example is the PRNU pattern, which can be regarded as a device fingerprint, and has received great attention in multimedia forensics. In this paper, we propose a method to extract a camera model fingerprint, called noiseprint, where the scene content is largely suppressed and model-related artifacts are enhanced. This is obtained by means of a Siamese network, which is trained with pairs of image patches coming from the same (label +1) or different (label -1) cameras. Although the noiseprints can be used for a large variety of forensic tasks, in this paper we focus on image forgery localization. Experiments on several datasets widespread in the forensic community show noiseprint-based methods to provide state-of-the-art performance.","","","10.1109/TIFS.2019.2916364","Defense Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8713484","Digital image forensics;noise residual;siamese networks;deep learning","Cameras;Training;Feature extraction;Forgery;Task analysis;Image forensics","cameras;convolutional neural nets;feature extraction;fingerprint identification;image forensics","CNN-based camera model fingerprint;forensic analyses;digital images;camera fingerprint;high-level scene content;forensic tasks;PRNU pattern;device fingerprint;multimedia forensics;model-related artifacts;image patches;image forgery localization;forensic community;noiseprint","","","76","","","","","IEEE","IEEE Journals"
"Accelerating Convolutional Neural Networks by Removing Interspatial and Interkernel Redundancies","L. Zeng; X. Tian","CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application Systems, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application Systems, University of Science and Technology of China, Hefei, China","IEEE Transactions on Cybernetics","","2020","50","2","452","464","Recently, the high computational resource demands of convolutional neural networks (CNNs) have hindered a wide range of their applications. To solve this problem, many previous works attempted to reduce the redundant calculations during the evaluation of CNNs. However, these works mainly focused on either interspatial or interkernel redundancy. In this paper, we further accelerate existing CNNs by removing both types of redundancies. First, we convert interspatial redundancy into interkernel redundancy by decomposing one convolutional layer to one block that we design. Then, we adopt rank-selection and pruning methods to remove the interkernel redundancy. The rank-selection method, which considerably reduces manpower, contributes to determining the number of kernels to be pruned in the pruning method. We apply a layer-wise training algorithm rather than the traditional end-to-end training to overcome the difficulty of convergence. Finally, we fine-tune the entire network to achieve better performance. Our method is applied on three widely used datasets of an image classification task. We achieve better results in terms of accuracy and compression rate compared with previous state-of-the-art methods.","","","10.1109/TCYB.2018.2873762","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Fok Ying Tung Education Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8497062","Convolutional neural networks (CNNs);deep learning application;image classification;network accelerating;network compression","Redundancy;Kernel;Training;Acceleration;Convolution;Computational efficiency;Neural networks","","","","","48","IEEE","","","","IEEE","IEEE Journals"
"End-to-End Latent Fingerprint Search","K. Cao; D. Nguyen; C. Tymoszek; A. K. Jain","Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA","IEEE Transactions on Information Forensics and Security","","2020","15","","880","894","Latent fingerprints are one of the most important and widely used sources of evidence in law enforcement and forensic agencies. Yet the performance of the state-of-the-art latent recognition systems is far from satisfactory, and they often require manual markups to boost the latent search performance. Further, the COTS systems are proprietary and do not output the true comparison scores between a latent and reference prints to conduct quantitative evidential analysis. We present an end-to-end latent fingerprint search system, including automated region of interest (ROI) cropping, latent image preprocessing, feature extraction, feature comparison, and outputs a candidate list. Two separate minutiae extraction models provide complementary minutiae templates. To compensate for the small number of minutiae in small ridge area and poor quality latents, a virtual minutiae set is generated to construct a texture template. A 96-dimensional descriptor is extracted for each minutia from its neighborhood. For computational efficiency, the descriptor length for virtual minutiae is further reduced to 16 using product quantization. Our end-to-end system is evaluated on four latent databases: NIST SD27 (258 latents); MSP (1200 latents), WVU (449 latents), and N2N (10 000 latents) against a background set of 100K rolled prints, which includes the true rolled mates of the latents with rank-1 retrieval rates of 65.7%, 69.4%, 65.5%, and 7.6%, respectively. A multi-core solution implemented on 24 cores obtains 1-ms per latent to rolled comparison.","","","10.1109/TIFS.2019.2930487","Intelligence Advanced Research Projects Activity; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768396","Latent fingerprint recognition;end-to-end system;deep learning;autoencoder;minutiae descriptor;texture template;reference fingerprint","Conferences;Indexes;Typesetting;Standards;Loading;Portable document format;Web sites","feature extraction;fingerprint identification;forensic science;image matching;image retrieval;image texture;law administration;visual databases","poor quality latents;virtual minutiae set;latent databases;state-of-the-art latent recognition systems;latent search performance;COTS systems;end-to-end latent fingerprint search system;latent image preprocessing;complementary minutiae templates;NIST SD27 database;reference prints;minutiae extraction models;law enforcement;forensic agencies;quantitative evidential analysis;automated region of interest cropping;automated ROI cropping;feature extraction;feature comparison;texture template;96-dimensional descriptor;descriptor length;virtual minutiae;product quantization;MSP database;WVU database;N2N database;rank-1 retrieval rates;multicore solution","","","43","","","","","IEEE","IEEE Journals"
"Depth Based Semantic Scene Completion With Position Importance Aware Loss","J. Li; Y. Liu; X. Yuan; C. Zhao; R. Siegwart; I. Reid; C. Cadena","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science, The University of Adelaide, Adelaide, SAAustralia; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland; School of Computer Science, The University of Adelaide, Adelaide, SAAustralia; Autonomous Systems Lab, ETH Zurich, Zurich, Switzerland","IEEE Robotics and Automation Letters","","2020","5","1","219","226","Semantic scene completion (SSC) refers to the task of inferring the 3D semantic segmentation of a scene while simultaneously completing the 3D shapes. We propose PALNet, a novel hybrid network for SSC based on single depth. PALNet utilizes a two-stream network to extract both 2D and 3D features from multi-stages using fine-grained depth information to efficiently capture the context, as well as the geometric cues of the scene. Current methods for SSC treat all parts of the scene equally causing unnecessary attention to the interior of objects. To address this problem, we propose Position Aware Loss (PA-Loss) which is position importance aware while training the network. Specifically, PA-Loss considers Local Geometric Anisotropy to determine the importance of different positions within the scene. It is beneficial for recovering key details like the boundaries of objects and the corners of the scene. Comprehensive experiments on two benchmark datasets demonstrate the effectiveness of the proposed method and its superior performance. Code and demo11Video demo can be found here: https://youtu.be/j-LAMcMh0yg. are avaliable at https://github.com/UniLauX/PALNet.","","","10.1109/LRA.2019.2953639","National Natural Science Foundation of China; Australian Research Council through the Centre of Excellence for Robotic; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8902045","Semantic scene understanding;deep learning in robotics and automation;RGB-D perception","","","","","","30","IEEE","","","","IEEE","IEEE Journals"
"Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data","R. Krueger; J. Beyer; W. Jang; N. W. Kim; A. Sokolov; P. K. Sorger; H. Pfister","School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA; School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA; School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA; School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA; Laboratory of Systems Pharmacology, Harvard Medical School, Boston, MA, USA; Laboratory of Systems Pharmacology, Harvard Medical School, Boston, MA, USA; School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA","IEEE Transactions on Visualization and Computer Graphics","","2020","26","1","227","237","Facetto is a scalable visual analytics application that is used to discover single-cell phenotypes in high-dimensional multi-channel microscopy images of human tumors and tissues. Such images represent the cutting edge of digital histology and promise to revolutionize how diseases such as cancer are studied, diagnosed, and treated. Highly multiplexed tissue images are complex, comprising 109 or more pixels, 60-plus channels, and millions of individual cells. This makes manual analysis challenging and error-prone. Existing automated approaches are also inadequate, in large part, because they are unable to effectively exploit the deep knowledge of human tissue biology available to anatomic pathologists. To overcome these challenges, Facetto enables a semi-automated analysis of cell types and states. It integrates unsupervised and supervised learning into the image and feature exploration process and offers tools for analytical provenance. Experts can cluster the data to discover new types of cancer and immune cells and use clustering results to train a convolutional neural network that classifies new cells accordingly. Likewise, the output of classifiers can be clustered to discover aggregate patterns and phenotype subsets. We also introduce a new hierarchical approach to keep track of analysis steps and data subsets created by users; this assists in the identification of cell types. Users can build phenotype trees and interact with the resulting hierarchical structures of both high-dimensional feature and image spaces. We report on use-cases in which domain scientists explore various large-scale fluorescence imaging datasets. We demonstrate how Facetto assists users in steering the clustering and classification process, inspecting analysis results, and gaining new scientific insights into cancer biology.","","","10.1109/TVCG.2019.2934547","NCI; KAUST; OSR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827951","Clustering;Classification;Visual Analysis;Multiplex Tissue Imaging;Digital Pathology;Cancer Systems Biology","Cancer;Tools;Visualization;Rendering (computer graphics);Biomedical imaging;Multiplexing","","","","","70","","","","","IEEE","IEEE Journals"
"SemiMap: A Semi-Folded Convolution Mapping for Speed-Overhead Balance on Crossbars","L. Deng; L. Liang; G. Wang; L. Chang; X. Hu; X. Ma; L. Liu; J. Pei; G. Li; Y. Xie","Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Center for Brain Inspired Computing Research, Beijing Innovation Center for Future Chip, Tsinghua University, Beijing, China; Fert Beijing Research Institute, BDBC, Beihang University, Beijing, China; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Center for Brain Inspired Computing Research, Beijing Innovation Center for Future Chip, Tsinghua University, Beijing, China; Center for Brain Inspired Computing Research, Beijing Innovation Center for Future Chip, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2020","39","1","117","130","Crossbar architecture has been widely used in neural network (NN) accelerators, involving conventional and emerging devices. It performs well on the fully connected layer through efficient vector–matrix multiplication. Whereas, the advantages degrade on the convolutional layer with huge data reuse, since the execution speed and resource overhead are imbalanced when using existing fully unfolded or fully folded mapping strategy. To address this issue, we propose a novel semi-folded mapping (SemiMap) framework for implementing the convolution on crossbars. It simultaneously folds the physical resources along the row dimension of feature maps (FMs) and unfolds them along the column dimension. The former reduces the resource overhead, and the latter maintains the parallelism. An FM slicing scheme is further proposed to enable the processing of large-size image. Via our mapping framework, a row-by-row streaming pipeline for intraimage dataflow and periodical pipeline for interimage dataflow are easy to be obtained. To validate the idea, we build a many-crossbar architecture with several designs to guarantee the overall functionality and performance. Based on the measurement data of a fabricated chip, a mapping compiler and a cycle-accurate simulator are developed for the hardware simulation of large-scale networks. We evaluate the proposed SemiMap on various convolutional NNs across different network scale.  ${>} 35 {\times }$  resource saving and several hundred times cycle reduction are demonstrated compared to the existing fully unfolded and fully folded strategies, respectively. This paper jumps out of the current extreme mapping schemes, and provides a balanced solution on how to efficiently deploy the computational graphs with data reuse on many-crossbar architecture.","","","10.1109/TCAD.2018.2883959","National Natural Science Foundation of China; National Science Foundation; SRC nCORE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8556049","Convolutional neural networks (CNNs);crossbar architecture;deep learning accelerator;network mapping","","","","","1","43","USGov","","","","IEEE","IEEE Journals"
"A Multimodal Target-Source Classifier with Attention Branches to Understand Ambiguous Instructions for Fetching Daily Objects","A. Magassouba; K. Sugiura; H. Kawai","Universal Communication Research Institute, NICT, Kyoto Japan 6190289 (e-mail: aly.magassouba@nict.go.jp); ASTREC, National Institute of Information and Communications Tech., Seika, Soraku, Kyoto Japan 619-0289 (e-mail: komei.sugiura@nict.go.jp); National Institute of Information and Communications Technology, Japan (e-mail: hisashi.kawai@nict.go.jp)","IEEE Robotics and Automation Letters","","2020","PP","99","1","1","In this study, we focus on multimodal language understanding for fetching instructions in the domestic service robots context. This task consists of predicting a target object, as instructed by the user, given an image and an unstructured sentence, such as “Bring me the yellow box (from the wooden cabinet)”. This is challenging because of the ambiguity of natural language, {i.e.}, the relevant information may be missing or there might be several candidates. To solve such a task, we propose the multimodal target-source classifier model with attention branches (MTCM-AB), which is an extension of the MTCM \cite{magassouba2019understanding}. Our methodology uses the attention branch network (ABN) \cite{Fukui_2019_CVPR} to develop a multimodal attention mechanism based on linguistic and visual inputs. Experimental validation using a standard dataset showed that the MTCM-AB outperformed both state-of-the-art methods and MTCM. In particular the MTCM-AB accuracy on average was 90.1% while human performance was 90.3% on the PFN-PIC dataset.","","","10.1109/LRA.2019.2963649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949709","Deep Learning in Robotics and Automation;Domestic Robots","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"SLOAM: Semantic Lidar Odometry and Mapping for Forest Inventory","S. W. Chen; G. Vicentim Nardari; E. S. Lee; C. Qu; X. Liu; R. A. F. Romero; V. Kumar","Computer and Information Science, University of Pennsylvania, Philadelphia, PA United States of America 19103 (e-mail: chenste@seas.upenn.edu); Department of Computer Sciences, University of Sao Paulo, Sao Carlos, Sao Paulo Brazil 13560210 (e-mail: guinardari@usp.br); Computer and Information Science, University of Pennsylvania, Philadelphia, PA United States of America 19146 (e-mail: elijahsjlee@gmail.com); Computer and Information Science, University of Pennsylvania, Philadelphia, Pennsylvania United States of America 19104 (e-mail: quchao@seas.upenn.edu); SEAS, University of Pennsylvania, Philadelphia, PA United States of America 19104 (e-mail: liuxu@seas.upenn.edu); Deparment of Comptuer Science, Universidade de Sao Paulo, SaO CARLOS, SAO PAULO Brazil 13560-970 (e-mail: rafrance@icmc.usp.br); University of Pennsylvania, School of Engineering and Applied Science, Department of Mechanical Engineering and Applied Mechanic, United States of America (e-mail: vijay.kumar@seas.upenn.edu)","IEEE Robotics and Automation Letters","","2020","PP","99","1","1","This paper describes an end-to-end pipeline for tree diameter estimation based on semantic segmentation and lidar odometry and mapping. Accurate mapping of this type of environment is challenging since the ground and the trees are surrounded by leaves, thorns and vines, and the sensor typically experiences extreme motion. We propose a semantic feature based pose optimization that simultaneously refines the tree models while estimating the robot pose. The pipeline utilizes a custom virtual reality tool for labeling 3D scans that is used to train a semantic segmentation network. The masked point cloud is used to compute a Trellis graph that identifies individual instances and extracts relevant features that are used by the SLAM module. We show that traditional lidar and image based methods fail in the forest environment on both UAV and hand-carry systems, while our method is more robust, scalable, and automatically generates tree diameter estimations.","","","10.1109/LRA.2019.2963823","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949363","Robotics in Agriculture and Forestry;SLAM;Deep Learning in Robotics and Automation;Virtual Reality and Interfaces","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Invariance-Preserving Localized Activation Functions for Graph Neural Networks","L. Ruiz; F. Gama; A. G. Marques; A. Ribeiro","Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA; Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA; Department of Signal Theory and Communications, King Juan Carlos University, Fuenlabrada, Madrid, Spain; Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA","IEEE Transactions on Signal Processing","","2020","68","","127","141","Graph signals are signals with an irregular structure that can be described by a graph. Graph neural networks (GNNs) are information processing architectures tailored to these graph signals and made of stacked layers that compose graph convolutional filters with nonlinear activation functions. Graph convolutions endow GNNs with invariance to permutations of the graph nodes’ labels. In this paper, we consider the design of trainable nonlinear activation functions that take into consideration the structure of the graph. This is accomplished by using graph median filters and graph max filters, which mimic linear graph convolutions and are shown to retain the permutation invariance of GNNs. We also discuss modifications to the backpropagation algorithm necessary to train local activation functions. The advantages of localized activation function architectures are demonstrated in four numerical experiments: source localization on synthetic graphs, authorship attribution of 19th century novels, movie recommender systems and scientific article classification. In all cases, localized activation functions are shown to improve model capacity.","","","10.1109/TSP.2019.2955832","National Science Foundation; ISTC-WAS and Intel DevCloud; Spanish MINECO; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911416","Deep learning;convolutional neural networks;graph signal processing;nonlinear graph filters;activation functions;max filters;median filters","Convolution;Convolutional neural nets;Signal resolution;Training;Large scale integration","","","","","40","IEEE","","","","IEEE","IEEE Journals"
"Exploiting Epistemic Uncertainty of Anatomy Segmentation for Anomaly Detection in Retinal OCT","P. Seeböck; J. I. Orlando; T. Schlegl; S. M. Waldstein; H. Bogunović; S. Klimscha; G. Langs; U. Schmidt-Erfurth","Department of Biomedical Imaging and Image-Guided Therapy, Computational Imaging Research Lab, Medical University Vienna, Vienna, Austria; Department of Biomedical Imaging and Image-Guided Therapy, Computational Imaging Research Lab, Medical University Vienna, Vienna, Austria; Department of Biomedical Imaging and Image-Guided Therapy, Computational Imaging Research Lab, Medical University Vienna, Vienna, Austria; Department of Biomedical Imaging and Image-Guided Therapy, Computational Imaging Research Lab, Medical University Vienna, Vienna, Austria; Department of Biomedical Imaging and Image-Guided Therapy, Computational Imaging Research Lab, Medical University Vienna, Vienna, Austria; Department of Biomedical Imaging and Image-Guided Therapy, Computational Imaging Research Lab, Medical University Vienna, Vienna, Austria; Department of Biomedical Imaging and Image-Guided Therapy, Computational Imaging Research Lab, Medical University Vienna, Vienna, Austria; Department of Biomedical Imaging and Image-Guided Therapy, Computational Imaging Research Lab, Medical University Vienna, Vienna, Austria","IEEE Transactions on Medical Imaging","","2020","39","1","87","98","Diagnosis and treatment guidance are aided by detecting relevant biomarkers in medical images. Although supervised deep learning can perform accurate segmentation of pathological areas, it is limited by requiring a priori definitions of these regions, large-scale annotations, and a representative patient cohort in the training set. In contrast, anomaly detection is not limited to specific definitions of pathologies and allows for training on healthy samples without annotation. Anomalous regions can then serve as candidates for biomarker discovery. Knowledge about normal anatomical structure brings implicit information for detecting anomalies. We propose to take advantage of this property using Bayesian deep learning, based on the assumption that epistemic uncertainties will correlate with anatomical deviations from a normal training set. A Bayesian U-Net is trained on a well-defined healthy environment using weak labels of healthy anatomy produced by existing methods. At test time, we capture epistemic uncertainty estimates of our model using Monte Carlo dropout. A novel post-processing technique is then applied to exploit these estimates and transfer their layered appearance to smooth blob-shaped segmentations of the anomalies. We experimentally validated this approach in retinal optical coherence tomography (OCT) images, using weak labels of retinal layers. Our method achieved a Dice index of 0.789 in an independent anomaly test set of age-related macular degeneration (AMD) cases. The resulting segmentations allowed very high accuracy for separating healthy and diseased cases with late wet AMD, dry geographic atrophy (GA), diabetic macular edema (DME) and retinal vein occlusion (RVO). Finally, we qualitatively observed that our approach can also detect other deviations in normal scans such as cut edge artifacts.","","","10.1109/TMI.2019.2919951","Christian Doppler Forschungsgesellschaft; Nvidia; National Foundation for Research, Technology, and Development; Austrian Science Fund; Vienna Science and Technology Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8727461","Weakly supervised learning;anomaly detection;biomarker discovery;optical coherence tomography;epistemic uncertainty","Retina;Uncertainty;Diseases;Anomaly detection;Image segmentation;Biomarkers;Training","","","","","47","IEEE","","","","IEEE","IEEE Journals"
"A Bluetooth-Low-Energy Sensor Node for Acoustic Monitoring of Small Birds","M. Magno; F. Vultier; B. Szebedy; H. Yamahachi; R. H. R. Hahnloser; L. Benini","Department of Information Technology and Electrical Engineering, ETH Zürich, Zürich, Switzerland; Department of Information Technology and Electrical Engineering, ETH Zürich, Zürich, Switzerland; Department of Information Technology and Electrical Engineering, ETH Zürich, Zürich, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zürich, Zürich, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zürich, Zürich, Switzerland; Department of Information Technology and Electrical Engineering, ETH Zürich, Zürich, Switzerland","IEEE Sensors Journal","","2020","20","1","425","433","Animals can generate sounds that serve a wide range of vital functions such as to defend themselves or their territories, to attract a partner, to maintain contact with other members of their social group, and to help themselves and their partner/group during navigation. Ethologists are interested in recording and analyzing these sounds, many of which are vocalizations. Advances in sensing and wireless technology permit today acoustic data acquisition and transmission in a wireless manner. In many applications, the wireless sensor needs to be placed on the animal’s body and should be unobtrusive, light-weight, small, and long-lasting. This paper presents the design and development of an ultra-low power miniaturized and lightweight wireless sensor node for monitoring captive zebra finches. The node is designed to be worn with minimal effort by small-sized birds to collect, process, and send/receive data to/from a remote host via Bluetooth Low-Energy. The main feature of the developed node is the capability to stream compressed or uncompressed audio and temperature data continuously. Multiple nodes can monitor several birds simultaneously and acquire and transmit high-quality audio streams, one for each bird, with low audio interference. Due to the combination of low-power hardware and software techniques and technologies, the 1.4 g node achieves a lifetime of up to 24 h at 4 kHz sampling rate on a single zinc-air battery. Experimental results on birds confirm the functionality of the developed wireless node and the lifetime benefits of compression.","","","10.1109/JSEN.2019.2940282","Swiss National Science Foundation Project ‘MicroLearn: Micropower Deep Learning’; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832176","Wireless sensors node;low power design;Bluetooth;acoustic sensor;audio compression;bird monitoring;energy efficiency","Sensors;Birds;Monitoring;Wireless communication;Wireless sensor networks;Bluetooth","","","","","30","IEEE","","","","IEEE","IEEE Journals"
"Evaluating Adversarial Evasion Attacks in the Context of Wireless Communications","B. Flowers; R. M. Buehrer; W. C. Headley","Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA","IEEE Transactions on Information Forensics and Security","","2020","15","","1102","1113","Recent advancements in radio frequency machine learning (RFML) have demonstrated the use of raw in-phase and quadrature (IQ) samples for multiple spectrum sensing tasks. Yet, deep learning techniques have been shown, in other applications, to be vulnerable to adversarial machine learning (ML) techniques, which seek to craft small perturbations that are added to the input to cause a misclassification. The current work differentiates the threats that adversarial ML poses to RFML systems based on where the attack is executed from: direct access to classifier input, synchronously transmitted over the air (OTA), or asynchronously transmitted from a separate device. Additionally, the current work develops a methodology for evaluating adversarial success in the context of wireless communications, where the primary metric of interest is bit error rate and not human perception, as is the case in image recognition. The methodology is demonstrated using the well known Fast Gradient Sign Method to evaluate the vulnerabilities of raw IQ based Automatic Modulation Classification and concludes RFML is vulnerable to adversarial examples, even in OTA attacks. However, RFML domain specific receiver effects, which would be encountered in an OTA attack, can present significant impairments to adversarial evasion.","","","10.1109/TIFS.2019.2934069","Bradley Masters Fellowship through the Bradley Department of Electrical and Computer Engineering at Virginia Tech; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792120","Cognitive radio security;machine learning;modulation classification","Perturbation methods;Receivers;Transmitters;Wireless communication;Modulation","","","","","34","IEEE","","","","IEEE","IEEE Journals"
"Cloud-Based Automated Clinical Decision Support System for Detection and Diagnosis of Lung Cancer in Chest CT","A. Masood; P. Yang; B. Sheng; H. Li; P. Li; J. Qin; V. Lanfranchi; J. Kim; D. D. Feng","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, University of Sheffield, Sheffield, U.K.; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University Affiliated Sixth People’s Hospital, Shanghai, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong; Department of Computer Science, University of Sheffield, Sheffield, U.K.; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, NSW, Australia; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, NSW, Australia","IEEE Journal of Translational Engineering in Health and Medicine","","2020","8","","1","13","Lung cancer is a major cause for cancer-related deaths. The detection of pulmonary cancer in the early stages can highly increase survival rate. Manual delineation of lung nodules by radiologists is a tedious task. We developed a novel computer-aided decision support system for lung nodule detection based on a 3D Deep Convolutional Neural Network (3DDCNN) for assisting the radiologists. Our decision support system provides a second opinion to the radiologists in lung cancer diagnostic decision making. In order to leverage 3-dimensional information from Computed Tomography (CT) scans, we applied median intensity projection and multi-Region Proposal Network (mRPN) for automatic selection of potential region-of-interests. Our Computer Aided Diagnosis (CAD) system has been trained and validated using LUNA16, ANODE09, and LIDC-IDR datasets; the experiments demonstrate the superior performance of our system, attaining sensitivity, specificity, AUROC, accuracy, of 98.4%, 92%, 96% and 98.51% with 2.1 FPs per scan. We integrated cloud computing, trained and validated our Cloud-Based 3DDCNN on the datasets provided by Shanghai Sixth People’s Hospital, as well as LUNA16, ANODE09, and LIDC-IDR. Our system outperformed the state-of-the-art systems and obtained an impressive 98.7% sensitivity at 1.97 FPs per scan. This shows the potentials of deep learning, in combination with cloud computing, for accurate and efficient lung nodule detection via CT imaging, which could help doctors and radiologists in treating lung cancer patients.","","","10.1109/JTEHM.2019.2955458","National Natural Science Foundation of China; National Key Research and Development Program of China; Hong Kong Polytechnic University; Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8922769","Computer-aided diagnosis;nodule detection;cloud computing;computed tomography;lung cancer","Cancer;Lung;Computed tomography;Training;Solid modeling;Cloud computing;Machine learning","","","","","38","CCBY","","","","IEEE","IEEE Journals"
"A New Two-Level Hierarchical Diagnosis Network Based on Convolutional Neural Network","L. Wen; X. Li; L. Gao","State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Instrumentation and Measurement","","2020","69","2","330","338","Fault diagnosis is vital for modern industry, and an increasing number of intelligent methods have been proposed for the fault diagnosis. However, most of the studies focus on distinguishing different fault patterns while ignoring fault deterioration. In this paper, a new hierarchical convolutional neural network (HCNN) is proposed as the two-level hierarchical diagnosis network, and it has two characteristics: 1) the fault pattern and fault severity are modeled as one hierarchical structure and 2) the fault pattern and fault severity can be estimated at the same time. Based on these, a new structure of HCNN is designed, which has two classifiers. Then, a two-stage training method is developed for HCNN to train these two classifiers at once training. The proposed HCNN is conducted on three case studies and has achieved state-of-the-art results. The results show that HCNN outperforms traditional two-layer hierarchical fault diagnosis network, and other machine learning and deep learning methods.","","","10.1109/TIM.2019.2896370","National Natural Science Foundation for Distinguished Young Scholars of China; National Natural Science Foundation of China; Natural Science Foundation of Hubei Province; China Postdoctoral Science Foundation; Program for HUST Academic Frontier Youth Team; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8649683","Convolutional neural network (CNN);fault diagnosis;hierarchical diagnosis network","Fault diagnosis;Training;Support vector machines;Convolutional neural networks;Feature extraction;Machine learning;Adaptation models","","","","3","40","IEEE","","","","IEEE","IEEE Journals"
"Automatically Dismantling Online Dating Fraud","G. Suarez-Tangil; M. Edwards; C. Peersman; G. Stringhini; A. Rashid; M. Whitty","King’s College London, London, U.K.; University of Bristol, Bristol, U.K.; University of Bristol, Bristol, U.K.; Boston University, Boston, MA, USA; University of Bristol, Bristol, U.K.; The University of Melbourne, Parkville, VIC, Australia","IEEE Transactions on Information Forensics and Security","","2020","15","","1128","1137","Online romance scams are a prevalent form of mass-marketing fraud in the West, and yet few studies have presented data-driven responses to this problem. In this type of scam, fraudsters craft fake profiles and manually interact with their victims. Because of the characteristics of this type of fraud and how dating sites operate, traditional detection methods (e.g., those used in spam filtering) are ineffective. In this paper, we investigate the archetype of online dating profiles used in this form of fraud, including their use of demographics, profile descriptions, and images, shedding light on both the strategies deployed by scammers to appeal to victims and the traits of victims themselves. Furthermore, in response to the severe financial and psychological harm caused by dating fraud, we develop a system to detect romance scammers on online dating platforms. This paper presents the first fully described system for automatically detecting this fraud. Our aim is to provide an early detection system to stop romance scammers as they create fraudulent profiles or before they engage with potential victims. Previous research has indicated that the victims of romance scams score highly on scales for idealized romantic beliefs. We combine a range of structured, unstructured, and deep-learned features that capture these beliefs in order to build a detection system. Our ensemble machine-learning approach is robust to the omission of profile details and performs at high accuracy (97%) in a hold-out validation set. The system enables development of automated tools for dating site providers and individual users.","","","10.1109/TIFS.2019.2930479","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768406","Computer security","Feature extraction;Machine learning;Tools;IP networks;Data mining;Sociology;Statistics","","","","1","35","IEEE","","","","IEEE","IEEE Journals"
"A Data-Efficient Approach for Automated Classification of OCT Images using Generative Adversarial Network","V. Das; S. Dandapat; P. K. Bora","EEE, Indian Institute of Technology Guwahati, 28678 Guwahati, Assam India 781039 (e-mail: vineetadas@iitg.ac.in); EEE, Indian Institute of Technology Guwahati, 28678 Guwahati, Assam India (e-mail: samaren@iitg.ac.in); EEE, Indian Institute of Technology Guwahati, 28678 Guwahati, Assam India (e-mail: prabin@iitg.ernet.in)","IEEE Sensors Letters","","2020","PP","99","1","1","Deep learning algorithms can offer a reliable automated interpretation of retinal optical coherence tomography (OCT) images to assist clinicians in disease diagnosis and management. However, retinal image processing presents pertinent obstacles such as the struggle of large scale data acquisition and high cost of annotation. To address this, we have developed a data-efficient semi-supervised generative adversarial network (GAN) based classifier for automated diagnosis with limited labeled data. The framework consists of a generator and a discriminator. The adversarial learning between them assists in building a generalizable classifier to predict progressive retinal diseases like age-related macular degeneration (AMD) and diabetic macular edema (DME). Experimental results on clinical-grade OCT images show an overall improvement of more than 10% in accuracy compared to the state-of-the-art methods.","","","10.1109/LSENS.2019.2963712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949716","Sensor signals processing;classification;optical coherence tomography;generative adversarial network;semi-supervised learning","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Discovering the Type 2 Diabetes in Electronic Health Records Using the Sparse Balanced Support Vector Machine","M. Bernardini; L. Romeo; P. Misericordia; E. Frontoni","Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy; Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy; Federazione Italiana Medici di Medicina Generale, Rome, Italy; Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy","IEEE Journal of Biomedical and Health Informatics","","2020","24","1","235","246","The diagnosis of type 2 diabetes (T2D) at an early stage has a key role for an adequate T2D integrated management system and patient's follow-up. Recent years have witnessed an increasing amount of available electronic health record (EHR) data and machine learning (ML) techniques have been considerably evolving. However, managing and modeling this amount of information may lead to several challenges, such as overfitting, model interpretability, and computational cost. Starting from these motivations, we introduced an ML method called sparse balanced support vector machine (SB-SVM) for discovering T2D in a novel collected EHR dataset (named Federazione Italiana Medici di Medicina Generale dataset). In particular, among all the EHR features related to exemptions, examination, and drug prescriptions, we have selected only those collected before T2D diagnosis from an uniform age group of subjects. We demonstrated the reliability of the introduced approach with respect to other ML and deep learning approaches widely employed in the state-of-the-art for solving this task. Results evidence that the SB-SVM overcomes the other state-of-the-art competitors providing the best compromise between predictive performance and computation time. Additionally, the induced sparsity allows to increase the model interpretability, while implicitly managing high-dimensional data and the usual unbalanced class distribution.","","","10.1109/JBHI.2019.2899218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641396","Type 2 diabetes;machine learning;electronic health record;support vector machine;decision support system","Diabetes;Support vector machines;Feature extraction;Computational modeling;Data models;Decision support systems;Informatics","","","","","51","IEEE","","","","IEEE","IEEE Journals"
"Receptive Field Size Versus Model Depth for Single Image Super-Resolution","R. Wang; M. Gong; D. Tao","Union Visual Innovation Technology Co., Ltd., Shenzhen, China; School of Mathematics and Statistics, The University of Melbourne, Melbourne, VIC, Australia; UBTECH Sydney Artificial Intelligence Centre, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Image Processing","","2020","29","","1669","1682","The performance of single image super-resolution (SISR) has been largely improved by innovative designs of deep architectures. An important claim raised by these designs is that the deep models have large receptive field size and strong nonlinearity. However, we are concerned about the question that which factor, receptive field size or model depth, is more critical for SISR. Towards revealing the answers, in this paper, we propose a strategy based on dilated convolution to investigate how the two factors affect the performance of SISR. Our findings from exhaustive investigations suggest that SISR is more sensitive to the changes of receptive field size than to the model depth variations, and that the model depth must be congruent with the receptive field size to produce improved performance. These findings inspire us to design a shallower architecture which can save computational and memory cost while preserving comparable effectiveness with respect to a much deeper architecture.","","","10.1109/TIP.2019.2941327","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8848837","Receptive field size;model depth;dilated convolution;single image super-resolution","Convolution;Image resolution;Computer architecture;Task analysis;Interpolation;Computational modeling;Image reconstruction","convolutional neural nets;image reconstruction;image resolution;learning (artificial intelligence)","receptive field size;single image superresolution;SISR;model depth variations;dilated convolution","","","58","IEEE","","","","IEEE","IEEE Journals"
"A Unified Probabilistic Formulation of Image Aesthetic Assessment","H. Zeng; Z. Cao; L. Zhang; A. C. Bovik","Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Da-Jiang Innovations, Shenzhen, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA","IEEE Transactions on Image Processing","","2020","29","","1548","1561","Image aesthetic assessment (IAA) has been attracting considerable attention in recent years due to the explosive growth of digital photography in Internet and social networks. The IAA problem is inherently challenging, owning to the ineffable nature of the human sense of aesthetics and beauty, and its close relationship to understanding pictorial content. Three different approaches to framing and solving the problem have been posed: binary classification, average score regression and score distribution prediction. Solutions that have been proposed have utilized different types of aesthetic labels and loss functions to train deep IAA models. However, these studies ignore the fact that the three different IAA tasks are inherently related. Here, we reveal that the use of the different types of aesthetic labels can be developed within the same statistical framework, which we use to create a unified probabilistic formulation of all the three IAA tasks. This unified formulation motivates the use of an efficient and effective loss function for training deep IAA models to conduct different tasks. We also discuss the problem of learning from a noisy raw score distribution which hinders network performance. We then show that by fitting the raw score distribution to a more stable and discriminative score distribution, we are able to train a single model which is able to obtain highly competitive performance on all three IAA tasks. Extensive qualitative analysis and experimental results on image aesthetic benchmarks validate the superior performance afforded by the proposed formulation. The source code is available at https://github.com/HuiZeng/Unified_IAA.","","","10.1109/TIP.2019.2941778","National Natural Science Foundation of China; Hong Kong RGC RIF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846580","Image aesthetic assessment;unified probabilistic formulation","Task analysis;Probabilistic logic;Computational modeling;Measurement;Predictive models;Training;Explosives","image classification;probability;regression analysis","unified probabilistic formulation;image aesthetic assessment;digital photography;social networks;aesthetics;beauty;average score regression;score distribution prediction;aesthetic labels;loss functions;deep IAA models;noisy raw score distribution;network performance;stable score distribution;discriminative score distribution;image aesthetic benchmarks;pictorial content;binary classification","","","49","","","","","IEEE","IEEE Journals"
"An Optimized Registration Method Based on Distribution Similarity and DVF Smoothness for 3D PET and CT Images","H. Kang; H. Jiang; X. Zhou; H. Yu; T. Hara; H. Fujita; Y. Yao","Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China; Department of Electrical, Electronic and Computer Engineering, Faculty of Engineering, Gifu University, Gifu-shi, Japan; Software College, Northeastern University, Shenyang, China; Department of Electrical, Electronic and Computer Engineering, Faculty of Engineering, Gifu University, Gifu-shi, Japan; Department of Electrical, Electronic and Computer Engineering, Faculty of Engineering, Gifu University, Gifu-shi, Japan; Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA","IEEE Access","","2020","8","","1135","1145","A fusion image combining both anatomical and functional information obtained by registering medical images of two different modalities, Positron Emission Tomography (PET) and Computed Tomography (CT), is of great significance for medical image analysis and diagnosis. Medical image registration relies on similarity measure which is low between PET/CT image voxels and therefore PET/CT registration is a challenging task. To address this issue, this paper presents an unsupervised end-to-end method, DenseRegNet, for deformable 3D PET/CT image registration. The method consists of two stages: (1) predicting 3D displacement vector field (DVF); and (2) registering 3D image. In the 3D DVF prediction stage, a two-level similarity measure together with a deformation regularization is proposed as loss function to optimize network training.In the image registration stage, a resampler and a spatial transformer are utilized to obtain the registration results. In this paper, 663 pairs of Uptake Value (SUV) and Hounsfield Unit (Hu) patches of 106 patients, 227 pairs of SUV and Hu patches of 35 patients and 259 pairs of SUV and Hu patches of 35 patients are randomly selected as training, validation and test set, respectively. Normalized cross correlation (NCC), intersection over union (IoU) of liver bounding box and euclidean distance (ED) on landmark points are used to evaluate the registration results. Experiment results show that the proposed method, DenseRegNet, achieves the best results in terms of liver bounding box IoU and ED, and the second highest value of NCC. For a trained model, given a new pair of PET/CT images, the registration result can be obtained with only one forward calculation within 10 seconds. Through qualitative and quantitative analyses, we demonstrate that, compared with other deep learning registration models, the proposed DenseRegNet achieves improved results in the challenging deformable PET/CT registration task.","","","10.1109/ACCESS.2019.2961268","National Natural Science Foundation of China; Ministry of Education, Culture, Sports, Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8937543","PET/CT registration;unsupervised learning;two-level similarity measure;deformation regularization","","","","","","29","CCBY","","","","IEEE","IEEE Journals"
"FAMED-Net: A Fast and Accurate Multi-Scale End-to-End Dehazing Network","J. Zhang; D. Tao","UBTECH Sydney Artificial Intelligence Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW, Australia; UBTECH Sydney Artificial Intelligence Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW, Australia","IEEE Transactions on Image Processing","","2020","29","","72","84","Single image dehazing is a critical image pre-processing step for subsequent high-level computer vision tasks. However, it remains challenging due to its ill-posed nature. Existing dehazing models tend to suffer from model overcomplexity and computational inefficiency or have limited representation capacity. To tackle these challenges, here, we propose a fast and accurate multi-scale end-to-end dehazing network, called FAMED-Net, which comprises encoders at three scales and a fusion module to efficiently and directly learn the haze-free image. Each encoder consists of cascaded and densely connected point-wise convolutional layers and pooling layers. Since no larger convolutional kernels are used and features are reused layer-by-layer, FAMED-Net is lightweight and computationally efficient. Thorough empirical studies on public synthetic datasets (including RESIDE) and real-world hazy images demonstrate the superiority of FAMED-Net over other representative state-of-the-art models with respect to model complexity, computational efficiency, restoration accuracy, and cross-set generalization. The code will be made publicly available.","","","10.1109/TIP.2019.2922837","Australian Research Council; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8753731","Dehazing;image restoration;point-wise convolution;deep neural network","Atmospheric modeling;Computational modeling;Scattering;Computer architecture;Convolutional codes;Channel estimation;Computer vision","computer vision;convolutional neural nets;image coding;image restoration","single image dehazing;dehazing models;encoder;haze-free image;image pre-processing step;high-level computer vision tasks;densely connected point-wise convolutional layers;cascaded connected point-wise convolutional layers;FAMED-Net;multiscale end-to-end dehazing network;pooling layers","","1","50","","","","","IEEE","IEEE Journals"
"Spatiotemporal Knowledge Distillation for Efficient Estimation of Aerial Video Saliency","J. Li; K. Fu; S. Zhao; S. Ge","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China","IEEE Transactions on Image Processing","","2020","29","","1902","1914","The performance of video saliency estimation techniques has achieved significant advances along with the rapid development of Convolutional Neural Networks (CNNs). However, devices like cameras and drones may have limited computational capability and storage space so that the direct deployment of complex deep saliency models becomes infeasible. To address this problem, this paper proposes a dynamic saliency estimation approach for aerial videos via spatiotemporal knowledge distillation. In this approach, five components are involved, including two teachers, two students and the desired spatiotemporal model. The knowledge of spatial and temporal saliency is first separately transferred from the two complex and redundant teachers to their simple and compact students, while the input scenes are also degraded from high-resolution to low-resolution to remove the probable data redundancy so as to greatly speed up the feature extraction process. After that, the desired spatiotemporal model is further trained by distilling and encoding the spatial and temporal saliency knowledge of two students into a unified network. In this manner, the inter-model redundancy can be removed for the effective estimation of dynamic saliency on aerial videos. Experimental results show that the proposed approach is comparable to 11 state-of-the-art models in estimating visual saliency on aerial videos, while its speed reaches up to 28,738 FPS and 1,490.5 FPS on the GPU and CPU platforms, respectively.","","","10.1109/TIP.2019.2946102","National Natural Science Foundation of China; Beijing Nova Program; Beijing Municipal Science and Technology Commission; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8868103","Spatiotemporal knowledge distillation;visual saliency estimation;aerial video","Computational modeling;Redundancy;Estimation;Visualization;Data models;Spatiotemporal phenomena;Drones","convolutional neural nets;feature extraction;image resolution;learning (artificial intelligence);video signal processing","spatiotemporal knowledge distillation;convolutional neural networks;computational capability;data redundancy;spatial saliency knowledge;temporal saliency knowledge;inter-model redundancy;aerial video saliency estimation;feature extraction","","","80","IEEE","","","","IEEE","IEEE Journals"
"Loopy Residual Hashing: Filling the Quantization Gap for Image Retrieval","J. Bai; Z. Li; B. Ni; M. Wang; X. Yang; C. Hu; W. Gao","Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering and the MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Technology, Peking University, Beijing, China","IEEE Transactions on Multimedia","","2020","22","1","215","228","Hashing has been widely used in large-scale image retrieval based on approximate nearest neighbor search. Most learning-to-hashing methods adopt a two-stage algorithm to generate binary codes. First, original images are mapped into continuous visual features. Then, binary codes are generated by quantization step or separate projection. Nevertheless, these methods are sensitive to quantization operation, i.e., thresholding. To explicitly address this issue, this study proposes a novel feature quantization scheme with a loopy recurrent neural network, called loopy residual hashing, for the purpose of high accuracy in image retrieval. Instead of one-off thresholding-based feature binarization, the proposed approach performs an iterative threshold-then-approximate operation, which calculates the quantization residual after each thresholding step and then imitates another round of binarization to further approximate the coding residual. The resulting sequences of binary codes possess higher representation accuracy and extensive experiments on image retrieval demonstrate its superior discriminative capability over the prior art. In the meantime, theoretical approximation error analysis is given.","","","10.1109/TMM.2019.2922130","National Natural Science Foundation of China; SJTU-BIGO LIVE; China's Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8734890","Deep Hashing;Recurrent Neural Networks;Saliency;Residual","Binary codes;Quantization (signal);Semantics;Image retrieval;Feature extraction;Visualization;Hamming distance","","","","","64","IEEE","","","","IEEE","IEEE Journals"
"Efficient and High-Quality Monocular Depth Estimation via Gated Multi-Scale Network","L. Lin; G. Huang; Y. Chen; L. Zhang; B. He","School of Mechanical Engineering and Automation, Fuzhou University, Fujian 350108, China.; School of Mechanical Engineering and Automation, Fuzhou University, Fujian 350108, China.; School of Mechanical Engineering and Automation, Fuzhou University, Fujian 350108, China.; School of Mechanical Engineering and Automation, Fuzhou University, Fujian 350108, China.; School of Mechanical Engineering and Automation, Fuzhou University, Fujian 350108, China.","IEEE Access","","2020","PP","99","1","1","The key issue in monocular depth estimation is how to construct the depth image better and improve the quality of the depth map. At present, most of the monocular depth estimation methods based on deep learning manipulate images at low resolution that leads to loss of detail and blurring of boundaries. Nevertheless, deep learning with a large number of parameters needs highly computational complexity, which makes it difficult to apply high-resolution (HR) images to the depth estimate. In this work, model accuracy and runtime are two important factors to be considered. To improve the depth map quality and reduce the running time of the network, we introduce super-resolution techniques as methods of up-sampling to generate high-quality depth images at a faster rate for the depth estimation network. A novel approach is proposed for collecting high-level features that are captured under different receptive fields. The gated multi-scale decoder allows us to effectively filter information by the gated module. By combining the gated module to aid the super resolution of depth images, our method reduces memory consumption while improves reconstruction quality. Experiment results on the challenging NYU Depth v2 dataset demonstrate that both contributions provide significant performance gains over the state-of-the-art in self-supervised depth estimation.","","","10.1109/ACCESS.2020.2964733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951177","Depth estimation;monocular vision;gated multi-scale network;super resolution","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Data-Driven Fault Diagnosis Method Based on Compressed Sensing and Improved Multiscale Network","Z. Hu; Y. Wang; M. Ge; J. Liu","School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Mechanical Engineering and Electronic Information, China University of Geosciences, Wuhan, China; School of Hydropower and Information Engineering, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Industrial Electronics","","2020","67","4","3216","3225","The diagnosis of the key components of rotating machinery systems is essential for the production efficiency and quality of manufacturing processes. The performance of the traditional diagnosis method depends heavily on feature extraction, which relies on the degree of individual's expertise or prior knowledge. Recently, a deep learning (DL) method is applied to automate feature extraction. However, training in the DL method requires a massive amount of sensor data, which is time consuming and poses a challenge for its applications in engineering. In this paper, a new data-driven fault diagnosis method based on compressed sensing (CS) and improved multiscale network (IMSN) is proposed to recognize and classify the faults in rotating machinery. CS is used to reduce the amount of raw data, from which the fault information is discovered. At the same time, it can be used to generate sufficient training samples for the subsequent learning. The one-dimensional compressed signal is converted to two-dimensional image for further learning. An IMSN is established for learning and obtaining deep features. It improves the diagnosis performance of the DL process. The faults of the key components are identified from a softmax model. Experimental analysis is performed to verify effectiveness of the proposed data-driven fault diagnosis method.","","","10.1109/TIE.2019.2912763","China Scholarship Council; National Natural Science Foundation of China; George W. Woodruff Faculty Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704327","Compressed sensing (CS);condition monitoring;fault diagnosis;improved multiscale network (IMSN);rotating machinery","","","","","1","33","IEEE","","","","IEEE","IEEE Journals"
"Dual-Domain based Adversarial Defense with Conditional VAE and Bayesian Network","J. Zhu; G. Peng; D. Wang","Hangzhou China 310027 (e-mail: wx_zjl@126.com); Singapore Singapore 639798 (e-mail: peng0086@e.ntu.edu.sg); School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore Singapore 639798 (e-mail: edwwang@ntu.edu.sg)","IEEE Transactions on Industrial Informatics","","2020","PP","99","1","1","Adversarial examples can be imperceptible to human eyes but can easily fool deep models. Such intrigue property has raised security issues for real-world industrial deep learning systems. To combat those malicious attacks, a novel defense strategy has been proposed based on conditional variational autoencoder and Bayesian network. This strategy incorporates the latent features and reconstruction residuals into a dual-domain defense framework which covers three modules named detection, diagnosis and recovery. First, a composite and hierarchical Bayesian network detector has been proposed to conduct the dual-domain detection through feature validation and output justification. Afterwards, a diagnosis strategy has been constructed for residual domain and different attacks can be evaluated in the unified framework. Finally, a two-step recovery mechanism has been established which can effectively restore the feature representations and the network predictions from various adversaries. The feasibility of the entire defense diagram has been extensively demonstrated on three real-world recognition problems.","","","10.1109/TII.2020.2964154","NTU start-up grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950195","Adversarial Examples;Security;Variational Autoencoder;Bayesian Network;Defense","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Real to H-Space Autoencoders for Theme Identification in Telephone Conversations","T. Parcollet; M. Morchid; X. Bost; G. Linarès; R. De Mori","ORKIS, Aix-en-Provence, France; Avignon Université, Laboratoire Informatique d’Avignon, Avignon, France; ORKIS, Aix-en-Provence, France; Avignon Université, Laboratoire Informatique d’Avignon, Avignon, France; McGill University, Montrèal, QC, Canada","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2020","28","","198","210","Machine learning (ML) and deep learning with deep neural networks (DNN), have drastically improved the performances of modern systems on numerous spoken language understanding (SLU) related tasks. Since most of current researches focus on new neural architectures to enhance the performances in realistic conditions, few recent works investigated the use of different algebras with neural networks (NN), to better represent the nature of the data being processed. To this extent, quaternion-valued neural networks (QNN) have shown better performances, and an important reduction of the number of neural parameters compared to traditional real-valued neural networks, when dealing with multidimensional signal. Nonetheless, the use of QNNs is strictly limited to quaternion input or output features. This article introduces a new unsupervised method based on a hybrid autoencoder (AE) called real-to-quaternion autoencoder (R2H), to extract a quaternion-valued input signal from any real-valued data, to be processed by QNNs. The experiments performed to identify the most related theme of a given telephone conversation from a customer care service (CCS), demonstrate that the R2H approach outperforms all the previously established models, either real- or quaternion-valued ones, in term of accuracy and with up to four times fewer neural parameters.","","","10.1109/TASLP.2019.2950596","AISSPER; Agence Nationale de la Recherche; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8888214","Features extraction;quaternion autoencoder;quaternion neural networks;spoken language understanding","Quaternions;Neural networks;Task analysis;Feature extraction;Telephone sets;Semantics;Speech recognition","","","","","61","IEEE","","","","IEEE","IEEE Journals"
"Instance-Level Microtubule Tracking","S. Masoudi; A. Razi; C. H. G. Wright; J. C. Gatlin; U. Bagci","University of Central Florida, Orlando, 32816 FL.; University of Central Florida, Orlando, 32816 FL.; University of Wyoming, Laramie, 82071 WY.; University of Wyoming, Laramie, 82071 WY.; University of Central Florida, Orlando, 32816 FL. (e-mail: ulasbagci@gmail.com)","IEEE Transactions on Medical Imaging","","2020","PP","99","1","1","We propose a new method of instance-level microtubule (MT) tracking in time-lapse image series using recurrent attention. Our novel deep learning algorithm segments individual MTs at each frame. Segmentation results from successive frames are used to assign correspondences among MTs. This ultimately generates a distinct path trajectory for each MT through the frames. Based on these trajectories, we estimate MT velocities. To validate our proposed technique, we conduct experiments using real and simulated data. We use statistics derived from real time-lapse series of MT gliding assays to simulate realistic MT time-lapse image series in our simulated data. This data set is employed as pre-training and hyperparameter optimization for our network before training on the real data. Our experimental results show that the proposed supervised learning algorithm improves the precision for MT instance velocity estimation drastically to 71.3% from the baseline result (29.3%). We also demonstrate how the inclusion of temporal information into our deep network can reduce the false negative rates from 67.8% (baseline) down to 28.7% (proposed). Our findings in this work are expected to help biologists characterize the spatial arrangement of MTs, specifically the effects of MT-MT interactions.","","","10.1109/TMI.2019.2963865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949549","Microtubules;TIRF microscopy;instance-level segmentation;instance-level sub-cellular tracking;microtubule-microtubule interaction","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"KISS+ for Rapid and Accurate Pedestrian Re-Identification","H. Han; M. Zhou; X. Shang; W. Cao; A. Abusorrah","School of Electronic and Electrical Engineering, Shanghai University of Engineering Science, Shanghai 201620, China.; ECE Department, New Jersey Institute of Technology, Newark, NJ 07102 USA (e-mail: zhou@njit.edu).; School of Electronic and Electrical Engineering, Shanghai University of Engineering Science, Shanghai 201620, China.; School of Electronic and Electrical Engineering, Shanghai University of Engineering Science, Shanghai 201620, China.; Center of Research Excellence in Renewable Energy and Power Systems, King Abdulaziz University, Jeddah 21589, Saudi Arabia.","IEEE Transactions on Intelligent Transportation Systems","","2020","PP","99","1","10","Pedestrian re-identification (Re-ID) is a very challenging and unavoidable problem in the field of multi-camera surveillance in smart transportation. Among many ways to solve this problem, keep it simple and straightforward (KISS) metric learning (KISSME) stands out since it has unbeatable advantages in running time while maintaining highly acceptable matching rate. It can be used to realize effective pedestrian Re-ID in an open world. Although it has achieved highly acceptable performance in some applications, it encounters a small sample size (S)³ problem that causes too small eigenvalues of its covariance matrix, thus resulting in an instability issue. Its large eigenvalues are overestimated; while its small ones are underestimated. In order to solve this problem, we use an orthogonal basis vector to generate virtual samples to overcome the S³ problem. The resulting algorithm named KISS+ is experimentally shown to have the eigenvalues of its covariance matrix significantly larger than those of the original KISSME. In order to show its advantage in pedestrian Re-ID, this work uses multi-feature fusion to extract more discriminant features, and obtain a low-dimensional expression of features through dimension reduction. Experiments based on several well-known databases show that our method can improve the matching rate, while maintaining the advantage of fast computation. Compared with deep learning algorithms, our algorithm does not achieve their matching rate, but it is highly suitable for real-time pedestrian Re-ID of an open world due to its simplicity, easy operation and fast execution.","","","10.1109/TITS.2019.2958741","National Nature Science Foundation of China; China Scholarship Council; Shanghai Municipal Education Commission and Shanghai Education Development Foundation through Chen Guang Project; Deanship of Scientific Research DSR through the King Abdulaziz University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951126","Pedestrian re-identification;virtual sample;KISSME;orthogonal basis vector;smart transportation.","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect","A. Li; S. L. Song; J. Chen; J. Li; X. Liu; N. R. Tallent; K. J. Barker","High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, WA, USA; High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, WA, USA; Computer Science and Mathematics Department, Oak Ridge National Laboratory, Oak Ridge, TN, USA; High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, WA, USA; Computer Science Department, College of William and Mary, Williamsburg, VA, USA; High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, WA, USA; High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, WA, USA","IEEE Transactions on Parallel and Distributed Systems","","2020","31","1","94","110","High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern GPUs can be connected and the real impact of state-of-the-art interconnect technology on multi-GPU application performance become a hurdle. In this paper, we fill the gap by conducting a thorough evaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1, NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080 GPUs. Based on the empirical evaluation, we have observed four new types of GPU communication network NUMA effects: three are triggered by NVLink's topology, connectivity and routing, while one is caused by PCIe chipset design issue. These observations indicate that, for an application running in a multi-GPU node, choosing the right GPU combination can impose considerable impact on GPU communication efficiency, as well as the application's overall performance. Our evaluation can be leveraged in building practical multi-GPU performance models, which are vital for GPU task allocation, scheduling and migration in a shared environment (e.g., AI cloud and HPC centers), as well as communication-oriented performance tuning.","","","10.1109/TPDS.2019.2928289","Exascale Computing Project; U.S. Department of Energy; National Nuclear Security Administration; U.S. Department of Energy; Pacific Northwest National Laboratory; U.S. Department of Energy; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8763922","Performance evaluation;GPU;interconnect;NUMA;PCIe;NVLink;NVSwitch;SLI;GPUDirect;RDMA;NCCL","Graphics processing units;Bandwidth;Topology;Peer-to-peer computing;Network topology;Switches;Routing","","","","1","60","IEEE","","","","IEEE","IEEE Journals"
"3D-CNN-SPP: A Patient Risk Prediction System From Electronic Health Records via 3D CNN and Spatial Pyramid Pooling","R. Ju; P. Zhou; S. Wen; W. Wei; Y. Xue; X. Huang; X. Yang","School of Electronic Information and Communications, Huazhong University of Science and Technology 12443, Wuhan 430074, China (e-mail: juronghui@hust.edu.cn).; School of Electronic Information and Communications, Huazhong University of Science and Technology 12443, Wuhan 430074, China (e-mail: panzhou@hust.edu.cn).; Centre for Artificial Intelligence, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo NSW 2007, Australia (e-mail: shiping.wen@uts.edu.au).; School of Computer of Science and Technology, Huazhong University of Science and Technology, Wuhan 430074 China (e-mail: weiw@hust.edu.cn).; College of Information Sciences and Technology, Penn State University, University Park, PA 16802 USA (e-mail: yzx139@psu.edu).; College of Information Sciences and Technology, Penn State University, University Park, PA 16802 USA (e-mail: suh972@psu.edu).; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan 430074, China (e-mail: xinyang2014@hust.edu.cn).","IEEE Transactions on Emerging Topics in Computational Intelligence","","2020","PP","99","1","15","The problem of extracting useful clinical representations from longitudinal electronic health record (EHR) data, also known as the computational phenotyping problem, is an important yet challenging task in the health-care academia and industry. Recent progress in the design and applications of deep learning methods has shown promising results towards solving this problem. In this paper, we propose 3D-CNN-SPP (3D Convolutional Neural Networks and Spatial Pyramid Pooling), a novel patient risk prediction system, to investigate the application of deep neural networks in modeling longitudinal EHR data. Particularly, we propose a 3D CNN structure, which is featured by SPP. Compared with 2D CNN methods, our proposed method can capture the complex relationships in EHRs more effectively and efficiently. Furthermore, previous works handle the issue of variable length in patient records by padding zeros to all vectors so that they have a fixed length. In our work, the proposed spatial pyramid pooling divides the records into several length sections for respective pooling processing, hence handling the variable length problem easily and naturally. We take heart failure and diabetes as examples to test the performance of the system, and the experiment results demonstrate great effectiveness in patient risk prediction, compared with several strong baselines.","","","10.1109/TETCI.2019.2960474","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949441","Risk prediction;electronic health records (EHR);convolutional neural network","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Improved Kiwifruit Detection Using Pre-Trained VGG16 With RGB and NIR Information Fusion","Z. Liu; J. Wu; L. Fu; Y. Majeed; Y. Feng; R. Li; Y. Cui","College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, China; Beijing Key Laboratory of Big Data Technology for Food Safety, Beijing Technology and Business University, Beijing, China; College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, China; Center for Precision and Automated Agricultural Systems, Washington State University, Prosser, WA, USA; College of Engineering, Shanxi Agricultural University, Jinzhong, China; College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, China; College of Mechanical and Electronic Engineering, Northwest A&F University, Yangling, China","IEEE Access","","2020","8","","2327","2336","This study presents a novel method to apply the RGB-D (Red Green Blue–Depth) sensors and fuse aligned RGB and NIR images with deep convolutional neural networks (CNN) for fruit detection. It aims to build a more accurate, faster, and more reliable fruit detection system, which is a vital element for fruit yield estimation and automated harvesting. Recent work in deep neural networks has led to the development of a state-of-the-art object detector termed Faster Region-based CNN (Faster R-CNN). A common Faster R-CNN network VGG16 was adopted through transfer learning, for the task of kiwifruit detection using imagery obtained from two modalities: RGB (red, green, blue) and Near-Infrared (NIR) images. Kinect v2 was used to take a bottom view of the kiwifruit canopy’s NIR and RGB images. The NIR (1 channel) and RGB images (3 channels) were aligned and arranged side by side into a 6-channel image. The input layer of the VGG16 was modified to receive the 6-channel image. Two different fusion methods were used to extract features: Image-Fusion (fusion of the RGB and NIR images on input layer) and Feature-Fusion (fusion of feature maps of two VGG16 networks where the RGB and NIR images were input respectively). The improved networks were trained end-to-end using back-propagation and stochastic gradient descent techniques and compared to original VGG16 networks with RGB and NIR image input only. Results showed that the average precision (APs) of the original VGG16 with RGB and NIR image input only were 88.4% and 89.2% respectively, the 6-channel VGG16 using the Feature-Fusion method reached 90.5%, while that using the Image-Fusion method reached the highest AP of 90.7% and the fastest detection speed of 0.134 s/image. The results indicated that the proposed kiwifruit detection approach shows a potential for better fruit detection.","","","10.1109/ACCESS.2019.2962513","Key Research and Development Program in Shaanxi Province of China; China Postdoctoral Science Foundation; National Natural Science Foundation of China; Beijing Technology and Business University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943411","Fruit detection;image alignment;information fusion;multi-modality faster R-CNN;RGB-D sensor","","","","","","40","CCBY","","","","IEEE","IEEE Journals"
"A CNN-Based Post-Processing Algorithm for Video Coding Efficiency Improvement","H. Zhao; M. He; G. Teng; X. Shang; G. Wang; Y. Feng","School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Electronic and Electrical Engineering, Shanghai University of Engineering Science, Shanghai, China; School of Electronic and Electrical Engineering, Shanghai University of Engineering Science, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China","IEEE Access","","2020","8","","920","929","Lossy compression algorithms are widely used in video coding. However, lossy compressed videos exist some annoying distortion and artifacts, such as blocking, blurring, and ringing. Thus, coding efficiency improvement is a steady-state topic in the domain of video coding. High Efficiency Video Coding (HEVC), a recent video standard, adopts two in-loop filters for the improvement of the coding efficiency, including deblocking (DB) and sample adaptive offset (SAO). In a certain extent, traditional in-loop filters reduce the distortion and improve the video quality. But the reduction of the distortion is a nonlinear problem that is difficult to be solved by traditional linear filters. Recently, the progress of deep learning shows the possibility to settle the complex problems in the computer vision field. Meanwhile, according to the compressive sensing theory, the post-processing method at the decoder end can further enhance the coding efficiency. In this paper, we propose a variable-filter-size Residue-learning convolutional neural network with batch normalization layer (VRCNN-BN). Our model is an end-to-end model. We feed the decoded pictures to the model at the decoder end. Different from previous methods, we apply the model to luma pictures and chroma pictures, respectively. In order to comprehensively evaluate the coding performance of both luma and chroma components, the color-sensitivity-based combined PSNR (CS-PSNR) is exploited to measure the effectiveness of the proposed method. Compared to HEVC baseline, our approach achieves an average BD-rate reduction of 10.3%, 8.9%, 13.1% and 11.8% in terms of CS-PSNR for random access, all intra, low delay P and low delay B configurations, respectively. Abundant experimental results indicate that our method is better than existing similar methods.","","","10.1109/ACCESS.2019.2961760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8939464","Convolutional neural network;end-to-end;post-processing;high efficiency video coding","","","","","","35","CCBY","","","","IEEE","IEEE Journals"
"Motion Guided Siamese Trackers for Visual Tracking","C. Wu; Y. Zhang; Y. Zhang; W. Zhang; H. Wang; Y. Zhang; X. Sun","Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China and School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing 100190, China and Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China.; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China and Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China.; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China and Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China.; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China and Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China.; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China and Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China.; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China and Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China.; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China and Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China.","IEEE Access","","2020","PP","99","1","1","Siamese trackers learn the appearance model of the target in the first frame and then exploit the model to locate the target in the subsequent frames. Meanwhile, the appearance model remains unchanged in the subsequent frames. Due to the powerful feature extraction capability of the deep convolutional neural networks, Siamese trackers achieve advanced performance. However, due to the non-update of the appearance model and the changing appearance of the target, the problem of tracking drift occurs frequently, especially in the background clutters scenarios. In order to tackle this issue, we propose a motion model and a discriminative model. Firstly, the motion model of the target is constructed to determine whether the tracking drift occurs or not since the position of the target predicted by the motion model is smooth in timing but the position of the target predicted by the Siamese tracker may be not smooth. In this case, the temporal information is utilized to supplement the Siamese tracker which only employs the spatial information. Secondly, the discriminative model is learned to determine the final position of the target when the tracking drift happens. Finally, a flexible model update strategy of the discriminative model is presented. In order to demonstrate the generality of the proposed method, we apply it for two famous Siamese trackers, SiamFC and SiamRPN_DW. Extensive experiments on OTB2013, OTB2015, VOT2016, VOT2019 and GOT-10k benchmarks demonstrate that the proposed trackers outperform the baseline trackers and achieve the state-of-the-art performance, especially in the background clutters scenarios. To the best of our knowledge, we are the first time to propose motion guided Siamese trackers. Moreover, We can release our code to encourage more researches in this direction.","","","10.1109/ACCESS.2020.2964269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950457","Siamese trackers;convolutional neural networks;motion model;discriminative model;tracking drift;background clutters","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"A Vector Quantized Variational Autoencoder (VQ-VAE) Autoregressive Neural $F_0$ Model for Statistical Parametric Speech Synthesis","X. Wang; S. Takaki; J. Yamagishi; S. King; K. Tokuda","National Institute of Informatics, Tokyo, Japan; Nagoya Institute of Technology, Nagoya, Japan; National Institute of Informatics, Tokyo, Japan; Centre for Speech Technology Research, The University of Edinburgh, Edinburgh, U.K.; Nagoya Institute of Technology, Nagoya, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2020","28","","157","170","Recurrent neural networks (RNNs) can predict fundamental frequency (F0) for statistical parametric speech synthesis systems, given linguistic features as input. However, these models assume conditional independence between consecutive $F_0$ values, given the RNN state. In a previous study, we proposed autoregressive (AR) neural $F_0$ models to capture the causal dependency of successive $F_0$ values. In subjective evaluations, a deep AR model (DAR) outperformed an RNN. Here, we propose a Vector Quantized Variational Autoencoder (VQ-VAE) neural $F_0$ model that is both more efficient and more interpretable than the DAR. This model has two stages: one uses the VQ-VAE framework to learn a latent code for the $F_0$ contour of each linguistic unit, and other learns to map from linguistic features to latent codes. In contrast to the DAR and RNN, which process the input linguistic features frame-by-frame, the new model converts one linguistic feature vector into one latent code for each linguistic unit. The new model achieves better objective scores than the DAR, has a smaller memory footprint and is computationally faster. Visualization of the latent codes for phones and moras reveals that each latent code represents an $F_0$ shape for a linguistic unit.","","","10.1109/TASLP.2019.2950099","JST CREST, Japan,; MEXT KAKENHI, Japan,; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8884734","Fundamental frequency;speech synthesis;neural network;variational auto-encoder","Hidden Markov models;Linguistics;Artificial neural networks;Computational modeling;Frequency synthesizers;Feature extraction;Speech processing","","","","","62","IEEE","","","","IEEE","IEEE Journals"
"On Mathematical Models of Optimal Video Memory Design","Y. Xu; H. Das; Y. Gong; N. Gong","Department of Industrial and Manufacturing Engineering, North Dakota State University, Fargo, ND, USA; Department of Electrical and Computer Engineering, North Dakota State University, Fargo, ND, USA; Department of Electrical and Computer Engineering, North Dakota State University, Fargo, ND, USA; Department of Electrical and Computer Engineering, University of South Alabama, Mobile, AL, USA","IEEE Transactions on Circuits and Systems for Video Technology","","2020","30","1","256","266","The big video data size today imposes huge pressure on storage. The variation and aging induced memory failures significantly influence the video output quality. Recently, researchers have developed different memory designs for videos, deep learning, and other data-intensive applications, which enables better energy-quality tradeoffs with design constraints. Unfortunately, designing memory has been proven to be a very challenging problem due to: 1) various design constraints; 2) multiple memory bitcell design options; and 3) challenging layout integration and cost analysis using different memory technologies. In this paper, we develop novel mathematical models for optimizing embedded video memory design without applying a time-consuming and laborious ASIC design process. The problems are formulated as nonlinear programs and integer linear programs. Different SRAM designs and hybrid SRAM and DRAM designs are considered in our models. The results of the numerical studies show that by applying our proposed method the average mean square error of the video storage can be greatly reduced, even by more than 90% in many cases.","","","10.1109/TCSVT.2018.2890383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598810","Approximate computing;memory;videos;optimization models;power efficiency","Random access memory;Mathematical model;Silicon;Memory management;Mean square error methods;Numerical models;Optimization","","","","1","24","IEEE","","","","IEEE","IEEE Journals"
"Un-Rectifying Non-Linear Networks for Signal Representation","W. Hwang; A. Heinecke","Institute of Information Science, Academia Sinica, Taipei, Taiwan; Yale-NUS College, Singapore","IEEE Transactions on Signal Processing","","2020","68","","196","210","We consider deep neural networks with rectifier activations and max-pooling from a signal representation perspective. In this view, such representations mark the transition from using a single linear representation for all signals to utilizing a large collection of affine linear representations that are tailored to particular regions of the signal space. We propose a novel technique to “un-rectify” the nonlinear activations into data-dependent linear equations and constraints, from which we derive explicit expressions for the affine linear operators, their domains and ranges in terms of the network parameters. We show how increasing the depth of the network refines the domain partitioning and derive atomic decompositions for the corresponding affine mappings that process data belonging to the same partitioning region. In each atomic decomposition the connections over all hidden network layers are summarized and interpreted in a single matrix. We apply the decompositions to study the Lipschitz regularity of the networks and give sufficient conditions for network-depth-independent stability of the representation, drawing a connection to compressible weight distributions. Such analyses may facilitate and promote further theoretical insight and exchange from both the signal processing and machine learning communities.","","","10.1109/TSP.2019.2957607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8922773","Neural networks;feedforward neural networks;multi-layer neural network;signal representation","Signal representation;Neural networks;Matrix decomposition;Asymptotic stability;Atomic layer deposition;Signal processing;Transforms","","","","","47","IEEE","","","","IEEE","IEEE Journals"
"Granule Vectors and Granular Convolutional Classifiers","Y. Chen; X. Zhang; W. Li; S. Zhu","College of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; College of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; College of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; College of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China","IEEE Access","","2020","8","","2042","2051","Convolutional operations can extract effective features and have been widely used in the field of deep learning. For the deficiency of convolution mainly dealing with numerical data, we propose a novel convolutional operator on granules with a set form, further we build a classifier on it. Firstly, feature granules are constructed on each single feature of a classification system by introducing neighborhood rough sets. Synchronously, decision granules are generated on the labels of samples. Secondly, feature granule vectors and weighted granule vectors are constructed from these granules, and a convolutional operation is proposed on feature granule vectors and weighted granule vectors, then a predicted granule is produced as a result of the convolutional operation. The predicted granule is compared with the decision granule, and their residual error is back propagated to the weighted granule vector for tuning its value. After multiple iterations of the granular convolutional operations and back propagation corrections, the weight of the granular vector is convergent and optimized. Furthermore, a granular classifier is designed based on the convolutional operation. The constringency of the granular convolution and the classification performance of the granular classifier are tested on some UCI datasets. Theoretical analysis and experimental results show that the granular convolution has a characteristic of fast convergence, and the granular convolutional classifier has a better classification performance.","","","10.1109/ACCESS.2019.2959126","National Natural Science Foundation of China; Natural Science Foundation of Fujian Province; Social Science Planning Project of Fujian Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931552","Granular computing;neighborhood rough sets;convolutional network;granular classifier;rough sets","","","","","","48","CCBY","","","","IEEE","IEEE Journals"
"A Simplified Speaker Recognition System Based on FPGA Platform","J. Xu; S. Li; J. Jiang; Y. Dou","College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China","IEEE Access","","2020","8","","1507","1516","Speaker recognition is a crucial bio-identification technology, which is extensively used in our daily life. With the development of deep learning, convolutional neural networks (CNNs) are applied to speaker recognition tasks given their excellent performance. However, in real life, speaker recognition systems are frequently deployed on end-devices. Therefore, while obtaining recognition accuracy, the model of speaker recognition is expected to be as simple as possible. Inspired by 1-max pooling CNN and Gaussian mixture model-universal background model (GMM-UBM), this study proposes a one dimension convolutional neural networks (1D CNN) on the basis of original 2D CNN. The proposed model reduces the computational complexity of ResNet20 by 64% and the amount of parameters by 53%. In comparison with the original ResNet20 models, the recognition accuracy will be reduced by about one percent on the 15s data set. Then, on the basis of the 1D CNN, we propose a pyramid layer-folding pipeline structure and implement it on the Xilinx VC709 platform. According to the time-dimension partition, the proposed pyramid pipeline structure can process speech data of various lengths. Moreover, our accelerator is  $5.1\times $  faster on 3s dataset and  $6.8\times $  quicker on 15s dataset than those of the CPU platform.","","","10.1109/ACCESS.2019.2944644","National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897096","Speaker recognition;1D convolution neural networks;pyramid pipeline;folding pipeline;FPGA","","","","","","25","CCBY","","","","IEEE","IEEE Journals"
"Multi-Scale Dilated Convolution Neural Network for Image Artifact Correction of Limited-Angle Tomography","H. Zhou; Y. Zhu; Q. Wang; J. Xu; G. Li; D. Chen; Y. Dong; H. Zhang","School of Mathematical Sciences, Capital Normal University, Beijing, China; School of Mathematical Sciences, Capital Normal University, Beijing, China; Department of Electrical and Computer Engineering, University of Massachusetts–Lowell, Lowell, MA, USA; Paul Scherrer Institut, Villigen, Switzerland; Ping An Technology, Beijing, China; School of Mathematical Sciences, Capital Normal University, Beijing, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; School of Mathematical Sciences, Capital Normal University, Beijing, China","IEEE Access","","2020","8","","1567","1576","Limited-angle computed tomography (CT) has arisen in some medical and industrial applications. It is also a challenging problem since some scan views are missing and the directly reconstructed images often suffer from severe distortions. For such kind of problems, we analyze the features of limited-angle CT images and propose a multi-scale dilated convolution neural network (MSD-CNN) to correct the artifacts and to restore the image. In this network, the dilated convolution layer and multi-scale pooling layer are combined to form a group and exited in the whole encoder-decoder process. Since the dilated convolutions support an exponential expansion of the receptive field without losing resolution and coverage, the obtained artifact features possess the multi-scale characteristic. Furthermore, to improve the effectiveness and accuracy of the training step, we employ a preprocessing method, which extracts image patches. Numerical experiments verify the out-performance of the proposed method compared with some conventional methods, such as Unet based deep learning,TV- and  $L_{0}$ -based optimization methods.","","","10.1109/ACCESS.2019.2962071","National Natural Science Foundation of China; Beijing Municipal Commission of Education; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941073","Limited-angle tomography;artifact correction;multi-scale;dilated convolution","","","","","","35","CCBY","","","","IEEE","IEEE Journals"
"A Two-Branch Convolution Residual Network for Image Compressive Sensing","C. Gan; X. Yan; Y. Wu; Z. Zhang","School of Communication and Information Engineering, Chongqing University of Post and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Post and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Post and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Post and Telecommunications, Chongqing, China","IEEE Access","","2020","8","","1705","1714","Deep learning has made great progress in image compressive sensing (CS) tasks recently, and several CS models based on it have achieved superior performance. In practice, sensing the entire image requires huge memory and computational effort. Although the block-based CS method can effectively realize image sensing, it will cause block effects that severely decrease the reconstruction performance. To this end, this paper proposes a two-branch convolution residual network for image compressive sensing (denoted as TCR-CS), which mainly consists of a two-branch convolution autoencoder network and a residual network. Specifically, the two-branch convolution autoencoder network senses the entire image through multiple scale convolutional filters to obtain measurements. For better CS reconstruction, the image is preliminarily reconstructed by the deconvolution decoder network, and then the residual network is used to optimize the pre-reconstructed image. Through the end-to-end training, all networks can be jointly optimized. Finally, experimental results demonstrate that the proposed TCR-CS method is superior to existing state-of-the-art CS methods in terms of structural similarity, reconstruction performance and visual quality at different measurement rates.","","","10.1109/ACCESS.2019.2961369","National Natural Science Foundation of China; Major Project of Science and Technology Research Program of Chongqing Education Commission of China; Chongqing Research Program of Basic Research and Frontier Technology; Chongqing Municipal Key Laboratory of Institutions of Higher Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938811","Image compressive sensing;two-branch convolution;residual network;structural similarity;reconstruction performance;visual quality","","","","","","33","CCBY","","","","IEEE","IEEE Journals"
"A Natural Language Process-Based Framework for Automatic Association Word Extraction","Z. Hu; J. Luo; C. Zhang; W. Li","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Laboratory for Intelligent Networks and Systems, Northern Illinois University, DeKalb, IL, USA","IEEE Access","","2020","8","","1986","1997","Word association, revealing mental representations and connections of human, has been widely studied in psychology. However, the scale of available associative cue-response words is severely restricted due to the traditional manually collecting methodology. Meanwhile, with the tremendous success in Natural Language Process (NLP) tasks, an extremely large amount of plain texts can be easily acquired. This suggests an insight about the potential to find association words automatically from the text corpus instead of manually collection. As an original attempt, this paper takes a small step toward proposing a deep learning based framework for automatic association word extraction. The framework mainly consists of two stages of association word detection and machine association network construction. In particular, attention mechanism based Reading Comprehension (RC) algorithm is explored to find valuable association words automatically. To validate the value of the extracted association words, the correlation coefficient between semantic similarities of machine and human association words is introduced as an effective measurement for evaluating association consistence. The experiments are conducted on two text datasets from which together about  $20k$  association words, more than the existing largest human association word dataset, are finally derived. The experiment further verifies that the machine association words are generally consistent with human association words with respect to semantic similarity, which highlights the promising utilization of the machine association words in the future researches of both psychology and NLP.","","","10.1109/ACCESS.2019.2962154","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945152","Word association;natural language process;semantic similarity;attention mechanism","","","","","","40","CCBY","","","","IEEE","IEEE Journals"
"A New Vehicular Fog Computing Architecture for Cooperative Sensing of Autonomous Driving","H. Du; S. Leng; F. Wu; X. Chen; S. Mao","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China.; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China.; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China.; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China.; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China.","IEEE Access","","2020","PP","99","1","1","The sensing coverage and accuracy of vehicles are vital for autonomous driving. However, the current sensing capability of a single autonomous vehicle is quite limited in the complicated road traffic environment, which leads to many sensing dead zones or frequent misdetection. In this paper, we propose to develop a Vehicular Fog Computing (VFC) architecture to implement cooperative sensing among multiple adjacent vehicles driving in the form of a platoon. Based on our VFC architecture greedy and Support Vector Machine (SVM) algorithms are adopted respectively to enhance the sensing coverage and accuracy in the platoon. Furthermore, the distributed deep learning is processed for trajectory prediction by applying the Light Gated Recurrent Unit (Li-GRU) neural network algorithm. Simulation results based on real-world traffic datasets indicate the sensing coverage and accuracy by the proposed algorithms can be significantly improved with low computational complexity.","","","10.1109/ACCESS.2020.2964029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950168","Intelligent vehicles;Vehicular fog computing;Cooperative sensing;Autonomous driving","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Scene Classification of Remote Sensing Images Based on Saliency Dual Attention Residual Network","D. Guo; Y. Xia; X. Luo","Chongqing Engineering Research Center for Spatial Big Data Intelligent Technology, Chongqing University of Posts and Telecommunications, Chongqing 400065, China and School of Software, Nanyang Institute of Technology, Nanyang 473000, China. (e-mail: gden_2008@126.com); Chongqing Engineering Research Center for Spatial Big Data Intelligent Technology, Chongqing University of Posts and Telecommunications, Chongqing 400065, China.; Chongqing Engineering Research Center for Spatial Big Data Intelligent Technology, Chongqing University of Posts and Telecommunications, Chongqing 400065, China.","IEEE Access","","2020","PP","99","1","1","Scene classification of high-resolution Remote Sensing Images (RSI) is one of basic challenges in RSI interpretation. Existing scene classification methods based on deep learning have achieved impressive performances. However, since RSI commonly contain various types of ground objects and complex backgrounds, most methods cannot focus on saliency features of scene, which limits the classification performances. To address this issue, we propose a novel Saliency Dual Attention Residual Network (SDAResNet) to extract both cross-channel and spatial saliency information for scene classification of RSI. More specifically, the proposed SDAResNet consists of spatial attention and channel attention, in which spatial attention is embedded into low-level feature to emphasize saliency location information and suppress background information, while channel attention is integrated into high-level features to extract saliency meaningful information. Additionally, several image classification tricks are used to further improve classification accuracy. Finally, extensive experiments on two challenging benchmark RSI datasets are presented to demonstrate that our methods outperform most state-of-the-art approaches significantly.","","","10.1109/ACCESS.2019.2963769","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949476","remote sensing images;scene classification;spatial attention;channel attention;residual attention network","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"Large-Scale Point Cloud Contour Extraction via 3-D-Guided Multiconditional Residual Generative Adversarial Network","Y. Zhang; Z. Liu; T. Liu; B. Peng; X. Li; Q. Zhang","School of Electronic Science, National University of Defense Technology, Changsha, China; School of Electronic Science, National University of Defense Technology, Changsha, China; School of Electronic Science, National University of Defense Technology, Changsha, China; School of Electronic Science, National University of Defense Technology, Changsha, China; School of Electronic Science, National University of Defense Technology, Changsha, China; School of Business, University of Leeds, Leeds, U.K.","IEEE Geoscience and Remote Sensing Letters","","2020","17","1","142","146","As one of the most important features for human perception, contours are widely applied in graphics and mapping applications. However, it is considerably challenging to extract contours from large-scale point clouds due to the irregular distribution of point clouds. In this letter, we propose a 3-D-guided multiconditional residual generative adversarial network (3-D-GMRGAN), the first deep-learning framework to generate contours for large-scale outdoor point clouds. To make the network handle huge amounts of points, we operate contours in the parametric space rather than raw point space, associated with a parametric chamfer distance. Then, to gather contour features from potential positions and avoid the huge solution space, we propose a guided residual generative adversarial framework, by utilizing a simple feature-based method to get the “over extraction” potential contour distribution. Experiments demonstrate that the proposed method is able to generate contours efficiently for large-scale point clouds, with fewer outliers and pseudo contours compared with state-of-the-art approaches.","","","10.1109/LGRS.2019.2917319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733065","Contour extraction;generative adversarial network;large-scale point clouds","Three-dimensional displays;Feature extraction;Generators;Surface reconstruction;Task analysis;Training;Generative adversarial networks","","","","","26","IEEE","","","","IEEE","IEEE Journals"
"Neural Source-Filter Waveform Models for Statistical Parametric Speech Synthesis","X. Wang; S. Takaki; J. Yamagishi","National Institute of Informatics, Tokyo, Japan; Nagoya Institute of Technology, Nagoya, Japan; National Institute of Informatics, Tokyo, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2020","28","","402","415","Neural waveform models have demonstrated better performance than conventional vocoders for statistical parametric speech synthesis. One of the best models, called WaveNet, uses an autoregressive (AR) approach to model the distribution of waveform sampling points, but it has to generate a waveform in a time-consuming sequential manner. Some new models that use inverse-autoregressive flow (IAF) can generate a whole waveform in a one-shot manner but require either a larger amount of training time or a complicated model architecture plus a blend of training criteria. As an alternative to AR and IAF-based frameworks, we propose a neural source-filter (NSF) waveform modeling framework that is straightforward to train and fast to generate waveforms. This framework requires three components to generate waveforms: a source module that generates a sine-based signal as excitation, a non-AR dilated-convolution-based filter module that transforms the excitation into a waveform, and a conditional module that pre-processes the input acoustic features for the source and filter modules. This framework minimizes spectral-amplitude distances for model training, which can be efficiently implemented using short-time Fourier transform routines. As an initial NSF study, we designed three NSF models under the proposed framework and compared them with WaveNet using our deep learning toolkit. It was demonstrated that the NSF models generated waveforms at least 100 times faster than our WaveNet-vocoder, and the quality of the synthetic speech from the best NSF model was comparable to that from WaveNet on a large single-speaker Japanese speech corpus.","","","10.1109/TASLP.2019.2956145","JST CREST; MEXT KAKENHI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8915761","Speech synthesis;neural network;waveform model;short-time Fourier transform","Training;Mathematical model;Acoustics;Computational modeling;Speech synthesis;Neural networks","","","","","60","IEEE","","","","IEEE","IEEE Journals"
"A Comprehensive Review on Malware Detection Approaches","Ö. Aslan; R. Samet","Ankara University, computer engineering department, Turkey and Siirt University, computer engineering department, Turkey. (e-mail: omer.aslan@siirt.edu.tr); Ankara University, computer engineering department, Turkey.","IEEE Access","","2020","PP","99","1","1","According to the recent studies, malicious software (malware) is increasing at an alarming rate, and some malware can hide in the system by using different obfuscation techniques. In order to protect computer systems and the Internet from the malware, the malware needs to be detected before it affects a large number of systems. Recently, there have been made several studies on malware detection approaches. However, the detection of malware still remains problematic. Signature-based and heuristic-based detection approaches are fast and efficient to detect known malware, but especially signature-based detection approach has failed to detect unknown malware. On the other hand, behavior-based, model checking-based, and cloud-based approaches perform well for unknown and complicated malware; and deep learning-based, mobile devices-based, and IoT-based approaches also emerge to detect some portion of known and unknown malware. However, no approach can detect all malware in the wild. This shows that to build an effective method to detect malware is a very challenging task, and there is a huge gap for new studies and methods. This paper presents a detailed review on malware detection approaches and recent detection methods which use these approaches. Paper goal is to help researchers to have a general idea of the malware detection approaches, pros and cons of each detection approach, and methods that are used in these approaches.","","","10.1109/ACCESS.2019.2963724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949524","Cyber security;malware classification;malware detection approaches;malware features","","","","","","","CCBY","","","","IEEE","IEEE Early Access Articles"
"An Efficient CNN Model Based on Object-level Attention Mechanism for Casting Defects Detection on Radiography Images","C. Hu; Y. Wang","University of Shanghai for Science and Technology, 47863 Shanghai China (e-mail: w64228013@126.com); University of Shanghai for Science and Technology, 47863 Shanghai, Shanghai China (e-mail: wyxiong@usst.edu.cn)","IEEE Transactions on Industrial Electronics","","2020","PP","99","1","1","Automatic detection of casting defects on radiography images is an important technology to automatize digital radiography (DR) defect inspection. Traditionally, in an industrial application, conventional methods are inefficient when the detection targets are small, local and subtle in the complex scenario. Meanwhile, the outperformance of deep learning models, such as the convolution neural network (CNN), is limited by a huge volume of data with precise annotations. To overcome these challenges, an efficient CNN model, only trained with image-level labels, is first proposed for detection of tiny casting defects in a complicated industrial scene. Then, we present a novel training strategy which can form a new object-level attention mechanism for the model during the training phase, and bilinear pooling is utilized to improve the model capability of detecting local contrast casting defects. Moreover, to enhance the interpretability, we extend class activation maps (CAM) to bilinear CAM (Bi-CAM) which is adapted to bilinear architectures as a visualization technique to describe the reason about the model output. Experimental results show that the proposed model achieves superior performance in terms of each quantitative metric and is suitable for most actual applications. The real-time defect detection of castings is efficiently implemented in the complex scenario.","","","10.1109/TIE.2019.2962437","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8948332","Bilinear CAM;bilinear pooling;convolutional neural network;digital radiography defect inspection;object-level attention mechanism","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Fault Diagnosis and Tolerance Control of Five-Level Nested NPP Converter Using Wavelet Packet and LSTM","S. Ye; J. Jiang; J. Li; Y. Liu; Z. Zhou; C. Liu","Key Laboratory of Control of Power Transmission and Conversion, Shanghai Jiao Tong University, Shanghai, China; Key Laboratory of Control of Power Transmission and Conversion, Shanghai Jiao Tong University, Shanghai, China; Key Laboratory of Control of Power Transmission and Conversion, Shanghai Jiao Tong University, Shanghai, China; School of Mechanical Electronic and Information Engineering, China University of Mining and Technology, Beijing, China; Key Laboratory of Control of Power Transmission and Conversion, Shanghai Jiao Tong University, Shanghai, China; Key Laboratory of Control of Power Transmission and Conversion, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Power Electronics","","2020","35","2","1907","1921","The five-level nested neutral-point-pilot (NPP) topology, as a new structure for converters, bears the advantages of a high power density, robustness, and flexibility and is therefore suitable for high-voltage and high-power applications. For a multilevel converter, as the number of power electronic switches increases, the risk of switch failure increases, together with the complexity of fault detection and tolerance control. The requirements for a higher operational stability and reliability continue to grow. However, studies on fault tolerance for multilevel converters are limited. In this paper, a fault diagnosis and tolerance solution for a five-level nested NPP converter is proposed. For the fault diagnosis, a deep learning method integrating the wavelet packet transform and long short-term memory is presented. Both open- and short-circuit switch failures can be precisely detected and located without the requirement of a large sample set. Two software-based control strategies for fault tolerance are adopted, and low-cost hardware reconfigurations are also implemented to prevent failure expansion and ensure continuous operation. Furthermore, the voltages of dc-link capacitors and flying capacitors are effectively balanced with the improved algorithm even when a failure occurs. Finally, the effectiveness of the fault-tolerant strategy are proven by simulations and experiments.","","","10.1109/TPEL.2019.2921677","National High-tech RandD Program of China 863 Program; National Program on Key Basic Research Project 973 Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8734774","Fault diagnosis;fault tolerance;five-level nested NPP converter;IGBT;long short-term memory (LSTM);wavelet transform and wavelet packet","Circuit faults;Fault tolerance;Fault tolerant systems;Insulated gate bipolar transistors;Capacitors;Fault diagnosis;Topology","","","","","49","IEEE","","","","IEEE","IEEE Journals"
"Bridging the Gap Between Monaural Speech Enhancement and Recognition With Distortion-Independent Acoustic Modeling","P. Wang; K. Tan; D. L. Wang","Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering and the Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2020","28","","39","48","Monaural speech enhancement has made dramatic advances since the introduction of deep learning a few years ago. Although enhanced speech has been demonstrated to have better intelligibility and quality for human listeners, feeding it directly to automatic speech recognition (ASR) systems trained with noisy speech has not produced expected improvements in ASR performance. The lack of an enhancement benefit on recognition, or the gap between monaural speech enhancement and recognition, is often attributed to speech distortions introduced in the enhancement process. In this article, we analyze the distortion problem, compare different acoustic models, and investigate a distortion-independent training scheme for monaural speech recognition. Experimental results suggest that distortion-independent acoustic modeling is able to overcome the distortion problem. Such an acoustic model can also work with speech enhancement models different from the one used during training. Moreover, the models investigated in this paper outperform the previous best system on the CHiME-2 corpus.","","","10.1109/TASLP.2019.2946789","National Science Foundation; Ohio Supercomputer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8873624","Speech enhancement;speech recognition;speech distortion;distortion-independent acoustic modeling","Speech enhancement;Acoustic distortion;Acoustics;Training;Speech recognition;Noise measurement","","","","","38","IEEE","","","","IEEE","IEEE Journals"
"Superpixel-Driven Optimized Wishart Network for Fast PolSAR Image Classification Using Global  ${k}$ -Means Algorithm","T. Gadhiya; A. K. Roy","Department of Information and Communication Technology, Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar, India; Department of Information and Communication Technology, Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar, India","IEEE Transactions on Geoscience and Remote Sensing","","2020","58","1","97","109","Limitation of optical remote sensing technology gave rise to synthetic aperture radar (SAR) imaging. SAR is a microwave imaging technique, which promises to have a long-range propagation characteristic allowing imaging under harsh weather conditions or in hostile lighting situation. This has opened up a domain of classification using polarimetric SAR (PolSAR) images. In this article, we propose a fast PolSAR image classification algorithm, which uses not only pixel-based feature but also spatial features around each pixel. This is achieved by introducing superpixel-driven optimized Wishart network. The first improvement suggested in this article is to take advantage of a fast global  $k$ -means algorithm for obtaining optimal cluster centers within each class. It uses real-valued vector representation of PolSAR coherency matrix along with fast matrix inverse and determinant algorithms to reduce computational overhead. Our method then exploits the information of neighboring pixels by forming a superpixel so that even a noisy pixel may not be assigned a wrong class label. The proposed network uses dual-branch architecture to efficiently combine pixel and superpixel features. We concluded that our proposed method has better efficiency in terms of classification accuracy and computational overhead compared with other deep learning-based methods available in the literature.","","","10.1109/TGRS.2019.2933483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8833504","Global k-means algorithm;neural network;optimized Wishart network (OWN);polarimetric synthetic aperture radar (PolSAR);revised Wishart distance (RWD)","Matrix decomposition;Scattering;Synthetic aperture radar;Microwave imaging;Microwave theory and techniques;Radar imaging;Microwave measurement","","","","","54","IEEE","","","","IEEE","IEEE Journals"
"Robust RGBD Tracking Via Weighted Convolution Operators","W. Liu; X. Tang; C. Zhao","College of Electronic Science, National University of Defense Technology, Changsha, Hunan, China.; College of Electronic Science, National University of Defense Technology, Changsha, Hunan, China.; College of Information Engineering, Shaoyang University, Shaoyang, Hunan, China,422000.","IEEE Sensors Journal","","2020","PP","99","1","1","Discriminative Correlation Filter (DCF) based trackers achieve superior performance with continuous conceptual improvement in the tracking field. By now, traditional DCF based trackers suppose that the tracked target is rigid and could be represented by an axis-aligned rectangular box well. Those trackers suffer from object deformation, irregular object shape and partial occlusion where the target bounding box is filled with both target and background pixels. Recently, the depth information captured by the depth sensors offer complementary information to RGB data. Generally, depth information highlights the foreground target from the background, which mitigates the backgournd effect in the bounding box. In this paper, we propose to learn Weighted Convolution Operators (WCO) for robust RGBD tracking. First, WCO integrate deep features extracted from the RGB channels and hand-craft features extracted from the depth channel to enhance target representation. Second, a weight map is jointly derived from the depth and color information to highlight the foreground area. Each value on the weight map demonstrates the possibility of this pixel pertaining to the foreground area. Last, WCO is optimized with the Preconditioned Congugate Gradient (PCG) Method during correlation filter training. Our proposed WCO tracker achieves the top performance on the Princetion Tracking Benchmark (PTB), which demonstrates the validity of our RGBD tracking framework.","","","10.1109/JSEN.2020.2964019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950173","depth sensor;weighted convolution operator;RGBD tracking;weight map","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Biomedical Imaging and Analysis in the Age of Big Data and Deep Learning [Scanning the Issue]","J. S. Duncan; M. F. Insana; N. Ayache","NA; NA; NA","Proceedings of the IEEE","","2020","108","1","3","10","Imaging of the human body using a number of different modalities has revolutionized the field of medicine over the past several decades and continues to grow at a rapid pace [2]. More than ever, previously unknown information about biology and disease is being unveiled at a range of spatiotemporal scales. Although results and clinical adoption of strategies related to the computational and quantitative analysis of the images have lagged behind development of image acquisition approaches, there has been a noticeable increase of effort and interest in these areas in recent years [6]. This special issue aims to define and highlight some of the “hot” newer ideas that are in biomedical imaging and analysis, intending to shine a light on where the field might move in the next several decades, and focuses on emphasizing where electrical engineers have been involved and could potentially have the most impact. These areas include image acquisition physics, image/signal processing, and image analysis, including pattern recognition and machine learning. This issue focuses on two themes common in much of this effort: first, engineers and computer scientists have found that the information contained in medical images, when viewed through image-based vector spaces, is generally quite sparse. This observation has been transformative in many ways and is quite pervasive in the articles we include here. Second, medical imaging is one of the largest producers of “big data,” and, data-driven machinelearning techniques (e.g., deep learning) are gaining significant attention because improved performance over previous approaches. Thus, data-driven techniques, e.g., formation via image reconstruction [11] and image analysis via deep learning [8], [9], are gaining momentum in their development.","","","10.1109/JPROC.2019.2956422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944337","","Special issues and sections;Deep learning;Machine learning;Biomedical imaging;Medical treatment;Artificial  intelligence","","","","","12","IEEE","","","","IEEE","IEEE Journals"
"Guest Editorial: Information Fusion for Medical Data: Early, Late, and Deep Fusion Methods for Multimodal Data","I. Domingues; H. Müller; A. Ortiz; B. V. Dasarathy; P. H. Abreu; V. D. Calhoun","Medical Physics, Radiobiology and Radiation Protection Group, IPO Porto Research Centre (CI-IPOP), Porto, Portugal; HES-SO, Sierre, Switzerland; Universidad de Málaga, Málaga, Spain; Consultant - Decision Systems & Information Fusion Technologies; Consultant - Decision Systems & Information Fusion Technologies; Consultant - Decision Systems & Information Fusion Technologies","IEEE Journal of Biomedical and Health Informatics","","2020","24","1","14","16","The papers in this special section examine important current topics on multimodal data fusion in the medical context. All clinical data, including genomic and proteomic, play a role in the diagnosis and in particular in the treatment planning and follow-up. This is true for all types of data analyses whether in classification, regression, retrieval, clustering, or other. The interaction between several types of information is not always well understood. Experienced clinicians automatically and even unconsciously add multiple sources of information into their decision process, but machine learning tools often concentrate on single information sources. This special issue presents five examples where several data sources are fused. The papers give several examples of fusion techniques and also the results obtained in quite different application scenarios.","","","10.1109/JBHI.2019.2958429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949775","","Special issus and sections;Deep learning;Bioinformatics;Medical diagnostic imaging;Diseases;Machine learning","","","","","0","IEEE","","","","IEEE","IEEE Journals"
"Deep Learning: Mathematical Foundations and Applications to Information Science","","","IEEE Journal on Selected Areas in Information Theory","","2020","1","1","1","1","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","","","10.1109/JSAIT.2019.2926818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768429","","","","","","","","","","","","IEEE","IEEE Journals"
