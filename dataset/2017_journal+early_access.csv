"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,"Reference Count","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"HD-MTL: Hierarchical Deep Multi-Task Learning for Large-Scale Visual Recognition","J. Fan; T. Zhao; Z. Kuang; Y. Zheng; J. Zhang; J. Yu; J. Peng","Department of Computer Science, The University of North Carolina at Charlotte, Charlotte, NC, USA; Department of Computer Science, The University of North Carolina at Charlotte, Charlotte, NC, USA; College of Computer and Communication Engineering, China University of Petroleum, Qingdao, China; School of Electronic Engineering, Xidian University, Xi’an, China; The University of North Carolina at Charlotte, Charlotte, NC, USA; The University of North Carolina at Charlotte, Charlotte, NC, USA; School of Information Science and Technology, Northwest University, Xi’an, China","IEEE Transactions on Image Processing","","2017","26","4","1923","1938","In this paper, a hierarchical deep multi-task learning (HD-MTL) algorithm is developed to support large-scale visual recognition (e.g., recognizing thousands or even tens of thousands of atomic object classes automatically). To achieve more effective accomplishment of the coarse-to-fine tasks for hierarchical visual recognition, multiple sets of deep features are first extracted from the different layers of deep convolutional neural networks (deep CNNs). A visual tree is then learned by assigning the visually-similar atomic object classes with similar learning complexities into the same group, and it can provide a good environment for identifying the inter-related learning tasks automatically. By leveraging the inter-task relatedness (inter-class similarities) to learn more discriminative group-specific deep representations, our deep multi-task learning algorithm can achieve the global optimum easily and obtain more discriminative node classifiers for distinguishing the visually-similar atomic object classes (in the same group) effectively. Our HD-MTL algorithm can control the inter-level error propagation effectively by using an end-to-end approach for jointly learning more representative deep CNNs (for image representation) and more discriminative tree classifier (for large-scale visual recognition) and updating them simultaneously. Our incremental deep learning algorithms can effectively adapt both the deep CNNs and the tree classifier to the new training images and the new object classes. Our experimental results have demonstrated that our HD-MTL algorithm can achieve very competitive results on both the accuracy rates and the computational efficiency for large-scale visual recognition.","","","10.1109/TIP.2017.2667405","National High-Technology Program of China (863 Program); Program for Changjiang Scholars and Innovative Research Team in University; Program of Shaanxi Province Innovative Research Team; National Science Foundation; National Natural Science Foundation of China; Zhejiang Provincial Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849143","Large-scale visual recognition;hierarchical deep multi-task learning (HD-MTL);group-specific deep representations;incremental deep learning;tree classifier;soft prediction","Visualization;Training;Feature extraction;Image recognition;Machine learning;Atomic layer deposition;Image representation","feature extraction;image classification;image recognition;learning (artificial intelligence);neural nets;trees (mathematics)","HD-MTL;hierarchical deep multitask learning algorithm;large-scale visual recognition;deep feature extraction;deep convolutional neural networks;deep CNN;visual tree;inter-related learning tasks;group-specific deep representations;incremental deep learning algorithms","","15","89","","","","","IEEE","IEEE Journals"
"State-of-the-Art Deep Learning: Evolving Machine Intelligence Toward Tomorrow’s Intelligent Network Traffic Control Systems","Z. M. Fadlullah; F. Tang; B. Mao; N. Kato; O. Akashi; T. Inoue; K. Mizutani","Graduate School of Information Sciences, Tohoku University, Sendai, Japan; Graduate School of Information Sciences, Tohoku University, Sendai, Japan; Graduate School of Information Sciences, Tohoku University, Sendai, Japan; Graduate School of Information Sciences, Tohoku University, Sendai, Japan; Nippon Telegraph and Telephone Corporation, Network Innovation Laboratories, Kanagawa, Japan; Nippon Telegraph and Telephone Corporation, Network Innovation Laboratories, Kanagawa, Japan; Nippon Telegraph and Telephone Corporation, Network Innovation Laboratories, Kanagawa, Japan","IEEE Communications Surveys & Tutorials","","2017","19","4","2432","2455","Currently, the network traffic control systems are mainly composed of the Internet core and wired/wireless heterogeneous backbone networks. Recently, these packet-switched systems are experiencing an explosive network traffic growth due to the rapid development of communication technologies. The existing network policies are not sophisticated enough to cope with the continually varying network conditions arising from the tremendous traffic growth. Deep learning, with the recent breakthrough in the machine learning/intelligence area, appears to be a viable approach for the network operators to configure and manage their networks in a more intelligent and autonomous fashion. While deep learning has received a significant research attention in a number of other domains such as computer vision, speech recognition, robotics, and so forth, its applications in network traffic control systems are relatively recent and garnered rather little attention. In this paper, we address this point and indicate the necessity of surveying the scattered works on deep learning applications for various network traffic control aspects. In this vein, we provide an overview of the state-of-the-art deep learning architectures and algorithms relevant to the network traffic control systems. Also, we discuss the deep learning enablers for network systems. In addition, we discuss, in detail, a new use case, i.e., deep learning based intelligent routing. We demonstrate the effectiveness of the deep learning-based routing approach in contrast with the conventional routing strategy. Furthermore, we discuss a number of open research issues, which researchers may find useful in the future.","","","10.1109/COMST.2017.2707140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932863","Machine learning;machine intelligence;artificial neural network;deep learning;deep belief system;network traffic control;routing","Machine learning;Computer architecture;Control systems;Routing;Learning (artificial intelligence);Biological neural networks;Machine intelligence","intelligent networks;Internet;learning (artificial intelligence);radio networks;telecommunication congestion control;telecommunication network management;telecommunication network routing;telecommunication traffic;traffic control","network traffic control systems;machine learning-intelligence area;network traffic growth;wired-wireless heterogeneous backbone networks;traffic growth;network policies;network systems;deep learning enablers;deep learning architectures;network traffic control aspects;deep learning applications;network operators;continually varying network conditions","","123","260","","","","","IEEE","IEEE Journals"
"Discriminative Deep Metric Learning for Face and Kinship Verification","J. Lu; J. Hu; Y. Tan","Department of Automation, State Key Laboratory of Intelligent Technologies and Systems and Tsinghua National Laboratory for Information Science and Technology (TNList), Tsinghua University, Beijing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Image Processing","","2017","26","9","4269","4282","This paper presents a new discriminative deep metric learning (DDML) method for face and kinship verification in wild conditions. While metric learning has achieved reasonably good performance in face and kinship verification, most existing metric learning methods aim to learn a single Mahalanobis distance metric to maximize the inter-class variations and minimize the intra-class variations, which cannot capture the nonlinear manifold where face images usually lie on. To address this, we propose a DDML method to train a deep neural network to learn a set of hierarchical nonlinear transformations to project face pairs into the same latent feature space, under which the distance of each positive pair is reduced and that of each negative pair is enlarged. To better use the commonality of multiple feature descriptors to make all the features more robust for face and kinship verification, we develop a discriminative deep multi-metric learning method to jointly learn multiple neural networks, under which the correlation of different features of each sample is maximized, and the distance of each positive pair is reduced and that of each negative pair is enlarged. Extensive experimental results show that our proposed methods achieve the acceptable results in both face and kinship verification.","","","10.1109/TIP.2017.2717505","National Natural Science Foundation of China; National 1000 Young Talents Plan Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953665","Face verification;kinship verification;deep learning;deep metric learning;multi-feature learning","Face;Measurement;Neural networks;Machine learning;Feature extraction;Learning systems;Training","face recognition;learning (artificial intelligence);neural nets","correlation;multiple neural networks;multiple feature descriptors;latent feature space;project face pairs;deep neural network;hierarchical nonlinear transformations;face images;interclass variations;Mahalanobis distance metric;DDML method;wild conditions;face verification;kinship verification;discriminative deep metric learning","Algorithms;Biometric Identification;Face;Family;Humans;Machine Learning;Neural Networks (Computer);Pattern Recognition, Automated","26","68","","","","","IEEE","IEEE Journals"
"Deep Direct Reinforcement Learning for Financial Signal Representation and Trading","Y. Deng; F. Bao; Y. Kong; Z. Ren; Q. Dai","Automation Department, Tsinghua University, Beijing, China; Automation Department, Tsinghua University, Beijing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; Automation Department, Tsinghua University, Beijing, China; Automation Department, Tsinghua University, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","3","653","664","Can we train the computer to beat experienced traders for financial assert trading? In this paper, we try to address this challenge by introducing a recurrent deep neural network (NN) for real-time financial signal representation and trading. Our model is inspired by two biological-related learning concepts of deep learning (DL) and reinforcement learning (RL). In the framework, the DL part automatically senses the dynamic market condition for informative feature learning. Then, the RL module interacts with deep representations and makes trading decisions to accumulate the ultimate rewards in an unknown environment. The learning system is implemented in a complex NN that exhibits both the deep and recurrent structures. Hence, we propose a task-aware backpropagation through time method to cope with the gradient vanishing issue in deep training. The robustness of the neural system is verified on both the stock and the commodity future markets under broad testing conditions.","","","10.1109/TNNLS.2016.2522401","Project of the National Natural Science Foundation of China; National Science Foundation of Jiangsu Province, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407387","Deep learning (DL);financial signal processing;neural network (NN) for finance;reinforcement learning (RL)","Training;Robustness;Learning (artificial intelligence);Artificial neural networks;Feature extraction;Optimization;Signal representation","backpropagation;decision making;feedforward neural nets;financial data processing;recurrent neural nets;stock markets","deep direct reinforcement learning;financial assert trading;recurrent deep neural network;real-time financial signal representation;biological-related learning concepts;dynamic market condition;informative feature learning;deep representations;trading decision making;complex NN;task-aware backpropagation;deep training;neural system robustness","","81","44","","","","","IEEE","IEEE Journals"
"Airline Passenger Profiling Based on Fuzzy Deep Machine Learning","Y. Zheng; W. Sheng; X. Sun; S. Chen","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","12","2911","2923","Passenger profiling plays a vital part of commercial aviation security, but classical methods become very inefficient in handling the rapidly increasing amounts of electronic records. This paper proposes a deep learning approach to passenger profiling. The center of our approach is a Pythagorean fuzzy deep Boltzmann machine (PFDBM), whose parameters are expressed by Pythagorean fuzzy numbers such that each neuron can learn how a feature affects the production of the correct output from both the positive and negative sides. We propose a hybrid algorithm combining a gradient-based method and an evolutionary algorithm for training the PFDBM. Based on the novel learning model, we develop a deep neural network (DNN) for classifying normal passengers and potential attackers, and further develop an integrated DNN for identifying group attackers whose individual features are insufficient to reveal the abnormality. Experiments on data sets from Air China show that our approach provides much higher learning ability and classification accuracy than existing profilers. It is expected that the fuzzy deep learning approach can be adapted for a variety of complex pattern analysis tasks.","","","10.1109/TNNLS.2016.2609437","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7577870","Biogeography-based optimization (BBO);deep Boltzmann machine (DBM);deep learning;evolutionary neural networks;passenger profiling;pythagorean fuzzy set (PFS)","Terrorism;Machine learning;Inspection;Atmospheric modeling;Fuzzy sets;Airports","Boltzmann machines;evolutionary computation;fuzzy set theory;learning (artificial intelligence);neural nets;pattern classification;travel industry","evolutionary algorithm;PFDBM;deep neural network;gradient-based method;Pythagorean fuzzy numbers;Pythagorean fuzzy deep Boltzmann machine;electronic records;commercial aviation security;fuzzy deep machine learning;airline passenger profiling;classification accuracy","","6","74","","","","","IEEE","IEEE Journals"
"Cost-Effective Active Learning for Deep Image Classification","K. Wang; D. Zhang; Y. Li; R. Zhang; L. Lin","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Guangzhou University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Circuits and Systems for Video Technology","","2017","27","12","2591","2600","Recent successes in learning-based image classification, however, heavily rely on the large number of annotated training samples, which may require considerable human effort. In this paper, we propose a novel active learning (AL) framework, which is capable of building a competitive classifier with optimal feature representation via a limited amount of labeled training instances in an incremental learning manner. Our approach advances the existing AL methods in two aspects. First, we incorporate deep convolutional neural networks into AL. Through the properly designed framework, the feature representation and the classifier can be simultaneously updated with progressively annotated informative samples. Second, we present a cost-effective sample selection strategy to improve the classification performance with less manual annotations. Unlike traditional methods focusing on only the uncertain samples of low prediction confidence, we especially discover the large amount of high-confidence samples from the unlabeled set for feature learning. Specifically, these high-confidence samples are automatically selected and iteratively assigned pseudolabels. We thus call our framework cost-effective AL (CEAL) standing for the two advantages. Extensive experiments demonstrate that the proposed CEAL framework can achieve promising results on two challenging image classification data sets, i.e., face recognition on the cross-age celebrity face recognition data set database and object categorization on Caltech-256.","","","10.1109/TCSVT.2016.2589879","National Natural Science Foundation of China; State Key Development Program; CCF-Tencent Open Fund; Special Program through the Applied Research on Super Computation of the Natural Science Foundation of China–Guangdong Joint Fund (the second phase); NVIDIA Corporation through the Tesla K40 GPU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508942","Active learning (AL);deep neural nets;image classification;incremental learning","Machine learning;Uncertainty;Measurement uncertainty;Learning systems;Neural networks;Visualization","face recognition;feature extraction;image classification;image representation;image retrieval;learning (artificial intelligence);neural nets;pattern classification","cost-effective active learning;deep image classification;annotated training samples;active learning framework;competitive classifier;optimal feature representation;labeled training instances;incremental learning manner;deep convolutional neural networks;cost-effective sample selection strategy;classification performance;manual annotations;uncertain samples;low prediction confidence;high-confidence samples;feature learning;framework cost-effective AL;CEAL framework;AL methods;image classification data sets;cross-age celebrity face recognition data set database;object categorization;Caltech-256","","33","42","","","","","IEEE","IEEE Journals"
"System Design Perspective for Human-Level Agents Using Deep Reinforcement Learning: A Survey","N. D. Nguyen; T. Nguyen; S. Nahavandi","Institute for Intelligent Systems Research and Innovation, Deakin University, Waurn Ponds Campus, Geelong, VIC, Australia; Institute for Intelligent Systems Research and Innovation, Deakin University, Waurn Ponds Campus, Geelong, VIC, Australia; Institute for Intelligent Systems Research and Innovation, Deakin University, Waurn Ponds Campus, Geelong, VIC, Australia","IEEE Access","","2017","5","","27091","27102","Reinforcement learning (RL) has distinguished itself as a prominent learning method to augment the efficacy of autonomous systems. Recent advances in deep learning studies have complemented existing RL methods and led to a crucial breakthrough in the effort of applying RL to automation and robotics. Artificial agents based on deep RL can take selective and intelligent actions comparable with those of a human to maximize the feedback reward from the interactive environment. In this paper, we survey recent developments in the literature regarding deep RL methods for building human-level agents. As a result, prominent studies that involve modeling every aspect of a human-level agent will be examined. We also provide an overview of constructing a framework for prospective autonomous systems. Moreover, various toolkits and frameworks are suggested to facilitate the development of deep RL methods. Finally, we open a discussion that potentially raises a range of future research directions in deep RL.","","","10.1109/ACCESS.2017.2777827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119919","Deep learning;human-level agents;reinforcement learning;robotics;survey;system design","Machine learning;Robots;Learning systems;Learning (artificial intelligence);Mathematical model;Monte Carlo methods","learning (artificial intelligence)","system design perspective;human-level agent;deep reinforcement learning;artificial agents;deep RL methods;autonomous systems;intelligent actions","","15","75","","","","","IEEE","IEEE Journals"
"Deep Learning for Health Informatics","D. Ravì; C. Wong; F. Deligianni; M. Berthelot; J. Andreu-Perez; B. Lo; G. Yang","Hamlyn Centre, Imperial College London, London, U.K.; Hamlyn Centre, Imperial College London, London, U.K.; Hamlyn Centre, Imperial College London, London, U.K.; Hamlyn Centre, Imperial College London, London, U.K.; Hamlyn Centre, Imperial College London, London, U.K.; Hamlyn Centre, Imperial College London, London, U.K.; Hamlyn Centre, Imperial College London, London, U.K.","IEEE Journal of Biomedical and Health Informatics","","2017","21","1","4","21","With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.","","","10.1109/JBHI.2016.2636665","EPSRC Smart Sensing for Surgery; EPSRC-NIHR HTC Partnership Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801947","Bioinformatics;deep learning;health informatics;machine learning;medical imaging;public health;wearable devices","Machine learning;Informatics;Training;Biomedical imaging;Neurons;Artificial neural networks;Biological neural networks","bioinformatics;learning (artificial intelligence);medical information systems;neural nets","deep learning;health informatics;multimodality data;machine learning;artificial intelligence;parallelization;translational bioinformatics;medical imaging;pervasive sensing;medical informatics;public health","Computational Biology;Humans;Machine Learning;Medical Informatics;Monitoring, Ambulatory;Public Health","269","145","CCBY","","","","IEEE","IEEE Journals"
"Active Deep Learning for Classification of Hyperspectral Images","P. Liu; H. Zhang; K. B. Eom","Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Laboratory of Brain and Cognition, National Institute of Mental Health, Bethesda, MD, USA; Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","2","712","724","Active deep learning classification of hyperspectral images is considered in this paper. Deep learning has achieved success in many applications, but good-quality labeled samples are needed to construct a deep learning network. It is expensive getting good labeled samples in hyperspectral images for remote sensing applications. An active learning algorithm based on a weighted incremental dictionary learning is proposed for such applications. The proposed algorithm selects training samples that maximize two selection criteria, namely representative and uncertainty. This algorithm trains a deep network efficiently by actively selecting training samples at each iteration. The proposed algorithm is applied for the classification of hyperspectral images, and compared with other classification algorithms employing active learning. It is shown that the proposed algorithm is efficient and effective in classifying hyperspectral images.","","","10.1109/JSTARS.2016.2598859","NSFC; RADI Director Youth foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7568999","Active learning;deep learning;remote sensing classification;sparse representation","Training;Uncertainty;Machine learning;Hyperspectral imaging;Tuning","hyperspectral imaging;image classification;learning (artificial intelligence)","active deep learning;hyperspectral image classification;deep learning network;remote sensing applications;weighted incremental dictionary learning;selection criteria;representative criteria;uncertainty criteria;classification algorithms","","39","25","","","","","IEEE","IEEE Journals"
"Nonlinear Deep Kernel Learning for Image Annotation","M. Jiu; H. Sahbi","LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay, Paris, France; CNRS, LIP6 UPMC Sorbonne Universités, Paris, France","IEEE Transactions on Image Processing","","2017","26","4","1820","1832","Multiple kernel learning (MKL) is a widely used technique for kernel design. Its principle consists in learning, for a given support vector classifier, the most suitable convex (or sparse) linear combination of standard elementary kernels. However, these combinations are shallow and often powerless to capture the actual similarity between highly semantic data, especially for challenging classification tasks, such as image annotation. In this paper, we redefine multiple kernels using deep multi-layer networks. In this new contribution, a deep multiple kernel is recursively defined as a multi-layered combination of nonlinear activation functions, each one involves a combination of several elementary or intermediate kernels, and results into a positive semi-definite deep kernel. We propose four different frameworks in order to learn the weights of these networks: supervised, unsupervised, kernel-based semi-supervised, and Laplacian-based semi-supervised. When plugged into support vector machines, the resulting deep kernel networks show clear gain, compared with several shallow kernels for the task of image annotation. Extensive experiments and analysis on the challenging ImageCLEF photo annotation benchmark, the COREL5k database, and the Banana data set validate the effectiveness of the proposed method.","","","10.1109/TIP.2017.2666038","Research Agency Agence Nationale de la Recherche under the MLVIS Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847355","Multiple kernel learning;image annotation;deep learning;semi-supervised learning","Kernel;Visualization;Standards;Semantics;Training;Support vector machines;Feature extraction","image retrieval;support vector machines;unsupervised learning","nonlinear deep kernel learning;image annotation;support vector classifier;deep multilayer networks;positive semi-definite deep kernel;Laplacian-based semi-supervised learning;unsupervised learning;supervised learning;kernel-based semi-supervised learning;ImageCLEF photo annotation benchmark;COREL5k database;Banana data set","","21","78","","","","","IEEE","IEEE Journals"
"Routing or Computing? The Paradigm Shift Towards Intelligent Computer Network Packet Transmission Based on Deep Learning","B. Mao; Z. M. Fadlullah; F. Tang; N. Kato; O. Akashi; T. Inoue; K. Mizutani","Graduate School of Information Sciences, Tohoku University, Sendai, Japan; Graduate School of Information Sciences, Tohoku University, Sendai, Japan; Graduate School of Information Sciences, Tohoku University, Sendai, Japan; Graduate School of Information Sciences, Tohoku University, Sendai, Japan; Nippon Telegraph and Telephone Corporation (NTT) Network Innovation Laboratories, Yokosuka, Kanagawa, Japan; Nippon Telegraph and Telephone Corporation (NTT) Network Innovation Laboratories, Yokosuka, Kanagawa, Japan; Nippon Telegraph and Telephone Corporation (NTT) Network Innovation Laboratories, Yokosuka, Kanagawa, Japan","IEEE Transactions on Computers","","2017","66","11","1946","1960","Recent years, Software Defined Routers (SDRs) (programmable routers) have emerged as a viable solution to provide a cost-effective packet processing platform with easy extensibility and programmability. Multi-core platforms significantly promote SDRs' parallel computing capacities, enabling them to adopt artificial intelligent techniques, i.e., deep learning, to manage routing paths. In this paper, we explore new opportunities in packet processing with deep learning to inexpensively shift the computing needs from rule-based route computation to deep learning based route estimation for high-throughput packet processing. Even though deep learning techniques have been extensively exploited in various computing areas, researchers have, to date, not been able to effectively utilize deep learning based route computation for high-speed core networks. We envision a supervised deep learning system to construct the routing tables and show how the proposed method can be integrated with programmable routers using both Central Processing Units (CPUs) and Graphics Processing Units (GPUs). We demonstrate how our uniquely characterized input and output traffic patterns can enhance the route computation of the deep learning based SDRs through both analysis and extensive computer simulations. In particular, the simulation results demonstrate that our proposal outperforms the benchmark method in terms of delay, throughput, and signaling overhead.","","","10.1109/TC.2017.2709742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7935536","Software defined routers;network traffic control;deep learning;backbone networks;core networks;routing","Machine learning;Routing;Computer architecture;Software defined networking;Graphics processing units;Central Processing Unit;Telecommunication traffic","computer networks;graphics processing units;learning (artificial intelligence);multiprocessing systems;telecommunication network routing","artificial intelligent techniques;routing paths;deep learning based route estimation;high-throughput packet processing;deep learning based route computation;high-speed core networks;supervised deep learning system;routing tables;Software Defined Routers;cost-effective packet processing platform;multicore platforms;parallel computing capacities;intelligent computer network packet transmission","","65","36","Traditional","","","","IEEE","IEEE Journals"
"A Deep Matrix Factorization Method for Learning Attribute Representations","G. Trigeorgis; K. Bousmalis; S. Zafeiriou; B. W. Schuller","Department of Computing, Imperial College London, London, United Kingdom; Google Research, Mountain View, CA; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","3","417","429","Semi-Non-negative Matrix Factorization is a technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original data matrix contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies cannot interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering according to different, unknown attributes of a given dataset. We also present a semi-supervised version of the algorithm, named Deep WSF, that allows the use of (partial) prior information for each of the known attributes of a dataset, that allows the model to be used on datasets with mixed attribute knowledge. Finally, we show that our models are able to learn low-dimensional representations that are better suited for clustering, but also classification, outperforming Semi-Non-negative Matrix Factorization, but also other state-of-the-art methodologies variants.","","","10.1109/TPAMI.2016.2554555","Konstantinos Bousmalis; Google Europe Fellowship in Social Signal Processing; EPSRC project; European Community's Horizon 2020 Framework Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453156","Semi-NMF;deep semi-NMF;unsupervised feature learning;face clustering;semi-supervised learning;Deep WSF;WSF;matrix factorization;face classification","Face;Clustering algorithms;Matrix decomposition;Data models;Algorithm design and analysis;Feature extraction;Face recognition","learning (artificial intelligence);matrix decomposition;pattern clustering","deep matrix factorization method;learning attribute representations;seminonnegative matrix factorization;low-dimensional dataset representation;clustering interpretation;data matrix;complex hierarchical information;implicit lower-level hidden attributes;classical one level clustering methodology;deep semiNMF model;semisupervised learning algorithm;deep WSF;mixed attribute knowledge","","54","55","","","","","IEEE","IEEE Journals"
"Deep Multimodal Distance Metric Learning Using Click Constraints for Image Ranking","J. Yu; X. Yang; F. Gao; D. Tao","Key Laboratory of Complex Systems Modeling and Simulation, School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Key Laboratory of Complex Systems Modeling and Simulation, School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology Sydney, Sydney, NSW, Australia","IEEE Transactions on Cybernetics","","2017","47","12","4014","4024","How do we retrieve images accurately? Also, how do we rank a group of images precisely and efficiently for specific queries? These problems are critical for researchers and engineers to generate a novel image searching engine. First, it is important to obtain an appropriate description that effectively represent the images. In this paper, multimodal features are considered for describing images. The images unique properties are reflected by visual features, which are correlated to each other. However, semantic gaps always exist between images visual features and semantics. Therefore, we utilize click feature to reduce the semantic gap. The second key issue is learning an appropriate distance metric to combine these multimodal features. This paper develops a novel deep multimodal distance metric learning (Deep-MDML) method. A structured ranking model is adopted to utilize both visual and click features in distance metric learning (DML). Specifically, images and their related ranking results are first collected to form the training set. Multimodal features, including click and visual features, are collected with these images. Next, a group of autoencoders is applied to obtain initially a distance metric in different visual spaces, and an MDML method is used to assign optimal weights for different modalities. Next, we conduct alternating optimization to train the ranking model, which is used for the ranking of new queries with click features. Compared with existing image ranking methods, the proposed method adopts a new ranking model to use multimodal features, including click features and visual features in DML. We operated experiments to analyze the proposed Deep-MDML in two benchmark data sets, and the results validate the effects of the method.","","","10.1109/TCYB.2016.2591583","National Natural Science Foundation of China; Zhejiang Provincial Natural Science Foundation of China; Program for New Century Excellent Talents in University; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529190","Deep learning;distance metric learning (DML);image ranking;multimodal","Machine learning;Distance learning;Image retrieval;Semantics;Ranking (statistics);Extraterrestrial measurements","feature extraction;image representation;image retrieval;learning (artificial intelligence);optimisation","related ranking results;multimodal features;click feature;Deep-MDML;click constraints;image searching engine;images unique properties;semantic gap;images visual features;deep multimodal distance metric learning method;structured ranking model;distance metric;visual spaces;image ranking methods","","26","62","","","","","IEEE","IEEE Journals"
"D-DASH: A Deep Q-Learning Framework for DASH Video Streaming","M. Gadaleta; F. Chiariotti; M. Rossi; A. Zanella","Department of Information Engineering, University of Padova, Padova, Italy; Department of Information Engineering, University of Padova, Padova, Italy; Department of Information Engineering, University of Padova, Padova, Italy; Department of Information Engineering, University of Padova, Padova, Italy","IEEE Transactions on Cognitive Communications and Networking","","2017","3","4","703","718","The ever-increasing demand for seamless high-definition video streaming, along with the widespread adoption of the dynamic adaptive streaming over HTTP (DASH) standard, has been a major driver of the large amount of research on bitrate adaptation algorithms. The complexity and variability of the video content and of the mobile wireless channel make this an ideal application for learning approaches. Here, we present D-DASH, a framework that combines deep learning and reinforcement learning techniques to optimize the quality of experience (QoE) of DASH. Different learning architectures are proposed and assessed, combining feed-forward and recurrent deep neural networks with advanced strategies. D-DASH designs are thoroughly evaluated against prominent algorithms from the state-of-the-art, both heuristic and learning-based, evaluating performance indicators such as image quality across video segments and freezing/rebuffering events. Our numerical results are obtained on real and simulated channel traces and show the superiority of D-DASH in nearly all the considered quality metrics. Besides yielding a considerably higher QoE, the D-DASH framework exhibits faster convergence to the rate-selection strategy than the other learning algorithms considered in the study. This makes it possible to shorten the training phase, making D-DASH a good candidate for client-side runtime learning.","","","10.1109/TCCN.2017.2755007","University of Padova through the project CPDA 151221 “IoT-SURF: a unifying abstraction and reasoning framework for connected and unconnected objects”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8048013","DASH;HTTP adaptive streaming;reinforcement learning;deep Q-learning;LSTM","Streaming media;Heuristic algorithms;Bit rate;Neural networks;Dynamic programming","feedforward neural nets;hypermedia;learning (artificial intelligence);quality of experience;recurrent neural nets;transport protocols;video streaming;wireless channels","deep q-learning framework;DASH video streaming;seamless high-definition video streaming;dynamic adaptive;HTTP standard;bitrate adaptation algorithms;variability;video content;mobile wireless channel;deep learning;deep neural networks;D-DASH designs;prominent algorithms;heuristic learning;image quality;video segments;learning algorithms;client-side runtime learning;D-DASH;D-DASH framework","","24","54","","","","","IEEE","IEEE Journals"
"Deep Learning Backend for Single and Multisession i-Vector Speaker Recognition","O. Ghahabi; J. Hernando","TALP Research Center, Department of Signal Theory and Communications, Universitat Politecnica de Catalunya—BarcelonaTech, Barcelona, Spain; TALP Research Center, Department of Signal Theory and Communications, Universitat Politecnica de Catalunya—BarcelonaTech, Barcelona, Spain","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","4","807","817","The lack of labeled background data makes a big performance gap between cosine and Probabilistic Linear Discriminant Analysis (PLDA) scoring baseline techniques for i-vectors in speaker recognition. Although there are some unsupervised clustering techniques to estimate the labels, they cannot accurately predict the true labels and they also assume that there are several samples from the same speaker in the background data that could not be true in reality. In this paper, the authors make use of Deep Learning (DL) to fill this performance gap given unlabeled background data. To this goal, the authors have proposed an impostor selection algorithm and a universal model adaptation process in a hybrid system based on deep belief networks and deep neural networks to discriminatively model each target speaker. In order to have more insight into the behavior of DL techniques in both single- and multisession speaker enrollment tasks, some experiments have been carried out in this paper in both scenarios. Experiments on National Institute of Standards and Technology 2014 i-vector challenge show that 46% of this performance gap, in terms of minimum of the decision cost function, is filled by the proposed DL-based system. Furthermore, the score combination of the proposed DL-based system and PLDA with estimated labels covers 79% of this gap.","","","10.1109/TASLP.2017.2661705","Spanish Project DeepVoice; European project CAMOMILE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847321","Deep learning;deep neural network;deep belief network;i-vector;speaker recognition","Speaker recognition;Speech;Adaptation models;Training;Machine learning;NIST;Speech processing","belief networks;feedforward neural nets;learning (artificial intelligence);speaker recognition","deep learning;i-vector speaker recognition;impostor selection algorithm;universal model adaptation process;hybrid system;deep belief networks;deep neural networks;DL techniques;multisession speaker enrollment tasks;single-session speaker enrollment tasks;decision cost function;DL-based system;PLDA;probabilistic linear discriminant analysis","","13","55","","","","","IEEE","IEEE Journals"
"A Deep Learning Scheme for Motor Imagery Classification based on Restricted Boltzmann Machines","N. Lu; T. Li; X. Ren; H. Miao","State Key Laboratory for Manufacturing Systems Engineering, Systems Engineering Institute, Xi’an Jiaotong University, Xi’an Shaanxi, China; State Key Laboratory for Manufacturing Systems Engineering, Systems Engineering Institute, Xi’an Jiaotong University, Xi’an Shaanxi, China; State Key Laboratory for Manufacturing Systems Engineering, Systems Engineering Institute, Xi’an Jiaotong University, Xi’an Shaanxi, China; Department of Biostatistics, School of Public Health, University of Texas at Houston, Houston, TX, USA","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","2017","25","6","566","576","Motor imagery classification is an important topic in brain-computer interface (BCI) research that enables the recognition of a subject's intension to, e.g., implement prosthesis control. The brain dynamics of motor imagery are usually measured by electroencephalography (EEG) as nonstationary time series of low signal-to-noise ratio. Although a variety of methods have been previously developed to learn EEG signal features, the deep learning idea has rarely been explored to generate new representation of EEG features and achieve further performance improvement for motor imagery classification. In this study, a novel deep learning scheme based on restricted Boltzmann machine (RBM) is proposed. Specifically, frequency domain representations of EEG signals obtained via fast Fourier transform (FFT) and wavelet package decomposition (WPD) are obtained to train three RBMs. These RBMs are then stacked up with an extra output layer to form a four-layer neural network, which is named the frequential deep belief network (FDBN). The output layer employs the softmax regression to accomplish the classification task. Also, the conjugate gradient method and backpropagation are used to fine tune the FDBN. Extensive and systematic experiments have been performed on public benchmark datasets, and the results show that the performance improvement of FDBN over other selected state-of-the-art methods is statistically significant. Also, several findings that may be of significant interest to the BCI community are presented in this article.","","","10.1109/TNSRE.2016.2601240","Fundamental Research Funds for the Central Universities, National Natural Science Foundation of China; Research Fund for the Doctoral Program of Higher Education of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546909","Brain–computer interface (BCI);deep learning;motor imagery;restricted Boltzman machine (RBM)","Machine learning;Electroencephalography;Biological neural networks;Frequency-domain analysis;Feature extraction;Training;Benchmark testing","bioelectric potentials;Boltzmann machines;brain-computer interfaces;electroencephalography;fast Fourier transforms;feature extraction;learning (artificial intelligence);medical signal processing;neurophysiology;signal classification;wavelet neural nets","deep learning scheme;motor imagery classification;restricted Boltzmann machines;brain-computer interface;implement prosthesis control;brain dynamics;electroencephalography;EEG signal feature;fast Fourier transform;wavelet package decomposition;four-layer neural network;frequential deep belief network","Algorithms;Brain Mapping;Brain-Computer Interfaces;Electroencephalography;Evoked Potentials, Motor;Fourier Analysis;Humans;Imagination;Intention;Machine Learning;Motor Cortex;Movement;Neural Networks (Computer);Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted","67","42","","","","","IEEE","IEEE Journals"
"DLAU: A Scalable Deep Learning Accelerator Unit on FPGA","C. Wang; L. Gong; Q. Yu; X. Li; Y. Xie; X. Zhou","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of California at Santa Barbara, Santa Barbara, CA, USA; University of Science and Technology of China, Hefei, China","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2017","36","3","513","517","As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. However, the size of the networks becomes increasingly large scale due to the demands of the practical applications, which poses significant challenge to construct a high performance implementations of deep learning neural networks. In order to improve the performance as well as to maintain the low power cost, in this paper we design deep learning accelerator unit (DLAU), which is a scalable accelerator architecture for large-scale deep learning networks using field-programmable gate array (FPGA) as the hardware prototype. The DLAU accelerator employs three pipelined processing units to improve the throughput and utilizes tile techniques to explore locality for deep learning applications. Experimental results on the state-of-the-art Xilinx FPGA board demonstrate that the DLAU accelerator is able to achieve up to 36.1× speedup comparing to the Intel Core2 processors, with the power consumption at 234 mW.","","","10.1109/TCAD.2016.2587683","NSFC; Jiangsu Anhui Provincial Natural Science Foundation; Anhui Provincial Natural Science Foundation; CCF-Tencent Open Research Fund; CSC Fellowship; Open Project of State Key Laboratory of Computer ArchitectureICT-CAS; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505926","Deep learning;field-programmable gate array (FPGA);hardware accelerator;neural network","Machine learning;Field programmable gate arrays;Hardware;Biological neural networks;Neurons;Computer architecture","field programmable gate arrays;neural nets;power consumption","scalable deep learning accelerator unit;machine learning;complex learning problem;deep learning neural networks;large-scale deep learning networks;field-programmable gate array;Xilinx FPGA board;DLAU accelerator;Intel Core2 processors;power consumption;power 234 mW","","23","9","","","","","IEEE","IEEE Journals"
"A Pythagorean-Type Fuzzy Deep Denoising Autoencoder for Industrial Accident Early Warning","Y. Zheng; S. Chen; Y. Xue; J. Xue","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Computer Science and Engineering, Tianjin University of Science and Technology, Tianjin, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; Jiangxi Provincial Laboratory of High-Performance Computing, Jiangxi Normal University, Nanchang, China","IEEE Transactions on Fuzzy Systems","","2017","25","6","1561","1575","Early warning is crucial for preventing industrial accidents and mitigating damage, but current methods are often time-consuming, error-prone, and incompetent to deal with uncertainty. This paper presents a fuzzy deep neural network for early warning of industrial accidents, which equips the classical deep denoising autoencoder (DDAE) model with Pythagorean-type fuzzy parameters in order to enhance the model's representation ability and robustness. To efficiently train the fuzzy deep model, we propose a hybrid algorithm combining Hessian-free optimization and biogeography-based optimization metaheuristic to balance global search and local search. Experiments on datasets from several industrial zones in China show that the proposed Pythagorean-type fuzzy DDAE (PFDDAE) can achieve much higher accuracy of accident risk classification than the classical DDAE and the fuzzy DDAE using regular fuzzy parameters, and the proposed hybrid learning algorithm exhibits significant performance advantage over some other learning algorithms in training PFDDAE. In particular, a test on the 2014 Kunshan aluminum dust explosion accident shows that the deep learning model would be very likely to prevent the accident if it was adopted in advance.","","","10.1109/TFUZZ.2017.2738605","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8007290","Accident early warning;biogeography-based optimization (BBO);deep denoising autoencoder (DDAE);deep learning;evolutionary learning;pythagorean fuzzy set (PFS)","Noise reduction;Fuzzy sets;Industrial accidents;Uncertainty;Machine learning;Explosions","accident prevention;dust;emergency management;fuzzy set theory;industrial accidents;learning (artificial intelligence);neural nets;optimisation","accident risk classification;classical DDAE;regular fuzzy parameters;2014 Kunshan aluminum dust explosion accident;deep learning model;industrial accident;early warning;classical deep denoising autoencoder model;Pythagorean-type fuzzy parameters;fuzzy deep model;Hessian-free optimization;global search;local search;industrial zones;Pythagorean-type fuzzy DDAE;Pythagorean-type fuzzy deep neural network","","6","75","Traditional","","","","IEEE","IEEE Journals"
"Optimized Structure of the Traffic Flow Forecasting Model With a Deep Learning Approach","H. Yang; T. S. Dillon; Y. P. Chen","Department of Computer Science and Information Technology, La Trobe University, Melbourne, VIC, Australia; Department of Computer Science and Information Technology, La Trobe University, Melbourne, VIC, Australia; Department of Computer Science and Information Technology, La Trobe University, Melbourne, VIC, Australia","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","10","2371","2381","Forecasting accuracy is an important issue for successful intelligent traffic management, especially in the domain of traffic efficiency and congestion reduction. The dawning of the big data era brings opportunities to greatly improve prediction accuracy. In this paper, we propose a novel model, stacked autoencoder Levenberg-Marquardt model, which is a type of deep architecture of neural network approach aiming to improve forecasting accuracy. The proposed model is designed using the Taguchi method to develop an optimized structure and to learn traffic flow features through layer-by-layer feature granulation with a greedy layerwise unsupervised learning algorithm. It is applied to real-world data collected from the M6 freeway in the U.K. and is compared with three existing traffic predictors. To the best of our knowledge, this is the first time that an optimized structure of the traffic flow forecasting model with a deep learning approach is presented. The evaluation results demonstrate that the proposed model with an optimized structure has superior performance in traffic flow forecasting.","","","10.1109/TNNLS.2016.2574840","Australia Research Linkage Grants Scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7517319","Deep learning;forecasting;neural network (NN) applications;stacked denoising autoencoders","Forecasting;Predictive models;Artificial neural networks;Machine learning;Prediction algorithms;Data models;Computational modeling","Big Data;learning (artificial intelligence);neural nets;road traffic;traffic engineering computing","traffic flow forecasting model;optimized structure;deep learning approach;intelligent traffic management;traffic efficiency;stacked autoencoder Levenberg-Marquardt model;neural network approach;Taguchi method;layer-by-layer feature granulation;greedy layerwise unsupervised learning algorithm","","27","40","","","","","IEEE","IEEE Journals"
"A Deep Learning Approach to on-Node Sensor Data Analytics for Mobile or Wearable Devices","D. Ravì; C. Wong; B. Lo; G. Yang","Hamlyn Centre, Imperial College London, London, U.K.; Hamlyn Centre, Imperial College London, London, U.K.; Hamlyn Centre, Imperial College London, London, U.K.; Hamlyn Centre, Imperial College London, London, U.K.","IEEE Journal of Biomedical and Health Informatics","","2017","21","1","56","64","The increasing popularity of wearable devices in recent years means that a diverse range of physiological and functional data can now be captured continuously for applications in sports, wellbeing, and healthcare. This wealth of information requires efficient methods of classification and analysis where deep learning is a promising technique for large-scale data analytics. While deep learning has been successful in implementations that utilize high-performance computing platforms, its use on low-power wearable devices is limited by resource constraints. In this paper, we propose a deep learning methodology, which combines features learned from inertial sensor data together with complementary information from a set of shallow features to enable accurate and real-time activity classification. The design of this combined method aims to overcome some of the limitations present in a typical deep learning framework where on-node computation is required. To optimize the proposed method for real-time on-node computation, spectral domain preprocessing is used before the data are passed onto the deep learning framework. The classification accuracy of our proposed deep learning approach is evaluated against state-of-the-art methods using both laboratory and real world activity datasets. Our results show the validity of the approach on different human activity datasets, outperforming other methods, including the two methods used within our combined pipeline. We also demonstrate that the computation times for the proposed method are consistent with the constraints of real-time on-node processing on smartphones and a wearable sensor platform.","","","10.1109/JBHI.2016.2633287","EPSRC; EPSRC-NIHR HTC Partnership Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797232","ActiveMiles;deep learning;Human Activity Recognition (HAR);Internet-of-Things (IoT);low-power devices;wearable","Machine learning;Feature extraction;Time-frequency analysis;Spectrogram;Real-time systems;Pipelines;Performance evaluation","data analysis;health care;Internet of Things;learning (artificial intelligence);wearable computers","on-node sensor data analytics;deep learning approach;wearable devices;inertial sensor;spectral domain preprocessing;smartphones;Internet-of-Things;Human Activity Recognition;HAR","Human Activities;Humans;Machine Learning;Monitoring, Ambulatory;Neural Networks (Computer);Signal Processing, Computer-Assisted","88","34","CCBY","","","","IEEE","IEEE Journals"
"Discriminative Robust Deep Dictionary Learning for Hyperspectral Image Classification","V. Singhal; H. K. Aggarwal; S. Tariyal; A. Majumdar","Indraprastha Institute of Information Technology at Delhi, New Delhi, India; Indraprastha Institute of Information Technology at Delhi, New Delhi, India; Indraprastha Institute of Information Technology at Delhi, New Delhi, India; Indraprastha Institute of Information Technology at Delhi, New Delhi, India","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","9","5274","5283","This paper proposes a new framework for deep learning that has been particularly tailored for hyperspectral image classification. We learn multiple levels of dictionaries in a robust fashion. The last layer is discriminative that learns a linear classifier. The training proceeds greedily; at a time, a single level of dictionary is learned and the coefficients used to train the next level. The coefficients from the final level are used for classification. Robustness is incorporated by minimizing the absolute deviations instead of the more popular Euclidean norm. The inbuilt robustness helps combat mixed noise (Gaussian and sparse) present in hyperspectral images. Results show that our proposed techniques outperform all other deep learning methods-deep belief network, stacked autoencoder, and convolutional neural network. The experiments have been carried out on both benchmark deep learning data sets (MNIST, CIFAR-10, and Street View House Numbers) as well as on real hyperspectral imaging data sets.","","","10.1109/TGRS.2017.2704590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7948751","Deep learning;dictionary learning;robust estimation","Dictionaries;Machine learning;Artificial neural networks;Hyperspectral imaging;Robustness;Tools","belief networks;Gaussian noise;hyperspectral imaging;image classification;image denoising;learning (artificial intelligence);neural nets","discriminative robust deep dictionary learning;hyperspectral image classification;linear classifier;Gaussian noise;sparse noise;Euclidean norm;deep belief network;stacked autoencoder;convolutional neural network","","19","50","","","","","IEEE","IEEE Journals"
"Simultaneous Feature and Dictionary Learning for Image Set Based Face Recognition","J. Lu; G. Wang; J. Zhou","Department of Automation, State Key Laboratory of Intelligent Technologies and Systems, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Alibaba AI Labs, Hangzhou, China; Department of Automation, State Key Laboratory of Intelligent Technologies and Systems, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","","2017","26","8","4042","4054","In this paper, we propose a simultaneous feature and dictionary learning (SFDL) method for image set-based face recognition, where each training and testing example contains a set of face images, which were captured from different variations of pose, illumination, expression, resolution, and motion. While a variety of feature learning and dictionary learning methods have been proposed in recent years and some of them have been successfully applied to image set-based face recognition, most of them learn features and dictionaries for facial image sets individually, which may not be powerful enough because some discriminative information for dictionary learning may be compromised in the feature learning stage if they are applied sequentially, and vice versa. To address this, we propose a SFDL method to learn discriminative features and dictionaries simultaneously from raw face pixels so that discriminative information from facial image sets can be jointly exploited by a one-stage learning procedure. To better exploit the nonlinearity of face samples from different image sets, we propose a deep SFDL (D-SFDL) method by jointly learning hierarchical non-linear transformations and class-specific dictionaries to further improve the recognition performance. Extensive experimental results on five widely used face data sets clearly shows that our SFDL and D-SFDL achieve very competitive or even better performance with the state-of-the-arts.","","","10.1109/TIP.2017.2713940","National Natural Science Foundation of China; National 1000 Young Talents Plan Program; National Basic Research Program of China; Ministry of Education of China; Tsinghua University Initiative Scientific Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7944673","Face recognition;feature learning;dictionary learning;deep learning;image set classification","Face;Dictionaries;Face recognition;Learning systems;Training;Feature extraction;Testing","face recognition;image resolution;image sampling;learning (artificial intelligence);transforms","deep simultaneous feature and dictionary learning method;image set based face recognition;image resolution;facial image set;one-stage learning procedure;D-SFDL method;learning hierarchical nonlinear transformation;class-specific dictionary","Algorithms;Biometric Identification;Databases, Factual;Face;Humans;Image Processing, Computer-Assisted;Machine Learning","29","77","","","","","IEEE","IEEE Journals"
"Single Image Super-Resolution Based on Deep Learning Features and Dictionary Model","L. Zhao; Q. Sun; Z. Zhang","School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing, China; School of Information and Control, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Access","","2017","5","","17126","17135","In traditional single image super-resolution (SR) methods based on dictionary model, a large number of image features are needed to train the SR dictionary. In general, these features are extracted by artificial rules, such as pixel gray, gradient, and texture structure. But, the dictionary model trained by these artificial features or their combinations has exhibited poor expression especially for the images with complex and rich structures. Therefore, how to improve the dictionary expression ability and make the dictionary have more accurate description of the image features is a problem worthy of further study. In this paper, based on the advantage of dictionary training and deep learning, a new method of single image SR based on deep learning features and dictionary model is proposed. The new algorithm contains three steps. First, the features of high-resolution and low-resolution training images are extracted by a Kernel deep learning network. Second, in the sparse representation of SR framework, the dictionary model is trained by these deep learning features. Finally, an LR image SR is completed. Theoretical analysis show that the dictionary trained by deep learning features can improve in the ability to express image complex structure and texture, and it has more advantage than traditional artificial features dictionary. The experimental results indicate that the proposed algorithm can produce good SR visual results than the comparison algorithm, such as Bicubic, sparse coding super-resolution, and super-resolution convolutional neural network. And the peak signal to noise ratio and structural similarity index measurement are improved, the Computation Time is also reasonable.","","","10.1109/ACCESS.2017.2736058","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8002557","Kernel function;deep learning features;dictionary model;single image super-resolution;PCA","Dictionaries;Feature extraction;Machine learning;Training;Image resolution;Image reconstruction;Kernel","feature extraction;image reconstruction;image representation;image resolution;learning (artificial intelligence);neural nets","deep learning features;image features;SR dictionary;dictionary expression ability;dictionary training;single image SR;low-resolution training images;Kernel deep learning network;LR image SR;image complex structure;gradient structure;single image super-resolution methods;feature extraction;texture structure;pixel gray structure;high-resolution training images;artificial feature dictionary;peak signal to noise ratio;structural similarity index measurement","","9","40","CCBY","","","","IEEE","IEEE Journals"
"Automated Analysis of Unregistered Multi-View Mammograms With Deep Learning","G. Carneiro; J. Nascimento; A. P. Bradley","Australian Centre for Visual Technologies, University of Adelaide, Adelaide, SA, Australia; Institute for Systems and Robotics, Instituto Superior Técnico, Lisbon, Portugal; School of Information Technology and Electrical Engineering, University of Queensland, Brisbane, QLD, Australia","IEEE Transactions on Medical Imaging","","2017","36","11","2355","2365","We describe an automated methodology for the analysis of unregistered cranio-caudal (CC) and medio-lateral oblique (MLO) mammography views in order to estimate the patient's risk of developing breast cancer. The main innovation behind this methodology lies in the use of deep learning models for the problem of jointly classifying unregistered mammogram views and respective segmentation maps of breast lesions (i.e., masses and micro-calcifications). This is a holistic methodology that can classify a whole mammographic exam, containing the CC and MLO views and the segmentation maps, as opposed to the classification of individual lesions, which is the dominant approach in the field. We also demonstrate that the proposed system is capable of using the segmentation maps generated by automated mass and micro-calcification detection systems, and still producing accurate results. The semi-automated approach (using manually defined mass and micro-calcification segmentation maps) is tested on two publicly available data sets (INbreast and DDSM), and results show that the volume under ROC surface (VUS) for a 3-class problem (normal tissue, benign, and malignant) is over 0.9, the area under ROC curve (AUC) for the 2-class “benign versus malignant” problem is over 0.9, and for the 2-class breast screening problem (malignancy versus normal/benign) is also over 0.9. For the fully automated approach, the VUS results on INbreast is over 0.7, and the AUC for the 2-class “benign versus malignant” problem is over 0.78, and the AUC for the 2-class breast screening is 0.86.","","","10.1109/TMI.2017.2751523","Australian Research Council’s Discovery Projects; Australian Research Council Future Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8032490","Deep learning;mammogram;multi-view classification;transfer learning","Lesions;Machine learning;Training;Mammography;Cancer;Breast","cancer;image classification;learning (artificial intelligence);mammography;medical image processing","lesions classification;micro-calcification detection systems;INbreast dataset;DDSM dataset;3-class problem;area under ROC curve;breast lesions;segmentation maps;breast cancer;CC mammography;MLO mammography;unregistered medio-lateral oblique mammography;unregistered cranio-caudal mammography;deep learning;unregistered multi-view mammograms automated analysis","Breast;Breast Neoplasms;Databases, Factual;Female;Humans;Imaging, Three-Dimensional;Machine Learning;Mammography;ROC Curve;Radiographic Image Interpretation, Computer-Assisted","9","83","Traditional","","","","IEEE","IEEE Journals"
"A Deep Learning Approach for Intrusion Detection Using Recurrent Neural Networks","C. Yin; Y. Zhu; J. Fei; X. He","State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China","IEEE Access","","2017","5","","21954","21961","Intrusion detection plays an important role in ensuring information security, and the key technology is to accurately identify various attacks in the network. In this paper, we explore how to model an intrusion detection system based on deep learning, and we propose a deep learning approach for intrusion detection using recurrent neural networks (RNN-IDS). Moreover, we study the performance of the model in binary classification and multiclass classification, and the number of neurons and different learning rate impacts on the performance of the proposed model. We compare it with those of J48, artificial neural network, random forest, support vector machine, and other machine learning methods proposed by previous researchers on the benchmark data set. The experimental results show that RNN-IDS is very suitable for modeling a classification model with high accuracy and that its performance is superior to that of traditional machine learning classification methods in both binary and multiclass classification. The RNN-IDS model improves the accuracy of the intrusion detection and provides a new research method for intrusion detection.","","","10.1109/ACCESS.2017.2762418","National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8066291","Recurrent neural networks;RNN-IDS;intrusion detection;deep learning;machine learning","Intrusion detection;Machine learning;Recurrent neural networks;Training;Computational modeling;Testing;Support vector machines","learning (artificial intelligence);pattern classification;recurrent neural nets;security of data;support vector machines","machine learning classification;information security;intrusion detection system;deep learning approach;RNN-IDS model;artificial neural network;multiclass classification;recurrent neural networks","","69","28","","","","","IEEE","IEEE Journals"
"Deep Multimetric Learning for Shape-Based 3D Model Retrieval","J. Xie; G. Dai; Y. Fang","New York University Multimedia and Vision Computing Laboratory, Department of Electrical and Computer Engineering, New York University Abu Dhabi, Abu Dhabi, UAE; New York University Multimedia and Vision Computing Laboratory, Department of Electrical and Computer Engineering, New York University Abu Dhabi, Abu Dhabi, UAE; New York University Multimedia and Vision Computing Laboratory, Department of Electrical and Computer Engineering, New York University Abu Dhabi, Abu Dhabi, UAE","IEEE Transactions on Multimedia","","2017","19","11","2463","2474","Recently, feature-learning-based 3D shape retrieval methods have been receiving more and more attention in the 3D shape analysis community. In these methods, the hand-crafted metrics or the learned linear metrics are usually used to compute the distances between shape features. Since there are complex geometric structural variations with 3D shapes, the single hand-crafted metric or learned linear metric cannot characterize the manifold, where 3D shapes lie well. In this paper, by exploring the nonlinearity of the deep neural network and the complementarity among multiple shape features, we propose a novel deep multimetric network for 3D shape retrieval. The developed multimetric network minimizes a discriminative loss function that, for each type of shape feature, the outputs of the network from the same class are encouraged to be as similar as possible and the outputs from different classes are encouraged to be as dissimilar as possible. Meanwhile, the Hilbert-Schmidt independence criterion is employed to enforce the outputs of different types of shape features to be as complementary as possible. Furthermore, the weights of the learned multiple distance metrics can be adaptively determined in our developed deep metric network. The weighted distance metric is then used as the similarity for shape retrieval. We conduct experiments with the proposed method on the four benchmark shape datasets. Experimental results demonstrate that the proposed method can obtain better performance than the learned deep single metric and outperform the state-of-the-art 3D shape retrieval methods.","","","10.1109/TMM.2017.2698200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7912387","3D shape retrieval;3D shape descriptor;deep neural network;multiple shape features;metric learning","Shape;Three-dimensional displays;Measurement;Feature extraction;Manifolds;Solid modeling;Neural networks","image retrieval;learning (artificial intelligence);neural nets","multimetric learning;3D model retrieval;feature-learning;shape retrieval;learned linear metrics;deep neural network;multiple shape features;deep multimetric network;benchmark shape datasets;multiple distance metrics;deep metric network;3D shape analysis community;hand-crafted metrics;Hilbert-Schmidt independence criterion;weighted distance metric;complex geometric structural variations","","4","52","Traditional","","","","IEEE","IEEE Journals"
"Deep Coupled Metric Learning for Cross-Modal Matching","V. E. Liong; J. Lu; Y. Tan; J. Zhou","Interdisciplinary Graduate School, Rapid-Rich Object Search Laboratory, Nanyang Technological University, Singapore; Department of Automation, State Key Laboratory of Intelligent Technologies and Systems, Tsinghua University, Beijing 100084, China, and the Tsinghua National Laboratory for Information Science and Technology, Beijing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Automation, State Key Laboratory of Intelligent Technologies and Systems, Tsinghua University, Beijing 100084, China, and the Tsinghua National Laboratory for Information Science and Technology, Beijing, China","IEEE Transactions on Multimedia","","2017","19","6","1234","1244","In this paper, we propose a new deep coupled metric learning (DCML) method for cross-modal matching, which aims to match samples captured from two different modalities (e.g., texts versus images, visible versus near infrared images). Unlike existing cross-modal matching methods which learn a linear common space to reduce the modality gap, our DCML designs two feedforward neural networks which learn two sets of hierarchical nonlinear transformations (one set for each modality) to nonlinearly map samples from different modalities into a shared latent feature subspace, under which the intraclass variation is minimized and the interclass variation is maximized, and the difference of each data pair captured from two modalities of the same class is minimized, respectively. Experimental results on four different cross-modal matching datasets validate the efficacy of the proposed approach.","","","10.1109/TMM.2016.2646180","National Key Research and Development Program of China; National Natural Science Foundation of China; National 1000 Young Talents Plan Program; National Basic Research Program of China; Ministry of Education of China; Tsinghua University Initiative Scientific Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801952","Coupled learning;cross-modal matching;deep model;metric learning;multimedia retrieval","Measurement;Machine learning;Correlation;Semantics;Neural networks;Kernel;Learning systems","feedforward neural nets;image matching;learning (artificial intelligence)","deep coupled metric learning;DCML method;cross-modal matching methods;modality gap;feedforward neural networks;nonlinear transformations;shared latent feature subspace;interclass variation","","30","59","","","","","IEEE","IEEE Journals"
"Unsupervised Feature Learning Based on Deep Models for Environmental Audio Tagging","Y. Xu; Q. Huang; W. Wang; P. Foster; S. Sigtia; P. J. B. Jackson; M. D. Plumbley","Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","6","1230","1241","Environmental audio tagging aims to predict only the presence or absence of certain acoustic events in the interested acoustic scene. In this paper, we make contributions to audio tagging in two parts, respectively, acoustic modeling and feature learning. We propose to use a shrinking deep neural network (DNN) framework incorporating unsupervised feature learning to handle the multilabel classification task. For the acoustic modeling, a large set of contextual frames of the chunk are fed into the DNN to perform a multilabel classification for the expected tags, considering that only chunk (or utterance) level rather than frame-level labels are available. Dropout and background noise aware training are also adopted to improve the generalization capability of the DNNs. For the unsupervised feature learning, we propose to use a symmetric or asymmetric deep denoising auto-encoder (syDAE or asyDAE) to generate new data-driven features from the logarithmic Mel-filter banks features. The new features, which are smoothed against background noise and more compact with contextual information, can further improve the performance of the DNN baseline. Compared with the standard Gaussian mixture model baseline of the DCASE 2016 audio tagging challenge, our proposed method obtains a significant equal error rate (EER) reduction from 0.21 to 0.13 on the development set. The proposed asyDAE system can get a relative 6.7% EER reduction compared with the strong DNN baseline on the development set. Finally, the results also show that our approach obtains the state-of-the-art performance with 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while EER of the first prize of this challenge is 0.17.","","","10.1109/TASLP.2017.2690563","Engineering and Physical Sciences Research Council (EPSRC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7933054","DCASE 2016;deep neural networks;deep de-noising auto-encoder;environmental audio tagging;unsupervised feature learning","Tagging;Noise measurement;Acoustics;Training;Machine learning;Speech;Audio recording","audio signals;Gaussian processes;mixture models;neural nets","unsupervised feature learning;deep model;environmental audio tagging;acoustic events;acoustic scene;acoustic modeling;shrinking deep neural network;DNN framework;multilabel classification task;frame-level label;background noise-aware training;dropout aware training;asymmetric deep denoising autoencoder;symmetric deep denoising auto-encoder;standard Gaussian mixture model baseline;DCASE 2016 audio tagging challenge;equal error rate reduction","","10","49","CCBY","","","","IEEE","IEEE Journals"
"A Deep-Learning-Based Forecasting Ensemble to Predict Missing Data for Remote Sensing Analysis","M. Das; S. K. Ghosh","Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","12","5228","5236","The problem of missing data in remote sensing analysis is manifold. The situation becomes more serious during multitemporal analysis when data at various a-periodic timestamps are missing. In this work, we have proposed a deep-learning-based framework (Deep-STEP_FE) for reconstructing the missing data to facilitate analysis with remote sensing time series. The idea is to utilize the available data from both earlier and subsequent timestamps, while maintaining the causality constraint in spatiotemporal analysis. The framework is based on an ensemble of multiple forecasting modules, built upon the observed data in the time-series sequence. The coupling between the forecasting modules is accomplished with the help of dummy data, initially predicted using the earlier part of the sequence. Then, the dummy data are progressively improved in an iterative manner so that it can best conform to the next part of the sequence. Each of the forecasting modules in the ensemble is based on Deep-STEP, a variant of the deep stacking network learning approach. The work has been validated using a case study on predicting the missing images in normalized difference vegetation index time series, derived from Landsat-7 TM-5 satellite imagery over two spatial zones in India. Comparative performance analysis demonstrates the effectiveness of the proposed forecasting ensemble.","","","10.1109/JSTARS.2017.2760202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170480","Causality constraint;deep learning;ensemble;missing data;prediction;remote sensing","Predictive models;TIme series analysis;Image reconstruction;Spatiotemporal phenomena;Machine learning","geophysical image processing;image classification;time series;vegetation mapping","multitemporal analysis;spatiotemporal analysis;deep stacking network learning approach;normalized difference vegetation index time series;comparative performance analysis;deep-learning-based forecasting ensemble;deep-STEP_FE analysis;a-periodic time stamp;multiple forecasting module;observed missing image data;remote sensing time series analysis;deep-learning time series stamp","","3","19","Traditional","","","","IEEE","IEEE Journals"
"BASS Net: Band-Adaptive Spectral-Spatial Feature Learning Neural Network for Hyperspectral Image Classification","A. Santara; K. Mani; P. Hatwar; A. Singh; A. Garg; K. Padia; P. Mitra","Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India; Department of Geology and Geophysics, IIT Kharagpur, Kharagpur, India; Department of Electrical Engineering, IIT Kharagpur, Kharagpur, India; Department of Electrical Engineering, IIT Kharagpur, Kharagpur, India; Space Applications Centre, Indian Space Research Organization, Ahmedabad, India; Space Applications Centre, Indian Space Research Organization, Ahmedabad, India; Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","9","5293","5301","Deep learning based land cover classification algorithms have recently been proposed in the literature. In hyperspectral images (HSIs), they face the challenges of large dimensionality, spatial variability of spectral signatures, and scarcity of labeled data. In this paper, we propose an end-to-end deep learning architecture that extracts band specific spectral-spatial features and performs land cover classification. The architecture has fewer independent connection weights and thus requires fewer training samples. The method is found to outperform the highest reported accuracies on popular HSI data sets.","","","10.1109/TGRS.2017.2705073","Space Applications Centre, Indian Space Research Organization through the Project titled Deep Learning for Automated Feature Discovery in Hyperspectral Images (LDH); Google India under the Google India Ph.D. Fellowship Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7938656","Convolutional neural network (CNN);deep learning;feature extraction;hyperspectral imagery;landcover classification;pattern classification","Neural networks;Feature extraction;Machine learning;Training;Hyperspectral imaging;Support vector machines","feature extraction;geophysical image processing;hyperspectral imaging;image classification;land cover;learning (artificial intelligence);neural net architecture","band-adaptive spectral-spatial feature learning neural network;hyperspectral image classification;deep learning based land cover classification algorithms;spatial variability;spectral signatures;end-to-end deep learning architecture;band specific spectral-spatial features;HSI data sets","","22","46","","","","","IEEE","IEEE Journals"
"Capturing High-Discriminative Fault Features for Electronics-Rich Analog System via Deep Learning","Z. Liu; Z. Jia; C. Vong; S. Bu; J. Han; X. Tang","Northwestern Polytechnical University, Xi’an, China; Northwestern Polytechnical University, Xi’an, China; Department of Computer and Information Science, University of Macau, Macau, China; Northwestern Polytechnical University, Xi’an, China; Northwestern Polytechnical University, Xi’an, China; Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Industrial Informatics","","2017","13","3","1213","1226","Fault detection and isolation (FDI) is very difficult for electronics-rich analog systems due to its sophisticated mechanism and variable operational conditions. Traditionally, FDI in such systems is done through the monitoring of deviation of output signals in voltage or current at system level, which commonly arises from the degradation of one or more critical components. Therefore, FDI can be transformed to a multiclass classification task given the extracted features of the output signals in voltage or current of the circuit. Traditional feature extraction on the circuit output is mostly based on time-domain, frequency-domain, or time-frequency signal processing, which collapse high-dimensional raw signals into a lower dimensional feature set. Such low-dimensional feature set usually suffers from information loss so as to affect the accuracy of the later fault diagnosis. In order to retain as much information as possible, deep learning is proposed which employs a hierarchical structure to capture the different levels of semantic representations of the signals. In this paper, a novel fault diagnostic application of Gaussian-Bernoulli deep belief network (GB-DBN) for electronics-rich analog systems is developed which can more effectively capture the high-order semantic features within the raw output signals. The novel fault diagnosis is validated experimentally on two typical analog filter circuits. Experimental results show the fault diagnosis based on GB-DBN is with superior diagnostic performance than the traditional feature extraction methods.","","","10.1109/TII.2017.2690940","Natural Science Foundation of China; NWPU Basic Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891896","Analog circuits;deep belief network;deep learning;diagnosis;failure;fault;restricted Boltzmann machines","Feature extraction;Circuit faults;Machine learning;Fault diagnosis;Time-frequency analysis;Analog circuits","analogue circuits;belief networks;fault diagnosis;feature extraction;Gaussian processes;learning (artificial intelligence);signal classification;signal representation;time-frequency analysis","high-discriminative fault feature extraction;electronics-rich analog system;deep learning;fault detection and isolation;FDI;multiclass classification task;time-domain signal processing;frequency-domain signal processing;time-frequency signal processing;high-dimensional raw signals;low-dimensional feature set;fault diagnosis;signal semantic representations;Gaussian-Bernoulli deep belief network;GB-DBN;high-order semantic features;analog filter circuits","","29","52","","","","","IEEE","IEEE Journals"
"On Deep Learning for Trust-Aware Recommendations in Social Networks","S. Deng; L. Huang; G. Xu; X. Wu; Z. Wu","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Advanced Analytics Institute, University of Technology at Sydney, Sydney, NSW, Australia; Department of Computer Science, The University of Vermont, Burlington, VT, USA; College of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","5","1164","1177","With the emergence of online social networks, the social network-based recommendation approach is popularly used. The major benefit of this approach is the ability of dealing with the problems with cold-start users. In addition to social networks, user trust information also plays an important role to obtain reliable recommendations. Although matrix factorization (MF) becomes dominant in recommender systems, the recommendation largely relies on the initialization of the user and item latent feature vectors. Aiming at addressing these challenges, we develop a novel trust-based approach for recommendation in social networks. In particular, we attempt to leverage deep learning to determinate the initialization in MF for trust-aware social recommendations and to differentiate the community effect in user's trusted friendships. A two-phase recommendation process is proposed to utilize deep learning in initialization and to synthesize the users' interests and their trusted friends' interests together with the impact of community effect for recommendations. We perform extensive experiments on real-world social network data to demonstrate the accuracy and effectiveness of our proposed approach in comparison with other state-of-the-art methods.","","","10.1109/TNNLS.2016.2514368","National Natural Science Foundation of China; National Key Technology Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7414528","Deep learning;recommender systems (RSs);social network;trust","Social network services;Motion pictures;Machine learning;Optimization;Reliability;Recommender systems;Computer science","learning (artificial intelligence);recommender systems;social networking (online)","deep learning;online social network-based recommendation approach;cold-start users;user trust information;user feature vector initialization;item latent feature vector initialization;MF;trust-aware social recommendations;community effect;user trusted friend interest synthesis;two-phase recommendation process;user trusted friendships","","66","40","","","","","IEEE","IEEE Journals"
"Place Classification With a Graph Regularized Deep Neural Network","Y. Liao; S. Kodagoda; Y. Wang; L. Shi; Y. Liu","State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Centre for Autonomous Systems, University of Technology Sydney, Ultimo, NSW, Australia; State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China; Centre for Autonomous Systems, University of Technology Sydney, Ultimo, NSW, Australia; State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China","IEEE Transactions on Cognitive and Developmental Systems","","2017","9","4","304","315","Place classification is a fundamental ability that a robot should possess to carry out effective human-robot interactions. In recent years, there is a high exploitation of artificial intelligence algorithms in robotics applications. Inspired by the recent successes of deep learning methods, we propose an end-to-end learning approach for the place classification problem. With deep architectures, this methodology automatically discovers features and contributes in general to higher classification accuracies. The pipeline of our approach is composed of three parts. First, we construct multiple layers of laser range data to represent the environment information in different levels of granularity. Second, each layer of data are fed into a deep neural network for classification, where a graph regularization is imposed to the deep architecture for keeping local consistency between adjacent samples. Finally, the predicted labels obtained from all layers are fused based on confidence trees to maximize the overall confidence. Experimental results validate the effectiveness of our end-to-end place classification framework in which both the multilayer structure and the graph regularization promote the classification performance. Furthermore, results show that the features automatically learned from the raw input range data can achieve competitive results to the features constructed based on statistical and geometrical information.","","","10.1109/TCDS.2016.2586183","National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; Joint Centre for Robotics Research between Zhejiang University and the University of Technology, Sydney; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7501830","Deep learning;graph regularization;place classification","Feature extraction;Lasers;Robot sensing systems;Machine learning;Robot kinematics;Classification algorithms;Machine learning","feature extraction;graph theory;human-robot interaction;learning (artificial intelligence);neural nets","deep neural network;fundamental ability;human-robot interactions;artificial intelligence algorithms;robotics applications;deep learning methods;end-to-end learning approach;place classification problem;deep architecture;higher classification accuracies;multiple layers;laser range data;graph regularization;end-to-end place classification framework","","4","41","","","","","IEEE","IEEE Journals"
"Real-Time Detection of False Data Injection Attacks in Smart Grid: A Deep Learning-Based Intelligent Mechanism","Y. He; G. J. Mendis; J. Wei","Department of Electrical and Computer Engineering, University of Akron, Akron, OH, USA; Department of Electrical and Computer Engineering, University of Akron, Akron, OH, USA; Department of Electrical and Computer Engineering, University of Akron, Akron, OH, USA","IEEE Transactions on Smart Grid","","2017","8","5","2505","2516","Application of computing and communications intelligence effectively improves the quality of monitoring and control of smart grids. However, the dependence on information technology also increases vulnerability to malicious attacks. False data injection (FDI), that attack on the integrity of data, is emerging as a severe threat to the supervisory control and data acquisition system. In this paper, we exploit deep learning techniques to recognize the behavior features of FDI attacks with the historical measurement data and employ the captured features to detect the FDI attacks in real-time. By doing so, our proposed detection mechanism effectively relaxes the assumptions on the potential attack scenarios and achieves high accuracy. Furthermore, we propose an optimization model to characterize the behavior of one type of FDI attack that compromises the limited number of state measurements of the power system for electricity theft. We illustrate the performance of the proposed strategy through the simulation by using IEEE 118-bus test system. We also evaluate the scalability of our proposed detection mechanism by using IEEE 300-bus test system.","","","10.1109/TSG.2017.2703842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7926429","False data injection (FDI) attacks;deep learning;state vector estimator (SVE);deep learning based identification (DLBI) scheme;supervisory control and data acquisition (SCADA)","Real-time systems;Machine learning;Feature extraction;Power systems;SCADA systems;Power measurement;Data models","data integrity;learning (artificial intelligence);optimisation;power engineering computing;power system security;SCADA systems;security of data;smart power grids","false data injection attacks real-time detection;smart grid;deep learning-based intelligent mechanism;computing and communications intelligence application;smart grid control;smart grid quality monitoring;information technology;malicious attacks;data integrity;supervisory control and data acquisition system;historical measurement data;FDI attacks;optimization model;power system state measurement;electricity theft;IEEE 118-bus test system;IEEE 300-bus test system","","57","42","","","","","IEEE","IEEE Journals"
"Deep Learning and Insomnia: Assisting Clinicians With Their Diagnosis","M. Shahin; B. Ahmed; S. T. Hamida; F. L. Mulaffer; M. Glos; T. Penzel","Electrical and Computer Engineering Program, Texas A&M University at Qatar, Doha, Qatar; Electrical and Computer Engineering Program, Texas A&M University at Qatar, Doha, Qatar; Electrical and Computer Engineering Program, Texas A&M University at Qatar, Doha, Qatar; Electrical and Computer Engineering Program, Texas A&M University at Qatar, Doha, Qatar; Interdisciplinary Centre for Sleep Medicine, Charité University Hospital, Berlin, Germany; Interdisciplinary Centre for Sleep Medicine, Charité University Hospital, Berlin, Germany","IEEE Journal of Biomedical and Health Informatics","","2017","21","6","1546","1553","Effective sleep analysis is hampered by the lack of automated tools catering to disordered sleep patterns and cumbersome monitoring hardware. In this paper, we apply deep learning on a set of 57 EEC features extracted from a maximum of two EEC channels to accurately differentiate between patients with insomnia or controls with no sleep complaints. We investigated two different approaches to achieve this. The first approach used EEC data from the whole sleep recording irrespective of the sleep stage (stage-independent classification), while the second used only EEC data from insomnia-impacted specific sleep stages (stage-dependent classification). We trained and tested our system using both healthy and disordered sleep collected from 41 controls and 42 primary insomnia patients. When compared with manual assessments, an NREM + REM based classifier had an overall discrimination accuracy of 92% and 86% between two groups using both two and one EEC channels, respectively. These results demonstrate that deep learning can be used to assist in the diagnosis of sleep disorders such as insomnia.","","","10.1109/JBHI.2017.2650199","NPRP; Qatar National Research Fund; Qatar Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811237","Automatic sleep stage scoring;deep learning;electroencephalogram (EEG);insomnia;sleep analysis","Sleep;Electroencephalography;Machine learning;Feature extraction;Monitoring;Electromyography;Electrooculography","electroencephalography;feature extraction;learning (artificial intelligence);medical disorders;medical signal processing;pattern classification;sleep","deep learning;sleep analysis;disordered sleep patterns;EEC feature extraction;EEC channels;EEC data;sleep recording;insomnia-impacted specific sleep stages;NREM-REM based classifier;sleep disorder diagnosis","Adult;Diagnosis, Computer-Assisted;Electroencephalography;Female;Humans;Machine Learning;Male;Middle Aged;Polysomnography;Signal Processing, Computer-Assisted;Sleep Initiation and Maintenance Disorders;Sleep Stages","4","37","Traditional","","","","IEEE","IEEE Journals"
"Modulation Format Recognition and OSNR Estimation Using CNN-Based Deep Learning","D. Wang; M. Zhang; Z. Li; J. Li; M. Fu; Y. Cui; X. Chen","State Key Laboratory of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Photonics Technology Letters","","2017","29","19","1667","1670","An intelligent eye-diagram analyzer is proposed to implement both modulation format recognition (MFR) and optical signal-to-noise rate (OSNR) estimation by using a convolution neural network (CNN)-based deep learning technique. With the ability of feature extraction and self-learning, CNN can process eye diagram in its raw form (pixel values of an image) from the perspective of image processing, without knowing other eye-diagram parameters or original bit information. The eye diagram images of four commonly-used modulation formats over a wide OSNR range (10~25 dB) are obtained from an eye-diagram generation module in oscilloscope combined with the simulation system. Compared with four other machine learning algorithms (decision tress, k-nearest neighbors, back-propagation artificial neural network, and support vector machine), CNN obtains the higher accuracies. The accuracies of OSNR estimation and MFR both attain 100%. The proposed technique has the potential to be embedded in the test instrument to perform intelligent signal analysis or applied for optical performance monitoring.","","","10.1109/LPT.2017.2742553","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8013796","Machine learning;deep learning;convolution neural network (CNN);eye diagram;optical performance monitoring (OPM);optical signal-to-noise rate (OSNR);modulation format recognition (MFR)","Optical noise;Signal to noise ratio;Kernel;Feature extraction;Convolution;Modulation;Optical imaging","feature extraction;feedforward neural nets;image processing;learning (artificial intelligence);optical modulation;optical noise","modulation format recognition;OSNR estimation;CNN-based deep learning;intelligent eye-diagram analyzer;optical signal-to-noise rate estimation;convolution neural network-based deep learning technique;feature extraction;self-learning;pixel values;image processing;bit information;oscilloscope","","30","17","OAPA","","","","IEEE","IEEE Journals"
"Resolution of Singularities Introduced by Hierarchical Structure in Deep Neural Networks","T. Nitta","National Institute of Advanced Industrial Science and Technology (AIST), Human Informatics Research Institute, Tsukuba, Japan","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","10","2282","2293","We present a theoretical analysis of singular points of artificial deep neural networks, resulting in providing deep neural network models having no critical points introduced by a hierarchical structure. It is considered that such deep neural network models have good nature for gradient-based optimization. First, we show that there exist a large number of critical points introduced by a hierarchical structure in deep neural networks as straight lines, depending on the number of hidden layers and the number of hidden neurons. Second, we derive a sufficient condition for deep neural networks having no critical points introduced by a hierarchical structure, which can be applied to general deep neural networks. It is also shown that the existence of critical points introduced by a hierarchical structure is determined by the rank and the regularity of weight matrices for a specific class of deep neural networks. Finally, two kinds of implementation methods of the sufficient conditions to have no critical points are provided. One is a learning algorithm that can avoid critical points introduced by the hierarchical structure during learning (called avoidant learning algorithm). The other is a neural network that does not have some critical points introduced by the hierarchical structure as an inherent property (called avoidant neural network).","","","10.1109/TNNLS.2016.2580741","Japan Society for the Promotion of Science through the Grants-in-Aid for Scientific Research (KAKENHI); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502092","Critical point;deep learning;neural networks;singular point","Biological neural networks;Neurons;Training;Periodic structures;Manganese;Learning systems;Optimization","gradient methods;learning (artificial intelligence);matrix algebra;neural nets;optimisation","singularities resolution;hierarchical structure;artificial deep neural networks;singular points;gradient-based optimization;straight lines;number-of-hidden layers;number-of-hidden neurons;general deep neural networks;weight matrices regularity;avoidant learning algorithm;avoidant neural network","","3","18","","","","","IEEE","IEEE Journals"
"Tree Classification in Complex Forest Point Clouds Based on Deep Learning","X. Zou; M. Cheng; C. Wang; Y. Xia; J. Li","Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Information Science and Engineering, Xiamen University, Xiamen, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","12","2360","2364","Recently, the classification of tree species using 3-D point clouds has drawn wide attention in surveys and forestry investigations. This letter proposes a new voxel-based deep learning method to classify tree species in 3-D point clouds collected from complex forest scenes. The proposed method includes three steps: 1) individual tree extraction based on the density of the point clouds; 2) low-level feature representation through voxel-based rasterization; and 3) classification of tree species by a deep learning model. Two data sets of 3-D forest point clouds acquired by terrestrial laser scanning systems are used to evaluate the proposed method. The method achieves an average classification accuracy of 93.1% and 95.6% on the two data sets. Furthermore, in comparative experiments, the proposed method exhibits performance superior to that of the other 3-D tree species classification methods.","","","10.1109/LGRS.2017.2764938","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8098645","Deep learning;point clouds;rasterization;terrestrial laser scanning (TLS);tree species classification","Vegetation;Three-dimensional displays;Machine learning;Feature extraction;Training;Forestry;Laser radar","feature extraction;forestry;geophysical image processing;image classification;image representation;learning (artificial intelligence);remote sensing by laser beam;vegetation mapping","low-level feature representation;voxel-based rasterization;deep learning model;deep learning method;tree classification;complex forest point clouds;3D forest point clouds;3D tree species classification methods;terrestrial laser scanning systems;tree extraction;forestry","","2","19","","","","","IEEE","IEEE Journals"
"An Introduction to Deep Learning for the Physical Layer","T. O’Shea; J. Hoydis","Bradley Department of Electrical and Computer Engineering, Virginia Tech and DeepSig, Arlington, VA, USA; Department of Software-Defined Mobile Networks, Nokia Bell Labs, Nozay, France","IEEE Transactions on Cognitive Communications and Networking","","2017","3","4","563","575","We present and discuss several novel applications of deep learning for the physical layer. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about communications system design as an end-to-end reconstruction task that seeks to jointly optimize transmitter and receiver components in a single process. We show how this idea can be extended to networks of multiple transmitters and receivers and present the concept of radio transformer networks as a means to incorporate expert domain knowledge in the machine learning model. Lastly, we demonstrate the application of convolutional neural networks on raw IQ samples for modulation classification which achieves competitive accuracy with respect to traditional schemes relying on expert features. This paper is concluded with a discussion of open challenges and areas for future investigation.","","","10.1109/TCCN.2017.2758370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8054694","Machine learning;deep learning;physical layer;digital communications;modulation;radio communication;cognitive radio","Artificial neural networks;Physical layer;Communication systems;Receivers;Modulation;Radio transmitters;Machine learning","computer networks;convolution;feedforward neural nets;learning (artificial intelligence);open systems;optimisation;receivers","deep learning;physical layer;communications system design;end-to-end reconstruction task;receiver components;multiple transmitters;radio transformer networks;expert domain knowledge;machine learning model;convolutional neural networks;autoencoder;joint optimisation;transmitter components;modulation classification","","216","66","","","","","IEEE","IEEE Journals"
"DeepList: Learning Deep Features With Adaptive Listwise Constraint for Person Reidentification","J. Wang; Z. Wang; C. Gao; N. Sang; R. Huang","National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; School of Computer, Wuhan University, Wuhan, China; National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China","IEEE Transactions on Circuits and Systems for Video Technology","","2017","27","3","513","524","Person reidentification (re-id) aims to match a specific person across nonoverlapping cameras, which is an important but challenging task in video surveillance. Conventional methods mainly focus either on feature constructing or metric learning. Recently, some deep learning-based methods have been proposed to learn image features and similarity measures jointly. However, current deep models for person re-id are usually trained with either pairwise loss, where the number of negative pairs greatly outnumbering that of positive pairs may lead the training model to be biased toward negative pairs or constant margin hinge loss, without considering the fact that hard negative samples should be paid more attention in the training stage. In this paper, we propose to learn deep representations with an adaptive margin listwise loss. First, ranking lists instead of image pairs are used as training samples, in this way, the problem of data imbalance is relaxed. Second, by introducing an adaptive margin parameter in the listwise loss function, it can assign larger margins to harder negative samples, which can be interpreted as an implementation of the automatic hard negative mining strategy. To gain robustness against changes in poses and part occlusions, our architecture combines four convolutional neural networks, each of which embeds images from different scales or different body parts. The final combined model performs much better than each single model. The experimental results show that our approach achieves very promising results on the challenging CUHK03, CUHK01, and VIPeR data sets.","","","10.1109/TCSVT.2016.2586851","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502111","Deep learning;learning to rank;person reidentification (re-id)","Training;Probes;Measurement;Feature extraction;Fasteners;Machine learning;Computer architecture","data mining;image matching;learning (artificial intelligence);neural nets;video surveillance","person reidentification;person matching;nonoverlapping cameras;video surveillance;learning-based methods;image features;similarity measures;pairwise loss;negative pairs;positive pairs;training model;constant margin hinge loss;training stage;deep representation;adaptive margin listwise loss function;ranking lists;training samples;data imbalance;adaptive margin parameter;automatic hard negative mining;neural networks;VIPeR data sets;CUHK01 data sets;CUHK03 data sets","","27","51","","","","","IEEE","IEEE Journals"
"Deep TSK Fuzzy Classifier With Stacked Generalization and Triplely Concise Interpretability Guarantee for Large Data","T. Zhou; F. Chung; S. Wang","School of Digital Media, Jiangnan University, Wuxi, China; Department of Computing, Hong Kong Polytechnic University, Hong Kong; School of Digital Media, Jiangnan University, Wuxi, China","IEEE Transactions on Fuzzy Systems","","2017","25","5","1207","1221","Although Takagi-Sugeno-Kang (TSK) fuzzy classifier has been applied to a wide range of practical scenarios, how to enhance its classification accuracy and interpretability simultaneously is still a challenging task. In this paper, based on the powerful stacked generalization principle, a deep TSK fuzzy classifier (D-TSK-FC) is proposed to achieve the enhanced classification accuracy and triplely concise interpretability for fuzzy rules. D-TSK-FC consists of base-building units. Just like the existing popular deep learning, D-TSK-FC can be built in a layer-by-layer way. In terms of the stacked generalization principle, the training set plus random shifts obtained from random projections of prediction results of current base-building unit are presented as the input of the next base-building unit. The hidden layer in each base-building unit of D-TSK-FC is represented by triplely concise interpretable fuzzy rules in the sense of randomly selected features with the fixed five fuzzy partitions, random rule combinations, and the same input space kept in every base-building unit of D-TSK-FC. The output layer of each base-building unit can be learnt quickly by least learning machine (LLM). Besides, benefiting from LLM, D-TSK-FC's deep learning can be well scaled up for large datasets. Our extensive experimental results witness the power of the proposed deep TSK fuzzy classifier.","","","10.1109/TFUZZ.2016.2604003","Hong Kong Polytechnic University; National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; Jiangsu 333 expert engineering; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7555341","Deep learning Takagi–Sugeno–Kang (TSK);fuzzy classifier;interpretability;large datastacked generalization;least learning machine (LLM)","Fuzzy systems;Machine learning;Classification algorithms;Buildings;Fuzzy sets;Neural networks;Genetic algorithms","fuzzy set theory;fuzzy systems;learning (artificial intelligence);pattern classification","fixed five fuzzy partitions;deep TSK fuzzy classifier;triplely concise interpretability guarantee;Takagi-Sugeno-Kang fuzzy classifier;powerful stacked generalization principle;enhanced classification accuracy;existing popular deep learning;D-TSK-FC;base-building units;random shifts;least learning machine;LLM;triplely concise interpretable fuzzy rules;current base-building unit","","9","47","Traditional","","","","IEEE","IEEE Journals"
"Repeatable Folding Task by Humanoid Robot Worker Using Deep Learning","P. Yang; K. Sasaki; K. Suzuki; K. Kase; S. Sugano; T. Ogata","Department of Modern Mechanical Engineering, Graduate School of Creative Science and Engineering, Waseda University, Tokyo, Japan; Department of Intermedia Art and Science, School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan; Department of Intermedia Art and Science, School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan; Department of Intermedia Art and Science, School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan; Department of Modern Mechanical Engineering, Graduate School of Creative Science and Engineering, Waseda University, Tokyo, Japan; Department of Intermedia Art and Science, School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan","IEEE Robotics and Automation Letters","","2017","2","2","397","403","We propose a practical state-of-the-art method to develop a machine-learning-based humanoid robot that can work as a production line worker. The proposed approach provides an intuitive way to collect data and exhibits the following characteristics: task performing capability, task reiteration ability, generalizability, and easy applicability. The proposed approach utilizes a real-time user interface with a monitor and provides a first-person perspective using a head-mounted display. Through this interface, teleoperation is used for collecting task operating data, especially for tasks that are difficult to be applied with a conventional method. A two-phase deep learning model is also utilized in the proposed approach. A deep convolutional autoencoder extracts images features and reconstructs images, and a fully connected deep time delay neural network learns the dynamics of a robot task process from the extracted image features and motion angle signals. The “Nextage Open” humanoid robot is used as an experimental platform to evaluate the proposed model. The object folding task utilizing with 35 trained and 5 untrained sensory motor sequences for test. Testing the trained model with online generation demonstrates a 77.8% success rate for the object folding task.","","","10.1109/LRA.2016.2633383","AIST; Research Institute for Science and Engineering, Waseda University; MEXT Grant-in-Aid for Scientific Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762066","Humanoid robots;learning and adaptive systems;motion control of manipulators;neurorobotics","Robot sensing systems;Machine learning;Feature extraction;Humanoid robots;Training;Data models","data handling;delays;feature extraction;generalisation (artificial intelligence);humanoid robots;human-robot interaction;image reconstruction;industrial robots;learning (artificial intelligence);neurocontrollers;robot dynamics;robot vision;telerobotics;user interfaces","repeatable folding task;humanoid robot worker;machine-learning-based humanoid robot;production line worker;task performing capability;task reiteration ability;generalizability;user interface;first-person perspective;head-mounted display;teleoperation;task operating data collection;two-phase deep learning;deep convolutional autoencoder;image feature extraction;image reconstruction;deep time delay neural network learning;robot task process dynamics;Nextage Open humanoid robot;sensory motor sequences","","39","21","","","","","IEEE","IEEE Journals"
"Learning deep discriminative features based on cosine loss function","J. Wang; Y. Li; Z. Miao; Y. Xu; G. Tao","PLA University of Science and Technology, People's Republic of China; PLA University of Science and Technology, People's Republic of China; PLA University of Science and Technology, People's Republic of China; PLA University of Science and Technology, People's Republic of China; Anhui Keli Inform. Industry Co. Ltd., Key Lab of Urban ITS Tech. Optimization and Integration, People's Republic of China","Electronics Letters","","2017","53","14","918","920","Deep feature representation is widely used in various visual applications. However, the feature extracted by the convolutional neural networks (CNNs) is inappropriate for cosine similarity measurement. Because the classical CNNs are designed for classification rather than for similarity comparison. A novel cosine loss function for learning deep discriminative features, which are fit to the cosine similarity measurement, is designed. The loss can constrain the distribution of the features in the same class to be in a narrow angle region. Furthermore, a discriminative feature learning network framework and its corresponding two-stage learning method to learn the parameters is proposed. Experimental results show that the proposed method achieves state-of-the-art performance on the public Cifar10 and Market1501 datasets.","","","10.1049/el.2017.0523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973266","","","convolution;data analysis;learning (artificial intelligence);neural nets","Market1501 datasets;public Cifar10 datasets;two-stage learning method;discriminative feature learning network framework;cosine similarity measurement;CNN;convolutional neural networks;visual applications;deep feature representation;cosine loss function;deep discriminative feature learning","","1","14","","","","","IET","IET Journals"
"Transfer Learning With Fully Pretrained Deep Convolution Networks for Land-Use Classification","B. Zhao; B. Huang; Y. Zhong","Department of Geography and Resource Management, The Chinese University of Hong Kong, Hong Kong; Department of Geography and Resource Management, The Chinese University of Hong Kong, Hong Kong; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","9","1436","1440","In recent years, transfer learning with pretrained convolutional networks (CNets) has been successfully applied to land-use classification with high spatial resolution (HSR) imagery. The commonly used transfer CNets partially use the feature descriptor part of the pretained CNets, and replace the classifier part of the pretrained CNets in the old task with a new one. This causes the separation and asynchrony between the feature descriptor part and the classifier part of the transferred CNets during the learning process, which reduces the effectiveness of the training process. To overcome this weakness, a transfer learning method with fully pretrained CNets is proposed in this letter for the land-use classification of HSR images. In the proposed method, a multilayer perceptron (MLP) classifier is quickly pretrained using the high-level features extracted by the feature descriptor of the pretrained CNets. Fully pretrained CNets can be generated by concatenating the feature descriptor of the pretrained CNets and the pretained MLP. Because both the feature descriptor and the classifier are pretrained, the separation and asynchrony between the two parts can be avoided during the training process. The final transferred CNets are then obtained by fine-tuning the fully pretrained CNets with the random cropping and mirroring strategy. The experiments show that the proposed method can accelerate the convergence of the training process with no loss of accuracy in land-use classification, and its performance is comparable to other latest methods.","","","10.1109/LGRS.2017.2691013","National Science Foundation of China; Hong Kong Research Grants Council through GRF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990182","Convolutional networks (CNets);deep learning;fine-tuning;fully pretrained;high spatial resolution (HSR) imagery;land-use classification;random cropping and mirroring","Training;Feature extraction;Learning systems;Spatial resolution;Machine learning;Semantics;Acceleration","feedforward neural nets;geophysical image processing;image classification;image resolution;land use;learning (artificial intelligence);multilayer perceptrons","fully pretrained deep convolution networks;land-use classification;high spatial resolution imagery;feature descriptor;learning process;training process;transfer learning method;HSR images;multilayer perceptron classifier;random cropping;mirroring strategy","","7","21","","","","","IEEE","IEEE Journals"
"Learning Deep Sharable and Structural Detectors for Face Alignment","H. Liu; J. Lu; J. Feng; J. Zhou","Department of Automation, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Department of Automation, State Key Lab of Intelligent Technologies and Systems, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","","2017","26","4","1666","1678","Face alignment aims at localizing multiple facial landmarks for a given facial image, which usually suffers from large variances of diverse facial expressions, aspect ratios and partial occlusions, especially when face images were captured in wild conditions. Conventional face alignment methods extract local features and then directly concatenate these features for global shape regression. Unlike these methods which cannot explicitly model the correlation of neighbouring landmarks and motivated by the fact that individual landmarks are usually correlated, we propose a deep sharable and structural detectors (DSSD) method for face alignment. To achieve this, we firstly develop a structural feature learning method to explicitly exploit the correlation of neighbouring landmarks, which learns to cover semantic information to disambiguate the neighbouring landmarks. Moreover, our model selectively learns a subset of sharable latent tasks across neighbouring landmarks under the paradigm of the multi-task learning framework, so that the redundancy information of the overlapped patches can be efficiently removed. To better improve the performance, we extend our DSSD to a recurrent DSSD (R-DSSD) architecture by integrating with the complementary information from multi-scale perspectives. Experimental results on the widely used benchmark datasets show that our methods achieve very competitive performance compared to the state-of-the-arts.","","","10.1109/TIP.2017.2657118","National Key Research and Development Program of China; National Natural Science Foundation of China; National 1000 Young Talents Plan Program, the National Basic Research Program of China; Ministry of Education of China; Tsinghua University Initiative Scientific Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829264","Face alignment;deep learning;biometrics","Face;Shape;Feature extraction;Detectors;Decision support systems;Correlation;Redundancy","face recognition;feature extraction;learning (artificial intelligence);regression analysis","face alignment;deep sharable and structural detectors method;DSSD method;feature extraction;structural feature learning method;neighbouring landmarks;multitask learning framework;global shape regression","Algorithms;Biometric Identification;Face;Facial Expression;Humans;Image Processing, Computer-Assisted;Machine Learning;Pattern Recognition, Automated","22","53","","","","","IEEE","IEEE Journals"
"An End-to-End Deep Learning Approach to Simultaneous Speech Dereverberation and Acoustic Modeling for Robust Speech Recognition","B. Wu; K. Li; F. Ge; Z. Huang; M. Yang; S. M. Siniscalchi; C. Lee","Georgia Institute of Technology, National Laboratory of Radar Signal Processing, Xidan University, Atlanta, Xi’an, GA, USAChina; Faculty of Architecture and Engineering, Georgia Institute of Technology, University of Enna “Kore,” Enna 94100, Atlanta, GA, ItalyUSA; Georgia Institute of Technology, Key Lab of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences, Atlanta, Beijing, GA, USAChina; Faculty of Architecture and Engineering, Georgia Institute of Technology, University of Enna “Kore,” Enna 94100, Atlanta, GA, ItalyUSA; Georgia Institute of Technology, National Laboratory of Radar Signal Processing, Xidan University, Atlanta, Xi’an, GA, USAChina; Georgia Institute of Technology, Key Lab of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences, Atlanta, Beijing, GA, USAChina; Faculty of Architecture and Engineering, Georgia Institute of Technology, University of Enna “Kore,” Enna 94100, Atlanta, GA, ItalyUSA","IEEE Journal of Selected Topics in Signal Processing","","2017","11","8","1289","1300","We propose an integrated end-to-end automatic speech recognition (ASR) paradigm by joint learning of the front-end speech signal processing and back-end acoustic modeling. We believe that “only good signal processing can lead to top ASR performance” in challenging acoustic environments. This notion leads to a unified deep neural network (DNN) framework for distant speech processing that can achieve both high-quality enhanced speech and high-accuracy ASR simultaneously. Our goal is accomplished by two techniques, namely: (i) a reverberation-time-aware DNN based speech dereverberation architecture that can handle a wide range of reverberation times to enhance speech quality of reverberant and noisy speech, followed by (ii) DNN-based multicondition training that takes both clean-condition and multicondition speech into consideration, leveraging upon an exploitation of the data acquired and processed with multichannel microphone arrays, to improve ASR performance. The final end-to-end system is established by a joint optimization of the speech enhancement and recognition DNNs. The recent REverberant Voice Enhancement and Recognition Benchmark (REVERB) Challenge task is used as a test bed for evaluating our proposed framework. We first report on superior objective measures in enhanced speech to those listed in the 2014 REVERB Challenge Workshop on the simulated data test set. Moreover, we obtain the best single-system word error rate (WER) of 13.28% on the 1-channel REVERB simulated data with the proposed DNN-based pre-processing algorithm and clean-condition training. Leveraging upon joint training with more discriminative ASR features and improved neural network based language models, a low single-system WER of 4.46% is attained. Next, a new multi-channel-condition joint learning and testing scheme delivers a state-of-the-art WER of 3.76% on the 8-channel simulated data with a single ASR system. Finally, we also report on a preliminary yet promising experimentation with the REVERB real test data.","","","10.1109/JSTSP.2017.2756439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8051278","End-to-end system;dereverberation;robust ASR;deep learning;joint training;microphone array","Automatic speech recognition;Speech recognition;Training;Speech enhancement;Microphone arrays;Machine learning","learning (artificial intelligence);microphone arrays;neural nets;reverberation;speech enhancement;speech recognition","speech enhancement;1-channel REVERB simulated data;clean-condition training;discriminative ASR features;multichannel-condition joint learning;single ASR system;end-to-end deep learning approach;simultaneous speech dereverberation;robust speech recognition;integrated end-to-end automatic speech recognition paradigm;front-end speech signal processing;back-end acoustic modeling;ASR performance;challenging acoustic environments;unified deep neural network framework;distant speech processing;high-quality enhanced speech;high-accuracy ASR;multicondition speech;reverberant voice enhancement;reverberation-time-aware DNN;acoustic environments;DNN-based multicondition training;multichannel microphone arrays;reverberant voice enhancement-recognition benchmark;word error rate","","11","78","Traditional","","","","IEEE","IEEE Journals"
"Learning Transportation Modes From Smartphone Sensors Based on Deep Neural Network","S. Fang; Y. Fei; Z. Xu; Y. Tsao","Department of Electrical Engineering, Innovation Center for Big Data and Digital Convergence, Yuan Ze University, Taoyuan, Taiwan; Department of Electrical Engineering, Innovation Center for Big Data and Digital Convergence, Yuan Ze University, Taoyuan, Taiwan; School of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","IEEE Sensors Journal","","2017","17","18","6111","6118","In recent years, the importance of user information has increased rapidly for context-aware applications. This paper proposes a deep learning mechanism to identify the transportation modes of smartphone users. The proposed mechanism is evaluated on a database that contains more than 1000 h of accelerometer, magnetometer, and gyroscope measurements from five transportation modes, including still, walk, run, bike, and vehicle. Experimental results confirm the effectiveness of the proposed mechanism, which achieves approximately 95% classification accuracy and outperforms four well-known machine learning methods. Meanwhile, we investigated the model size and execution time of different algorithms to address practical issues.","","","10.1109/JSEN.2017.2737825","Ministry of Science and Technology, Taiwan; NSF of China; NSF of Fujian Province of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8006227","Transportation mode;big data;deep learning;mobile phone;sensors","Sensors;Transportation;Feature extraction;Data models;Machine learning;Biological neural networks;Accelerometers","accelerometers;computerised instrumentation;gyroscopes;learning (artificial intelligence);magnetometers;neural nets;sensors;smart phones;transportation","transportation mode;smartphone sensor;deep neural network;context-aware application;deep learning mechanism;accelerometer;magnetometer;gyroscope measurement;machine learning method","","16","42","","","","","IEEE","IEEE Journals"
"Spatial-Aware Hierarchical Collaborative Deep Learning for POI Recommendation","H. Yin; W. Wang; H. Wang; L. Chen; X. Zhou","School of ITEE, The University of Queensland, St Lucia, QLD, Australia; School of ITEE, The University of Queensland, St Lucia, QLD, Australia; 360 Search Lab, Qihoo 360 Inc., Beijing, China; Center for Artificial Intelligence, University of Technology, Sydney, NSW, Australia; School of ITEE, The University of Queensland, St Lucia, QLD, Australia","IEEE Transactions on Knowledge and Data Engineering","","2017","29","11","2537","2551","Point-of-interest (POI) recommendation has become an important way to help people discover attractive and interesting places, especially when they travel out of town. However, the extreme sparsity of user-POI matrix and cold-start issues severely hinder the performance of collaborative filtering-based methods. Moreover, user preferences may vary dramatically with respect to the geographical regions due to different urban compositions and cultures. To address these challenges, we stand on recent advances in deep learning and propose a Spatial-Aware Hierarchical Collaborative Deep Learning model (SH-CDL). The model jointly performs deep representation learning for POIs from heterogeneous features and hierarchically additive representation learning for spatial-aware personal preferences. To combat data sparsity in spatial-aware user preference modeling, both the collective preferences of the public in a given target region and the personal preferences of the user in adjacent regions are exploited in the form of social regularization and spatial smoothing. To deal with the multimodal heterogeneous features of the POIs, we introduce a late feature fusion strategy into our SH-CDL model. The extensive experimental analysis shows that our proposed model outperforms the state-of-the-art recommendation models, especially in out-of-town and cold-start recommendation scenarios.","","","10.1109/TKDE.2017.2741484","ARC Discovery Early Career Researcher Award; ARC Discovery Project; New Staff Research Grant of The University of Queensland; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8013107","POI recommendation;spatial-aware user modeling;deep learning;multimodal modeling;cold start","Semantics;Collaboration;Machine learning;Feature extraction;Probabilistic logic;Additives;Data models","collaborative filtering;learning (artificial intelligence);recommender systems","POI recommendation;point-of-interest recommendation;collaborative filtering;geographical regions;Spatial-Aware Hierarchical Collaborative Deep Learning model;deep representation;hierarchically additive representation;spatial-aware personal preferences;spatial-aware user preference modeling;collective preferences;spatial smoothing;multimodal heterogeneous features;SH-CDL model;recommendation scenarios;target region;urban compositions;POI;social regularization;late feature fusion","","16","55","Traditional","","","","IEEE","IEEE Journals"
"Exploiting Restricted Boltzmann Machines and Deep Belief Networks in Compressed Sensing","L. F. Polanía; K. E. Barner","American Family Mutual Insurance Company, Madison, WI, USA; Department of Electrical and Computer Engineering, University of Delaware, Newark, DE, USA","IEEE Transactions on Signal Processing","","2017","65","17","4538","4550","This paper proposes a CS scheme that exploits the representational power of restricted Boltzmann machines and deep learning architectures to model the prior distribution of the sparsity pattern of signals belonging to the same class. The determined probability distribution is then used in a maximum a posteriori approach for the reconstruction. The parameters of the prior distribution are learned from training data. The motivation behind this approach is to model the higher-order statistical dependencies between the coefficients of the sparse representation, with the final goal of improving the reconstruction. The performance of the proposed method is validated on the Berkeley Segmentation Dataset and the MNIST Database of handwritten digits.","","","10.1109/TSP.2017.2712128","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7938698","Compressed sensing (CS);restricted Boltzmann machine (RBM);deep learning;deep belief network (DBN);wavelets;dictionary learning","Dictionaries;Training;Machine learning;Compressed sensing;Training data;Data models;Reconstruction algorithms","Boltzmann machines;compressed sensing;higher order statistics;image reconstruction;image representation;learning (artificial intelligence);maximum likelihood estimation;statistical distributions","restricted Boltzmann machines;deep belief networks;compressed sensing;CS scheme;deep learning architectures;prior signal sparsity pattern distribution;determined probability distribution;maximum-a-posteriori approach;higher-order statistical dependencies;sparse representation;Berkeley Segmentation Dataset;MNIST Database;handwritten digits","","5","43","","","","","IEEE","IEEE Journals"
"Mesh Convolutional Restricted Boltzmann Machines for Unsupervised Learning of Features With Structure Preservation on 3-D Meshes","Z. Han; Z. Liu; J. Han; C. Vong; S. Bu; C. L. P. Chen","Northwestern Polytechnical University, Xi’an, China; Northwestern Polytechnical University, Xi’an, China; Northwestern Polytechnical University, Xi’an, China; Department of Computer and Information Science, University of Macau, Macau, China; Northwestern Polytechnical University, Xi’an, China; Faculty of Science and Technology, University of Macau, Macau, China","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","10","2268","2281","Discriminative features of 3-D meshes are significant to many 3-D shape analysis tasks. However, handcrafted descriptors and traditional unsupervised 3-D feature learning methods suffer from several significant weaknesses: 1) the extensive human intervention is involved; 2) the local and global structure information of 3-D meshes cannot be preserved, which is in fact an important source of discriminability; 3) the irregular vertex topology and arbitrary resolution of 3-D meshes do not allow the direct application of the popular deep learning models; 4) the orientation is ambiguous on the mesh surface; and 5) the effect of rigid and nonrigid transformations on 3-D meshes cannot be eliminated. As a remedy, we propose a deep learning model with a novel irregular model structure, called mesh convolutional restricted Boltzmann machines (MCRBMs). MCRBM aims to simultaneously learn structure-preserving local and global features from a novel raw representation, local function energy distribution. In addition, multiple MCRBMs can be stacked into a deeper model, called mesh convolutional deep belief networks (MCDBNs). MCDBN employs a novel local structure preserving convolution (LSPC) strategy to convolve the geometry and the local structure learned by the lower MCRBM to the upper MCRBM. LSPC facilitates resolving the challenging issue of the orientation ambiguity on the mesh surface in MCDBN. Experiments using the proposed MCRBM and MCDBN were conducted on three common aspects: global shape retrieval, partial shape retrieval, and shape correspondence. Results show that the features learned by the proposed methods outperform the other state-of-the-art 3-D shape features.","","","10.1109/TNNLS.2016.2582532","NWPU Basic Research Fund; Science and Technology Development Fund of Macau; Open Fund of State Key Laboratory of CAD & CG, Zhejiang University; Universidade de Macau; Open Research Foundation of State Key Laboratory of Digital Manufacturing Equipment & Technology in Huazhong University of Science & Technology; Fund of National Engineering and Research Center for Commercial Aircraft Manufacturing; Shaanxi Natural Science Fund; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502161","3-D mesh;Laplace–Beltrami operator;mesh convolutional deep belief networks (MCDBNs);mesh convolutional restricted Boltzmann machines (MCRBMs)","Shape;Machine learning;Solid modeling;Feature extraction;Convolution;Unsupervised learning","Boltzmann machines;mesh generation;solid modelling;unsupervised learning","mesh convolutional restricted Boltzmann machines;unsupervised learning;3D meshes;global structure information;local structure information;deep learning models;MCRBM;novel irregular model structure;local function energy distribution;novel local structure preserving convolution strategy;global shape retrieval;partial shape retrieval;shape correspondence","","6","63","","","","","IEEE","IEEE Journals"
"Deep Correlation Feature Learning for Face Verification in the Wild","W. Deng; B. Chen; Y. Fang; J. Hu","Pattern Recognition and Intelligent System Laboratory, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition and Intelligent System Laboratory, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; International School, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition and Intelligent System Laboratory, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Signal Processing Letters","","2017","24","12","1877","1881","Convolutional neural networks (CNNs) commonly uses the softmax loss function as the supervision signal. In order to enhance the discriminative power of the deeply learned features, this letter proposes a new supervision signal, called correlation loss, for face verification task. Specifically, the correlation loss encourages the large correlation between the deep feature vectors and their corresponding weight vectors in softmax loss. With the joint supervision of softmax loss and correlation loss, the deep correlation feature learning (DCFL) network can learn the deep features with both the interclass separability and the intraclass compactness, which are highly discriminative for face verification. More importantly, by applying the weight vector of softmax function as the class prototype, the proposed correlation loss function is easy to be optimized during the backpropatation of CNN. Finally, the DCFL method achieves 99.55% and 96.06% face verification accuracy using a 64-layer ResNet on the labeled face in-the-Wild (LFW) and you-tube face (YTF) benchmark, respectively.","","","10.1109/LSP.2017.2726105","Beijing Nova Program; National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976346","Convolutional neural networks (CNNs);deep learning;feature learning;face verification;softmax","Learning systems;Training;Feature extraction;Neural networks;Prototypes;Convolutional codes","backpropagation;correlation methods;face recognition;feedforward neural nets;vectors","YTF benchmark;you-tube face benchmark;LFW benchmark;labeled face in-the-wild benchmark;64-layer ResNet;backpropatation;intraclass compactness;interclass separability;DCFL network;weight vectors;deep feature vectors;correlation loss function;supervision signal;softmax loss function;CNN;convolutional neural networks;face verification task;deep correlation feature learning network","","2","27","Traditional","","","","IEEE","IEEE Journals"
"Error Tolerance Analysis of Deep Learning Hardware Using a Restricted Boltzmann Machine Toward Low-Power Memory Implementation","T. Marukame; K. Ueyoshi; T. Asai; M. Motomura; A. Schmid; M. Suzuki; Y. Higashi; Y. Mitani","Microelectronic Systems Laboratory, Advanced LSI Technology Laboratory, Swiss Federal Institute of Technology (EPFL), Toshiba Corporation, Lausanne, Kawasaki, SwitzerlandJapan; Graduate School of Information Science and Technology, Hokkaido University, Sapporo, Japan; Graduate School of Information Science and Technology, Hokkaido University, Sapporo, Japan; Graduate School of Information Science and Technology, Hokkaido University, Sapporo, Japan; Microelectronic Systems Laboratory, Swiss Federal Institute of Technology (EPFL), Lausanne, Switzerland; Advanced LSI Technology Laboratory, Toshiba Corporation, Kawasaki, Japan; Advanced LSI Technology Laboratory, Toshiba Corporation, Kawasaki, Japan; Advanced LSI Technology Laboratory, Toshiba Corporation, Kawasaki, Japan","IEEE Transactions on Circuits and Systems II: Express Briefs","","2017","64","4","462","466","Remarkable hardware robustness of deep learning (DL) is revealed by error injection analyses performed using a custom hardware model implementing parallelized restricted Boltzmann machines (RBMs). RBMs in deep belief networks demonstrate robustness against memory errors during and after learning. Fine-tuning significantly affects the recovery of accuracy for static errors injected to the structural data of RBMs. The memory error tolerance is observable using our hardware networks with fine-graded memory distribution, resulting in reliable DL hardware with low-voltage driven memory suitable to low-power applications.","","","10.1109/TCSII.2016.2585675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7501537","Deep learning (DL);fault tolerance;low power;restricted Boltzmann machines (RBMs);static random access memory (SRAM)","Circuit faults;Hardware;Field programmable gate arrays;Computer architecture;Machine learning;Robustness;Data models","belief networks;Boltzmann machines;error analysis;learning (artificial intelligence);low-power electronics;neural chips;storage management chips","deep learning hardware;low-power memory implementation;low-voltage driven memory;fine-graded memory distribution;memory error tolerance analysis;structural data;deep belief networks;RBMs;parallelized restricted Boltzmann machines;error injection analysis;custom hardware model","","9","13","","","","","IEEE","IEEE Journals"
"Face Verification via Learned Representation on Feature-Rich Video Frames","G. Goswami; M. Vatsa; R. Singh","Indraprastha Institute of Information Technology Delhi, Delhi, India; Indraprastha Institute of Information Technology Delhi, Delhi, India; Indraprastha Institute of Information Technology Delhi, Delhi, India","IEEE Transactions on Information Forensics and Security","","2017","12","7","1686","1698","Abundance and availability of video capture devices, such as mobile phones and surveillance cameras, have instigated research in video face recognition, which is highly pertinent in law enforcement applications. While the current approaches have reported high accuracies at equal error rates, performance at lower false accept rates requires significant improvement. In this paper, we propose a novel face verification algorithm, which starts with selecting feature-rich frames from a video sequence using discrete wavelet transform and entropy computation. Frame selection is followed by representation learning-based feature extraction, where three contributions are presented: 1) deep learning architecture, which is a combination of stacked denoising sparse autoencoder (SDAE) and deep Boltzmann machine (DBM); 2) formulation for joint representation in an autoencoder; and 3) updating the loss function of DBM by including sparse and low rank regularization. Finally, a multilayer neural network is used as the classifier to obtain the verification decision. The results are demonstrated on two publicly available databases, YouTube Faces and Point and Shoot Challenge. Experimental analysis suggests that: 1) the proposed feature-richness-based frame selection offers noticeable and consistent performance improvement compared with frontal only frames, random frames, or frame selection using perceptual no-reference image quality measures and 2) joint feature learning in SDAE and sparse and low rank regularization in DBM helps in improving face verification performance. On the benchmark Point and Shoot Challenge database, the algorithm yields the verification accuracy of over 97% at 1% false accept rate whereas, on the YouTube Faces database, over 95% verification accuracy is observed at equal error rate.","","","10.1109/TIFS.2017.2668221","MEITY, India, NVIDIA GPU grant, and Infosys CAI, IIIT-Delhi; IBM Ph.D. fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850956","Deep learning;autoencoder;deep Boltzmann machine;face recognition;frame selection","Face;Face recognition;Databases;Feature extraction;YouTube;Machine learning;Neural networks","Boltzmann machines;discrete wavelet transforms;entropy;face recognition;feature selection;image denoising;image sequences;learning (artificial intelligence);multilayer perceptrons;video signal processing","face verification algorithm;representation learning;feature-rich video frames;video capture devices;video face recognition;feature-rich frames selection;video sequence;discrete wavelet transform;entropy computation;feature extraction;deep learning architecture;stacked denoising sparse autoencoder;SDAE;deep Boltzmann machine;DBM;multilayer neural network","","18","50","","","","","IEEE","IEEE Journals"
"Deep Hashing for Scalable Image Search","J. Lu; V. E. Liong; J. Zhou","Department of Automation, Tsinghua University, Beijing, China; Rapid-Rich Object Search Laboratory, Interdisciplinary Graduate School, Nanyang Technological University, Singapore; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","","2017","26","5","2352","2367","In this paper, we propose a new deep hashing (DH) approach to learn compact binary codes for scalable image search. Unlike most existing binary codes learning methods, which usually seek a single linear projection to map each sample into a binary feature vector, we develop a deep neural network to seek multiple hierarchical non-linear transformations to learn these binary codes, so that the non-linear relationship of samples can be well exploited. Our model is learned under three constraints at the top layer of the developed deep network: 1) the loss between the compact real-valued code and the learned binary vector is minimized, 2) the binary codes distribute evenly on each bit, and 3) different bits are as independent as possible. To further improve the discriminative power of the learned binary codes, we extend DH into supervised DH (SDH) and multi-label SDH by including a discriminative term into the objective function of DH, which simultaneously maximizes the inter-class variations and minimizes the intra-class variations of the learned binary codes with the single-label and multi-label settings, respectively. Extensive experimental results on eight widely used image search data sets show that our proposed methods achieve very competitive results with the state-of-the-arts.","","","10.1109/TIP.2017.2678163","National Key Research and Development Program of China; National Natural Science Foundation of China; National 1000 Young Talents Plan Program; National Basic Research Program of China; Ministry of Education of China; Tsinghua University Initiative Scientific Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7870632","Scalable image search;fast similarity search;hashing;deep learning;multi-label learning","Binary codes;DH-HEMTs;Machine learning;Visualization;Neural networks;Training;Synchronous digital hierarchy","binary codes;image coding;image retrieval;learning (artificial intelligence);minimisation;neural nets","deep hashing;scalable image search;compact binary code learning;deep neural network;multiple hierarchical nonlinear transformations;intra-class variation minimization;inter-class variation maximization;multilabel SDH;supervised DH;binary vector;compact real-valued code","","34","86","","","","","IEEE","IEEE Journals"
"Device-Free Wireless Localization and Activity Recognition: A Deep Learning Approach","J. Wang; X. Zhang; Q. Gao; H. Yue; H. Wang","Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Department of Computer Science, San Francisco State University, San Francisco, CA, USA; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Vehicular Technology","","2017","66","7","6258","6267","Device-free wireless localization and activity recognition (DFLAR) is a new technique, which could estimate the location and activity of a target by analyzing its shadowing effect on surrounding wireless links. This technique neither requires the target to be equipped with any device nor involves privacy concerns, which makes it an attractive and promising technique for many emerging smart applications. The key question of DFLAR is how to characterize the influence of the target on wireless signals. Existing work generally utilizes statistical features extracted from wireless signals, such as mean and variance in the time domain and energy as well as entropy in the frequency domain, to characterize the influence of the target. However, a feature suitable for distinguishing some activities or gestures may perform poorly when it is used to recognize other activities or gestures. Therefore, one has to manually design handcraft features for a specific application. Inspired by its excellent performance in extracting universal and discriminative features, in this paper, we propose a deep learning approach for realizing DFLAR. Specifically, we design a sparse autoencoder network to automatically learn discriminative features from the wireless signals and merge the learned features into a softmax-regression-based machine learning framework to realize location, activity, and gesture recognition simultaneously. Extensive experiments performed in a clutter indoor laboratory and an apartment with eight wireless nodes demonstrate that the DFLAR system using the learned features could achieve 0.85 or higher accuracy, which is better than the systems utilizing traditional handcraft features.","","","10.1109/TVT.2016.2635161","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Scientific Research Staring Foundation for the Returned Overseas Chinese Scholars, and Xinghai Scholars Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7765094","Activity recognition;deep learning;device-free localization;wireless networks","Wireless communication;Feature extraction;Wireless sensor networks;Machine learning;Performance evaluation;Time-domain analysis","clutter;feature extraction;frequency-domain analysis;gesture recognition;indoor navigation;learning (artificial intelligence);mobile computing;radio links;regression analysis;time-domain analysis","wireless nodes;clutter indoor laboratory;gesture recognition;softmax-regression-based machine learning;sparse autoencoder network;discriminative feature extraction;universal feature extraction;frequency domain;entropy;time domain;statistical feature extraction;wireless signals;DFLAR;smart applications;wireless links;shadowing effect;activity estimation;location estimation;deep learning;device-free wireless localization and activity recognition","","43","29","","","","","IEEE","IEEE Journals"
"Multilevel Cloud Detection in Remote Sensing Images Based on Deep Learning","F. Xie; M. Shi; Z. Shi; J. Yin; D. Zhao","Beijing Key Laboratory of Digital Media and the Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Beijing Key Laboratory of Digital Media and the Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Beijing Key Laboratory of Digital Media and the Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Beijing Key Laboratory of Digital Media and the Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Beijing Key Laboratory of Digital Media and the Image Processing Center, School of Astronautics, Beihang University, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","8","3631","3640","Cloud detection is one of the important tasks for remote sensing image processing. In this paper, a novel multilevel cloud detection method based on deep learning is proposed for remote sensing images. First, the simple linear iterative clustering (SLIC) method is improved to segment the image into good quality superpixels. Then, a deep convolutional neural network (CNN) with two branches is designed to extract the multiscale features from each superpixel and predict the superpixel as one of three classes including thick cloud, thin cloud, and noncloud. Finally, the predictions of all the superpixels in the image yield the cloud detection result. In the proposed cloud detection framework, the improved SLIC method can obtain accurate cloud boundaries by optimizing initial cluster centers, designing dynamic distance measure, and expanding search space. Moreover, different from traditional cloud detection methods that cannot achieve multilevel detection of cloud, the designed deep CNN model can not only detect cloud but also distinguish thin cloud from thick cloud. Experimental results indicate that the proposed method can detect cloud with higher accuracy and robustness than compared methods.","","","10.1109/JSTARS.2017.2686488","National Natural Science Foundation of China; Chinese Academy of Sciences; Joint Fund of Astronomy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7895175","Cloud detection;convolutional neural network (CNN);deep learning;remote sensing images;superpixel","Clouds;Remote sensing;Image color analysis;Feature extraction;Machine learning;Image segmentation;Clustering algorithms","clouds;feature extraction;geophysical image processing;image segmentation;iterative methods;learning (artificial intelligence);neural nets;pattern clustering;remote sensing","multilevel cloud detection;deep learning;remote sensing image processing;simple linear iterative clustering;SLIC method;image segmentation;deep convolutional neural network;feature extraction;thick cloud;thin cloud;noncloud","","20","33","","","","","IEEE","IEEE Journals"
"Self-Taught Feature Learning for Hyperspectral Image Classification","R. Kemker; C. Kanan","Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA; Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","5","2693","2705","In this paper, we study self-taught learning for hyperspectral image (HSI) classification. Supervised deep learning methods are currently state of the art for many machine learning problems, but these methods require large quantities of labeled data to be effective. Unfortunately, existing labeled HSI benchmarks are too small to directly train a deep supervised network. Alternatively, we used self-taught learning, which is an unsupervised method to learn feature extracting frameworks from unlabeled hyperspectral imagery. These models learn how to extract generalizable features by training on sufficiently large quantities of unlabeled data that are distinct from the target data set. Once trained, these models can extract features from smaller labeled target data sets. We studied two self-taught learning frameworks for HSI classification. The first is a shallow approach that uses independent component analysis and the second is a three-layer stacked convolutional autoencoder. Our models are applied to the Indian Pines, Salinas Valley, and Pavia University data sets, which were captured by two separate sensors at different altitudes. Despite large variation in scene type, our algorithms achieve state-of-the-art results across all the three data sets.","","","10.1109/TGRS.2017.2651639","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875467","Autoencoder;deep learning;feature learning;hyperspectral imaging;independent component analysis (ICA);self-taught learning","Feature extraction;Data mining;Data models;Sensors;Hyperspectral imaging;Training;Encoding","geophysical image processing;hyperspectral imaging;image classification;independent component analysis;learning (artificial intelligence)","hyperspectral image classification;self-taught learning;HSI classification;supervised deep learning methods;machine learning problem;deep supervised network;independent component analysis;three-layer stacked convolutional autoencoder;Indian Pines;Salinas Valley;Pavia University data sets","","24","70","","","","","IEEE","IEEE Journals"
"Deep Learning for Quality Assessment in Live Video Streaming","M. T. Vega; D. C. Mocanu; J. Famaey; S. Stavrou; A. Liotta","Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands; Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands; iMinds University of Antwerp, Antwerp, Belgium; Faculty of Pure and Applied Sciences, Open University of Cyprus, Nicosia, Cyprus; Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands","IEEE Signal Processing Letters","","2017","24","6","736","740","Video content providers put stringent requirements on the quality assessment methods realized on their services. They need to be accurate, real-time, adaptable to new content, and scalable as the video set grows. In this letter, we introduce a novel automated and computationally efficient video assessment method. It enables accurate real-time (online) analysis of delivered quality in an adaptable and scalable manner. Offline deep unsupervised learning processes are employed at the server side and inexpensive no-reference measurements at the client side. This provides both real-time assessment and performance comparable to the full reference counterpart, while maintaining its no-reference characteristics. We tested our approach on the LIMP Video Quality Database (an extensive packet loss impaired video set) obtaining a correlation between 78% and 91% to the FR benchmark (the video quality metric). Due to its unsupervised learning essence, our method is flexible and dynamically adaptable to new content and scalable with the number of videos.","","","10.1109/LSP.2017.2691160","European Research Council project BROWSE; ICT COST Action 3D-ConTourNet; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892928","Deep learning (DL);multimedia video services;unsupervised learning (UL);video quality assessment","Streaming media;Quality assessment;Video recording;Real-time systems;Measurement;Feature extraction;Machine learning","unsupervised learning;video signal processing;video streaming","video content providers;quality assessment methods;video assessment method;real-time analysis;online analysis;offline deep unsupervised learning processes;inexpensive no-reference measurements;real-time assessment;LIMP Video Quality Database;extensive packet loss impaired video set;FR benchmark;video quality metric","","23","43","","","","","IEEE","IEEE Journals"
"Learning and Transferring Deep Joint Spectral–Spatial Features for Hyperspectral Classification","J. Yang; Y. Zhao; J. C. Chan","Key Laboratory of Information Fusion Technology (Ministry of Education of China), School of Automation, Northwestern Polytechnical University, Xi’an, China; Key Laboratory of Information Fusion Technology (Ministry of Education of China), School of Automation, Northwestern Polytechnical University, Xi’an, China; Department of Electronics and Informatics, Vrije Universiteit Brussel, Brussel, Belgium","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","8","4729","4742","Feature extraction is of significance for hyperspectral image (HSI) classification. Compared with conventional hand-crafted feature extraction, deep learning can automatically learn features with discriminative information. However, two issues exist in applying deep learning to HSIs. One issue is how to jointly extract spectral features and spatial features, and the other one is how to train the deep model when training samples are scarce. In this paper, a deep convolutional neural network with two-branch architecture is proposed to extract the joint spectral-spatial features from HSIs. The two branches of the proposed network are devoted to features from the spectral domain as well as the spatial domain. The learned spectral features and spatial features are then concatenated and fed to fully connected layers to extract the joint spectral-spatial features for classification. When the training samples are limited, we investigate the transfer learning to improve the performance. Low and mid-layers of the network are pretrained and transferred from other data sources; only top layers are trained with limited training samples extracted from the target scene. Experiments on Airborne Visible/Infrared Imaging Spectrometer and Reflective Optics System Imaging Spectrometer data demonstrate that the learned deep joint spectral-spatial features are discriminative, and competitive classification results can be achieved when compared with state-of-the-art methods. The experiments also reveal that the transferred features boost the classification performance.","","","10.1109/TGRS.2017.2698503","National Natural Science Foundation of China; National Natural Science Foundation of China; South Korean National Research Foundation Joint Funded Cooperation Program; New Century Excellent Talents Award Program from the Ministry of Education of China; Ministry of Education Scientific Research Foundation for the Returned Overseas; Fundamental Research Funds for the Central Universities; China Scholarship Council for joint Ph.D. students; Innovation Foundation of Doctor Dissertation of Northwestern Polytechnical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927776","Convolutional neural network (CNN);deeplearning;feature extraction;hyperspectral classification;transfer learning","Feature extraction;Training;Machine learning;Data mining;Hyperspectral imaging;Principal component analysis","feature extraction;hyperspectral imaging;image classification;learning (artificial intelligence);neural nets;remote sensing","deep joint spectral-spatial feature;hyperspectral image classification;deep learning;spectral feature extraction;spatial feature extraction;deep convolutional neural network;airborne visible-infrared imaging spectrometer;reflective optics system imaging spectrometer","","43","54","","","","","IEEE","IEEE Journals"
"Deep Learning With Grouped Features for Spatial Spectral Classification of Hyperspectral Images","X. Zhou; S. Li; F. Tang; K. Qin; S. Hu; S. Liu","Chongqing Engineering Laboratory of High Performance Integrated Circuits, College of Communication Engineering, Chongqing University, Chongqing, China; Chongqing Engineering Laboratory of High Performance Integrated Circuits, College of Communication Engineering, Chongqing University, Chongqing, China; Chongqing Engineering Laboratory of High Performance Integrated Circuits, College of Communication Engineering, Chongqing University, Chongqing, China; National Key Laboratory of Science and Technology on Remote Sensing Information and Image Analysis, Beijing, China; Chongqing Engineering Laboratory of High Performance Integrated Circuits, College of Communication Engineering, Chongqing University, Chongqing, China; Chongqing Engineering Laboratory of High Performance Integrated Circuits, College of Communication Engineering, Chongqing University, Chongqing, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","1","97","101","This letter presents a novel deep learning algorithm for feature extraction from the hyperspectral images. The proposed method takes advantage of the knowledge that the features of the spatial-spectral data naturally fall into an array of groups with respect to different spectral bands. Aiming to reduce the influence of redundant spectral bands adaptively using unlabeled hyperspectral data, we incorporate the group information in the training algorithm of the deep neural network via a regularized weight-decay process. Experiments over different benchmarks of hyperspectral images show that the proposed method provides competitive solution with the state-of-the-art approaches.","","","10.1109/LGRS.2016.2630045","National Natural Science Foundation of China; Post-Doctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7778156","Deep belief network (DBN);deep learning;grouped features;hyperspectral images (HSIs);unlabeled samples","Hyperspectral imaging;Feature extraction;Training;Data models;Machine learning;Standards","feature extraction;geophysical image processing;hyperspectral imaging;image classification;neural nets;remote sensing","regularized weight-decay process;deep neural network;unlabeled hyperspectral data;spectral bands;spatial-spectral data;feature extraction;deep learning algorithm;hyperspectral image spatial spectral classification","","24","15","","","","","IEEE","IEEE Journals"
"Constrained Deep Weak Supervision for Histopathology Image Segmentation","Z. Jia; X. Huang; E. I. Chang; Y. Xu","Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology, Ministry of Education, Research Institute of Beihang University in Shenzhen, Beihang University, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China","IEEE Transactions on Medical Imaging","","2017","36","11","2376","2388","In this paper, we develop a new weakly supervised learning algorithm to learn to segment cancerous regions in histopathology images. This paper is under a multiple instance learning (MIL) framework with a new formulation, deep weak supervision (DWS); we also propose an effective way to introduce constraints to our neural networks to assist the learning process. The contributions of our algorithm are threefold: 1) we build an end-to-end learning system that segments cancerous regions with fully convolutional networks (FCNs) in which image-to-image weakly-supervised learning is performed; 2) we develop a DWS formulation to exploit multi-scale learning under weak supervision within FCNs; and 3) constraints about positive instances are introduced in our approach to effectively explore additional weakly supervised information that is easy to obtain and enjoy a significant boost to the learning process. The proposed algorithm, abbreviated as DWS-MIL, is easy to implement and can be trained efficiently. Our system demonstrates the state-of-the-art results on large-scale histopathology image data sets and can be applied to various applications in medical imaging beyond histopathology images, such as MRI, CT, and ultrasound images.","","","10.1109/TMI.2017.2724070","Microsoft Research through the eHealth Program; Beijing National Science Foundation in China; Technology and Innovation Commission of Shenzhen in China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7971941","Convolutional neural networks;histopathology image segmentation;weakly supervised learning;fully convolutional networks;multiple instance learning","Image segmentation;Supervised learning;Biomedical imaging;Cancer;Training;Neural networks;Prediction algorithms","cancer;image segmentation;learning (artificial intelligence);medical image processing;neural nets","constrained deep weak supervision;histopathology image segmentation;cancerous regions;multiple instance learning framework;MIL framework;neural networks;end-to-end learning system;fully convolutional networks;DWS-MIL","Algorithms;Colon;Colonic Neoplasms;Databases, Factual;Histocytochemistry;Humans;Image Processing, Computer-Assisted;Neural Networks (Computer);Supervised Machine Learning;Tissue Array Analysis","5","52","Traditional","","","","IEEE","IEEE Journals"
"An Incremental Framework for Video-Based Traffic Sign Detection, Tracking, and Recognition","Y. Yuan; Z. Xiong; Q. Wang","School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Intelligent Transportation Systems","","2017","18","7","1918","1929","Video-based traffic sign detection, tracking, and recognition is one of the important components for the intelligent transport systems. Extensive research has shown that pretty good performance can be obtained on public data sets by various state-of-the-art approaches, especially the deep learning methods. However, deep learning methods require extensive computing resources. In addition, these approaches mostly concentrate on single image detection and recognition task, which is not applicable in real-world applications. Different from previous research, we introduce a unified incremental computational framework for traffic sign detection, tracking, and recognition task using the mono-camera mounted on a moving vehicle under non-stationary environments. The main contributions of this paper are threefold: (1) to enhance detection performance by utilizing the contextual information, this paper innovatively utilizes the spatial distribution prior of the traffic signs; (2) to improve the tracking performance and localization accuracy under non-stationary environments, a new efficient incremental framework containing off-line detector, online detector, and motion model predictor together is designed for traffic sign detection and tracking simultaneously; and (3) to get a more stable classification output, a scale-based intra-frame fusion method is proposed. We evaluate our method on two public data sets and the performance has shown that the proposed system can obtain results comparable with the deep learning method with less computing resource in a near-real-time manner.","","","10.1109/TITS.2016.2614548","National Natural Science Foundation of China; Natural Science Foundation Research Project of Shaanxi Province; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7605450","Machine learning;traffic sign;detection;tracking;recognition;incremental learning;ITS","Detectors;Image color analysis;Shape;Target tracking;Machine learning;Color","cameras;edge detection;image classification;intelligent transportation systems;learning (artificial intelligence);traffic engineering computing","traffic sign detection;traffic sign tracking;traffic sign recognition;intelligent transport systems;deep learning;image detection;image recognition;offline detector;online detector;motion model predictor;incremental learning","","55","49","","","","","IEEE","IEEE Journals"
"A Data-Driven Soft Sensor Modeling Method Based on Deep Learning and its Application","W. Yan; D. Tang; Y. Lin","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Webank, Shenzhen, China","IEEE Transactions on Industrial Electronics","","2017","64","5","4237","4245","Soft sensors have been widely used in industrial processes. The core issue of data-driven soft sensors is building soft sensor models with excellent performance and robustness. This paper introduces deep learning to soft sensor modeling and proposes a novel soft sensor modeling method based on a deep learning network that integrates denoising autoencoders with a neural network (DAE-NN). An improved gradient descent is employed to update the model parameters. The proposed modeling method is able to capture the essential information of input data through deep architecture, building soft sensors with excellent performance. The DAE-NN-based soft sensor is applied in practical applications to estimate the oxygen content in flue gasses in 1000-MW ultrasuperficial units. Comparing conventional soft sensor modeling methods, i.e., shallow learning methods, DAE-NN-based soft sensor significantly improves the performance and generalization of data-driven soft sensors. Deep learning provides a very effective and promising method for soft sensor modeling.","","","10.1109/TIE.2016.2622668","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7723870","Deep learning;denoising autoencoder (DAE);soft sensor;unlabeled data","Machine learning;Biological system modeling;Computational modeling;Data models;Noise reduction;Training;Robustness","gradient methods;industrial engineering;learning (artificial intelligence);neural nets","data-driven soft sensor modeling;industrial process;robustness;deep learning network;denoising autoencoders;neural network;DAE-NN;gradient descent method","","41","31","","","","","IEEE","IEEE Journals"
"Integrating Online and Offline Three-Dimensional Deep Learning for Automated Polyp Detection in Colonoscopy Videos","L. Yu; H. Chen; Q. Dou; J. Qin; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Journal of Biomedical and Health Informatics","","2017","21","1","65","75","Automated polyp detection in colonoscopy videos has been demonstrated to be a promising way for colorectal cancer prevention and diagnosis. Traditional manual screening is time consuming, operator dependent, and error prone; hence, automated detection approach is highly demanded in clinical practice. However, automated polyp detection is very challenging due to high intraclass variations in polyp size, color, shape, and texture, and low interclass variations between polyps and hard mimics. In this paper, we propose a novel offline and online three-dimensional (3-D) deep learning integration framework by leveraging the 3-D fully convolutional network (3D-FCN) to tackle this challenging problem. Compared with the previous methods employing hand-crafted features or 2-D convolutional neural network, the 3D-FCN is capable of learning more representative spatio-temporal features from colonoscopy videos, and hence has more powerful discrimination capability. More importantly, we propose a novel online learning scheme to deal with the problem of limited training data by harnessing the specific information of an input video in the learning process. We integrate offline and online learning to effectively reduce the number of false positives generated by the offline network and further improve the detection performance. Extensive experiments on the dataset of MICCAI 2015 Challenge on Polyp Detection demonstrated the better performance of our method when compared with other competitors.","","","10.1109/JBHI.2016.2637004","Hong Kong Special Administrative Region; National Natural Science Foundation of China; Shenzhen Science and Technology Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776845","Automated polyp detection;colonoscopy video;computer-aided diagnosis;convolutional neural networks (CNNs);deep learning","Videos;Colonoscopy;Three-dimensional displays;Feature extraction;Cancer;Shape;MIMICs","cancer;endoscopes;learning (artificial intelligence);medical image processing;neural nets;video signal processing","online three-dimensional deep learning integration framework;offline three-dimensional deep learning integration framework;automated polyp detection;colonoscopy videos;3D fully convolutional network;spatiotemporal features;colorectal cancer prevention;colorectal cancer diagnosis","Colonic Polyps;Colonoscopy;Humans;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Neural Networks (Computer);Video Recording","36","46","","","","","IEEE","IEEE Journals"
"Rough Deep Neural Architecture for Short-Term Wind Speed Forecasting","M. Khodayar; O. Kaynak; M. E. Khodayar","Electrical and Computer Engineering Department, Khajeh Nasir Toosi University of Technology, Tehran, Iran; Department of Electrical and Electronic Engineering, Bogazici University, Istanbul, Turkey; Department of Electrical Engineering, Southern Methodist University, Dallas, TX, USA","IEEE Transactions on Industrial Informatics","","2017","13","6","2770","2779","Accurate wind speed forecasting is a fundamental requirement for large-scale integration of wind power generation. However, the intermittent and stochastic nature of wind speed makes this task challenging. Artificial neural networks (ANNs) are widely used in this area; however, they may fail to provide the accuracy that may be required. This is due to applying shallow architectures with error-prone hand-engineered features. This paper proposes a deep neural network (DNN) architecture with stacked autoencoder (SAE) and stacked denoising autoencoder (SDAE) for ultrashort-term and short-term wind speed forecasting. Autoencoders (AEs) are applied for unsupervised feature learning from the unlabeled wind data and a supervised regression layer is applied at the top of the AEs for wind speed forecasting. Several uncertain factors exist in the wind data that degrade the accuracy of current methodologies. In order to improve the accuracy, rough neural networks are incorporated in the proposed deep learning models to develop novel rough extensions of SAE and SDAE that are robust to wind uncertainties. Experimental results show that the proposed rough DNN models outperform classic DNNs and previous models that apply shallow architectures in the view of lower RMSE and mean absolute error measurements.","","","10.1109/TII.2017.2730846","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990193","Deep neural network (DNN);forecasting;rough neural network (RNN);uncertainty;wind","Wind speed;Forecasting;Neural networks;Wind forecasting;Predictive models;Autoregressive processes;Machine learning","encoding;learning (artificial intelligence);mean square error methods;neural nets;power engineering computing;regression analysis;signal denoising;unsupervised learning;wind power plants","wind speed forecasting;rough extensions;stacked denoising autoencoder;unsupervised feature learning;mean absolute error measurements;RMSE;short-term wind speed forecasting;rough neural networks;unlabeled wind data;ultrashort-term;stacked autoencoder;deep neural network architecture;artificial neural networks;stochastic nature;wind power generation;deep neural architecture;shallow architectures;rough DNN models;SDAE;SAE;deep learning models","","25","36","Traditional","","","","IEEE","IEEE Journals"
"A deep learning framework using convolution neural network for classification of impulse fault patterns in transformers with increased accuracy","D. Dey; B. Chatterjee; S. Dalai; S. Munshi; S. Chakravorti","Department of Electrical Engineering, Jadavpur University, India; Department of Electrical Engineering, Jadavpur University, India; Department of Electrical Engineering, Jadavpur University, India; Department of Electrical Engineering, Jadavpur University, India; NIT, Calicut, India","IEEE Transactions on Dielectrics and Electrical Insulation","","2017","24","6","3894","3897","The paper presents a method using deep learning framework based on convolution neural network (CNN), for identification and localization of faults of transformer winding under impulse test. The results show that the proposed method outperforms the existing methods significantly. The present scheme eliminates the requirement of separate feature extraction and classification algorithms for the analysis of fault current patterns. A part of the proposed network performs feature learning and the other part classifies the features in a supervised manner. The method is computation intensive but capable of achieving very high degree of accuracy; on an average a margin of more than 7% compared to other published literature till date.","","","10.1109/TDEI.2017.006793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8315313","Insulation diagnosis;impulse test;fault classification;convolution neural network (CNN);deep learning","Convolution;Windings;Kernel;Machine learning;Feature extraction;Neural networks;Fault diagnosis","electrical engineering computing;fault currents;feedforward neural nets;learning (artificial intelligence);pattern classification;transformer windings","deep learning framework;convolution neural network;impulse test;fault current patterns;transformer winding;fault identification;feature extraction;impulse fault pattern classification;fault localization;feature learning","","2","13","","","","","IEEE","IEEE Journals"
"R-VCANet: A New Deep-Learning-Based Hyperspectral Image Classification Method","B. Pan; Z. Shi; X. Xu","Image Processing Center and the State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; Image Processing Center and the State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; Image Processing Center and the State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","5","1975","1986","Deep-learning-based methods have displayed promising performance for hyperspectral image (HSI) classification, due to their capacity of extracting deep features from HSI. However, these methods usually require a large number of training samples. It is quite difficult for deep-learning model to provide representative feature expression for HSI data when the number of samples are limited. In this paper, a novel simplified deep-learning model, rolling guidance filter (RGF) and vertex component analysis network (R-VCANet), is proposed, which achieves higher accuracy when the number of training samples is not abundant. In R-VCANet, the inherent properties of HSI data, spatial information and spectral characteristics, are utilized to construct the network. And by this means the obtained model could generate more powerful feature expression with less samples. First, spectral and spatial information are combined via the RGF, which could explore the contextual structure features and remove small details from HSI. More importantly, we have designed a new network called vertex component analysis network for deep features extraction from the smoothed HSI. Experiments on three popular datasets indicate that the proposed R-VCANet based method reveals better performance than some state-of-the-art methods, especially when the training samples available are not abundant.","","","10.1109/JSTARS.2017.2655516","National Natural Science Foundation of China; Beijing Natural Science Foundation; State Key Laboratory of Virtual Reality Technology and Systems; Beihang University; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7855674","Deep learning;hyperspectral image (HSI) classification;limited samples;rolling guidance filter (RGF) and vertex component analysis network (R-VCANet)","Feature extraction;Training;Hyperspectral imaging;Data mining;Principal component analysis;Convolution","feature extraction;hyperspectral imaging;image classification;image filtering;learning (artificial intelligence);spectral analysis","R-VCANet;deep-learning-based hyperspectral image classification method;HSI classification;deep feature extraction;representative feature expression;rolling guidance filter;RGF;vertex component analysis network;spatial information;spectral characteristics;spectral information;contextual structure feature","","61","41","","","","","IEEE","IEEE Journals"
"Data-Driven Intelligent Efficient Synaptic Storage for Deep Learning","J. Edstrom; Y. Gong; D. Chen; J. Wang; N. Gong","Department of Electrical and Computer Engineering, North Dakota State University, Fargo, ND, USA; Department of Electrical and Computer Engineering, North Dakota State University, Fargo, ND, USA; Department of Electrical and Computer Engineering, North Dakota State University, Fargo, ND, USA; Department of Electrical and Computer Engineering, North Dakota State University, Fargo, ND, USA; Department of Electrical and Computer Engineering, North Dakota State University, Fargo, ND, USA","IEEE Transactions on Circuits and Systems II: Express Briefs","","2017","64","12","1412","1416","With the availability of big data and advanced hardware technologies, deep learning has been applied in different applications, such as self-driving cars and face recognition. While considering various sources of uncertainty and balancing multiple objectives of a system, the hardware implementation of deep learning needs continuous model updating and intensive synaptic weight storage, and SRAM is critical for the overall performance and energy efficiency. In this brief, we introduce offline data mining to the hardware design process, and the discovered data knowledge combined with a data-driven hardware design technique enables a more intelligent memory with better tradeoff between energy efficiency, cost, and classification accuracy, thereby helping relieve the huge burden of data storage in deep learning systems. A 45 nm 64 kbits (256 words × 256 bits) synaptic SRAM is presented that enables 45.6% active power saving and 83.2% leakage power saving, with low implementation cost (3.17%) and less than 1% degradation in classification accuracy.","","","10.1109/TCSII.2017.2767900","National Science Foundation; National Science Foundation; ND NASA EPSCoR; ND Venture Grant; NDSU-RCA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8089410","Deep learning;SRAM;data-driven;data mining;power efficient;synaptic;self-correction","SRAM chips;Neural networks;Machine learning;Artificial neural networks;Memory management;Data mining","Big Data;data mining;learning (artificial intelligence);SRAM chips","energy efficiency;offline data mining;hardware design process;discovered data knowledge;hardware design technique;intelligent memory;data storage;deep learning systems;synaptic SRAM;data-driven intelligent efficient synaptic storage;big data;advanced hardware technologies;self-driving cars;face recognition;balancing multiple objectives;hardware implementation;continuous model;intensive synaptic weight storage;classification accuracy","","2","13","","","","","IEEE","IEEE Journals"
"Deep-Reinforcement-Learning-Based Optimization for Cache-Enabled Opportunistic Interference Alignment Wireless Networks","Y. He; Z. Zhang; F. R. Yu; N. Zhao; H. Yin; V. C. M. Leung; Y. Zhang","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; Beijing Advanced Innovation Center for Future Internet Technology, Beijing University of Technology, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Beijing Advanced Innovation Center for Future Internet Technology, Beijing University of Technology, Beijing, China","IEEE Transactions on Vehicular Technology","","2017","66","11","10433","10445","Both caching and interference alignment (IA) are promising techniques for next-generation wireless networks. Nevertheless, most of the existing works on cache-enabled IA wireless networks assume that the channel is invariant, which is unrealistic considering the time-varying nature of practical wireless environments. In this paper, we consider realistic time-varying channels. Specifically, the channel is formulated as a finite-state Markov channel (FSMC). The complexity of the system is very high when we consider realistic FSMC models. Therefore, in this paper, we propose a novel deep reinforcement learning approach, which is an advanced reinforcement learning algorithm that uses a deep Q network to approximate the Q value-action function. We use Google TensorFlow to implement deep reinforcement learning in this paper to obtain the optimal IA user selection policy in cache-enabled opportunistic IA wireless networks. Simulation results are presented to show that the performance of cache-enabled opportunistic IA networks in terms of the network's sum rate and energy efficiency can be significantly improved by using the proposed approach.","","","10.1109/TVT.2017.2751641","Xinghai Scholars Program; Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036259","Caching;interference alignment;deep reinforcement learning","Learning (artificial intelligence);Transmitters;Wireless networks;Interference;Receivers;Time-varying channels","learning (artificial intelligence);Markov processes;next generation networks;optimisation;radiofrequency interference;telecommunication computing;time-varying channels;wireless channels","next-generation wireless networks;finite-state Markov channel;IA user selection policy;IA wireless networks;deep-reinforcement-learning-based optimization;cache-enabled opportunistic interference alignment wireless networks;Google TensorFlow","","47","38","Traditional","","","","IEEE","IEEE Journals"
"Automatic Modulation Classification Using Deep Learning Based on Sparse Autoencoders With Nonnegativity Constraints","A. Ali; F. Yangyu","School of Electronics and Information Engineering, Northwestern Polytechnical University, Xian, China; School of Electronics and Information Engineering, Northwestern Polytechnical University, Xian, China","IEEE Signal Processing Letters","","2017","24","11","1626","1630","We demonstrate a novel method for the automatic modulation classification based on a deep learning autoencoder network, trained by a nonnegativity constraint algorithm. The learning algorithm aims to constrain the negative weights, learns features that amount to a part-based representation of data, and disentangles a more meaningful hidden structure. The performance of this algorithm is tested on the fourth-order cumulants of the modulated signals. The results indicate that the autoencoder with nonnegativity constraint (ANC) improves the sparsity and minimizes the reconstruction error in comparison with the conventional sparse autoencoder. The classification accuracy of an ANC based deep network shows improved accuracy under limited signal length and fading channel.","","","10.1109/LSP.2017.2752459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8038046","Autoencoder;automatic modulation classification;cumulants;deep learning networks;nonnegativity constraints","Modulation;Training;Signal processing algorithms;Feature extraction;Machine learning;Cost function;Fading channels","higher order statistics;learning (artificial intelligence);modulation;signal classification;signal reconstruction","automatic modulation classification;sparse autoencoders;deep learning autoencoder network;negative weights;part-based data representation;autoencoder with nonnegativity constraint;ANC;fourth-order cumulants;modulated signals;reconstruction error minimization;fading channel;signal length","","12","25","Traditional","","","","IEEE","IEEE Journals"
"Visualization of Driving Behavior Based on Hidden Feature Extraction by Using Deep Learning","H. Liu; T. Taniguchi; Y. Tanaka; K. Takenaka; T. Bando","Graduate School of Information Science and Engineering, Ritsumeikan University, Kusatsu, Japan; College of Information Science and Engineering, Ritsumeikan University, Kusatsu, Japan; Sensing System Research and Development Department, Corporate Research and Development Division1, DENSO Corporation, Aichi, Japan; Sensing System Research and Development Department, Corporate Research and Development Division1, DENSO Corporation, Aichi, Japan; DENSO International America, Inc., San Jose, CA, USA","IEEE Transactions on Intelligent Transportation Systems","","2017","18","9","2477","2489","In this paper, we propose a visualization method for driving behavior that helps people to recognize distinctive driving behavior patterns in continuous driving behavior data. Driving behavior can be measured using various types of sensors connected to a control area network. The measured multi-dimensional time series data are called driving behavior data. In many cases, each dimension of the time series data is not independent of each other in a statistical sense. For example, accelerator opening rate and longitudinal acceleration are mutually dependent. We hypothesize that only a small number of hidden features that are essential for driving behavior are generating the multivariate driving behavior data. Thus, extracting essential hidden features from measured redundant driving behavior data is a problem to be solved to develop an effective visualization method for driving behavior. In this paper, we propose using deep sparse autoencoder (DSAE) to extract hidden features for visualization of driving behavior. Based on the DSAE, we propose a visualization method called a driving color map by mapping the extracted 3-D hidden feature to the red green blue (RGB) color space. A driving color map is produced by placing the colors in the corresponding positions on the map. The subjective experiment shows that feature extraction method based on the DSAE is effective for visualization. In addition, its performance is also evaluated numerically by using pattern recognition method. We also provide examples of applications that use driving color maps in practical problems. In summary, it is shown the driving color map based on DSAE facilitates better visualization of driving behavior.","","","10.1109/TITS.2017.2649541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839988","Data visualization;deep learning;driving behavior analysis;feature extraction","Feature extraction;Data visualization;Image color analysis;Sensors;Data mining;Machine learning;Time series analysis","data visualisation;driver information systems;feature extraction;image colour analysis;image recognition;learning (artificial intelligence);time series","driving behavior visualization;hidden feature extraction;deep learning;driving behavior patterns recognition;time series data;driving behavior data;deep sparse autoencoder;DSAE;red green blue color space;RGB color space;driving color map","","15","30","","","","","IEEE","IEEE Journals"
"Evaluating the Visualization of What a Deep Neural Network Has Learned","W. Samek; A. Binder; G. Montavon; S. Lapuschkin; K. Müller","Fraunhofer Heinrich Hertz Institute, Berlin, Germany; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore; Technische Universität Berlin, Berlin, Germany; Fraunhofer Heinrich Hertz Institute, Berlin, Germany; Technische Universität Berlin, Berlin, Germany","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","11","2660","2673","Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.","","","10.1109/TNNLS.2016.2599820","Brain Korea 21 Plus Program through the National Research Foundation of Korea Funded by the Ministry of Education; DFG; German Ministry for Education and Research as Berlin Big Data Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552539","Convolutional neural networks;explaining classification;image classification;interpretable machine learning;relevance models","Heating;Neurons;Biological neural networks;Deconvolution;Sensitivity;Learning systems;Algorithm design and analysis","data visualisation;image classification;learning (artificial intelligence);neural nets","sensitivity-based approach;deconvolution method;heatmap;deep neural network;complex machine learning tasks;multilayer nonlinear structure;DNN;MIT Places data sets;SUN397;ILSVRC2012;relevance propagation algorithm;data visualization","","34","42","Traditional","","","","IEEE","IEEE Journals"
"Unsupervised t-Distributed Video Hashing and Its Deep Hashing Extension","Y. Hao; T. Mu; J. Y. Goulermas; J. Jiang; R. Hong; M. Wang","School of Computer and Information, Hefei University of Technology, Hefei, China; School of Computer Science, The University of Manchester, Manchester, U.K.; Department of Computer Science, University of Liverpool, Liverpool, U.K.; School of Computer and Information, Hefei University of Technology, Hefei, China; School of Computer and Information, Hefei University of Technology, Hefei, China; School of Computer and Information, Hefei University of Technology, Hefei, China","IEEE Transactions on Image Processing","","2017","26","11","5531","5544","In this paper, a novel unsupervised hashing algorithm, referred to as t-USMVH, and its extension to unsupervised deep hashing, referred to as t-UDH, are proposed to support large-scale video-to-video retrieval. To improve robustness of the unsupervised learning, the t-USMVH combines multiple types of feature representations and effectively fuses them by examining a continuous relevance score based on a Gaussian estimation over pairwise distances, and also a discrete neighbor score based on the cardinality of reciprocal neighbors. To reduce sensitivity to scale changes for mapping objects that are far apart from each other, Student t-distribution is used to estimate the similarity between the relaxed hash code vectors for keyframes. This results in more accurate preservation of the desired unsupervised similarity structure in the hash code space. By adapting the corresponding optimization objective and constructing the hash mapping function via a deep neural network, we develop a robust unsupervised training strategy for a deep hashing network. The efficiency and effectiveness of the proposed methods are evaluated on two public video collections via comparisons against multiple classical and the state-of-the-art methods.","","","10.1109/TIP.2017.2737329","National 973 Program of China; China Scholarship Council; International Research Base for Developing Innovative Gerontechnology sponsored by the National “111” Project of China; National Nature Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8003492","Video retrieval;hashing;deep neural network;multi-view learning;unsupervised learning;Student t-distribution","Feature extraction;Visualization;Training;Neural networks;Robustness;Machine learning;Computational modeling","cryptography;neural nets;unsupervised learning;video signal processing","unsupervised t-distributed video hashing;deep hashing extension;t-USMVH;t-UDH;large-scale video-to-video retrieval;unsupervised learning;Gaussian estimation;reciprocal neighbor cardinality;discrete neighbor score;relaxed hash code vectors;hash code space;optimization objective;hash mapping function;deep neural network;public video collections","","6","71","","","","","IEEE","IEEE Journals"
"Blind Deep S3D Image Quality Evaluation via Local to Global Feature Aggregation","H. Oh; S. Ahn; J. Kim; S. Lee","Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Image Processing","","2017","26","10","4923","4936","Previously, no-reference (NR) stereoscopic 3D (S3D) image quality assessment (IQA) algorithms have been limited to the extraction of reliable hand-crafted features based on an understanding of the insufficiently revealed human visual system or natural scene statistics. Furthermore, compared with full-reference (FR) S3D IQA metrics, it is difficult to achieve competitive quality score predictions using the extracted features, which are not optimized with respect to human opinion. To cope with this limitation of the conventional approach, we introduce a novel deep learning scheme for NR S3D IQA in terms of local to global feature aggregation. A deep convolutional neural network (CNN) model is trained in a supervised manner through two-step regression. First, to overcome the lack of training data, local patch-based CNNs are modeled, and the FR S3D IQA metric is used to approximate a reference ground-truth for training the CNNs. The automatically extracted local abstractions are aggregated into global features by inserting an aggregation layer in the deep structure. The locally trained model parameters are then updated iteratively using supervised global labeling, i.e., subjective mean opinion score (MOS). In particular, the proposed deep NR S3D image quality evaluator does not estimate the depth from a pair of S3D images. The S3D image quality scores predicted by the proposed method represent a significant improvement over those of previous NR S3D IQA algorithms. Indeed, the accuracy of the proposed method is competitive with FR S3D IQA metrics, having ~ 91% correlation in terms of MOS.","","","10.1109/TIP.2017.2725584","National Research Foundation of Korea through the Korean Government (MSIP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973187","Stereoscopic 3D;no-reference image quality assessment;deep learning;convolutional neural network;local feature aggregation","Feature extraction;Image quality;Measurement;Two dimensional displays;Visualization;Three-dimensional displays;Machine learning","feature extraction;learning (artificial intelligence);neural nets;regression analysis;stereo image processing","no-reference stereoscopic 3D image quality assessment algorithms;no-reference S3D IQA algorithms;reliable hand-crafted features;human visual system;natural scene statistics;full-reference S3D IQA metrics;competitive quality score predictions;deep learning scheme;NR S3D IQA;local to global feature aggregation;deep convolutional neural network model;deep CNN model;two-step regression;local patch-based CNN;reference ground-truth;aggregation layer;locally trained model parameters;supervised global labeling;subjective mean opinion score;MOS;deep NR S3D image quality evaluator;S3D image quality scores","","13","72","","","","","IEEE","IEEE Journals"
"Detecting Cardiovascular Disease from Mammograms With Deep Learning","J. Wang; H. Ding; F. A. Bidgoli; B. Zhou; C. Iribarren; S. Molloi; P. Baldi","Department of Computer Science, Institute for Genomics and Bioinformatics, University of California at Irvine, Irvine, CA, USA; Department of Radiological Sciences, University of California at Irvine, Irvine, CA, USA; Department of Radiological Sciences, University of California at Irvine, Irvine, CA, USA; Department of Radiological Sciences, University of California at Irvine, Irvine, CA, USA; Kaiser Permanente Northern California Division of Research, Oakland, CA, USA; Department of Radiological Sciences, University of California at Irvine, Irvine, CA, USA; Department of Computer Science, Institute for Genomics and Bioinformatics, University of California at Irvine, Irvine, CA, USA","IEEE Transactions on Medical Imaging","","2017","36","5","1172","1181","Coronary artery disease is a major cause of death in women. Breast arterial calcifications (BACs), detected in mammograms, can be useful risk markers associated with the disease. We investigate the feasibility of automated and accurate detection of BACs in mammograms for risk assessment of coronary artery disease. We develop a 12-layer convolutional neural network to discriminate BAC from non-BAC and apply a pixelwise, patch-based procedure for BAC detection. To assess the performance of the system, we conduct a reader study to provide ground-truth information using the consensus of human expert radiologists. We evaluate the performance using a set of 840 full-field digital mammograms from 210 cases, using both free-response receiver operating characteristic (FROC) analysis and calcium mass quantification analysis. The FROC analysis shows that the deep learning approach achieves a level of detection similar to the human experts. The calcium mass quantification analysis shows that the inferred calcium mass is close to the ground truth, with a linear regression between them yielding a coefficient of determination of 96.24%. Taken together, these results suggest that deep learning can be used effectively to develop an automated system for BAC detection in mammograms to help identify and assess patients with cardiovascular risks.","","","10.1109/TMI.2017.2655486","National Science Foundation; Google Faculty Research Award under Grant; National Heart, Lung, and Blood Institute (Bethesda, MD) to CI and SM; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827150","Breast arterial calcification (BAC);coronary artery disease;deep learning;mammography","Mammography;Diseases;Arteries;Machine learning;Neural networks;Calcium;Breast","cardiovascular system;diseases;learning (artificial intelligence);mammography;medical image processing;neural nets","cardiovascular disease detection;deep learning;coronary artery disease;death;breast arterial calcifications;12-layer convolutional neural network;patch based procedure;BAC detection;digital mammograms;FROC analysis;calcium mass quantification analysis","Calcinosis;Cardiovascular Diseases;Coronary Artery Disease;Female;Humans;Mammography;Neural Networks (Computer)","43","53","","","","","IEEE","IEEE Journals"
"Deep Learning Based Approach for Bearing Fault Diagnosis","M. He; D. He","Department of Mechanical and Industrial Engineering, University of Illinois at Chicago, Chicago, IL, USA; College of Mechanical Engineering and Automation, Northeastern University, Shenyang, China","IEEE Transactions on Industry Applications","","2017","53","3","3057","3065","Bearing is one of the most critical components in most electrical and power drives. Effective bearing fault diagnosis is important for keeping the electrical and power drives safe and operating normally. In the age of Internet of Things and Industrial 4.0, massive real-time data are collected from bearing health monitoring systems. Mechanical big data have the characteristics of large volume, diversity, and high velocity. There are two major problems in using the existing methods for bearing fault diagnosis with big data. The features are manually extracted relying on much prior knowledge about signal processing techniques and diagnostic expertise, and the used models have shallow architectures, limiting their capability in fault diagnosis. Effectively mining features from big data and accurately identifying the bearing health conditions with new advanced methods have become new issues. This paper presents a deep learning-based approach for bearing fault diagnosis. The presented approach preprocesses sensor signals using short-time Fourier transform (STFT). Based on a simple spectrum matrix obtained by STFT, an optimized deep learning structure, large memory storage retrieval (LAMSTAR) neural network, is built to diagnose the bearing faults. Acoustic emission signals acquired from a bearing test rig are used to validate the presented method. The validation results show the accurate classification performance on various bearing faults under different working conditions. The performance of the presented method is also compared with other effective bearing fault diagnosis methods reported in the literature. The comparison results have shown that the presented method gives much better diagnostic performance, even at relatively low rotating speeds.","","","10.1109/TIA.2017.2661250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836314","Acoustic emission (AE) sensor;bearing fault diagnosis;deep learning;large memory storage retrieval (LAMSTAR) neural network","Fault diagnosis;Neurons;Machine learning;Feature extraction;Machinery;Big data;Signal processing","acoustic signal processing;fault diagnosis;learning (artificial intelligence);machine bearings;mechanical engineering computing","deep learning based approach;bearing fault diagnosis;sensor signal preprocessing;short-time Fourier transform;STFT;spectrum matrix;large memory storage retrieval;LAMSTAR neural network;acoustic emission signals","","61","28","","","","","IEEE","IEEE Journals"
"BiLoc: Bi-Modal Deep Learning for Indoor Localization With Commodity 5GHz WiFi","X. Wang; L. Gao; S. Mao","Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA; DataYes, Inc., Shanghai, China; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA","IEEE Access","","2017","5","","4209","4220","In this paper, we study fingerprinting-based indoor localization in commodity 5-GHz WiFi networks. We first theoretically and experimentally validate three hypotheses on the channel state information (CSI) data of 5-GHz OFDM channels. We then propose a system termed BiLoc, which uses bi-modality deep learning for localization in the indoor environment using off-the-shelf WiFi devices. We develop a deep learning-based algorithm to exploit bi-modal data, i.e., estimated angle of arrivings and average amplitudes (which are calibrated CSI data using several proposed techniques), for both the off-line and online stages of indoor fingerprinting. The proposed BiLoc system is implemented using commodity WiFi devices. Its superior performance is validated with extensive experiments under three typical indoor environments and through comparison with three benchmark schemes.","","","10.1109/ACCESS.2017.2688362","U.S. National Science Foundation; Wireless Engineering Research and Education Center, Auburn University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888438","Indoor localization;fingerprinting;deep learning;5GHz commodity WiFi;channel state information;bi-modality fingerprinting","Antennas;Wireless fidelity;OFDM;Feature extraction;Data mining;Machine learning;Antenna measurements","indoor radio;learning (artificial intelligence);OFDM modulation;telecommunication computing;wireless channels;wireless LAN","indoor localization;bi-modal deep learning;BiLoc;WiFi networks;channel state information;OFDM channels;indoor fingerprinting;frequency 5 GHz","","46","38","","","","","IEEE","IEEE Journals"
"Deep Video Hashing","V. E. Liong; J. Lu; Y. Tan; J. Zhou","Interdisciplinary Graduate School, Rapid-Rich Object Search Laboratory, Nanyang Technological University, Singapore; Department of Automation, State Key Laboratory of Intelligent Technologies and Systems, Tsinghua University, Beijing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Automation, State Key Laboratory of Intelligent Technologies and Systems, Tsinghua University, Beijing, China","IEEE Transactions on Multimedia","","2017","19","6","1209","1219","In this work, we propose a deep video hashing (DVH) method for scalable video search. Unlike most existing video hashing methods that first extract features for each single frame and then use conventional image hashing techniques, our DVH learns binary codes for the entire video with a deep learning framework so that both the temporal and discriminative information can be well exploited. Specifically, we fuse the temporal information across different frames within each video to learn the feature representation under two criteria: the distance between a feature pair obtained at the top layer is small if they are from the same class, and large if they are from different classes; and the quantization loss between the real-valued features and the binary codes is minimized. We exploit different deep architectures to utilize spatial-temporal information in different manners and compare them with single-frame-based deep models and state-of-the-art image hashing methods. Experimental results demonstrate the effectiveness of our proposed method.","","","10.1109/TMM.2016.2645404","National Key Research and Development Program of China; National Natural Science Foundation of China; National 1000 Young Talents Plan Program; MSRA Collaborative Research Program; National Basic Research Program of China; Ministry of Education of China; Tsinghua University Initiative Scientific Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797446","Deep learning;scalable video search;video hashing","Feature extraction;Binary codes;Visualization;Convolutional codes;Machine learning;Quantization (signal);Computer architecture","binary codes;file organisation;image representation;learning (artificial intelligence);video retrieval","deep video hashing;scalable video search;DVH;binary codes;deep learning;discriminative information;feature representation;feature pair;real-valued features;deep architectures;spatial-temporal information","","21","71","","","","","IEEE","IEEE Journals"
"Age Groups Classification in Social Network Using Deep Learning","R. G. Guimarães; R. L. Rosa; D. De Gaetano; D. Z. Rodríguez; G. Bressan","Federal University of Lavras, Lavras, Brazil; Polytechnic School, University of São Paulo, São Paulo, Brazil; University of Ulster, Coleraine, Northern Ireland; Federal University of Lavras, Lavras, Brazil; Polytechnic School, University of São Paulo, São Paulo, Brazil","IEEE Access","","2017","5","","10805","10816","Social networks have a large amount of data available, but often, people do not provide some of their personal data, such as age, gender, and other demographics. Although the sentiment analysis uses such data to develop useful applications in people's daily lives, there are still failures in this type of analysis, either by the restricted number of words contained in the word dictionaries or because they do not consider the most diverse parameters that can influence the sentiments in a sentence; thus, more reliable results can be obtained, if the users profile information and their writing characteristics are considered. This research suggests that one of the most relevant parameter contained in the user profile is the age group, showing that there are typical behaviors among users of the same age group, specifically, when these users write about the same topic. A detailed analysis with 7000 sentences was performed to determine which characteristics are relevant, such as, the use of punctuation, number of characters, media sharing, topics, among others; and which ones can be disregarded for the age groups classification. Different learning machine algorithms are tested for the classification of the teenager and adult age group, and the deep convolutional neural network had the best performance, reaching a precision of 0.95 in the validation tests. Furthermore, in order to validate the usefulness of the proposed model for classifying age groups, it is implemented into the enhanced sentiment metric (eSM). In the performance validation, subjective tests are performed and the eSM with the proposed model reached a root mean square error and a Pearson correlation coefficient of 0.25 and 0.94, respectively, outperforming the eSM metric, when the age group information is not available.","","","10.1109/ACCESS.2017.2706674","University of São Paulo; Federal University of Lavras; Minas Gerais State Agency for Research and Development (FAPEMIG); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932459","Social network services;sentiment analysis;machine learning;text analysis;artificial neural networks;deep networ","Sentiment analysis;Measurement;Twitter;Writing;Media;Neural networks","age issues;dictionaries;feedforward neural nets;learning (artificial intelligence);pattern classification;sentiment analysis;social networking (online);statistics","age groups classification;social network;deep learning;sentiment analysis;word dictionaries;users profile information;writing characteristics;typical behaviors;learning machine algorithms;teenager;adult age group;deep convolutional neural network;enhanced sentiment metric;eSM;subjective tests;root mean square error;Pearson correlation coefficient","","19","67","","","","","IEEE","IEEE Journals"
"Deep Learning on Sparse Manifolds for Faster Object Segmentation","J. C. Nascimento; G. Carneiro","Instituto de Sistemas e Rob&#x00F3;tica, Instituto Superior T&#x00E9;cnico, Lisboa, Portugal; School of Computer ScienceThe University of Adelaide","IEEE Transactions on Image Processing","","2017","26","10","4978","4990","We propose a new combination of deep belief networks and sparse manifold learning strategies for the 2D segmentation of non-rigid visual objects. With this novel combination, we aim to reduce the training and inference complexities while maintaining the accuracy of machine learning-based non-rigid segmentation methodologies. Typical non-rigid object segmentation methodologies divide the problem into a rigid detection followed by a non-rigid segmentation, where the low dimensionality of the rigid detection allows for a robust training (i.e., a training that does not require a vast amount of annotated images to estimate robust appearance and shape models) and a fast search process during inference. Therefore, it is desirable that the dimensionality of this rigid transformation space is as small as possible in order to enhance the advantages brought by the aforementioned division of the problem. In this paper, we propose the use of sparse manifolds to reduce the dimensionality of the rigid detection space. Furthermore, we propose the use of deep belief networks to allow for a training process that can produce robust appearance models without the need of large annotated training sets. We test our approach in the segmentation of the left ventricle of the heart from ultrasound images and lips from frontal face images. Our experiments show that the use of sparse manifolds and deep belief networks for the rigid detection stage leads to segmentation results that are as accurate as the current state of the art, but with lower search complexity and training processes that require a small amount of annotated training data.","","","10.1109/TIP.2017.2725582","FCT; Australian Research Council’s Discovery Projects funding scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973159","Deep belief netwolks;defonnable objects;non-rigid segmentation;sparse manifold","Training;Image segmentation;Visualization;Shape;Search problems;Manifolds;Robustness","belief networks;compressed sensing;image segmentation;learning (artificial intelligence)","deep learning;sparse manifold learning strategy;nonrigid object segmentation methodologies;deep belief networks;2D segmentation;nonrigid visual objects;machine learning-based nonrigid segmentation methodologies;annotated training sets;ultrasound images","","3","70","","","","","IEEE","IEEE Journals"
"Recursive Autoencoders-Based Unsupervised Feature Learning for Hyperspectral Image Classification","X. Zhang; Y. Liang; C. Li; N. Huyan; L. Jiao; H. Zhou","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronics, Electrical Engineering and Computer Science, Queen’s University of Belfast, Belfast, U.K.","IEEE Geoscience and Remote Sensing Letters","","2017","14","11","1928","1932","For hyperspectral image (HSI) classification, it is very important to learn effective features for the discrimination purpose. Meanwhile, the ability to combine spectral and spatial information together in a deep level is also important for feature learning. In this letter, we propose an unsupervised feature learning method for HSI classification, which is based on recursive autoencoders (RAE) network. RAE utilizes the spatial and spectral information and produces high-level features from the original data. It learns features from the neighborhood of the investigated pixel to represent the whole local homogeneous area of the image. In addition, to obtain more accurate representation of the investigated pixel, a weighting scheme is adopted based on the neighboring pixels, where the weights are determined by the spectral similarity between the neighboring pixels and the investigated pixel. The effectiveness of our method is evaluated by the experiments on two hyperspectral data sets, and the results show that our proposed method has a better performance.","","","10.1109/LGRS.2017.2737823","National Natural Science Foundation of China; Program for New Scientific and Technological Star of Shaanxi Province; UK EPSRC; Royal Society-Newton Advanced Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8065033","Deep learning;hyperspectral image (HSI) classification;recursive autoencoders (RAE);unsupervised feature learning","Hyperspectral imaging;Learning systems;Neurons;Support vector machines;Machine learning;Feature extraction","feature extraction;hyperspectral imaging;image classification;image representation;unsupervised learning","hyperspectral image classification;discrimination purpose;spectral information;spatial information;deep level;unsupervised feature learning method;HSI classification;recursive autoencoders network;high-level features;local homogeneous area;neighboring pixels;spectral similarity;hyperspectral data sets;RAE network;weighting scheme","","8","27","Traditional","","","","IEEE","IEEE Journals"
"Training Deep Convolutional Neural Networks for Land–Cover Classification of High-Resolution Imagery","G. J. Scott; M. R. England; W. A. Starms; R. A. Marcum; C. H. Davis","Center for Geospatial Intelligence, University of Missouri-Columbia, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri-Columbia, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri-Columbia, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri-Columbia, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri-Columbia, Columbia, MO, USA","IEEE Geoscience and Remote Sensing Letters","","2017","14","4","549","553","Deep convolutional neural networks (DCNNs) have recently emerged as a dominant paradigm for machine learning in a variety of domains. However, acquiring a suitably large data set for training DCNN is often a significant challenge. This is a major issue in the remote sensing domain, where we have extremely large collections of satellite and aerial imagery, but lack the rich label information that is often readily available for other image modalities. In this letter, we investigate the use of DCNN for land-cover classification in high-resolution remote sensing imagery. To overcome the lack of massive labeled remote-sensing image data sets, we employ two techniques in conjunction with DCNN: transfer learning (TL) with fine-tuning and data augmentation tailored specifically for remote sensing imagery. TL allows one to bootstrap a DCNN while preserving the deep visual feature extraction learned over an image corpus from a different image domain. Data augmentation exploits various aspects of remote sensing imagery to dramatically expand small training image data sets and improve DCNN robustness for remote sensing image data. Here, we apply these techniques to the well-known UC Merced data set to achieve the land-cover classification accuracies of 97.8 ± 2.3%, 97.6 ± 2.6%, and 98.5 ± 1.4% with CaffeNet, GoogLeNet, and ResNet, respectively.","","","10.1109/LGRS.2017.2657778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858676","Deep convolutional neural network (DCNN);deep learning;high-resolution remote sensing imagery;land–cover classification;transfer learning (TL)","Iron;Remote sensing;Feature extraction;Training;Visualization;Training data;Neural networks","geophysical image processing;image classification;land cover;neural nets;remote sensing","training deep convolutional neural networks;land-cover classification;high-resolution imagery;machine learning paradigm;satellite imagery;aerial imagery;image modalities;high-resolution remote sensing imagery;massive labeled remote-sensing image;transfer learning;data augmentation;deep visual feature extraction;image domain;training image data sets;UC Merced data set;CaffeNet;GoogLeNet;ResNet","","76","24","","","","","IEEE","IEEE Journals"
"CSI-Based Fingerprinting for Indoor Localization: A Deep Learning Approach","X. Wang; L. Gao; S. Mao; S. Pandey","Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA; Cisco Systems, Inc., San Jose, CA, USA","IEEE Transactions on Vehicular Technology","","2017","66","1","763","776","With the fast-growing demand of location-based services in indoor environments, indoor positioning based on fingerprinting has attracted significant interest due to its high accuracy. In this paper, we present a novel deep-learning-based indoor fingerprinting system using channel state information (CSI), which is termed DeepFi. Based on three hypotheses on CSI, the DeepFi system architecture includes an offline training phase and an online localization phase. In the offline training phase, deep learning is utilized to train all the weights of a deep network as fingerprints. Moreover, a greedy learning algorithm is used to train the weights layer by layer to reduce complexity. In the online localization phase, we use a probabilistic method based on the radial basis function to obtain the estimated location. Experimental results are presented to confirm that DeepFi can effectively reduce location error, compared with three existing methods in two representative indoor environments.","","","10.1109/TVT.2016.2545523","National Science Foundation; Auburn University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7438932","Channel state information (CSI);deep learning;fingerprinting;indoor localization;WiFi","Training;Machine learning;IEEE 802.11 Standard;Indoor environments;Fading channels;Antennas;Complexity theory","greedy algorithms;indoor communication;telecommunication channels;telecommunication security","CSI-based fingerprinting;indoor localization;deep learning approach;location-based services;indoor environments;indoor positioning;channel state information;localization phase;deep network;probabilistic method;DeepFi;representative indoor environments","","189","48","","","","","IEEE","IEEE Journals"
"Domain Adaptation Using Representation Learning for the Classification of Remote Sensing Images","A. Elshamli; G. W. Taylor; A. Berg; S. Areibi","School of Engineering, University of Guelph, Guelph, ON, Canada; School of Engineering, University of Guelph, Guelph, ON, Canada; Department of Geography, University of Guelph, Guelph, ON, Canada; School of Engineering, University of Guelph, Guelph, ON, Canada","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","9","4198","4209","Traditional machine learning (ML) techniques are often employed to perform complex pattern recognition tasks for remote sensing images, such as land-use classification. In order to obtain acceptable classification results, these techniques require there to be sufficient training data available for every particular image. Obtaining training samples is challenging, particularly for near real-time applications. Therefore, past knowledge must be utilized to overcome the lack of training data in the current regime. This challenge is known as domain adaptation (DA), and one of the common approaches to this problem is based on finding invariant representations for both the training and test data, which are often assumed to come from different “domains.” In this study, we consider two deep learning techniques for learning domain-invariant representations: Denoising autoencoders (DAE) and domain-adversarial neural networks (DANN). While the DAE is a typical two-stage DA technique (unsupervised invariant representation learning followed by supervised classification), DANN is an end-to-end approach where invariant representation learning and classification are considered jointly during training. The proposed techniques are applied to both hyperspectral and multispectral images under different DA scenarios. Results obtained show that the proposed techniques outperform traditional approaches, such as principal component analysis (PCA) and kernel PCA, and can also compete with a fully supervised model in the multispatial scenario.","","","10.1109/JSTARS.2017.2711360","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954001","Adversarial neural network;autoencoders (AEs);deep learning;domain adaptation (DA);land-use classification;representation learning","Remote sensing;Training;Principal component analysis;Neural networks;Earth;Agriculture;Machine learning","geophysical image processing;hyperspectral imaging;image classification;image denoising;land use;neural nets;principal component analysis;remote sensing;unsupervised learning","domain adaptation;remote sensing image classification;machine learning technique;pattern recognition task;land use classification;deep learning technique;denoising autoencoder;domain-adversarial neural networks;unsupervised invariant representation learning;supervised classification;hyperspectral image;multispectral image;principal component analysis;kernel PCA","","11","36","Traditional","","","","IEEE","IEEE Journals"
"End-to-End Multimodal Emotion Recognition Using Deep Neural Networks","P. Tzirakis; G. Trigeorgis; M. A. Nicolaou; B. W. Schuller; S. Zafeiriou","Department of Computing, Imperial College London, London, U.K.; Department of Computing, Imperial College London, London, U.K.; Department of Computing, Goldsmiths University of London, London, U.K.; Department of Computing, Imperial College London, London, U.K.; Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany","IEEE Journal of Selected Topics in Signal Processing","","2017","11","8","1301","1309","Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human-computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a convolutional neural network (CNN) to extract features from the speech, while for the visual modality a deep residual network of 50 layers is used. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, long short-term memory networks are utilized. The system is then trained in an end-to-end fashion where-by also taking advantage of the correlations of each of the streams-we manage to significantly outperform, in terms of concordance correlation coefficient, traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition.","","","10.1109/JSTSP.2017.2764438","EPSRC Center for Doctoral Training in High Performance Embedded and Distributed Systems (HiPEDS); Google Fellowship in Machine Perception, Speech Technology and Computer Vision; EU Horizon 2020 Framework Programme; FiDiPro Program of Tekes; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070966","End-to-end learning;emotion recognition;deep learning;affective computing","Feature extraction;Emotion recognition;Affective computing;Machine learning;Hidden Markov models;Neural networks","emotion recognition;feature extraction;human computer interaction;learning (artificial intelligence);neural nets;speech recognition","deep neural networks;automatic affect recognition;emotional states;emotion recognition system;visual modalities;emotional content;convolutional neural network;short-term memory networks;visual handcrafted features;spontaneous emotions;natural emotions;multimedia retrieval;human-computer interaction;visual modality;speech feature extraction;deep residual network;end-to-end multimodal emotion recognition;auditory modality;speaking style;machine learning algorithm;concordance correlation coefficient;RECOLA database","","28","50","Traditional","","","","IEEE","IEEE Journals"
"iPrivacy: Image Privacy Protection by Identifying Sensitive Objects via Deep Multi-Task Learning","J. Yu; B. Zhang; Z. Kuang; D. Lin; J. Fan","UNC Charlotte, Charlotte, NC, USA; UNC Charlotte, Charlotte, NC, USA; UNC Charlotte, Charlotte, NC, USA; Department of Computer Science, Missouri University of Science and Technology, Rolla, MO, USA; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA","IEEE Transactions on Information Forensics and Security","","2017","12","5","1005","1016","To achieve automatic recommendation of privacy settings for image sharing, a new tool called iPrivacy (image privacy) is developed for releasing the burden from users on setting the privacy preferences when they share their images for special moments. Specifically, this paper consists of the following contributions: 1) massive social images and their privacy settings are leveraged to learn the object-privacy relatedness effectively and identify a set of privacy-sensitive object classes automatically; 2) a deep multi-task learning algorithm is developed to jointly learn more representative deep convolutional neural networks and more discriminative tree classifier, so that we can achieve fast and accurate detection of large numbers of privacy-sensitive object classes; 3) automatic recommendation of privacy settings for image sharing can be achieved by detecting the underlying privacy-sensitive objects from the images being shared, recognizing their classes, and identifying their privacy settings according to the object-privacy relatedness; and 4) one simple solution for image privacy protection is provided by blurring the privacy-sensitive objects automatically. We have conducted extensive experimental studies on real-world images and the results have demonstrated both the efficiency and effectiveness of our proposed approach.","","","10.1109/TIFS.2016.2636090","National Science Foundation; National Natural Science Foundation of China; Zhejiang Provincial Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7775034","Image sharing;privacy setting recommendation;object-privacy alignment;image privacy protection;privacy-sensitive object classes;deep multi-task learning;tree classifier for hierarchical object detection","Privacy;Object recognition;Visualization;Object detection;Semantics;Image segmentation;Prediction algorithms","convolution;data protection;image coding;learning (artificial intelligence);neural nets;object detection;trees (mathematics)","iPrivacy;image privacy protection;object detection;deep multitask learning algorithm;object-privacy relatedness;privacy-sensitive object class;deep convolutional neural network;discriminative tree classifier","","128","52","","","","","IEEE","IEEE Journals"
"Learning Framework for Robust Obstacle Detection, Recognition, and Tracking","V. D. Nguyen; H. Van Nguyen; D. T. Tran; S. J. Lee; J. W. Jeon","School of Information and Communication Engineering, Sungkyunkwan University, South Korea; School of Information and Communication Engineering, Sungkyunkwan University, South Korea; School of Information and Communication Engineering, Sungkyunkwan University, South Korea; School of Information and Communication Engineering, Sungkyunkwan University, South Korea; School of Information and Communication Engineering, Sungkyunkwan University, South Korea","IEEE Transactions on Intelligent Transportation Systems","","2017","18","6","1633","1646","This paper introduces a general framework for detection, recognition, and tracking preceding vehicles and pedestrians based on a deep learning approach. The proposed framework combines a novel deep learning approach with the use of multiple sources of local patterns and depth information to yield robust on-road vehicle and pedestrian detection, recognition, and tracking. The proposed system is first based on robust obstacle detection to identify obstacles appearing along the road that are likely to be vehicles and pedestrians, implemented as an efficient adaptive U-V disparity algorithm. Second, the results from the obstacle detection stage are input into a novel vehicle and pedestrian recognition system based on a deep learning model that processes multiple sources of depth information and local patterns. Finally, the results from the recognition stage are used to track detected vehicles or pedestrians in the next frame by means of a proposed tracking and validation model. The proposed framework has been thoroughly evaluated by inputting several vehicle and pedestrian data sets that were collected under various driving conditions. Experimental results show that this framework provides robust vehicle and pedestrian detection, recognition, and tracking with high accuracy, and also satisfies the real-time requirements of driver assistance systems.","","","10.1109/TITS.2016.2614818","Samsung Research Funding Center of Samsung Electronics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7600385","Deep learning;vehicle and pedestrian detection;unsupervised learning","Vehicles;Machine learning;Robustness;Roads;Cameras;Neural networks;Training","learning (artificial intelligence);object recognition;pedestrians","learning framework;robust obstacle detection;robust obstacle recognition;robust obstacle tracking;deep learning approach;robust on-road vehicle detection;robust on-road vehicle recognition;robust on-road vehicle tracking;robust pedestrian detection;robust pedestrian recognition;robust pedestrian tracking;adaptive U-V disparity algorithm;detected vehicle tracking;detected pedestrian tracking;driving conditions;driver assistance systems","","14","38","","","","","IEEE","IEEE Journals"
"Deep Label Distribution Learning With Label Ambiguity","B. Gao; C. Xing; C. Xie; J. Wu; X. Geng","National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; MOE Key Laboratory of Computer Network and Information Integration, School of Computer Science and Engineering, Southeast University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; MOE Key Laboratory of Computer Network and Information Integration, School of Computer Science and Engineering, Southeast University, Nanjing, China","IEEE Transactions on Image Processing","","2017","26","6","2825","2838","Convolutional neural networks (ConvNets) have achieved excellent recognition performance in various visual recognition tasks. A large labeled training set is one of the most important factors for its success. However, it is difficult to collect sufficient training images with precise labels in some domains, such as apparent age estimation, head pose estimation, multilabel classification, and semantic segmentation. Fortunately, there is ambiguous information among labels, which makes these tasks different from traditional classification. Based on this observation, we convert the label of each image into a discrete label distribution, and learn the label distribution by minimizing a Kullback-Leibler divergence between the predicted and ground-truth label distributions using deep ConvNets. The proposed deep label distribution learning (DLDL) method effectively utilizes the label ambiguity in both feature learning and classifier learning, which help prevent the network from overfitting even when the training set is small. Experimental results show that the proposed approach produces significantly better results than the state-of-the-art methods for age estimation and head pose estimation. At the same time, it also improves recognition performance for multi-label classification and semantic segmentation tasks.","","","10.1109/TIP.2017.2689998","National Natural Science Foundation of China; Jiangsu Natural Science Funds for Distinguished Young Scholar; Collaborative Innovation Center of Novel Software Technology and Industrialization; Collaborative Innovation Center of Wireless Communications Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890384","Label distribution;deep learning;age estimation;head pose estimation;semantic segmentation","Head;Training;Pose estimation;Semantics;Image segmentation;Correlation","image classification;image recognition;image segmentation;neural nets;pose estimation","deep label distribution learning;label ambiguity;convolutional neural networks;recognition performance;visual recognition tasks;large-labeled training set;training images;apparent age estimation;head pose estimation;multilabel classification;semantic segmentation;ambiguous information;discrete label distribution;Kullback-Leibler divergence;predicted label distribution;ground-truth label distribution;deep ConvNets;DLDL method;feature learning;classifier learning;semantic segmentation task","","36","52","","","","","IEEE","IEEE Journals"
"Modeling Grasp Motor Imagery Through Deep Conditional Generative Models","M. Veres; M. Moussa; G. W. Taylor","School of Engineering, University of Guelph, Guelph, ON, Canada; School of Engineering, University of Guelph, Guelph, ON, Canada; School of Engineering, University of Guelph, Guelph, ON, Canada","IEEE Robotics and Automation Letters","","2017","2","2","757","764","Grasping is a complex process involving knowledge of the object, the surroundings, and of oneself. While humans are able to integrate and process all of the sensory information required for performing this task, equipping machines with this capability is an extremely challenging endeavor. In this paper, we investigate how deep learning techniques can allow us to translate high-level concepts such as motor imagery to the problem of robotic grasp synthesis. We explore a paradigm based on generative models for learning integrated object-action representations and demonstrate its capacity for capturing and generating multimodal multifinger grasp configurations on a simulated grasping dataset.","","","10.1109/LRA.2017.2651945","Natural Sciences and Engineering Research Council of Canada; Canada Foundation for Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814247","Deep learning;generative models;grasping;multifingered hands;visual learning","Grasping;Robot sensing systems;Machine learning;Encoding;Visualization;Generators","end effectors;learning (artificial intelligence)","grasp motor imagery modeling;deep conditional generative models;deep learning techniques;robotic grasp synthesis;integrated object-action representations;multimodal multifinger grasp configurations","","6","31","","","","","IEEE","IEEE Journals"
"Optimized Deep Learning for EEG Big Data and Seizure Prediction BCI via Internet of Things","M. Hosseini; D. Pompili; K. Elisevich; H. Soltanian-Zadeh","Department of Electrical and Computer Engineering, Rutgers University, New Brunswick, NJ; Department of Electrical and Computer Engineering, Rutgers University, New Brunswick, NJ; Department of Clinical Neurosciences, Spectrum Health, Grand Rapids, MI; Departments of Radiology and Research Administration, Medical Image Analysis Laboratory, Henry Ford Health System, Detroit, MI","IEEE Transactions on Big Data","","2017","3","4","392","404","A brain-computer interface (BCI) for seizure prediction provides a means of controlling epilepsy in medically refractory patients whose site of epileptogenicity cannot be resected but yet can be defined sufficiently to be selectively influenced by strategically implanted electrodes. Challenges remain in offering real-time solutions with such technology because of the immediacy of electrographic ictal behavior. The nonstationary nature of electroencephalographic (EEG) and electrocorticographic (ECoG) signals results in wide variation of both normal and ictal patterns among patients. The use of manually extracted features in a prediction task is impractical and the large amount of data generated even among a limited set of electrode contacts will create significant processing delays. Big data in such circumstances not only must allow for safe storage but provide high computational resources for recognition, capture and real-time processing of the preictal period in order to execute the timely abrogation of the ictal event. By leveraging the potential of cloud computing and deep learning, we develop and deploy BCI seizure prediction and localization from scalp EEG and ECoG big data. First, a new method for epileptic seizure prediction and localization of the seizure focus is presented. Second, an extended optimization approach on existing deep-learning structures, Stacked Auto-encoder and Convolutional Neural Network (CNN), is proposed based on principle component analysis (PCA), independent component analysis (ICA), and Differential Search Algorithm (DSA). Third, a cloud-computing solution (i.e., Internet of Things (IoT)), is developed to define the proposed structures for real-time processing, automatic computing and storage of big data. The ECoG clinical datasets on 11 patients illustrate the superiority of the proposed patient-specific BCI as an alternative to current methodology to offer support for patients with intractable focal epilepsy.","","","10.1109/TBDATA.2017.2769670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094871","BCI;EEG big data;cloud computing;deep learning;epilepsy;seizure prediction and localization","Big Data;Electroencephalography;Cloud computing;Epilepsy;Feature extraction;Real-time systems;Electrodes;Brain-computer interfaces;Predictive models","Big Data;brain-computer interfaces;diseases;electroencephalography;feature extraction;independent component analysis;learning (artificial intelligence);medical signal processing;neural nets;neurophysiology;principal component analysis","patient-specific BCI;ECoG clinical datasets;automatic computing;cloud-computing solution;independent component analysis;extended optimization approach;epileptic seizure prediction;BCI seizure prediction;deep learning;cloud computing;real-time processing;significant processing delays;electrode contacts;prediction task;manually extracted features;ictal patterns;electrographic ictal behavior;real-time solutions;medically refractory patients;brain-computer interface;seizure prediction BCI;EEG big data;optimized deep","","12","60","Traditional","","","","IEEE","IEEE Journals"
"Boosting the Accuracy of Multispectral Image Pansharpening by Learning a Deep Residual Network","Y. Wei; Q. Yuan; H. Shen; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Resource and Environmental Science, Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, the Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","10","1795","1799","In the field of multispectral (MS) and panchromatic image fusion (pansharpening), the impressive effectiveness of deep neural networks has recently been employed to overcome the drawbacks of the traditional linear models and boost the fusion accuracy. However, the existing methods are mainly based on simple and flat networks with relatively shallow architectures, which severely limits their performance. In this letter, the concept of residual learning is introduced to form a very deep convolutional neural network to make the full use of the high nonlinearity of the deep learning models. Through both quantitative and visual assessments on a large number of high-quality MS images from various sources, it is confirmed that the proposed model is superior to all the mainstream algorithms included in the comparison, and achieves the highest spatial-spectral unified accuracy.","","","10.1109/LGRS.2017.2736020","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Natural Science Foundation of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8012503","Convolutional neural network;data fusion;pansharpening;remote sensing;residual learning","Machine learning;Neural networks;Spectral analysis;Spatial resolution;Training;Feature extraction","convolution;image colour analysis;image fusion;image resolution;learning (artificial intelligence);neural nets","multispectral image pansharpening;deep residual network;panchromatic image fusion;residual learning;deep convolutional neural network;image super-resolution","","32","24","Traditional","","","","IEEE","IEEE Journals"
"Deep-learning-based license plate detection method using vehicle region extraction","S. G. Kim; H. G. Jeon; H. I. Koo","Ajou University, Korea; Ajou University, Korea; Ajou University, Korea","Electronics Letters","","2017","53","15","1034","1036","A new license plate detection method for challenging environments is proposed. Background clutters are common in road scene images and the detection of license plates (occupying only a small part of an image) is considered as a difficult problem. In order to address this problem, a two-step approach is developed: first vehicle regions are detected and the license plate in each vehicle region is localised. This vehicle region detection based approach provides scale information and limits search ranges in license plate detection, so that one can reliably detect license plate regions. To be precise, the faster region-based convolutional neural network algorithm for the vehicle region detection is adopted and candidates for license plates in each detected region with the hierarchical sampling method are generated. Finally, non-plate candidates are filtered out by training a deep convolutional neural network. The proposed method is evaluated on the Caltech dataset and the method showed a precision of 98.39% and a recall of 96.83%, which outperforms conventional methods.","","","10.1049/el.2017.1373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990396","","","automobiles;feature extraction;feedforward neural nets;learning (artificial intelligence);object detection;traffic engineering computing","vehicle region extraction;background clutters;road scene images;two-step approach;license plate localization;vehicle region detection based approach;scale information;deep-learning-based license plate detection;Caltech dataset;deep convolutional neural network training;nonplate candidate filtering;hierarchical sampling method;region-based convolutional neural network algorithm;search ranges","","6","15","","","","","IET","IET Journals"
"A Deep Learning Approach for Blind Drift Calibration of Sensor Networks","Y. Wang; A. Yang; X. Chen; P. Wang; Y. Wang; H. Yang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Sensors Journal","","2017","17","13","4158","4171","Temporal drift of sensory data is a severe problem impacting the data quality of wireless sensor networks (WSNs). With the proliferation of large-scale and long-term WSNs, it is becoming more important to calibrate sensors when the ground truth is unavailable. This problem is called ""blind calibration"". In this paper, we propose a novel deep learning method named projection-recovery network (PRNet) to blindly calibrate sensor measurements online. The PRNet first projects the drifted data to a feature space, and uses a powerful deep convolutional neural network to recover the estimated driftfree measurements. We deploy a 24-sensor testbed and provide comprehensive empirical evidence showing that the proposed method significantly improves the sensing accuracy and drifted sensor detection. Compared with previous methods, PRNet can calibrate 2× of drifted sensors at the recovery rate of 80% under the same level of accuracy requirement. We also provide helpful insights for designing deep neural networks for sensor calibration. We hope our proposed simple and effective approach will serve as a solid baseline in blind drift calibration of sensor networks.","","","10.1109/JSEN.2017.2703885","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927400","Sensor networks;blind calibration;deep learning;convolutional neural networks","Sensors;Calibration;Wireless sensor networks;Monitoring;Noise measurement;Neural networks;Data models","calibration;learning (artificial intelligence);neural nets;telecommunication computing;wireless sensor networks","deep learning approach;blind drift calibration;sensor network;wireless sensor networks;data quality;long-term WSN;large-scale WSN;projection-recovery network;PRNet;deep convolutional neural network;drift-free measurements;24-sensor testbed;drifted sensor detection;sensor calibration","","17","35","","","","","IEEE","IEEE Journals"
"Mining Fashion Outfit Composition Using an End-to-End Deep Learning Approach on Set Data","Y. Li; L. Cao; J. Zhu; J. Luo","Department of Computer Science, University of Rochester, Rochester, NY, USA; Yahoo! Research, New York, NY, USA; Yahoo! Inc., Sunnyvale, CA, USA; Department of Computer Science, University of Rochester, Rochester, NY, USA","IEEE Transactions on Multimedia","","2017","19","8","1946","1955","Composing fashion outfits involves deep under-standing of fashion standards while incorporating creativity for choosing multiple fashion items (e.g., jewelry, bag, pants, dress). In fashion websites, popular or high-quality fashion outfits are usually designed by fashion experts and followed by large audiences. In this paper, we propose a machine learning system to compose fashion outfits automatically. The core of the proposed automatic composition system is to score fashion outfit candidates based on the appearances and metadata. We propose to leverage outfit popularity on fashion-oriented websites to supervise the scoring component. The scoring component is a multimodal multiinstance deep learning system that evaluates instance aesthetics and set compatibility simultaneously. In order to train and evaluate the proposed composition system, we have collected a large-scale fashion outfit dataset with 195K outfits and 368K fashion items from Polyvore. Although the fashion outfit scoring and composition is rather challenging, we have achieved an AUC of 85% for the scoring component, and an accuracy of 77% for a constrained composition task.","","","10.1109/TMM.2017.2690144","New York State through the Goergen Institute for Data Science at the University of Rochester; FREP award from Yahoo! Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890387","Big data applications;multilayer neural network;multimedia computing","Silicon;Visualization;Footwear;Context;Image coding;Machine learning","behavioural sciences computing;clothing industry;data mining;learning (artificial intelligence);Web sites","fashion outfit composition mining;end-to-end deep learning approach;set data;fashion standards;multiple fashion items;high-quality fashion outfits;machine learning system;automatic composition system;fashion outfit candidates;fashion-oriented Websites;multimodal multiinstance deep learning system;instance aesthetics;set compatibility;large-scale fashion outfit dataset;Polyvore;constrained composition task","","13","32","","","","","IEEE","IEEE Journals"
"Deep-Cascade: Cascading 3D Deep Neural Networks for Fast Anomaly Detection and Localization in Crowded Scenes","M. Sabokrou; M. Fayyaz; M. Fathy; R. Klette","Department of ICT, Malek-Ashtar University of Technology, Tehran, Iran; Department of ICT, Malek-Ashtar University of Technology, Tehran, Iran; Iran University of Science and Technology, Tehran, Iran; School of Engineering, Computer & Mathematical Sciences, Auckland University of Technology, Auckland, New Zealand","IEEE Transactions on Image Processing","","2017","26","4","1992","2004","This paper proposes a fast and reliable method for anomaly detection and localization in video data showing crowded scenes. Time-efficient anomaly localization is an ongoing challenge and subject of this paper. We propose a cubic-patch-based method, characterised by a cascade of classifiers, which makes use of an advanced feature-learning approach. Our cascade of classifiers has two main stages. First, a light but deep 3D auto-encoder is used for early identification of “many” normal cubic patches. This deep network operates on small cubic patches as being the first stage, before carefully resizing the remaining candidates of interest, and evaluating those at the second stage using a more complex and deeper 3D convolutional neural network (CNN). We divide the deep auto-encoder and the CNN into multiple sub-stages, which operate as cascaded classifiers. Shallow layers of the cascaded deep networks (designed as Gaussian classifiers, acting as weak single-class classifiers) detect “simple” normal patches, such as background patches and more complex normal patches, are detected at deeper layers. It is shown that the proposed novel technique (a cascade of two cascaded classifiers) performs comparable to current top-performing detection and localization methods on standard benchmarks, but outperforms those in general with respect to required computation time.","","","10.1109/TIP.2017.2670780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858798","Anomaly detection;deep neural network;video analysis;pedestrian scenes","Feature extraction;Hidden Markov models;Neural networks;Training;Context;Complexity theory;Detectors","feature extraction;feedforward neural nets;image classification;learning (artificial intelligence);video coding","complex normal patches;background patches;weak-single-class classifiers;Gaussian classifiers;CNN;3D convolutional neural network;deep 3D auto-encoder;feature-learning approach;classifier cascade;cubic-patch-based method;video data;anomaly localization;anomaly detection;crowded scenes;cascaded 3D deep-neural networks;deep-cascade","","53","54","","","","","IEEE","IEEE Journals"
"Deep Learning Segmentation of Optical Microscopy Images Improves 3-D Neuron Reconstruction","R. Li; T. Zeng; H. Peng; S. Ji","School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA; School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA; Allen Institute for Brain Science, Seattle, WA, USA; School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA","IEEE Transactions on Medical Imaging","","2017","36","7","1533","1541","Digital reconstruction, or tracing, of 3-D neuron structure from microscopy images is a critical step toward reversing engineering the wiring and anatomy of a brain. Despite a number of prior attempts, this task remains very challenging, especially when images are contaminated by noises or have discontinued segments of neurite patterns. An approach for addressing such problems is to identify the locations of neuronal voxels using image segmentation methods, prior to applying tracing or reconstruction techniques. This preprocessing step is expected to remove noises in the data, thereby leading to improved reconstruction results. In this paper, we proposed to use 3-D convolutional neural networks (CNNs) for segmenting the neuronal microscopy images. Specifically, we designed a novel CNN architecture, that takes volumetric images as the inputs and their voxel-wise segmentation maps as the outputs. The developed architecture allows us to train and predict using large microscopy images in an end-to-end manner. We evaluated the performance of our model on a variety of challenging 3-D microscopy images from different organisms. Results showed that the proposed methods improved the tracing performance significantly when combined with different reconstruction algorithms.","","","10.1109/TMI.2017.2679713","National Science Foundation; Old Dominion University; Washington State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7874113","Deep learning;image denoising;image segmentation;neuron reconstruction;BigNeuron","Neurons;Three-dimensional displays;Image reconstruction;Convolution;Image segmentation;Microscopy;Morphology","biomedical optical imaging;brain;image denoising;image reconstruction;image segmentation;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;optical microscopy","deep learning segmentation;optical microscopy images;3-D neuron reconstruction;digital reconstruction;digital tracing;3-D neuron structure;reversing engineering;brain wiring;brain anatomy;discontinued segments;neurite patterns;neuronal voxels;image segmentation methods;preprocessing step;noise removal;3-D convolutional neural networks;neuronal microscopy images;CNN architecture;volumetric images;voxel-wise segmentation maps;3-D microscopy images;organisms;tracing performance;reconstruction algorithms","Algorithms;Brain;Imaging, Three-Dimensional;Machine Learning;Microscopy;Neural Networks (Computer);Neurons","7","37","","","","","IEEE","IEEE Journals"
"Learning Multi-Instance Deep Discriminative Patterns for Image Classification","P. Tang; X. Wang; B. Feng; W. Liu","School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Image Processing","","2017","26","7","3385","3396","Finding an effective and efficient representation is very important for image classification. The most common approach is to extract a set of local descriptors, and then aggregate them into a high-dimensional, more semantic feature vector, like unsupervised bag-of-features and weakly supervised part-based models. The latter one is usually more discriminative than the former due to the use of information from image labels. In this paper, we propose a weakly supervised strategy that using multi-instance learning (MIL) to learn discriminative patterns for image representation. Specially, we extend traditional multi-instance methods to explicitly learn more than one patterns in positive class, and find the “most positive” instance for each pattern. Furthermore, as the positiveness of instance is treated as a continuous variable, we can use stochastic gradient decent to maximize the margin between different patterns meanwhile considering MIL constraints. To make the learned patterns more discriminative, local descriptors extracted by deep convolutional neural networks are chosen instead of hand-crafted descriptors. Some experimental results are reported on several widely used benchmarks (Action 40, Caltech 101, Scene 15, MIT-indoor, SUN 397), showing that our method can achieve very remarkable performance.","","","10.1109/TIP.2016.2642781","National Natural Science Foundation of China, Prof. Gang Hua; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792710","Image classification;discriminative patterns;multi-instance learning;stochastic gradient decent;deep convolutional neural networks","Feature extraction;Training;Support vector machines;Semantics;Image representation;Stochastic processes;Neural networks","feature extraction;image classification;learning (artificial intelligence);neural nets","learning multiinstance deep discriminative patterns;image classification;local descriptors;semantic feature vector;unsupervised bag-of-features;image labels;multiinstance learning;MIL;discriminative patterns;image representation;multiinstance methods;convolutional neural networks;hand crafted descriptors","","12","57","","","","","IEEE","IEEE Journals"
"Deep-Learned Collision Avoidance Policy for Distributed Multiagent Navigation","P. Long; W. Liu; J. Pan","City University of Hong Kong, Dorabot, Inc., Shenzhen 518057, Hong KongChina; Department of Computer Science, Fuzhou University, Fuzhou, China; Department of Mechanical and Biomedical Engineering, City University of Hong Kong, Hong Kong","IEEE Robotics and Automation Letters","","2017","2","2","656","663","High-speed, low-latency obstacle avoidance that is insensitive to sensor noise is essential for enabling multiple decentralized robots to function reliably in cluttered and dynamic environments. While other distributed multiagent collision avoidance systems exist, these systems require online geometric optimization where tedious parameter tuning and perfect sensing are necessary. We present a novel end-to-end framework to generate reactive collision avoidance policy for efficient distributed multiagent navigation. Our method formulates an agent's navigation strategy as a deep neural network mapping from the observed noisy sensor measurements to the agent's steering commands in terms of movement velocity. We train the network on a large number of frames of collision avoidance data collected by repeatedly running a multiagent simulator with different parameter settings. We validate the learned deep neural network policy in a set of simulated and real scenarios with noisy measurements and demonstrate that our method is able to generate a robust navigation strategy that is insensitive to imperfect sensing and works reliably in all situations. We also show that our method can be well generalized to scenarios that do not appear in our training data, including scenes with static obstacles and agents with different sizes. Videos are available at https://sites.google.com/view/deepmaca.","","","10.1109/LRA.2017.2651371","HKSAR Research Grants Council (RGC); General Research Fund; CityU; NSFC; RGC; Joint Research Scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7812634","Collision avoidance;deep learning;distributed robot systems;multi-agent navigation","Collision avoidance;Robot sensing systems;Navigation;Noise measurement;Velocity measurement","collision avoidance;learning (artificial intelligence);multi-robot systems;neural nets","deep-learned collision avoidance policy;distributed multiagent navigation;high-speed low-latency obstacle avoidance;sensor noise;multiple decentralized robots;cluttered environment;dynamic environment;reactive collision avoidance policy;deep neural network mapping;noisy sensor measurement;agent steering commands;movement velocity;multiagent simulator;noisy measurements;robust navigation strategy","","23","23","","","","","IEEE","IEEE Journals"
"Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising","K. Zhang; W. Zuo; Y. Chen; D. Meng; L. Zhang","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; ULSee Inc., China; School of Mathematics and Statistics and Ministry of Education Key Lab of Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Image Processing","","2017","26","7","3142","3155","The discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks, such as Gaussian denoising, single image super-resolution, and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.","","","10.1109/TIP.2017.2662206","HK RGC GRF; National Natural Scientific Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839189","Image denoising;convolutional neural networks;residual learning;batch normalization","Noise reduction;Image denoising;Training;Computational modeling;Noise level;Neural networks;Transform coding","AWGN;feedforward neural nets;Gaussian processes;image denoising;learning (artificial intelligence);neural net architecture","Gaussian denoiser;deep CNN;image denoising;discriminative model learning;feedforward denoising convolutional neural networks;regularization method;learning algorithm;very deep architecture;batch normalization;training process;discriminative denoising models;additive white Gaussian noise;DnCNN model;unknown noise level;residual learning strategy;single image super-resolution;JPEG image deblocking;GPU computing","","763","48","","","","","IEEE","IEEE Journals"
"Detecting Anatomical Landmarks From Limited Medical Imaging Data Using Two-Stage Task-Oriented Deep Neural Networks","J. Zhang; M. Liu; D. Shen","Department of Radiology and the Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology and the Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology and the Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Image Processing","","2017","26","10","4753","4764","One of the major challenges in anatomical landmark detection, based on deep neural networks, is the limited availability of medical imaging data for network learning. To address this problem, we present a two-stage task-oriented deep learning method to detect large-scale anatomical landmarks simultaneously in real time, using limited training data. Specifically, our method consists of two deep convolutional neural networks (CNN), with each focusing on one specific task. Specifically, to alleviate the problem of limited training data, in the first stage, we propose a CNN based regression model using millions of image patches as input, aiming to learn inherent associations between local image patches and target anatomical landmarks. To further model the correlations among image patches, in the second stage, we develop another CNN model, which includes a) a fully convolutional network that shares the same architecture and network weights as the CNN used in the first stage and also b) several extra layers to jointly predict coordinates of multiple anatomical landmarks. Importantly, our method can jointly detect large-scale (e.g., thousands of) landmarks in real time. We have conducted various experiments for detecting 1200 brain landmarks from the 3D T1-weighted magnetic resonance images of 700 subjects, and also 7 prostate landmarks from the 3D computed tomography images of 73 subjects. The experimental results show the effectiveness of our method regarding both accuracy and efficiency in the anatomical landmark detection.","","","10.1109/TIP.2017.2721106","NIH; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961205","Anatomical landmark detection;deep convolutional neural networks;task-oriented;real-time;limited medical imaging data","Biomedical imaging;Three-dimensional displays;Machine learning;Training;Biological neural networks;Training data;Testing","biomedical MRI;computerised tomography;feedforward neural nets;learning (artificial intelligence);medical image processing;object detection;regression analysis","anatomical landmark detection;medical imaging data;two-stage task-oriented deep neural networks;network learning;two-stage task-oriented deep learning method;training data;deep convolutional neural networks;CNN based regression model;local image patches;fully convolutional network;anatomical landmark coordinate prediction;brain landmarks;3D T1-weighted magnetic resonance images;prostate landmarks;3D computed tomography images","","10","40","","","","","IEEE","IEEE Journals"
"Fusion of Deep Learning and Compressed Domain Features for Content-Based Image Retrieval","P. Liu; J. Guo; C. Wu; D. Cai","College of Engineering, Huaqiao University, Quanzhou, China; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; School of Mathematics and Computer Science, Quanzhou Normal University, Quanzhou, China","IEEE Transactions on Image Processing","","2017","26","12","5706","5717","This paper presents an effective image retrieval method by combining high-level features from convolutional neural network (CNN) model and low-level features from dot-diffused block truncation coding (DDBTC). The low-level features, e.g., texture and color, are constructed by vector quantization -indexed histogram from DDBTC bitmap, maximum, and minimum quantizers. Conversely, high-level features from CNN can effectively capture human perception. With the fusion of the DDBTC and CNN features, the extended deep learning two-layer codebook features is generated using the proposed two-layer codebook, dimension reduction, and similarity reweighting to improve the overall retrieval rate. Two metrics, average precision rate and average recall rate (ARR), are employed to examine various data sets. As documented in the experimental results, the proposed schemes can achieve superior performance compared with the state-of-the-art methods with either low-or high-level features in terms of the retrieval rate. Thus, it can be a strong candidate for various image retrieval related applications.","","","10.1109/TIP.2017.2736343","National Natural Science Foundation of China; Natural Science Foundation of Fujian Province, China; Fundamental Research Funds for the Central Universities; Fujian Provincial Key Laboratory of Data-intensive Computing and Fujian University Laboratory of Intelligent Computing and Information Processing; National Science Council of Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019823","Content-based image retrieval;deep learning;convolutional-neural network;block truncation coding;halftoning","Feature extraction;Image color analysis;Histograms;Image retrieval;Machine learning;Image coding;Computational complexity","block codes;content-based retrieval;feature extraction;image colour analysis;image retrieval;image texture;learning (artificial intelligence);neural nets;vector quantisation","average recall rate;average precision rate;similarity reweighting;dimension reduction;two-layer codebook features;extended deep learning;human perception;minimum quantizer;maximum quantizer;DDBTC bitmap;histogram;vector quantization;image color;image texture;dot-diffused block truncation coding;low-level features;CNN model;convolutional neural network model;high-level features;content-based image retrieval;compressed domain features","","13","62","Traditional","","","","IEEE","IEEE Journals"
"AMP-Inspired Deep Networks for Sparse Linear Inverse Problems","M. Borgerding; P. Schniter; S. Rangan","Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA; Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA; Department of Electrical and Computer Engineering, New York University, Brooklyn, NY, USA","IEEE Transactions on Signal Processing","","2017","65","16","4293","4308","Deep learning has gained great popularity due to its widespread success on many inference problems. We consider the application of deep learning to the sparse linear inverse problem, where one seeks to recover a sparse signal from a few noisy linear measurements. In this paper, we propose two novel neural-network architectures that decouple prediction errors across layers in the same way that the approximate message passing (AMP) algorithms decouple them across iterations: through Onsager correction. First, we propose a “learned AMP” network that significantly improves upon Gregor and LeCun's “learned ISTA.” Second, inspired by the recently proposed “vector AMP” (VAMP) algorithm, we propose a “learned VAMP” network that offers increased robustness to deviations in the measurement matrix from i.i.d. Gaussian. In both cases, we jointly learn the linear transforms and scalar nonlinearities of the network. Interestingly, with i.i.d. signals, the linear transforms and scalar nonlinearities prescribed by the VAMP algorithm coincide with the values learned through back-propagation, leading to an intuitive interpretation of learned VAMP. Finally, we apply our methods to two problems from 5G wireless communications: compressive random access and massive-MIMO channel estimation.","","","10.1109/TSP.2017.2708040","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934066","Deep learning;compressive sensing;approximate message passing;random access;massive MIMO","Signal processing algorithms;Inverse problems;Approximation algorithms;Machine learning;Message passing;Transforms;Probability density function","5G mobile communication;backpropagation;channel estimation;inverse problems;iterative methods;message passing;MIMO communication;neural nets;telecommunication computing","AMP-inspired deep networks;sparse linear inverse problem;deep learning;inference problem;sparse signal recovery;noisy linear measurements;neural-network architecture;prediction error decoupling;approximate message passing algorithm;AMP algorithm;iterations;Onsager correction;learned AMP network;vector AMP algorithm;VAMP algorithm;learned VAMP network;measurement matrix;iid Gaussian;linear transform;scalar nonlinearities;back-propagation;5G wireless communication;massive-MIMO channel estimation","","42","54","","","","","IEEE","IEEE Journals"
"Forensic Face Photo-Sketch Recognition Using a Deep Learning-Based Architecture","C. Galea; R. A. Farrugia","Department of Communications and Computer Engineering, University of Malta, Msida, MSD, Malta; Department of Communications and Computer Engineering, University of Malta, Msida, MSD, Malta","IEEE Signal Processing Letters","","2017","24","11","1586","1590","Numerous methods that automatically identify subjects depicted in sketches as described by eyewitnesses have been implemented, but their performance often degrades when using real-world forensic sketches and extended galleries that mimic law enforcement mug-shot galleries. Moreover, little work has been done to apply deep learning for face photo-sketch recognition despite its success in numerous application domains including traditional face recognition. This is primarily due to the limited number of sketch images available, which are insufficient to robustly train large networks. This letter aims to tackle these issues with the following contributions: 1) a state-of-the-art model pre-trained for face photo recognition is tuned for face photo-sketch recognition by applying transfer learning, 2) a three-dimensional morphable model is used to synthesise new images and artificially expand the training data, allowing the network to prevent over-fitting and learn better features, 3) multiple synthetic sketches are also used in the testing stage to improve performance, and 4) fusion of the proposed method with a state-of-the-art algorithm is shown to further boost performance. An extensive evaluation of several popular and state-of-the-art algorithms is also performed using publicly available datasets, thereby serving as a benchmark for future algorithms. Compared to a leading method, the proposed framework is shown to reduce the error rate by 80.7% for viewed sketches and lowers the mean retrieval rank by 32.5% for real-world forensic sketches.","","","10.1109/LSP.2017.2749266","Malta Government Scholarship Scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8025793","Augmentation;convolutional neural network;deep learning;fusion;hand-drawn sketch;morphological model","Face;Face recognition;Forensics;Databases;Feature extraction;Solid modeling;Image recognition","face recognition;image forensics;image fusion;learning (artificial intelligence);neural net architecture","convolutional neural network;multiple synthetic sketches;image synthesis;three-dimensional morphable model;transfer learning;sketch images;law enforcement mug-shot galleries;real-world forensic sketches;deep learning-based architecture;forensic face photo-sketch recognition","","9","44","Traditional","","","","IEEE","IEEE Journals"
"Scene-Adaptive Vehicle Detection Algorithm Based on a Composite Deep Structure","Y. Cai; H. Wang; Z. Zheng; X. Sun","Intelligent Vehicle Research Institute, Automotive Engineering Research Institute, Jiangsu University, Zhenjiang, China; Department of Automotive Engineering, School of Automotive and Traffic Engineering, Jiangsu University, Zhenjiang, China; Department of Automotive Engineering, School of Automotive and Traffic Engineering, Jiangsu University, Zhenjiang, China; Intelligent Vehicle Research Institute, Automotive Engineering Research Institute, Jiangsu University, Zhenjiang, China","IEEE Access","","2017","5","","22804","22811","Existing machine-learning-based vehicle detection algorithms for intelligent vehicles have an obvious disadvantage in that the detection effect decreases dramatically when the distribution of training samples and the scene target samples do not match. To address this issue, a scene-adaptive vehicle detection algorithm based on a composite deep structure is proposed in this paper. Inspired by the Bagging (Bootstrap aggregating) mechanism, multiple relatively independent source samples are first used to build multiple classifiers and then voting is used to generate target training samples with confidence scores. The automatic feature extraction ability of deep convolutional neural network is then used to perform source-target scene feature similarity calculations with a deep auto-encoder in order to design a composite deep-structure-based scene-adaptive classifier and its training method. Experiments on the KITTI data set and a data set captured by our group demonstrate that the proposed method performs better than existing machine-learning-based vehicle detection methods. In addition, compared with existing scene-adaptive object detection methods, our method improves the detection rate by an average of approximately 3%.","","","10.1109/ACCESS.2017.2756081","National Natural Science Foundation of China; Key Research and Development Program of Jiangsu Province; Natural Science Foundation of Jiangsu Province; Six Talent Peaks Project of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8049276","Image recognition;vehicle detection;scene adaptive;composite deep structure;deep convolutional neural network","Training;Feature extraction;Vehicle detection;Training data;Classification algorithms;Detectors","computer vision;feature extraction;image classification;learning (artificial intelligence);neural nets;object detection;pattern classification","machine-learning;scene-adaptive object detection methods;vehicle detection algorithms;detection rate;vehicle detection methods;scene-adaptive classifier;composite deep-structure;deep auto-encoder;source-target scene;deep convolutional neural network;target training samples;multiple relatively independent source samples;composite deep structure;scene-adaptive vehicle detection algorithm;scene target samples;detection effect;intelligent vehicles","","1","25","","","","","IEEE","IEEE Journals"
"Heterogeneous Sensor Data Fusion By Deep Multimodal Encoding","Z. Liu; W. Zhang; S. Lin; T. Q. S. Quek","Information Systems Technology and Design Pillar, Singapore University, Singapore; Department of Statistical Science, Cornell University, NY, USA; Engineering Systems and Design Pillar, Singapore University of Technology and Design, Singapore; Information Systems Technology and Design Pillar, Singapore University, Singapore","IEEE Journal of Selected Topics in Signal Processing","","2017","11","3","479","491","Heterogeneous sensor data fusion is a challenging field that has gathered significant interest in recent years. Two of these challenges are learning from data with missing values, and finding shared representations for multimodal data to improve inference and prediction. In this paper, we propose amultimodal data fusion framework, the deep multimodal encoder (DME), based on deep learning techniques for sensor data compression, missing data imputation, and new modality prediction under multimodal scenarios. While traditional methods capture only the intramodal correlations, DME is able to mine both the intramodal correlations in the initial layers and the enhanced intermodal correlations in the deeper layers. In this way, the statistical structure of sensor data may be better exploited for data compression. By incorporating our new objective function, DME shows remarkable ability for missing data imputation tasks in sensor data. The shared multimodal representation learned by DME may be used directly for predicting new modalities. In experiments with a real-world dataset collected from a 40-node agriculture sensor network which contains three modalities, DME can achieve a root mean square error (RMSE) of missing data imputation which is only 20% of the traditional methods like K-nearest neighbors and sparse principal component analysis and the performance is robust to different missing rates. It can also reconstruct temperature modality from humidity and illuminance with an RMSE of 7 °C, directly from a highly compressed (2.1%) shared representation that was learned from incomplete (80% missing) data.","","","10.1109/JSTSP.2017.2679538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7874158","Deep learning;heterogeneous sensor data;multimodal data fusion;missing data imputation","Data integration;Correlation;Encoding;Machine learning;Data compression;Training;Cost function","learning (artificial intelligence);mean square error methods;sensor fusion;statistical analysis","heterogeneous sensor data fusion;deep multimodal encoding;DME;deep learning techniques;sensor data compression;missing data imputation;modality prediction;agriculture sensor network;root mean square error;RMSE;K-nearest neighbors;sparse principal component analysis","","8","30","","","","","IEEE","IEEE Journals"
"Experimental Study on Extreme Learning Machine Applications for Speech Enhancement","T. Hussain; S. M. Siniscalchi; C. Lee; S. Wang; Y. Tsao; W. Liao","Taiwan International Graduate Program, Social Network and Human Centered Computing Program, Institute of Information Science, Academia Sinica, Taipei, Taiwan; Department of Computer Engineering, Kore University of Enna, Enna, Italy; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Department of Computer Science, National Chengchi University, Taipei, Taiwan","IEEE Access","","2017","5","","25542","25554","In wireless telephony and audio data mining applications, it is desirable that noise suppression can be made robust against changing noise conditions and operates in real time (or faster). The learning effectiveness and speed of artificial neural networks are therefore critical factors in applications for speech enhancement tasks. To address these issues, we present an extreme learning machine (ELM) framework, aimed at the effective and fast removal of background noise from a single-channel speech signal, based on a set of randomly chosen hidden units and analytically determined output weights. Because feature learning with shallow ELM may not be effective for natural signals, such as speech, even with a large number of hidden nodes, hierarchical ELM (H-ELM) architectures are deployed by leveraging sparse autoencoders. In this manner, we not only keep all the advantages of deep models in approximating complicated functions and maintaining strong regression capabilities, but we also overcome the cumbersome and time-consuming features of both greedy layer-wise pre-training and back-propagation (BP)-based fine tuning schemes, which are typically adopted for training deep neural architectures. The proposed ELM framework was evaluated on the Aurora-4 speech database. The Aurora-4 task provides relatively limited training data, and test speech data corrupted with both additive noise and convolutive distortions for matched and mismatched channels and signal-to-noise ratio (SNR) conditions. In addition, the task includes a subset of testing data involving noise types and SNR levels that are not seen in the training data. The experimental results indicate that when the amount of training data is limited, both ELMand H-ELM-based speech enhancement techniques consistently outperform the conventional BP-based shallow and deep learning algorithms, in terms of standardized objective evaluations, under various testing conditions.","","","10.1109/ACCESS.2017.2766675","National Science Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8085130","Speech enhancement;artificial neural networks;extreme learning machine;hierarchical extreme learning machines","Speech enhancement;Speech;Noise measurement;Training data;Signal to noise ratio;Noise reduction","backpropagation;data mining;feedforward neural nets;learning (artificial intelligence);regression analysis;signal denoising;speech enhancement","extreme learning machine applications;wireless telephony;audio data mining applications;noise suppression;noise conditions;learning effectiveness;artificial neural networks;critical factors;speech enhancement tasks;background noise;single-channel speech signal;randomly chosen hidden units;output weights;shallow ELM;natural signals;hidden nodes;hierarchical ELM architectures;sparse autoencoders;deep models;complicated functions;greedy layer-wise pre-training;fine tuning schemes;deep neural architectures;ELM framework;Aurora-4 speech database;training data;test speech data;additive noise;mismatched channels;testing data;noise types;ELMand H-ELM;deep learning algorithms;testing conditions;regression capabilities","","6","69","","","","","IEEE","IEEE Journals"
"Part-Based Deep Hashing for Large-Scale Person Re-Identification","F. Zhu; X. Kong; L. Zheng; H. Fu; Q. Tian","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; Centre for Quantum Computation and Intelligent Systems, University of Technology Sydney, Ultimo, NSW, Australia; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA","IEEE Transactions on Image Processing","","2017","26","10","4806","4817","Large-scale is a trend in person re-identification (re-id). It is important that real-time search be performed in a large gallery. While previous methods mostly focus on discriminative learning, this paper makes the attempt in integrating deep learning and hashing into one framework to evaluate the efficiency and accuracy for large-scale person re-id. We integrate spatial information for discriminative visual representation by partitioning the pedestrian image into horizontal parts. Specifically, Part-based Deep Hashing (PDH) is proposed, in which batches of triplet samples are employed as the input of the deep hashing architecture. Each triplet sample contains two pedestrian images (or parts) with the same identity and one pedestrian image (or part) of the different identity. A triplet loss function is employed with a constraint that the Hamming distance of pedestrian images (or parts) with the same identity is smaller than ones with the different identity. In the experiment, we show that the proposed PDH method yields very competitive re-id accuracy on the large-scale Market-1501 and Market-1501+500K datasets.","","","10.1109/TIP.2017.2695101","Foundation for Innovative Research Groups of the National Natural Science Foundation of China (NSFC); NSFC; Open Projects Program of the National Laboratory of Pattern Recognition; ARO; Faculty Research Gift Awards by the NEC Laboratories of America and Blippar; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7903578","Deep learning;hashing;part-based;large-scale person re-identification","Visualization;Neural networks;Measurement;Lighting;Computational modeling;Image retrieval;Face","intelligent transportation systems;learning (artificial intelligence);object detection;pedestrians","part-based deep hashing;large-scale person re-identification;discriminative learning;deep learning;discriminative visual representation;pedestrian image;PDH method;deep hashing architecture;Hamming distance","","15","71","","","","","IEEE","IEEE Journals"
"Automatic Quantification of Tumour Hypoxia From Multi-Modal Microscopy Images Using Weakly-Supervised Learning Methods","G. Carneiro; T. Peng; C. Bayer; N. Navab","Australian Centre for Visual Technologies, University of Adelaide, Adelaide, SA, Australia; Computer Aided Medical Procedures, Technische Universität München, Garching bei München, Germany; Department of Radiation Oncology, Technische Universität München, Garching bei München, Germany; Computer Aided Medical Procedures, Technische Universität München, Garching bei München, Germany","IEEE Transactions on Medical Imaging","","2017","36","7","1405","1417","In recently published clinical trial results, hypoxia-modified therapies have shown to provide more positive outcomes to cancer patients, compared with standard cancer treatments. The development and validation of these hypoxia-modified therapies depend on an effective way of measuring tumor hypoxia, but a standardized measurement is currently unavailable in clinical practice. Different types of manual measurements have been proposed in clinical research, but in this paper we focus on a recently published approach that quantifies the number and proportion of hypoxic regions using high resolution (immuno-)fluorescence (IF) and hematoxylin and eosin (HE) stained images of a histological specimen of a tumor. We introduce new machine learning-based methodologies to automate this measurement, where the main challenge is the fact that the clinical annotations available for training the proposed methodologies consist of the total number of normoxic, chronically hypoxic, and acutely hypoxic regions without any indication of their location in the image. Therefore, this represents a weakly-supervised structured output classification problem, where training is based on a high-order loss function formed by the norm of the difference between the manual and estimated annotations mentioned above. We propose four methodologies to solve this problem: 1) a naive method that uses a majority classifier applied on the nodes of a fixed grid placed over the input images; 2) a baseline method based on a structured output learning formulation that relies on a fixed grid placed over the input images; 3) an extension to this baseline based on a latent structured output learning formulation that uses a graph that is flexible in terms of the amount and positions of nodes; and 4) a pixel-wise labeling based on a fully-convolutional neural network. Using a data set of 89 weakly annotated pairs of IF and HE images from eight tumors, we show that the quantitative results of methods (3) and (4) above are equally competitive and superior to the naive (1) and baseline (2) methods. All proposed methodologies show high correlation values with respect to the clinical annotations.","","","10.1109/TMI.2017.2677479","Alexander von Humboldt Foundation for the Fellowship for Experienced Researchers; Australian Research Council’s Discovery Projects funding scheme; Alexander von Humboldt Foundation for the Fellowship for Postdoctoral Researchers; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869416","Microscopy;structured output learning;deep learning;weakly-supervised training;high-order loss functions","Tumors;Training;Medical treatment;Biomedical imaging;Manuals;Cancer;Computational modeling","biomedical optical imaging;cancer;fluorescence;image classification;learning (artificial intelligence);medical image processing;neural nets;optical microscopy;tumours","automatic quantification;tumour hypoxia;multimodal microscopy images;weakly-supervised learning methods;hypoxia-modified therapies;cancer patients;standard cancer treatments;tumor hypoxia;standardized measurement;high resolution immunofluorescence images;hematoxylin and eosin stained images;histological specimen;machine learning-based methodologies;clinical annotations;normoxic regions;chronically hypoxic regions;acutely hypoxic regions;weakly-supervised structured output classification problem;high-order loss function;manual annotations;estimated annotations;naive method;majority classifier;fixed grid;input images;baseline method;latent structured output learning formulation;pixel-wise labeling;fully-convolutional neural network;IF images;HE images","Humans;Microscopy;Neural Networks (Computer);Supervised Machine Learning;Tumor Hypoxia","2","51","","","","","IEEE","IEEE Journals"
"Deep Learning of Graphs with Ngram Convolutional Neural Networks","Z. Luo; L. Liu; J. Yin; Y. Li; Z. Wu","College of Computer Science, Zhejiang University, Hangzhou, Zhejiang, China; School of Computer Science, Georgia Institute of Technology, Atlanta, GA; College of Computer Science, Zhejiang University, Hangzhou, Zhejiang, China; College of Computer Science, Zhejiang University, Hangzhou, Zhejiang, China; College of Computer Science, Zhejiang University, Hangzhou, Zhejiang, China","IEEE Transactions on Knowledge and Data Engineering","","2017","29","10","2125","2139","Convolutional Neural Network (CNN) has gained attractions in image analytics and speech recognition in recent years. However, employing CNN for classification of graphs remains to be challenging. This paper presents the Ngram graph-block based convolutional neural network model for classification of graphs. Our Ngram deep learning framework consists of three novel components. First, we introduce the concept of n-gram block to transform each raw graph object into a sequence of n-gram blocks connected through overlapping regions. Second, we introduce a diagonal convolution step to extract local patterns and connectivity features hidden in these n-gram blocks by performing n-gram normalization. Finally, we develop deeper global patterns based on the local patterns and the ways that they respond to overlapping regions by building a n-gram deep learning model using convolutional neural network. We evaluate the effectiveness of our approach by comparing it with the existing state of art methods using five real graph repositories from bioinformatics and social networks domains. Our results show that the Ngram approach outperforms existing methods with high accuracy and comparable performance.","","","10.1109/TKDE.2017.2720734","School of Computer Science at the Georgia Institute of Technology; CSC; US National Science Foundation; National Natural Science Foundation of China; National Science and Technology Supporting Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961216","Classification of graphs;convolutional neural network;N-gram modeling;subgraph pattern extraction","Machine learning;Convolution;Neural networks;Neurons;Symmetric matrices;Kernel","convolution;feature extraction;graph theory;learning (artificial intelligence);neural nets;social networking (online);speech recognition","CNN;raw graph;overlapping regions;diagonal convolution step;local patterns;n-gram normalization;n-gram deep learning model;graph repositories;social networks;N-gram graph-block-based convolutional neural network model;N-gram deep learning framework","","4","36","Traditional","","","","IEEE","IEEE Journals"
"DeepSkeleton: Learning Multi-Task Scale-Associated Deep Side Outputs for Object Skeleton Extraction in Natural Images","W. Shen; K. Zhao; Y. Jiang; Y. Wang; X. Bai; A. Yuille","Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai University, Shanghai, China; Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA; Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA; Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA","IEEE Transactions on Image Processing","","2017","26","11","5298","5311","Object skeletons are useful for object representation and object detection. They are complementary to the object contour, and provide extra information, such as how object scale (thickness) varies among object parts. But object skeleton extraction from natural images is very challenging, because it requires the extractor to be able to capture both local and non-local image context in order to determine the scale of each skeleton pixel. In this paper, we present a novel fully convolutional network with multiple scale-associated side outputs to address this problem. By observing the relationship between the receptive field sizes of the different layers in the network and the skeleton scales they can capture, we introduce two scale-associated side outputs to each stage of the network. The network is trained by multi-task learning, where one task is skeleton localization to classify whether a pixel is a skeleton pixel or not, and the other is skeleton scale prediction to regress the scale of each skeleton pixel. Supervision is imposed at different stages by guiding the scale-associated side outputs toward the ground-truth skeletons at the appropriate scales. The responses of the multiple scale-associated side outputs are then fused in a scale-specific way to detect skeleton pixels using multiple scales effectively. Our method achieves promising results on two skeleton extraction datasets, and significantly outperforms other competitors. In addition, the usefulness of the obtained skeletons and scales (thickness) are verified on two object detection applications: foreground object segmentation and object proposal detection.","","","10.1109/TIP.2017.2735182","National Natural Science Foundation of China; Chen Guang Project through the Shanghai Municipal Education Commission and Shanghai Education Development Foundation; Intelligence Advanced Research Projects Activity through the Department of Interior/Interior Business Center; Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8000414","Skeleton;fully convolutional network;scaleassociated side outputs;multi-task learning;object segmentation;object proposal detection","Skeleton;Feature extraction;Image segmentation;Image edge detection;Object detection;Object segmentation;Electronic mail","feature extraction;image representation;image segmentation;learning (artificial intelligence);object detection","deepskeleton;object skeleton extraction;natural images;object skeletons;object representation;object detection;object contour;convolutional network;multi-task learning;skeleton localization;foreground object segmentation;object proposal detection;learning multi-task scale-associated deep side outputs","Algorithms;Animals;Humans;Image Processing, Computer-Assisted;Machine Learning;Neural Networks (Computer)","17","45","","","","","IEEE","IEEE Journals"
"Deep Representation-Based Feature Extraction and Recovering for Finger-Vein Verification","H. Qin; M. A. El-Yacoubi","SAMOVAR, Telecom SudParis, CNRS, University Paris-Saclay, Paris, France; SAMOVAR, Telecom SudParis, CNRS, University Paris-Saclay, Paris, France","IEEE Transactions on Information Forensics and Security","","2017","12","8","1816","1829","Finger-vein biometrics has been extensively investigated for personal verification. Despite recent advances in finger-vein verification, current solutions completely depend on domain knowledge and still lack the robustness to extract finger-vein features from raw images. This paper proposes a deep learning model to extract and recover vein features using limited a priori knowledge. First, based on a combination of the known state-of-the-art handcrafted finger-vein image segmentation techniques, we automatically identify two regions: a clear region with high separability between finger-vein patterns and background, and an ambiguous region with low separability between them. The first is associated with pixels on which all the above-mentioned segmentation techniques assign the same segmentation label (either foreground or background), while the second corresponds to all the remaining pixels. This scheme is used to automatically discard the ambiguous region and to label the pixels of the clear region as foreground or background. A training data set is constructed based on the patches centered on the labeled pixels. Second, a convolutional neural network (CNN) is trained on the resulting data set to predict the probability of each pixel of being foreground (i.e., vein pixel), given a patch centered on it. The CNN learns what a finger-vein pattern is by learning the difference between vein patterns and background ones. The pixels in any region of a test image can then be classified effectively. Third, we propose another new and original contribution by developing and investigating a fully convolutional network to recover missing finger-vein patterns in the segmented image. The experimental results on two public finger-vein databases show a significant improvement in terms of finger-vein verification accuracy.","","","10.1109/TIFS.2017.2689724","Direction générale des Entreprises of the Ministère de l’économie, de l’industrie et du numérique; National Natural Science Foundation of China; Natural Science Foundation Project of Chongqing; Scientific Research Foundation of Chongqing Technology and Business University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890487","Hand biometrics;finger-vein verification;deep learning;convolutional neural network;convolutional autoencoder;representation learning","Veins;Feature extraction;Image segmentation;Neural networks;Iris recognition;Machine learning","feature extraction;feedforward neural nets;image representation;image resolution;image segmentation;learning (artificial intelligence);vein recognition","finger-vein pattern;CNN;labeled pixels;convolutional neural network training;training data set;finger-vein background;handcrafted finger-vein image segmentation;a priori knowledge;raw images;finger-vein feature extraction;personal verification;finger-vein biometrics;finger-vein verification;deep representation-based feature recovery;deep representation-based feature extraction","","51","41","","","","","IEEE","IEEE Journals"
"An Ensemble Deep Learning Method for Vehicle Type Classification on Visual Traffic Surveillance Sensors","W. Liu; M. Zhang; Z. Luo; Y. Cai","Virtual Reality and Interactive Techniques Institute, East China Jiaotong University, Nanchang, China; Institute of Energy, Jiangxi Academy of Sciences, Nanchang, China; Cognitive Science Department, Xiamen University, Xiamen, China; Department of Computer Science, Minjiang University, Fuzhou, China","IEEE Access","","2017","5","","24417","24425","Visual traffic surveillance systems play important roles in intelligent transport systems nowadays. The first step of a visual traffic surveillance system usually needs to correctly detect objects from images or videos and classify them into different categories (e.g., car, truck, and bus). This paper aims to introduce a new vehicle type classification scheme on the images acquired from multi-view visual traffic surveillance sensors. Most image classification algorithms focus on maximizing the percentage of the correct predictions, which have a deficiency that the images from minority categories are prone to be misclassified as the dominant categories. To address this challenge of classifying imbalanced data acquired from visual traffic surveillance sensors, we propose a method, which integrates deep neural networks with balanced sampling in this paper. The proposed method consists of two main stages. In the first stage, data augmentation with balanced sampling is applied to alleviate the unbalanced data set problem. In the second stage, an ensemble of convolutional neural network models with different architectures is constructed with parameters learned on the augmented training data set. Experiments on the MIOvision traffic camera dataset classification challenge data set demonstrate that the proposed method is able to enhance the mean precision of all categories, in the condition of high overall accuracy, compared with the baseline algorithms.","","","10.1109/ACCESS.2017.2766203","Nature Science Foundation of China; Natural Science Foundation of Jiangxi Province; Open Fund of the Hubei Province Key Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8082506","Traffic data;traffic surveillance systems;intelligent transport systems;image classification;ensemble learning;imbalanced data","Surveillance;Visualization;Neural networks;Machine learning;Training;Sensors","feedforward neural nets;image classification;intelligent transportation systems;learning (artificial intelligence);neural net architecture;object detection;road traffic;support vector machines;video surveillance","vehicle type classification scheme;multiview visual traffic surveillance sensors;balanced sampling;MIOvision traffic camera dataset classification challenge data;ensemble deep learning method;intelligent transport systems;image classification algorithms;deep neural networks;data augmentation;convolutional neural network models;augmented training data set","","11","37","","","","","IEEE","IEEE Journals"
"Oscillometric Blood Pressure Estimation Based on Deep Learning","S. Lee; J. Chang","Department of Electronic Engineering, Hanyang University, Seoul, South Korea; Department of Electronic Engineering, Hanyang University, Seoul, South Korea","IEEE Transactions on Industrial Informatics","","2017","13","2","461","472","Oscillometric measurement is widely used to estimate systolic blood pressure (SBP) and diastolic blood pressure (DBP). In this paper, we propose a deep belief network (DBN)-deep neural network (DNN) to learn about the complex nonlinear relationship between the artificial feature vectors obtained from the oscillometric wave and the reference nurse blood pressures using the DBN-DNN-based-regression model. Our DBN-DNN is a powerful generative network for feature extraction and can address to stick in local minima through a special pretraining phase. Therefore, this model provides an alternative way for replacing a popular shallow model. Based on this, we apply the DBN-DNN-based regression model to estimate the SBP and DBP. However, there are a small amount of data samples, which is not enough to train the DBN-DNN without the overfitting problem. For this reason, we use the parametric bootstrap-based artificial features, which are used as training samples to efficiently learn the complex nonlinear functions between the feature vectors obtained and the reference nurse blood pressures. As far as we know, this is one of the first studies using the DBN-DNN-based regression model for BP estimation when a small training sample is available. Our DBN-DNN-based regression model provides a lower standard deviation of error, mean error, and mean absolute error for the SBP and DBP as compared with the conventional methods.","","","10.1109/TII.2016.2612640","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7576674","Blood pressure (BP);bootstrap;deep neural networks (DNNs);machine learning;oscillometric blood pressure estimation","Training;Blood pressure;Neural networks;Estimation;Informatics;Biomedical monitoring;Standards","belief networks;blood pressure measurement;medical computing;neural nets;nonlinear functions;regression analysis","oscillometric blood pressure estimation;deep learning;systolic blood pressure;diastolic blood pressure;deep belief network;deep neural network;artificial feature vectors;oscillometric wave;reference nurse blood pressures;SBP;DBP;parametric bootstrap-based artificial features;complex nonlinear functions;DBN-DNN-based regression model;error standard deviation;mean error;mean absolute error","","17","35","","","","","IEEE","IEEE Journals"
"Video Superresolution via Motion Compensation and Deep Residual Learning","D. Li; Z. Wang","Department of Automation, University of Science and Technology of China, Hefei, China; Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, China","IEEE Transactions on Computational Imaging","","2017","3","4","749","762","Video superresolution (SR) techniques are of essential usages for high-resolution display devices due to the current lack of high-resolution videos. Although many algorithms have been proposed, video SR still remains a very challenging inverse problem under different conditions. In this paper, we propose a new method for video SR named motion compensation and residual net (MCResNet). We use optical flow algorithm for motion estimation and motion compensation as a preprocessing step. Then, we employ a novel deep residual convolutional neural network (CNN) to predict a high-resolution image using multiple motion compensated observations. The new residual CNN model preserves the low-frequency contents and facilitates the restoration of high-frequency details. Our method is able to handle large and complex motions adaptively. Extensive experimental results validate that our proposed method outperforms state-of-the-art single-image-based and multi-frame-based algorithms for video SR quantitatively and qualitatively.","","","10.1109/TCI.2017.2671360","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858640","Convolutional neural networks (CNNs);deep residual learning;multi-frame super-resolution;video super-resolution","Motion compensation;Image resolution;Image reconstruction;Motion estimation;Neural networks;Optical imaging","image resolution;image restoration;image sequences;inverse problems;learning (artificial intelligence);motion compensation;motion estimation;neural nets;video signal processing","deep residual learning;inverse problem;motion compensation;MCResNet;high-frequency details restoration;complex motions;low-frequency contents;residual CNN model;multiple motion compensated observations;high-resolution image;deep residual convolutional neural network;motion estimation;optical flow algorithm;residual net;high-resolution videos;high-resolution display devices;video superresolution techniques","","19","70","Traditional","","","","IEEE","IEEE Journals"
"A Neuromorphic Chip Optimized for Deep Learning and CMOS Technology With Time-Domain Analog and Digital Mixed-Signal Processing","D. Miyashita; S. Kousai; T. Suzuki; J. Deguchi","Toshiba Corp., Kawasaki, Japan; Toshiba Corp., Kawasaki, Japan; Toshiba Corp., Kawasaki, Japan; Toshiba Corp., Kawasaki, Japan","IEEE Journal of Solid-State Circuits","","2017","52","10","2679","2689","Demand for highly energy-efficient coprocessor for the inference computation of deep neural networks is increasing. We propose the time-domain neural network (TDNN), which employs time-domain analog and digital mixed-signal processing (TDAMS) that uses delay time as the analog signal. TDNN not only exploits energy-efficient analog computing, but also enables fully spatially unrolled architecture by the hardware-efficient feature of TDAMS. The proposed fully spatially unrolled architecture reduces energy-hungry data moving for weight and activations, thus contributing to significant improvement of energy efficiency. We also propose useful training techniques that mitigate the non-ideal effect of analog circuits, which enables to simplify the circuits and leads to maximizing the energy efficiency. The proof-of-concept chip shows unprecedentedly high energy efficiency of 48.2 TSop/s/W.","","","10.1109/JSSC.2017.2712626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7959543","Analog computing;binarized neural network (BNN);convolutional neural network (CNN);deep learning;neuromorphic computing;time domain","Convolution;Time-domain analysis;Computer architecture;Training;Neural networks;Kernel;Neuromorphics","CMOS integrated circuits;electronic engineering computing;learning (artificial intelligence);mixed analogue-digital integrated circuits;neural nets;time-domain analysis","training techniques;proof-of-concept chip;analog circuits;fully spatially unrolled architecture;energy-efficient analog computing;delay time;TDNN;time-domain neural network;deep neural networks;inference computation;energy-efficient coprocessor;TDAMS;time-domain analog and digital mixed-signal processing;CMOS technology;deep learning;neuromorphic chip","","18","27","Traditional","","","","IEEE","IEEE Journals"
"Voronoi-Based Compact Image Descriptors: Efficient Region-of-Interest Retrieval With VLAD and Deep-Learning-Based Descriptors","A. Chadha; Y. Andreopoulos","Department of Electronic and Electrical Engineering, University College London, London, U.K.; Department of Electronic and Electrical Engineering, University College London, London, U.K.","IEEE Transactions on Multimedia","","2017","19","7","1596","1608","We investigate the problem of image retrieval based on visual queries when the latter comprise arbitrary regions-of-interest (ROI) rather than entire images. Our proposal is a compact image descriptor that combines the state-of-the-art in content-based descriptor extraction with a multilevel, Voronoi-based spatial partitioning of each dataset image. The proposed multilevel Voronoi-based encoding uses a spatial hierarchical K-means over interest-point locations, and computes a content-based descriptor over each cell. In order to reduce the matching complexity with minimal or no sacrifice in retrieval performance: 1) we utilize the tree structure of the spatial hierarchical K-means to perform top-to-bottom pruning for local similarity maxima; 2) we propose a new image similarity score that combines relevant information from all partition levels into a single measure for similarity; 3) we combine our proposal with a novel and efficient approach for optimal bit allocation within quantized descriptor representations. By deriving both a Voronoi-based VLAD descriptor (called Fast-VVLAD) and a Voronoi-based deep convolutional neural network (CNN) descriptor (called Fast-VDCNN), we demonstrate that our Voronoi-based framework is agnostic to the descriptor basis, and can easily be slotted into existing frameworks. Via a range of ROI queries in two standard datasets, it is shown that the Voronoi-based descriptors achieve comparable or higher mean average precision against conventional grid-based spatial search, while offering more than twofold reduction in complexity. Finally, beyond ROI queries, we show that Voronoi partitioning improves the geometric invariance of compact CNN descriptors, thereby resulting in competitive performance to the current state-of-the-art on whole image retrieval.","","","10.1109/TMM.2017.2673415","Innovate UK; EPSRC; EPSRC CASE award co-sponsored by BAFTA; Royal Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862881","Convolutional neural networks (CNN);deep learning;vector of locally aggregated descriptors;visual queries;Voronoi partitioning","Visualization;Complexity theory;Image coding;Quantization (signal);Image retrieval;Proposals;Encoding","feature extraction;image retrieval;learning (artificial intelligence);neural nets","Voronoi-based compact image descriptors;region-of-interest retrieval;VLAD;deep-learning-based descriptors;image retrieval;visual queries;content-based descriptor extraction;Voronoi-based spatial partitioning;dataset image partitioning;spatial hierarchical K-means;interest-point locations;top-to-bottom pruning;image similarity score;Voronoi-based deep convolutional neural network;Fast-VDCNN;grid-based spatial search;geometric invariance","","14","72","","","","","IEEE","IEEE Journals"
"Land-Use Classification via Extreme Learning Classifier Based on Deep Convolutional Features","Q. Weng; Z. Mao; J. Lin; W. Guo","School of Economics and Management, Fuzhou University, Fuzhou, China; Key Laboratory of Spatial Data Mining and Information Sharing, Ministry of Education, Fuzhou University, Fuzhou, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","5","704","708","One of the challenging issues in high-resolution remote sensing images is classifying land-use scenes with high quality and accuracy. An effective feature extractor and classifier can boost classification accuracy in scene classification. This letter proposes a deep-learning-based classification method, which combines convolutional neural networks (CNNs) and extreme learning machine (ELM) to improve classification performance. A pretrained CNN is initially used to learn deep and robust features. However, the generalization ability is finite and suboptimal, because the traditional CNN adopts fully connected layers as classifier. We use an ELM classifier with the CNN-learned features instead of the fully connected layers of CNN to obtain excellent results. The effectiveness of the proposed method is tested on the UC-Merced data set that has 2100 remotely sensed land-use-scene images with 21 categories. Experimental results show that the proposed CNN-ELM classification method achieves satisfactory results.","","","10.1109/LGRS.2017.2672643","National Natural Science Foundation of China; Fujian Province Education Research Project for Young and Middle-aged Teachers; funding from the Visiting Scholar Program of Fujian Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885555","Convolutional neural network (CNN);extreme learning machine (ELM);land-use classification;scene understanding","Feature extraction;Training;Remote sensing;Support vector machines;Computer architecture;Neural networks;Neurons","geophysical techniques;land use;neural nets;remote sensing","land-use scene classification;extreme learning classifier;deep convolutional features;high-resolution remote sensing images;effective feature extractor;effective feature classifier;deep-learning-based classification method;convolutional neural networks;CNN-learned features;extreme learning machine;ELM classifier;UC-Merced data set;remotely sensed land-use-scene images;CNN-ELM classification method","","44","27","","","","","IEEE","IEEE Journals"
"Improved deep face identification with multi-class pairwise discriminant loss","J. Y. Choi","Hankuk University of Foreign Studies, Republic of Korea","Electronics Letters","","2017","53","20","1356","1358","A novel method to extract discriminative deep feature representations of facial images for face identification is presented. A new `multi-class pairwise discriminant loss' is devised and incorporated it into the general deep convolutional neural network learning framework in a novel way, leading to highly discriminative deep face features. The method shows significant improvement over existing deep feature extraction techniques relying on softmax or triplet loss. Moreover, the method achieves a level of accuracy on the widely used identification protocols, which are better and comparable results than other state-of-the-art methods.","","","10.1049/el.2017.2108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8052356","","","face recognition;feature extraction;feedforward neural nets;learning (artificial intelligence)","identification protocols;triplet loss;softmax;general deep convolutional neural network learning framework;facial images;discriminative deep feature representation extraction;multiclass pairwise discriminant loss;improved deep face identification","","1","7","","","","","IET","IET Journals"
"Changing the Game of Mobile Data Analysis with Deep Learning","P. Kasnesis; C. Patrikakis; I. Venieris","NA; NA; NA","IT Professional","","2017","PP","99","1","1","Situational and context awareness is becoming more and more important on the course towards intelligent machines and devices, offering a comprehensive toolset for improving our quality of life. The increased computational capacity of personal/smart devices, and their constantly increasing capabilities for sensing, allow for a large amount of collected data to be stored, processed and transmitted over mobile devices and networks. As a result, fast processing and analysis of this mobile data is becoming a big challenge. In this article, we start from the presentation of common mobile context-aware applications and a reference to the current practices and approaches on mobile data analysis, and propose the use of deep learning for analyzing sensor data from mobile devices, while we discuss open issues related to this approach.","","","10.1109/MITP.2017.265110829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950863","mobile data analysis;deep learning;context-awareness;human activity recognition;accelerometer data","Mobile communication;Feature extraction;Accelerometers;Context-aware services;Convolution;Sensors","","","","3","","","","","","IEEE","IEEE Early Access Articles"
"Fiber-Optic Acoustic-Based Disturbance Prediction in Pipelines Using Deep Learning","K. Ma; H. Leung; E. Jalilian; D. Huang","Department of Electrical and Computer Engineering, University of Calgary, Calgary, AB, Canada; Department of Electrical and Computer Engineering, University of Calgary, Calgary, AB, Canada; Hifi Engineering Inc., Calgary, AB, Canada; Hifi Engineering Inc., Calgary, AB, Canada","IEEE Sensors Letters","","2017","1","6","1","4","The problem of detecting nonstationary disturbances in a pipeline is demonstrated using a predictive framework based on high spatial resolution fiber-optic acoustic sensors. We show that the root-mean-square (RMS) acoustic power is related to flow and density changes in the fluid. However, in practice, fluid parameters are not known at the resolution of the acoustics. In an experimental study, we trained long-short-term memory (LSTM) networks to exploit hidden patterns in an acoustic time series to predict the RMS acoustic power. We found LSTM perform efficiently and shows improvement over baseline neural network predictor, and its strength lies in discriminating sequential order from spatial input data. The system is verified on 25 m resolution fiber-optic acoustic data. Results show promise in predicting anomalous disturbances despite unknown pipe and fluid parameters..","","","10.1109/LSENS.2017.2775148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8114249","Sensor applications;fiber optic sensors;acoustic sensing;deep learning;pipeline monitoring;spatio-temporal prediction","Acoustic sensors;Optical fiber sensors;Machine learning;Pipelines;Acoustic measurements;Neural networks","acoustic transducers;computerised instrumentation;fibre optic sensors;learning (artificial intelligence);neural nets;pipelines;time series","sequential order discrimination;baseline neural network predictor;RMS acoustic power prediction;acoustic time series;LSTM network;long-short-term memory network;root-mean-square acoustic power;fiber-optic acoustic sensor;nonstationary disturbance detection;deep learning;pipeline;fiber-optic acoustic-based disturbance prediction","","2","16","Traditional","","","","IEEE","IEEE Journals"
"Rainfall-integrated traffic speed prediction using deep learning method","Y. Jia; J. Wu; M. Ben-Akiva; R. Seshadri; Y. Du","Tsinghua University, People's Republic of China; Tsinghua University, People's Republic of China; Massachusetts Institution of Technology, USA; Singapore-MIT Alliance for Research and Technology, Singapore; Tsinghua University, People's Republic of China","IET Intelligent Transport Systems","","2017","11","9","531","536","Traffic information prediction is one of the most essential studies for traffic research, operation and management. The successful prediction of traffic speed is increasingly significant for the benefits of both road users and traffic authorities. However, accurate prediction is challenging, due to the stochastic feature of traffic flow and shallow model structure. Furthermore, environmental factors, such as rainfall influence, should also be incorporated to improve accuracy. Inspired by deep learning, this paper investigates the performance of deep belief network (DBN) and long short-term memory (LSTM) to conduct short-term traffic speed prediction with the consideration of rainfall impact as a non-traffic input. The deep learning models have the ability to learn complex features of traffic flow pattern under various rainfall conditions. To validate the performance of rainfall-integrated DBN and LSTM, the traffic detector data from an arterial in Beijing are utilised for model training and testing. The experiment results indicate that with the combination input of speed and additional rainfall data, deep learning models have better prediction accuracy over other existing models, and also yields improvements over the models without rainfall input. Furthermore, the LSTM can outperform the DBN to capture the time-series characteristics of traffic speed data.","","","10.1049/iet-its.2016.0257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075261","","","belief networks;environmental factors;learning (artificial intelligence);time series;traffic information systems","time-series characteristics;deep testing;deep training;Beijing;traffic detector data;nontraffic input;short-term traffic speed prediction;LSTM;long short-term memory;DBN;deep belief network;rainfall influence;environmental factors;shallow model structure;stochastic feature;traffic flow;traffic authorities;road users;traffic management;traffic operation;traffic research;traffic information prediction;deep learning method;rainfall-integrated traffic speed prediction","","11","41","","","","","IET","IET Journals"
"Deep Relative Tracking","J. Gao; T. Zhang; X. Yang; C. Xu","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2017","26","4","1845","1858","Most existing tracking methods are direct trackers, which directly exploit foreground or/and background information for object appearance modeling and decide whether an image patch is target object or not. As a result, these trackers cannot perform well when target appearance changes heavily and becomes different from its model. To deal with this issue, we propose a novel relative tracker, which can effectively exploit the relative relationship among image patches from both foreground and background for object appearance modeling. Different from direct trackers, the proposed relative tracker is robust to localize target object by use of the best image patch with the highest relative score to the target appearance model. To model relative relationship among large-scale image patch pairs, we propose a novel and effective deep relative learning algorithm through the convolutional neural network. We test the proposed approach on challenging sequences involving heavy occlusion, drastic illumination changes, and large pose variations. Experimental results show that our method consistently outperforms the state-of-the-art trackers due to the powerful capacity of the proposed deep relative model.","","","10.1109/TIP.2017.2656628","National Natural Science Foundation of China; Importation and Development of High-Caliber Talents Project of Beijing Municipal Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828108","Visual tracking;deep learning;relative model","Target tracking;Visualization;Machine learning;Support vector machines;Training;Robustness","learning (artificial intelligence);neural nets;object tracking","deep relative tracking method;object appearance modeling;background information;foreground information;target object localization;target appearance model;large-scale image patch pairs;deep relative learning algorithm;convolutional neural network","","32","57","","","","","IEEE","IEEE Journals"
"Multimodal 2D+3D Facial Expression Recognition With Deep Fusion Convolutional Neural Network","H. Li; J. Sun; Z. Xu; L. Chen","Institute for Information and System Sciences, School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Institute for Information and System Sciences, School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Institute for Information and System Sciences, School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; LIRIS UMR 5205, Department of Mathematics and Informatics, Ecole Centrale de Lyon, Lyon, France","IEEE Transactions on Multimedia","","2017","19","12","2816","2831","This paper presents a novel and efficient deep fusion convolutional neural network (DF-CNN) for multimodal 2D+3D facial expression recognition (FER). DF-CNN comprises a feature extraction subnet, a feature fusion subnet, and a softmax layer. In particular, each textured three-dimensional (3D) face scan is represented as six types of 2D facial attribute maps (i.e., geometry map, three normal maps, curvature map, and texture map), all of which are jointly fed into DF-CNN for feature learning and fusion learning, resulting in a highly concentrated facial representation (32-dimensional). Expression prediction is performed by two ways: 1) learning linear support vector machine classifiers using the 32-dimensional fused deep features, or 2) directly performing softmax prediction using the six-dimensional expression probability vectors. Different from existing 3D FER methods, DF-CNN combines feature learning and fusion learning into a single end-to-end training framework. To demonstrate the effectiveness of DF-CNN, we conducted comprehensive experiments to compare the performance of DFCNN with handcrafted features, pre-trained deep features, finetuned deep features, and state-of-the-art methods on three 3D face datasets (i.e., BU-3DFE Subset I, BU-3DFE Subset II, and Bosphorus Subset). In all cases, DF-CNN consistently achieved the best results. To the best of our knowledge, this is the first work of introducing deep CNN to 3D FER and deep learning-based featurelevel fusion for multimodal 2D+3D FER.","","","10.1109/TMM.2017.2713408","NSFC; Chinese Postdoctoral Science Foundation; International Exchange Foundation of China NSFC; United Kingdom RS; NSFC; French Research Agency; l’Agence Nationale de Recherche; Partner University Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7944639","Deep fusion convolutional neural network (DF-CNN);facial expression recognition (FER);multimodal;textured three-dimensional (3D) face scan","Three-dimensional displays;Convolutional neural networks;Two dimensional displays;Face recognition;Feature extraction;Shape","emotion recognition;face recognition;feature extraction;image fusion;image representation;image texture;learning (artificial intelligence);neural nets;probability","multimodal 2D+3D facial expression recognition;deep fusion convolutional neural network;facial representation;DF-CNN;featurelevel fusion;deep learning;six-dimensional expression probability vectors;fusion learning;feature learning;2D facial attribute maps;three-dimensional face scan;feature fusion subnet;feature extraction subnet;FER","","46","69","Traditional","","","","IEEE","IEEE Journals"
"Deep Feature Engineering for Noise Robust Spoofing Detection","Y. Qian; N. Chen; H. Dinkel; Z. Wu","Computer Science and Engineering Department, Shanghai Jiao Tong University, Shanghai, China; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Computer Science and Engineering Department, Shanghai Jiao Tong University, Shanghai, China; Centre for Speech Technology Research, Apple Inc., University of Edinburgh, Edinburgh, Cupertino, CA, U.KUSA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","10","1942","1955","Spoofing detection for automatic speaker verification (ASV) aims to discriminate between genuine and spoofed speech. This topic has received increased attentions recently due to safety concerns with deploying an ASV system. While the performance of spoofing detection has improved significantly in clean condition in recent studies, the performance degrades dramatically in noisy conditions. To address this issue, in this paper, we propose to extract robust and discriminative deep features by using deep learning techniques for spoofing detection. In particular, we employ deep feedforward, recurrent, and convolutional neural networks to extract discriminative features. We also introduce multicondition training, noise-aware training, and annealed dropout training to make neural networks more robust against noise and to avoid overfitting to specific spoofing attacks and noise types. The proposed neural networks and training techniques are combined into a single framework for spoofing detection. Experimental evaluation is carried out on a noisy version of the standard ASVspoof 2015 corpus, including both additive noisy and reverberant scenarios. Experimental results confirm that the proposed system dramatically decreases averaged equal error rates from 19.1% and 22.6% to 3.2% and 5.1% for seen and unseen noisy conditions, respectively.","","","10.1109/TASLP.2017.2732162","Shanghai Sailing Program; NSFC; Interdisciplinary Program 14JCZ03 of Shanghai Jiao Tong University in China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7993036","Deep learning;deep features;noise robust;speaker verification;spoofing detection","Feature extraction;Speech;Training;Noise measurement;Neural networks;Noise robustness;Robustness","feature extraction;feedforward neural nets;recurrent neural nets;speaker recognition","deep feature engineering;noise robust spoofing detection;automatic speaker verification;robust deep feature extraction;discriminative deep feature extraction;deep learning techniques;deep feedforward neural networks;recurrent neural networks;convolutional neural networks;multicondition training;noise-aware training;annealed dropout training;standard ASV spoof 2015 corpus;additive noisy;reverberant scenarios","","4","63","","","","","IEEE","IEEE Journals"
"Clearing the Skies: A Deep Network Architecture for Single-Image Rain Removal","X. Fu; J. Huang; X. Ding; Y. Liao; J. Paisley","Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Department of Electrical Engineering, Data Science Institute, Columbia University, New York, NY, USA","IEEE Transactions on Image Processing","","2017","26","6","2944","2956","We introduce a deep network architecture called DerainNet for removing rain streaks from an image. Based on the deep convolutional neural network (CNN), we directly learn the mapping relationship between rainy and clean image detail layers from data. Because we do not possess the ground truth corresponding to real-world rainy images, we synthesize images with rain for training. In contrast to other common strategies that increase depth or breadth of the network, we use image processing domain knowledge to modify the objective function and improve deraining with a modestly sized CNN. Specifically, we train our DerainNet on the detail (high-pass) layer rather than in the image domain. Though DerainNet is trained on synthetic data, we find that the learned network translates very effectively to real-world images for testing. Moreover, we augment the CNN framework with image enhancement to improve the visual results. Compared with the state-of-the-art single image de-raining methods, our method has improved rain removal and much faster computation time after network training.","","","10.1109/TIP.2017.2691802","National Natural Science Foundation of China; Guangdong Natural Science Foundation; Fundamental Research Funds for the Central Universities; Natural Science Foundation of Fujian Province of China; CCF-Tencent research fund; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893758","Rain removal;deep learning;convolutional neural networks;image enhancement","Rain;Training;Image enhancement;Machine learning;Linear programming;Testing","feedforward neural nets;image enhancement;learning (artificial intelligence);neural net architecture;rain","deep network architecture;single-image rain removal;DerainNet;rain streaks removal;deep convolutional neural network;deep CNN;rainy image detail layers;clean image detail layers;image synthesis;network depth;network breadth;image processing domain knowledge;deraining improvement;image enhancement;visual results improvement;improved rain removal","","90","34","","","","","IEEE","IEEE Journals"
"A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology","N. Kumar; R. Verma; S. Sharma; S. Bhargava; A. Vahadane; A. Sethi","IIT Guwahati, Guwahati, India; IIT Guwahati, Guwahati, India; IIT Guwahati, Guwahati, India; IIT Guwahati, Guwahati, India; IIT Guwahati, Guwahati, India; IIT Guwahati, Guwahati, India","IEEE Transactions on Medical Imaging","","2017","36","7","1550","1560","Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (H&E)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other H&E-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.","","","10.1109/TMI.2017.2677499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872382","Annotation;boundaries;dataset;deep learning;nuclear segmentation;nuclei","Image segmentation;Measurement;Training;Machine learning;Image color analysis;Pathology;Diseases","biological organs;biological tissues;biomedical optical imaging;cellular biophysics;diseases;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;object recognition;optical microscopy","generalized nuclear segmentation;computational pathology;digital microscopic tissue images;high-quality feature extraction;nuclear morphometrics;conventional image processing techniques;Otsu thresholding;watershed segmentation;chromatin-sparse;crowded nuclei;machine learning-based segmentation;nuclear appearances;machine learning algorithms;image classification problems;object recognition;hematoxylin and eosin-stained tissue images;nuclear boundaries;disease states;organs;right out-of-the-box;H&E-stained images;object-level errors;pixel-level errors;segmentation technique;deep learning;overlapping nuclei","Algorithms;Cell Nucleus;Humans;Image Processing, Computer-Assisted;Machine Learning;Staining and Labeling","20","52","","","","","IEEE","IEEE Journals"
"Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data","N. Kussul; M. Lavreniuk; S. Skakun; A. Shelestov","Department of Space Information Technologies and Systems, Space Research Institute, National Academy of Sciences of Ukraine and SSA Ukraine, Kyiv, Ukraine; Department of Space Information Technologies and Systems, Space Research Institute, National Academy of Sciences of Ukraine and SSA Ukraine, Kyiv, Ukraine; Department of Geographical Sciences, University of Maryland, College Park, MD, USA; Department of Information Security, National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute,”, Kyiv, Ukraine","IEEE Geoscience and Remote Sensing Letters","","2017","14","5","778","782","Deep learning (DL) is a powerful state-of-the-art technique for image processing including remote sensing (RS) images. This letter describes a multilevel DL architecture that targets land cover and crop type classification from multitemporal multisource satellite imagery. The pillars of the architecture are unsupervised neural network (NN) that is used for optical imagery segmentation and missing data restoration due to clouds and shadows, and an ensemble of supervised NNs. As basic supervised NN architecture, we use a traditional fully connected multilayer perceptron (MLP) and the most commonly used approach in RS community random forest, and compare them with convolutional NNs (CNNs). Experiments are carried out for the joint experiment of crop assessment and monitoring test site in Ukraine for classification of crops in a heterogeneous environment using nineteen multitemporal scenes acquired by Landsat-8 and Sentinel-1A RS satellites. The architecture with an ensemble of CNNs outperforms the one with MLPs allowing us to better discriminate certain summer crop types, in particular maize and soybeans, and yielding the target accuracies more than 85% for all major crops (wheat, maize, sunflower, soybeans, and sugar beet).","","","10.1109/LGRS.2017.2681128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891032","Agriculture;convolutional neural networks (CNNs);crop classification;deep learning (DL);joint experiment of crop assessment and monitoring (JECAM);Landsat-8;remote sensing (RS);Sentinel-1;TensorFlow;Ukraine","Agriculture;Satellites;Remote sensing;Earth;Artificial neural networks;Optical imaging;Optical sensors","crops;image segmentation;land cover;multilayer perceptrons;neural nets;vegetation mapping","multitemporal multisource satellite imagery;unsupervised neural network;optical imagery segmentation;missing data restoration;supervised neural network ensemble;supervised neural network architecture;multilayer perceptron;MLP;remote sensing community random forest;crop assessment;crop monitoring;Ukraine;heterogeneous environment;Landsat-8 remote sensing satellite;Sentinel-1A remote sensing satellite;maize;soybeans;wheat;sunflower;sugar beet;summer crop type classification;deep learning architecture;remote sensing images;image processing;land cover;deep learning classification","","208","40","","","","","IEEE","IEEE Journals"
"Multitalker Speech Separation With Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks","M. Kolbæk; D. Yu; Z. Tan; J. Jensen","Department of Electronic Systems, Aalborg University, Aalborg, Denmark; Microsoft Research, Tencent AI Lab, Redmond, Bellevue, WA, WA, USAUSA; Department of Electronic Systems, Aalborg University, Aalborg, Denmark; Department of Electronic Systems, Aalborg University, Oticon A/S, Aalborg, Smørum, DenmarkDenmark","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","10","1901","1913","In this paper, we propose the utterance-level permutation invariant training (uPIT) technique. uPIT is a practically applicable, end-to-end, deep-learning-based solution for speaker independent multitalker speech separation. Specifically, uPIT extends the recently proposed permutation invariant training (PIT) technique with an utterance-level cost function, hence eliminating the need for solving an additional permutation problem during inference, which is otherwise required by frame-level PIT. We achieve this using recurrent neural networks (RNNs) that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream. In practice, this allows RNNs, trained with uPIT, to separate multitalker mixed speech without any prior knowledge of signal duration, number of speakers, speaker identity, or gender. We evaluated uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on nonnegative matrix factorization and computational auditory scene analysis, and compares favorably with deep clustering, and the deep attractor network. Furthermore, we found that models trained with uPIT generalize well to unseen speakers and languages. Finally, we found that a single model, trained with uPIT, can handle both two-speaker, and three-speaker speech mixtures.","","","10.1109/TASLP.2017.2726762","Oticon Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979557","CNN;cocktail party problem;deep learning;DNN;LSTM;permutation invariant training;speech separation","Speech;Training;Machine learning;Speech processing;Recurrent neural networks;Time-frequency analysis;Computational modeling","learning (artificial intelligence);matrix decomposition;pattern clustering;recurrent neural nets;speaker recognition","utterance-level permutation invariant training;deep recurrent neural network;uPIT technique;speaker independent multitalker speech separation;utterance-level separation error minimization;nonnegative matrix factorization;computational auditory scene analysis;deep clustering;deep attractor network","","55","55","","","","","IEEE","IEEE Journals"
"Smart Augmentation Learning an Optimal Data Augmentation Strategy","J. Lemley; S. Bazrafkan; P. Corcoran","Collage of Engineering and Informatics, National University of Ireland Galway, Galway, Ireland; Collage of Engineering and Informatics, National University of Ireland Galway, Galway, Ireland; Collage of Engineering and Informatics, National University of Ireland Galway, Galway, Ireland","IEEE Access","","2017","5","","5858","5869","A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks. There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method, which we call smart augmentation and we show how to use it to increase the accuracy and reduce over fitting on a target network. Smart augmentation works, by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network. Smart augmentation has shown the potential to increase accuracy by demonstrably significant measures on all data sets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.","","","10.1109/ACCESS.2017.2696121","SFI Strategic Partnership Program on Next Generation Imaging for Smartphone and Embedded Platforms by Science Foundation Ireland and FotoNation Ltd; Irish Research Council Employment Based Programme Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906545","Artificial intelligence;artificial neural networks;machine learning;computer vision supervised learning;machine learning algorithms;image databases","Training;Biological neural networks;Informatics;Electronic mail;Machine learning;Data models","data handling;learning (artificial intelligence);neural nets","smart augmentation learning;optimal data augmentation strategy;training neural networks;deep neural networks;dropout;transfer learning;target network;networks loss","","39","20","","","","","IEEE","IEEE Journals"
"Polarimetric SAR Feature Extraction With Neighborhood Preservation-Based Deep Learning","H. Liu; S. Yang; S. Gou; D. Zhu; R. Wang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Xidian University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","4","1456","1466","As an advanced nonlinear technique, deep learning, which is based on deep neural networks (DNNs), has attracted considerable attentions. In this paper, we propose a novel neighborhood preserved deep neural network (NPDNN) for polarimetric synthetic aperture radar feature extraction and classification. The spatial relation between pixels is exploited by a jointly weighting strategy. Not only the spatial neighbors but also the pixels in the same superpixel are utilized to weight each pixel. This strategy maintains the spatial dependence leading to superior homogeneity of the terrains without extra computational memory. Moreover, a few labeled samples and their nearest neighbors are employed to train the multilayer NPDNN, which preserves the local structure and reduces the number of labeled samples for classification. Experimental results on synthesized and real PolSAR data show that the proposed NPDNN can improve the classification accuracy compared with state-of-the-art DNNs despite a few input samples.","","","10.1109/JSTARS.2016.2618891","National Basic Research Program of China; National Natural Science Foundation of China; Fund for Foreign Scholars in University Research and Teaching Programs; Major Research Plan of the National Natural Science Foundation of China; Program for Cheung Kong Scholars and Innovative Research Team in University; Ministry of Education of China; National Science Basic Research Plan in Shaanxi Province of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752793","Deep learning;feature extraction (FE);polarimetric synthetic aperture radar (PolSAR)","Feature extraction;Iron;Covariance matrices;Machine learning;Synthetic aperture radar;Scattering;Training","geophysical techniques;synthetic aperture radar","polarimetric SAR feature extraction;neighborhood preservation-based deep learning;advanced nonlinear technique;polarimetric synthetic aperture radar feature extraction;NPDNN","","13","28","","","","","IEEE","IEEE Journals"
"Energy-Fluctuated Multiscale Feature Learning With Deep ConvNet for Intelligent Spindle Bearing Fault Diagnosis","X. Ding; Q. He","Department of Precision Machinery and Precision Instrumentation, University of Science and Technology of China, Hefei, China; Department of Precision Machinery and Precision Instrumentation, University of Science and Technology of China, Hefei, China","IEEE Transactions on Instrumentation and Measurement","","2017","66","8","1926","1935","Considering various health conditions under varying operational conditions, the mining sensitive feature from the measured signals is still a great challenge for intelligent fault diagnosis of spindle bearings. This paper proposed a novel energy-fluctuated multiscale feature mining approach based on wavelet packet energy (WPE) image and deep convolutional network (ConvNet) for spindle bearing fault diagnosis. Different from the vector characteristics applied in intelligent diagnosis of spindle bearings, wavelet packet transform is first combined with phase space reconstruction to rebuild a 2-D WPE image of the frequency subspaces. This special image can reconstruct the local relationship of the WP nodes and hold the energy fluctuation of the measured signal. Then, the identifiable characteristics can be further learned by a special architecture of the deep ConvNet. Other than the traditional neural network architecture, to maintain the global and local information simultaneously, deep ConvNet combines the skipping layer with the last convolutional layer as the input of the multiscale layer. The comparisons of clustering distribution and classification accuracy with six other features show that the proposed feature mining approach is quite suitable for spindle bearing fault diagnosis with multiclass classification regardless of the load fluctuation.","","","10.1109/TIM.2017.2674738","Program for New Century Excellent Talents in University; National Natural Science Foundation of China; Youth Innovation Promotion Association of the Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7880628","Deep convolutional network (ConvNet);multiscale feature;spindle bearing fault diagnosis;wavelet packet energy (WPE) image","Feature extraction;Fault diagnosis;Convolution;Machine tool spindles;Robustness;Wavelet packets","data mining;fault diagnosis;learning (artificial intelligence);machine bearings;machine tool spindles;neural nets;pattern classification;pattern clustering;wavelet transforms","feature mining approach;classification accuracy;clustering distribution;phase space reconstruction;wavelet packet transform;deep convolutional network;wavelet packet energy;intelligent spindle bearing fault diagnosis;Deep ConvNet;energy-fluctuated multiscale feature learning","","27","27","","","","","IEEE","IEEE Journals"
"Automatic Scoring of Multiple Semantic Attributes With Multi-Task Feature Leverage: A Study on Pulmonary Nodules in CT Images","S. Chen; J. Qin; X. Ji; B. Lei; T. Wang; D. Ni; J. Cheng","Department of Biomedical Engineering, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Medicine, Shenzhen University, Shenzhen, China; Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Biomedical Engineering, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Medicine, Shenzhen University, Shenzhen, China; Department of Biomedical Engineering, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Medicine, Shenzhen University, Shenzhen, China; Department of Biomedical Engineering, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Medicine, Shenzhen University, Shenzhen, China; Department of Biomedical Engineering, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Medicine, Shenzhen University, Shenzhen, China; Department of Biomedical Engineering, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Medicine, Shenzhen University, Shenzhen, China","IEEE Transactions on Medical Imaging","","2017","36","3","802","814","The gap between the computational and semantic features is the one of major factors that bottlenecks the computer-aided diagnosis (CAD) performance from clinical usage. To bridge this gap, we exploit three multi-task learning (MTL) schemes to leverage heterogeneous computational features derived from deep learning models of stacked denoising autoencoder (SDAE) and convolutional neural network (CNN), as well as hand-crafted Haar-like and HoG features, for the description of 9 semantic features for lung nodules in CT images. We regard that there may exist relations among the semantic features of “spiculation”, “texture”, “margin”, etc., that can be explored with the MTL. The Lung Image Database Consortium (LIDC) data is adopted in this study for the rich annotation resources. The LIDC nodules were quantitatively scored w.r.t. 9 semantic features from 12 radiologists of several institutes in U.S.A. By treating each semantic feature as an individual task, the MTL schemes select and map the heterogeneous computational features toward the radiologists' ratings with cross validation evaluation schemes on the randomly selected 2400 nodules from the LIDC dataset. The experimental results suggest that the predicted semantic scores from the three MTL schemes are closer to the radiologists' ratings than the scores from single-task LASSO and elastic net regression methods. The proposed semantic attribute scoring scheme may provide richer quantitative assessments of nodules for better support of diagnostic decision and management. Meanwhile, the capability of the automatic association of medical image contents with the clinical semantic terms by our method may also assist the development of medical search engine.","","","10.1109/TMI.2016.2629462","National Natural Science Foundation of China; Shenzhen Key Basic Research Project; (Key) Project of Department of Education of Guangdong Province; Natural Science Foundation of SZU; Shenzhen-Hong Kong Innovation Circle Funding Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745891","Computer-aided diagnosis (CAD);lung nodule;computed tomography (CT);multi-task learning;deep learning;feature learning","Semantics;Computed tomography;Medical diagnostic imaging;Computational modeling;Machine learning;Lungs","computerised tomography;feature extraction;lung;medical image processing;pneumodynamics;regression analysis","computer-aided diagnosis performance;multitask learning schemes;deep learning models;stacked denoising autoencoder;convolutional neural network;HoG features;lung nodules;CT images;semantic features;Lung Image Database Consortium data;heterogeneous computational features;single-task LASSO;elastic net regression methods;medical image;clinical semantic terms;medical search engine;pulmonary nodules","Algorithms;Humans;Lung Neoplasms;Machine Learning;Neural Networks (Computer);Radiographic Image Interpretation, Computer-Assisted;Solitary Pulmonary Nodule;Tomography, X-Ray Computed","31","48","","","","","IEEE","IEEE Journals"
"Conditional High-Order Boltzmann Machines for Supervised Relation Learning","Y. Huang; W. Wang; L. Wang; T. Tan","Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, China; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, China","IEEE Transactions on Image Processing","","2017","26","9","4297","4310","Relation learning is a fundamental problem in many vision tasks. Recently, high-order Boltzmann machine and its variants have shown their great potentials in learning various types of data relation in a range of tasks. But most of these models are learned in an unsupervised way, i.e., without using relation class labels, which are not very discriminative for some challenging tasks, e.g., face verification. In this paper, with the goal to perform supervised relation learning, we introduce relation class labels into conventional high-order multiplicative interactions with pairwise input samples, and propose a conditional high-order Boltzmann Machine (CHBM), which can learn to classify the data relation in a binary classification way. To be able to deal with more complex data relation, we develop two improved variants of CHBM: 1) latent CHBM, which jointly performs relation feature learning and classification, by using a set of latent variables to block the pathway from pairwise input samples to output relation labels and 2) gated CHBM, which untangles factors of variation in data relation, by exploiting a set of latent variables to multiplicatively gate the classification of CHBM. To reduce the large number of model parameters generated by the multiplicative interactions, we approximately factorize high-order parameter tensors into multiple matrices. Then, we develop efficient supervised learning algorithms, by first pretraining the models using joint likelihood to provide good parameter initialization, and then finetuning them using conditional likelihood to enhance the discriminant ability. We apply the proposed models to a series of tasks including invariant recognition, face verification, and action similarity labeling. Experimental results demonstrate that by exploiting supervised relation labels, our models can greatly improve the performance.","","","10.1109/TIP.2017.2698918","National Key Research and Development Program of China; National Natural Science Foundation of China; Strategic Priority Research Program of the CAS; Beijing Natural Science Foundation; NVIDIA; NVIDIA DGX-1 AI Supercomputer; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7913581","Deep learning;high-order Boltzmann machine;relation learning;face verification;action similarity labeling","Logic gates;Data models;Face;Computational modeling;Tensile stress;Measurement;Supervised learning","Boltzmann machines;face recognition;image classification;learning (artificial intelligence);matrix decomposition;tensors","conditional high-order Boltzmann machines;supervised relation learning;face verification;high-order multiplicative interactions;pairwise input samples;CHBM;data relation;binary classification;relation feature learning;relation feature classification;latent variables;high-order parameter tensors;multiple matrix factorization;conditional likelihood;joint likelihood;discriminant ability enhancement;invariant recognition;action similarity labeling","","2","69","","","","","IEEE","IEEE Journals"
"Traffic Sign Recognition Using Kernel Extreme Learning Machines With Deep Perceptual Features","Y. Zeng; X. Xu; D. Shen; Y. Fang; Z. Xiao","College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; College of Information Systems and Management, National University of Defense Technology, Changsha, China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China; College of Mechatronics and Automation, National University of Defense Technology, Changsha, China","IEEE Transactions on Intelligent Transportation Systems","","2017","18","6","1647","1653","Traffic sign recognition plays an important role in autonomous vehicles as well as advanced driver assistance systems. Although various methods have been developed, it is still difficult for the state-of-the-art algorithms to obtain high recognition precision with low computational costs. In this paper, based on the investigation on the influence that color spaces have on the representation learning of convolutional neural network, a novel traffic sign recognition approach called DP-KELM is proposed by using a kernel-based extreme learning machine (KELM) classifier with deep perceptual features. Unlike the previous approaches, the representation learning process in DP-KELM is implemented in the perceptual Lab color space. Based on the learned deep perceptual feature, a kernel-based ELM classifier is trained with high computational efficiency and generalization performance. Through the experiments on the German traffic sign recognition benchmark, the proposed method is demonstrated to have higher precision than most of the state-of-the-art approaches. In particular, when compared with the hinge loss stochastic gradient descent method which has the highest precision, the proposed method can achieve a comparable recognition rate with significantly fewer computational costs.","","","10.1109/TITS.2016.2614916","National Natural Science Foundation of China (NSFC); Joint Innovation Foundation between NSFC and Chinese Automobile Industry; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7742353","Traffic sign recognition;convolutional neural network;extreme learning machine;kernel;color space;lab","Image color analysis;Feature extraction;Kernel;Neural networks;Computational efficiency;Training;Color","feature extraction;image classification;image colour analysis;image representation;learning (artificial intelligence);traffic engineering computing","traffic sign recognition;kernel extreme learning machines;KELM classifier;deep perceptual features;DP-KELM;representation learning process;perceptual Lab color space","","15","54","","","","","IEEE","IEEE Journals"
"Dynamic Delay Predictions for Large-Scale Railway Networks: Deep and Shallow Extreme Learning Machines Tuned via Thresholdout","L. Oneto; E. Fumeo; G. Clerico; R. Canepa; F. Papa; C. Dambra; N. Mazzino; D. Anguita","Department of Informatics, BioEngineering, Robotics, and Systems Engineering, University of Genoa, Genoa, Italy; Department of Informatics, BioEngineering, Robotics, and Systems Engineering, University of Genoa, Genoa, Italy; Department of Informatics, BioEngineering, Robotics, and Systems Engineering, University of Genoa, Genoa, Italy; Rete Ferroviaria Italiana S.p.A., Genoa, Italy; Ansaldo STS, Genoa, Italy; Ansaldo STS, Genoa, Italy; Ansaldo STS, Genoa, Italy; Department of Informatics, BioEngineering, Robotics, and Systems Engineering, University of Genoa, Genoa, Italy","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2017","47","10","2754","2767","Current train delay (TD) prediction systems do not take advantage of state-of-the-art tools and techniques for handling and extracting useful and actionable information from the large amount of endogenous (i.e., generated by the railway system itself) and exogenous (i.e., related to railway operation but generated by external phenomena) data available. Additionally, they are not designed in order to deal with the intrinsic time varying nature of the problem (e.g., regular changes in the nominal timetable, etc.). The purpose of this paper is to build a dynamic data-driven TD prediction system that exploits the most recent tools and techniques in the field of time varying big data analysis. In particular, we map the TD prediction problem into a time varying multivariate regression problem that allows exploiting both historical data about the train movements and exogenous data about the weather provided by the national weather services. The performance of these methods have been tuned through the state-of-the-art thresholdout technique, a very powerful procedure which relies on the differential privacy theory. Finally, the performance of two efficient implementations of shallow and deep extreme learning machines that fully exploit the recent in-memory large-scale data processing technologies have been compared with the current state-of-the-art TD prediction systems. Results on real-world data coming from the Italian railway network show that the proposal of this paper is able to remarkably improve the state-of-the-art systems.","","","10.1109/TSMC.2017.2693209","European Union through the Projects Capacity for Rail—C4R; Innovative Intelligent Rail—In2Rail; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917288","Apache Spark;big data;deep extreme learning machine (DELM);delay prediction;dynamic varying systems;in-memory computing;intelligent transportation systems;model selection (MS);railway;shallow extreme learning machine (SELM);thresholdout","Rail transportation;Data models;Delays;Tools;Big Data;Meteorology;Predictive models","learning (artificial intelligence);railways;regression analysis","dynamic delay predictions;large-scale railway networks;extreme learning machines;dynamic data-driven TD prediction system;time varying big data analysis;time varying multivariate regression problem;differential privacy theory","","6","110","OAPA","","","","IEEE","IEEE Journals"
"Discriminative Training of Deep Fully Connected Continuous CRFs With Task-Specific Loss","F. Liu; G. Lin; C. Shen","School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Image Processing","","2017","26","5","2127","2136","Recent works on deep conditional random fields (CRFs) have set new records on many vision tasks involving structured predictions. Here, we propose a fully connected deep continuous CRF model with task-specific losses for both discrete and continuous labeling problems. We exemplify the usefulness of the proposed model on multi-class semantic labeling (discrete) and the robust depth estimation (continuous) problems. In our framework, we model both the unary and the pairwise potential functions as deep convolutional neural networks (CNNs), which are jointly learned in an end-to-end fashion. The proposed method possesses the main advantage of continuously valued CRFs, which is a closed-form solution for the maximum a posteriori (MAP) inference. To better take into account the quality of the predicted estimates during the cause of learning, instead of using the commonly employed maximum likelihood CRF parameter learning protocol, we propose task-specific loss functions for learning the CRF parameters. It enables direct optimization of the quality of the MAP estimates during the learning process. Specifically, we optimize the multi-class classification loss for the semantic labeling task and the Tukey's biweight loss for the robust depth estimation problem. Experimental results on the semantic labeling and robust depth estimation tasks demonstrate that the proposed method compare favorably against both baseline and state-of-the-art methods. In particular, we show that although the proposed deep CRF model is continuously valued, with the equipment of task-specific loss, it achieves impressive results even on discrete labeling tasks.","","","10.1109/TIP.2017.2675166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7864398","Continuous conditional random field (CRFs);deep convolutional neural networks (CNN);depth estimation;semantic labelling","Labeling;Semantics;Training;Robustness;Maximum likelihood estimation;Neural networks","feedforward neural nets;image classification;learning (artificial intelligence);maximum likelihood estimation;optimisation;random processes","structured predictions;fully connected deep continuous CRF model;discrete labeling problems;continuous labeling problems;multiclass semantic labeling;robust depth estimation;pairwise potential functions;unary functions;deep convolutional neural networks;deep CNN;closed-form solution;maximum a posteriori inference;MAP inference;maximum likelihood CRF parameter learning protocol;task-specific loss functions;MAP estimates;multiclass classification loss optimization;biweight loss;deep conditional random fields;discriminative training","","5","38","","","","","IEEE","IEEE Journals"
"Deeply Learned View-Invariant Features for Cross-View Action Recognition","Y. Kong; Z. Ding; J. Li; Y. Fu","Department of ECE, Northeastern University, Boston, MA, USA; Department of ECE, Northeastern University, Boston, MA, USA; Department of ECE, Northeastern University, Boston, MA, USA; Department of ECE, College of Computer and Information Science, Northeastern University, Boston, MA, USA","IEEE Transactions on Image Processing","","2017","26","6","3028","3037","Classifying human actions from varied views is challenging due to huge data variations in different views. The key to this problem is to learn discriminative view-invariant features robust to view variations. In this paper, we address this problem by learning view-specific and view-shared features using novel deep models. View-specific features capture unique dynamics of each view while view-shared features encode common patterns across views. A novel sample-affinity matrix is introduced in learning shared features, which accurately balances information transfer within the samples from multiple views and limits the transfer across samples. This allows us to learn more discriminative shared features robust to view variations. In addition, the incoherence between the two types of features is encouraged to reduce information redundancy and exploit discriminative information in them separately. The discriminative power of the learned features is further improved by encouraging features in the same categories to be geometrically closer. Robust view-invariant features are finally learned by stacking several layers of features. Experimental results on three multi-view data sets show that our approaches outperform the state-of-the-art approaches.","","","10.1109/TIP.2017.2696786","NSF IIS; ONR Young Investigator; U.S. Army Research Office Young Investigator; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7907301","Action recognition;autoencoder;multi-view learning;view-invariant features","Cameras;Sensor phenomena and characterization;Robustness;Magnetic sensors;Feature extraction;Magnetometers","gesture recognition;image classification;learning (artificial intelligence);matrix algebra","deeply learned view-invariant features;cross-view action recognition;human action classification;view-shared features;view-specific features;novel sample-affinity matrix","Algorithms;Databases, Factual;Deep Learning;Human Activities;Humans;Image Processing, Computer-Assisted;Pattern Recognition, Automated;Video Recording","16","44","","","","","IEEE","IEEE Journals"
"Zero-Shot Learning of SAR Target Feature Space With Deep Generative Neural Networks","Q. Song; F. Xu","Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","12","2245","2249","Zero-shot learning (ZSL) is of critical importance for practical synthetic aperture radar (SAR) automatic target recognition (ATR) as training samples are not always available for all targets and all observation configurations. We propose a novel generative-based deep neural network framework for ZSL of SAR ATR. The key component of the framework is a generative deconvolutional neural network referred to as generator. It learns a faithful hierarchical representation of known targets while automatically constructing a continuous SAR target feature space spanned by orientation-invariant features and orientation angle. It is then used as a reference to design and initialize an interpreter convolutional neural network, which is inversely symmetric to the generator network. The interpreter network is then trained to map any input SAR image, including those of unseen targets, into the target feature space. In a preliminary experiment with the Moving and Stationary Target Acquisition and Recognition data set, seven targets are used in the training of generator and interpreter networks. Then, the eighth target is used to test the interpreter, where it is correctly mapped to the reasonable spot spanned by the previous seven targets and its orientation can also be estimated.","","","10.1109/LGRS.2017.2758900","National Key R&D Program of China; NSFC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8103339","Deep generative neural network;orientation-invariant feature space;synthetic aperture radar (SAR)","Synthetic aperture radar;Training;Generators;Target recognition;Artificial neural networks;Biological neural networks","deconvolution;feature extraction;learning (artificial intelligence);neural nets;object recognition;radar imaging;radar target recognition;synthetic aperture radar","SAR image;synthetic aperture radar automatic target recognition;orientation angle;orientation-invariant features;continuous SAR target feature space;generative deconvolutional neural network;SAR ATR;deep neural network framework;ZSL;zero-shot learning;deep generative neural networks;interpreter network;generator network;interpreter convolutional neural network","","9","12","","","","","IEEE","IEEE Journals"
"Stacked Sparse Autoencoder-Based Deep Network for Fault Diagnosis of Rotating Machinery","Y. Qi; C. Shen; D. Wang; J. Shi; X. Jiang; Z. Zhu","School of Urban Rail Transportation, Soochow University, Suzhou, China; School of Urban Rail Transportation, Soochow University, Suzhou, China; Department of Systems Engineering and Engineering Management, City University of Hong Kong, Hong Kong; School of Urban Rail Transportation, Soochow University, Suzhou, China; School of Urban Rail Transportation, Soochow University, Suzhou, China; School of Urban Rail Transportation, Soochow University, Suzhou, China","IEEE Access","","2017","5","","15066","15079","As a breakthrough in the field of machine fault diagnosis, deep learning has great potential to extract more abstract and discriminative features automatically without much prior knowledge compared with other methods, such as the signal processing and analysis-based methods and machine learning methods with shallow architectures. One of the most important aspects in measuring the extracted features is whether they can explore more information of the inputs and avoid redundancy to be representative. Thus, a stacked sparse autoencoder (SAE)-based machine fault diagnosis method is proposed in this paper. The penalty term of the SAE can help mine essential information and avoid redundancy. To help the constructed diagnosis network further mine more abstract and representative high-level features, the collected non-stationary and transient signals are preprocessed with ensemble empirical mode decomposition and autoregressive (AR) models to obtain AR parameters, which are extracted based on the intrinsic mode functions (IMFs) and regarded as the low-level features for the inputs of the proposed diagnosis network. Only the first four IMFs are considered, because fault information is mainly reflected in high-frequency IMFs. Experiments and comparisons are complemented to validate the superiority of the presented diagnosis network. Results fully demonstrate that the stacked SAE-based diagnosis method can extract more discriminative high-level features and has a better performance in rotating machinery fault diagnosis compared with the traditional machine learning methods with shallow architectures.","","","10.1109/ACCESS.2017.2728010","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; Jiangsu Postdoctoral Science Foundation funded project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7983338","Sparse autoencoder;ensemble empirical mode decomposition;autoregressive model;fault diagnosis","Feature extraction;Fault diagnosis;Machinery;Machine learning;Support vector machines;Data mining;Learning systems","autoregressive processes;encoding;fault diagnosis;feature extraction;learning (artificial intelligence);machining;mechanical engineering computing;neural nets;signal processing","stacked sparse autoencoder;SAE;deep network;rotating machinery;machine fault diagnosis;deep learning;machine learning;feature extraction;nonstationary signals;transient signals;ensemble empirical mode decomposition;autoregressive model;AR;intrinsic mode functions;IMF;shallow architectures","","25","31","","","","","IEEE","IEEE Journals"
"Unsupervised Iterative Deep Learning of Speech Features and Acoustic Tokens with Applications to Spoken Term Detection","C. Chung; C. Tsai; C. Liu; L. Lee","Graduate Institute of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","10","1914","1928","In this paper, we aim to automatically discover high-quality frame-level speech features and acoustic tokens directly from unlabeled speech data. A multigranular acoustic tokenizer (MAT) was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters describing the model configuration. These different sets of acoustic tokens carry different characteristics for the given corpus and the language behind and, thus, can be mutually reinforced. The multiple sets of token labels are then used as the targets of a multitarget deep neural network (MDNN) trained on frame-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. The multigranular acoustic token sets and the frame-level speech features can be iteratively optimized in the iterative deep learning framework. We call this framework the MAT deep neural network. The results were evaluated using the metrics and corpora defined in the Zero Resource Speech Challenge organized at Interspeech 2015, and improved performance was obtained with a set of experiments of query-by-example spoken term detection on the same corpora. Visualization for the discovered tokens against the English phonemes was also shown.","","","10.1109/TASLP.2017.2729024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984872","Acoustic tokens;DNN;HMM;unsupervised learning","Acoustics;Hidden Markov models;Speech;Feature extraction;Neural networks;Speech processing;Measurement","iterative methods;neural nets;speech recognition;unsupervised learning","unsupervised iterative deep learning;acoustic token;spoken term detection;multigranular acoustic tokenizer;multitarget deep neural network training;frame-level acoustic feature extraction;frame-level speech features;query-by-example spoken term detection;English phonemes","","2","57","","","","","IEEE","IEEE Journals"
"Deep Learning-Based Large-Scale Automatic Satellite Crosswalk Classification","R. F. Berriel; A. T. Lopes; A. F. de Souza; T. Oliveira-Santos","Universidade Federal do Espirito Santo, Vitória, Brazil; Universidade Federal do Espirito Santo, Vitória, Brazil; Universidade Federal do Espirito Santo, Vitória, Brazil; Universidade Federal do Espirito Santo, Vitória, Brazil","IEEE Geoscience and Remote Sensing Letters","","2017","14","9","1513","1517","High-resolution satellite imagery has been increasingly used on remote sensing classification problems. One of the main factors is the availability of this kind of data. Despite the high availability, very little effort has been placed on the zebra crossing classification problem. In this letter, crowdsourcing systems are exploited in order to enable the automatic acquisition and annotation of a large-scale satellite imagery database for crosswalks related tasks. Then, this data set is used to train deep-learning-based models in order to accurately classify satellite images that contain or not contain zebra crossings. A novel data set with more than 240000 images from 3 continents, 9 countries, and more than 20 cities was used in the experiments. The experimental results showed that freely available crowdsourcing data can be used to accurately (97.11%) train robust models to perform crosswalk classification on a global scale.","","","10.1109/LGRS.2017.2719863","UFES; CAPES for the scholarships; CNPq; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979607","Crosswalk classification;deep learning;large-scale satellite imagery;zebra crossing classification","Satellites;Google;Urban areas;Training;Data models;Continents;Roads","geophysical image processing;image classification;learning (artificial intelligence);remote sensing;roads","deep learning;large-scale automatic satellite crosswalk classification;high-resolution satellite imagery;remote sensing classification;zebra crossing classification;crowdsourcing system;automatic acquisition;large-scale satellite imagery database;satellite image classification","","8","12","","","","","IEEE","IEEE Journals"
"Remote Sensing Image Scene Classification: Benchmark and State of the Art","G. Cheng; J. Han; X. Lu","School of Automation, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Automation, Northwestern Polytechnical University, Xi’an, Shaanxi, China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, Shaanxi, P.R. China","Proceedings of the IEEE","","2017","105","10","1865","1883","Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various data sets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning data sets and methods for scene classification is still lacking. In addition, almost all existing data sets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale data set, termed “NWPU-RESISC45,” which is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This data set contains 31 500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 1) is large-scale on the scene classes and the total image number; 2) holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion; and 3) has high within-class diversity and between-class similarity. The creation of this data set will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed data set, and the results are reported as a useful baseline for future research.","","","10.1109/JPROC.2017.2675998","National Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891544","Benchmark data set;deep learning;handcrafted features;remote sensing image;scene classification;unsupervised feature learning","Remote sensing;Benchmark testing;Spatial resolution;Social network services;Satellites;Image analysis;Machine learning;Unsupervised learning;Classification","geophysical image processing;learning (artificial intelligence);remote sensing","remote sensing image scene classification;various data sets;scene classification;data sets;image numbers;image variations;image diversity;learning based methods;NWPU-RESISC45;RESISC;Northwestern Polytechnical University;NWPU;representative methods;data-driven algorithms","","131","176","Traditional","","","","IEEE","IEEE Journals"
"Deep Multitask Learning for Railway Track Inspection","X. Gibert; V. M. Patel; R. Chellappa","University of Maryland, X, College Park, Mountain View, MD, CA, USAUSA; Department of Electrical Engineering, Rutgers University, Piscataway, NJ, USA; Department of Electrical Engineering and the Center for Automation Research, University of Maryland Institute for Advanced Computer Studies, College Park, MD, USA","IEEE Transactions on Intelligent Transportation Systems","","2017","18","1","153","164","Railroad tracks need to be periodically inspected and monitored to ensure safe transportation. Automated track inspection using computer vision and pattern recognition methods has recently shown the potential to improve safety by allowing for more frequent inspections while reducing human errors. Achieving full automation is still very challenging due to the number of different possible failure modes, as well as the broad range of image variations that can potentially trigger false alarms. In addition, the number of defective components is very small, so not many training examples are available for the machine to learn a robust anomaly detector. In this paper, we show that detection performance can be improved by combining multiple detectors within a multitask learning framework. We show that this approach results in improved accuracy for detecting defects on railway ties and fasteners.","","","10.1109/TITS.2016.2568758","Federal Railroad Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506117","Deep convolutional neural networks;material identification;multitask learning;railway track inspection","Fasteners;Inspection;Rail transportation;Rails;Neural networks;Training;Detectors","automatic optical inspection;computer vision;failure analysis;learning (artificial intelligence);object detection;railway safety;railways","deep multitask learning;railway track inspection;safe transportation;automated track inspection;computer vision;pattern recognition;human error reduction;failure modes;image variations;false alarms;anomaly detector;defects detection;railway ties;railway fasteners","","62","48","","","","","IEEE","IEEE Journals"
"Representation Learning Based Speech Assistive System for Persons With Dysarthria","S. Chandrakala; N. Rajeswari","School of Computing, SASTRA University, Thanjavur, India; Department of Computer Science and Engineering, Sri Venkateswara College of Engineering, Sriperumbudur","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","2017","25","9","1510","1517","An assistive system for persons with vocal impairment due to dysarthria converts dysarthric speech to normal speech or text. Because of the articulatory deficits, dysarthric speech recognition needs a robust learning technique. Representation learning is significant for complex tasks such as dysarthric speech recognition. We focus on robust representation for dysarthric speech recognition that involves recognizing sequential patterns of varying length utterances. We propose a hybrid framework that uses a generative learning based data representation with a discriminative learning based classifier. In this hybrid framework, we propose to use Example Specific Hidden Markov Models (ESHMMs) to obtain log-likelihood scores for a dysarthric speech utterance to form fixed dimensional score vector representation. This representation is used as an input to discriminative classifier such as support vector machine. The performance of the proposed approach is evaluated using UA-Speech database.The recognition accuracy is much better than the conventional hidden Markov model based approach and Deep Neural Network-Hidden Markov Model (DNN-HMM). The efficiency of the discriminative nature of score vector representation is proved for “very low” intelligibility words.","","","10.1109/TNSRE.2016.2638830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782309","Dysarthric speech recognition;example specific hidden Markov model (ESHMMs);representation learning;score vector representation;support vector machine;varying length sequential data","Hidden Markov models;Speech recognition;Speech;Databases;Mel frequency cepstral coefficient;Support vector machines;Feature extraction","data structures;diseases;hidden Markov models;learning (artificial intelligence);pattern classification;speech;speech intelligibility;speech recognition;support vector machines","representation learning based speech assistive system;vocal impairment;normal speech;normal text;articulatory deficits;dysarthric speech recognition;complex tasks;sequential patterns;length utterances;generative learning based data representation;discriminative learning based classifier;example specific hidden Markov models;log-likelihood scores;dysarthric speech utterance;fixed dimensional score vector representation;support vector machine;UA-Speech database;recognitionaccuracy;conventional hidden Markov model;deep neural network-hidden Markov model;DNN-HMM;score vector representation;very-low intelligibility words","Algorithms;Communication Aids for Disabled;Dysarthria;Humans;Machine Learning;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Speech Intelligibility;Speech Production Measurement;Treatment Outcome","3","36","","","","","IEEE","IEEE Journals"
"Deep Learning Based Binaural Speech Separation in Reverberant Environments","X. Zhang; D. Wang","Department of Computer Science, Inner Mongolia University, Hohhot, China; Department of Computer Science and Engineering, Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","5","1075","1084","Speech signal is usually degraded by room reverberation and additive noises in real environments. This paper focuses on separating target speech signal in reverberant conditions from binaural inputs. Binaural separation is formulated as a supervised learning problem, and we employ deep learning to map from both spatial and spectral features to a training target. With binaural inputs, we first apply a fixed beamformer and then extract several spectral features. A new spatial feature is proposed and extracted to complement the spectral features. The training target is the recently suggested ideal ratio mask. Systematic evaluations and comparisons show that the proposed system achieves very good separation performance and substantially outperforms related algorithms under challenging multisource and reverberant environments.","","","10.1109/TASLP.2017.2687104","National Natural Science Foundation of China; Air Force Office of Scientific Research; National Institute on Deafness and Other Communication Disorders; The Ohio State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886357","Beamforming;binaural speech separation;computational auditory scene analysis (CASA);deep neural network (DNN);room reverberation","Speech;Feature extraction;Training;Microphones;Speech processing;Array signal processing;Reverberation","array signal processing;feature extraction;learning (artificial intelligence);reverberation;source separation;spectral analysis;speech processing","binaural speech separation;room reverberation;additive noises;speech signal separation;supervised learning;fixed beamformer;spectral feature extraction;spatial feature extraction;separation performance;multisource environments;deep learning","","28","39","","","","","IEEE","IEEE Journals"
"A Deep Learning Model for Robust Wafer Fault Monitoring With Sensor Measurement Noise","H. Lee; Y. Kim; C. O. Kim","Department of Information and Industrial Engineering, Yonsei University, Seoul, South Korea; Department of Information and Industrial Engineering, Yonsei University, Seoul, South Korea; Department of Information and Industrial Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Semiconductor Manufacturing","","2017","30","1","23","31","Standard fault detection and classification (FDC) models detect wafer faults by extracting features useful for fault detection from time-indexed measurements of the equipment recorded by in situ sensors (sensor signals) and feeding the extracted information into a classifier. However, the preprocessing-and-classification approach often results in the loss of information in the sensor signals that is important for detecting wafer faults. Furthermore, the sensor signals usually contain noise induced by mechanical and electrical disturbances. In this paper, we propose the use of a stacked denoising autoencoder (SdA), which is a deep learning algorithm, to establish an FDC model for simultaneous feature extraction and classification. The SdA model can identify global and invariant features in the sensor signals for fault monitoring and is robust against measurement noise. Through experiments using wafer samples collected from a work-site photolithography tool, we confirmed that as the sensor measurement noise severity increased, the SdA's classification accuracy could be as much as 14% higher than those of the twelve models considered for comparison, each of which employed one of three feature extractors and one of four classifiers.","","","10.1109/TSM.2016.2628865","Technology Innovation Program (Development of Big-Data-Based Analysis and Control Platforms for Semiconductor Manufacturing Plants) by the Ministry of Trade, Industry, and Energy, MOTIE, South Korea; National Research Foundation of Korea through the Korean Government (MSIP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744687","Semiconductor manufacturing;fault detection and classification;sensor measurement noise;deep learning;stacked denoising autoencoder","Feature extraction;Semiconductor device modeling;Machine learning;Biological neural networks;Semiconductor device measurement;Noise reduction","fault diagnosis;feature extraction;semiconductor device manufacture;semiconductor device measurement;semiconductor device noise;sensors","deep learning model;robust wafer fault monitoring;sensor measurement noise;standard fault detection and classification models;time-indexed measurements;stacked denoising autoencoder;feature extraction;feature classification","","22","30","","","","","IEEE","IEEE Journals"
"Direct Multitype Cardiac Indices Estimation via Joint Representation and Regression Learning","W. Xue; A. Islam; M. Bhaduri; S. Li","Department of Medical Imaging, Western University, London, ON, Canada; Department of Medical Imaging, Western University, London, ON, Canada; Department of Medical Imaging, Western University, London, ON, Canada; Department of Medical Imaging, Western University, London, ON, Canada","IEEE Transactions on Medical Imaging","","2017","36","10","2057","2067","Cardiac indices estimation is of great importance during identification and diagnosis of cardiac disease in clinical routine. However, estimation of multitype cardiac indices with consistently reliable and high accuracy is still a great challenge due to the high variability of cardiac structures and the complexity of temporal dynamics in cardiac MR sequences. While efforts have been devoted into cardiac volumes estimation through feature engineering followed by a independent regression model, these methods suffer from the vulnerable feature representation and incompatible regression model. In this paper, we propose a semi-automated method for multitype cardiac indices estimation. After the manual labeling of two landmarks for ROI cropping, an integrated deep neural network Indices-Net is designed to jointly learn the representation and regression models. It comprises two tightly-coupled networks, such as a deep convolution autoencoder for cardiac image representation, and a multiple output convolution neural network for indices regression. Joint learning of the two networks effectively enhances the expressiveness of image representation with respect to cardiac indices, and the compatibility between image representation and indices regression, thus leading to accurate and reliable estimations for all the cardiac indices. When applied with five-fold cross validation on MR images of 145 subjects, Indices-Net achieves consistently low estimation error for LV wall thicknesses (1.44 ± 0.71 mm) and areas of cavity and myocardium (204 ± 133 mm2). It outperforms, with significant error reductions, segmentation method (55.1% and 17.4%), and two-phase direct volume-only methods (12.7% and 14.6%) for wall thicknesses and areas, respectively. These advantages endow the proposed method a great potential in clinical cardiac function assessment.","","","10.1109/TMI.2017.2709251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934404","Multitype cardiac indices;direct estimation;joint learning;deep convolution autoencoder;cardiac MR","Estimation;Image representation;Feature extraction;Image segmentation;Convolution;Myocardium;Volume measurement","biomedical MRI;cardiology;diseases;image coding;image segmentation;image sequences;medical image processing;neural nets;regression analysis","LV wall thicknesses(1.44;areas-of-cavity;myocardium;error reductions;segmentation method;two-phase direct volume-only methods;wall areas;clinical cardiac function assessment;low estimation error;five-fold cross validation;multiple output convolution neural network;cardiac image representation;deep convolution autoencoder;tightly-coupled networks;Indices-Net;integrated deep neural network;ROI cropping;manual labeling;incompatible regression model;vulnerable feature representation;independent regression model;feature engineering;cardiac volumes estimation;cardiac MR sequences;temporal dynamics complexity;multitype cardiac indices;clinical routine;cardiac disease diagnosis;cardiac disease identification;regression learning;joint representation;direct multitype cardiac indices estimation","Heart;Heart Function Tests;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Magnetic Resonance Imaging, Cine;Neural Networks (Computer);Regression Analysis","8","35","Traditional","","","","IEEE","IEEE Journals"
"DeepShape: Deep-Learned Shape Descriptor for 3D Shape Retrieval","J. Xie; G. Dai; F. Zhu; E. K. Wong; Y. Fang","Department of Electrical and Computer Engineering, New York University Abu Dhabi, Abu Dhabi, UAE; Department of Electrical and Computer Engineering, New York University Abu Dhabi, Abu Dhabi, UAE; Department of Electrical and Computer Engineering, New York University Abu Dhabi, Abu Dhabi, UAE; Department of Computer Science and Engineering, Tandon School of Engineering, New York University, New York, NY; Department of Electrical and Computer Engineering, New York University Abu Dhabi, Abu Dhabi, UAE","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","7","1335","1345","Complex geometric variations of 3D models usually pose great challenges in 3D shape matching and retrieval. In this paper, we propose a novel 3D shape feature learning method to extract high-level shape features that are insensitive to geometric deformations of shapes. Our method uses a discriminative deep auto-encoder to learn deformation-invariant shape features. First, a multiscale shape distribution is computed and used as input to the auto-encoder. We then impose the Fisher discrimination criterion on the neurons in the hidden layer to develop a deep discriminative auto-encoder. Finally, the outputs from the hidden layers of the discriminative auto-encoders at different scales are concatenated to form the shape descriptor. The proposed method is evaluated on four benchmark datasets that contain 3D models with large geometric variations: McGill, SHREC'10 ShapeGoogle, SHREC'14 Human and SHREC'14 Large Scale Comprehensive Retrieval Track Benchmark datasets. Experimental results on the benchmark datasets demonstrate the effectiveness of the proposed method for 3D shape retrieval.","","","10.1109/TPAMI.2016.2596722","New York University Abu Dhabi; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7526450","3D shape retrieval;heat kernel signature;heat diffusion;auto-encoder;Fisher discrimination criterion","Shape;Three-dimensional displays;Heating;Kernel;Feature extraction;Neurons;Solid modeling","computer graphics;feature extraction;image retrieval;learning (artificial intelligence);neural nets;shape recognition","deep-learned shape descriptor;3D shape retrieval;3D shape feature learning method;high-level shape feature extraction;discriminative deep auto-encoder;deformation-invariant shape features;multiscale shape distribution;Fisher discrimination criterion;neurons;McGill datasets;SHREC'10 ShapeGoogle datasets;SHREC'14 Human datasets;SHREC'14 Large Scale Comprehensive Retrieval Track Benchmark datasets","","39","34","","","","","IEEE","IEEE Journals"
"Communication-Based Train Control System Performance Optimization Using Deep Reinforcement Learning","L. Zhu; Y. He; F. R. Yu; B. Ning; T. Tang; N. Zhao","State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China; State Key Laboratory of Rail Traffic Control and Safety, Beijing Jiaotong University, Beijing, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Vehicular Technology","","2017","66","12","10705","10717","Communication-based train control (CBTC) systems are automated train control systems based on continuous and bidirectional train-ground communications. CBTC is the direction of future train control systems. Complex channel condition and frequent handoff can severely affect urban rail CBTC train-ground communication performance, and consequently affect CBTC operation efficiency. Current research regarding CBTC does not consider the impact of train-ground communications on train control performance. This paper studies the joint optimization of communication and control in CBTC systems. With the objective to minimize the optimal operation profile tracking error and energy consumption, linear quadratic cost is defined as the control performance measure. In order to ensure train operation safety, the optimization model puts constraints on train control actions related to safety. Moreover, based on the stochastic channel characteristics and the real-time train position information, handoff decision and train control policy are jointly optimized using deep reinforcement learning. Extensive simulation results based on real-field channel measurements illustrate that the proposed optimization method can significantly improve the train control performance in CBTC systems, and CBTC systems need to sacrifice part of performance to ensure system safety.","","","10.1109/TVT.2017.2724060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7971968","Communication systems;learning (artificial intelligence);train control","Safety;Learning (artificial intelligence);Railway transportation;Control systems;Optimization;Delays;Energy consumption","learning (artificial intelligence);mobility management (mobile radio);optimisation;rail traffic control;railway communication;railway safety;stochastic processes","continuous train-ground communications;bidirectional train-ground communications;urban rail CBTC train-ground communication performance;CBTC operation efficiency;optimal operation profile tracking error;train operation safety;handoff decision;deep reinforcement learning;train control system performance optimization;train position information;communication-based train control system;energy consumption;linear quadratic cost;stochastic channel characteristics","","6","41","Traditional","","","","IEEE","IEEE Journals"
"A Hierarchical Fused Fuzzy Deep Neural Network for Data Classification","Y. Deng; Z. Ren; Y. Kong; F. Bao; Q. Dai","Automation Department, Tsinghua University, Beijing, China; Automation Department, Tsinghua University, Beijing, China; School of Computer Science, Southeast University, Nanjing, China; Automation Department, Tsinghua University, Beijing, China; Automation Department, Tsinghua University, Beijing, China","IEEE Transactions on Fuzzy Systems","","2017","25","4","1006","1012","Deep learning (DL) is an emerging and powerful paradigm that allows large-scale task-driven feature learning from big data. However, typical DL is a fully deterministic model that sheds no light on data uncertainty reductions. In this paper, we show how to introduce the concepts of fuzzy learning into DL to overcome the shortcomings of fixed representation. The bulk of the proposed fuzzy system is a hierarchical deep neural network that derives information from both fuzzy and neural representations. Then, the knowledge learnt from these two respective views are fused altogether forming the final data representation to be classified. The effectiveness of the model is verified on three practical tasks of image categorization, high-frequency financial data prediction and brain MRI segmentation that all contain high level of uncertainties in the raw data. The fuzzy dDL paradigm greatly outperforms other nonfuzzy and shallow learning approaches on these tasks.","","","10.1109/TFUZZ.2016.2574915","National Natural Science Foundation of China; NSF of Jiangsu Province, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482843","Feature learning;fuzzy neural networks;pattern classification","Biological neural networks;Uncertainty;Fuzzy logic;Neurons;Tuning;Fuzzy systems;Magnetic resonance imaging","Big Data;data structures;fuzzy set theory;hierarchical systems;learning (artificial intelligence);neural nets;pattern classification","hierarchical fused fuzzy deep neural network;data classification;deep learning;large-scale task-driven feature learning;big data;data uncertainty reductions;fuzzy learning;data representation","","22","34","","","","","IEEE","IEEE Journals"
"New Algorithms for Encoding, Learning and Classification of fMRI Data in a Spiking Neural Network Architecture: A Case on Modeling and Understanding of Dynamic Cognitive Processes","N. Kasabov; L. Zhou; M. Gholami Doborjeh; Z. Gholami Doborjeh; J. Yang","Knowledge Engineering and Discovery Research Institute, Auckland University of Technology, Auckland, New Zealand; Shanghai Jiao Tong University, Shanghai, China; Knowledge Engineering and Discovery Research Institute, Auckland University of Technology, Auckland, New Zealand; Knowledge Engineering and Discovery Research Institute, Auckland University of Technology, Auckland, New Zealand; Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Cognitive and Developmental Systems","","2017","9","4","293","303","This paper argues that, the third generation of neural networks-the spiking neural networks (SNNs), can be used to model dynamic, spatio-temporal, cognitive brain processes measured as functional magnetic resonance imaging (fMRI) data. This paper proposes a novel method based on the NeuCube SNN architecture for which the following new algorithms are introduced: fMRI data encoding into spike sequences; deep unsupervised learning of fMRI data in a 3-D SNN reservoir; classification of cognitive states; and connectivity visualization and analysis for the purpose of understanding cognitive dynamics. The method is illustrated on two case studies of cognitive data modeling from a benchmark fMRI data set of seeing a picture versus reading a sentence.","","","10.1109/TCDS.2016.2636291","Knowledge Engineering and Discovery Research Institute of the Auckland University of Technology; Key Laboratory of the Ministry of Education for System Control and Information Processing of Shanghai Jiao Tong University; Tripartite Project Between AUT-SJTU and Xinjiang University, China, through the Education New Zealand and the Ministry of Education in China; NSFC, China; 973 Plan, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776755","Brain functional connectivity;classification;deep learning in spiking neural networks;functional magnetic resonance imaging (fMRI) data;NeuCube;neuromorphic cognitive systems;perceptual dynamics;spiking neural networks (SNNs)","Neurons;Brain modeling;Heuristic algorithms;Data models;Encoding;Classification algorithms;Neural networks;Functional magnetic resonance imaging;Neuromorphics","biomedical MRI;brain;brain-computer interfaces;cognition;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;unsupervised learning","encoding learning;benchmark fMRI data set;cognitive data;cognitive dynamics;cognitive states;3-D SNN reservoir;deep unsupervised learning;spike sequences;NeuCube SNN architecture;functional magnetic resonance imaging data;cognitive brain processes;spiking neural networks;dynamic cognitive processes;spiking neural network architecture","","4","48","","","","","IEEE","IEEE Journals"
"SAR Automatic Target Recognition Based on Euclidean Distance Restricted Autoencoder","S. Deng; L. Du; C. Li; J. Ding; H. Liu","National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","7","3323","3333","Deep learning algorithms have been introduced into target recognition of synthetic aperture radar (SAR) images for extracting deep features because of its accuracy on various recognition problems with sufficient training samples. However, applying deep structures in recognizing SAR images may suffer lack of training samples. Therefore, a deep learning method is proposed in this study based on a multilayer autoencoder (AE) combined with a supervised constraint. We bind the original AE algorithm with a restriction based on Euclidean distance to use the limited training images well. Moreover, a dropout step is added to our algorithm, which is designed to prevent overfitting caused by supervised learning. Experimental results on the MSTAR dataset demonstrate the effectiveness of the proposed method on real SAR images.","","","10.1109/JSTARS.2017.2670083","National Science Foundation of China; Foundation for Doctoral Supervisor of PR China; Science Foundation of Shaanxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7879824","Autoencoder   (AE);  deep   learning;  dropout;Euclidean distance restriction;synthetic aperture radar (SAR) imagery;target recognition","Training;Synthetic aperture radar;Machine learning;Euclidean distance;Feature extraction;Image reconstruction;Target recognition","feature extraction;image recognition;learning (artificial intelligence);object recognition;radar computing;radar imaging;synthetic aperture radar","SAR automatic target recognition;Euclidean distance restricted autoencoder;synthetic aperture radar images;deep learning algorithm;deep feature extraction;multilayer autoencoder;supervised constraint;overfitting prevention;MSTAR dataset;supervised learning","","17","44","","","","","IEEE","IEEE Journals"
"Land Cover Classification via Multitemporal Spatial Data by Deep Recurrent Neural Networks","D. Ienco; R. Gaetano; C. Dupaquier; P. Maurel","UMR-TETIS Laboratory, IRSTEA, Montpellier, France; UMR-TETIS Laboratory, CIRAD, Montpellier, France; UMR-TETIS Laboratory, IRSTEA, Montpellier, France; UMR-TETIS Laboratory, IRSTEA, Montpellier, France","IEEE Geoscience and Remote Sensing Letters","","2017","14","10","1685","1689","Nowadays, modern earth observation programs produce huge volumes of satellite images time series that can be useful to monitor geographical areas through time. How to efficiently analyze such a kind of information is still an open question in the remote sensing field. Recently, deep learning methods proved suitable to deal with remote sensing data mainly for scene classification(i.e., convolutional neural networks on single images) while only very few studies exist involving temporal deep learning approaches [i.e., recurrent neural networks (RNNs)] to deal with remote sensing time series. In this letter, we evaluate the ability of RNNs, in particular, the long short-term memory (LSTM) model, to perform land cover classification considering multitemporal spatial data derived from a time series of satellite images. We carried out experiments on two different data sets considering both pixel-based and object-based classifications. The obtained results show that RNNs are competitive compared with the state-of-the-art classifiers, and may outperform classical approaches in the presence of low represented and/or highly mixed classes. We also show that the alternative feature representation generated by LSTM can improve the performances of standard classifiers.","","","10.1109/LGRS.2017.2728698","National Research Agency in the framework of the Program Investissements d’Avenir for the GEOSUD Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8006221","Deep learning;land cover classification;recurrent neural networks (RNNs);satellite image time series","Time series analysis;Standards;Remote sensing;Recurrent neural networks;Feature extraction;Neurons;Satellites","environmental monitoring (geophysics);geophysical image processing;image classification;land cover;learning (artificial intelligence);neural nets;remote sensing;time series","land cover classification;multitemporal spatial data;deep recurrent neural networks;Earth observation programs;satellite images time series;geographical area monitoring;deep learning method;remote sensing data;scene classification;convolutional neural networks;long short-term memory;LSTM model;pixel-based classification;object-based classification","","10","18","Traditional","","","","IEEE","IEEE Journals"
"Hyperspectral Image Superresolution by Transfer Learning","Y. Yuan; X. Zheng; X. Lu","Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","5","1963","1974","Hyperspectral image superresolution is a highly attractive topic in computer vision and has attracted many researchers' attention. However, nearly all the existing methods assume that multiple observations of the same scene are required with the observed low-resolution hyperspectral image. This limits the application of superresolution. In this paper, we propose a new framework to enhance the resolution of hyperspectral images by exploiting the knowledge from natural images: The relationship between low/high-resolution images is the same as that between low/high-resolution hyperspectral images. In the proposed framework, the mapping between low- and high-resolution images can be learned by deep convolutional neural network and be transferred to hyperspectral image by borrowing the idea of transfer learning. In addition, to study the spectral characteristic between low- and high-resolution hyperspectral image, collaborative nonnegative matrix factorization (CNMF) is proposed to enforce collaborations between the low- and high-resolution hyperspectral images, which encourages the estimated solution to extract the same endmembers with low-resolution hyperspectral image. The experimental results on ground based and remote sensing data suggest that the proposed method achieves comparable performance without requiring any auxiliary images of the same scene.","","","10.1109/JSTARS.2017.2655112","National Basic Research Program of China (Youth 973 Program); National Natural Science of China; National Natural Science of China; National Natural Science Foundation of China; Chinese Academy of Sciences; Open Research Fund of the Key Laboratory of Spectral Imaging Technology; Chinese Academy of Sciences; Young Top-Notch Talent Program; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7855724","Collaborative nonnegative matrix factorization (CNMF);convolutional neural network (CNN);hyperspectral image (HSI) superresolution","Spatial resolution;Hyperspectral imaging;Collaboration;Optical imaging","computer vision;convolution;geophysical image processing;hyperspectral imaging;image resolution;learning (artificial intelligence);matrix decomposition;neural nets","hyperspectral image superresolution;transfer learning;computer vision;deep convolutional neural network;collaborative nonnegative matrix factorization;CNMF","","60","58","","","","","IEEE","IEEE Journals"
"Bayesian Unsupervised Batch and Online Speaker Adaptation of Activation Function Parameters in Deep Models for Automatic Speech Recognition","Z. Huang; S. M. Siniscalchi; C. Lee","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Faculty of Architecture and Engineering, University of Enna “Kore,”, Enna, Italy; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","1","64","75","We present a Bayesian framework to obtain maximum a posteriori (MAP) estimation of a small set of hidden activation function parameters in context-dependent-deep neural network-hidden markov model (CD-DNN-HMM)-based automatic speech recognition (ASR) systems. When applied to speaker adaptation, we aim at transfer learning from a well-trained deep model for a “general” usage to a “personalized” model geared toward a particular talker by using a collection of speaker-specific data. To make the framework applicable to practical situations, we perform adaptation in an unsupervised manner assuming that the transcriptions of the adaptation utterances are not readily available to the ASR system. We conduct a series of comprehensive batch adaptation experiments on the Switchboard ASR task and show that the proposed approach is effective even with CD-DNN-HMM built with discriminative sequential training. Indeed, MAP speaker adaptation reduces the word error rate (WER) to 20.1% from an initial 21.9% on the full NIST 2000 Hub5 benchmark test set. Moreover, MAP speaker adaptation compares favorably with other techniques evaluated on the same speech tasks. We also demonstrate its complementarity to other approaches by applying MAP adaptation to CD-DNN-HMM trained with speaker adaptive features generated through constrained maximum likelihood linear regression and further reduces the WER to 18.6%. Leveraging upon the intrinsic recursive nature in Bayesian adaptation and mitigating possible system constraints on batch learning, we also proposed an incremental approach to unsupervised online speaker adaptation by simultaneously updating the hyperparameters of the approximate posterior densities and the DNN parameters sequentially. The advantage of such a sequential learning algorithm over a batch method is not necessarily in the final performance, but in computational efficiency and reduced storage needs, without having to wait for all the data to be processed. So far, the experimental results are promising.","","","10.1109/TASLP.2016.2621669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7707403","Automatic speech recognition;Bayesian learning;deep neural networks;online adaptation;prior evolution;transfer learning;unsupervised speaker adaptation","Adaptation models;Hidden Markov models;Speech;Acoustics;Bayes methods;Speech recognition;Data models","Bayes methods;hidden Markov models;maximum likelihood estimation;regression analysis;speech recognition","Bayesian unsupervised batch;online speaker adaptation;activation function parameters;deep models;automatic speech recognition;Bayesian framework;maximum a posteriori;MAP estimation;hidden activation function parameters;context-dependent-deep neural network-Hidden Markov model;CD-DNN-HMM;ASR systems;transfer learning;discriminative sequential training;word error rate;WER;constrained maximum likelihood linear regression;Bayesian adaptation;unsupervised online speaker adaptation;approximate posterior densities;DNN parameters","","2","60","","","","","IEEE","IEEE Journals"
"An Investigation of Deep-Learning Frameworks for Speaker Verification Antispoofing","C. Zhang; C. Yu; J. H. L. Hansen","Center for Robust Speech Systems, Erik Jonsson School of Engineering and Computer Science, University of Texas at Dallas, Richardson, TX, USA; Center for Robust Speech Systems, Erik Jonsson School of Engineering and Computer Science, University of Texas at Dallas, Richardson, TX, USA; Center for Robust Speech Systems, Erik Jonsson School of Engineering and Computer Science, University of Texas at Dallas, Richardson, TX, USA","IEEE Journal of Selected Topics in Signal Processing","","2017","11","4","684","694","In this study, we explore the use of deep-learning approaches for spoofing detection in speaker verification. Most spoofing detection systems that have achieved recent success employ hand-craft features with specific spoofing prior knowledge, which may limit the feasibility to unseen spoofing attacks. We aim to investigate the genuine-spoofing discriminative ability from the back-end stage, utilizing recent advancements in deep-learning research. In this paper, alternative network architectures are exploited to target spoofed speech. Based on this analysis, a novel spoofing detection system, which simultaneously employs convolutional neural networks (CNNs) and recurrent neural networks (RNNs) is proposed. In this framework, CNN is treated as a convolutional feature extractor applied on the speech input. On top of the CNN processed output, recurrent networks are employed to capture long-term dependencies across the time domain. Novel features including Teager energy operator critical band autocorrelation envelope, perceptual minimum variance distortionless response, and a more general spectrogram are also investigated as inputs to our proposed deep-learning frameworks. Experiments using the ASVspoof 2015 Corpus show that the integrated CNN-RNN framework achieves state-of-the-art single-system performance. The addition of score-level fusion further improves system robustness. A detailed analysis shows that our proposed approach can potentially compensate for the issue due to short duration test utterances, which is also an issue in the evaluation corpus.","","","10.1109/JSTSP.2016.2647199","Air Force Research Laboratory; University of Texas at Dallas; Distinguished University Chair in Telecommunications Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815339","Convolutional neural networks;PMVDR;recurrent neural networks;spectrogram;spoofing detection;TEO-CB-Auto-Env","Speech;Feature extraction;Machine learning;Recurrent neural networks;Context;Spectrogram;Robustness","learning (artificial intelligence);recurrent neural nets;speaker recognition;time-domain analysis","speaker verification antispoofing;deep-learning framework;spoofing detection;hand-craft feature;genuine-spoofing discriminative ability;convolutional neural network;recurrent neural network;time domain;Teager energy operator critical band autocorrelation envelope;perceptual minimum variance distortionless response;ASVspoof 2015 Corpus;integrated CNN-RNN framework;score-level fusion","","30","53","","","","","IEEE","IEEE Journals"
"Deep 6-DOF Tracking","M. Garon; J. Lalonde","Université Laval; Université Laval","IEEE Transactions on Visualization and Computer Graphics","","2017","23","11","2410","2418","We present a temporal 6-DOF tracking method which leverages deep learning to achieve state-of-the-art performance on challenging datasets of real world capture. Our method is both more accurate and more robust to occlusions than the existing best performing approaches while maintaining real-time performance. To assess its efficacy, we evaluate our approach on several challenging RGBD sequences of real objects in a variety of conditions. Notably, we systematically evaluate robustness to occlusions through a series of sequences where the object to be tracked is increasingly occluded. Finally, our approach is purely data-driven and does not require any hand-designed features: robust tracking is automatically learned from data.","","","10.1109/TVCG.2017.2734599","FRQ-NT New Researcher; NSERC; REPARTI; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8007334","Tracking;Deep Learning;Augmented Reality","Robustness;Cameras;Real-time systems;Rendering (computer graphics);Three-dimensional displays;Machine learning;Neural networks;Augmented reality;Tracking","computer vision;image sequences;learning (artificial intelligence);object tracking;pose estimation","real-time performance;robust tracking;tracking method;deep learning;state-of-the-art performance;datasets;RGBD sequences","","7","34","Traditional","","","","IEEE","IEEE Journals"
"A Comprehensive Study on Cross-View Gait Based Human Identification with Deep CNNs","Z. Wu; Y. Huang; L. Wang; X. Wang; T. Tan","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Chinese University of Hong Kong, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","2","209","226","This paper studies an approach to gait based human identification via similarity learning by deep convolutional neural networks (CNNs). With a pretty small group of labeled multi-view human walking videos, we can train deep networks to recognize the most discriminative changes of gait patterns which suggest the change of human identity. To the best of our knowledge, this is the first work based on deep CNNs for gait recognition in the literature. Here, we provide an extensive empirical evaluation in terms of various scenarios, namely, cross-view and cross-walking-condition, with different preprocessing approaches and network architectures. The method is first evaluated on the challenging CASIA-B dataset in terms of cross-view gait recognition. Experimental results show that it outperforms the previous state-of-the-art methods by a significant margin. In particular, our method shows advantages when the cross-view angle is large, i.e., no less than 36 degree. And the average recognition rate can reach 94 percent, much better than the previous best result (less than 65 percent). The method is further evaluated on the OU-ISIR gait dataset to test its generalization ability to larger data. OU-ISIR is currently the largest dataset available in the literature for gait recognition, with 4,007 subjects. On this dataset, the average accuracy of our method under identical view conditions is above 98 percent, and the one for cross-view scenarios is above 91 percent. Finally, the method also performs the best on the USF gait dataset, whose gait sequences are imaged in a real outdoor scene. These results show great potential of this method for practical applications.","","","10.1109/TPAMI.2016.2545669","National Basic Research Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439821","Deep learning;CNN;human identification;gait;cross-view","Gait recognition;Videos;Probes;Face;Legged locomotion;Three-dimensional displays;Feature extraction","feedforward neural nets;gait analysis;image motion analysis;image sequences;learning (artificial intelligence);video signal processing","cross-view gait based human identification;deep CNN;similarity learning;deep convolutional neural networks;labeled multiview human walking videos;gait patterns;human identity;cross-walking-condition;cross-view-condition;CASIA-B dataset;cross-view gait recognition;OU-ISIR gait dataset;identical view conditions;USF gait dataset;gait sequences;real outdoor scene","","108","58","","","","","IEEE","IEEE Journals"
"Human Activity Classification With Transmission and Reflection Coefficients of On-Body Antennas Through Deep Convolutional Neural Networks","Y. Kim; Y. Li","Electrical and Computer Engineering Department, California State University, Fresno, CA, USA; Electrical and Computer Engineering Department, Baylor University, Waco, TX, USA","IEEE Transactions on Antennas and Propagation","","2017","65","5","2764","2768","We propose to classify human activities based on transmission coefficient (S21) and reflection coefficient (S11) of on-body antennas with deep convolutional neural networks (DCNNs). It is shown that spectrograms of S21 and S11 exhibit unique time-varying signatures for different body motion activities that can be used for classification purposes. DCNN, a deep learning approach, is applied to spectrograms to learn the necessary features and classification boundaries. It is found that DCNN can achieve classification accuracies of 98.8% using S21 and 97.1% using S11. The effects of operating frequency and antenna location on the accuracy have been investigated.","","","10.1109/TAP.2017.2677918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7870576","Convolutional neural network (CNN);deep learning;human activity classification;joint time-frequency transform;on-body channel","Wrist;Spectrogram;Time-frequency analysis;Machine learning;Transmitting antennas","neural nets;telecommunication computing;wearable antennas","human activity classification;reflection coefficients;transmission coefficients;on-body antennas;deep convolutional neural network;DCNN;spectrograms;time-varying signatures;body motion activities;deep learning approach;operating frequency;antenna location","","14","20","","","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning With Visual Attention for Vehicle Classification","D. Zhao; Y. Chen; L. Lv","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Cognitive and Developmental Systems","","2017","9","4","356","367","Automatic vehicle classification is crucial to intelligent transportation system, especially for vehicle-tracking by police. Due to the complex lighting and image capture conditions, image-based vehicle classification in real-world environments is still a challenging task and the performance is far from being satisfactory. However, owing to the mechanism of visual attention, the human vision system shows remarkable capability compared with the computer vision system, especially in distinguishing nuances processing. Inspired by this mechanism, we propose a convolutional neural network (CNN) model of visual attention for image classification. A visual attention-based image processing module is used to highlight one part of an image and weaken the others, generating a focused image. Then the focused image is input into the CNN to be classified. According to the classification probability distribution, we compute the information entropy to guide a reinforcement learning agent to achieve a better policy for image classification to select the key parts of an image. Systematic experiments on a surveillance-nature dataset which contains images captured by surveillance cameras in the front view, demonstrate that the proposed model is more competitive than the large-scale CNN in vehicle classification tasks.","","","10.1109/TCDS.2016.2614675","National Natural Science Foundation of China; National Key Research and Development Plan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7580631","Convolutional neural network (CNN);reinforcement learning;vehicle classification;visual attention","Convolutional neural networks;Visualization;Learning (artificial intelligence);Feature extraction;Cameras;Computer architecture;Information entropy","computer vision;feature extraction;image classification;learning (artificial intelligence);object detection;traffic engineering computing","automatic vehicle classification;intelligent transportation system;vehicle-tracking;complex lighting;human vision system;computer vision system;convolutional neural network model;CNN;image classification;visual attention-based image processing module;focused image;classification probability distribution;vehicle classification tasks;deep reinforcement learning","","26","43","","","","","IEEE","IEEE Journals"
"Learning Aerial Image Segmentation From Online Maps","P. Kaiser; J. D. Wegner; A. Lucchi; M. Jaggi; T. Hofmann; K. Schindler","ETH Zürich, Zürich, Switzerland; ETH Zürich, Zürich, Switzerland; ETH Zürich, Zürich, Switzerland; ETH Zürich, Zürich, Switzerland; ETH Zürich, Zürich, Switzerland; ETH Zürich, Zürich, Switzerland","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","11","6054","6068","This paper deals with semantic segmentation of high-resolution (aerial) images where a semantic class label is assigned to each pixel via supervised classification as a basis for automatic map generation. Recently, deep convolutional neural networks (CNNs) have shown impressive performance and have quickly become the de-facto standard for semantic segmentation, with the added benefit that task-specific feature design is no longer necessary. However, a major downside of deep learning methods is that they are extremely data hungry, thus aggravating the perennial bottleneck of supervised classification, to obtain enough annotated training data. On the other hand, it has been observed that they are rather robust against noise in the training labels. This opens up the intriguing possibility to avoid annotating huge amounts of training data, and instead train the classifier from existing legacy data or crowd-sourced maps that can exhibit high levels of noise. The question addressed in this paper is: can training with large-scale publicly available labels replace a substantial part of the manual labeling effort and still achieve sufficient performance? Such data will inevitably contain a significant portion of errors, but in return virtually unlimited quantities of it are available in larger parts of the world. We adapt a state-of-the-art CNN architecture for semantic segmentation of buildings and roads in aerial images, and compare its performance when using different training data sets, ranging from manually labeled pixel-accurate ground truth of the same city to automatic training data derived from OpenStreetMap data from distant locations. We report our results that indicate that satisfying performance can be obtained with significantly less manual annotation effort, by exploiting noisy large-scale training data.","","","10.1109/TGRS.2017.2719738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7987710","Crowdsourcing;image classification;machine learning;neural networks;supervised learning;terrain mapping;urban areas","Training data;Training;Roads;Semantics;Image segmentation;Urban areas;Manuals","image classification;image representation;image segmentation;learning (artificial intelligence);neural nets","large-scale publicly available labels;manual labeling effort;semantic segmentation;manually labeled pixel-accurate ground truth;automatic training data;OpenStreetMap data;manual annotation effort;noisy large-scale training data;online maps;high-resolution images;semantic class label;supervised classification;automatic map generation;deep convolutional neural networks;task-specific feature design;deep learning methods;annotated training data;crowd-sourced maps;aerial image segmentation learning;CNN","","18","55","Traditional","","","","IEEE","IEEE Journals"
"Robust Stereo Data Cost With a Learning Strategy","V. D. Nguyen; H. V. Nguyen; J. W. Jeon","School of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; School of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; School of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Transactions on Intelligent Transportation Systems","","2017","18","2","248","258","The performance of stereo matching algorithms strongly depends on the quality of the stereo data/matching cost. Most state-of-the-art data costs require expert knowledge for the design of a transformation function, such as census for handling gray-level changes monotonically, adaptive normalized cross correlation for handling Lambertian cases, guided filtering for preserving edge information, and local density encoding for handling illumination differences. However, it is difficult to design a complex transformation function to handle unknown factors that often occur in driving conditions such as snow, rain, and sun. Therefore, this paper has investigated the deep learning strategy to develop a novel stereo matching cost model without using much expert knowledge. Experimental results show that the proposed deep learning model obtains better results than the state-of-the-art stereo matching cost as judged by the standard KITTI benchmark, Middlebury, and HCI datasets.","","","10.1109/TITS.2016.2563661","Samsung Research Funding Center of Samsung Electronics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7548359","Stereo matching cost;deep learning;unsupervised training;unlabeled data","Machine learning;Robustness;Training;Image color analysis;Feature extraction;Algorithm design and analysis;Snow","edge detection;image matching;learning (artificial intelligence);statistical analysis;stereo image processing","robust stereo data cost;stereo matching algorithms;stereo matching cost;transformation function design;gray-level change handling;adaptive normalized cross correlation;Lambertian case handling;preserving edge information;guided filtering;local density encoding;illumination difference handling;driving conditions;deep learning;KITTI benchmark;Middlebury;HCI datasets","","2","30","","","","","IEEE","IEEE Journals"
"Correction to “Scalable High-Performance Image Registration Framework by Unsupervised Deep Feature Representations Learning” [Jul 16 1505-1516]","G. Wu; M. Kim; Q. Wang; B. C. Munsell; D. Shen","Department of Radiology and BRIC, The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology and BRIC, The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology and BRICThe University of North Carolina at Chapel Hill; Med-X Research Institute of Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, The College of Charleston, Charleston, SC, USA","IEEE Transactions on Biomedical Engineering","","2017","64","1","250","250","Presents corrections to the paper, ""Scalable high performance image registration framework by unsupervised deep feature representations"", (Wu, G. et al.), IEEE Trans. Biomed. Eng., vol. 63, no. 7, pp. 1505–1516, Jul. 2016. ","","","10.1109/TBME.2016.2633139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792251","","Machine learning;Image registration;Unsupervised learning;Scalability","","","","5","1","","","","","IEEE","IEEE Journals"
"Efficient Processing of Deep Neural Networks: A Tutorial and Survey","V. Sze; Y. Chen; T. Yang; J. S. Emer","Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA","Proceedings of the IEEE","","2017","105","12","2295","2329","Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.","","","10.1109/JPROC.2017.2761740","DARPA YFA; MIT CICS; gift from Nvidia; gift from Intel; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8114708","ASIC;computer architecture;convolutional neural networks;dataflow processing;deep learning;deep neural networks;energy-efficient accelerators;low power;machine learning;spatial architectures;VLSI","Neurons;Biological neural networks;Artificial intelligence;Machine learning;Neural networks;Tutorials;Convolutional neural networks;Artificial intelligence;Benchmark testing;Computer architecture","artificial intelligence;computational complexity;neural nets","energy efficiency;hardware design changes;DNN hardware designs;deep neural networks;hardware cost;computation cost reduction;artificial intelligence;computational complexity;hardware platforms;hardware architecture;DNN hardware implementations","","178","165","","","","","IEEE","IEEE Journals"
"Regularized Deep Belief Network for Image Attribute Detection","F. Wu; Z. Wang; W. Lu; X. Li; Y. Yang; J. Luo; Y. Zhuang","College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; University of Technology at Sydney, Sydney, NSW, Australia; Department of Computer Science, University of Rochester, Rochester, NY, USA; College of Computer Science, Zhejiang University, Hangzhou, China","IEEE Transactions on Circuits and Systems for Video Technology","","2017","27","7","1464","1477","In general, an image attribute is a human-nameable visual property that has a semantic connotation. Appropriate modeling of the intrinsic contextual correlations among attributes plays a fundamental role in attribute detection. In this paper, we consider image attribute detection from the perspective of regularized deep learning. In particular, we propose a regularized deep belief network (rDBN) to perform the image attribute detection task, which is composed of two parts: 1) a detection DBN (dDBN) that models the joint distribution of images and their corresponding attributes, which acts as an attribute detector and 2) a contextual restricted Boltzmann machine that explicitly models the correlations among attributes acting as a regularizer that restraints the output detection result given by the dDBN to meet the contextual prior of attributes. Furthermore, we propose an efficient fine-tuning scheme that can further optimize the performance of the dDBN by backpropagation. Experimental results show that the proposed rDBN obtains improvements over the state-of-the-art methods for attribute detection on the benchmark data sets.","","","10.1109/TCSVT.2016.2539604","National Basic Research Program of China; National Natural Science Foundation of China; China Knowledge Center for Engineering Sciences and Technology; Fundamental Research Funds for the Central Universities; New York State through the Goergen Institute for Data Science and the Xerox Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7428944","Contextual correlation;deep belief network (DBN);deep learning;image attribute","Feature extraction;Correlation;Context modeling;Neural networks;Computational modeling;Semantics;Training","backpropagation;belief networks;Boltzmann machines;correlation methods;object detection;semantic networks","image attribute detection;human-nameable visual property;semantic connotation;intrinsic contextual correlations;regularized deep learning;regularized deep belief network;rDBN;image distribution;Boltzmann machine;fine-tuning scheme;backpropagation","","2","53","","","","","IEEE","IEEE Journals"
"Gland Instance Segmentation Using Deep Multichannel Neural Networks","Y. Xu; Y. Li; Y. Wang; M. Liu; Y. Fan; M. Lai; E. I. Chang","State Key Laboratory of Software Development Environment and Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education and Research Institute of Beihang University in ShenzhenBeihang UniversityMicrosoft Research; State Key Laboratory of Software Development Environment and Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education and Research Institute of Beihang University in ShenzhenBeihang University; State Key Laboratory of Software Development Environment and Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education and Research Institute of Beihang University in ShenzhenBeihang University; State Key Laboratory of Software Development Environment and Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education and Research Institute of Beihang University in ShenzhenBeihang University; State Key Laboratory of Software Development Environment and Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education and Research Institute of Beihang University in ShenzhenBeihang University; Department of PathologySchool of MedicineZhejiang University; Microsoft Research, Beijing, China","IEEE Transactions on Biomedical Engineering","","2017","64","12","2901","2912","Objective: A new image instance segmentation method is proposed to segment individual glands (instances) in colon histology images. This process is challenging since the glands not only need to be segmented from a complex background, they must also be individually identified. Methods: We leverage the idea of image-to-image prediction in recent deep learning by designing an algorithm that automatically exploits and fuses complex multichannel information-regional, location, and boundary cues-in gland histology images. Our proposed algorithm, a deep multichannel framework, alleviates heavy feature design due to the use of convolutional neural networks and is able to meet multifarious requirements by altering channels. Results: Compared with methods reported in the 2015 MICCAI Gland Segmentation Challenge and other currently prevalent instance segmentation methods, we observe state-of-the-art results based on the evaluation metrics. Conclusion: The proposed deep multichannel algorithm is an effective method for gland instance segmentation. Significance: The generalization ability of our model not only enable the algorithm to solve gland instance segmentation problems, but the channel is also alternative that can be replaced for a specific task.","","","10.1109/TBME.2017.2686418","Microsoft Research under the eHealth program; Beijing National Science Foundation in China; Technology and Innovation Commission of Shenzhen in China; Beijing Young Talent Project in China; Fundamental Research Funds for the Central Universities of China; State Key Laboratory of Software Development Environment in Beihang University in China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885586","Convolutional neural network;instance segmentation;histology image;multichannel;segmentation","Glands;Image segmentation;Image edge detection;Detectors;Neural networks;Morphology;Feature extraction","biological organs;image segmentation;learning (artificial intelligence);medical image processing;neural nets","gland instance segmentation;deep multichannel neural networks;image instance segmentation method;colon histology images;image-to-image prediction;deep learning;complex multichannel information;gland histology images;multichannel framework;convolutional neural networks;deep multichannel algorithm","Algorithms;Colon;Colorectal Neoplasms;Histocytochemistry;Humans;Image Processing, Computer-Assisted;Intestinal Mucosa;Machine Learning;Neural Networks (Computer)","1","38","Traditional","","","","IEEE","IEEE Journals"
"Machine Learning With Big Data: Challenges and Approaches","A. L’Heureux; K. Grolinger; H. F. Elyamany; M. A. M. Capretz","Department of Electrical and Computer Engineering, Western University, London, ON, Canada; Department of Electrical and Computer Engineering, Western University, London, ON, Canada; Department of Electrical and Computer Engineering, Western University, London, ON, Canada; Department of Electrical and Computer Engineering, Western University, London, ON, Canada","IEEE Access","","2017","5","","7776","7797","The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause-effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.","","","10.1109/ACCESS.2017.2696365","NSERC CRD at Western University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906512","Big Data;Big Data Vs;data analysis;data analytics;deep learning;distributed computing;machine learning;neural networks","Big Data;Machine learning algorithms;Data mining;Algorithm design and analysis;Data analysis;Support vector machines;Classification algorithms","Big Data;data analysis;learning (artificial intelligence);optimisation","big data revolution;process optimization;insight discovery;decision making;data analytics;data driven insights;machine learning approaches;cause-effect relationship;big data V;research gaps","","105","130","CCBY","","","","IEEE","IEEE Journals"
"Face Verification via Class Sparsity Based Supervised Encoding","A. Majumdar; R. Singh; M. Vatsa","IIIT-Delhi, New Delhi, India; IIIT-Delhi, New Delhi, India; IIIT-Delhi, New Delhi, India","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","6","1273","1280","Autoencoders are deep learning architectures that learn feature representation by minimizing the reconstruction error. Using an autoencoder as baseline, this paper presents a novel formulation for a class sparsity based supervised encoder, termed as CSSE. We postulate that features from the same class will have a common sparsity pattern/support in the latent space. Therefore, in the formulation of the autoencoder, a supervision penalty is introduced as a jointsparsity promoting l2;1-norm. The formulation of CSSE is derived for a single hidden layer and it is applied for multiple hidden layers using a greedy layer-bylayer learning approach. The proposed CSSE approach is applied for learning face representation and verification experiments are performed on the LFW and PaSC face databases. The experiments show that the proposed approach yields improved results compared to autoencoders and comparable results with state-ofthe-art face recognition algorithms.","","","10.1109/TPAMI.2016.2569436","Department of Information Technology Ministry of Communications and Information Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7470420","Face verification;deep learning;supervised feature learning;autoencoders","Face;Encoding;Training;Convolution;Machine learning;Face recognition;Algorithm design and analysis","encoding;error analysis;face recognition;image reconstruction;learning (artificial intelligence);minimisation;pose estimation","face verification;class sparsity based supervised encoding;deep learning architectures;autoencoders;reconstruction error minimization;CSSE;single-hidden layer;greedy layer-by-layer learning approach;PaSC face databases;LFW face databases","","19","60","","","","","IEEE","IEEE Journals"
"Hierarchical Representation Learning for Kinship Verification","N. Kohli; M. Vatsa; R. Singh; A. Noore; A. Majumdar","Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA; Indraprastha Institute of Information Technology Delhi, New Delhi, India; Indraprastha Institute of Information Technology Delhi, New Delhi, India; Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA; Indraprastha Institute of Information Technology Delhi, New Delhi, India","IEEE Transactions on Image Processing","","2017","26","1","289","302","Kinship verification has a number of applications such as organizing large collections of images and recognizing resemblances among humans. In this paper, first, a human study is conducted to understand the capabilities of human mind and to identify the discriminatory areas of a face that facilitate kinshipcues. The visual stimuli presented to the participants determine their ability to recognize kin relationship using the whole face as well as specific facial regions. The effect of participant gender and age and kin-relation pair of the stimulus is analyzed using quantitative measures such as accuracy, discriminability index d', and perceptual information entropy. Utilizing the information obtained from the human study, a hierarchical kinship verification via representation learning (KVRL) framework is utilized to learn the representation of different face regions in an unsupervised manner. We propose a novel approach for feature representation termed as filtered contractive deep belief networks (fcDBN). The proposed feature representation encodes relational information present in images using filters and contractive regularization penalty. A compact representation of facial images of kin is extracted as an output from the learned model and a multi-layer neural network is utilized to verify the kin accurately. A new WVU kinship database is created, which consists of multiple images per subject to facilitate kinship verification. The results show that the proposed deep learning framework (KVRL-fcDBN) yields the state-of-the-art kinship verification accuracy on the WVU kinship database and on four existing benchmark data sets. Furthermore, kinship information is used as a soft biometric modality to boost the performance of face verification via product of likelihood ratio and support vector machine based approaches. Using the proposed KVRL-fcDBN framework, an improvement of over 20% is observed in the performance of face verification.","","","10.1109/TIP.2016.2609811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7567585","Kinship verification;face verification;deep belief networks;soft biometrics","Face;Face recognition;Entropy;Indexes;Machine learning;Training","belief networks;face recognition;image representation;learning (artificial intelligence)","hierarchical representation learning;kinship verification;human study;human mind capability;kin relationship;facial region;kin-relation pair;participant gender;participant age;quantitative measure;accuracy measure;discriminability index;perceptual information entropy;KVRL framework;unsupervised learning;filtered contractive deep belief networks;fcDBN;feature representation;contractive regularization penalty;WVU kinship database;KVRL-fcDBN framework","","22","59","","","","","IEEE","IEEE Journals"
"Deep Model Based Domain Adaptation for Fault Diagnosis","W. Lu; B. Liang; Y. Cheng; D. Meng; J. Yang; T. Zhang","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China; Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Industrial Electronics","","2017","64","3","2296","2305","In recent years, machine learning techniques have been widely used to solve many problems for fault diagnosis. However, in many real-world fault diagnosis applications, the distribution of the source domain data (on which the model is trained) is different from the distribution of the target domain data (where the learned model is actually deployed), which leads to performance degradation. In this paper, we introduce domain adaptation, which can find the solution to this problem by adapting the classifier or the regression model trained in a source domain for use in a different but related target domain. In particular, we proposed a novel deep neural network model with domain adaptation for fault diagnosis. Two main contributions are concluded by comparing to the previous works: first, the proposed model can utilize domain adaptation meanwhile strengthening the representative information of the original data, so that a high classification accuracy in the target domain can be achieved, and second, we proposed several strategies to explore the optimal hyperparameters of the model. Experimental results, on several real-world datasets, demonstrate the effectiveness and the reliability of both the proposed model and the exploring strategies for the parameters.","","","10.1109/TIE.2016.2627020","Natural Science Foundation of Guangdong Province; Research Foundation of Shenzhen; National Natural Science Foundation of China; Shenzhen Key Laboratory of Space Robotic Technology and Telescience; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7740016","Deep neural network (DNN);domain adaptation;fault diagnosis","Fault diagnosis;Adaptation models;Neural networks;Training;Data models;Feature extraction;Machine learning","fault diagnosis;learning (artificial intelligence);neural nets;pattern classification;regression analysis","deep model based domain adaptation;fault diagnosis;machine learning techniques;source domain data distribution;target domain data distribution;performance degradation;classifier model;regression model;novel deep neural network model;optimal model hyperparameters","","84","39","","","","","IEEE","IEEE Journals"
"UTiLearn: A Personalised Ubiquitous Teaching and Learning System for Smart Societies","R. Mehmood; F. Alam; N. N. Albogami; I. Katib; A. Albeshri; S. M. Altowaijri","High Performance Computing Center, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Faculty of Computing and Information Technology, Northern Border University, Arar, Saudi Arabia","IEEE Access","","2017","5","","2615","2635","The education industry around the globe is undergoing major transformations. Organizations, such as Coursera are advancing new business models for education. A number of major industries have dropped degrees from the job requirements. While the economics of higher education institutions are under threat in a continuing gloomy global economy, digital and lifelong learners are increasingly demanding new teaching and learning paradigms from educational institutions. There is an urgent need to transform teaching and learning landscape in order to drive global economic growth. The use of distance eTeaching and eLearning (DTL) is on the rise among digital natives alongside our evolution toward smart societies. However, the DTL systems today lack the necessary sophistication due to several challenges including data analysis and management, learner-system interactivity, system cognition, resource planning, agility, and scalability. This paper proposes a personalised Ubiquitous eTeaching & eLearning (UTiLearn) framework that leverages Internet of Things, big data, supercomputing, and deep learning to provide enhanced development, management, and delivery of teaching and learning in smart society settings. A proof of concept UTiLearn system has been developed based on the framework. A detailed design, implementation, and evaluation of the UTiLearn system, including its five components, are provided using 11 widely used datasets.","","","10.1109/ACCESS.2017.2668840","Deanship of Scientific Research, King Abdulaziz University, Jeddah, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7855752","Big data;computational and artificial intelligence;distance learning;high performance computing;Internet of Things","Big Data;Smart cities;Computer aided instruction;Economics;Electronic learning","Big Data;computer aided instruction;continuing professional development;distance learning;educational institutions;further education;Internet of Things;learning (artificial intelligence);parallel processing;societies;teaching","smart societies;education industry;Coursera;business models;job requirements;higher education institution economics;global economy;lifelong learners;digital learners;distance eTeaching and eLearning;DTL systems;digital natives;learner-system interactivity;system cognition;resource planning;data analysis;data management;agility;scalability;personalised ubiquitous eTeaching & eLearning framework;UTiLearn system;supercomputing;Big Data;deep learning","","39","113","","","","","IEEE","IEEE Journals"
"Learning Deep NBNN Representations for Robust Place Categorization","M. Mancini; S. R. Bulò; E. Ricci; B. Caputo","University of Rome La Sapienza, Fondazione Bruno Kessler, Rome, Trento, ItalyItaly; Fondazione Bruno Kessler, Mapillary, Trento, Graz, ItalyAustria; Fondazione Bruno Kessler, University of Perugia, Trento, Perugia, ItalyItaly; University of Rome La Sapienza, Rome, Italy","IEEE Robotics and Automation Letters","","2017","2","3","1794","1801","This letter presents an approach for semantic place categorization using data obtained from RGB cameras. Previous studies on visual place recognition and classification have shown that by considering features derived from pretrained convolutional neural networks (CNNs) in combination with part-based classification models, high recognition accuracy can be achieved, even in the presence of occlusions and severe viewpoint changes. Inspired by these works, we propose to exploit local deep representations, representing images as set of regions applying a Naïve Bayes nearest neighbor (NBNN) model for image classification. As opposed to previous methods, where CNNs are merely used as feature extractors, our approach seamlessly integrates the NBNN model into a fully CNN. Experimental results show that the proposed algorithm outperforms previous methods based on pretrained CNN models and that, when employed in challenging robot place recognition tasks, it is robust to occlusions, environmental and sensor changes.","","","10.1109/LRA.2017.2705282","ERC; CHIST-ERA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7930504","Recognition;semantic scene understanding;visual learning","Feature extraction;Semantics;Training;Computer architecture;Robustness;Robot sensing systems","cameras;feature extraction;feedforward neural nets;image classification;image colour analysis;image representation;learning (artificial intelligence);robot vision","learning deep NBNN representation;robust place categorization;semantic place categorization;RGB cameras;local deep representations;image representation;naive Bayes near-neighbor model;image classification;CNN;convolutional neural networks;robot place recognition tasks","","10","41","","","","","IEEE","IEEE Journals"
"Epithelium-Stroma Classification via Convolutional Neural Networks and Unsupervised Domain Adaptation in Histopathological Images","Y. Huang; H. Zheng; C. Liu; X. Ding; G. K. Rohde","School of Information Science and Engineering, Xiamen University, Xiamen, China; School of Information Science and Engineering, Xiamen University, Xiamen, China; Department of Biomedical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; School of Information Science and Engineering, Xiamen University, Xiamen, China; Electrical Engineering Department and Biomedical Engineering Department, University of Virginia, VA, USA","IEEE Journal of Biomedical and Health Informatics","","2017","21","6","1625","1632","Epithelium-stroma classification is a necessary preprocessing step in histopathological image analysis. Current deep learning based recognition methods for histology data require collection of large volumes of labeled data in order to train a new neural network when there are changes to the image acquisition procedure. However, it is extremely expensive for pathologists to manually label sufficient volumes of data for each pathology study in a professional manner, which results in limitations in real-world applications. A very simple but effective deep learning method, that introduces the concept of unsupervised domain adaptation to a simple convolutional neural network (CNN), has been proposed in this paper. Inspired by transfer learning, our paper assumes that the training data and testing data follow different distributions, and there is an adaptation operation to more accurately estimate the kernels in CNN in feature extraction, in order to enhance performance by transferring knowledge from labeled data in source domain to unlabeled data in target domain. The model has been evaluated using three independent public epithelium-stroma datasets by cross-dataset validations. The experimental results demonstrate that for epithelium-stroma classification, the proposed framework outperforms the state-of-the-art deep neural network model, and it also achieves better performance than other existing deep domain adaptation methods. The proposed model can be considered to be a better option for real-world applications in histopathological image analysis, since there is no longer a requirement for large-scale labeled data in each specified domain.","","","10.1109/JBHI.2017.2691738","National Natural Science Foundation of China; Guangdong Natural Science Foundation; Natural Science Foundation of Fujian Province of China; Fundamental Research Funds; Central Universities; CCF-Tencent; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893702","Convolutional neural networks;domain adaptation;epithelium-stroma classification;histopathological image analysis;transfer learning","Neural networks;Training;Kernel;Feature extraction;Image analysis;Machine learning;Adaptation models","biomedical optical imaging;cancer;feature extraction;image classification;medical image processing;neural nets;unsupervised learning","epithelium-stroma classification;convolutional neural networks;unsupervised domain adaptation;histopathological image analysis;recognition method;histology data;image acquisition procedure;deep learning method;transfer learning;epithelium-stroma dataset;feature extraction","Algorithms;Breast Neoplasms;Connective Tissue;Epithelium;Female;Histocytochemistry;Humans;Image Processing, Computer-Assisted;Machine Learning;Neural Networks (Computer)","3","28","Traditional","","","","IEEE","IEEE Journals"
"Deformable Patterned Fabric Defect Detection With Fisher Criterion-Based Deep Learning","Y. Li; W. Zhao; J. Pan","School of Electronic and Information Engineering, North China University of Technology, Beijing, China; Shijiazhuang Tiedao University, Institute of Structure Health Monitoring and Control, Shijiazhuang, China; School of Electronic and Information Engineering, North China University of Technology, Beijing, China","IEEE Transactions on Automation Science and Engineering","","2017","14","2","1256","1264","In this paper, we propose a discriminative representation for patterned fabric defect detection when only limited negative samples are available. Fabric patches are efficiently classified into defectless and defective categories by Fisher criterion-based stacked denoising autoencoders (FCSDA). First, fabric images are divided into patches of the same size, and both defective and defectless samples are utilized to train FCSDA. Second, test patches are classified through FCSDA into defective and defectless categories. Finally, the residual between the reconstructed image and defective patch is calculated, and the defect is located by thresholding. Experimental results demonstrate the effectiveness of the proposed scheme in the defect detection for periodic patterned fabric and more complex jacquard warp-knitted fabric.","","","10.1109/TASE.2016.2520955","Beijing Education Committee Science and Technology Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398188","Deep learning;denoising autoencoder (DA);fabric defect detection;Fisher criterion;patterned fabric","Fabrics;Neural networks;Training;Machine learning;Inspection;Transforms;Noise reduction","fabrics;flaw detection;image classification;image reconstruction;image segmentation;object detection;production engineering computing;quality control","deformable patterned fabric defect detection;Fisher criterion-based deep learning;fabric patches classification;Fisher criterion-based stacked denoising autoencoders;FCSDA;image reconstruction;thresholding;jacquard warp-knitted fabric;quality control","","48","28","","","","","IEEE","IEEE Journals"
"Layerwise Class-Aware Convolutional Neural Network","Z. Cui; Z. Niu; L. Liu; S. Yan","Key Laboratory of Child Development and Learning Science, Ministry of Education, Research Center for Learning Science, Southeast University, Nanjing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Circuits and Systems for Video Technology","","2017","27","12","2601","2612","The human vision system usually has a specifically activated area of neurons when recognizing a category of images. Inspired by this visual mechanism, we propose a layerwise class-aware convolutional neural network architecture to explicitly discover category-tailored neurons on intermediate hidden layers to improve the network learning ability. Instead of directly selecting activated neurons for different categories, we inversely suppress those neurons of intermediate layers irrelevant with the given target class to produce a class-specific subnetwork, which implicitly enhances the discriminability of hidden layer features due to the increase of the inter-class discrepancy on them. Together with the classifier of the top layer, we jointly learn this network by formulating the suppressor of hidden layers as a penalty term in the objective function. To address class-specific neuron suppression in each hidden layer, we also introduce a statistic method based on mutual information to dynamically and automatically update the suppressed neurons during the network training. Extensive experiments demonstrate that the proposed model is superior to the state-of-the-art models.","","","10.1109/TCSVT.2016.2587389","National Basic Research Program of China; National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505898","Convolutional neural network (CNN);deep learning;mutual information;object classification","Machine  learning;Biological neural networks;Training;Mutual information;Computer architecture;Computational modeling;Convolutional codes","learning (artificial intelligence);neural nets","layerwise class-aware convolutional neural network architecture;network learning ability;activated neurons;class-specific subnetwork;hidden layer features;inter-class discrepancy;class-specific neuron suppression;suppressed neurons;network training;human vision system;specifically activated area;visual mechanism","","","36","","","","","IEEE","IEEE Journals"
"Person Reidentification Using Deep Convnets With Multitask Learning","N. McLaughlin; J. M. del Rincon; P. C. Miller","Institute of Electronics, Communications and Information Technology, Queen’s University Belfast, Belfast, U.K.; Institute of Electronics, Communications and Information Technology, Queen’s University Belfast, Belfast, U.K.; Institute of Electronics, Communications and Information Technology, Queen’s University Belfast, Belfast, U.K.","IEEE Transactions on Circuits and Systems for Video Technology","","2017","27","3","525","539","Person reidentification involves recognizing a person across nonoverlapping camera views, with different pose, illumination, and camera characteristics. We propose to tackle this problem by training a deep convolutional network to represent a person's appearance as a low-dimensional feature vector that is invariant to common appearance variations encountered in the reidentification problem. Specifically, a Siamese network architecture is used to train a feature extraction network using pairs of similar and dissimilar images. We show that the use of a novel multitask learning objective is crucial for regularizing the network parameters in order to prevent overfitting due to the small size of the training data set. We complement the verification task, which is at the heart of reidentification, by training the network to jointly perform verification and identification and to recognize attributes related to the clothing and pose of the person in each image. In addition, we show that our proposed approach performs well even in the challenging cross-data set scenario, which may better reflect real-world expected performance.","","","10.1109/TCSVT.2016.2619498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7600455","Deep learning;feature embedding;neural networks;person reidentification","Feature extraction;Measurement;Cameras;Training;Image color analysis;Lighting;Neural networks","cameras;feature extraction;image recognition;learning (artificial intelligence);lighting;neural nets;pose estimation","person reidentification;person recognition;nonoverlapping camera views;illumination;camera characteristics;pose characteristics;deep convolutional network;person appearance representation;low-dimensional feature vector;appearance variations;Siamese network architecture;similar images;dissimilar images;feature extraction network training;training data set;clothing;attribute recognition;cross-data set","","15","67","","","","","IEEE","IEEE Journals"
"Deep Recurrent Neural Networks for Hyperspectral Image Classification","L. Mou; P. Ghamisi; X. X. Zhu","German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany; German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany; German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","7","3639","3655","In recent years, vector-based machine learning algorithms, such as random forests, support vector machines, and 1-D convolutional neural networks, have shown promising results in hyperspectral image classification. Such methodologies, nevertheless, can lead to information loss in representing hyperspectral pixels, which intrinsically have a sequence-based data structure. A recurrent neural network (RNN), an important branch of the deep learning family, is mainly designed to handle sequential data. Can sequence-based RNN be an effective method of hyperspectral image classification? In this paper, we propose a novel RNN model that can effectively analyze hyperspectral pixels as sequential data and then determine information categories via network reasoning. As far as we know, this is the first time that an RNN framework has been proposed for hyperspectral image classification. Specifically, our RNN makes use of a newly proposed activation function, parametric rectified tanh (PRetanh), for hyperspectral sequential data analysis instead of the popular tanh or rectified linear unit. The proposed activation function makes it possible to use fairly high learning rates without the risk of divergence during the training procedure. Moreover, a modified gated recurrent unit, which uses PRetanh for hidden representation, is adopted to construct the recurrent layer in our network to efficiently process hyperspectral data and reduce the total number of parameters. Experimental results on three airborne hyperspectral images suggest competitive performance in the proposed mode. In addition, the proposed network architecture opens a new window for future research, showcasing the huge potential of deep recurrent networks for hyperspectral data analysis.","","","10.1109/TGRS.2016.2636241","China Scholarship Council; Alexander von Humboldt Foundation; Helmholtz Association through the framework of the Young Investigators Group SiPEO; European Research Council (ERC) under the European Unions Horizon 2020 Research and Innovation Programme (Acronym: So2Sat); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7914752","Convolutional neural network (CNN);deep learning;gated recurrent unit (GRU);hyperspectral image classification;long short-term memory (LSTM);recurrent neural network (RNN)","Hyperspectral imaging;Recurrent neural networks;Logic gates;Support vector machines;Data models","data analysis;geophysical techniques;hyperspectral imaging;image classification;neural nets","deep recurrent neural networks;hyperspectral image classification method;vector-based machine learning algorithms;random forests;support vector machines;1-D convolutional neural networks;hyperspectral image classification;information loss;hyperspectral pixels;sequence-based data structure;deep learning family;sequential data;sequence-based RNN model;information categories;network reasoning;parametric rectified tanh;activation function;hyperspectral sequential data analysis;rectified linear unit;modified gated recurrent unit;hyperspectral data process;airborne hyperspectral images;network architecture","","156","48","","","","","IEEE","IEEE Journals"
"Surface Water Mapping by Deep Learning","F. Isikdogan; A. C. Bovik; P. Passalacqua","University of Texas at Austin, Austin, TX, USA; University of Texas at Austin, Austin, TX, USA; University of Texas at Austin, Austin, TX, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","11","4909","4918","Mapping of surface water is useful in a variety of remote sensing applications, such as estimating the availability of water, measuring its change in time, and predicting droughts and floods. Using the imagery acquired by currently active Landsat missions, a surface water map can be generated from any selected region as often as every 8 days. Traditional Landsat water indices require carefully selected threshold values that vary depending on the region being imaged and on the atmospheric conditions. They also suffer from many false positives, arising mainly from snow and ice, and from terrain and cloud shadows being mistaken for water. Systems that produce high-quality water maps usually rely on ancillary data and complex rule-based expert systems to overcome these problems. Here, we instead adopt a data-driven, deep-learning-based approach to surface water mapping. We propose a fully convolutional neural network that is trained to segment water on Landsat imagery. Our proposed model, named Deep-WaterMap, learns the characteristics of water bodies from data drawn from across the globe. The trained model separates water from land, snow, ice, clouds, and shadows using only Landsat bands as input. Our code and trained models are publicly available at http://live.ece.utexas.edu/research/deepwatermap/.","","","10.1109/JSTARS.2017.2735443","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8013683","Computer vision;convolutional neural networks;landsat;machine learning;remote sensing","Remote sensing;Earth;Satellites;Image segmentation;Water;Biological neural networks","expert systems;floods;geophysical image processing;hydrological techniques;image classification;image processing;image segmentation;learning (artificial intelligence);neural nets;remote sensing;statistical analysis;water","surface water mapping;currently active Landsat missions;surface water map;Traditional Landsat water indices;high-quality water maps;deep-learning-based approach;segment water;water bodies;trained model separates water;time 8.0 d","","12","40","Traditional","","","","IEEE","IEEE Journals"
"Deep Models for Engagement Assessment With Scarce Label Information","F. Li; G. Zhang; W. Wang; R. Xu; T. Schnell; J. Wen; F. McKenzie; J. Li","Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA; Intelligent Automation, Inc., Rockville, MD, USA; Intelligent Automation, Inc., Rockville, MD, USA; Intelligent Automation, Inc., Rockville, MD, USA; Department of Industrial Engineering, University of Iowa, Iowa City, IA, USA; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA; Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA","IEEE Transactions on Human-Machine Systems","","2017","47","4","598","605","Task engagement is delined as loadings on energetic arousal (affect), task motivation, and concentration (cognition) [1]. It is usually challenging and expensive to label cognitive state data, and traditional computational models trained with limited label information for engagement assessment do not perform well because of overlitting. In this paper, we proposed two deep models (i.e., a deep classilier and a deep autoencoder) for engagement assessment with scarce label information. We recruited 15 pilots to conduct a 4-h flight simulation from Seattle to Chicago and recorded their electroencephalograph (EEG) signals during the simulation. Experts carefully examined the EEG signals and labeled 20 min of the EEG data for each pilot. The EEG signals were preprocessed and power spectral features were extracted. The deep models were pretrained by the unlabeled data and were line-tuned by a different proportion of the labeled data (top 1%, 3%, 5%, 10%, 15%, and 20%) to learn new representations for engagement assessment. The models were then tested on the remaining labeled data. We compared performances of the new data representations with the original EEG features for engagement assessment. Experimental results show that the representations learned by the deep models yielded better accuracies for the six scenarios (77.09%, 80.45%, 83.32%, 85.74%, 85.78%, and 86.52%), based on different proportions of the labeled data for training, as compared with the corresponding accuracies (62.73%, 67.19%, 73.38%, 79.18%, 81.47%, and 84.92%) achieved by the original EEG features. Deep models are effective for engagement assessment especially when less label information was used for training.","","","10.1109/THMS.2016.2608933","NASA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7579229","Deep learning;electroencephalography (EEG);engagement assessment;scarce label information","Electroencephalography;Brain models;Data models;Training;Machine learning;Feature extraction","electroencephalography;feature extraction;medical signal processing","scarce label information;engagement assessment;energetic arousal;task motivation;cognition;cognitive state data labeling;deep classilier model;deep autoencoder model;electroencephalograph signal;EEG signal;power spectral feature extraction;EEG signals;electroencephalography","","6","41","","","","","IEEE","IEEE Journals"
"Automated Melanoma Recognition in Dermoscopy Images via Very Deep Residual Networks","L. Yu; H. Chen; Q. Dou; J. Qin; P. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Medical Imaging","","2017","36","4","994","1004","Automated melanoma recognition in dermoscopy images is a very challenging task due to the low contrast of skin lesions, the huge intraclass variation of melanomas, the high degree of visual similarity between melanoma and non-melanoma lesions, and the existence of many artifacts in the image. In order to meet these challenges, we propose a novel method for melanoma recognition by leveraging very deep convolutional neural networks (CNNs). Compared with existing methods employing either low-level hand-crafted features or CNNs with shallower architectures, our substantially deeper networks (more than 50 layers) can acquire richer and more discriminative features for more accurate recognition. To take full advantage of very deep networks, we propose a set of schemes to ensure effective training and learning under limited training data. First, we apply the residual learning to cope with the degradation and overfitting problems when a network goes deeper. This technique can ensure that our networks benefit from the performance gains achieved by increasing network depth. Then, we construct a fully convolutional residual network (FCRN) for accurate skin lesion segmentation, and further enhance its capability by incorporating a multi-scale contextual information integration scheme. Finally, we seamlessly integrate the proposed FCRN (for segmentation) and other very deep residual networks (for classification) to form a two-stage framework. This framework enables the classification network to extract more representative and specific features based on segmented results instead of the whole dermoscopy images, further alleviating the insufficiency of training data. The proposed framework is extensively evaluated on ISBI 2016 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset. Experimental results demonstrate the significant performance gains of the proposed framework, ranking the first in classification and the second in segmentation among 25 teams and 28 teams, respectively. This study corroborates that very deep CNNs with effective training mechanisms can be employed to solve complicated medical image analysis tasks, even with limited training data.","","","10.1109/TMI.2016.2642839","Research Grants Council of the Hong Kong Special Administrative Region; Guangdong Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792699","Automated melanoma recognition;fully convolutional neural networks;residual learning;skin lesion analysis;very deep convolutional neural networks","Malignant tumors;Lesions;Skin;Image segmentation;Feature extraction;Training data;Biomedical imaging","biomedical optical imaging;cancer;image recognition;image segmentation;learning (artificial intelligence);medical image processing;neural nets;optical microscopy;skin","automated melanoma recognition;dermoscopy images;deep residual networks;deep convolutional neural networks;low-level hand-crafted features;training data;fully convolutional residual network;skin lesion segmentation;classification network;medical image analysis","Artifacts;Dermoscopy;Humans;Melanoma;Neural Networks (Computer)","159","49","","","","","IEEE","IEEE Journals"
"SOLAR: Services-oriented Deep Learning Architectures","C. Wang; L. Gong; A. Wang; X. Li; P. C. K. Hung; Z. Xuehai","Computer Science, Universiity of Science and Technology of China, Suzhou, Jiangsu China 215123 (e-mail: saintwc@mail.ustc.edu.cn); Computer Science, Universiity of Science and Technology of China, Suzhou, Jiangsu China (e-mail: leigong0203@mail.ustc.edu.cn); Software Engineering, University of Science and Technology of China, Suzhou, Jiangsu China (e-mail: wangal@ustc.edu.cn); Computer Science, Universiity of Science and Technology of China, Hefei, Anhui China (e-mail: llxx@ustc.edu.cn); Faculty of Business and Information Technology, University of Ontario Institute of Technology (UOIT), Oshawa, Ontario Canada L1H 7K4 (e-mail: patrick.hung@uoit.ca); Computer Science Department, University of Science and Technology of China, Hefei, Anhui China (e-mail: xhzhou@ustc.edu.cn)","IEEE Transactions on Services Computing","","2017","PP","99","1","1","Deep learning has been an emerging field of machine learning during past decades. However, the diversity and large scale data size have posed significant challenge to construct a flexible and high performance implementations of deep learning neural networks. In order to improve the performance as well to maintain the scalability, in this paper we present SOLAR, a services-oriented deep learning architecture using various ac-celerators like GPU and FPGA. SOLAR provides a uniform programming model to users so that the hardware implemen-tation and the scheduling is invisible to the programmers. At runtime, the services can be executed either on the software processors or the hardware accelerators. To leverage the trade-offs between the metrics among performance, power, energy, and efficiency, we present a multitarget design space explora-tion. Experimental results on the real state-of-the-art FPGA board demonstrate that the SOLAR is able to provide a ubiq-uitous framework for diverse applications without increasing the burden of the programmers. Moreover, the speedup of the GPU and FPGA hardware accelerator in SOLAR can achieve significant speedup comparing to the conventional Intel i5 pro-cessors with great scalability.","","","10.1109/TSC.2017.2777478","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119814","Services-oriented Architecture;Deep Learn-ing;neural network;accelerator","Machine learning;Hardware;Service-oriented architecture;Computer architecture;Field programmable gate arrays;Training","","","","1","","","","","","IEEE","IEEE Early Access Articles"
"Sparseness Analysis in the Pretraining of Deep Neural Networks","J. Li; T. Zhang; W. Luo; J. Yang; X. Yuan; J. Zhang","School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing, China; Department of Statistics, Rutgers University, Piscataway, NJ, USA; College of Mathematics and Informatics, South China Agricultural University, Guangzhou, China; School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing, China; School of Information and Control, Nanjing University of Information Science and Technology, Nanjing, China; Department of Computer Engineering, Huaihai Institute of Technology, Lianyungang, China","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","6","1425","1438","A major progress in deep multilayer neural networks (DNNs) is the invention of various unsupervised pretraining methods to initialize network parameters which lead to good prediction accuracy. This paper presents the sparseness analysis on the hidden unit in the pretraining process. In particular, we use the L1-norm to measure sparseness and provide some sufficient conditions for that pretraining leads to sparseness with respect to the popular pretraining models- such as denoising autoencoders (DAEs) and restricted Boltzmann machines (RBMs). Our experimental results demonstrate that when the sufficient conditions are satisfied, the pretraining models lead to sparseness. Our experiments also reveal that when using the sigmoid activation functions, pretraining plays an important sparseness role in DNNs with sigmoid (Dsigm), and when using the rectifier linear unit (ReLU) activation functions, pretraining becomes less effective for DNNs with ReLU (Drelu). Luckily, Drelu can reach a higher recognition accuracy than DNNs with pretraining (DAEs and RBMs), as it can capture the main benefit (such as sparseness-encouraging) of pretraining in Dsigm. However, ReLU is not adapted to the different firing rates in biological neurons, because the firing rate actually changes along with the varying membrane resistances. To address this problem, we further propose a family of rectifier piecewise linear units (RePLUs) to fit the different firing rates. The experimental results show that the performance of RePLU is better than ReLU, and is comparable with those with some pretraining techniques, such as RBMs and DAEs.","","","10.1109/TNNLS.2016.2541681","National Science Fund for Distinguished Young Scholars; Program for Changjiang Scholars and Innovative Research Team in University; Key Project of Chinese Ministry of Education; 973 Program; Fundamental Research Funds for the Central Universities; Natural Science Foundation of Jiangsu Province of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7445251","Activation function;deep neural networks;infomax principle;sparseness;unsupervised pretraining","Biological neural networks;Neurons;Training;Biomembranes;Learning systems;Signal processing algorithms","Boltzmann machines;learning (artificial intelligence)","sparseness analysis;deep multilayer neural networks;DNN;unsupervised pretraining methods;pretraining models;denoising autoencoders;DAE;restricted Boltzmann machines;RBM;sigmoid activation functions;sparseness role;rectifier linear unit;ReLU activation functions;Dsigm;firing rates;biological neurons;membrane resistances;rectifier piecewise linear units;RePLU","","22","52","","","","","IEEE","IEEE Journals"
"Fisher kernels match deep models","T. Azim","Institute of Management Sciences, Pakistan","Electronics Letters","","2017","53","6","397","399","Deep models have recently shown improved performance on numerous benchmark tasks in computer vision and machine learning. The availability of huge amount of digital data, possibility of massively parallel computations on graphics processing units and the development of advance optimisation techniques have pushed the limits of the deep learning framework by superseding the performance of state-of-the-art research, in specific the kernel methods. This research proposes a novel connection between the two paradigms of research and shows empirical evidence to emphasise that the knowledge learnt from one domain could be supplemented with the significant properties of the other domain to achieve the best of both the worlds. The proposed hybrid methodology illustrates the advantages of deep architectures for kernel methods by showing significant improvement in the classification performance on benchmark tasks with kernel methods. It is shown empirically that the results achieved are either better or competitive to the leading benchmarks from support vector machines and deep models.","","","10.1049/el.2016.3320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7880776","","","computer vision;learning (artificial intelligence);optimisation;support vector machines","support vector machines;advance optimisation techniques;machine learning;computer vision;deep models;Fisher kernel method","","","16","","","","","IET","IET Journals"
"Deep Fully Convolutional Networks for the Detection of Informal Settlements in VHR Images","C. Persello; A. Stein","Faculty of Geo-Information Science and Earth Observation, University of Twente, Enschede, NB, The Netherlands; Faculty of Geo-Information Science and Earth Observation, University of Twente, Enschede, NB, The Netherlands","IEEE Geoscience and Remote Sensing Letters","","2017","14","12","2325","2329","This letter investigates fully convolutional networks (FCNs) for the detection of informal settlements in very high resolution (VHR) satellite images. Informal settlements or slums are proliferating in developing countries and their detection and classification provides vital information for decision making and planning urban upgrading processes. Distinguishing different urban structures in VHR images is challenging because of the abstract semantic definition of the classes as opposed to the separation of standard land-cover classes. This task requires extraction of texture and spatial features. To this aim, we introduce deep FCNs to perform pixel-wise image labeling by automatically learning a higher level representation of the data. Deep FCNs can learn a hierarchy of features associated to increasing levels of abstraction, from raw pixel values to edges and corners up to complex spatial patterns. We present a deep FCN using dilated convolutions of increasing spatial support. It is capable of learning informative features capturing long-range pixel dependencies while keeping a limited number of network parameters. Experiments carried out on a Quickbird image acquired over the city of Dar es Salaam, Tanzania, show that the proposed FCN outperforms state-of-the-art convolutional networks. Moreover, the computational cost of the proposed technique is significantly lower than standard patch-based architectures.","","","10.1109/LGRS.2017.2763738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094993","Convolutional neural networks (CNNs);deep learning;image classification;informal settlements;very high resolution (VHR) satellite imagery","Feature extraction;Kernel;Training;Urban areas;Satellites;Image resolution;Standards","geophysical image processing;image classification;image resolution;image segmentation;learning (artificial intelligence);neural nets;remote sensing;terrain mapping","informal settlements;VHR images;high resolution satellite images;deep FCN;pixel-wise image labeling;dilated convolutions;informative features;network parameters;Quickbird image;deep fully convolutional networks","","12","15","","","","","IEEE","IEEE Journals"
"GPU-Accelerated Parallel Hierarchical Extreme Learning Machine on Flink for Big Data","C. Chen; K. Li; A. Ouyang; Z. Tang; K. Li","College of Information Science and Engineering, Hunan University, Changsha, China; College of Information Science and Engineering, Hunan University, Changsha, China; College of Information Science and Engineering, Hunan University, Changsha, China; College of Information Science and Engineering, Hunan University, Changsha, China; College of Information Science and Engineering, Hunan University, Changsha, China","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2017","47","10","2740","2753","The extreme learning machine (ELM) has become one of the most important and popular algorithms of machine learning, because of its extremely fast training speed, good generalization, and universal approximation/classification capability. The proposal of hierarchical ELM (H-ELM) extends ELM from single hidden layer feedforward networks to multilayer perceptron, greatly strengthening the applicability of ELM. Generally speaking, during training H-ELM, large-scale datasets (DSTs) are needed. Therefore, how to make use of H-ELM framework in processing big data is worth further exploration. This paper proposes a parallel H-ELM algorithm based on Flink, which is one of the in-memory cluster computing platforms, and graphics processing units (GPUs). Several optimizations are adopted to improve the performance, such as cache-based scheme, reasonable partitioning strategy, memory mapping scheme for mapping specific Java virtual machine objects to buffers. Most importantly, our proposed framework for utilizing GPUs to accelerate Flink for big data is general. This framework can be utilized to accelerate many other variants of ELM and other machine learning algorithms. To the best of our knowledge, it is the first kind of library, which combines in-memory cluster computing with GPUs to parallelize H-ELM. The experimental results have demonstrated that our proposed GPU-accelerated parallel H-ELM named as GPH-ELM can efficiently process large-scale DSTs with good performance of speedup and scalability, leveraging the computing power of both CPUs and GPUs in the cluster.","","","10.1109/TSMC.2017.2690673","Key Program of National Natural Science Foundation of China; National Outstanding Youth Science Program of National Natural Science Foundation of China; International (Regional) Cooperation and Exchange Program of National Natural Science Foundation of China; National Natural Science Foundation of China; International Science and Technology Cooperation Program of China; National High-Tech Research and Development Program of China; Key Technology Research and Development Programs of Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7908958","Big data;deep learning (DL);flink;GPGPU;hierarchical extreme learning machine (H-ELM);parallel","Approximation algorithms;Big Data;Acceleration;Training;Libraries;Machine learning algorithms;Clustering algorithms","Big Data;graphics processing units;learning (artificial intelligence);parallel algorithms","GPU-accelerated parallel hierarchical extreme learning machine;Big Data;single hidden layer feedforward networks;multilayer perceptron;parallel H-ELM algorithm;Flink;in-memory cluster computing platforms;graphics processing units;Java virtual machine","","10","37","OAPA","","","","IEEE","IEEE Journals"
"Coupled Deep Autoencoder for Single Image Super-Resolution","K. Zeng; J. Yu; R. Wang; C. Li; D. Tao","Computer Science Department, School of Information Science and Engineering, Xiamen University, Xiamen, China; School of Computer Science and the Key Laboratory of Complex Systems Modeling and Simulation, Ministry of Education, Hangzhou Dianzi University, Hangzhou, China; Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; Computer Science Department, School of Information Science and Engineering, Xiamen University, Xiamen, China; Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Cybernetics","","2017","47","1","27","37","Sparse coding has been widely applied to learning-based single image super-resolution (SR) and has obtained promising performance by jointly learning effective representations for low-resolution (LR) and high-resolution (HR) image patch pairs. However, the resulting HR images often suffer from ringing, jaggy, and blurring artifacts due to the strong yet ad hoc assumptions that the LR image patch representation is equal to, is linear with, lies on a manifold similar to, or has the same support set as the corresponding HR image patch representation. Motivated by the success of deep learning, we develop a data-driven model coupled deep autoencoder (CDA) for single image SR. CDA is based on a new deep architecture and has high representational capability. CDA simultaneously learns the intrinsic representations of LR and HR image patches and a big-data-driven function that precisely maps these LR representations to their corresponding HR representations. Extensive experimentation demonstrates the superior effectiveness and efficiency of CDA for single image SR compared to other state-of-the-art methods on Set5 and Set14 datasets.","","","10.1109/TCYB.2015.2501373","National Natural Science Foundation of China; Zhejiang Provincial Natural Science Foundation of China; Specialized Research Fund for the Doctoral Program of Higher Education of China; National Defense Basic Scientific Research Program of China; National Defense Science and Technology Key Laboratory Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339460","Autoencoder;deep learning;single image super-resolution (SR)","Image reconstruction;Image resolution;Feature extraction;Encoding;Neural networks;Manifolds;Dictionaries","image resolution;learning (artificial intelligence)","learning-based single image super-resolution;high-resolution image patch;low-resolution image patch;data-driven model coupled deep autoencoder;CDA;big-data-driven function;HR image patch;LR image patch;sparse coding","","72","52","","","","","IEEE","IEEE Journals"
"Automatic Skin Lesion Segmentation Using Deep Fully Convolutional Networks With Jaccard Distance","Y. Yuan; M. Chao; Y. Lo","Department of Radiation Oncology, Icahn School of Medicine at Mount Sinai, New York, NY, USA; Department of Radiation Oncology, Icahn School of Medicine at Mount Sinai, New York, NY, USA; Department of Radiation Oncology, Icahn School of Medicine at Mount Sinai, New York, NY, USA","IEEE Transactions on Medical Imaging","","2017","36","9","1876","1886","Automatic skin lesion segmentation in dermoscopic images is a challenging task due to the low contrast between lesion and the surrounding skin, the irregular and fuzzy lesion borders, the existence of various artifacts, and various imaging acquisition conditions. In this paper, we present a fully automatic method for skin lesion segmentation by leveraging 19-layer deep convolutional neural networks that is trained end-to-end and does not rely on prior knowledge of the data. We propose a set of strategies to ensure effective and efficient learning with limited training data. Furthermore, we design a novel loss function based on Jaccard distance to eliminate the need of sample re-weighting, a typical procedure when using cross entropy as the loss function for image segmentation due to the strong imbalance between the number of foreground and background pixels. We evaluated the effectiveness, efficiency, as well as the generalization capability of the proposed framework on two publicly available databases. One is from ISBI 2016 skin lesion analysis towards melanoma detection challenge, and the other is the PH2 database. Experimental results showed that the proposed method outperformed other state-of-the-art algorithms on these two databases. Our method is general enough and only needs minimum pre- and post-processing, which allows its adoption in a variety of medical image segmentation tasks.","","","10.1109/TMI.2017.2695227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7903636","Deep learning;fully convolutional neural networks;image segmentation;jaccard distance;melanoma;dermoscopy","Image segmentation;Skin;Lesions;Malignant tumors;Databases;Biomedical imaging","biomedical optical imaging;cancer;convolution;endoscopes;entropy;image segmentation;learning (artificial intelligence);medical image processing;neural nets;skin;tumours","automatic skin lesion segmentation;deep fully convolutional neural networks;Jaccard distance;dermoscopic images;fuzzy lesion borders;deep learning efficiency;cross entropy;melanoma detection challenge","Algorithms;Artifacts;Dermoscopy;Humans;Melanoma;Neural Networks (Computer);Skin","33","47","","","","","IEEE","IEEE Journals"
"HNIP: Compact Deep Invariant Representations for Video Matching, Localization, and Retrieval","J. Lin; L. Duan; S. Wang; Y. Bai; Y. Lou; V. Chandrasekhar; T. Huang; A. Kot; W. Gao","Institute for Infocomm Research, A*STAR, Singapore; Institute of Digital Media, Peking University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China; Institute of Digital Media, Peking University, Beijing, China; Institute of Digital Media, Peking University, Beijing, China; Nanyang Technological University, Singapore; Institute of Digital Media, Peking University, Beijing, China; Institute for Infocomm Research, A*STAR, Singapore; Institute of Digital Media, Peking University, Beijing, China","IEEE Transactions on Multimedia","","2017","19","9","1968","1983","With emerging demand for large-scale video analysis, MPEG initiated the compact descriptor for video analysis (CDVA) standardization in 2014. Beyond handcrafted descriptors adopted by the current MPEG-CDVA reference model, we study the problem of deep learned global descriptors for video matching, localization, and retrieval. First, inspired by a recent invariance theory, we propose a nested invariance pooling (NIP) method to derive compact deep global descriptors from convolutional neural networks (CNNs), by progressively encoding translation, scale, and rotation invariances into the pooled descriptors. Second, our empirical studies have shown that a sequence of well designed pooling moments (e.g., max or average) may drastically impact video matching performance, which motivates us to design hybrid pooling operations via NIP (HNIP). HNIP has further improved the discriminability of deep global descriptors. Third, the technical merits and performance improvements by combining deep and handcrafted descriptors are provided to better investigate the complementary effects. We evaluate the effectiveness of HNIP within the well-established MPEG-CDVA evaluation framework. The extensive experiments have demonstrated that HNIP outperforms the state-of-the-art deep and canonical handcrafted descriptors with significant mAP gains of 5.5% and 4.7%, respectively. In particular the combination of HNIP incorporated and handcrafted global descriptors has significantly boosted the performance of CDVA core techniques with comparable descriptor size.","","","10.1109/TMM.2017.2713410","National Hightech R&D Program of China; National Natural Science Foundation of China; PKU-NTU Joint Research Institute (JRI); Ng Teng Fong Charitable Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7944594","Convolutional neural networks (CNNs);deep global descriptors;handcrafted descriptors;hybrid nested invariance pooling;MPEG CDVA;MPEG CDVS","Transform coding;Encoding;Feature extraction;Robustness;Standards;Visualization;Image coding","feedforward neural nets;image matching;image representation;learning (artificial intelligence);video retrieval;video signal processing","HNIP;compact deep invariant representations;video matching;localization;retrieval;large-scale video analysis;compact descriptor for video analysis standardization;MPEG-CDVA reference model;deep learned global descriptors;invariance theory;nested invariance pooling method;compact deep global descriptors;convolutional neural networks;translation invariances;scale invariances;rotation invariances;pooled descriptors;pooling moments;hybrid pooling operation design;complementary effects;MPEG-CDVA evaluation framework;canonical handcrafted descriptors;CNN descriptors","","10","66","","","","","IEEE","IEEE Journals"
"Learning and Transferring Convolutional Neural Network Knowledge to Ocean Front Recognition","E. Lima; X. Sun; J. Dong; H. Wang; Y. Yang; L. Liu","Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; College of Physical and Environmental Oceanography, Ocean University of China, Qingdao, China.; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","3","354","358","In this letter, we investigated how to apply a deep learning method, in particular convolutional neural networks (CNNs), to an ocean front recognition task. Exploring deep CNNs knowledge to ocean front recognition is a challenging task, because the training data is very scarce. This letter overcomes this challenge using a sequence of transfer learning steps via fine-tuning. The core idea is to extract deep knowledge of the CNN model from a large data set and then transfer the knowledge to our ocean front recognition task on limited remote sensing (RS) images. We conducted experiments on two different RS image data sets, with different visual properties, i.e., colorful and gray-level data, which were both downloaded from the National Oceanic and Atmospheric Administration (NOAA). The proposed method was compared with the conventional handcraft descriptor with bag-of-visual-words, original CNN model, and last-layer fine-tuned CNN model. Our method showed a significantly higher accuracy than other methods in both datasets.","","","10.1109/LGRS.2016.2643000","National Natural Science Foundation of China; Natural Science Foundation of Shandong Province; NVIDIA Academic Hardware Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829262","Convolutional neural networks (CNNs);fine-tuning;ocean front recognition;transfer learning","Oceans;Feature extraction;Computer architecture;Machine learning;Training data;Neural networks;Data mining","geophysical image processing;image recognition;knowledge acquisition;learning (artificial intelligence);neural nets;oceanographic techniques;remote sensing","knowledge learning;knowledge transferring;convolutional neural network knowledge;ocean front recognition;deep learning method;fine-tuning;limited remote sensing images;colorful data;gray-level data;NOAA;handcraft descriptor;bag-of-visual-words;last-layer fine-tuned CNN model","","16","17","","","","","IEEE","IEEE Journals"
"Multiview Convolutional Neural Networks for Multidocument Extractive Summarization","Y. Zhang; M. J. Er; R. Zhao; M. Pratama","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Computer Science and IT, La Trobe university, Melbourne, VIC, Australia","IEEE Transactions on Cybernetics","","2017","47","10","3230","3242","Multidocument summarization has gained popularity in many real world applications because vital information can be extracted within a short time. Extractive summarization aims to generate a summary of a document or a set of documents by ranking sentences and the ranking results rely heavily on the quality of sentence features. However, almost all previous algorithms require hand-crafted features for sentence representation. In this paper, we leverage on word embedding to represent sentences so as to avoid the intensive labor in feature engineering. An enhanced convolutional neural networks (CNNs) termed multiview CNNs is successfully developed to obtain the features of sentences and rank sentences jointly. Multiview learning is incorporated into the model to greatly enhance the learning capability of original CNN. We evaluate the generic summarization performance of our proposed method on five Document Understanding Conference datasets. The proposed system outperforms the state-of-the-art approaches and the improvement is statistically significant shown by paired t-test.","","","10.1109/TCYB.2016.2628402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7756666","Convolutional neural networks (CNNs);deep learning;multidocument summarization (MDS);multiview learning;word embedding","Feature extraction;Neural networks;Machine learning;Semantics;Data mining;Computer vision;Computational modeling","document handling;feedforward neural nets;learning (artificial intelligence);statistical testing","paired t-test;Document Understanding Conference datasets;learning capability;multiview CNN;feature engineering;sentence representation;hand-crafted features;sentence feature quality;multidocument extractive summarization;multiview convolutional neural networks","Algorithms;Data Mining;Machine Learning;Natural Language Processing;Neural Networks (Computer)","6","62","","","","","IEEE","IEEE Journals"
"Embedded Streaming Deep Neural Networks Accelerator With Applications","A. Dundar; J. Jin; B. Martini; E. Culurciello","Weldon School of Biomedical Engineering, Purdue University, West Lafayette, IN, USA; Department of Electrical Engineering, Purdue University, West Lafayette, IN, USA; Weldon School of Biomedical Engineering, Purdue University, West Lafayette, IN, USA; Weldon School of Biomedical Engineering, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","7","1572","1583","Deep convolutional neural networks (DCNNs) have become a very powerful tool in visual perception. DCNNs have applications in autonomous robots, security systems, mobile phones, and automobiles, where high throughput of the feedforward evaluation phase and power efficiency are important. Because of this increased usage, many field-programmable gate array (FPGA)-based accelerators have been proposed. In this paper, we present an optimized streaming method for DCNNs' hardware accelerator on an embedded platform. The streaming method acts as a compiler, transforming a high-level representation of DCNNs into operation codes to execute applications in a hardware accelerator. The proposed method utilizes maximum computational resources available based on a novel-scheduled routing topology that combines data reuse and data concatenation. It is tested with a hardware accelerator implemented on the Xilinx Kintex-7 XC7K325T FPGA. The system fully explores weight-level and node-level parallelizations of DCNNs and achieves a peak performance of 247 G-ops while consuming less than 4 W of power. We test our system with applications on object classification and object detection in real-world scenarios. Our results indicate high-performance efficiency, outperforming all other presented platforms while running these applications.","","","10.1109/TNNLS.2016.2545298","Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7450197","Applications of deep neural networks;deep neural networks;embedded systems","Hardware;Feature extraction;Parallel processing;Neural networks;Convolutional codes;Computer architecture;Training","computer vision;convolution;field programmable gate arrays;image classification;neural nets;object detection;topology","embedded streaming deep neural networks accelerator;deep convolutional neural networks;DCNNs;visual perception;field-programmable gate array;FPGA-based accelerators;optimized streaming method;DCNN hardware accelerator;compiler;high-level DCNN representation;scheduled routing topology;data reuse;data concatenation;Xilinx Kintex-7 XC7K325T FPGA;DCNN weight-level parallelizations;DCNN node-level parallelizations;object classification;object detection","","26","49","","","","","IEEE","IEEE Journals"
"Hyperspectral Image Classification Using Deep Pixel-Pair Features","W. Li; G. Wu; F. Zhang; Q. Du","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; Department of Electrical and Computer Engineering, Mississippi State University, Mississippi State, MS, USA","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","2","844","853","The deep convolutional neural network (CNN) is of great interest recently. It can provide excellent performance in hyperspectral image classification when the number of training samples is sufficiently large. In this paper, a novel pixel-pair method is proposed to significantly increase such a number, ensuring that the advantage of CNN can be actually offered. For a testing pixel, pixel-pairs, constructed by combining the center pixel and each of the surrounding pixels, are classified by the trained CNN, and the final label is then determined by a voting strategy. The proposed method utilizing deep CNN to learn pixel-pair features is expected to have more discriminative power. Experimental results based on several hyperspectral image data sets demonstrate that the proposed method can achieve better classification performance than the conventional deep learning-based method.","","","10.1109/TGRS.2016.2616355","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Higher Education and High-Quality and World-Class Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7736139","Convolutional neural network (CNN);deep learning;feature extraction;hyperspectral imagery;pattern classification","Training;Hyperspectral imaging;Computer architecture;Feature extraction;Tensile stress;Kernel","hyperspectral imaging;neural nets;remote sensing by radar","hyperspectral image data;discriminative power;deep learning-based method;deep convolutional neural network;deep pixel-pair features;hyperspectral image classification","","186","30","","","","","IEEE","IEEE Journals"
"Learning Sensor-Specific Spatial-Spectral Features of Hyperspectral Images via Convolutional Neural Networks","S. Mei; J. Ji; J. Hou; X. Li; Q. Du","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; Department of Computer Science, City University of Hong Kong, Hong Kong; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","8","4520","4533","Convolutional neural network (CNN) is well known for its capability of feature learning and has made revolutionary achievements in many applications, such as scene recognition and target detection. In this paper, its capability of feature learning in hyperspectral images is explored by constructing a five-layer CNN for classification (C-CNN). The proposed C-CNN is constructed by including recent advances in deep learning area, such as batch normalization, dropout, and parametric rectified linear unit (PReLU) activation function. In addition, both spatial context and spectral information are elegantly integrated into the C-CNN such that spatial-spectral features are learned for hyperspectral images. A companion feature-learning CNN (FL-CNN) is constructed by extracting fully connected feature layers in this C-CNN. Both supervised and unsupervised modes are designed for the proposed FL-CNN to learn sensor-specific spatial-spectral features. Extensive experimental results on four benchmark data sets from two well-known hyperspectral sensors, namely airborne visible/infrared imaging spectrometer (AVIRIS) and reflective optics system imaging spectrometer (ROSIS) sensors, demonstrate that our proposed C-CNN outperforms the state-of-the-art CNN-based classification methods, and its corresponding FL-CNN is very effective to extract sensor-specific spatial-spectral features for hyperspectral applications under both supervised and unsupervised modes.","","","10.1109/TGRS.2017.2693346","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7919223","Classification;convolutional neural network (CNN);feature learning;hyperspectral;spatial-spectral","Feature extraction;Hyperspectral imaging;Sensors;Principal component analysis;Image sensors;Machine learning","feature extraction;hyperspectral imaging;image classification;learning (artificial intelligence);neural nets;remote sensing","hyperspectral image;convolutional neural networks;scene recognition;target detection;image classification;C-CNN method;deep learning area;feature-learning CNN;FL-CNN;sensor-specific spatial-spectral feature;hyperspectral sensor;airborne visible-infrared imaging spectrometer;reflective optics system imaging spectrometer;AVIRIS sensor;ROSIS sensor","","37","43","","","","","IEEE","IEEE Journals"
"Mapping, Learning, Visualization, Classification, and Understanding of fMRI Data in the NeuCube Evolving Spatiotemporal Data Machine of Spiking Neural Networks","N. K. Kasabov; M. G. Doborjeh; Z. G. Doborjeh","Auckland University of Technology, Auckland, New Zealand; Auckland University of Technology, Auckland, New Zealand; Auckland University of Technology, Auckland, New Zealand","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","4","887","899","This paper introduces a new methodology for dynamic learning, visualization, and classification of functional magnetic resonance imaging (fMRI) as spatiotemporal brain data. The method is based on an evolving spatiotemporal data machine of evolving spiking neural networks (SNNs) exemplified by the NeuCube architecture [1]. The method consists of several steps: mapping spatial coordinates of fMRI data into a 3-D SNN cube (SNNc) that represents a brain template; input data transformation into trains of spikes; deep, unsupervised learning in the 3-D SNNc of spatiotemporal patterns from data; supervised learning in an evolving SNN classifier; parameter optimization; and 3-D visualization and model interpretation. Two benchmark case study problems and data are used to illustrate the proposed methodology - fMRI data collected from subjects when reading affirmative or negative sentences and another one - on reading a sentence or seeing a picture. The learned connections in the SNNc represent dynamic spatiotemporal relationships derived from the fMRI data. They can reveal new information about the brain functions under different conditions. The proposed methodology allows for the first time to analyze dynamic functional and structural connectivity of a learned SNN model from fMRI data. This can be used for a better understanding of brain activities and also for online generation of appropriate neurofeedback to subjects for improved brain functions. For example, in this paper, tracing the 3-D SNN model connectivity enabled us for the first time to capture prominent brain functional pathways evoked in language comprehension. We found stronger spatiotemporal interaction between left dorsolateral prefrontal cortex and left temporal while reading a negated sentence. This observation is obviously distinguishable from the patterns generated by either reading affirmative sentences or seeing pictures. The proposed NeuCube-based methodology offers also a superior classification accuracy when compared with traditional AI and statistical methods. The created NeuCube-based models of fMRI data are directly and efficiently implementable on high performance and low energy consumption neuromorphic platforms for real-time applications.","","","10.1109/TNNLS.2016.2612890","Knowledge Engineering and Discovery Research Institute of the Auckland University of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585110","Brain data classification;brain data modeling;brain data visualization;evolving spatiotemporal data machines (eSTDMs);functional magnetic resonance imaging (fMRI);NeuCube;neuro information processing;spatiotemporal brain data (STBD);spiking neural networks (SNNs)","Data models;Brain modeling;Data visualization;Spatiotemporal phenomena;Solid modeling;Unsupervised learning","biomedical MRI;data visualisation;image classification;medical image processing;neural nets;unsupervised learning","fMRI data;NeuCube evolving spatiotemporal data machine;spiking neural networks;dynamic learning;functional magnetic resonance imaging;spatiotemporal brain data;evolving spiking neural networks;SNN;brain template;input data transformation;unsupervised learning;3-D SNNc;3-D visualization;affirmative sentences;negative sentences;dynamic spatiotemporal relationships;brain functions;neurofeedback;brain functional pathways;language comprehension;spatiotemporal interaction;left dorsolateral prefrontal cortex;NeuCube-based methodology;classification accuracy;low energy consumption neuromorphic platforms","","26","44","","","","","IEEE","IEEE Journals"
"Multitask Learning of Context-Dependent Targets in Deep Neural Network Acoustic Models","P. Bell; P. Swietojanski; S. Renals","Centre for Speech Technology Research, University of Edinburgh, Edinburgh, U.K.; Centre for Speech Technology Research, University of Edinburgh, Edinburgh, U.K.; Centre for Speech Technology Research, University of Edinburgh, Edinburgh, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","2","238","247","This paper investigates the use of multitask learning to improve context-dependent deep neural network (DNN) acoustic models. The use of hybrid DNN systems with clustered triphone targets is now standard in automatic speech recognition. However, we suggest that using a single set of DNN targets in this manner may not be the most effective choice, since the targets are the result of a somewhat arbitrary clustering process that may not be optimal for discrimination. We propose to remedy this problem through the addition of secondary tasks predicting alternative content-dependent or context-independent targets. We present a comprehensive set of experiments on a lecture recognition task showing that DNNs trained through multitask learning in this manner give consistently improved performance compared to standard hybrid DNNs. The technique is evaluated across a range of data and output sizes. Improvements are seen when training uses the cross entropy criterion and also when sequence training is applied.","","","10.1109/TASLP.2016.2630305","EPSRC; Natural Speech Technology; European Union under H2020; SUMMA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7747518","Automatic speech recognition;multitask learning;context modelling","Hidden Markov models;Acoustics;Context;Context modeling;Neural networks;Speech;Standards","learning (artificial intelligence);neural nets;pattern clustering;speech recognition;voice activity detection","multitask learning;context-dependent targets;deep neural network acoustic models;triphone target clustering;automatic speech recognition;arbitrary clustering process","","6","44","","","","","IEEE","IEEE Journals"
"Hyperspectral Imagery Denoising by Deep Learning With Trainable Nonlinearity Function","W. Xie; Y. Li","State Key Laboratory of Integrated Service Network, Xidian University, Xi&#x2019;an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi&#x2019;an, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","11","1963","1967","Hyperspectral images (HSIs) can describe subtle differences in the spectral signatures of objects, and thus they are effective in a wide array of applications. However, an HSI is inevitably contaminated with some unwanted components like noise resulting in spectral distortion, which significantly decreases the performance of postprocessing. In this letter, a deep stage convolutional neural network (CNN) with trainable nonlinearity functions is applied for the first time to remove noise in HSIs. Besides the fact that the weight and bias matrices are learned from cubic training clean-noisy HSI patches, the nonlinearity functions in each stage are also trainable, which differ from the conventional CNN with a fixed nonlinearity function. Compared with the state-of-the-art HSI denoising methods, the experimental results on both synthetic and real HSIs confirm that the proposed method can obtain a more effective and efficient performance.","","","10.1109/LGRS.2017.2743738","National Natural Science Foundation of China; 111 project; Shaanxi Province Science and Technology Innovation Team Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030333","Cubic neighbor patches;deep convolutional neural network (CNN);hyperspectral image (HSI) denoising;trainable nonlinearity function","Training;Noise reduction;Convolution;Noise measurement;Hyperspectral imaging;Training data","convolution;geophysical image processing;hyperspectral imaging;image denoising;learning (artificial intelligence);neural nets;nonlinear functions","deep learning;trainable nonlinearity function;spectral distortion;nonlinearity functions;convolutional neural network;hyperspectral imagery denoising;HSI denoising;CNN","","15","13","Traditional","","","","IEEE","IEEE Journals"
"Fully Deep Blind Image Quality Predictor","J. Kim; S. Lee","Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea","IEEE Journal of Selected Topics in Signal Processing","","2017","11","1","206","220","In general, owing to the benefits obtained from original information, full-reference image quality assessment (FR-IQA) achieves relatively higher prediction accuracy than no-reference image quality assessment (NR-IQA). By fully utilizing reference images, conventional FR-IQA methods have been investigated to produce objective scores that are close to subjective scores. In contrast, NR-IQA does not consider reference images; thus, its performance is inferior to that of FR-IQA. To alleviate this accuracy discrepancy between FR-IQA and NR-IQA methods, we propose a blind image evaluator based on a convolutional neural network (BIECON). To imitate FR-IQA behavior, we adopt the strong representation power of a deep convolutional neural network to generate a local quality map, similar to FR-IQA. To obtain the best results from the deep neural network, replacing hand-crafted features with automatically learned features is necessary. To apply the deep model to the NR-IQA framework, three critical problems must be resolved: 1) lack of training data; 2) absence of local ground truth targets; and 3) different purposes of feature learning. BIECON follows the FR-IQA behavior using the local quality maps as intermediate targets for conventional neural networks, which leads to NR-IQA prediction accuracy that is comparable with that of state-of-the-art FR-IQA methods.","","","10.1109/JSTSP.2016.2639328","National Research Foundation of Korea; Korea government (MSIP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782419","Convolutional neural network;deep learning;image quality assessment;no-reference image quality assessment","Feature extraction;Neural networks;Training;Image quality;Measurement;Distortion;Machine learning","image enhancement;neural nets","fully deep blind image quality predictor;full-reference image quality assessment;no-reference image quality assessment;FR-IQA method;NR-IQA method;blind image evaluator;BIECON;deep convolutional neural network","","86","59","","","","","IEEE","IEEE Journals"
"Efficient Compressed Sensing for Wireless Neural Recording: A Deep Learning Approach","B. Sun; H. Feng","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China","IEEE Signal Processing Letters","","2017","24","6","863","867","Data compression is crucial for resource-constrained wireless neural recording applications with limited data bandwidth, and compressed sensing (CS) theory has successfully demonstrated its potential in neural recording applications. Based on deep learning theory, this paper presents a binarized autoencoder scheme for CS, in which a binary sensing matrix and a noniterative recovery solver are jointly optimized. Experimental results on synthetic dataset reveal that the proposed approach outperforms the state-of-the-art CS-based methods both in terms of recovery quality and computation time.","","","10.1109/LSP.2017.2697970","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7911224","Compressed sensing (CS);deep neural network;wireless neural recording","Sensors;Training;Wireless sensor networks;Wireless communication;Dictionaries;Compressed sensing;Cost function","binary codes;brain;compressed sensing;data compression;iterative methods;learning (artificial intelligence);matrix algebra;medical signal processing","compressed sensing;deep learning approach;data compression;resource-constrained wireless neural recording application;limited data bandwidth;binarized autoencoder scheme;binary sensing matrix;noniterative recovery solver;CS-based method","","5","27","","","","","IEEE","IEEE Journals"
"Transferred Deep Learning for Anomaly Detection in Hyperspectral Imagery","W. Li; G. Wu; Q. Du","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Geoscience and Remote Sensing Letters","","2017","14","5","597","601","In this letter, a novel anomaly detection framework with transferred deep convolutional neural network (CNN) is proposed. The framework is designed by considering the following facts: 1) a reference data with labeled samples are utilized, because no prior information is available about the image scene for anomaly detection and 2) pixel pairs are generated to enlarge the sample size, since the advantage of CNN can be realized only if the number of training samples is sufficient. A multilayer CNN is trained by using difference between pixel pairs generated from the reference image scene. Then, for each pixel in the image for anomaly detection, difference between pixel pairs, constructed by combining the center pixel and its surrounding pixels, is classified by the trained CNN with the result of similarity measurement. The detection output is simply generated by averaging these similarity scores. Experimental performance demonstrates that the proposed algorithm outperforms the classic Reed-Xiaoli and the state-of-the-art representation-based detectors, such as sparse representation-based detector (SRD) and collaborative representation-based detector.","","","10.1109/LGRS.2017.2657818","National Natural Science Foundation of China; Higher Education and High-Quality and World-Class Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875485","Anomaly detection;convolutional neural network (CNN);deep learning;hyperspectral imagery","Detectors;Hyperspectral imaging;Testing;Feature extraction;Machine learning;Training","geophysical image processing;geophysical techniques;hyperspectral imaging;image representation;neural nets;sampling methods","transferred deep convolutional neural network;hyperspectral imagery;reference data;anomaly detection;image scene;multilayer CNN;reference image scene;state-of-the-art representation-based detector;sparse representation-based detector;collaborative representation-based detector","","61","25","","","","","IEEE","IEEE Journals"
"Nuclear Power Plant Thermocouple Sensor-Fault Detection and Classification Using Deep Learning and Generalized Likelihood Ratio Test","S. Mandal; B. Santhi; S. Sridhar; K. Vinolia; P. Swaminathan","School of Computing, SASTRA University, Thanjavur, India; School of Computing, SASTRA University, Thanjavur, India; Reactor Maintenance Division, Indira Gandhi centre for Atomic Research, Kalpakkam, India; Central Data Processing Section, Indira Gandhi centre for Atomic Research, Kalpakkam, India; School of Computing Science and Engineering, Vels University, Chennai, India","IEEE Transactions on Nuclear Science","","2017","64","6","1526","1534","In this paper, an online fault detection and classification method is proposed for thermocouples used in nuclear power plants. In the proposed method, the fault data are detected by the classification method, which classifies the fault data from the normal data. Deep belief network (DBN), a technique for deep learning, is applied to classify the fault data. The DBN has a multilayer feature extraction scheme, which is highly sensitive to a small variation of data. Since the classification method is unable to detect the faulty sensor; therefore, a technique is proposed to identify the faulty sensor from the fault data. Finally, the composite statistical hypothesis test, namely generalized likelihood ratio test, is applied to compute the fault pattern of the faulty sensor signal based on the magnitude of the fault. The performance of the proposed method is validated by field data obtained from thermocouple sensors of the fast breeder test reactor.","","","10.1109/TNS.2017.2697919","Board of Research in Nuclear Science, India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7911304","Deep belief network (DBN);fast breeder test reactor (FBTR);generalized likelihood ratio test (GLRT);thermocouple sensor fault","Inductors;Sodium;Fault detection;Circuit faults;Fault diagnosis;Mathematical model;Temperature measurement","belief networks;fault diagnosis;feature extraction;nuclear power;nuclear power stations","nuclear power plant thermocouple sensor-fault detection;deep learning;fast breeder test reactor;thermocouple sensors;faulty sensor signal;composite statistical hypothesis test;multilayer feature extraction;DBN;deep belief network;fault data;nuclear power plants;thermocouples;classification method;online fault detection;generalized likelihood ratio test","","5","26","","","","","IEEE","IEEE Journals"
"Traffic light control using deep policy-gradient and value-function-based reinforcement learning","S. S. Mousavi; M. Schukat; E. Howley","National University of Ireland, Ireland; National University of Ireland, Ireland; National University of Ireland, Ireland","IET Intelligent Transport Systems","","2017","11","7","417","423","Recent advances in combining deep neural network architectures with reinforcement learning (RL) techniques have shown promising potential results in solving complex control problems with high-dimensional state and action spaces. Inspired by these successes, in this study, the authors built two kinds of RL algorithms: deep policy-gradient (PG) and value-function-based agents which can predict the best possible traffic signal for a traffic intersection. At each time step, these adaptive traffic light control agents receive a snapshot of the current state of a graphical traffic simulator and produce control signals. The PG-based agent maps its observation directly to the control signal; however, the value-function-based agent first estimates values for all legal control signals. The agent then selects the optimal control action with the highest value. Their methods show promising results in a traffic network simulated in the simulation of urban mobility traffic simulator, without suffering from instability issues during the training process.","","","10.1049/iet-its.2017.0153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8061182","","","adaptive control;control engineering computing;digital simulation;gradient methods;learning (artificial intelligence);road traffic control;traffic engineering computing","traffic light control;value-function-based reinforcement learning;deep neural network architectures;complex control problems;high-dimensional state space;action spaces;deep policy-gradient RL algorithm;value-function-based agent RL algorithms;traffic signal;traffic intersection;adaptive traffic light control agents;graphical traffic simulator;control signals;PG-based agent maps;optimal control;urban mobility traffic simulator;training process","","13","47","","","","","IET","IET Journals"
"Learning to Diversify Deep Belief Networks for Hyperspectral Image Classification","P. Zhong; Z. Gong; S. Li; C. Schönlieb","College of Electrical Science and Engineering, National University of Defense Technology, Changsha, China; College of Electrical Science and Engineering, National University of Defense Technology, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Faculty of Department of Applied Mathematics and Theoretical Physics, University of Cambridge, Cambridge, U.K.","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","6","3516","3530","In the literature of remote sensing, deep models with multiple layers have demonstrated their potentials in learning the abstract and invariant features for better representation and classification of hyperspectral images. The usual supervised deep models, such as convolutional neural networks, need a large number of labeled training samples to learn their model parameters. However, the real-world hyperspectral image classification task provides only a limited number of training samples. This paper adopts another popular deep model, i.e., deep belief networks (DBNs), to deal with this problem. The DBNs allow unsupervised pretraining over unlabeled samples at first and then a supervised fine-tuning over labeled samples. But the usual pretraining and fine-tuning method would make many hidden units in the learned DBNs tend to behave very similarly or perform as “dead” (never responding) or “potential over-tolerant” (always responding) latent factors. These results could negatively affect description ability and thus classification performance of DBNs. To further improve DBN's performance, this paper develops a new diversified DBN through regularizing pretraining and fine-tuning procedures by a diversity promoting prior over latent factors. Moreover, the regularized pretraining and fine-tuning can be efficiently implemented through usual recursive greedy and back-propagation learning framework. The experiments over real-world hyperspectral images demonstrated that the diversity promoting prior in both pretraining and fine-tuning procedure lead to the learned DBNs with more diverse latent factors, which directly make the diversified DBNs obtain much better results than original DBNs and comparable or even better performances compared with other recent hyperspectral image classification methods.","","","10.1109/TGRS.2017.2675902","Natural Science Foundation of China; Program for New Century Excellent Talents in University; Foundation for the Author of National Excellent Doctoral Dissertation of China; Chinese Scholarship Council; Leverhulme Trust Project on Breaking the Nonconvexity Barrier; EPSRC; EPSRC Center; Cantab Capital Institute for the Mathematics of Information; CHiPS (Horizon 2020 RISE Project Grant); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885590","Deep belief network (DBN);diversity;hyperspectral image;image classification","Hyperspectral imaging;Training;Feature extraction;Hidden Markov models;Neurons","geophysical image processing;hyperspectral imaging;image classification","deep belief networks;hyperspectral image classification;convolutional neural networks;unlabeled samples;supervised fine-tuning;fine-tuning procedures;latent factors;back-propagation learning framework;real-world hyperspectral images","","78","48","","","","","IEEE","IEEE Journals"
"Stacked Convolutional Denoising Auto-Encoders for Feature Representation","B. Du; W. Xiong; J. Wu; L. Zhang; L. Zhang; D. Tao","State Key Laboratory of Software Engineering, Key Laboratory of Aerospace Information Security and Trusted Computing Ministry of Education, School of Computer, Collaborative Innovation Center of Geospatial Information Technology, Wuhan University, Wuhan, China; State Key Laboratory of Software Engineering, Key Laboratory of Aerospace Information Security and Trusted Computing Ministry of Education, School of Computer, Collaborative Innovation Center of Geospatial Information Technology, Wuhan University, Wuhan, China; Centre for Quantum Computation and Intelligent Systems and the Department of Engineering and Information Technology, University of Technology Sydney, Sydney, Australia; State Key Laboratory of Software Engineering, Key Laboratory of Aerospace Information Security and Trusted Computing Ministry of Education, School of Computer, Collaborative Innovation Center of Geospatial Information Technology, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Centre for Quantum Computation and Intelligent Systems and the Department of Engineering and Information Technology, University of Technology Sydney, Sydney, Australia","IEEE Transactions on Cybernetics","","2017","47","4","1017","1027","Deep networks have achieved excellent performance in learning representation from visual data. However, the supervised deep models like convolutional neural network require large quantities of labeled data, which are very expensive to obtain. To solve this problem, this paper proposes an unsupervised deep network, called the stacked convolutional denoising auto-encoders, which can map images to hierarchical representations without any label information. The network, optimized by layer-wise training, is constructed by stacking layers of denoising auto-encoders in a convolutional way. In each layer, high dimensional feature maps are generated by convolving features of the lower layer with kernels learned by a denoising auto-encoder. The auto-encoder is trained on patches extracted from feature maps in the lower layer to learn robust feature detectors. To better train the large network, a layer-wise whitening technique is introduced into the model. Before each convolutional layer, a whitening layer is embedded to sphere the input data. By layers of mapping, raw images are transformed into high-level feature representations which would boost the performance of the subsequent support vector machine classifier. The proposed algorithm is evaluated by extensive experimentations and demonstrates superior classification performance to state-of-the-art unsupervised networks.","","","10.1109/TCYB.2016.2536638","National Basic Research Program of China (973 Program); National Natural Science Foundation of China; Natural Science Foundation of Hubei Province; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7434593","Convolution;deep learning;denoising auto-encoders;unsupervised learning","Training;Convolutional codes;Convolution;Noise reduction;Robustness;Visualization;Data models","data visualisation;image classification;learning (artificial intelligence);self-organising feature maps","stacked convolutional denoising auto-encoders;learning representation;visual data;supervised deep models;convolutional neural network;unsupervised deep network;hierarchical representations;label information;layer-wise training;high dimensional feature maps;robust feature detectors;layer-wise whitening technique;convolutional layer;whitening layer;high-level feature representations;support vector machine classifier;classification performance;unsupervised networks","","129","52","","","","","IEEE","IEEE Journals"
"Identity-aware convolutional neural networks for facial expression recognition","C. Zhang; P. Wang; K. Chen; J. Kämäräinen","The Big Data Research Center, Henan University, Kaifeng 475001, China; The Big Data Research Center, Henan University, Kaifeng 475001, China; Department of Signal Processing, Tampere University of Technology, Tampere 33720, Finland; Department of Signal Processing, Tampere University of Technology, Tampere 33720, Finland","Journal of Systems Engineering and Electronics","","2017","28","4","784","792","Facial expression recognition is a hot topic in computer vision, but it remains challenging due to the feature inconsistency caused by person-specific characteristics of facial expressions. To address such a challenge, and inspired by the recent success of deep identity network (DeepID-Net) for face identification, this paper proposes a novel deep learning based framework for recognising human expressions with facial images. Compared to the existing deep learning methods, our proposed framework, which is based on multi-scale global images and local facial patches, can significantly achieve a better performance on facial expression recognition. Finally, we verify the effectiveness of our proposed framework through experiments on the public benchmarking datasets JAFFE and extended Cohn-Kanade (CK+).","","","10.21629/JSEE.2017.04.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8038215","facial expression recognition;deep learning;classification;identity-aware","Face recognition;Face;Feature extraction;Image recognition;Training","computer vision;face recognition;learning (artificial intelligence);neural nets","multiscale global images;human expression recognition;face identification;DeepID-Net;person-specific characteristics;computer vision;deep learning methods;identity-aware convolutional neural networks;local facial patches;facial images;novel deep learning based framework;deep identity network;facial expression recognition","","3","","","","","","BIAI","BIAI Journals"
"Counting Apples and Oranges With Deep Learning: A Data-Driven Approach","S. W. Chen; S. S. Shivakumar; S. Dcunha; J. Das; E. Okon; C. Qu; C. J. Taylor; V. Kumar","GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; University of Massachusetts Amherst, Amherst, MA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA; GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA","IEEE Robotics and Automation Letters","","2017","2","2","781","788","This paper describes a fruit counting pipeline based on deep learning that accurately counts fruit in unstructured environments. Obtaining reliable fruit counts is challenging because of variations in appearance due to illumination changes and occlusions from foliage and neighboring fruits. We propose a novel approach that uses deep learning to map from input images to total fruit counts. The pipeline utilizes a custom crowdsourcing platform to quickly label large data sets. A blob detector based on a fully convolutional network extracts candidate regions in the images. A counting algorithm based on a second convolutional network then estimates the number of fruits in each region. Finally, a linear regression model maps that fruit count estimate to a final fruit count. We analyze the performance of the pipeline on two distinct data sets of oranges in daylight, and green apples at night, utilizing human generated labels as ground truth. We also show that the pipeline has a short training time and performs well with a limited data set size. Our method generalizes across both data sets and is able to perform well even on highly occluded fruits that are challenging for human labelers to annotate.","","","10.1109/LRA.2017.2651944","U.S. Department of Agriculture; National Robotics Initiative; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814145","Agricultural automation;categorization;object detection;segmentation;visual learning","Pipelines;Neural networks;Machine learning;Training;Image segmentation;Lighting;Labeling","agricultural products;crowdsourcing;feature extraction;feedforward neural nets;learning (artificial intelligence);object detection;performance evaluation;regression analysis","data-driven approach;apple counting;orange counting;fruit counting pipeline;illumination changes;occlusions;crowdsourcing platform;large data sets;blob detector;fully convolutional network;candidate region extraction;counting algorithm;linear regression model;performance analysis;green apples;human generated label utilization;agricultural automation;object detection","","53","25","","","","","IEEE","IEEE Journals"
"Survey on deep learning methods in human action recognition","M. Koohzadi; N. M. Charkari","Tarbiat Modares University, Iran; Tarbiat Modares University, Iran","IET Computer Vision","","2017","11","8","623","632","A study on one of the most important issues in a human action recognition task, i.e. how to create proper data representations with a high-level abstraction from large dimensional noisy video data, is carried out. Most of the recent successful studies in this area are mainly focused on deep learning. Deep learning methods have gained superiority to other approaches in the field of image recognition. In this survey, the authors first investigate the role of deep learning in both image and video processing and recognition. Owing to the variety and plenty of deep learning methods, the authors discuss them in a comparative form. For this purpose, the authors present an analytical framework to classify and to evaluate these methods based on some important functional measures. Furthermore, a categorisation of the state-of-the-art approaches in deep learning for human action recognition is presented. The authors summarise the significantly related works in each approach and discuss their performance.","","","10.1049/iet-cvi.2016.0355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8120122","","","data structures;image motion analysis;image recognition;learning (artificial intelligence);video signal processing","deep learning methods;human action recognition;data representations;noisy video data;image recognition;video processing;video recognition","","4","114","","","","","IET","IET Journals"
"Multi-Aspect-Aware Bidirectional LSTM Networks for Synthetic Aperture Radar Target Recognition","F. Zhang; C. Hu; Q. Yin; W. Li; H. Li; W. Hong","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; National Research Center for High-Performance Computing Engineering Technology, Sugon Information Industry Co., Ltd, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Access","","2017","5","","26880","26891","The outstanding pattern recognition performance of deep learning brings new vitality to the synthetic aperture radar (SAR) automatic target recognition (ATR). However, there is a limitation in current deep learning based ATR solution that each learning process only handles one SAR image, namely learning the static scattering information, while missing the space-varying information. It is obvious that space-varying scattering information introduced in the multi-aspect joint recognition should improve the classification accuracy and robustness. In this paper, a novel multi-aspect-aware method is proposed to achieve this idea through the bidirectional long short-term memory (LSTM) recurrent neural networksbased space-varying scattering information learning. Specifically, we first select different aspect images to generate the multi-aspect space-varying image sequences. Then, the Gabor filter and three-patch local binary pattern are progressively implemented to extract comprehensive spatial features, followed by dimensionality reduction with the multi-layer perceptron network. Finally, we design a bidirectional LSTM recurrent neural network to learn the multi-aspect features with further integrating the softmax classifier to achieve target recognition. Experimental results demonstrate that the proposed method can achieve 99.9% accuracy for 10-class recognition. Besides, its anti-noise and anti-confusion performances are also better than the conventional deep learning-based methods.","","","10.1109/ACCESS.2017.2773363","National Natural Science Foundation of China; Beijing Natural Science Foundation; National Major Research High Performance Computing Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8106789","Synthetic aperture radar (SAR);automatic target recognition (ATR);multi-aspect SAR;long short-term memory (LSTM)","Feature extraction;Synthetic aperture radar;Target recognition;Machine learning;Hidden Markov models;Scattering;Image sequences","feature extraction;Gabor filters;image classification;image sequences;learning (artificial intelligence);multilayer perceptrons;object recognition;radar imaging;radar target recognition;recurrent neural nets;synthetic aperture radar","multiaspect-aware bidirectional LSTM networks;synthetic aperture radar automatic target recognition;learning process;SAR image;multiaspect joint recognition;classification accuracy;robustness;multiaspect space-varying image sequences;three-patch local binary pattern;multilayer perceptron network;bidirectional LSTM recurrent neural network;conventional deep learning-based method;anticonfusion performances;antinoise performances;target recognition;softmax classifier;multiaspect feature learning;dimensionality reduction;comprehensive spatial feature extraction;Gabor filter;space-varying scattering information learning;bidirectional long short-term memory recurrent neural networks;multiaspect-aware method;space-varying scattering information;static scattering information learning;deep learning based ATR solution;pattern recognition performance","","12","53","","","","","IEEE","IEEE Journals"
"$\mathtt {Deepr}$: A Convolutional Net for Medical Records","P. Nguyen; T. Tran; N. Wickramasinghe; S. Venkatesh","Centre for Pattern Recognition and Data Analytics, Faculty of Science and Technology, Deakin University, Geelong, Vic, Australia; Centre for Pattern Recognition and Data Analytics, Faculty of Science and Technology, Deakin University, Geelong, Vic, Australia; Health Informatics Management, Deakin University, Geelong, Vic, Australia; Centre for Pattern Recognition and Data Analytics, Faculty of Science and Technology, Deakin University, Geelong, Vic, Australia","IEEE Journal of Biomedical and Health Informatics","","2017","21","1","22","30","Feature engineering remains a major bottleneck when creating predictive systems from electronic medical records. At present, an important missing element is detecting predictive regular clinical motifs from irregular episodic records. We present Deepr (short for Deep record), a new end-to-end deep learning system that learns to extract features from medical records and predicts future risk automatically. Deepr transforms a record into a sequence of discrete elements separated by coded time gaps and hospital transfers. On top of the sequence is a convolutional neural net that detects and combines predictive local clinical motifs to stratify the risk. Deepr permits transparent inspection and visualization of its inner working. We validate Deepr on hospital data to predict unplanned readmission after discharge. Deepr achieves superior accuracy compared to traditional techniques, detects meaningful clinical motifs, and uncovers the underlying structure of the disease and intervention space.","","","10.1109/JBHI.2016.2633963","Telstra-Deakin Centre of Excellence in Big Data and Machine Learning; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762861","Convolutional neural networks;deep learning;medical records","Medical diagnostic imaging;Machine learning;Diseases;Feature extraction;Hospitals;Electronic medical records","diseases;electronic health records;feature extraction;hospitals;learning (artificial intelligence);neural nets","Deepr;convolutional neural networks;electronic medical records;end-to-end deep learning system;feature extraction;coded time gaps;hospital transfers;hospital data;disease","Artificial Intelligence;Databases, Factual;Electronic Health Records;Humans;Medical Informatics;Models, Theoretical;Neural Networks (Computer);Software","58","51","","","","","IEEE","IEEE Journals"
"Deep-learning architecture to forecast destinations of bus passengers from entry-only smart-card data","J. Jung; K. Sohn","Chung-Ang University, Korea; Chung-Ang University, Korea","IET Intelligent Transport Systems","","2017","11","6","334","339","Although smart-card data secures collective travel information on public transportation users, the reality is that only a few cities are equipped with an automatic fare collection (AFC) system that can provide user information for both boarding and alighting locations. Many researchers have delved into forecasting the destinations of smart-card users. Such effort, however, have never been validated with actual data on a large scale. In the present study, a deep-learning model was developed to estimate the destinations of bus passengers based on both entry-only smart-card data and land-use characteristics. A supervised machine-learning model was trained using exact information on both boarding and alighting. That information was provided by the AFC system in Seoul, Korea. The model performance was superior to that of the most prevalent schemes developed thus far.","","","10.1049/iet-its.2016.0276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979737","","","learning (artificial intelligence);public transport;smart cards;traffic information systems","destinations forecasting;alighting locations;boarding locations;user information;supervised machine-learning model;land-use characteristics;AFC system;automatic fare collection system;collective travel information;public transportation users;entry-only smart-card data;bus passengers;deep-learning architecture","","7","21","","","","","IET","IET Journals"
"Robust Large Margin Deep Neural Networks","J. Sokolić; R. Giryes; G. Sapiro; M. R. D. Rodrigues","Department of Electronic and Electrical Engineering, Univeristy College London, London, U.K.; Faculty of Engineering, School of Electrical Engineering, Tel-Aviv University, Tel Aviv, Israel; Department of Electrical and Computer Engineering, Duke University, NC, USA; Department of Electronic and Electrical Engineering, Univeristy College London, London, U.K.","IEEE Transactions on Signal Processing","","2017","65","16","4265","4280","The generalization error of deep neural networks via their classification margin is studied in this paper. Our approach is based on the Jacobian matrix of a deep neural network and can be applied to networks with arbitrary nonlinearities and pooling layers, and to networks with different architectures such as feed forward networks and residual networks. Our analysis leads to the conclusion that a bounded spectral norm of the network's Jacobian matrix in the neighbourhood of the training samples is crucial for a deep neural network of arbitrary depth and width to generalize well. This is a significant improvement over the current bounds in the literature, which imply that the generalization error grows with either the width or the depth of the network. Moreover, it shows that the recently proposed batch normalization and weight normalization reparametrizations enjoy good generalization properties, and leads to a novel network regularizer based on the network's Jacobian matrix. The analysis is supported with experimental results on the MNIST, CIFAR-10, LaRED, and ImageNet datasets.","","","10.1109/TSP.2017.2708039","Engineering and Physical Sciences Research Council; GIF; German-Israeli Foundation for Scientific Research and Development; National Science Foundation; Office of Naval Research; ARO; NGA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934087","Deep learning;deep neural networks;generalization error;robustness","Jacobian matrices;Robustness;Neural networks;Training;Optimization;Electronic mail;Transforms","Jacobian matrices;neural nets;signal processing","robust large margin deep neural networks;generalization error;deep neural networks;Jacobian matrix;pooling layers;feed forward networks;residual networks;generalization properties;MNIST;CIFAR-10;LaRED;ImageNet datasets","","12","55","CCBY","","","","IEEE","IEEE Journals"
"Multilayer Projective Dictionary Pair Learning and Sparse Autoencoder for PolSAR Image Classification","Y. Chen; L. Jiao; Y. Li; J. Zhao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center of Intelligent Perception and Computation, International Collaboration Joint Laboratory in Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center of Intelligent Perception and Computation, International Collaboration Joint Laboratory in Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center of Intelligent Perception and Computation, International Collaboration Joint Laboratory in Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center of Intelligent Perception and Computation, International Collaboration Joint Laboratory in Intelligent Perception and Computation, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","12","6683","6694","Polarimetric synthetic aperture radar (PolSAR) image classification is a vital application in remote sensing image processing. In general, PolSAR image classification is actually a high-dimensional nonlinear mapping problem. The methods based on sparse representation and deep learning have shown a great potential for PolSAR image classification. Therefore, a novel PolSAR image classification method based on multilayer projective dictionary pair learning (MDPL) and sparse auto encoder (SAE) is proposed in this paper. First, MDPL is used to extract features, and the abstract degree of the extracted features is high. Second, in order to get the nonlinear relationship between elements of feature vectors in an adaptive way, SAE is also used in this paper. Three PolSAR images are used to test the effectiveness of our method. Compared with several state-of-the-art methods, our method achieves very competitive results in PolSAR image classification.","","","10.1109/TGRS.2017.2727067","National Natural Science Foundation of China; Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project); Program for New Century Excellent Talents in University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070460","Multilayer projective dictionary pair learning (MDPL);polarimetric synthetic aperture radar (PolSAR);sparse autoencoder (SAE);sparse representation","Dictionaries;Feature extraction;Encoding;Nonhomogeneous media;Machine learning;Scattering","feature extraction;geophysical image processing;image classification;image representation;learning (artificial intelligence);radar imaging;radar polarimetry;remote sensing;remote sensing by radar;synthetic aperture radar","multilayer projective dictionary pair learning;polarimetric synthetic aperture radar image classification;remote sensing image processing;PolSAR image classification method;deep learning;feature vectors","","4","51","","","","","IEEE","IEEE Journals"
"DLTSR: A Deep Learning Framework for Recommendation of Long-tail Web Services","B. Bai; Y. Fan; W. Tan; J. Zhang","NA; Department of Automation, Tsinghua University, Beijing, China.(email:fanyus@tsinghua.edu.cn); NA; NA","IEEE Transactions on Services Computing","","2017","PP","99","1","1","With the growing popularity of web services, more and more developers are composing multiple services into mashups. Developers show an increasing interest in non-popular services (i.e., long-tail ones), however, there are very scarce studies trying to address the long-tail web service recommendation problem. The major challenges for recommending long-tail services accurately include severe sparsity of historical usage data and unsatisfactory quality of description content. In this paper, we propose to build a deep learning framework to address these challenges and perform accurate long-tail recommendations. To tackle the problem of unsatisfactory quality of description content, we use stacked denoising autoencoders (SDAE) to perform feature extraction. Additionally, we impose the usage records in hot services as a regularization of the encoding output of SDAE, to provide feedback to content extraction. To address the sparsity of historical usage data, we learn the patterns of developers’ preference instead of modeling individual services. Our experimental results on a real-world dataset demonstrate that, with such joint autoencoder based feature representation and content-usage learning framework, the proposed algorithm outperforms the state-of-the-art baselines significantly.","","","10.1109/TSC.2017.2681666","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876831","Deep learning;mashup creation;service recommendation;long-tail","Mashups;Machine learning;Noise reduction;Ecosystems;Neural networks;Data mining","","","","2","","","","","","IEEE","IEEE Early Access Articles"
"Deep CNNs With Spatially Weighted Pooling for Fine-Grained Car Recognition","Q. Hu; H. Wang; T. Li; C. Shen","The University of Adelaide, Adelaide, SA, Australia; Faculty of Electronic Information and Electrical Engineering, School of Computer Science and Technology, Dalian University of Technology, Dalian, China; The University of Adelaide, Adelaide, SA, Australia; The University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Intelligent Transportation Systems","","2017","18","11","3147","3156","Fine-grained car recognition aims to recognize the category information of a car, such as car make, car model, or even the year of manufacture. A number of recent studies have shown that a deep convolutional neural network (DCNN) trained on a large-scale data set can achieve impressive results at a range of generic object classification tasks. In this paper, we propose a spatially weighted pooling (SWP) strategy, which considerably improves the robustness and effectiveness of the feature representation of most dominant DCNNs. More specifically, the SWP is a novel pooling layer, which contains a predefined number of spatially weighted masks or pooling channels. The SWP pools the extracted features of DCNNs with the guidance of its learnt masks, which measures the importance of the spatial units in terms of discriminative power. As the existing methods that apply uniform grid pooling on the convolutional feature maps of DCNNs, the proposed method can extract the convolutional features and generate the pooling channels from a single DCNN. Thus minimal modification is needed in terms of implementation. Moreover, the parameters of the SWP layer can be learned in the end-to-end training process of the DCNN. By applying our method to several fine-grained car recognition data sets, we demonstrate that the proposed method can achieve better performance than recent approaches in the literature. We advance the state-of-the-art results by improving the accuracy from 92.6% to 93.1% on the Stanford Cars-196 data set and 91.2% to 97.6% on the recent CompCars data set. We have also tested the proposed method on two additional large-scale data sets with impressive results observed.","","","10.1109/TITS.2017.2679114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891907","Deep learning;fine-grained recognition;car model classification;spatially weighted pooling","Automobiles;Feature extraction;Three-dimensional displays;Surveillance;Robustness;Visualization;Cameras","automobiles;computer vision;feature extraction;feedforward neural nets;image classification;image representation;learning (artificial intelligence);object recognition;traffic engineering computing","deep CNNs;car model;deep convolutional neural network;large-scale data set;generic object classification tasks;spatially weighted pooling strategy;feature representation;dominant DCNNs;pooling layer;spatially weighted masks;pooling channels;spatial units;uniform grid pooling;convolutional features;SWP layer;end-to-end training process;fine-grained car recognition data sets;Stanford Cars-196 data;large-scale data sets;CompCars data;car make","","10","30","","","","","IEEE","IEEE Journals"
"A Novel Methodology to Label Urban Remote Sensing Images Based on Location-Based Social Media Photos","M. Chi; Z. Sun; Y. Qin; J. Shen; J. A. Benediktsson","School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","Proceedings of the IEEE","","2017","105","10","1926","1936","With the rapid development of the internet and popularization of intelligent mobile devices, social media is evolving fast and contains rich spatial information, such as geolocated posts, tweets, photos, video, and audio. Those location-based social media data have offered new opportunities for hazards and disaster identification or tracking, recommendations for locations, friends or tags, pay-per-click advertising, etc. Meanwhile, a massive amount of remote sensing (RS) data can be easily acquired in both high temporal and spatial resolution with a multiple satellite system, if RS maps can be provided, to possibly enable the monitoring of our location-based living environments with some devices like charge-coupled device (CCD) cameras but on a much larger scale. To generate the classification maps, usually, labeled RS image pixels should be provided by RS experts to train a classification system. Traditionally, labeled samples are obtained according to ground surveys, image photo interpretation or a combination of the aforementioned strategies. All the strategies should be taken care of by domain experts, in a means which is costly, time consuming, and sometimes of a low quality due to reasons such as photo interpretation based on RS images only. These practices and constraints make it more challenging to classify land-cover RS images using big RS data. In this paper, a new methodology is proposed to classify urban RS images by exploiting the semantics of location-based social media photos (SMPs). To validate the effectiveness of this methodology, an automatic classification system is developed based on RS images as well as SMPs via big data analysis techniques including active learning, crowdsourcing, shallow machine learning, and deep learning. As the labels of RS training data are given by ordinary people with a crowdsourcing technique, the developed system is named Crowd4RS. The quantitative and qualitative experiments confirm the effectiveness of the proposed Crowd4RS system as well as the proposed methodology for automatically generating RS image maps in terms of classification results based on big RS data made up of multispectral RS images in a high spatial resolution and a large amount of photos from social media sites, such as Flickr and Panoramio.","","","10.1109/JPROC.2017.2730585","Natural Science Foundation of China; State Key Research and Development Program of China; Research Fund of the University of Iceland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8002577","Big data;crowdsourcing;deep learning;remote sensing;social media","Social network services;Crowdsourcing;Big data;Spatial resolution;Remote sensing;Satellites;Urban areas;Social network services;Mobile communication;Machine learning","Big Data;crowdsourcing;data acquisition;data analysis;image classification;image resolution;Internet;learning (artificial intelligence);mobile computing;remote sensing;social networking (online)","urban remote sensing image labeling;location-based social media photos;Internet;intelligent mobile devices;location-based social media data;remote sensing data acquisition;multiple satellite system;classification maps;RS image classification;image photo interpretation;big RS data;big data analysis;active learning;crowdsourcing;shallow machine learning;deep learning;RS training data;Crowd4RS;multispectral RS image resolution","","3","60","Traditional","","","","IEEE","IEEE Journals"
"A Novel Simple Visual Tracking Algorithm Based on Hashing and Deep Learning","S. Zhu; J. Du; N. Ren","Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China","Chinese Journal of Electronics","","2017","26","5","1073","1078","Deep network has been proven efficient and robust to capture object features in some conditions. It still remains in the stage of classifying or detecting objects. In the field of visual tracking, deep network has not been applied widely. One of the reasons is that its time consuming made the strong method could not meet the speed need of visual tracking. A novel simple tracker is proposed to complete tracking task. A simple six-layer feed-forward backpropagation neural network is applied to capture object features. Nevertheless, this representation is not robust enough when illumination changes or drastic scale changes in dynamic condition. To improve the performance and not to increase much time spent, image perceptual hashing method is employed, which extracts low frequency information of object as its fingerprint to recognize the object from its structure. 64-bit characters are calculated by it, and they are utilized to be the bias terms of the neutral network. This leads more significant improvement for performance of extracting sufficient object features. Then we take particle filter to complete the tracking process with the proposed representation. The experimental results demonstrate that the proposed algorithm is efficient and robust compared with the state-of-the-art tracking methods.","","","10.1049/cje.2016.06.026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8055204","","","backpropagation;learning (artificial intelligence);neural nets;object detection;visual perception","visual tracking algorithm;deep learning;deep network;object features;object classification;object detection;six-layer feed-forward backpropagation neural network;image perceptual hashing;word length 64 bit","","","","","","","","IET","IET Journals"
"Computer-aided mammogram diagnosis system using deep learning convolutional fully complex-valued relaxation neural network classifier","S. Duraisamy; S. Emperumal","Manakula Vinayagar Institute of Technology, India; Manakula Vinayagar Institute of Technology, India","IET Computer Vision","","2017","11","8","656","662","In this study, a novel deep learning-based framework for classifying the digital mammograms is introduced. The development of this methodology is based on deep learning strategies that model the presence of the tumour tissues with level sets. It is difficult to robustly segment mammogram image due to low contrast between normal and lesion tissues. Therefore, Chan-Vese level set method is used to extract the initial contour of mammograms and deep learning convolutional neural network (DL-CNN) algorithm is used to learn the features of mammary-specific mass and microcalcification clusters. To increase the classification accuracy and reduce the false positives, a well-known fully complex-valued relaxation network classifier is used in the last stage of DL-CNN network. Experimental results using the standard benchmarking breast cancer dataset (MIAS and BCDR) show that the proposed method exhibits significant improvement in performance over the traditional methods. Performance measures such as accuracy, sensitivity, specificity, AUC achieved are 99%, 0.9875, 1.0 and 0.9815, respectively. The proposed framework performs well in classifying the digital mammograms as normal, benign or malignant and its subclasses as well.","","","10.1049/iet-cvi.2016.0425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8120067","","","cancer;convolution;feature extraction;image classification;image segmentation;learning (artificial intelligence);mammography;medical image processing;neural nets;tumours","computer-aided mammogram diagnosis system;relaxation neural network classifier;digital mammogram classification;tumour tissues;mammogram image segmentation;lesion tissues;Chan-Vese level set method;contour extraction;deep learning convolutional neural network;DL-CNN algorithm;mammary-specific mass features;microcalcification cluster features;classification accuracy;breast cancer dataset","","2","35","","","","","IET","IET Journals"
"Multisource Transfer Learning With Convolutional Neural Networks for Lung Pattern Analysis","S. Christodoulidis; M. Anthimopoulos; L. Ebner; A. Christe; S. Mougiakakou","ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland; ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland; Department of Diagnostic, Interventional and Pediatric Radiology, Bern University Hospital “Inselspital”, Bern, Switzerland; Department of Diagnostic, Interventional and Pediatric Radiology, Bern University Hospital “Inselspital”, Bern, Switzerland; Department of Diagnostic, Interventional and Pediatric Radiology, Bern University Hospital “Inselspital”, Bern, Switzerland","IEEE Journal of Biomedical and Health Informatics","","2017","21","1","76","84","Early diagnosis of interstitial lung diseases is crucial for their treatment, but even experienced physicians find it difficult, as their clinical manifestations are similar. In order to assist with the diagnosis, computer-aided diagnosis systems have been developed. These commonly rely on a fixed scale classifier that scans CT images, recognizes textural lung patterns, and generates a map of pathologies. In a previous study, we proposed a method for classifying lung tissue patterns using a deep convolutional neural network (CNN), with an architecture designed for the specific problem. In this study, we present an improved method for training the proposed network by transferring knowledge from the similar domain of general texture classification. Six publicly available texture databases are used to pretrain networks with the proposed architecture, which are then fine-tuned on the lung tissue data. The resulting CNNs are combined in an ensemble and their fused knowledge is compressed back to a network with the original architecture. The proposed approach resulted in an absolute increase of about 2% in the performance of the proposed CNN. The results demonstrate the potential of transfer learning in the field of medical image analysis, indicate the textural nature of the problem and show that the method used for training a network can be as important as designing its architecture.","","","10.1109/JBHI.2016.2636929","Bern University Hospital; Swiss National Science Foundation (SNSF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776792","Convolutional neural networks (CNNs);interstitial lung diseases (ILDs);knowledge distillation;model compression;model ensemble;texture classification;transfer learning","Lungs;Biomedical imaging;Training;Databases;Computed tomography;Knowledge engineering;Machine learning","biological tissues;computerised tomography;diseases;image classification;image texture;learning (artificial intelligence);lung;medical image processing;neural nets","multisource transfer learning;convolutional neural networks;lung pattern analysis;interstitial lung disease diagnosis;computer-aided diagnosis;CT images;texture classification;texture databases;lung tissue data;medical image analysis;fused knowledge compression;computed tomography","Humans;Image Interpretation, Computer-Assisted;Lung;Lung Diseases, Interstitial;Machine Learning;Neural Networks (Computer);Pattern Recognition, Automated;Tomography, X-Ray Computed","55","47","","","","","IEEE","IEEE Journals"
"Friend or Foe: Fine-Grained Categorization With Weak Supervision","Z. Xu; D. Tao; S. Huang; Y. Zhang","Cooperative Medianet Innovation Center, Shanghai Key Laboratory of Multimedia Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, China; Centre for Artificial Intelligence, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; Centre for Artificial Intelligence, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; Cooperative Medianet Innovation Center, Shanghai Key Laboratory of Multimedia Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Image Processing","","2017","26","1","135","146","Multi-instance learning (MIL) is widely acknowledged as a fundamental method to solve weakly supervised problems. While MIL is usually effective in standard weakly supervised object recognition tasks, in this paper, we investigate the applicability of MIL on an extreme case of weakly supervised learning on the task of fine-grained visual categorization, in which intra-class variance could be larger than inter-class due to the subtle differences between subordinate categories. For this challenging task, we propose a new method that generalizes the standard multi-instance learning framework, for which a novel multi-task co-localization algorithm is proposed to take advantage of the relationship among fine-grained categories and meanwhile performs as an effective initialization strategy for the non-convex multi-instance objective. The localization results also enable object-level domain-specific fine-tuning of deep neural networks, which significantly boosts the performance. Experimental results on three fine-grained datasets reveal the effectiveness of the proposed method, especially the importance of exploiting inter-class relationships between object categories in weakly supervised fine-grained recognition.","","","10.1109/TIP.2016.2621661","High Technology Research and Development Program of China; NSFC; STCSM; 111 Project; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7707411","Fine-grained visual categorization;weakly supervised learning;co-localization;multi-instance learning;multi-task learning","Feature extraction;Training;Clustering algorithms;Object recognition;Visualization;Supervised learning;Neural networks","learning (artificial intelligence);object recognition;pattern classification","MIL;weakly supervised object recognition tasks;weakly supervised learning;fine-grained visual categorization;multi-instance learning framework;multitask co-localization algorithm;initialization strategy;nonconvex multi-instance objective;object-level domain-specific fine-tuning;deep neural networks;fine-grained classification","","12","78","","","","","IEEE","IEEE Journals"
"Stripes: Bit-Serial Deep Neural Network Computing","P. Judd; J. Albericio; A. Moshovos","The Edward S. Rogers Sr. Department of Electrical & Computer Engineering, University of Toronto, Toronto, ON, Canada; The Edward S. Rogers Sr. Department of Electrical & Computer Engineering, University of Toronto, Toronto, ON, Canada; The Edward S. Rogers Sr. Department of Electrical & Computer Engineering, University of Toronto, Toronto, ON, Canada","IEEE Computer Architecture Letters","","2017","16","1","80","83","The numerical representation precision required by the computations performed by Deep Neural Networks (DNNs) varies across networks and between layers of a same network. This observation motivates a precision-based approach to acceleration which takes into account both the computational structure and the required numerical precision representation. This work presents Stripes (STR), a hardware accelerator that uses bit-serial computations to improve energy efficiency and performance. Experimental measurements over a set of state-ofthe-art DNNs for image classification show that STR improves performance over a state-of-the-art accelerator from 1.35x to 5.33x and by 2.24x on average. STR's area and power overhead are estimated at 5 percent and 12 percent respectively. STR is 2.00x more energy efficient than the baseline.","","","10.1109/LCA.2016.2597140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529197","Hardware acceleration;deep learning;deep neural networks;convolution;numerical representation;serial computing","Neurons;Parallel processing;Three-dimensional displays;Nickel;Graphics processing units;Artificial neural networks","image classification;learning (artificial intelligence);neural nets","Stripes;STR;bit-serial deep neural network computing;precision-based approach;bit-serial computations;image classification;energy efficiency","","9","19","","","","","IEEE","IEEE Journals"
"A Deep Normalization and Convolutional Neural Network for Image Smoke Detection","Z. Yin; B. Wan; F. Yuan; X. Xia; J. Shi","Department of Communications and Electronics, Jiangxi Science and Technology Normal University, Nanchang, China; Department of Communications and Electronics, Jiangxi Science and Technology Normal University, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Access","","2017","5","","18429","18438","It is a challenging task to recognize smoke from images due to large variance of smoke color, texture, and shapes. There are smoke detection methods that have been proposed, but most of them are based on hand-crafted features. To improve the performance of smoke detection, we propose a novel deep normalization and convolutional neural network (DNCNN) with 14 layers to implement automatic feature extraction and classification. In DNCNN, traditional convolutional layers are replaced with normalization and convolutional layers to accelerate the training process and boost the performance of smoke detection. To reduce overfitting caused by imbalanced and insufficient training samples, we generate more training samples from original training data sets by using a variety of data enhancement techniques. Experimental results show that our method achieved very low false alarm rates below 0.60% with detection rates above 96.37% on our smoke data sets.","","","10.1109/ACCESS.2017.2747399","Natural Science Foundation of China; Cultivated Talent Program for Young Scientists of Jiangxi Province; Science Technology Application Project of Jiangxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8022860","Deep neural networks;deep learning;smoke detection;image classification","Feature extraction;Fires;Training;Temperature sensors;Neurons","convolution;feature extraction;image classification;learning (artificial intelligence);neural nets;smoke","automatic feature extraction;DNCNN;traditional convolutional layers;training process;detection rates;smoke data sets;deep normalization;convolutional neural network;image smoke detection;smoke color;smoke detection methods;training data sets;hand-crafted features;automatic feature classification","","19","27","OAPA","","","","IEEE","IEEE Journals"
"Malaria Parasite Detection From Peripheral Blood Smear Images Using Deep Belief Networks","D. Bibin; M. S. Nair; P. Punitha","Department of Research and Development Centre, Bharathiar University, Coimbatore, India; Department of Computer Science, University of Kerala, Thiruvananthapuram; Department of Computer Applications, PES Institute of Technology, Bengaluru, India","IEEE Access","","2017","5","","9099","9108","In this paper, we propose a novel method to identify the presence of malaria parasites in human peripheral blood smear images using a deep belief network (DBN). This paper introduces a trained model based on a DBN to classify 4100 peripheral blood smear images into the parasite or non-parasite class. The proposed DBN is pre-trained by stacking restricted Boltzmann machines using the contrastive divergence method for pre-training. To train the DBN, we extract features from the images and initialize the visible variables of the DBN. A concatenated feature of color and texture is used as a feature vector in this paper. Finally, the DBN is discriminatively fine-tuned using a backpropagation algorithm that computes the probability of class labels. The optimum size of the DBN architecture used in this paper is 484-600-600-600-600-2, in which the visible layer has 484 nodes and the output layer has two nodes with four hidden layers containing 600 hidden nodes in every layer. The proposed method has performed significantly better than the other state-of-the-art methods with an F-score of 89.66%, a sensitivity of 97.60%, and specificity of 95.92%. This paper is the first application of a DBN for malaria parasite detection in human peripheral blood smear images.","","","10.1109/ACCESS.2017.2705642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7931565","Deep learning;deep belief network;malaria parasite detection;restricted Boltzmann machine;contrastive divergence;discriminative training","Diseases;Blood;Computer architecture;Feature extraction;Image color analysis;Training;Microscopy","backpropagation;belief networks;biology computing;blood;Boltzmann machines;feature extraction;image classification;learning (artificial intelligence)","human peripheral blood smear images;backpropagation algorithm;feature vector;feature extraction;contrastive divergence method;restricted Boltzmann machines;DBN;deep belief networks;malaria parasite detection","","13","53","","","","","IEEE","IEEE Journals"
"Deep Learning Hierarchical Representations for Image Steganalysis","J. Ye; J. Ni; Y. Yi","School of Electronic and Information Technology, Sun Yat-Sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-Sen University, Guangdong, China; School of Data and Computer Science, Sun Yat-Sen University, Guangdong, China","IEEE Transactions on Information Forensics and Security","","2017","12","11","2545","2557","Nowadays, the prevailing detectors of steganographic communication in digital images mainly consist of three steps, i.e., residual computation, feature extraction, and binary classification. In this paper, we present an alternative approach to steganalysis of digital images based on convolutional neural network (CNN), which is shown to be able to well replicate and optimize these key steps in a unified framework and learn hierarchical representations directly from raw images. The proposed CNN has a quite different structure from the ones used in conventional computer vision tasks. Rather than a random strategy, the weights in the first layer of the proposed CNN are initialized with the basic high-pass filter set used in the calculation of residual maps in a spatial rich model (SRM), which acts as a regularizer to suppress the image content effectively. To better capture the structure of embedding signals, which usually have extremely low SNR (stego signal to image content), a new activation function called a truncated linear unit is adopted in our CNN model. Finally, we further boost the performance of the proposed CNN-based steganalyzer by incorporating the knowledge of selection channel. Three state-of-the-art steganographic algorithms in spatial domain, e.g., WOW, S-UNIWARD, and HILL, are used to evaluate the effectiveness of our model. Compared to SRM and its selection-channel-aware variant maxSRMd2, our model achieves superior performance across all tested algorithms for a wide variety of payloads.","","","10.1109/TIFS.2017.2710946","National Natural Science Foundation of China; National Research Foundation for the Doctoral Program of Higher Education of China; Key Program of Natural Science Foundation of Guangdong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937836","Steganalysis;convolutional neural networks;feature learning","Feature extraction;Convolution;Neural networks;Computational modeling;Sun;Pipelines;Kernel","computer vision;high-pass filters;image coding;image filtering;image representation;learning (artificial intelligence);neural nets;steganography","hierarchical representation deep learning;steganographic communication;residual computation;feature extraction;binary classification;digital image steganalysis;convolutional neural network;computer vision;high-pass filter;residual map;spatial rich model;stego signal;image content;activation function;truncated linear unit;CNN-based steganalyzer;steganographic algorithm;spatial domain;WOW;S-UNIWARD;HILL;maxSRMd2","","22","32","","","","","IEEE","IEEE Journals"
"Topic-Aware Deep Compositional Models for Sentence Classification","R. Zhao; K. Mao","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","2","248","260","In recent years, deep compositional models have emerged as a popular technique for representation learning of sentence in computational linguistic and natural language processing. These models normally train various forms of neural networks on top of pretrained word embeddings using a task-specific corpus. However, most of these works neglect the multisense nature of words in the pretrained word embeddings. In this paper we introduce topic models to enrich the word embeddings for multisenses of words. The integration of the topic model with various semantic compositional processes leads to topic-aware convolutional neural network and topic-aware long short term memory networks. Different from previous multisense word embeddings models that assign multiple independent and sense-specific embeddings to each word, our proposed models are lightweight and have flexible frameworks that regard word sense as the composition of two parts: a general sense derived from a large corpus and a topic-specific sense derived from a task-specific corpus. In addition, our proposed models focus on semantic composition instead of word understanding. With the help of topic models, we can integrate the topic-specific sense at word-level before the composition and sentence-level after the composition. Comprehensive experiments on five public sentence classification datasets are conducted and the results show that our proposed topic-aware deep compositional models produce competitive or better performance than other text representation learning methods.","","","10.1109/TASLP.2016.2632521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7755816","Machine learning;natural language processing;sentence classification;text representation learning","Semantics;Neural networks;Context;Natural language processing;Numerical models;Machine learning;Computational modeling","convolution;natural language processing;neural nets;pattern classification;text analysis","topic-aware deep compositional models;sentence classification;representation learning;computational linguistic;natural language processing;semantic compositional processes;topic-aware convolutional neural network;topic-aware long short term memory networks;multisense word embeddings;sense-specific embeddings;word sense;task-specific corpus;topic-specific sense;word-level;sentence-level","","10","63","","","","","IEEE","IEEE Journals"
"Learning CNN to Pair UAV Video Image Patches","Y. Chen; L. Liu; Z. Gong; P. Zhong","College of Electrical Science and Engineering, National University of Defense Technology, Changsha, China; College of Electrical Science and Engineering, National University of Defense Technology, Changsha, China; College of Electrical Science and Engineering, National University of Defense Technology, Changsha, China; College of Electrical Science and Engineering, National University of Defense Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","12","5752","5768","Pairing image patches is to decide whether two image patches belong to the same scene but taken from different imaging conditions. It is a key procedure in the applications of unmanned aerial vehicle (UAV) video images. The challenges in pairing UAV image patches derive from the complex imaging conditions on UAV platforms such as jitter, frequent undefined motion, viewpoint changes, and illumination changes. Available popular methods usually follow the flowchart: preprocess images at first, then extract hand-crafted features, and finally match the extracted features through evaluating an independently predefined similarity metric. These methods could only handle part of negative factors from the complex imaging conditions and thus cannot effectively handle the challenges in pairing UAV image patches. This study aims to handle the challenges through automatically and simultaneously learning more representative features and accurate metric. Especially, this study proposes a deep learning method to jointly learn the feature representations and similarity metric over the training samples obtained from various imaging conditions. The model structure of the proposed pairing system consists of three parts: two stream convolutional neural networks (CNNs), one similarity metric layer and one softmax layer. They are jointly trained through the usual back propagation algorithm. Moreover, to further improve the performance, this study develops a transfer learning strategy for the proposed deep model. Two new training datasets from satellite scenes and UAV scenes, respectively, are built to evaluate the proposed pairing system, and the experimental results show that our method outperforms the most recent approaches in pairing UAV video image patches.","","","10.1109/JSTARS.2017.2740898","Natural Science Foundation of China; Foundation for the Author of National Excellent Doctoral Dissertation of China; Program for New Century Excellent Talents in University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8064733","Convolutional neural network (CNN);image patch pairing;joint learning;similarity metric;transfer learning;unmanned aerial vehicle (UAV)","Feature extraction;Unmanned aerial vehicles;Image processing;Convolutional neural networks","autonomous aerial vehicles;feature extraction;feedforward neural nets;image classification;image representation;learning (artificial intelligence)","unmanned aerial vehicle video images;complex imaging conditions;UAV platforms;independently predefined similarity metric;deep learning method;feature representations;UAV scenes;UAV video image patches","","2","32","Traditional","","","","IEEE","IEEE Journals"
"Deep Belief Networks Ensemble for Blood Pressure Estimation","S. Lee; J. Chang","Department of Electronic Engineering, Hanyang University, Seoul, South Korea; Department of Electronic Engineering, Hanyang University, Seoul, South Korea","IEEE Access","","2017","5","","9962","9972","In this paper, we propose a deep belief network (DBN)-deep neural network (DNN) with mimic features based on the bootstrap inspired technique to learn the complex nonlinear relationship between the mimic feature vectors obtained from the oscillometry signals and the target blood pressures. Unfortunately, we have two problems in utilizing the DBN-DNN technique to estimate the systolic blood pressure (SBP) and diastolic blood pressure (DBP). First, our set of input feature vectors is very small, which is a fatal drawback to training based on the DBN-DNN technique. Second, the special pre-training phase can also trigger an unstable estimation, because there are still a lot of random initialized assigns, such as the training data set, weights, and biases. For these reasons, we employ the bootstrap-inspired technique as a fusion ensemble estimator based on the DBN-DNN-based regression model, which is used to create the mimic features to estimate the SBP and DBP. Our DBN-DNN-based ensemble regression estimator provides a lower standard deviation of error, mean error, and mean absolute error for the SBP and DBP as compared with those of the conventional methods.","","","10.1109/ACCESS.2017.2701800","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7921528","Blood pressure measurement;oscillometry blood pressure estimation;deep neural networks;bootstrap-inspired technique;ensemble","Blood pressure;Biomedical monitoring;Estimation;Training;Standards;Neural networks;Training data","belief networks;blood pressure measurement;estimation theory;learning (artificial intelligence);medical computing;neural nets;regression analysis","mean absolute error;mean error;standard error deviation;DBN-DNN-based ensemble regression estimator;fusion ensemble estimator;random initialized assigns;pre-training phase;DBP;diastolic blood pressure;SBP;systolic blood pressure;oscillometry signals;mimic feature vectors;complex nonlinear relationship learning;bootstrap inspired technique;deep neural network;blood pressure estimation;deep belief networks ensemble","","8","33","","","","","IEEE","IEEE Journals"
"Discriminative Nonlinear Analysis Operator Learning: When Cosparse Model Meets Image Classification","Z. Wen; B. Hou; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China","IEEE Transactions on Image Processing","","2017","26","7","3449","3462","A linear synthesis model-based dictionary learning framework has achieved remarkable performances in image classification in the last decade. Behaved as a generative feature model, it, however, suffers from some intrinsic deficiencies. In this paper, we propose a novel parametric nonlinear analysis cosparse model (NACM) with which a unique feature vector will be much more efficiently extracted. Additionally, we derive a deep insight to demonstrate that NACM is capable of simultaneously learning the task-adapted feature transformation and regularization to encode our preferences, domain prior knowledge, and task-oriented supervised information into the features. The proposed NACM is devoted to the classification task as a discriminative feature model and yield a novel discriminative nonlinear analysis operator learning framework (DNAOL). The theoretical analysis and experimental performances clearly demonstrate that DNAOL will not only achieve the better or at least competitive classification accuracies than the state-of-the-art algorithms, but it can also dramatically reduce the time complexities in both training and testing phases.","","","10.1109/TIP.2017.2700761","National Natural Science Foundation of China; National Research Foundation for the Doctoral Program of Higher Education of China; National Basic Research Program (973 Program) of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918618","Nonlinear analysis cosparse model;analysis operator learning;regularization learning;generative model;discriminative model;dictionary learning;linear synthesis model;image classification","Feature extraction;Analytical models;Training;Dictionaries;Testing;Adaptation models;Computational modeling","image classification;learning (artificial intelligence);vectors","testing phases;training phases;DNAOL;task-oriented supervised information;domain prior knowledge;preferences;task-adapted regularization;task-adapted feature transformation;feature vector;NACM;parametric nonlinear analysis cosparse model;generative feature model;dictionary learning framework;linear synthesis model;image classification;discriminative nonlinear analysis operator learning","","7","60","","","","","IEEE","IEEE Journals"
"Deep convolutional neural networks for automatic segmentation of left ventricle cavity from cardiac magnetic resonance images","X. Yang; Z. Zeng; S. Yi","Institute of High Performance Computing (IHPC), A*STAR, Singapore; Institute for Infocomm Research (I2R), A*STAR, Singapore; Institute of High Performance Computing (IHPC), A*STAR, Singapore","IET Computer Vision","","2017","11","8","643","649","This work conducts a feasibility study of deep learning approaches for automatic segmentation of left ventricle (LV) cavity from cardiac magnetic resonance (CMR) images. Automatic LV cavity segmentation is a challenging task, partially due to the small size of the object as compared to the large CMR image background, especially at the apex. To cater for small object segmentation, the authors present a localisation-segmentation framework, to first locate the object in the large full image, then segment the object within the small cropped region of interest. The localisation is performed by a deep regression model based on convolutional neural networks, while the segmentation is done by the deep neural networks based on U-Net architecture. They also employ the Dice loss function for the training process of the segmentation models, to investigate its effects on the segmentation performance. The deep learning models are trained and evaluated by using public endocardium-annotated CMR datasets from York University and MICCAI 2009 LV Challenge websites. The average dice metric values of the authors' proposed framework are 0.91 and 0.93, respectively, on these two databases. These results are promising as compared to the best results achieved by the current state-of-art, which shows the potentials of deep learning approaches for this particular application.","","","10.1049/iet-cvi.2016.0482","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8120080","","","biomedical MRI;cardiovascular system;convolution;image segmentation;learning (artificial intelligence);medical image processing;neural nets;object detection;regression analysis","deep convolutional neural networks;automatic left ventricle cavity segmentation framework;cardiac magnetic resonance images;deep learning approaches;object segmentation framework;localisation-segmentation framework;regression model based convolutional neural networks;deep neural network based U-Net architecture;Dice loss function;public endocardium-annotated CMR datasets","","5","47","","","","","IET","IET Journals"
"Detecting Silicone Mask-Based Presentation Attack via Deep Dictionary Learning","I. Manjani; S. Tariyal; M. Vatsa; R. Singh; A. Majumdar","Indraprastha Institute of Information Technology Delhi, New Delhi, India; Indraprastha Institute of Information Technology Delhi, New Delhi, India; Indraprastha Institute of Information Technology Delhi, New Delhi, India; Indraprastha Institute of Information Technology Delhi, New Delhi, India; Indraprastha Institute of Information Technology Delhi, New Delhi, India","IEEE Transactions on Information Forensics and Security","","2017","12","7","1713","1723","In movies, film stars portray another identity or obfuscate their identity with the help of silicone/latex masks. Such realistic masks are now easily available and are used for entertainment purposes. However, their usage in criminal activities to deceive law enforcement and automatic face recognition systems is also plausible. Therefore, it is important to guard biometrics systems against such realistic presentation attacks. This paper introduces the first-of-its-kind silicone mask attack database which contains 130 real and attacked videos to facilitate research in developing presentation attack detection algorithms for this challenging scenario. Along with silicone mask, there are several other presentation attack instruments that are explored in literature. The next contribution of this research is a novel multilevel deep dictionary learning-based presentation attack detection algorithm that can discern different kinds of attacks. An efficient greedy layer by layer training approach is formulated to learn the deep dictionaries followed by SVM to classify an input sample as genuine or attacked. Experimental are performed on the proposed SMAD database, some samples with real world silicone mask attacks, and four existing presentation attack databases, namely, replay-attack, CASIA-FASD, 3DMAD, and UVAD. The results show that the proposed algorithm yields better performance compared with state-ofthe-art algorithms, in both intra-database and cross-database experiments.","","","10.1109/TIFS.2017.2676720","Ministry of Electronics and Information Technology, India and Infosys Center for Artificial Intelligence at IIIT Delhi; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7867821","Face recognition;silicone mask;presentation attack detection;deep dictionary","Dictionaries;Databases;Videos;Face;Detection algorithms;Training;Protocols","biometrics (access control);entertainment;face recognition","silicone mask-based presentation attack detection;deep dictionary learning;movies;film stars;entertainment purpose;automatic face recognition systems;biometrics systems;SMAD database;replay-attack;CASIA-FASD;3DMAD;UVAD","","52","61","","","","","IEEE","IEEE Journals"
"Fully-Parallel Area-Efficient Deep Neural Network Design Using Stochastic Computing","Y. Xie; S. Liao; B. Yuan; Y. Wang; Z. Wang","City University of New York, New York City, NY, USA; City University of New York, New York City, NY, USA; City University of New York, New York City, NY, USA; Department of Electrical Engineering Computer Science, Syracuse University, Syracuse, NY, USA; School of Electronic Science and Engineering, Nanjing University, Nanjing, China","IEEE Transactions on Circuits and Systems II: Express Briefs","","2017","64","12","1382","1386","Deep neural network (DNN) has emerged as a powerful machine learning technique for various artificial intelligence applications. Due to the unique advantages on speed, area, and power, specific hardware design has become a very attractive solution for the efficient deployment of DNN. However, the huge resource cost of multipliers makes the fully-parallel implementations of multiplication-intensive DNN still very prohibitive in many real-time resource-constrained embedded applications. This brief proposes a fully-parallel area-efficient stochastic DNN design. By leveraging stochastic computing (SC) technique, the computations of DNN are implemented using very simple stochastic logic, thereby enabling low-complexity fully-parallel DNN design. In addition, to avoid the accuracy loss incurred by the approximation of SC, we propose an accuracy-aware DNN datapath architecture to retain the test accuracy of stochastic DNN. Moreover, we propose a novel low-complexity architecture for the binary-to-stochastic (B-to-S) interface to drastically reduce the footprint of the peripheral B-to-S circuit. Experimental results show that the proposed stochastic DNN design achieves much better hardware performance than non-stochastic design with negligible test accuracy loss.","","","10.1109/TCSII.2017.2746749","National Science Foundation; Algorithm in the Field and Defense Advanced Research Projects Agency SAGA Programs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8022910","Deep neural network;stochastic computing;fully-parallel;area-efficient","Stochastic processes;Neural networks;Machine learning;Network topology;Optimization","embedded systems;learning (artificial intelligence);neural nets;stochastic processes","powerful machine learning technique;artificial intelligence applications;specific hardware design;multiplication-intensive DNN;embedded applications;fully-parallel area-efficient stochastic DNN design;simple stochastic logic;fully-parallel DNN design;accuracy-aware DNN datapath architecture;nonstochastic design;area-efficient deep neural network design;stochastic computing technique","","4","10","","","","","IEEE","IEEE Journals"
"Water Desalination Fault Detection Using Machine Learning Approaches: A Comparative Study","M. Derbali; S. M. Buhari; G. Tsaramirsis; M. Stojmenovic; H. Jerbi; M. N. Abdelkrim; M. H. Al-Beirutty","Analyse et Commande Des Systemes, ENIG, Unite de recherché Modelisation, Gabes, Tunisie; Faculty of Computing and IT, King Abdulaziz University, Jeddah, Saudi Arabia; Faculty of Computing and IT, King Abdulaziz University, Jeddah, Saudi Arabia; Faculty of Informatics and Computing, Singidunum University, Belgrade, Serbia; Deanship of the Scientific Research, University of Hail, Baqaa, Saudi Arabia; Analyse et Commande Des Systemes, ENIG, Unite de recherché Modelisation, Gabes, Tunisie; Centre of Excellence in Desalination Technology, King Abdulaziz University, Jeddah, Saudi Arabia","IEEE Access","","2017","5","","23266","23275","The presence of faulty valves has been studied in the literature with various machine learning approaches. The impact of using fault data only to train the system could solve the class imbalance problem in the machine learning approach. The data sets used for fault detection contain many independent variables, where the salient ones were selected using stepwise regression and applied to various machine learning techniques. A significant test for the given regression technique was used to validate the outcome. Machine learning techniques, such as decision trees and deep learning, are applied to the given data and the results reveal that the decision tree was able to obtain more than 95% accuracy and performed better than other algorithms when considering the tradeoff between the processing time and accuracy.","","","10.1109/ACCESS.2017.2716978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953498","Machine learning;stepwise regression;fault detection;water desalination","Desalination;Feature extraction;Fault diagnosis;Principal component analysis;Fault detection;Valves;Decision trees","decision trees;desalination;fault diagnosis;learning (artificial intelligence);mechanical engineering computing;regression analysis;valves","machine learning approach;fault data;machine learning techniques;deep learning;water desalination fault detection;faulty valves;stepwise regression;decision tree","","","15","","","","","IEEE","IEEE Journals"
"Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification","J. Salamon; J. P. Bello","Music and Audio Research Laboratory and the Center for Urban Science and Progress, New York University, New York, NY, USA; Music and Audio Research Laboratory and the Center for Urban Science and Progress, New York University, New York, NY, USA","IEEE Signal Processing Letters","","2017","24","3","279","283","The ability of deep convolutional neural networks (CNNs) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep CNN architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a “shallow” dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.","","","10.1109/LSP.2017.2657381","NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829341","Deep convolutional neural networks (CNNs);deep learning;environmental sound classification;urban sound dataset","Convolution;Training;Neural networks;Data models;Shape;Training data;Time-frequency analysis","audio signal processing;neural nets;signal classification","deep convolutional neural networks;audio data augmentation;environmental sound classification;discriminative spectro-temporal patterns;high-capacity models;augmented training set;class-conditional data augmentation","","191","32","","","","","IEEE","IEEE Journals"
"Stacked Sparse Autoencoder Modeling Using the Synergy of Airborne LiDAR and Satellite Optical and SAR Data to Map Forest Above-Ground Biomass","Z. Shao; L. Zhang; L. Wang","State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing & Collaborative Innovation Center for Geospatial Technology, Wuhan University, Wuhan, China; State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing & Collaborative Innovation Center for Geospatial Technology, Wuhan University, Wuhan, China; State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing & Collaborative Innovation Center for Geospatial Technology, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","12","5569","5582","Timely, spatially complete, and reliable forest above-ground biomass (AGB) data are a prerequisite to support forest management and policy formulation. Traditionally, forest AGB is spatially estimated by integrating satellite images, in particular, optical data, with field plots from forest inventory programs. However, field data are limited in remote and unmanaged areas. In addition, optical reflectance usually saturates at high-density biomass level and is subject to cloud contaminations. Thus, this study aimed to develop a deep learning based workflow for mapping forest AGB by integrating Landsat 8 and Sentinel-1A images with airborne light detection and ranging (LiDAR) data. A reference AGB map was derived from the wall-to-wall LiDAR data and field measurements. The LiDAR plots-stratified random samples of forest biomass extracted from the LiDAR simulated strips in the reference map-were adopted as a surrogate for traditional field plots. In addition to the deep learning model, i.e., stacked sparse autoencoder network (SSAE), five different prediction techniques including multiple stepwise linear regressions, K-nearest neighbor, support vector machine, back propagation neural networks, and random forest were individually used to establish the relationship between LiDAR-derived forest biomass and the satellite predictors. Optical variables (Landsat 8 OLI), SAR variables (Sentinel-1A), and their combined variables were individually input to the six prediction models. Results showed that the SSAE model had the best performance for the forest biomass estimation. The combined optical and microwave dataset as explanatory variables improved the modeling performance compared to either the optical-only or microwave-only data, regardless of prediction algorithms. The best mapping accuracy was obtained by the SSAE model with inputs of optical and microwave integrated metrics that yielded R2of 0.812, root mean squared error (RMSE) of 21.753 Mg/ha, and relative RMSE (RMSEr) of 14.457%. Overall, the SSAE model with inputs of combined Landsat 8 OLI and Sentinel-1A information could result in accurate estimation of forest biomass by using the stratification-sampled and LiDAR-derived AGB as ground reference data. The modeling workflow has the potential to promote future forest growth monitoring and carbon stock assessment across large areas.","","","10.1109/JSTARS.2017.2748341","National Key Technologies Research and Development Program; Fundamental Research Funds for the Central Universities; Guangzhou Science and Technology Project; National Administration of Surveying, Mapping and Geoinformation; Wuhan Chen Guang Project; Special task of technical innovation in Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8039160","Biomass;deep learning (DL);Landsat 8;light detection and ranging (LiDAR);stacked sparse autoencoder network (SSAE);Sentinel-1A","Satellite communication;Remote sensing;Biomass;Laser radar;Biomedical optical imaging;Optical imaging;Optical saturation","forestry;geophysical image processing;image classification;learning (artificial intelligence);optical radar;regression analysis;remote sensing by laser beam;remote sensing by radar;statistical analysis;support vector machines;synthetic aperture radar;vegetation;vegetation mapping","stacked sparse autoencoder modeling;airborne LiDAR;satellite optical;SAR data;map forest above-ground;reliable forest above-ground biomass;forest management;policy formulation;integrating satellite images;particular data;optical data;forest inventory programs;field data;optical reflectance;high-density biomass level;deep learning based workflow;mapping forest AGB;Sentinel-1A;airborne light detection;reference AGB map;wall-to-wall LiDAR data;field measurements;reference map;traditional field plots;deep learning model;stacked sparse autoencoder network;random forest;optical variables;prediction models;SSAE model;forest biomass estimation;combined optical dataset;microwave dataset;modeling performance;mapping accuracy;optical metrics;microwave integrated metrics;combined Landsat 8 OLI;ground reference data;future forest growth monitoring;carbon stock assessment;LiDAR plots","","4","72","Traditional","","","","IEEE","IEEE Journals"
"Lung field segmentation in chest radiographs: a historical review, current status, and expectations from deep learning","A. Mittal; R. Hooda; S. Sofat","UIET, Panjab University, India; PEC University of Technology, India; PEC University of Technology, India","IET Image Processing","","2017","11","11","937","952","Lung field defines a region-of-interest in which specific radiologic signs such as septal lines, pulmonary opacities, cavities, consolidations, and lung nodules are searched by a chest radiographic computer-aided diagnostic system. Thus, its precise segmentation is extremely important. To precisely segment it, numerous methods have been developed during the last four decades. However, no exclusive survey consolidating the advancements in these methods has been presented till date, thus indicating a void and the need. This study fills the void by presenting a comprehensive survey of these methods with a focus on their underlying principle, the dataset used, reported performance, and relative merits and demerits. It refrains from doing a hard comparative evaluation by bringing all of them on a common platform, since the datasets used in their development and testing are of varied quality, complexity, and are not publicly available. It also provides a glimpse of deep learning, the present state of deep-learning-based lung field segmentation methods, expectations from it, and the challenges ahead of it.","","","10.1049/iet-ipr.2016.0526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8089599","","","diagnostic radiography;image segmentation;learning (artificial intelligence);lung;medical image processing","chest radiographic computer-aided diagnostic system;radiologic signs;region-of-interest;deep learning;lung field segmentation","","1","119","","","","","IET","IET Journals"
"A Multi-Task Learning Framework for Emotion Recognition Using 2D Continuous Space","R. Xia; Y. Liu","Department of Computer Science, The University of Texas at Dallas, TX; Department of Computer Science, The University of Texas at Dallas, TX","IEEE Transactions on Affective Computing","","2017","8","1","3","14","Dimensional models have been proposed in psychology studies to represent complex human emotional expressions. Activation and valence are two common dimensions in such models. They can be used to describe certain emotions. For example, anger is one type of emotion with a low valence and high activation value; neutral has both a medium level valence and activation value. In this work, we propose to apply multi-task learning to leverage activation and valence information for acoustic emotion recognition based on the deep belief network (DBN) framework. We treat the categorical emotion recognition task as the major task. For the secondary task, we leverage activation and valence labels in two different ways, category level based classification and continuous level based regression. The combination of the loss functions from the major and secondary tasks is used as the objective function in the multi-task learning framework. After iterative optimization, the values from the last hidden layer in the DBN are used as new features and fed into a support vector machine classifier for emotion recognition. Our experimental results on the Interactive Emotional Dyadic Motion Capture and Sustained Emotionally Colored Machine-Human Interaction Using Nonverbal Expression databases show significant improvements on unweighted accuracy, illustrating the benefit of utilizing additional information in a multi-task learning setup for emotion recognition.","","","10.1109/TAFFC.2015.2512598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7366551","Categorical emotion recognition;multi-task learning;deep belief network;activation;valence","Emotion recognition;Feature extraction;Support vector machines;Acoustics;Speech;Psychology;Linear programming","emotion recognition;feature extraction;image classification;iterative methods;learning (artificial intelligence);optimisation;support vector machines;visual databases","multitask learning framework;2D continuous space;dimensional models;psychology;anger emotion;neutral emotion;valence information;activation value;acoustic emotion recognition;deep belief network framework;DBN framework;categorical emotion recognition task;category level based classification;continuous level based regression;loss functions;major tasks;secondary tasks;objective function;iterative optimization;support vector machine classifier;interactive emotional dyadic motion capture database;sustained emotionally colored machine-human interaction-using-nonverbal expression database","","26","44","","","","","IEEE","IEEE Journals"
"DeepSleepNet: A Model for Automatic Sleep Stage Scoring Based on Raw Single-Channel EEG","A. Supratak; H. Dong; C. Wu; Y. Guo","Department of Computing, Imperial College London, London, U.K.; Department of Computing, Imperial College London, London, U.K.; Department of Computing, Imperial College London, London, U.K.; Department of Computing, Imperial College London, London, U.K.","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","2017","25","11","1998","2008","This paper proposes a deep learning model, named DeepSleepNet, for automatic sleep stage scoring based on raw single-channelEEG. Most of the existing methods rely on hand-engineered features, which require prior knowledge of sleep analysis. Only a few of them encode the temporal information, such as transition rules, which is important for identifying the next sleep stages, into the extracted features. In the proposed model, we utilize convolutional neural networks to extract time-invariant features, and bidirectional-longshort-term memory to learn transition rules among sleep stages automatically from EEG epochs. We implement a two-step training algorithm to train our model efficiently. We evaluated our model using different single-channel EEGs (F4-EOG (left), Fpz-Cz, and Pz-Oz) from two public sleep data sets, that have different properties (e.g., sampling rate) and scoring standards (AASM and R&K). The results showed that our model achieved similar overall accuracy and macro F1-score (MASS: 86.2%-81.7, Sleep-EDF: 82.0%-76.9) compared with the state-of-the-art methods (MASS: 85.9%-80.5, Sleep-EDF: 78.9%-73.7) on both data sets. This demonstrated that, without changing the model architecture and the training algorithm, our model could automatically learn features for sleep stage scoring from different raw single-channel EEGs from different data sets without utilizing any hand-engineered features.","","","10.1109/TNSRE.2017.2721116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961240","Sleep stage scoring;deep learning;single-channel EEG","Sleep;Electroencephalography;Feature extraction;Brain modeling;Training;Data mining;Convolution","electroencephalography;encoding;feature extraction;learning (artificial intelligence);medical signal processing;neural nets;signal sampling;sleep","DeepSleepNet;automatic sleep stage scoring;raw single-channel EEG;deep learning model;hand-engineered features;sleep analysis;temporal information encoding;transition rules;utilize convolutional neural networks;time-invariant feature extraction;bidirectional-longshort-term memory;EEG epochs;two-step training algorithm;public sleep data sets;sampling rate;scoring standards;macroF1-score;training algorithm","Adult;Algorithms;Automation;Electroencephalography;Female;Healthy Volunteers;Humans;Machine Learning;Male;Memory, Long-Term;Memory, Short-Term;Models, Neurological;Neural Networks (Computer);Polysomnography;Reproducibility of Results;Sleep Stages","53","33","","","","","IEEE","IEEE Journals"
"Trajectory Predictor by Using Recurrent Neural Networks in Visual Tracking","L. Wang; L. Zhang; Z. Yi","College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China","IEEE Transactions on Cybernetics","","2017","47","10","3172","3183","Motion models have been proved to be a crucial part in the visual tracking process. In recent trackers, particle filter and sliding windows-based motion models have been widely used. Treating motion models as a sequence prediction problem, we can estimate the motion of objects using their trajectories. Moreover, it is possible to transfer the learned knowledge from annotated trajectories to new objects. Inspired by recent advance in deep learning for visual feature extraction and sequence prediction, we propose a trajectory predictor to learn prior knowledge from annotated trajectories and transfer it to predict the motion of target objects. In this predictor, convolutional neural networks extract the visual features of target objects. Long short-term memory model leverages the annotated trajectory priors as well as sequential visual information, which includes the tracked features and center locations of the target object, to predict the motion. Furthermore, to extend this method to videos in which it is difficult to obtain annotated trajectories, a dynamic weighted motion model that combines the proposed trajectory predictor with a random sampler is proposed. To evaluate the transfer performance of the proposed trajectory predictor, we annotated a real-world vehicle dataset. Experiment results on both this real-world vehicle dataset and an online tracker benchmark dataset indicate that the proposed method outperforms several state-of-the-art trackers.","","","10.1109/TCYB.2017.2705345","Key Program of National Natural Science Foundation of China; Foundation for National Key Research and Development Plan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7935541","Convolutional neural networks (CNNs);deep learning;recurrent neural networks (RNNs);trajectory prediction;visual tracking","Target tracking;Trajectory;Feature extraction;Visualization;Predictive models;Videos","feature extraction;learning (artificial intelligence);particle filtering (numerical methods);recurrent neural nets;target tracking","trajectory predictor;recurrent neural networks;visual tracking;particle filter;sliding windows-based motion models;sequence prediction;deep learning;visual feature extraction;long short-term memory model;real-world vehicle dataset","Computer Simulation;Humans;Image Processing, Computer-Assisted;Machine Learning;Neural Networks (Computer);Video Recording","8","47","","","","","IEEE","IEEE Journals"
"Personalized Image Annotation Using Deep Architecture","F. Feng; R. Liu; X. Wang; X. Li; S. Bi","Beijing Key Laboratory of Network System and Network Culture, School of Digital Media and Design, Beijing University of Posts and Telecommunications, Beijing, China; Algorithms Research Group, Nice App Mobile Technology Co., Ltd., Beijing, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Network System and Network Culture, School of Digital Media and Design, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Fieldbus Technology and Automation, North China University of Technology, Beijing, China","IEEE Access","","2017","5","","23078","23085","In image-centric social networks, such as Instagram and Pinterest, users tend to share photos with several tags. These tags describe the content of the image or provide additional contextual information, and therefore may not be necessarily tied to image content and usually carry personal preference. Annotating images in social networks in a personalized manner is in demand. However, the existing image annotation models, which rely only on image content information, cannot capture the user's tagging preference. In this paper, we propose a deep architecture for personalized image annotation by leveraging the wealth of information in user's tagging history. The proposed architecture consists of three components: two components for learning features of the image content and user's history tags and the other one for combining the two learned features to predict the tags. We also explore two ways to model user's history tags: 1) simply average the embeddings of user's history tags and 2) model user's history tags with a sequence model by long short-term memory recurrent neural network. We evaluate our proposed deep architecture on a large-scale and realistic data set, consisting of ~22.8 million public images uploaded by ~4.69 million users. Experimental results show that our proposed deep architecture is effective on a personalized image annotation task.","","","10.1109/ACCESS.2017.2764510","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8076835","Deep learning;history tags;personalized image annotation;social network","History;Computer architecture;Tagging;Social network services;Metadata;Computational modeling","image retrieval;learning (artificial intelligence);neural net architecture;recurrent neural nets;social networking (online)","image content information;deep architecture;tags;short-term memory recurrent neural network;personalized image annotation task;image-centric social networks;personal preference;user tagging history;contextual information","","","31","","","","","IEEE","IEEE Journals"
"Cross-Domain Visual Matching via Generalized Similarity Measure and Feature Learning","L. Lin; G. Wang; W. Zuo; X. Feng; L. Zhang","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, P.R. China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, P.R. China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, P.R. China; School of Mathematics and Statistics, Xidian University, Xi’an, P.R. China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","6","1089","1102","Cross-domain visual data matching is one of the fundamental problems in many real-world vision tasks, e.g., matching persons across ID photos and surveillance videos. Conventional approaches to this problem usually involves two steps: i) projecting samples from different domains into a common space, and ii) computing (dis-)similarity in this space based on a certain distance. In this paper, we present a novel pairwise similarity measure that advances existing models by i) expanding traditional linear projections into affine transformations and ii) fusing affine Mahalanobis distance and Cosine similarity by a data-driven combination. Moreover, we unify our similarity measure with feature representation learning via deep convolutional neural networks. Specifically, we incorporate the similarity measure matrix into the deep architecture, enabling an end-to-end way of model optimization. We extensively evaluate our generalized similarity model in several challenging cross-domain matching tasks: person re-identification under different views and face verification over different modalities (i.e., faces from still images and videos, older and younger faces, and sketch and photo portraits). The experimental results demonstrate superior performance of our model over other state-of-the-art methods.","","","10.1109/TPAMI.2016.2567386","Hong Kong Scholar Program; Guangdong Natural Science Foundation; Guangzhou Zhujiang Star of Science and Technology; Fundamental Research Funds for the Central Universities; NSFC-Guangdong Joint Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7469374","Similarity model;cross-domain matching;person verification;deep learning","Face;Neural networks;Visualization;Pattern matching;Videos;Euclidean distance","computer vision;data handling;face recognition;image matching;image representation;learning (artificial intelligence);matrix algebra;neural nets","generalized similarity measurement;cross-domain visual data matching;real-world vision tasks;pairwise similarity measurement;Mahalanobis distance;cosine similarity;feature representation learning;deep convolutional neural networks;similarity measure matrix;person re-identification;face verification","","43","60","","","","","IEEE","IEEE Journals"
"Tiny hand gesture recognition without localization via a deep convolutional network","P. Bao; A. I. Maqueda; C. R. del-Blanco; N. García","Grupo de Tratamiento de Imágenes (GTI), Information Processing and Telecommunications Center (IPTC) , Universidad Politécnica de Madrid, Madrid, 28040, Spain; Grupo de Tratamiento de Imágenes (GTI), Information Processing and Telecommunications Center (IPTC), Universidad Politécnica de Madrid, Madrid, 28040, Spain; Grupo de Tratamiento de Imágenes (GTI), Information Processing and Telecommunications Center (IPTC), Universidad Politécnica de Madrid, Madrid, 28040, Spain; Grupo de Tratamiento de Imágenes (GTI), Information Processing and Telecommunications Center (IPTC), Universidad Politécnica de Madrid, Madrid, 28040, Spain","IEEE Transactions on Consumer Electronics","","2017","63","3","251","257","Visual hand-gesture recognition is being increasingly desired for human-computer interaction interfaces. In many applications, hands only occupy about 10% of the image, whereas the most of it contains background, human face, and human body. Spatial localization of the hands in such scenarios could be a challenging task and ground truth bounding boxes need to be provided for training, which is usually not accessible. However, the location of the hand is not a requirement when the criteria is just the recognition of a gesture to command a consumer electronics device, such as mobiles phones and TVs. In this paper, a deep convolutional neural network is proposed to directly classify hand gestures in images without any segmentation or detection stage that could discard the irrelevant not-hand areas. The designed hand-gesture recognition network can classify seven sorts of hand gestures in a user-independent manner and on real time, achieving an accuracy of 97.1% in the dataset with simple backgrounds and 85.3% in the dataset with complex backgrounds.","","","10.1109/TCE.2017.014971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8103373","Deep learning;hand gesture recognition;human-machine interface;mobile phone;neural network;no localization;TV","Gesture recognition;Proposals;Neural networks;Feature extraction;Image recognition;Training","feature extraction;gesture recognition;human computer interaction;image classification;image segmentation;learning (artificial intelligence);neural nets","tiny hand gesture recognition;deep convolutional network;visual hand-gesture recognition;human-computer interaction interfaces;human face;human body;spatial localization;consumer electronics device;mobiles phones;deep convolutional neural network;directly classify hand gestures;hand-gesture recognition network","","3","30","","","","","IEEE","IEEE Journals"
"Feature Learning With Matrix Factorization Applied to Acoustic Scene Classification","V. Bisot; R. Serizel; S. Essid; G. Richard","LTCI, Télécom ParisTech, Université Paris-Saclay, Paris, France; LTCI, Télécom ParisTech, Université Paris-Saclay, Paris, France; LTCI, Télécom ParisTech, Université Paris-Saclay, Paris, France; LTCI, Télécom ParisTech, Université Paris-Saclay, Paris, France","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","6","1216","1229","In this paper, we study the usefulness of various matrix factorization methods for learning features to be used for the specific acoustic scene classification (ASC) problem. A common way of addressing ASC has been to engineer features capable of capturing the specificities of acoustic environments. Instead, we show that better representations of the scenes can be automatically learned from time-frequency representations using matrix factorization techniques. We mainly focus on extensions including sparse, kernel-based, convolutive and a novel supervised dictionary learning variant of principal component analysis and nonnegative matrix factorization. An experimental evaluation is performed on two of the largest ASC datasets available in order to compare and discuss the usefulness of these methods for the task. We show that the unsupervised learning methods provide better representations of acoustic scenes than the best conventional hand-crafted features on both datasets. Furthermore, the introduction of a novel nonnegative supervised matrix factorization model and deep neural networks trained on spectrograms, allow us to reach further improvements.","","","10.1109/TASLP.2017.2690570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7933045","Acoustic scene classification;feature learning;matrix factorization","Dictionaries;Time-frequency analysis;Hidden Markov models;Mel frequency cepstral coefficient;Feature extraction;Sparse matrices","acoustic signal processing;matrix decomposition;neural nets;principal component analysis;time-frequency analysis;unsupervised learning","feature learning;specific acoustic scene classification problem;time-frequency representations;supervised dictionary learning variant;kernel-based variant;principal component analysis;PCA;ASC datasets;unsupervised learning methods;hand-crafted features;nonnegative supervised matrix factorization model;deep neural networks;spectrograms","","14","70","","","","","IEEE","IEEE Journals"
"Deep Aesthetic Quality Assessment With Semantic Information","Y. Kao; R. He; K. Huang","Chinese Academy of Sciences, Center for Research on Intelligent Perception and Computing and National Laboratory of Pattern Recognition of Institute of Automation, Beijing, China; Chinese Academy of Sciences, Center for Research on Intelligent Perception and Computing and National Laboratory of Pattern Recognition of Institute of Automation, Beijing, China; Chinese Academy of Sciences, Center for Research on Intelligent Perception and Computing and National Laboratory of Pattern Recognition of Institute of Automation, Beijing, China","IEEE Transactions on Image Processing","","2017","26","3","1482","1495","Human beings often assess the aesthetic quality of an image coupled with the identification of the image's semantic content. This paper addresses the correlation issue between automatic aesthetic quality assessment and semantic recognition. We cast the assessment problem as the main task among a multi-task deep model, and argue that semantic recognition task offers the key to address this problem. Based on convolutional neural networks, we employ a single and simple multi-task framework to efficiently utilize the supervision of aesthetic and semantic labels. A correlation item between these two tasks is further introduced to the framework by incorporating the inter-task relationship learning. This item not only provides some useful insight about the correlation but also improves assessment accuracy of the aesthetic task. In particular, an effective strategy is developed to keep a balance between the two tasks, which facilitates to optimize the parameters of the framework. Extensive experiments on the challenging Aesthetic Visual Analysis dataset and Photo.net dataset validate the importance of semantic recognition in aesthetic quality assessment, and demonstrate that multitask deep models can discover an effective aesthetic representation to achieve the state-of-the-art results.","","","10.1109/TIP.2017.2651399","National Natural Science Foundation of China; International Partnership Program of Chinese Academy of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814292","Visual aesthetic quality assessment;semantic information;multi-task learning","Semantics;Quality assessment;Correlation;Neural networks;Visualization;Linear programming;Computational modeling","data visualisation;image recognition;image representation;learning (artificial intelligence);neural nets","automatic deep aesthetic image quality assessment;semantic information;image semantic content identification;semantic recognition;multitask deep model;convolutional neural networks;inter-task relationship learning;aesthetic visual analysis dataset;Photo.net dataset;aesthetic representation","","35","64","","","","","IEEE","IEEE Journals"
"Deep-learning-based scenario generation strategy considering correlation between multiple wind farms","Y. Xu; L. Shi; Y. Ni","Graduate School at Shenzhen, Tsinghua University, People's Republic of China; Graduate School at Shenzhen, Tsinghua University, People's Republic of China; Graduate School at Shenzhen, Tsinghua University, People's Republic of China","The Journal of Engineering","","2017","2017","13","2207","2210","It is important to model the future scenarios of wind farm power output in enhancing capability of wind power accommodation and decreasing wind power curtailment. A scenario generation strategy considering the correlation between multiple wind farms is proposed. A convolutional neural network combined with quantile regression technique is introduced to achieve detailed quantiles of corresponding predicted wind power output, which can be regarded as the cumulative distribution function (CDF) by approximation. Marginal conditional probability density function (PDF) for each wind farm can be constructed from the CDF. A copula function-based method is used to form the joint PDF of multiple wind farm power outputs from the marginal PDF constructed before. By inversing the joint PDF, the required scenario set can be formed. In case studies, the proposed strategy is tested with two wind farms data, and the simulation results verify the effectiveness of the proposed strategy.","","","10.1049/joe.2017.0722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8311193","","","wind power plants;wind power;neural nets;regression analysis;probability;learning (artificial intelligence);statistical distributions","scenario generation strategy;multiple wind farms;future scenarios;wind farm power output;wind power accommodation;decreasing wind power curtailment;quantile regression technique;wind power output;marginal conditional probability density function;joint PDF;multiple wind farm power;required scenario set;wind farms data","","","13","","","","","IET","IET Journals"
"Representational learning approach for power system transient stability assessment based on convolutional neural network","B. Tan; J. Yang; X. Pan; J. Li; P. Xie; C. Zeng","Wuhan University, Wuhan, Hubei, People's Republic of China; Wuhan University, Wuhan, Hubei, People's Republic of China; WuHan Power Supply Company, Wuhan, Hubei, People's Republic of China; HuNan Electric Power Corporation, Changsha, Hunan, People's Republic of China; HuNan Electric Power Corporation, Changsha, Hunan, People's Republic of China; HuNan Electric Power Corporation, Changsha, Hunan, People's Republic of China","The Journal of Engineering","","2017","2017","13","1847","1850","The transient stability assessment (TSA) problem can be mapped into a two-class classification problem in machine learning, which estimates the dynamic security boundary of the power system by learning from large amount samples. A representational learning approach is proposed to solve the problem based on big data collected from Phasor Measurement Units (PMUs), which includes four stages: (i) Construct original input features by using PMUs data to describe the dynamic characteristics of the power system. (ii) Unsupervised representational feature learning by using the original features. Stacked autoencoders (SAEs) perform representational learning for crucial features. (iii) Supervised classifier training. A powerful deep learning model, convolutional neural network, which is added to SAE, is trained and tested with the learned representation. (iv) Online application, the trained model is applied to the online evaluation for TSA. Simulation on the New England 39-bus test system shows that the proposed approach has high accuracy, rare misclassification of the unstable sample and excellent robustness with noise in PMUs for TSA.","","","10.1049/joe.2017.0651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8311182","","","pattern classification;neural nets;power system security;power system transient stability;power engineering computing;unsupervised learning;Big Data;phasor measurement","TSA;New England 39-bus test system;representational learning approach;power system transient stability assessment;convolutional neural network;transient stability assessment problem;two-class classification problem;machine learning;Phasor Measurement Units;PMUs data;dynamic characteristics;learned representation;Big Data;deep learning model;dynamic security boundary estimation;Unsupervised representational feature learning;stacked autoencoders;SAEs;supervised classifier training","","5","12","","","","","IET","IET Journals"
"Hyperspectral Images Classification With Gabor Filtering and Convolutional Neural Network","Y. Chen; L. Zhu; P. Ghamisi; X. Jia; G. Li; L. Tang","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; German Aerospace Center (DLR), Remote Sensing Technology Institute (IMF), Wessling, Germany; School of Engineering and Information Technology, The University of New South Wales, Canberra, NSW, Australia; State Key Laboratory of Frozen Soil Engineering, Cold and Arid Regions Environmental and Engineering Research Institute, Chinese Academy of Sciences, Lanzhou, China; School of Civil Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","12","2355","2359","Recently, the capability of deep learning-based approaches, especially deep convolutional neural networks (CNNs), has been investigated for hyperspectral remote sensing feature extraction (FE) and classification. Due to the large number of learnable parameters in convolutional filters, lots of training samples are needed in deep CNNs to avoid the overfitting problem. On the other hand, Gabor filtering can effectively extract spatial information including edges and textures, which may reduce the FE burden of the CNNs. In this letter, in order to make the most of deep CNN and Gabor filtering, a new strategy, which combines Gabor filters with convolutional filters, is proposed for hyperspectral image classification to mitigate the problem of overfitting. The obtained results reveal that the proposed model provides competitive results in terms of classification accuracy, especially when only a limited number of training samples are available.","","","10.1109/LGRS.2017.2764915","National Natural Science Foundation of China; Open Fund of State Key Laboratory of Frozen Soil Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8100719","Convolutional neural network (CNN);deep learning;feature extraction (FE);Gabor filtering;hyperspectral images (HSIs)","Feature extraction;Training;Principal component analysis;Hyperspectral imaging;Iron","convolution;edge detection;feature extraction;filtering theory;Gabor filters;hyperspectral imaging;image classification;image texture;learning (artificial intelligence);neural nets;remote sensing","deep CNN;Gabor filters;convolutional filters;hyperspectral image classification;hyperspectral images classification;deep learning-based approaches;deep convolutional neural networks;textures;Gabor filtering;spatial information extraction;edges;hyperspectral remote sensing feature extraction","","19","17","","","","","IEEE","IEEE Journals"
"HeartID: A Multiresolution Convolutional Neural Network for ECG-Based Biometric Human Identification in Smart Health Applications","Q. Zhang; D. Zhou; X. Zeng","Department of Electrical Engineering, The University of Texas at Dallas, Richardson, TX, USA; Department of Electrical Engineering, The University of Texas at Dallas, Richardson, TX, USA; Fudan University, Shanghai, China","IEEE Access","","2017","5","","11805","11816","Body area networks, including smart sensors, are widely reshaping health applications in the new era of smart cities. To meet increasing security and privacy requirements, physiological signalbased biometric human identification is gaining tremendous attention. This paper focuses on two major impediments: the signal processing technique is usually both complicated and data-dependent and the feature engineering is time-consuming and can fit only specific datasets . To enable a data-independent and highly generalizable signal processing and feature learning process, a novel wavelet domain multiresolution convolutional neural network is proposed. Specifically, it allows for blindly selecting a physiological signal segment for identification purpose, avoiding the complicated signal fiducial characteristics extraction process. To enrich the data representation, the random chosen signal segment is then transformed to the wavelet domain, where multiresolution time-frequency representation is achieved. An auto-correlation operation is applied to the transformed data to remove the phase difference as the result of the blind segmentation operation. Afterward, a multiresolution 1-D-convolutional neural network (1-D-CNN) is introduced to automatically learn the intrinsic hierarchical features from the wavelet domain raw data without datadependent and heavy feature engineering, and perform the user identification task. The effectiveness of the proposed algorithm is thoroughly evaluated on eight electrocardiogram datasets with diverse behaviors, such as with or without severe heart diseases, and with different sensor placement methods. Our evaluation is much more extensive than the state-of-the-art works, and an average identification rate of 93.5% is achieved. The proposed multiresolution 1-D-CNN algorithm can effectively identify human subjects, even from randomly selected signal segments and without heavy feature engineering. This paper is expected to demonstrate the feasibility and effectiveness of applying the blind signal processing and deep learning techniques to biometric human identification, to enable a low algorithm engineering effort and also a high generalization ability.","","","10.1109/ACCESS.2017.2707460","Recruitment Program of Global Experts (the Thousand Talents Plan); National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7933065","ECG;wavelet transformation;convolutional neural network;deep learning;machine learning;feature learning;blind signal processing;data representation","Electrocardiography;Signal resolution;Feature extraction;Heart rate variability;Convolution;Wavelet domain;Wavelet transforms","blind source separation;electrocardiography;learning (artificial intelligence);medical signal processing;neural nets","HeartID;multiresolution convolutional neural network;ECG-based biometric human identification;smart health applications;signal processing technique;blind signal processing;deep learning techniques;biometric human identification;body sensor networks","","31","45","","","","","IEEE","IEEE Journals"
"Aggregating Deep Convolutional Feature Maps for Insulator Detection in Infrared Images","Z. Zhao; X. Fan; G. Xu; L. Zhang; Y. Qi; K. Zhang","School of Electrical and Electronic Engineering, North China Electric Power University, Baoding, China; School of Electrical and Electronic Engineering, North China Electric Power University, Baoding, China; NetEase, Hangzhou, China; School of Electrical and Electronic Engineering, North China Electric Power University, Baoding, China; School of Electrical and Electronic Engineering, North China Electric Power University, Baoding, China; School of Electrical and Electronic Engineering, North China Electric Power University, Baoding, China","IEEE Access","","2017","5","","21831","21839","Insulator detection using an infrared image is challenged by variance of temperature, orientations, and a cluttered background. A robust and discriminative representation of insulators in electric power systems is needed. This paper proposes a novel method for generating this type of representation in infrared images by taking advantage of high-level discriminative Convolutional Neural Networks (CNNs) to feature the extraction framework and the deformation invariant nature of the Vector of Locally Aggregated Descriptors (VLAD) aggregator. Different from existing methods, we delve deep into the convolutional feature maps. We first extract deep activation maps from convolutional layers of a pretrained deep model and replace the last three fully-connected layers with a VLAD pooling layer to generate the representation of an insulator. Then, we train a Support Vector Machine (SVM) for binary classification. To further verify the effectiveness and robustness of our proposed feature, an insulator detection pipeline based on an object proposal is introduced. The experimental results show that our proposed method can achieve an accuracy of 93%. Meanwhile, the detection results demonstrate that our insulator detection pipeline has satisfied performance goals.","","","10.1109/ACCESS.2017.2757030","National Natural Science Foundation of China; Natural Science Foundation of Hebei Province; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8052216","Convolutional feature map;Deep Convolutional Neural Network;feature extraction;infrared image;insulators;object detection","Insulators;Feature extraction;Convolutional codes;Visualization;Electronic mail;Robustness;Neural networks","feature extraction;feedforward neural nets;image classification;image representation;infrared imaging;learning (artificial intelligence);object detection;power engineering computing;power overhead lines;support vector machines;video signal processing","convolutional layers;pretrained deep model;VLAD;insulator detection pipeline;infrared image;cluttered background;robust representation;discriminative representation;electric power systems;extraction framework;deformation invariant nature;deep activation maps;support vector machine;deep convolutional feature maps;high-level discriminative convolutional neural networks;locally aggregated descriptor aggregator;binary classification","","7","37","","","","","IEEE","IEEE Journals"
"Automatic Detection and Classification of Colorectal Polyps by Transferring Low-Level CNN Features From Nonmedical Domain","R. Zhang; Y. Zheng; T. W. C. Mak; R. Yu; S. H. Wong; J. Y. W. Lau; C. C. Y. Poon","Department of Surgery, The Chinese University of Hong Kong, Shatin, Hong Kong SAR; Department of Surgery, The Chinese University of Hong Kong, Shatin, Hong Kong SAR; Department of Surgery, The Chinese University of Hong Kong, Shatin, Hong Kong SAR; Department of Surgery, The Chinese University of Hong Kong, Shatin, Hong Kong SAR; Department of Medicine & Therapeutics, State Key Laboratory of Digestive Diseases, Li Ka Shing Institute of Health Sciences, Institute of Digestive Diseases, The Chinese University of Hong Kong, Shatin, Hong Kong SAR; Department of Surgery, The Chinese University of Hong Kong, Shatin, Hong Kong SAR; Department of Surgery, The Chinese University of Hong Kong, Shatin, Hong Kong SAR","IEEE Journal of Biomedical and Health Informatics","","2017","21","1","41","47","Colorectal cancer (CRC) is a leading cause of cancer deaths worldwide. Although polypectomy at early stage reduces CRC incidence, 90% of the polyps are small and diminutive, where removal of them poses risks to patients that may outweigh the benefits. Correctly detecting and predicting polyp type during colonoscopy allows endoscopists to resect and discard the tissue without submitting it for histology, saving time, and costs. Nevertheless, human visual observation of early stage polyps varies. Therefore, this paper aims at developing a fully automatic algorithm to detect and classify hyperplastic and adenomatous colorectal polyps. Adenomatous polyps should be removed, whereas distal diminutive hyperplastic polyps are considered clinically insignificant and may be left in situ . A novel transfer learning application is proposed utilizing features learned from big nonmedical datasets with 1.4-2.5 million images using deep convolutional neural network. The endoscopic images we collected for experiment were taken under random lighting conditions, zooming and optical magnification, including 1104 endoscopic nonpolyp images taken under both white-light and narrowband imaging (NBI) endoscopy and 826 NBI endoscopic polyp images, of which 263 images were hyperplasia and 563 were adenoma as confirmed by histology. The proposed method identified polyp images from nonpolyp images in the beginning followed by predicting the polyp histology. When compared with visual inspection by endoscopists, the results of this study show that the proposed method has similar precision (87.3% versus 86.4%) but a higher recall rate (87.6% versus 77.0%) and a higher accuracy (85.9% versus 74.3%). In conclusion, automatic algorithms can assist endoscopists in identifying polyps that are adenomatous but have been incorrectly judged as hyperplasia and, therefore, enable timely resection of these polyps at an early stage before they develop into invasive cancer.","","","10.1109/JBHI.2016.2635662","Hong Kong Innovation and Technology Fund, Shaw Endoscopy Center; Chow Yuk Ho Technology Centre for Innovative Medicine; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7769237","Colorectal cancer;deep learning;health informatics;polyp diagnosis","Cancer;Feature extraction;Neurons;Biomedical imaging;Training;Machine learning;Colonoscopy","biological tissues;biomedical optical imaging;cancer;endoscopes","colorectal cancer;cancer deaths;polypectomy;colonoscopy;tissue;human visual observation;fully automatic algorithm;hyperplastic colorectal polyps;adenomatous colorectal polyps;learning application;nonmedical datasets;deep convolutional neural network;endoscopic images;optical magnification;endoscopic nonpolyp images;narrowband imaging endoscopy;white-light endoscopy;NBI endoscopic polyp images;hyperplasia;low-level CNN features;colorectal polyps classification;colorectal polyps automatic detection;polyp histology","Colonic Polyps;Colonoscopy;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Neural Networks (Computer);ROC Curve","54","31","","","","","IEEE","IEEE Journals"
"Multiobjective Deep Belief Networks Ensemble for Remaining Useful Life Estimation in Prognostics","C. Zhang; P. Lim; A. K. Qin; K. C. Tan","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Advance Technology Center of Rolls Royce Singapore, Singapore; School of Science, RMIT University, Melbourne, VIC, Australia; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","10","2306","2318","In numerous industrial applications where safety, efficiency, and reliability are among primary concerns, condition-based maintenance (CBM) is often the most effective and reliable maintenance policy. Prognostics, as one of the key enablers of CBM, involves the core task of estimating the remaining useful life (RUL) of the system. Neural networks-based approaches have produced promising results on RUL estimation, although their performances are influenced by handcrafted features and manually specified parameters. In this paper, we propose a multiobjective deep belief networks ensemble (MODBNE) method. MODBNE employs a multiobjective evolutionary algorithm integrated with the traditional DBN training technique to evolve multiple DBNs simultaneously subject to accuracy and diversity as two conflicting objectives. The eventually evolved DBNs are combined to establish an ensemble model used for RUL estimation, where combination weights are optimized via a single-objective differential evolution algorithm using a task-oriented objective function. We evaluate the proposed method on several prognostic benchmarking data sets and also compare it with some existing approaches. Experimental results demonstrate the superiority of our proposed method.","","","10.1109/TNNLS.2016.2582798","Ministry of Education, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508982","Deep belief network (DBN);ensemble learning;evolutionary algorithm (EA);multiobjective;prognostics","Estimation;Degradation;Maintenance engineering;Training;Artificial neural networks;Reliability;Benchmark testing","belief networks;condition monitoring;evolutionary computation;maintenance engineering;mechanical engineering computing;neural nets","multiobjective deep belief networks ensemble method;remaining useful life estimation;prognostics;condition-based maintenance;CBM;RUL estimation;neural networks-based approaches;maintenance policy;MODBNE method;evolutionary algorithm;DBN training technique;combination weights;single-objective differential evolution algorithm;task-oriented objective function;prognostic benchmarking data sets","","46","83","","","","","IEEE","IEEE Journals"
"Recent progresses in deep learning based acoustic models","D. Yu; J. Li","Tencent AI Lab, Tencent, Bellevue, WA 98034, USA; Microsoft AI and Research, Microsoft, Redmmond, WA 98052, USA","IEEE/CAA Journal of Automatica Sinica","","2017","4","3","396","409","In this paper, we summarize recent progresses made in deep learning based acoustic models and the motivation and insights behind the surveyed techniques. We first discuss models such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) that can effectively exploit variable-length contextual information, and their various combination with other models. We then describe models that are optimized end-to-end and emphasize on feature representations learned jointly with the rest of the system, the connectionist temporal classification (CTC) criterion, and the attention-based sequence-to-sequence translation model. We further illustrate robustness issues in speech recognition systems, and discuss acoustic model adaptation, speech enhancement and separation, and robust training strategies. We also cover modeling techniques that lead to more efficient decoding and discuss possible future directions in acoustic model research.","","","10.1109/JAS.2017.7510508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7974889","","Hidden Markov models;Acoustics;Convolution;Adaptation models;Context modeling;Speech recognition;Time-frequency analysis","acoustic signal processing;decoding;feature extraction;learning (artificial intelligence);optimisation;signal classification;signal representation;speech enhancement;speech recognition","deep learning based acoustic models;variable-length contextual information;end-to-end optimization;feature representations;connectionist temporal classification criterion;CTC criterion;attention-based sequence-to-sequence translation model;robustness issues;speech recognition systems;acoustic model adaptation;speech enhancement;speech separation;robust training strategies;modeling techniques;decoding","","32","","","","","","IEEE","IEEE Journals"
"Self-Supervised Visual Descriptor Learning for Dense Correspondence","T. Schmidt; R. Newcombe; D. Fox","Department of Computer Science and Engineering, University of Washington, Seattle, WA, USA; Oculus Research, Redmond, WA, USA; Department of Computer Science and Engineering, University of Washington, Seattle, WA, USA","IEEE Robotics and Automation Letters","","2017","2","2","420","427","Robust estimation of correspondences between image pixels is an important problem in robotics, with applications in tracking, mapping, and recognition of objects, environments, and other agents. Correspondence estimation has long been the domain of hand-engineered features, but more recently deep learning techniques have provided powerful tools for learning features from raw data. The drawback of the latter approach is that a vast amount of (labeled, typically) training data are required for learning. This paper advocates a new approach to learning visual descriptors for dense correspondence estimation in which we harness the power of a strong three-dimensional generative model to automatically label correspondences in RGB-D video data. A fully convolutional network is trained using a contrastive loss to produce viewpoint- and lighting-invariant descriptors. As a proof of concept, we collected two datasets: The first depicts the upper torso and head of the same person in widely varied settings, and the second depicts an office as seen on multiple days with objects rearranged within. Our datasets focus on revisitation of the same objects and environments, and we show that by training the CNN only from local tracking data, our learned visual descriptor generalizes toward identifying nonlabeled correspondences across videos. We furthermore show that our approach to descriptor learning can be used to achieve state-of-the-art single-frame localization results on the MSR 7-scenes dataset without using any labels identifying correspondences between separate videos of the same scenes at training time.","","","10.1109/LRA.2016.2634089","Intel Science and Technology Center for Pervasive Computing; ONR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762851","Recognition;RGB-D perception;visual learning","Visualization;Training data;Labeling;Simultaneous localization and mapping;Computational modeling;Training","convolution;estimation theory;learning (artificial intelligence);neural nets;robot vision;SLAM (robots)","visual descriptor learning;correspondence estimation;image pixel;robotics;deep learning;fully convolutional network training;CNN training;simultaneous localization and mapping;SLAM","","22","32","","","","","IEEE","IEEE Journals"
"Stacked Multilevel-Denoising Autoencoders: A New Representation Learning Approach for Wind Turbine Gearbox Fault Diagnosis","G. Jiang; H. He; P. Xie; Y. Tang","School of Electrical Engineering, Yanshan University, Qihuangdao, China; Department of Electrical, Computer and Biomedical Engineering, The University of Rhode Island, Kingston, RI, USA; School of Electrical Engineering, Yanshan University, Qihuangdao, China; Department of Computer and Electrical Engineering and Computer Science, Florida Atlantic University, Boca Raton, FL, USA","IEEE Transactions on Instrumentation and Measurement","","2017","66","9","2391","2402","Currently, vibration analysis has been widely considered as an effective way to fulfill the fault diagnosis task of gearboxes in wind turbines (WTs). However, vibration signals are usually with abundant noise and characterized as nonlinearity and nonstationarity. Therefore, it is quite challenging to extract robust and useful fault features from complex vibration signals to achieve an accurate and reliable diagnosis. This paper proposes a novel feature representation learning approach, named stacked multilevel-denoising autoencoders (SMLDAEs), with the aim to learn robust and discriminative fault feature representations through a deep network architecture for diagnosis accuracy improvement. In our proposed approach, we design an MLD training scheme, which uses multiple noise levels to train AEs. It enables to learn more general and detailed fault feature patterns simultaneously at different scales from the complex frequency spectra of the raw vibration data, and therefore helps enhance the feature learning and fault diagnosis capability. Furthermore, SMLDAE-based fault diagnosis is performed with an unsupervised representation learning procedure followed by a supervised fine-tuning process with label information for classification. Our approach is evaluated by using the field vibration data collected from a self-designed WT gearbox test rig. The results show that our proposed approach learned more robust and discriminative fault feature representations and achieved the best diagnosis accuracy compared with the traditional approaches.","","","10.1109/TIM.2017.2698738","National Natural Science Foundation of China; Natural Science Foundation of Hebei Province; Scientific Research Project of the Higher Education Institutions of Hebei Province; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932980","Fault diagnosis;multilevel-denoising (MLD) training;stacked denoising autoencoders (SDAEs);vibration representation learning;wind turbine (WT) gearbox","Feature extraction;Fault diagnosis;Vibrations;Training;Robustness;Noise reduction;Wind turbines","condition monitoring;encoding;fault diagnosis;feature extraction;gears;learning (artificial intelligence);neural nets;power engineering computing;signal classification;signal denoising;vibrational signal processing;wind turbines","wind turbine gearbox fault diagnosis;vibration signals;signal classification;supervised fine tuning process;deep network architecture;discriminative fault feature representations learning;stacked multilevel denoising autoencoder;novel feature representation learning method","","27","41","","","","","IEEE","IEEE Journals"
"MARTA GANs: Unsupervised Representation Learning for Remote Sensing Image Classification","D. Lin; K. Fu; Y. Wang; G. Xu; X. Sun","Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","11","2092","2096","With the development of deep learning, supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks. However, due to the limited amount of labeled data available, supervised learning is often difficult to carry out. Therefore, we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) to learn a representation using only unlabeled data. MARTA GANs consists of both a generative model G and a discriminative model D. We treat D as a feature extractor. To fit the complex properties of remote sensing data, we use a fusion layer to merge the mid-level and global features. G can produce numerous images that are similar to the training data; therefore, D can learn better representations of remotely sensed images using the training data provided by G. The classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-the-art methods.","","","10.1109/LGRS.2017.2752750","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8059820","Generative adversarial networks (GANs);scene classification;unsupervised representation learning","Generators;Gallium nitride;Training;Remote sensing;Feature extraction;Training data;Computational modeling","feature extraction;geophysical image processing;image classification;image fusion;image matching;image representation;remote sensing;unsupervised learning","global features;training data;unsupervised representation;remote sensing image classification;deep learning;supervised learning;convolutional networks;unsupervised model;multiple-layer feature-matching generative adversarial networks;unlabeled data;generative model G;discriminative model D;feature extractor;remote sensing data;remote sensing image databases;MARTA GAN;fusion layer","","21","15","Traditional","","","","IEEE","IEEE Journals"
"Forest Change Detection in Incomplete Satellite Images With Deep Neural Networks","S. H. Khan; X. He; F. Porikli; M. Bennamoun","Data61-CSIRO, Canberra, ACT, Australia; Data61-CSIRO, Canberra, ACT, Australia; The Australian National University, Canberra, ACT, Australia; The University of Western Australia, Crawley, WA, Australia","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","9","5407","5423","Land cover change monitoring is an important task from the perspective of regional resource monitoring, disaster management, land development, and environmental planning. In this paper, we analyze imagery data from remote sensing satellites to detect forest cover changes over a period of 29 years (1987-2015). Since the original data are severely incomplete and contaminated with artifacts, we first devise a spatiotemporal inpainting mechanism to recover the missing surface reflectance information. The spatial filling process makes use of the available data of the nearby temporal instances followed by a sparse encoding-based reconstruction. We formulate the change detection task as a region classification problem. We build a multiresolution profile (MRP) of the target area and generate a candidate set of bounding-box proposals that enclose potential change regions. In contrast to existing methods that use handcrafted features, we automatically learn region representations using a deep neural network in a data-driven fashion. Based on these highly discriminative representations, we determine forest changes and predict their onset and offset timings by labeling the candidate set of proposals. Our approach achieves the state-of-the-art average patch classification rate of 91.6% (an improvement of ~16%) and the mean onset/offset prediction error of 4.9 months (an error reduction of five months) compared with a strong baseline. We also qualitatively analyze the detected changes in the unlabeled image regions, which demonstrate that the proposed forest change detection approach is scalable to new regions.","","","10.1109/TGRS.2017.2707528","Data61/CSIRO, UWA through IPRS; Australian Research Council’s Discovery Projects funding scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7948741","Change detection;deep learning;image inpainting;multitemporal spectral data;remote sensing","Satellites;Remote sensing;Australia;Clouds;Monitoring;Spatial resolution;Earth","geomorphology;geophysical image processing;image classification;land cover;vegetation mapping","forest change detection;incomplete satellite images;deep neural networks;land cover change monitoring;regional resource monitoring;disaster management;environmental planning;AD 1987 to 2015;spatiotemporal inpainting mechanism;surface reflectance information;sparse encoding-based reconstruction;multiresolution profile;target area;bounding-box proposals;automatically learn region;deep neural network;data-driven fashion;state-of-the-art average patch classification rate","","7","69","","","","","IEEE","IEEE Journals"
"Detection of Interictal Discharges With Convolutional Neural Networks Using Discrete Ordered Multichannel Intracranial EEG","A. Antoniades; L. Spyrou; D. Martin-Lopez; A. Valentin; G. Alarcon; S. Sanei; C. Cheong Took","Department of Computer Science, University of Surrey, Guildford, U.K.; School of Engineering, The University of Edinburgh, Edinburgh, U.K.; Kingston Hospital NHS FT, London, U.K.; King’s College Hospital, London, U.K.; King’s College London, London, U.K.; Department of Computer Science, University of Surrey, Guildford, U.K.; Department of Computer Science, University of Surrey, Guildford, U.K.","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","2017","25","12","2285","2294","Detection algorithms for electroencephalography (EEG) data, especially in the field of interictal epileptiform discharge (IED) detection, have traditionally employed handcrafted features, which utilized specific characteristics of neural responses. Although these algorithms achieve high accuracy, mere detection of an IED holds little clinical significance. In this paper, we consider deep learning for epileptic subjects to accommodate automatic feature generation from intracranial EEG data, while also providing clinical insight. Convolutional neural networks are trained in a subject independent fashion to demonstrate how meaningful features are automatically learned in a hierarchical process. We illustrate how the convolved filters in the deepest layers provide insight toward the different types of IEDs within the group, as confirmed by our expert clinicians. The morphology of the IEDs found in filters can help evaluate the treatment of a patient. To improve the learning of the deep model, moderately different score classes are utilized as opposed to binary IED and non-IED labels. The resulting model achieves state-of-the-art classification performance and is also invariant to time differences between the IEDs. This paper suggests that deep learning is suitable for automatic feature generation from intracranial EEG data, while also providing insight into the data.","","","10.1109/TNSRE.2017.2755770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8048515","Convolutional neural networks;epilepsy detection;intracranial EEG;multi score class learning","Electroencephalography;Electrodes;Convolution;Brain modeling;Biological neural networks;Machine learning;Feature extraction","electroencephalography;learning (artificial intelligence);medical signal detection;neural nets","state-of-the-art classification;deep model;deepest layers;convolved filters;hierarchical process;convolutional neural network training;intracranial EEG data;automatic feature generation;clinical significance;neural responses;interictal epileptiform discharge;electroencephalography;discrete ordered multichannel intracranial EEG;convolutional neural networks;interictal discharges","Adolescent;Adult;Algorithms;Electrocorticography;Electrodes, Implanted;Epilepsy, Temporal Lobe;Female;Humans;Machine Learning;Male;Neural Networks (Computer);Nonlinear Dynamics;Reproducibility of Results;Seizures;Telemetry;Young Adult","6","53","","","","","IEEE","IEEE Journals"
"Deep Supervised and Contractive Neural Network for SAR Image Classification","J. Geng; H. Wang; J. Fan; X. Ma","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; Department of Ocean Remote Sensing, National Marine Environmental Monitoring Center, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","4","2442","2459","The classification of a synthetic aperture radar (SAR) image is a significant yet challenging task, due to the presence of speckle noises and the absence of effective feature representation. Inspired by deep learning technology, a novel deep supervised and contractive neural network (DSCNN) for SAR image classification is proposed to overcome these problems. In order to extract spatial features, a multiscale patch-based feature extraction model that consists of gray level-gradient co-occurrence matrix, Gabor, and histogram of oriented gradient descriptors is developed to obtain primitive features from the SAR image. Then, to get discriminative representation of initial features, the DSCNN network that comprises four layers of supervised and contractive autoencoders is proposed to optimize features for classification. The supervised penalty of the DSCNN can capture the relevant information between features and labels, and the contractive restriction aims to enhance the locally invariant and robustness of the encoding representation. Consequently, the DSCNN is able to produce effective representation of sample features and provide superb predictions of the class labels. Moreover, to restrain the influence of speckle noises, a graph-cut-based spatial regularization is adopted after classification to suppress misclassified pixels and smooth the results. Experiments on three SAR data sets demonstrate that the proposed method is able to yield superior classification performance compared with some related approaches.","","","10.1109/TGRS.2016.2645226","National Nature Science Foundation of China; High Resolution Special Research; National’s Key Project of Research and Development Plan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827114","Contractive autoencoder (AE);deep neural network (DNN);supervised classification;synthetic aperture radar (SAR) image","Synthetic aperture radar;Feature extraction;Transforms;Speckle;Robustness;Decoding;Machine learning","feature extraction;geophysical image processing;gradient methods;image classification;learning (artificial intelligence);matrix algebra;neural nets;radar imaging;remote sensing by radar;synthetic aperture radar","novel deep supervised and contractive neural network;SAR image classification;synthetic aperture radar image;DSCNN;spatial feature extraction;multiscale patch-based feature extraction model;gray level-gradient co-occurrence matrix;histogram of oriented gradient descriptors;Gabor model;graph-cut-based spatial regularization","","41","57","","","","","IEEE","IEEE Journals"
"A Comprehensive Big-Data-Based Monitoring System for Yield Enhancement in Semiconductor Manufacturing","K. Nakata; R. Orihara; Y. Mizuoka; K. Takagi","Knowledge Media Laboratory, Corporate Research and Development Center, Toshiba Corporation, Kawasaki, Japan; Knowledge Media Laboratory, Corporate Research and Development Center, Toshiba Corporation, Kawasaki, Japan; Knowledge Media Laboratory, Corporate Research and Development Center, Toshiba Corporation, Kawasaki, Japan; Knowledge Media Laboratory, Corporate Research and Development Center, Toshiba Corporation, Kawasaki, Japan","IEEE Transactions on Semiconductor Manufacturing","","2017","30","4","339","344","In this paper, we focus on yield analysis task where engineers identify the cause of failure from wafer failure map patterns and manufacturing histories. We organize yield analysis task into the following three stages, namely, failure map pattern monitoring, failure cause identification, and failure recurrence monitoring, and incorporate machine learning and data mining technologies into each stage to support engineers' work. The important point is that big data analysis enables comprehensive and long-term monitoring automation. We make use of fast and scalable methods of clustering and pattern mining and realize daily comprehensive monitoring with massive manufacturing data. We also apply deep learning, which has been an innovative core technology of machine learning in recent years, to classification of wafer failure map patterns, and explore its performance in detail. Finally, these machine learning and data mining techniques are integrated into an automated monitoring system with interfaces familiar to engineers to attain large yield enhancement.","","","10.1109/TSM.2017.2753251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8039264","Data mining;pattern recognition;machine learning;deep learning;semiconductor defects","Monitoring;Semiconductor device manufacture;Pattern recognition;Machine learning;Neural networks;Semiconductor device modeling;Data mining","Big Data;data mining;learning (artificial intelligence);production engineering computing;semiconductor device manufacture;semiconductor industry","daily comprehensive monitoring;clustering pattern mining;long-term monitoring automation;big data analysis;data mining technologies;incorporate machine learning;failure recurrence monitoring;failure cause identification;failure map pattern monitoring;manufacturing histories;semiconductor manufacturing;comprehensive big-data;automated monitoring system;data mining techniques;wafer failure map patterns;massive manufacturing data","","4","20","OAPA","","","","IEEE","IEEE Journals"
"Sensor-Based Gait Parameter Extraction With Deep Convolutional Neural Networks","J. Hannink; T. Kautz; C. F. Pasluosta; K. Gaßmann; J. Klucken; B. M. Eskofier","Digital Sports Group, Pattern Recognition Lab, Department of Computer Science, University of Erlangen-Nürnberg (FAU), Erlangen, Germany; Digital Sports Group, Pattern Recognition Lab, Department of Computer Science, University of Erlangen-Nürnberg (FAU), Erlangen, Germany; Digital Sports Group, Pattern Recognition Lab, Department of Computer Science, University of Erlangen-Nürnberg (FAU), Erlangen, Germany; Geriatrics Center Erlangen, Waldkrankenhaus St. Marien, Erlangen, Germany; Department of Molecular Neurology, University Hospital Erlangen, University of Erlangen-Nürnberg (FAU), Erlangen, Germany; Digital Sports Group, Pattern Recognition Lab, Department of Computer Science, University of Erlangen-Nürnberg (FAU), Erlangen, Germany","IEEE Journal of Biomedical and Health Informatics","","2017","21","1","85","93","Measurement of stride-related, biomechanical parameters is the common rationale for objective gait impairment scoring. State-of-the-art double-integration approaches to extract these parameters from inertial sensor data are, however, limited in their clinical applicability due to the underlying assumptions. To overcome this, we present a method to translate the abstract information provided by wearable sensors to context-related expert features based on deep convolutional neural networks. Regarding mobile gait analysis, this enables integration-free and data-driven extraction of a set of eight spatio-temporal stride parameters. To this end, two modeling approaches are compared: a combined network estimating all parameters of interest and an ensemble approach that spawns less complex networks for each parameter individually. The ensemble approach is outperforming the combined modeling in the current application. On a clinically relevant and publicly available benchmark dataset, we estimate stride length, width and medio-lateral change in foot angle up to -0.15 ± 6.09 cm, -0.09 ± 4.22 cm and 0.13 ± 3.78° respectively. Stride, swing and stance time as well as heel and toe contact times are estimated up to ±0.07, ±0.05, ±0.07, ±0.07 and ±0.12 s respectively. This is comparable to and in parts outperforming or defining state of the art. Our results further indicate that the proposed change in the methodology could substitute assumption-driven double-integration methods and enable mobile assessment of spatio-temporal stride parameters in clinically critical situations as, e.g., in the case of spastic gait impairments.","","","10.1109/JBHI.2016.2636456","FAU Emerging Fields Initiative (EFIMoves); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7778173","Convolutional neural networks (CNNs);deep learning;mobile gait analysis;regression;spatio-temporal gait parameters","Mobile communication;Neural networks;Feature extraction;Foot;Biomedical measurement;Biomechanics;Wearable sensors","biomedical equipment;gait analysis;neural nets;spatiotemporal phenomena","sensor-based gait parameter extraction;deep convolutional neural networks;biomechanical parameters;gait impairment;inertial sensor data;wearable sensors;mobile gait analysis;integration-free extraction;data-driven extraction;spatio-temporal stride parameters;spastic gait impairments;mobile assessment","Accelerometry;Foot;Gait;Humans;Machine Learning;Neural Networks (Computer);Regression Analysis;Signal Processing, Computer-Assisted;Walking","37","34","","","","","IEEE","IEEE Journals"
"DeepPap: Deep Convolutional Networks for Cervical Cell Classification","L. Zhang; Le Lu; I. Nogues; R. M. Summers; S. Liu; J. Yao","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory and the Clinical Image Processing Service, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory and the Clinical Image Processing Service, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory and the Clinical Image Processing Service, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Department of Pathology, Peoples Hospital of Nanshan District, Shenzhen, China; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory and the Clinical Image Processing Service, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA","IEEE Journal of Biomedical and Health Informatics","","2017","21","6","1633","1643","Automation-assisted cervical screening via Pap smear or liquid-based cytology (LBC) is a highly effective cell imaging based cancer detection tool, where cells are partitioned into “abnormal” and “normal” categories. However, the success of most traditional classification methods relies on the presence of accurate cell segmentations. Despite sixty years of research in this field, accurate segmentation remains a challenge in the presence of cell clusters and pathologies. Moreover, previous classification methods are only built upon the extraction of hand-crafted features, such as morphology and texture. This paper addresses these limitations by proposing a method to directly classify cervical cells-without prior segmentation- based on deep features, using convolutional neural networks (ConvNets). First, the ConvNet is pretrained on a natural image dataset. It is subsequently fine-tuned on a cervical cell dataset consisting of adaptively resampled image patches coarsely centered on the nuclei. In the testing phase, aggregation is used to average the prediction scores of a similar set of image patches. The proposed method is evaluated on both Pap smear and LBC datasets. Results show that our method outperforms previous algorithms in classification accuracy (98.3%), area under the curve (0.99) values, and especially specificity (98.3%), when applied to the Herlev benchmark Pap smear dataset and evaluated using five-fold cross validation. Similar superior performances are also achieved on the HEMLBC (H&E stained manual LBC) dataset. Our method is promising for the development of automation-assisted reading systems in primary cervical screening.","","","10.1109/JBHI.2017.2705583","NIH Clinical Center; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932065","Cell classification;Cervical cytology;deep learning;neural networks;Pap smear","Image segmentation;Feature extraction;Training;Informatics;Imaging;Neural networks;Testing","biomedical optical imaging;cancer;cellular biophysics;feature extraction;image classification;image segmentation;image texture;medical image processing;neural nets","DeepPap;deep convolutional networks;cervical cell classification;automation-assisted cervical screening;Pap smear;liquid-based cytology;cell imaging;cancer detection;classification method;cell segmentation;cell cluster;cell pathology;hand-crafted feature extraction;cell morphology;cell texture;convolutional neural networks;ConvNet;cervical cell dataset;image patch;nuclei;classification accuracy;HEMLBC dataset;automation-assisted reading system;primary cervical screening","Cervix Uteri;Female;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Neural Networks (Computer);Papanicolaou Test;Vaginal Smears","20","56","Traditional","","","","IEEE","IEEE Journals"
"Long-Term Recurrent Convolutional Networks for Visual Recognition and Description","J. Donahue; L. A. Hendricks; M. Rohrbach; S. Venugopalan; S. Guadarrama; K. Saenko; T. Darrell","Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA; Department of Computer Science, University of Texas at Austin, Austin, TX; Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA; Department of Computer Science, University of Massachusetts Lowell, Lowell, MA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","4","677","691","Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent are effective for tasks involving sequences, visual and otherwise. We describe a class of recurrent convolutional architectures which is end-to-end trainable and suitable for large-scale visual understanding tasks, and demonstrate the value of these models for activity recognition, image captioning, and video description. In contrast to previous models which assume a fixed visual representation or perform simple temporal averaging for sequential processing, recurrent convolutional models are “doubly deep” in that they learn compositional representations in space and time. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Differentiable recurrent models are appealing in that they can directly map variable-length inputs (e.g., videos) to variable-length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent sequence models are directly connected to modern visual convolutional network models and can be jointly trained to learn temporal dynamics and convolutional perceptual representations. Our results show that such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined or optimized.","","","10.1109/TPAMI.2016.2599174","DARPA's; MSEE; SMISC; NSF; Berkeley Vision and Learning Center; NVIDIA; FITweltweit-Program; German Academic Exchange Service (DAAD); NDSEG; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558228","Computer vision;convolutional nets;deep learning;transfer learning","Visualization;Computational modeling;Computer architecture;Data models;Logic gates;Predictive models;Recurrent neural networks","backpropagation;computer vision;image sequences;neural net architecture;object recognition;recurrent neural nets","long-term recurrent convolutional networks;visual recognition;visual description;recurrent convolutional architectures;large-scale visual understanding tasks;activity recognition;image captioning;video description;compositional representation learning;long-term dependency Learning;network state updates;differentiable recurrent models;variable-length input mapping;variable-length output mapping;complex temporal dynamics;backpropagation;recurrent sequence models;visual convolutional network models;temporal dynamic learning;convolutional perceptual representations","","192","73","","","","","IEEE","IEEE Journals"
"Supervised Representation Learning for Audio Scene Classification","A. Rakotomamonjy","LITIS EA 4108, Université de Rouen Normandie, Saint-Étienne-du-Rouvray, France","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","6","1253","1265","This paper investigates the use of supervised feature learning approaches for extracting relevant and discriminative features from acoustic scene recordings. Owing to the recent release of open datasets for acoustic scene classification problems, representation learning techniques can now be envisioned for solving the problem of feature extraction. This paper makes a step toward this goal by first introducing a supervised nonnegative matrix factorization (SNMF). Our goal through this SNMF is to induce the matrix decomposition to carry out discriminative information in addition to the usual generative ones. We achieve this objective by augmenting the nonnegative matrix factorization optimization problem with a novel loss function related to class labels of each column of the matrix to decompose. While the scale of the datasets available is still small compared to those available in computer vision, we have studied models based on convolutional neural networks. We have analyzed the performances of these models on the DCASE-16 dataset and a corrected version of the LITIS Rouen one. Our experiments show that despite the small-scale setting, supervised feature learning is favorably competitive compared to the current state-of-the-art features. We also point out that for smaller scale dataset, SNMF is indeed slightly less prone to overfitting than convolutional neural networks. While the performances of these learned features are interesting per se, a deeper analysis of their behavior in the acoustic scene problem context raises open and difficult questions that we believe, need to be addressed for further performance breakthroughs.","","","10.1109/TASLP.2017.2690561","ANR Deep in France Project; Regional Project GRR; FEDER; DAISI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7933051","Audio scene classification;convolutional neural networks;feature learning;non-negative matrix factorization;time-frequency representation","Acoustics;Matrix decomposition;Context;Feature extraction;Dictionaries;Spectrogram;Neural networks","audio signal processing;computer vision;feature extraction;learning (artificial intelligence);matrix decomposition;neural nets","supervised representation learning;audio scene classification;discriminative feature extraction;acoustic scene recordings;acoustic scene classification problems;supervised nonnegative matrix factorization;SNMF;matrix decomposition;nonnegative matrix factorization optimization problem;computer vision;convolutional neural networks;DCASE-16 dataset;LITIS Rouen;supervised feature learning;acoustic scene problem","","3","54","","","","","IEEE","IEEE Journals"
"Machine Learning Method Applied in Readout System of Superheated Droplet Detector","Y. Liu; C. J. Sullivan; F. d’Errico","Department of Nuclear, Plasma, and Radiological Engineering, University of Illinois at Urbana–Champaign, Urbana, IL, USA; Department of Nuclear, Plasma, and Radiological Engineering, University of Illinois at Urbana–Champaign, Urbana, IL, USA; Yale University, New Haven, CT, USA","IEEE Transactions on Nuclear Science","","2017","64","7","1659","1663","Direct readability is one advantage of superheated droplet detectors in neutron dosimetry. Utilizing such a distinct characteristic, an imaging readout system analyzes image of the detector for neutron dose readout. To improve the accuracy and precision of algorithms in the imaging readout system, machine learning algorithms were developed. Deep learning neural network and support vector machine algorithms are applied and compared with generally used Hough transform and curvature analysis methods. The machine learning methods showed a much higher accuracy and better precision in recognizing circular gas bubbles.","","","10.1109/TNS.2017.2708725","University of Illinois at Urbana-Chamapign; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934386","Neural network;neutron detection;superheated droplet detector (SDD);support vector machine (SVM)","Detectors;Support vector machines;Imaging;Algorithm design and analysis;Image edge detection;Machine learning algorithms;Biological neural networks","dosimeters;Hough transforms;learning (artificial intelligence);neural nets;neutron detection;readout electronics;support vector machines","superheated droplet detector;neutron dosimetry;imaging readout system;neutron dose readout detector;support vector machine algorithm;deep learning neural network;circular gas bubble;machine learning algorithm","","","27","","","","","IEEE","IEEE Journals"
"DROW: Real-Time Deep Learning-Based Wheelchair Detection in 2-D Range Data","L. Beyer; A. Hermans; B. Leibe","Institute of Visual Computing, RWTH Aachen University, Aachen, Germany; Institute of Visual Computing, RWTH Aachen University, Aachen, Germany; Institute of Visual Computing, RWTH Aachen University, Aachen, Germany","IEEE Robotics and Automation Letters","","2017","2","2","585","592","We introduce the DROW detector, a deep learning-based object detector operating on 2-dimensional (2-D) range data. Laser scanners are lighting invariant, provide accurate 2-D range data, and typically cover a large field of view, making them interesting sensors for robotics applications. So far, research on detection in laser 2-D range data has been dominated by hand-crafted features and boosted classifiers, potentially losing performance due to suboptimal design choices. We propose a convolutional neural network (CNN) based detector for this task. We show how to effectively apply CNNs for detection in 2-D range data, and propose a depth preprocessing step and a voting scheme that significantly improve CNN performance. We demonstrate our approach on wheelchairs and walkers, obtaining state of the art detection results. Apart from the training data, none of our design choices limits the detector to these two classes, though. We provide a ROS node for our detector and release our dataset containing 464 k laser scans, out of which 24 k were annotated.","","","10.1109/LRA.2016.2645131","EU project STRANDS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797258","Categorization;human detection and tracking;object detection;segmentation","Wheelchairs;Detectors;Two dimensional displays;Mobile robots;Laser beam cutting","handicapped aids;learning (artificial intelligence);multi-robot systems;neural nets;object detection;optical scanners;real-time systems;wheelchairs","real-time deep learning;wheelchair detection;2D range data;DROW detector;object detector;laser scanners;robotics applications;laser 2D range data;hand crafted features;boosted classifiers;suboptimal design;convolutional neural network;CNN;ROS node","","10","36","","","","","IEEE","IEEE Journals"
"LSTM network: a deep learning approach for short-term traffic forecast","Z. Zhao; W. Chen; X. Wu; P. C. Y. Chen; J. Liu","Beihang University, People's Republic of China; Beihang University, People's Republic of China; Beihang University, People's Republic of China; National University of Singapore, Singapore; Beihang University, People's Republic of China","IET Intelligent Transport Systems","","2017","11","2","68","75","Short-term traffic forecast is one of the essential issues in intelligent transportation system. Accurate forecast result enables commuters make appropriate travel modes, travel routes, and departure time, which is meaningful in traffic management. To promote the forecast accuracy, a feasible way is to develop a more effective approach for traffic data analysis. The availability of abundant traffic data and computation power emerge in recent years, which motivates us to improve the accuracy of short-term traffic forecast via deep learning approaches. A novel traffic forecast model based on long short-term memory (LSTM) network is proposed. Different from conventional forecast models, the proposed LSTM network considers temporal-spatial correlation in traffic system via a two-dimensional network which is composed of many memory units. A comparison with other representative forecast models validates that the proposed LSTM network can achieve a better performance.","","","10.1049/iet-its.2016.0208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7874313","","","intelligent transportation systems;learning (artificial intelligence);recurrent neural nets;road traffic control","LSTM network;LSTM deep-learning approach;short-term traffic forecasting;intelligent transportation system;travel modes;travel routes;departure time;traffic management;traffic data analysis;computation power;long-short-term memory network;temporal-spatial correlation;two-dimensional network;memory units","","77","46","","","","","IET","IET Journals"
"Deep Edge Guided Recurrent Residual Learning for Image Super-Resolution","W. Yang; J. Feng; J. Yang; F. Zhao; J. Liu; Z. Guo; S. Yan","Institute of Computer Science and Technology, Peking University, Beijing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Snapchat Inc., Los Angeles, CA, USA; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Institute of Computer Science and Technology, Peking University, Beijing, China; Institute of Computer Science and Technology, Peking University, Beijing, China; Qihoo 360 Technology Company, Ltd., Artificial Intelligence Institute, Beijing, China","IEEE Transactions on Image Processing","","2017","26","12","5895","5907","In this paper, we consider the image super-resolution (SR) problem. The main challenge of image SR is to recover high-frequency details of a low-resolution (LR) image that are important for human perception. To address this essentially ill-posed problem, we introduce a Deep Edge Guided REcurrent rEsidual (DEGREE) network to progressively recover the high-frequency details. Different from most of the existing methods that aim at predicting high-resolution (HR) images directly, the DEGREE investigates an alternative route to recover the difference between a pair of LR and HR images by recurrent residual learning. DEGREE further augments the SR process with edge-preserving capability, namely the LR image and its edge map can jointly infer the sharp edge details of the HR image during the recurrent recovery process. To speed up its training convergence rate, by-pass connections across the multiple layers of DEGREE are constructed. In addition, we offer an understanding on DEGREE from the view-point of sub-band frequency decomposition on image signal and experimentally demonstrate how the DEGREE can recover different frequency bands separately. Extensive experiments on three benchmark data sets clearly demonstrate the superiority of DEGREE over the well-established baselines and DEGREE also provides new state-of-the-arts on these data sets. We also present addition experiments for JPEG artifacts reduction to demonstrate the good generality and flexibility of our proposed DEGREE network to handle other image processing tasks.","","","10.1109/TIP.2017.2750403","National Natural Science Foundation of China; CCF-Tencent Open Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030140","Super-resolution;edge guidance;recurrent residual network;sub-band recovery","Image edge detection;Image resolution;Signal resolution;Training;Feature extraction;Image reconstruction","image resolution;learning (artificial intelligence)","JPEG artifacts reduction;sub-band frequency decomposition;LR image;HR image;DEGREE network;image super-resolution problem;deep edge guided recurrent residual learning","","20","42","Traditional","","","","IEEE","IEEE Journals"
"Optimal Feature Selection and Deep Learning Ensembles Method for Emotion Recognition From Human Brain EEG Sensors","R. Majid Mehmood; R. Du; H. J. Lee","Division of Computer Science and Engineering, Chonbuk National University, Jeonju, South Korea; College of Geographic and Biologic Information, Nanjing University of Posts and Telecommunications, Nanjing, China; Division of Computer Science and Engineering, Chonbuk National University, Jeonju, South Korea","IEEE Access","","2017","5","","14797","14806","Recent advancements in human-computer interaction research have led to the possibility of emotional communication via brain-computer interface systems for patients with neuropsychiatric disorders or disabilities. In this paper, we efficiently recognize emotional states by analyzing the features of electroencephalography (EEG) signals, which are generated from EEG sensors that noninvasively measure the electrical activity of neurons inside the human brain, and select the optimal combination of these features for recognition. In this paper, the scalp EEG data of 21 healthy subjects (12-14 years old) were recorded using a 14-channel EEG machine while the subjects watched images with four types of emotional stimuli (happy, calm, sad, or scared). After preprocessing, the Hjorth parameters (activity, mobility, and complexity) were used to measure the signal activity of the time series data. We selected the optimal EEG features using a balanced one-way ANOVA after calculating the Hjorth parameters for different frequency ranges. Features selected by this statistical method outperformed univariate and multivariate features. The optimal features were further processed for emotion classification using support vector machine, k-nearest neighbor, linear discriminant analysis, Naive Bayes, random forest, deep learning, and four ensembles methods (bagging, boosting, stacking, and voting). The results show that the proposed method substantially improves the emotion recognition rate with respect to the commonly used spectral power band method.","","","10.1109/ACCESS.2017.2724555","Brain Korea 21 PLUS Project; National Research Foundation (NRF) of Korea; Ministry of Science, ICT and Future Planning, Korea, under the Information Technology Research Center, supervised by the Institute for Information and Communications Technology Promotion; Basic Science Research Program through the NRF of South Korea, through the Ministry of Education; National Natural Science Foundation for Young Scholars of China; Natural Science Foundation for Young Scholars of Jiangsu Province; NUPTSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7997991","EEG pattern recognition;Hjorth parameter;EEG feature extraction;EEG emotion recognition","Electroencephalography;Feature extraction;Emotion recognition;Electrodes;Sensors;Support vector machines;Medical services","Bayes methods;electroencephalography;emotion recognition;feature selection;learning (artificial intelligence);medical signal processing;signal classification;support vector machines;time series","linear discriminant analysis;naive Bayes;random forest;bagging;boosting;stacking;voting;k-nearest neighbor;support vector machine;emotion classification;statistical method;ANOVA;EEG features;time series data;signal activity;Hjorth parameters;emotional stimuli;14-channel EEG machine;neurons electrical activity;EEG signals;electroencephalography signals;emotional states;disabilities;neuropsychiatric disorders;brain-computer interface systems;emotional communication;human-computer interaction;human brain EEG sensors;emotion recognition;deep learning ensembles;optimal feature selection","","13","46","","","","","IEEE","IEEE Journals"
"Multitask Feature Learning for Low-Resource Query-by-Example Spoken Term Detection","H. Chen; C. Leung; L. Xie; B. Ma; H. Li","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, Department of Electrical and Computer Engineering, National University of Singapore, SingaporeSingapore","IEEE Journal of Selected Topics in Signal Processing","","2017","11","8","1329","1339","We propose a novel technique that learns a low-dimensional feature representation from unlabeled data of a target language, and labeled data from a nontarget language. The technique is studied as a solution to query-by-example spoken term detection (QbE-STD) for a low-resource language. We extract low-dimensional features from a bottle-neck layer of a multitask deep neural network, which is jointly trained with speech data from the low-resource target language and resource-rich nontarget language. The proposed feature learning technique aims to extract acoustic features that offer phonetic discriminability. It explores a new way of leveraging cross-lingual speech data to overcome the resource limitation in the target language. We conduct QbE-STD experiments using the dynamic time warping distance of the multitask bottle-neck features between the query and the search database. The QbE-STD process does not rely on an automatic speech recognition pipeline of the target language. We validate the effectiveness of multitask feature learning through a series of comparative experiments.","","","10.1109/JSTSP.2017.2764270","China Scholarship Council; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070974","Query-by-example;spoken term detection;multi-task learning;bottle-neck feature","Speech;Feature extraction;Neural networks;Speech processing;Gaussian mixture model","feature extraction;learning (artificial intelligence);natural language processing;neural nets;query processing;speech recognition","resource-rich nontarget language;feature learning technique;cross-lingual speech data;QbE-STD experiments;multitask bottle-neck features;QbE-STD process;multitask feature;low-dimensional feature representation;unlabeled data;low-resource language;bottle-neck layer;multitask deep neural network;low-resource target language;acoustic feature extraction;multitask feature learning;low-resource query-by-example spoken term detection;low-dimensional feature extraction;automatic speech recognition","","2","72","Traditional","","","","IEEE","IEEE Journals"
"Learning Depth-Aware Deep Representations for Robotic Perception","L. Porzi; S. R. Buló; A. Penate-Sanchez; E. Ricci; F. Moreno-Noguer","University of Perugia, Perugia, Italy; Fondazione Bruno Kessler, Trento, Italy; University College London, London, U.K.; University of Perugia, Perugia, Italy; Institut de Robòtica i Informàtica Industrial, Barcelona, Spain","IEEE Robotics and Automation Letters","","2017","2","2","468","475","Exploiting RGB-D data by means of convolutional neural networks (CNNs) is at the core of a number of robotics applications, including object detection, scene semantic segmentation, and grasping. Most existing approaches, however, exploit RGB-D data by simply considering depth as an additional input channel for the network. In this paper we show that the performance of deep architectures can be boosted by introducing DaConv, a novel, general-purpose CNN block which exploits depth to learn scale-aware feature representations. We demonstrate the benefits of DaConv on a variety of robotics oriented tasks, involving affordance detection, object coordinate regression, and contour detection in RGB-D images. In each of these experiments we show the potential of the proposed block and how it can be readily integrated into existing CNN architectures.","","","10.1109/LRA.2016.2637444","Spanish MINECO project RobInstruct; ERA-Net Chistera project I-DRESS; EU project AEROARMS; EU project SECOND HANDS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7778240","RGB-D perception;visual learning","Convolution;Kernel;Robot kinematics;Computer architecture;Standards;Three-dimensional displays","convolution;image colour analysis;image representation;image segmentation;neural net architecture;object detection;regression analysis;robot vision","depth-aware deep representations;robotic perception;RGB-D data;convolutional neural networks;object detection;scene semantic segmentation;grasping;deep architectures;DaConv;general-purpose CNN block;scale-aware feature representations;affordance detection;object coordinate regression;contour detection;RGB-D images;CNN architectures","","9","41","","","","","IEEE","IEEE Journals"
"High Class-Imbalance in pre-miRNA Prediction: A Novel Approach Based on deepSOM","G. Stegmayer; C. Yones; L. Kamenetzky; D. H. Milone","Research Institute for Signals, Systems, and Computational Intelligence (sinc(i)), FICH-UNL, CONICET, Santa Fe, Santa Fe, Argentina; Research Institute for Signals, Systems, and Computational Intelligence (sinc(i)), FICH-UNL, CONICET, Santa Fe, Santa Fe, Argentina; Instituto de Investigaciones en Microbiologa y Parasitologa Mdica - IMPAM, UBA-CONICET, Buenos Aires, Buenos Aires, 1121; Research Institute for Signals, Systems, and Computational Intelligence (sinc(i)), FICH-UNL, CONICET, Santa Fe, Santa Fe, Argentina","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2017","14","6","1316","1326","The computational prediction of novel microRNA within a full genome involves identifying sequences having the highest chance of being a miRNA precursor (pre-miRNA). These sequences are usually named candidates to miRNA. The well-known pre-miRNAs are usually only a few in comparison to the hundreds of thousands of potential candidates to miRNA that have to be analyzed, which makes this task a high class-imbalance classification problem. The classical way of approaching it has been training a binary classifier in a supervised manner, using well-known pre-miRNAs as positive class and artificially defining the negative class. However, although the selection of positive labeled examples is straightforward, it is very difficult to build a set of negative examples in order to obtain a good set of training samples for a supervised method. In this work, we propose a novel and effective way of approaching this problem using machine learning, without the definition of negative examples. The proposal is based on clustering unlabeled sequences of a genome together with well-known miRNA precursors for the organism under study, which allows for the quick identification of the best candidates to miRNA as those sequences clustered with known precursors. Furthermore, we propose a deep model to overcome the problem of having very few positive class labels. They are always maintained in the deep levels as positive class while less likely pre-miRNA sequences are filtered level after level. Our approach has been compared with other methods for pre-miRNAs prediction in several species, showing effective predictivity of novel miRNAs. Additionally, we will show that our approach has a lower training time and allows for a better graphical navegability and interpretation of the results. A web-demo interface to try deepSOM is available at http://fich.unl.edu.ar/sinc/web-demo/deepsom/.","","","10.1109/TCBB.2016.2576459","National Scientific and Technical Research Council; National University of Litoral; Agencia Nacional de Promocion Cientifica y Tecnologica (ANPCyT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7484734","Unsupervised learning;classification;high class-imbalance;deep self-organizing maps","Bioinformatics;Training;Neurons;Genomics;Computational biology;Unsupervised learning;Data models;Self-organizing networks","biology computing;molecular biophysics;RNA;unsupervised learning","high class-imbalance;predictivity;miRNA precursor;microRNA;deepSOM;pre-miRNA prediction","Animals;Computational Biology;Humans;MicroRNAs;Models, Statistical;Plants;Software;Unsupervised Machine Learning","","43","Traditional","","","","IEEE","IEEE Journals"
"Liver Fibrosis Classification Based on Transfer Learning and FCNet for Ultrasound Images","D. Meng; L. Zhang; G. Cao; W. Cao; G. Zhang; B. Hu","MOE Research Center for Software/Hardware Co-Design Engineering, East China Normal University, Shanghai, China; Chinese Academy of Sciences, Institute of Software, Beijing, China; MOE Research Center for Software/Hardware Co-Design Engineering, East China Normal University, Shanghai, China; College of Information Engineering, Shenzhen University, Shenzhen, China; School of Computer Science and Software Engineering, East China Normal University, Shanghai, China; Department of Ultrasound, Shanghai Jiaotong University Affiliated Sixth People’s Hospital, Shanghai, China","IEEE Access","","2017","5","","5804","5810","Diagnostic ultrasound offers great improvements in diagnostic accuracy and robustness. However, it is difficult to make subjective and uniform diagnoses, because the quality of ultrasound images can be easily influenced by machine settings, the characteristics of ultrasonic waves, the interactions between ultrasound and body tissues, and other uncontrollable factors. In this paper, we propose a novel liver fibrosis classification method based on transfer learning (TL) using VGGNet and a deep classifier called fully connected network (FCNet). In case of insufficient samples, deep features extracted using TL strategy can provide sufficient classification information. These deep features are then sent to FCNet for the classification of different liver fibrosis statuses. With this framework, tests show that our deep features combined with the FCNet can provide suitable information to enable the construction of the most accurate prediction model when compared with other methods.","","","10.1109/ACCESS.2017.2689058","National Natural Science Foundation of China; NSFC-Zhejiang Joint Fund for the Integration of Industrialization and Informatization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890483","Deep neural networks;fully connected layers;transfer learning;liver fibrosis","Liver;Ultrasonic imaging;Feature extraction;Heating systems;Neural networks;Medical services;Training","biological tissues;biomedical ultrasonics;feature extraction;image classification;learning (artificial intelligence);liver;medical disorders;medical image processing","feature extraction;fully connected network;VGGNet;ultrasound-tissue interactions;robustness;diagnostic accuracy;diagnostic ultrasound;ultrasound images;FCNet classifier;transfer learning;liver fibrosis classification","","14","23","","","","","IEEE","IEEE Journals"
"Deep Neural Network Initialization Methods for Micro-Doppler Classification With Low Training Sample Support","M. S. Seyfioğlu; S. Z. Gürbüz","Department of Electrical Electronics Engineering, TOBB University of Economics and Technology, Ankara, Turkey; Department of Electrical and Computer Engineering, University of Alabama, Tuscaloosa, AL, USA","IEEE Geoscience and Remote Sensing Letters","","2017","14","12","2462","2466","Deep neural networks (DNNs) require large-scale labeled data sets to prevent overfitting while having good generalization. In radar applications, however, acquiring a measured data set of the order of thousands is challenging due to constraints on manpower, cost, and other resources. In this letter, the efficacy of two neural network initialization techniques-unsupervised pretraining and transfer learning-for dealing with training DNNs on small data sets is compared. Unsupervised pretraining is implemented through the design of a convolutional autoencoder (CAE), while transfer learning from two popular convolutional neural network architectures (VGGNet and GoogleNet) is used to augment measured RF data for training. A 12-class problem for discrimination of micro-Doppler signatures for indoor human activities is utilized to analyze activation maps, bottleneck features, class model, and classification accuracy with respect to training sample size. Results show that on meager data sets, transfer learning outperforms unsupervised pretraining and random initialization by 10% and 25%, respectively, but that when the sample size exceeds 650, unsupervised pretraining surpasses transfer learning and random initialization by 5% and 10%, respectively. Visualization of activation layers and learned models reveals how the CAE succeeds in representing the micro-Doppler signature.","","","10.1109/LGRS.2017.2771405","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119733","Convolutional autoencoders (CAEs);convolutional neural networks (CNN);gait classification;micro-Doppler;radar;transfer learning;vGGNet","Training;Feature extraction;Spectrogram;Decoding;Neural networks;Radar imaging","Doppler radar;feature extraction;image classification;learning (artificial intelligence);neural nets;radar imaging","convolutional autoencoder;microDoppler signature;indoor human activities;low training sample support;large-scale labeled data sets;radar applications;transfer learning;DNNs;deep neural network initialization methods;microDoppler classification;CAE","","18","32","","","","","IEEE","IEEE Journals"
"Exploiting Deep Neural Networks and Head Movements for Robust Binaural Localization of Multiple Sources in Reverberant Environments","N. Ma; T. May; G. J. Brown","Department of Computer Science, University of Sheffield, Sheffield, U.K.; Hearing Systems Group, Technical University of Denmark, Denmark; Department of Computer Science, University of Sheffield, Sheffield, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","12","2444","2453","This paper presents a novel machine-hearing system that exploits deep neural networks (DNNs) and head movements for robust binaural localization of multiple sources in reverberant environments. DNNs are used to learn the relationship between the source azimuth and binaural cues, consisting of the complete cross-correlation function (CCF) and interaural level differences (ILDs). In contrast to many previous binaural hearing systems, the proposed approach is not restricted to localization of sound sources in the frontal hemifield. Due to the similarity of binaural cues in the frontal and rear hemifields, front-back confusions often occur. To address this, a head movement strategy is incorporated in the localization model to help reduce the front-back errors. The proposed DNN system is compared to a Gaussian-mixture-model-based system that employs interaural time differences (ITDs) and ILDs as localization features. Our experiments show that the DNN is able to exploit information in the CCF that is not available in the ITD cue, which together with head movements substantially improves localization accuracies under challenging acoustic scenarios, in which multiple talkers and room reverberation are present.","","","10.1109/TASLP.2017.2750760","European Union FP7 project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8086216","Binaural sound source localisation;deep neural networks;head movements;machine hearing;multi-conditional training;reverberation","Auditory system;Machine learning;Training;Reverberation;Speech processing;Neural networks","acoustic radiators;acoustic signal processing;Gaussian processes;handicapped aids;hearing;learning (artificial intelligence);neural nets;reverberation","multiple sources;reverberant environments;DNNs;source azimuth;binaural cues;interaural level differences;ILDs;sound sources;frontal hemifields;rear hemifields;interaural time differences;head movements;room reverberation;deep neural networks;robust binaural localization;novel machine-hearing system;binaural hearing systems;cross-correlation function;CCF;front-back errors;Gaussian-mixture-model","","17","26","CCBY","","","","IEEE","IEEE Journals"
"Building Occupancy Estimation with Environmental Sensors via CDBLSTM","Z. Chen; R. Zhao; Q. Zhu; M. K. Masood; Y. C. Soh; K. Mao","Department of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Industrial Electronics","","2017","64","12","9549","9559","Buildings consume quite a lot of energy; hence, the issue of building energy efficiency has attracted a great deal of attention in recent years. A key factor in achieving this objective is occupancy information that directly impacts on energy-related building control systems. In this paper, we leverage on environmental sensors that are nonintrusive and cost-effective for building occupancy estimation. Our result relies on feature engineering and learning. The conventional feature engineering requires one to manually extract relevant features without a clear guideline. This blind feature extraction is labor intensive and may miss some significant implicit features. To address this issue, we propose a convolutional deep bidirectional long short-term memory (CDBLSTM) approach that contains a convolutional network and a deep structure to automatically learn significant features from the sensory data without human intervention. Moreover, the long short-term memory networks are able to capture temporal dependencies in the data and the bidirectional structure can take the past and future contexts into consideration for the final identification of occupancy. We have conducted real experiments to evaluate the performance of our proposed CDBLSTM approach. Instead of estimating the exact number of occupants, we attempt to identify the range of occupants, i.e., zero, low, medium, and high, which is adequate for most of building control systems. The experimental results indicate the effectiveness of our proposed approach compared with the state-of-the-art methods.","","","10.1109/TIE.2017.2711530","Building Efficiency and Sustainability in the Tropics (SinBerBEST) program; Singapore's National Research Foundation; University of California; Berkeley in collaboration with Singapore Universities; Singapore's National Research Foundation's Competitive Research Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7938392","Building occupancy estimation;convolutional deep bidirectional long short-term memory (CDBLSTM);environmental sensors;feature learning","Feature extraction;Estimation;Convolution;Temperature sensors;Buildings;Bidirectional control","building management systems;energy conservation;feedforward neural nets;learning (artificial intelligence)","building occupancy estimation;environmental sensors;energy efficiency;energy-related building control systems;CDBLSTM approach;feature engineering;convolutional deep bidirectional long short-term memory;deep structure;automatically feature learning;temporal dependencies;bidirectional structure","","5","32","Traditional","","","","IEEE","IEEE Journals"
"Distinguishing Cloud and Snow in Satellite Images via Deep Convolutional Network","Y. Zhan; J. Wang; J. Shi; G. Cheng; L. Yao; W. Sun","Department of Electronic Engineering, Tsinghua University, Beijing, China; State Key Laboratory of Space-Ground Integrated Information Technology, Space Star Technology Co., Ltd., Beijing, China; SenseTime Group Ltd., Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Space-Ground Integrated Information Technology, Space Star Technology Co., Ltd., Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","10","1785","1789","Cloud and snow detection has significant remote sensing applications, while they share similar low-level features due to their consistent color distributions and similar local texture patterns. Thus, accurately distinguishing cloud from snow in pixel level from satellite images is always a challenging task with traditional approaches. To solve this shortcoming, in this letter, we proposed a deep learning system to classify cloud and snow with fully convolutional neural networks in pixel level. Specifically, a specially designed fully convolutional network was introduced to learn deep patterns for cloud and snow detection from the multispectrum satellite images. Then, a multiscale prediction strategy was introduced to integrate the low-level spatial information and high-level semantic information simultaneously. Finally, a new and challenging cloud and snow data set was labeled manually to train and further evaluate the proposed method. Extensive experiments demonstrate that the proposed deep model outperforms the state-of-the-art methods greatly both in quantitative and qualitative performances.","","","10.1109/LGRS.2017.2735801","Open Research Fund of State Key Laboratory of Space-Ground Integrated Information Technology of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8013916","Cloud and snow detection;fully convolutional network;multiscale prediction","Snow;Convolution;Semantics;Satellites;Remote sensing;Image color analysis;Feature extraction","clouds;geophysical image processing;image classification;learning (artificial intelligence);neural nets;remote sensing;snow","deep convolutional network;cloud detection;snow detection;remote sensing;color distribution;local texture pattern;pixel level;deep learning system;cloud classification;snow classification;fully convolutional neural networks;multispectrum satellite image","","4","21","Traditional","","","","IEEE","IEEE Journals"
"A Machine Learning Based Framework for Verification and Validation of Massive Scale Image Data","J. Ding; X. Hu; V. Gudivada","Department of Computer Science, East Carolina University, Greenville, NC, 27858 USA (e-mail: dingj@ecu.ed).; Department of Computer Science, East Carolina University, Greenville, NC, 27858 USA (e-mail: dingj@ecu.ed).; Department of Computer Science, East Carolina University, Greenville, NC, 27858 USA (e-mail: dingj@ecu.ed).","IEEE Transactions on Big Data","","2017","PP","99","1","1","Big data validation and system verification are crucial for ensuring the quality of big data applications. However, a rigorous technique for such tasks is yet to emerge. During the past decade, we have developed a big data system called CMA for investigating the classification of biological cells based on cell morphology that is captured in diffraction images. CMA includes a group of scientific software tools, machine learning algorithms, and a large scale cell image repository. We have also developed a framework for rigorous validation of the massive scale image data and verification of both the software systems and machine learning algorithms. Different machine learning algorithms integrated with image processing techniques were used to automate the selection and validation of the massive scale image data in CMA. An experiment based technique guided by a feature selection algorithm was introduced in the framework to select optimal machine learning features. An iterative metamorphic testing approach is applied for testing the scientific software. Due to the non-testable characteristic of the scientific software, a machine learning approach is introduced for developing test oracles iteratively to ensure the adequacy of the test coverage criteria. Performance of the machine learning algorithms is evaluated with the stratified N-fold cross validation and confusion matrix. We describe the design of the proposed framework with CMA as the case study. The effectiveness of the framework is demonstrated through verifying and validating the data set, software systems and algorithms in CMA.","","","10.1109/TBDATA.2017.2680460","NSF REU on Software Testing; NSF REU on Software Testing and Analytics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875094","Big Data;Diffraction Image;Machine Learning;Deep Learning;Metamorphic Testing","Big data;Diffraction;Morphology;Software;Machine learning algorithms;Three-dimensional displays;Testing","","","","1","","","","","","IEEE","IEEE Early Access Articles"
"Deep Convolutional Highway Unit Network for SAR Target Classification With Limited Labeled Training Data","Z. Lin; K. Ji; M. Kang; X. Leng; H. Zou","School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","7","1091","1095","The deep convolutional neural network (CNN) has been widely used for target classification, because it can learn highly useful representations from data. However, it is difficult to apply a CNN for synthetic aperture radar (SAR) target classification directly, for it often requires a large volume of labeled training data, which is impractical for SAR applications. The highway network is a newly proposed architecture based on CNN that can be trained with smaller data sets. This letter proposes a novel architecture called the convolutional highway unit to train deeper networks with limited SAR data. The unit architecture is formed by modified convolutional highway layers, a maxpool layer, and a dropout layer. Then, the networks can be flexibly formed by stacking the unit architecture to extract deep feature representations for classification. Experimental results on the moving and stationary target acquisition and recognition data set indicate that the branched ensemble model based on the unit architecture can achieve 99% classification accuracy with all training data. When the training data are reduced to 30%, the classification accuracy of the ensemble model can still reach 94.97%.","","","10.1109/LGRS.2017.2698213","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7926358","Convolutional neural network (CNN);deep learning;feature representation;highway network;syntheticaperture radar (SAR);training data","Road transportation;Synthetic aperture radar;Feature extraction;Training;Training data;Transforms;Neural networks","convolution;feature extraction;image classification;image representation;neural nets;radar imaging;synthetic aperture radar","target recognition data set;target acquisition;deep feature representations;dropout layer;maxpool layer;modified convolutional highway layers;convolutional highway unit;highway network;labeled training data;SAR target classification;synthetic aperture radar target classification;CNN;deep convolutional neural network","","40","29","","","","","IEEE","IEEE Journals"
"DeepCloud: Ground-Based Cloud Image Categorization Using Deep Convolutional Features","L. Ye; Z. Cao; Y. Xiao","National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","10","5729","5740","Accurate ground-based cloud image categorization is a critical but challenging task that has not been well addressed. One of the essential issues that affect the performance is to extract the representative visual features. Nearly all of the existing methods rely on the hand-crafted descriptors (e.g., local binary patterns, CENsus TRsansform hISTogram, and scale-invariant feature transform). Their limited discriminative power indeed leads to the unsatisfactory performance. To alleviate this, we propose “DeepCloud” as a novel cloud image feature extraction approach by resorting to the deep convolutional visual features. In the recent years, the deep convolutional neural network (CNN) has achieved the promising results in lots of computer vision and image understanding fields. Nevertheless, it has not been applied to cloud image classification yet. Thus, we actually pay the first effort to fill this blank. Since cloud image classification can be attributed to a multi-instance learning problem, simply employing the convolutional features within CNN cannot achieve the promising result. To address this, Fisher vector encoding is applied to executing the spatial feature aggregation and high-dimensional feature mapping on the raw deep convolutional features. Moreover, the hierarchical convolutional layers are used simultaneously to capture the fine textural characteristics and high-level semantic information in the unified manner. To further leverage the performance, a cloud pattern mining and selection method are also proposed. It targets at finding the discriminative local patterns to better distinguish the different kinds of clouds. The experiments on a challenging ground-based cloud image data set demonstrate the superiority of the proposition over the state-of-the-art methods.","","","10.1109/TGRS.2017.2712809","National Natural Science Foundation of China; National High-tech R&D Program of China (863 Program); Chinese Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7964703","Convolutional neural network (CNN);deep learning;Fisher vector (FV);ground-based cloud image categorization;pattern mining","Feature extraction;Clouds;Visualization;Semantics;Meteorology;Encoding;Convolutional codes","geophysical image processing;geophysical techniques","DeepCloud;ground-based cloud image categorization;deep convolutional features;cloud image feature extraction approach;CNN;multiinstance learning problem","","6","59","Traditional","","","","IEEE","IEEE Journals"
"SkeletonNet: Mining Deep Part Features for 3-D Action Recognition","Q. Ke; S. An; M. Bennamoun; F. Sohel; F. Boussaid","School of Computer Science and Software Engineering, The University of Western Australia, Crawley, WA, Australia; School of Computer Science and Software Engineering, The University of Western Australia, Crawley, WA, Australia; School of Computer Science and Software Engineering, The University of Western Australia, Crawley, WA, Australia; School of Engineering and Information Technology, Murdoch University, Murdoch, WA, Australia; School of Electrical, Electronic and Computer Engineering, The University of Western Australia, Crawley, WA, Australia","IEEE Signal Processing Letters","","2017","24","6","731","735","This letter presents SkeletonNet, a deep learning framework for skeleton-based 3-D action recognition. Given a skeleton sequence, the spatial structure of the skeleton joints in each frame and the temporal information between multiple frames are two important factors for action recognition. We first extract body-part-based features from each frame of the skeleton sequence. Compared to the original coordinates of the skeleton joints, the proposed features are translation, rotation, and scale invariant. To learn robust temporal information, instead of treating the features of all frames as a time series, we transform the features into images and feed them to the proposed deep learning network, which contains two parts: one to extract general features from the input images, while the other to generate a discriminative and compact representation for action recognition. The proposed method is tested on the SBU kinect interaction dataset, the CMU dataset, and the large-scale NTU RGB+D dataset and achieves state-of-the-art performance.","","","10.1109/LSP.2017.2690339","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891014","Convolutional neural networks (CNNs);robust features;3-D action recognition","Skeleton;Feature extraction;Legged locomotion;Three-dimensional displays;Robustness;Hip;Australia","data mining;image colour analysis;image representation;image sequences;learning (artificial intelligence);time series","SkeletonNet;deep part feature mining;3D action recognition;deep learning framework;skeleton sequence;robust temporal information;compact representation;large-scale NTU RGB+D dataset;time series","","49","35","","","","","IEEE","IEEE Journals"
"Fully Convolutional Networks for Semantic Segmentation","E. Shelhamer; J. Long; T. Darrell","Department of Electrical Engineering and Computer Science (CS Division), University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science (CS Division), University of California, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science (CS Division), University of California, Berkeley, CA, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","4","640","651","Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.","","","10.1109/TPAMI.2016.2572683","DARPA's MSEE; SMISC; US National Science Foundation; NSF; GRFP; Toyota; Berkeley Vision and Learning Center; NVIDIA; GPU; SIFT; IU; IU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478072","Semantic Segmentation;Convolutional Networks;Deep Learning;Transfer Learning","Semantics;Image segmentation;Training;Convolution;Computer architecture;Proposals;Fuses","feedforward neural nets;image classification;image representation;image resolution;image segmentation;learning (artificial intelligence);transforms","fully convolutional networks;semantic segmentation;visual models;correspondingly-sized output;spatially dense prediction tasks;contemporary classification networks;learned representations;coarse layer;fine layer;PASCAL VOC;NYUDv2;SIFT Flow;PASCAL-Context","","975","62","","","","","IEEE","IEEE Journals"
"Ultrasound Standard Plane Detection Using a Composite Neural Network Framework","H. Chen; L. Wu; Q. Dou; J. Qin; S. Li; J. Cheng; D. Ni; P. Heng","Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; School of Biomedical Engineering, Shenzhen University, Shenzhen, China; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; School of Nursing, Hong Kong Polytechnic University, Hong Kong; Department of Ultrasound, Affiliated Shenzhen Maternal and Child Healthcare Hospital, Nanfang Medical University, Shenzhen, China; School of Biomedical Engineering, Shenzhen University, Shenzhen, China; School of Biomedical Engineering, Shenzhen University, Shenzhen, China; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Cybernetics","","2017","47","6","1576","1586","Ultrasound (US) imaging is a widely used screening tool for obstetric examination and diagnosis. Accurate acquisition of fetal standard planes with key anatomical structures is very crucial for substantial biometric measurement and diagnosis. However, the standard plane acquisition is a labor-intensive task and requires operator equipped with a thorough knowledge of fetal anatomy. Therefore, automatic approaches are highly demanded in clinical practice to alleviate the workload and boost the examination efficiency. The automatic detection of standard planes from US videos remains a challenging problem due to the high intraclass and low interclass variations of standard planes, and the relatively low image quality. Unlike previous studies which were specifically designed for individual anatomical standard planes, respectively, we present a general framework for the automatic identification of different standard planes from US videos. Distinct from conventional way that devises hand-crafted visual features for detection, our framework explores in- and between-plane feature learning with a novel composite framework of the convolutional and recurrent neural networks. To further address the issue of limited training data, a multitask learning framework is implemented to exploit common knowledge across detection tasks of distinctive standard planes for the augmentation of feature learning. Extensive experiments have been conducted on hundreds of US fetus videos to corroborate the better efficacy of the proposed framework on the difficult standard plane detection problem.","","","10.1109/TCYB.2017.2685080","National Basic Research Program of China, 973 Program; National Natural Science Foundation of China; Research Grants Council of Hong Kong Special Administrative Region; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890445","Convolutional neural network (CNN);deep learning;knowledge transfer;recurrent neural network (RNN);standard plane;ultrasound (US)","Standards;Fetus;Videos;Feature extraction;Biomedical imaging;Training data;Machine learning","biomedical ultrasonics;convolution;learning (artificial intelligence);medical image processing;obstetrics;recurrent neural nets;ultrasonic imaging;video signal processing","ultrasound standard plane detection;composite neural network framework;ultrasound imaging;obstetric examination;fetal standard plane acquisition;substantial biometric measurement;fetal anatomy;in-plane feature learning;between-plane feature learning;convolutional neural networks;recurrent neural networks;multitask learning framework;US fetus videos","Algorithms;Female;Fetus;Humans;Image Processing, Computer-Assisted;Neural Networks (Computer);Pregnancy;Ultrasonography, Prenatal;Video Recording","22","63","","","","","IEEE","IEEE Journals"
"IBM Deep Learning Service","B. Bhattacharjee; S. Boag; C. Doshi; P. Dube; B. Herta; V. Ishakian; K. R. Jayaram; R. Khalaf; A. Krishna; Y. B. Li; V. Muthusamy; R. Puri; Y. Ren; F. Rosenberg; S. R. Seelam; Y. Wang; J. M. Zhang; L. Zhang","NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","IBM Journal of Research and Development","","2017","61","4/5","10:1","10:11","Deep learning, driven by large neural network models, is overtaking traditional machine learning methods for understanding unstructured and perceptual data domains such as speech, text, and vision. At the same time, the “As-a-Service”-based business model for the cloud is fundamentally transforming the information technology industry. These two trends, deep learning and “As-a-Service,” are colliding to give rise to a new business model for cognitive application delivery: deep learning as a service in the cloud. In this paper, we discuss the details of the software architecture behind IBM's deep learning as a service (DLaaS). DLaaS provides developers the flexibility to use popular deep learning libraries—such as Caffe, Torch, and TensorFlow—in the cloud in a scalable and resilient manner with minimal effort. The platform uses a distribution and orchestration layer that facilitates learning from a large amount of data in a reasonable amount of time across compute nodes. A resource provisioning layer enables flexible job management on heterogeneous resources, such as graphics processing units and central processing units, in an infrastructure-as-a-service cloud.","","","10.1147/JRD.2017.2716578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030274","","Training;Machine learning;Data models;Monitoring;Neural networks;Graphics processing units","","","","4","22","","","","","IBM","IBM Journals"
"Discriminative Deep Belief Networks with Ant Colony Optimization for Health Status Assessment of Machine","M. Ma; C. Sun; X. Chen","State Key Laboratory for Manufacturing Systems Engineering, Xi&#x2019;an Jiaotong University, Xi&#x2019;an, China; State Key Laboratory for Manufacturing Systems Engineering, Xi&#x2019;an Jiaotong University, Xi&#x2019;an, China; State Key Laboratory for Manufacturing Systems Engineering, Xi&#x2019;an Jiaotong University, Xi&#x2019;an, China","IEEE Transactions on Instrumentation and Measurement","","2017","66","12","3115","3125","On-line health status monitoring, a key part of prognostics and health management, provides various benefits, such as preventing unexpected failure and improving safety and reliability. In this paper, a data-driven approach for health status assessment is presented. A novel method based on discriminative deep belief networks (DDBN) and ant colony optimization (ACO) is used to predict health status of machine. DDBN is a new paradigm that utilizes a deep architecture to combine the advantages of deep belief networks and discriminative ability of back-propagation strategy. DDBN works through a greedy layer-by-layer training with multiple stacked restricted Boltzmann machines, which preserves information well when embedding features from high-dimensional space to low-dimensional space. However, selecting the parameters of DDBN is quite challenging. To address the problem, ACO is introduced to DDBN in this paper. By optimization, the structure of DDBN model is determined automatically without prior knowledge and the performance is enhanced. To evaluate the proposed approach, two case studies were carried out, which shows that it can achieve a good result. The performance of this model is also compared with support vector machine. It is concluded that the proposed method is very promising in the field of prognostics.","","","10.1109/TIM.2017.2735661","National Key Basic Program of China; National Natural Science Foundation of China; China Postdoctoral Science Foundation; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8012551","Ant colony optimization (ACO);discriminative deep belief network (DDBN);health status monitoring;prognostics and health management (PHM)","Prognostics and health management;Maintenance engineering;Data models;Machine learning;Ant colony optimization;Monitoring","ant colony optimisation;backpropagation;belief networks;Boltzmann machines;condition monitoring;fault diagnosis;machinery;mechanical engineering computing","online health status monitoring;deep architecture;prognostics;back-propagation strategy;greedy layer-by-layer training;restricted Boltzmann machines;DDBN model;ACO;data-driven approach;health management;health status assessment;ant colony optimization;discriminative deep belief networks","","11","37","","","","","IEEE","IEEE Journals"
"Deep Features Learning for Medical Image Analysis with Convolutional Autoencoder Neural Network","M. Chen; X. Shi; Y. Zhang; D. Wu; M. Guizani","School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, HuBei China (e-mail: minchen@ieee.org); Huazhong University of Science and Technology, 12443 Wuhan, Hubei China (e-mail: xiaoboshi.cs@qq.com); Zhongnan University of Economics and Law, 12445 Wuhan, Hubei China 430073 (e-mail: yin.zhang.cn@ieee.org); the Department of Computer Science, School of Information, Sun Yat-sen University, Guangzhou, Guangdong China (e-mail: wudi27@sysu.edu.cn); ECE, University of Idaho, 5640 Moscow, Idaho United States 83844-1023 (e-mail: mguizani@ieee.org)","IEEE Transactions on Big Data","","2017","PP","99","1","1","At present, computed tomography (CT) are widely used to assist diagnosis. Especially, computer aided diagnosis (CAD) based on artificial intelligence (AI) is an extremely important research field in intelligent healthcare. However, it is a great challenge to establish an adequate labeled dataset for CT analysis assistance, due to the privacy and security issues. Therefore, this paper proposes a convolutional autoencoder deep learning framework to support unsupervised image features learning for lung nodule through unlabeled data, which only needs a small amount of labeled data for efficient feature learning. Through comprehensive experiments, it evaluates that the proposed scheme is superior to other approaches, which effectively solves the intrinsic labor-intensive problem during of artificial image labeling. Moreover, it verifies that the proposed convolutional autoencoder approach can be extended for similarity measurement of lung nodules images. Especially, the features extracted through unsupervised learning are also applicable in other related scenarios.","","","10.1109/TBDATA.2017.2717439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954012","Convolutional autoencoder neural network;Lung nodule;Feature learning;Hand-craft feature;Unsupervised learning","Feature extraction;Computed tomography;Biomedical imaging;Convolutional codes;Lungs;Training;Image analysis","","","","23","","","","","","IEEE","IEEE Early Access Articles"
"Network Traffic Classifier With Convolutional and Recurrent Neural Networks for Internet of Things","M. Lopez-Martin; B. Carro; A. Sanchez-Esguevillas; J. Lloret","Departamento TSyCeIT, ETSIT, Universidad de Valladolid, Valladolid, Spain; Departamento TSyCeIT, ETSIT, Universidad de Valladolid, Valladolid, Spain; Departamento TSyCeIT, ETSIT, Universidad de Valladolid, Valladolid, Spain; Instituto de Investigación para la Gestión Integrada de Zonas Costeras, Universitat Politècnica de València, Valencia, Spain","IEEE Access","","2017","5","","18042","18050","A network traffic classifier (NTC) is an important part of current network monitoring systems, being its task to infer the network service that is currently used by a communication flow (e.g., HTTP and SIP). The detection is based on a number of features associated with the communication flow, for example, source and destination ports and bytes transmitted per packet. NTC is important, because much information about a current network flow can be learned and anticipated just by knowing its network service (required latency, traffic volume, and possible duration). This is of particular interest for the management and monitoring of Internet of Things (IoT) networks, where NTC will help to segregate traffic and behavior of heterogeneous devices and services. In this paper, we present a new technique for NTC based on a combination of deep learning models that can be used for IoT traffic. We show that a recurrent neural network (RNN) combined with a convolutional neural network (CNN) provides best detection results. The natural domain for a CNN, which is image processing, has been extended to NTC in an easy and natural way. We show that the proposed method provides better detection results than alternative algorithms without requiring any feature engineering, which is usual when applying other models. A complete study is presented on several architectures that integrate a CNN and an RNN, including the impact of the features chosen and the length of the network flows used for training.","","","10.1109/ACCESS.2017.2747560","Ministerio de Economía y Competitividad del Gobierno de España and the Fondo de Desarrollo Regional (FEDER) within the project “Inteligencia distribuida para el control y adaptación de redes dinámicas definidas por software, Ref: TIN2014-57991-C3-2-P,”; Ministerio de Economía y Competitividad in the Programa Estatal de Fomento de la Investigación Científica y Técnica de Excelencia, Subprograma Estatal de Generación de Conocimiento within the Project “Distribucion inteligente de servicios multimedia utilizando redes cognitivas adaptativas definidas por software, Ref: TIN2014-57991-C3-1-P.”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8026581","Convolutional neural network;deep learning;network traffic classification;recurrent neural network","Ports (Computers);Telecommunication traffic;Feature extraction;Recurrent neural networks;Machine learning;Payloads;Biological neural networks","Internet of Things;learning (artificial intelligence);recurrent neural nets;telecommunication computing;telecommunication traffic","network traffic classifier;convolutional networks;recurrent neural networks;NTC;current network monitoring systems;network service;communication flow;current network flow;traffic volume;heterogeneous devices;IoT traffic;convolutional neural network;Internet of Things networks;CNN;deep learning models","","43","38","CCBY","","","","IEEE","IEEE Journals"
"Crosslingual and Multilingual Speech Recognition Based on the Speech Manifold","R. Sahraeian; D. Van Compernolle","Center of Processing Speech and Image, Department of Electrical Engineering, Katholieke Universiteit Leuven, 3000 Leuven, Belgium; Center of Processing Speech and Image, Department of Electrical Engineering, Katholieke Universiteit Leuven, 3000 Leuven, Belgium","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","12","2301","2312","Speech signals are produced by the smooth and continuous movements of the human articulators. An articulatory representation of speech is considered to be a more compact, more universal, and language-independent speech feature space and can, therefore, improve crosslingual and multilingual speech recognition systems, especially when porting components from one language to another in low-resource scenarios. However, learning the acoustic-to-articulatory conversion has proven to be a very challenging task. In this paper, we utilize a manifold learning technique to derive a nonlinear feature transformation from the conventional filterbank feature space to an articulatory-like feature space. The coordinates in the resultant representation of which some have demonstrable phonological meaning are shown to be highly portable across languages. We propose a proper framework in terms of data selection and graph construction to train coordinates from multilingual data, which allows for training the coordinate space when we have abundant out-of-language data. Deep neural network (DNN) bottleneck features are demonstrated to exhibit a greater degree of language independence when using this representation than in the case of filterbank features as inputs. The usability of this representation is further demonstrated in a number of speech recognition experiments using DNNs in a variety of crosslingual and multilingual scenarios using the multilingual GlobalPhone dataset. Especially, speech recognition systems developed in low-resource settings profit from the improved portability across languages.","","","10.1109/TASLP.2017.2751747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8114354","Crosslingual and multilingual speech recognition;acoustic-to-articulatory mapping;manifold learning;deep neural networks","Biology;Speech recognition;Acoustics;Neural networks;Manifolds;Filter banks;Learning systems;Natural languages","feature selection;graph theory;learning (artificial intelligence);natural language processing;neural nets;speech recognition","multilingual GlobalPhone dataset;multilingual speech recognition;speech signals;acoustic-to-articulatory conversion;nonlinear feature transformation;articulatory-like feature space;data selection;graph construction;multilingual data;deep neural network;manifold learning;language-independent speech feature space;filterbank feature space;crosslingual speech recognition systems","","3","61","Traditional","","","","IEEE","IEEE Journals"
"No Bot Expects the DeepCAPTCHA! Introducing Immutable Adversarial Examples, With Applications to CAPTCHA Generation","M. Osadchy; J. Hernandez-Castro; S. Gibson; O. Dunkelman; D. Pérez-Cabo","Computer Science Department, University of Haifa, Haifa, Israel; School of Computing, University of Kent, Canterbury, U.K.; School of Physical Sciences, University of Kent, Canterbury, U.K.; Computer Science Department, University of Haifa, Haifa, Israel; Gradiant, Campus Universitario de Vigo, Vigo, Spain","IEEE Transactions on Information Forensics and Security","","2017","12","11","2640","2653","Recent advances in deep learning (DL) allow for solving complex AI problems that used to be considered very hard. While this progress has advanced many fields, it is considered to be bad news for Completely Automated Public Turing tests to tell Computers and Humans Apart (CAPTCHAs), the security of which rests on the hardness of some learning problems. In this paper, we introduce DeepCAPTCHA, a new and secure CAPTCHA scheme based on adversarial examples, an inherit limitation of the current DL networks. These adversarial examples are constructed inputs, either synthesized from scratch or computed by adding a small and specific perturbation called adversarial noise to correctly classified items, causing the targeted DL network to misclassify them. We show that plain adversarial noise is insufficient to achieve secure CAPTCHA schemes, which leads us to introduce immutable adversarial noise-an adversarial noise that is resistant to removal attempts. In this paper, we implement a proof of concept system, and its analysis shows that the scheme offers high security and good usability compared with the best previously existing CAPTCHAs.","","","10.1109/TIFS.2017.2718479","U.K. Engineering and Physical Sciences Research Council; Israeli Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954632","CAPTCHA;deep learning;CNN;adversarial examples;HIP","CAPTCHAs;Security;Robustness;Machine learning;Tools;Usability","authorisation;learning (artificial intelligence);Turing machines","deepCAPTCHA generation;immutable adversaries;deep learning;Completely Automated Public Turing tests-to-tell-Computers-and-Humans Apart;completely secure CAPTCHA scheme;DL networks;immutable adversarial noise","","8","53","CCBY","","","","IEEE","IEEE Journals"
"FusionNet: Edge Aware Deep Convolutional Networks for Semantic Segmentation of Remote Sensing Harbor Images","D. Cheng; G. Meng; S. Xiang; C. Pan","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","12","5769","5783","Sea-land segmentation and ship detection are two prevalent research domains for optical remote sensing harbor images and can find many applications in harbor supervision and management. As the spatial resolution of imaging technology improves, traditional methods struggle to perform well due to the complicated appearance and background distributions. In this paper, we unify the above two tasks into a single framework and apply the deep convolutional neural networks to predict pixelwise label for an input. Specifically, an edge aware convolutional network is proposed to parse a remote sensing harbor image into three typical objects, e.g., sea, land, and ship. Two innovations are made on top of the deep structure. First, we design a multitask model by simultaneously training the segmentation and edge detection networks. Hierarchical semantic features from the segmentation network are extracted to learn the edge network. Second, the outputs of edge pipeline are further employed to refine entire model by adding an edge aware regularization, which helps our method to yield very desirable results that are spatially consistent and well boundary located. It also benefits the segmentation of docked ships that are quite challenging for many previous methods. Experimental results on two datasets collected from Google Earth have demonstrated the effectiveness of our approach both in quantitative and qualitative performance compared with state-of-the-art methods.","","","10.1109/JSTARS.2017.2747599","National 863 projects; National Natural Science Foundation of China; Beijing Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8047295","Edge aware regularization;harbor images;multitask learning;semantic segmentation","Marine vehicles;Image segmentation;Feature extraction;Image edge detection;Semantics;Remote sensing","convolution;edge detection;feature extraction;geophysical image processing;image resolution;image segmentation;learning (artificial intelligence);neural nets;object detection;remote sensing","edge aware deep convolutional networks;semantic segmentation;remote sensing harbor image;sea-land segmentation;ship detection;optical remote sensing harbor images;deep convolutional neural networks;edge aware convolutional network;edge detection networks;segmentation network;FusionNet;spatial resolution;imaging technology","","6","55","Traditional","","","","IEEE","IEEE Journals"
"Adapting Remote Sensing to New Domain With ELM Parameter Transfer","S. Xu; X. Mu; D. Chai; S. Wang","Xi&#x2019;an Research Institute of Hi-Tech, Xi&#x2019;an, China; Xi&#x2019;an Research Institute of Hi-Tech, Xi&#x2019;an, China; Beijing Aeronautical Technology Research Center, Beijing, China; Xi&#x2019;an Research Institute of Hi-Tech, Xi&#x2019;an, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","9","1618","1622","It is time consuming to annotate unlabeled remote sensing images. One strategy is taking the labeled remote sensing images from another domain as training samples, and the target remote sensing labels are predicted by supervised classification. However, this may lead to negative transfer due to the distribution difference between the two domains. To address this issue, we propose a novel domain adaptation method through transferring the parameters of extreme learning machine (ELM). The core of this method is learning a transformation to map the target ELM parameters to the source, making the classifier parameters of the target domain maximally aligned with the source. Our method has several advantages which was previously unavailable within a single method: multiclass adaptation through parameter transferring, learning the final classifier and transformation simultaneously, and avoiding negative transfer. We perform experiments on three data sets that indicate improved accuracy and computational advantages compared to baseline approaches.","","","10.1109/LGRS.2017.2726760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7999281","Deep feature;domain adaptation;extreme learning machine (ELM);middle-level feature;regularization;transfer learning","Remote sensing;Training;Feature extraction;Linear programming;Learning systems;Transforms;Neural networks","geophysical image processing;image classification;learning (artificial intelligence);remote sensing","remote sensing;ELM parameter transfer;unlabeled remote sensing images;target remote sensing labels;negative transfer;extreme learning machine;map transformation;classifier parameters;target domain;final classifier","","3","20","","","","","IEEE","IEEE Journals"
"A Bottom-Up Approach for Pancreas Segmentation Using Cascaded Superpixels and (Deep) Image Patch Labeling","A. Farag; L. Lu; H. R. Roth; J. Liu; E. Turkbey; R. M. Summers","Department of Radiology and Imaging Sciences, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health Clinical Center, Bethesda, MD, USA; Department of Radiology and Imaging Sciences, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health Clinical Center, Bethesda, MD, USA; Nagoya University, Nagoya, Japan; Department of Radiology and Imaging Sciences, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health Clinical Center, Bethesda, MD, USA; Clinical Image Processing Service, National Institutes of Health Clinical Center, Bethesda, MD, USA; Department of Radiology and Imaging Sciences, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health Clinical Center, Bethesda, MD, USA","IEEE Transactions on Image Processing","","2017","26","1","386","399","Robust organ segmentation is a prerequisite for computer-aided diagnosis, quantitative imaging analysis, pathology detection, and surgical assistance. For organs with high anatomical variability (e.g., the pancreas), previous segmentation approaches report low accuracies, compared with well-studied organs, such as the liver or heart. We present an automated bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans. The method generates a hierarchical cascade of information propagation by classifying image patches at different resolutions and cascading (segments) superpixels. The system contains four steps: 1) decomposition of CT slice images into a set of disjoint boundary-preserving superpixels; 2) computation of pancreas class probability maps via dense patch labeling; 3) superpixel classification by pooling both intensity and probability features to form empirical statistics in cascaded random forest frameworks; and 4) simple connectivity based post-processing. Dense image patch labeling is conducted using two methods: efficient random forest classification on image histogram, location and texture features; and more expensive (but more accurate) deep convolutional neural network classification, on larger image windows (i.e., with more spatial contexts). Over-segmented 2-D CT slices by the simple linear iterative clustering approach are adopted through model/parameter calibration and labeled at the superpixel level for positive (pancreas) or negative (non-pancreas or background) classes. The proposed method is evaluated on a data set of 80 manually segmented CT volumes, using six-fold cross-validation. Its performance equals or surpasses other state-of-the-art methods (evaluated by “leave-one-patient-out”), with a dice coefficient of 70.7% and Jaccard index of 57.9%. In addition, the computational efficiency has improved significantly, requiring a mere 6 ~ 8 min per testing case, versus ≥ 10 h for other methods. The segmentation framework using deep patch labeling confidences is also more numerically stable, as reflected in the smaller performance metric standard deviations. Finally, we implement a multi-atlas label fusion (MALF) approach for pancreas segmentation using the same data set. Under six-fold cross-validation, our bottom-up segmentation method significantly outperforms its MALF counterpart: 70.7±13.0% versus 52.51±20.84% in dice coefficients.","","","10.1109/TIP.2016.2624198","Intramural Research Program of the National Institutes of Health Clinical Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727966","Abdominal computed tomography (CT);deep convolutional neural networks;dense image patch labeling;cascaded random forest;pancreas segmentation","Pancreas;Image segmentation;Computed tomography;Labeling;Shape;Liver","biological organs;computerised tomography;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;neural nets;probability","bottom-up approach;pancreas segmentation;cascaded superpixels;deep image patch labeling;computer-aided diagnosis;quantitative imaging analysis;pathology detection;surgical assistance;abdominal computed tomography scans;CT slice image decomposition;disjoint boundary-preserving superpixels;pancreas class probability maps;dense patch labeling;superpixel classification;probability features;random forest classification;image histogram;image location;texture features;deep convolutional neural network classification;linear iterative clustering approach;model-parameter calibration;multiatlas label fusion approach","","28","49","","","","","IEEE","IEEE Journals"
"Deep-Sparse-Representation-Based Features for Speech Recognition","P. Sharma; V. Abrol; A. K. Sao","Multimedia Analytics and Systems Lab., School of Computing and Electrical Engineering, Indian Institute of Technology Mandi, Mandi, India; Multimedia Analytics and Systems Lab., School of Computing and Electrical Engineering, Indian Institute of Technology Mandi, Mandi, India; Multimedia Analytics and Systems Lab., School of Computing and Electrical Engineering, Indian Institute of Technology Mandi, Mandi, India","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","11","2162","2175","Features derived using sparse representation (SR)-based approaches have been shown to yield promising results for speech recognition tasks. In most of the approaches, the SR corresponding to speech signal is estimated using a dictionary, which could be either exemplar based or learned. However, a single-level decomposition may not be suitable for the speech signal, as it contains complex hierarchical information about various hidden attributes. In this paper, we propose to use a multilevel decomposition (having multiple layers), also known as the deep sparse representation (DSR), to derive a feature representation for speech recognition. Instead of having a series of sparse layers, the proposed framework employs a dense layer between two sparse layers, which helps in efficient implementation. Our studies reveal that the representations obtained at different sparse layers of the proposed DSR model have complimentary information. Thus, the final feature representation is derived after concatenating the representations obtained at the sparse layers. This results in a more discriminative representation, and improves the speech recognition performance. Since the concatenation results in a high-dimensional feature, principal component analysis is used to reduce the dimension of the obtained feature. Experimental studies demonstrate that the proposed feature outperforms existing features for various speech recognition tasks.","","","10.1109/TASLP.2017.2748240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8023884","Deep sparse representation (DSR);dictionary learning;speech recognition","Speech;Speech recognition;Dictionaries;Machine learning;Data models;Speech processing;Principal component analysis","principal component analysis;speech recognition","deep sparse representation;speech recognition;speech signal;single-level decomposition;complex hierarchical information;hidden attributes;multilevel decomposition;DSR;complimentary information;final feature representation;discriminative representation;speech recognition performance;principal component analysis","","3","50","Traditional","","","","IEEE","IEEE Journals"
"A Joint Deep Boltzmann Machine (jDBM) Model for Person Identification Using Mobile Phone Data","M. R. Alam; M. Bennamoun; R. Togneri; F. Sohel","Department of Computer Science and Software Engineering, University of Western Australia, Crawley, WA, Australia; Department of Computer Science and Software Engineering, University of Western Australia, Crawley, WA, Australia; Department of Electrical, Electronics, and Computer Engineering, University of Western Australia, Crawley, WA, Australia; School of Engineering and Information Technology, Murdoch University, Murdoch, WA, Australia","IEEE Transactions on Multimedia","","2017","19","2","317","326","We propose an audio-visual person identification approach based on a joint deep Boltzmann machine (jDBM) model. The proposed jDBM model is trained in three steps: 1) learning the unimodal DBM models corresponding to the speech and facial image modalities, 2) learning the shared layer parameters using a joint restricted Boltzmann machine (jRBM) model, and 3) the fine-tuning of the jDBM model after the initialization with the parameters of the unimodal DBMs and the shared layer. The activation probabilities of the units of the shared layer are used as the joint features and a logistic regression classifier is used for the combined speech and facial image recognition. We show that by learning the shared layer parameters using a jRBM, a higher accuracy can be achieved compared to the greedy layer-wise initialization. The performance of our proposed model is also compared with a state-of-the art support vector machine (SVM), deep belief network (DBN), and the deep auto-encoder (DAE) models. In addition, our experimental results show that the joint representations obtained from the proposed jDBM model are robust to noise and missing information. Experiments were carried out on the challenging MOBIO database, which includes audio-visual data captured using mobile phones.","","","10.1109/TMM.2016.2615524","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7583729","Audio-visual biometrics;deep Boltzmann machines;joint features","Biometrics (access control);Speech recognition;Training;Speech;Mobile handsets;Robustness;Data models","Boltzmann machines;face recognition;greedy algorithms;mobile handsets;regression analysis;speech recognition;support vector machines;visual databases","joint deep Boltzmann machine model;mobile phone data;audiovisual person identification approach;jDBM model;joint restricted Boltzmann machine;jRBM model;shared layer parameters;speech modalities;facial image modalities;activation probabilities;logistic regression classifier;speech recognition;facial image recognition;greedy layer-wise initialization;support vector machine;SVM;deep belief network;DBN;deep auto-encoder models;DAE models;MOBIO database","","8","33","","","","","IEEE","IEEE Journals"
"Discriminative Feature Learning for Unsupervised Change Detection in Heterogeneous Images Based on a Coupled Neural Network","W. Zhao; Z. Wang; M. Gong; J. Liu","School of Computer Science and Technology, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","12","7066","7080","With the application requirement, the technique for change detection based on heterogeneous remote sensing images is paid more attention. However, detecting changes between two heterogeneous images is challenging as they cannot be compared in low-dimensional space. In this paper, we construct an approximately symmetric deep neural network with two sides containing the same number of coupled layers to transform the two images into the same feature space. The two images are connected with the two sides and transformed into the same feature space, in which their features are more discriminative and the difference image can be generated by comparing paired features pixel by pixel. The network is first built by stacked restricted Boltzmann machines, and then, the parameters are updated in a special way based on clustering. The special way, motivated by that two heterogeneous images share the same reality in unchanged areas and retain respective properties in changed areas, shrinks the distance between paired features transformed from unchanged positions, and enlarges the distance between paired features extracted from changed positions. It is achieved through introducing two types of labels and updating parameters by adaptively changed learning rate. This is different from the existing methods based on deep learning that just do operations on positions predicted to be unchanged and extract only one type of labels. The whole process is completely unsupervised without any priori knowledge. Besides, the method can also be applied to homogeneous images. We test our method on heterogeneous images and homogeneous images. The proposed method achieves quite high accuracy.","","","10.1109/TGRS.2017.2739800","National Natural Science Foundation of China; National Program for Support of Top-Notch Young Professionals of China; Shaanxi Major Basic Research Project of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8024160","Change detection;deep neural network;features extracting;heterogeneous images","Feature extraction;Neural networks;Remote sensing;Image sensors;Synthetic aperture radar;Optical sensors","Boltzmann machines;feature extraction;image classification;learning (artificial intelligence);remote sensing;transforms","homogeneous images;discriminative feature learning;unsupervised change detection;coupled neural network;heterogeneous remote sensing images;deep neural network;features extraction;restricted Boltzmann machines;image change detection","","10","39","","","","","IEEE","IEEE Journals"
"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation","V. Badrinarayanan; A. Kendall; R. Cipolla","Department of Engineering, Machine Intelligence Lab, University of Cambridge, Cambridge, United Kingdom; Department of Engineering, Machine Intelligence Lab, University of Cambridge, Cambridge, United Kingdom; Department of Engineering, Machine Intelligence Lab, University of Cambridge, Cambridge, United Kingdom","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","12","2481","2495","We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.","","","10.1109/TPAMI.2016.2644615","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7803544","Deep convolutional neural networks;semantic pixel-wise segmentation;indoor scenes;road scenes;encoder;decoder;pooling;upsampling","Decoding;Neural networks;Training;Computer architecture;Image segmentation;Semantics;Convolutional codes","feature extraction;gradient methods;image classification;image colour analysis;image representation;image resolution;image segmentation;inference mechanisms;learning (artificial intelligence);self-organising feature maps;topology","convolutional layers;SegNet;decoder network;low resolution encoder feature maps;DeconvNet architectures;DeepLab-LargeFOV;FCN;stochastic gradient descent;SUN RGB-D indoor scene segmentation;competitive inference time;Caffe implementation;SUN RGB-D indoor scene segmentation tasks;dense feature maps;nonlinear upsampling;max-pooling step;lower resolution input feature map;VGG16 network;pixel-wise classification layer;encoder network;core trainable segmentation engine;pixel-wise segmentation;practical deep fully convolutional neural network architecture;image segmentation;deep convolutional encoder-decoder architecture","","557","69","CCBY","","","","IEEE","IEEE Journals"
"Deep Network Analyzer (DNA): A Big Data Analytics Platform for Cellular Networks","K. Yang; R. Liu; Y. Sun; J. Yang; X. Chen","Department of Computer Science, Tongji University, Shanghai, China; Huawei Research USA, Bridgewater, NJ, USA; Huawei Research USA, Bridgewater, NJ, USA; Huawei Research USA, Bridgewater, NJ, USA; Huawei Research USA, Bridgewater, NJ, USA","IEEE Internet of Things Journal","","2017","4","6","2019","2027","In this paper, we present deep network analyzer (DNA), a big data analytics platform for anomaly detection (AD) and root cause analysis (RCA) in mobile wireless networks. DNA is motivated by the growing scale and complexity of cellular networks along with the lack of advanced big data analytics tools for effective network management. It abstracts the RCA process into two modules, namely rule (fingerprint) learning and the module of AD and fingerprint matching. We first develop a rare association rule mining method to learn the symptoms of network anomalies and to build a fingerprint knowledge database from the historic data. Then a statistical machine learning approach is employed to identify the anomalies within the incoming dataset collected via various probes in the network and map the fingerprints of the detected anomalies to the rules in the knowledge database. The DNA platform has been tested using the real production data from the field and has been shown to be a highly effective platform for AD and RCA for large-scale cellular systems serving tens of millions of mobile users.","","","10.1109/JIOT.2016.2624761","China Youth 1000-Talent Research Grant; Tongji Initiative Research Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733158","Big data;fault diagnosis;Internet of Things;machine learning algorithms;wireless networks","Databases;Training;Big Data;Cellular networks;Wireless networks;DNA","Big Data;cellular radio;data analysis;data mining;learning (artificial intelligence);telecommunication computing","mobile wireless networks;cellular networks;effective network management;rare association rule mining method;network anomalies;fingerprint knowledge database;historic data;statistical machine learning approach;detected anomalies;DNA platform;production data;large-scale cellular systems;deep network analyzer;root cause analysis;fingerprints;Big Data analytics platform;advanced Big Data analytics tools","","8","22","","","","","IEEE","IEEE Journals"
"Object-Based Convolutional Neural Network for High-Resolution Imagery Classification","W. Zhao; S. Du; W. J. Emery","Beijing Key Laboratory of Spatial Information Integration and Its Applications, Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, China; Beijing Key Laboratory of Spatial Information Integration and Its Applications, Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, China; Colorado Center for Astrodynamics Research, University of Colorado, Boulder, CO, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","7","3386","3396","Timely and accurate classification and interpretation of high-resolution images are very important for urban planning and disaster rescue. However, as spatial resolution gets finer, it is increasingly difficultto recognize complex patterns in high-resolution remote sensing images. Deep learning offers an efficient strategy to fill the gap between complex image patterns and their semantic labels. However, due to the hierarchical abstract nature of deep learning methods, it is difficult to capture the precise outline of different objects at the pixel level. To further reduce this problem, we propose an object-based deep learning method to accurately classify the high-resolution imagery without intensive human involvement. In this study, high-resolution images were used to accurately classify three different urban scenes: Beijing (China), Pavia (Italy), and Vaihingen (Germany). The proposed method is built on a combination of a deep feature learning strategy and an object-based classification for the interpretation of high-resolution images. Specifically, high-level feature representations extracted through the convolutional neural networks framework have been systematically investigated over five different layer configurations. Furthermore, to improve the classification accuracy, an object-based classification method also has been integrated with the deep learning strategy for more efficient image classification. Experimental results indicate that with the combination of deep learning and object-based classification, it is possible to discriminate different building types in Beijing Scene, such as commercial buildings and residential buildings with classification accuracies above 90%.","","","10.1109/JSTARS.2017.2680324","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890382","Convolutional neural network (CNN);deep learning;high-resolution image;image classification","Machine learning;Feature extraction;Remote sensing;Image segmentation;Neural networks;Spatial resolution;Robustness","convolution;geophysical image processing;image classification;neural nets;object-oriented methods;remote sensing;town and country planning","object-based convolutional neural network;high-resolution imagery classification;urban planning;disaster rescue;high-resolution remote sensing images;deep learning;semantic labels;Beijing;China;Pavia;Italy;Vaihingen;Germany;commercial buildings;residential buildings","","17","39","","","","","IEEE","IEEE Journals"
"Optimizing the efficiency of deep learning through accelerator virtualization","M. Gschwind; T. Kaldewey; D. K. Tam","NA; NA; NA","IBM Journal of Research and Development","","2017","61","4/5","12:1","12:11","Training deep learning models often occupies entire compute clusters, built solely for this purpose, for days or even weeks at a time. There exists a large body of work on approaches for improving training performance, ranging from novel algorithms to full custom hardware accelerators. Offering compute capabilities of multiple teraflops (trillion floating point operations per second), graphics processing units (GPUs) have established themselves as a de-facto standard for accelerating deep learning network training. As systems with up to 16 GPUs—each GPU consuming up to 300 W—become available, efficient usage of these resources becomes imperative. We conduct a detailed analysis of deep learning workloads to characterize their efficiency in making use of GPU acceleration. We found that many deep learning workloads consume only a fraction of GPU resources, and we demonstrate how sharing GPU resources can improve throughput by a factor of 3, effectively turning a 4-GPU commodity cloud system into a high-end 12-GPU supercomputer. Using Watson workloads from three major areas that incorporate deep learning technology—i.e., language classification, visual recognition, and speech recognition—we document the effectiveness and scalability of our approach. We are working toward enabling GPU virtualization not only to reduce cost, but also to accelerate new breakthroughs in deep learning by increasing compute capacity without making further hardware investments.","","","10.1147/JRD.2017.2716598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030299","","Training;Graphics processing units;Acceleration;Speech recognition;Machine learning;Virtualization;Hardware","","","","2","41","","","","","IBM","IBM Journals"
"Wildfire: Approximate synchronization of parameters in distributed deep learning","R. Nair; S. Gupta","NA; NA","IBM Journal of Research and Development","","2017","61","4/5","7:1","7:9","In distributed deep learning approaches, contributions to changes in the parameter values from multiple learners are gathered at periodic intervals and collectively used to update weights associated with the learning network. Gathering these contributions at a centralized location, as in common synchronous parameter server models, causes a bottleneck in two ways. First, the parameter server needs to wait until gradients from all learners have been received, and second, the traffic pattern of the gradients between the learners and the parameter server causes an imbalance in bandwidth utilization in most common networks. In this paper, we introduce a scheme called Wildfire, which communicates weights among subsets of parallel learners, each of which updates its model using only the information received from other learners in the subset. Different subsets of learners communicate at different times, allowing the learning to diffuse through the system. Wildfire reduces the communication overhead in deep learning by allowing learners to communicate directly among themselves rather than through a parameter server, and by limiting the time that learners need to wait before updating their models. We demonstrate the effectiveness of Wildfire on common deep learning benchmarks, using the IBM Rudra deep learning framework.","","","10.1147/JRD.2017.2709198","Defense Advanced Research Projects Agency (DARPA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030302","","Neural networks;Servers;Training;Computational modeling;Synchronization;Machine learning;Bandwidth","","","","2","31","","","","","IBM","IBM Journals"
"Hyperspectral Band Selection Based on Deep Convolutional Neural Network and Distance Density","Y. Zhan; D. Hu; H. Xing; X. Yu","College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","12","2365","2369","In this letter, a band-selection approach based on the deep convolutional neural network (CNN) and distance density (DD) is proposed. This method effectively mitigates the curse of dimensionality for hyperspectral images (HSIs). First, we use the hyperspectral full-band data to train a custom 1-D CNN to obtain a well-trained model. Second, we select band combinations based on DD. Using the rectified linear unit, which is the activation function of the CNN that is only activated with a nonzero value, we can effectively test the band combinations without retraining the model. Finally, the method selects the band combinations with the highest precision as the final selected bands. This precision measure is a new criterion for band selection. To further improve the performance, a data augmentation method based on DD is also proposed. To justify the effectiveness of the proposed method, experiments are conducted on two HSIs. The results show that the proposed method can acquire more satisfactory results than traditional methods.","","","10.1109/LGRS.2017.2765339","Ministry of Land and Resources for the Public Welfare Industry Research Special Funds; National Natural Science Foundation of China; Natural Science Foundation of Hainan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8113688","1-D convolution;band selection;convolutional neural network (CNN);deep learning;distance density (DD);hyperspectral imagery","Hyperspectral imaging;Training;Data models;Feature extraction;Hidden Markov models","convolution;feature selection;hyperspectral imaging;image classification;learning (artificial intelligence);neural nets","hyperspectral band selection;deep convolutional neural network;distance density;band-selection approach;CNN;DD;hyperspectral images;hyperspectral full-band data;data augmentation method;HSIs;band combinations selection;activation function;precision measure","","9","17","","","","","IEEE","IEEE Journals"
"Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks","Y. Chen; T. Krishna; J. S. Emer; V. Sze","Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE Journal of Solid-State Circuits","","2017","52","1","127","138","Eyeriss is an accelerator for state-of-the-art deep convolutional neural networks (CNNs). It optimizes for the energy efficiency of the entire system, including the accelerator chip and off-chip DRAM, for various CNN shapes by reconfiguring the architecture. CNNs are widely used in modern AI systems but also bring challenges on throughput and energy efficiency to the underlying hardware. This is because its computation requires a large amount of data, creating significant data movement from on-chip and off-chip that is more energy-consuming than computation. Minimizing data movement energy cost for any CNN shape, therefore, is the key to high throughput and energy efficiency. Eyeriss achieves these goals by using a proposed processing dataflow, called row stationary (RS), on a spatial architecture with 168 processing elements. RS dataflow reconfigures the computation mapping of a given shape, which optimizes energy efficiency by maximally reusing data locally to reduce expensive data movement, such as DRAM accesses. Compression and data gating are also applied to further improve energy efficiency. Eyeriss processes the convolutional layers at 35 frames/s and 0.0029 DRAM access/multiply and accumulation (MAC) for AlexNet at 278 mW (batch size N = 4), and 0.7 frames/s and 0.0035 DRAM access/MAC for VGG-16 at 236 mW (N = 3).","","","10.1109/JSSC.2016.2616357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7738524","Convolutional neural networks (CNNs);dataflow processing;deep learning;energy-efficient accelerators;spatial architecture","Shape;Random access memory;Computer architecture;Throughput;Clocks;Neural networks;Hardware","data flow computing;DRAM chips;energy conservation;feedforward neural nets;learning (artificial intelligence);neural net architecture;power aware computing;reconfigurable architectures","AlexNet;MAC;multiply and accumulation;convolutional layers;DRAM accesses;RS dataflow reconfiguration;spatial architecture;dataflow processing;row stationary;energy efficiency;data movement energy cost;AI systems;reconfiguring architecture;off-chip DRAM;accelerator chip;CNN shapes;deep convolutional neural networks;energy-efficient reconfigurable accelerator;Eyeriss","","390","36","","","","","IEEE","IEEE Journals"
"A Fast Feature Fusion Algorithm in Image Classification for Cyber Physical Systems","Y. Wang; B. Song; P. Zhang; N. Xin; G. Cao","State Key Laboratory of Integrated Services Networks, Xidian University, China; State Key Laboratory of Integrated Services Networks, Xidian University, China; State Key Laboratory of Integrated Services Networks, Xidian University, China; Institute of Telecommunication Satellite, China Academy of Space Technology, Beijing, China; Institute of Telecommunication Satellite, China Academy of Space Technology, Beijing, China","IEEE Access","","2017","5","","9089","9098","Collaborative applications of physical systems and algorithms bring the rapid development of cyber physical systems (CPS). Establishing CPS with image classification systems, however, is difficult, because both categories of algorithms, deep learning methods and traditional feature extraction methods, are independent and individual currently. Therefore, in this paper, we propose a fast feature fusion algorithm to satisfy the requirement of CPS in the area of image classification from a comprehensive perspective. First, we fuse the shallow-layer network feature, large pre-trained convolutional neural network feature and traditional image features together by genetic algorithm, in order to guarantee high accuracy with little training time and hardware cost. Second, we increase the distance between different centers by dynamic weight assignment to improve distinguishability of different classes. Third, we propose a partial selection method to reduce the length of the fused feature vectors and to improve the classification accuracy further by centralizing the features within the same class. Finally, experimental results show that our method can achieve high classification accuracy with lower training time and hardware consumption, which can greatly improve the efficiency and flexibility of image classification in cyber physical systems.","","","10.1109/ACCESS.2017.2705798","National Natural Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7935501","Cyber physical systems;image classification;feature fusion;deep learning;genetic algorithm;partial selection","Training;Feature extraction;Image classification;Machine learning;Classification algorithms;Fuses;Biological system modeling","cyber-physical systems;feature extraction;genetic algorithms;image classification;image fusion;learning (artificial intelligence)","partial selection method;dynamic weight assignment;genetic algorithm;convolutional neural network feature;shallow-layer network feature;feature extraction methods;deep learning methods;image classification systems;CPS;collaborative applications;cyber physical systems;fast feature fusion algorithm","","5","25","","","","","IEEE","IEEE Journals"
"A Neural Word Embeddings Approach for Multi-Domain Sentiment Analysis","M. Dragoni; G. Petrucci","Fondazione Bruno Kessler, University of Trento, Trento, TN, Italy; Fondazione Bruno Kessler, University of Trento, Trento, TN, Italy","IEEE Transactions on Affective Computing","","2017","8","4","457","470","Multi-domain sentiment analysis consists in estimating the polarity of a given text by exploiting domain-specific information. One of the main issues common to the approaches discussed in the literature is their poor capabilities of being applied on domains which are different from those used for building the opinion model. In this paper, we will present an approach exploiting the linguistic overlap between domains to build sentiment models supporting polarity inference for documents belonging to every domain. Word embeddings together with a deep learning architecture have been implemented into the NeuroSent tool for enabling the building of multi-domain sentiment model. The proposed technique is validated by following the Dranziera protocol in order to ease the repeatability of the experiments and the comparison of the results. The outcomes demonstrate the effectiveness of the proposed approach and also set a plausible starting point for future work.","","","10.1109/TAFFC.2017.2717879","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954756","Sentiment analysis;natural language processing;neural networks;multi-domain sentiment analysis;deep learning","Sentiment analysis;Machine learning;Analytical models;Computer architecture;Context awareness;Social network services","computational linguistics;learning (artificial intelligence);natural language processing;pattern classification;sentiment analysis;text analysis","multidomain sentiment analysis;domain-specific information;opinion model;polarity inference;deep learning architecture;neural word embedding approach","","8","50","Traditional","","","","IEEE","IEEE Journals"
"Multichannel Signal Processing With Deep Neural Networks for Automatic Speech Recognition","T. N. Sainath; R. J. Weiss; K. W. Wilson; B. Li; A. Narayanan; E. Variani; M. Bacchiani; I. Shafran; A. Senior; K. Chin; A. Misra; C. Kim","Google, New York, NY, USA; Google, New York, NY, USA; Google, New York, NY, USA; Google, Inc., Mountain View, CA, USA; Google, Inc., Mountain View, CA, USA; Google, Inc., Mountain View, CA, USA; Google, New York, NY, USA; Google, Inc., Mountain View, CA, USA; Google, New York, NY, USA; Google, Inc., Mountain View, CA, USA; Google, Inc., Mountain View, CA, USA; Google, Inc., Mountain View, CA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","5","965","979","Multichannel automatic speech recognition (ASR) systems commonly separate speech enhancement, including localization, beamforming, and postfiltering, from acoustic modeling. In this paper, we perform multichannel enhancement jointly with acoustic modeling in a deep neural network framework. Inspired by beamforming, which leverages differences in the fine time structure of the signal at different microphones to filter energy arriving from different directions, we explore modeling the raw time-domain waveform directly. We introduce a neural network architecture, which performs multichannel filtering in the first layer of the network, and show that this network learns to be robust to varying target speaker direction of arrival, performing as well as a model that is given oracle knowledge of the true target speaker direction. Next, we show how performance can be improved by factoring the first layer to separate the multichannel spatial filtering operation from a single channel filterbank which computes a frequency decomposition. We also introduce an adaptive variant, which updates the spatial filter coefficients at each time frame based on the previous inputs. Finally, we demonstrate that these approaches can be implemented more efficiently in the frequency domain. Overall, we find that such multichannel neural networks give a relative word error rate improvement of more than 5% compared to a traditional beamforming-based multichannel ASR system and more than 10% compared to a single channel waveform model.","","","10.1109/TASLP.2017.2672401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859320","Beamforming;deep learning;noise-robust speech recognition","Acoustics;Neural networks;Array signal processing;Speech;Microphone arrays;Training","array signal processing;channel bank filters;direction-of-arrival estimation;microphones;neural nets;spatial filters;speaker recognition;speech enhancement","frequency domain;single channel filterbank;multichannel spatial filtering operation;target speaker direction of arrival;multichannel filtering;raw time-domain waveform;deep neural network framework;acoustic modeling;multichannel enhancement;postfiltering;beamforming;speech enhancement;ASR system;automatic speech recognition;deep neural network;multichannel signal processing","","40","44","","","","","IEEE","IEEE Journals"
"Unsupervised Sequential Outlier Detection With Deep Architectures","W. Lu; Y. Cheng; C. Xiao; S. Chang; S. Huang; B. Liang; T. Huang","Department of Automation, Tsinghua University, Beijing, China; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; Department of Industrial and Systems Engineering, University of Washington, Seattle, WA, USA; Department of Automation, Tsinghua University, Beijing, China; Beckman Institute, University of Illinois at Urbana–Champaign, IL, USA","IEEE Transactions on Image Processing","","2017","26","9","4321","4330","Unsupervised outlier detection is a vital task and has high impact on a wide variety of applications domains, such as image analysis and video surveillance. It also gains long-standing attentions and has been extensively studied in multiple research areas. Detecting and taking action on outliers as quickly as possible are imperative in order to protect network and related stakeholders or to maintain the reliability of critical systems. However, outlier detection is difficult due to the one class nature and challenges in feature construction. Sequential anomaly detection is even harder with more challenges from temporal correlation in data, as well as the presence of noise and high dimensionality. In this paper, we introduce a novel deep structured framework to solve the challenging sequential outlier detection problem. We use autoencoder models to capture the intrinsic difference between outliers and normal instances and integrate the models to recurrent neural networks that allow the learning to make use of previous context as well as make the learners more robust to warp along the time axis. Furthermore, we propose to use a layerwise training procedure, which significantly simplifies the training procedure and hence helps achieve efficient and scalable training. In addition, we investigate a fine-tuning step to update all parameters set by incorporating the temporal correlation in the sequence. We further apply our proposed models to conduct systematic experiments on five real-world benchmark data sets. Experimental results demonstrate the effectiveness of our model, compared with other state-of-the-art approaches.","","","10.1109/TIP.2017.2713048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7942034","Sequential anomaly detection;deep learning;denoising autoencoder;recurrent neural networks","Training;Data models;Feature extraction;Context modeling;Context;Principal component analysis;Correlation","correlation methods;object detection;recurrent neural nets","unsupervised sequential outlier detection;deep architectures;image analysis;video surveillance;feature construction;sequential anomaly detection;temporal correlation;deep structured framework;recurrent neural networks;time axis;layerwise training procedure;fine-tuning step;real-world benchmark data sets;autoencoder models;critical systems;applications domains","","6","50","","","","","IEEE","IEEE Journals"
"Histopathological Image Classification With Color Pattern Random Binary Hashing-Based PCANet and Matrix-Form Classifier","J. Shi; J. Wu; Y. Li; Q. Zhang; S. Ying","Institute of Biomedical Engineering, School of Communication and Information Engineering, Shanghai University, Shanghai, China; Institute of Biomedical Engineering, School of Communication and Information Engineering, Shanghai University, Shanghai, China; Shenzhen City Key Laboratory of Embedded System Design, Shenzhen Laboratory of IC Design for Internet of Things, College of Computer Science and Software Engineering, Shenzhen University Shenzhen, China; Institute of Biomedical Engineering, School of Communication and Information Engineering, Shanghai University, Shanghai, China; Department of Mathematics, School of Science, Shanghai University, Shanghai, China","IEEE Journal of Biomedical and Health Informatics","","2017","21","5","1327","1337","The computer-aided diagnosis for histopathological images has attracted considerable attention. Principal component analysis network (PCANet) is a novel deep learning algorithm for feature learning with the simple network architecture and parameters. In this study, a color pattern random binary hashing-based PCANet (C-RBH-PCANet) algorithm is proposed to learn an effective feature representation from color histopathological images. The color norm pattern and angular pattern are extracted from the principal component images of R, G, and B color channels after cascaded PCA networks. The random binary encoding is then performed on both color norm pattern images and angular pattern images to generate multiple binary images. Moreover, we rearrange the pooled local histogram features by spatial pyramid pooling to a matrix-form for reducing the dimension of feature and preserving spatial information. Therefore, a C-RBH-PCANet and matrix-form classifier-based feature learning and classification framework is proposed for diagnosis of color histopathological images. The experimental results on three color histopathological image datasets show that the proposed C-RBH-PCANet algorithm is superior to the original PCANet and other conventional unsupervised deep learning algorithms, while the best performance is achieved by the proposed feature learning and classification framework that combines C-RBH-PCANet and matrix-form classifier.","","","10.1109/JBHI.2016.2602823","National Natural Science Foundation of China; Innovation Program of Shanghai Municipal Education Commission; Projects of Guangdong R/D Foundation; Fundamental Science Projects of Shenzhen City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552528","Color angular pattern;color histopathological image;color norm pattern;matrix-form classifier;PCANet;random binary hashing","Image color analysis;Principal component analysis;Feature extraction;Classification algorithms;Histograms;Algorithm design and analysis;Image classification","image classification;image colour analysis;learning (artificial intelligence);medical image processing;principal component analysis","histopathological image classification;color pattern random binary hashing-based PCaNet;matrix-form classifier;computer-aided diagnosis;principal component analysis network;deep learning algorithm;feature learning;color norm pattern;angular pattern;color channels;multiple binary images;C-RBH-PCANet algorithm","Algorithms;Breast Neoplasms;Databases, Factual;Female;Histocytochemistry;Humans;Image Interpretation, Computer-Assisted;Kidney Diseases;Machine Learning;Principal Component Analysis;ROC Curve","28","40","Traditional","","","","IEEE","IEEE Journals"
"A Deep Learning Approach to UAV Image Multilabeling","A. Zeggada; F. Melgani; Y. Bazi","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia","IEEE Geoscience and Remote Sensing Letters","","2017","14","5","694","698","In this letter, we face the problem of multilabeling unmanned aerial vehicle (UAV) imagery, typically characterized by a high level of information content, by proposing a novel method based on convolutional neural networks. These are exploited as a means to yield a powerful description of the query image, which is analyzed after subdividing it into a grid of tiles. The multilabel classification task of each tile is performed by the combination of a radial basis function neural network and a multilabeling layer (ML) composed of customized thresholding operations. Experiments conducted on two different UAV image data sets demonstrate the promising capability of the proposed method compared to the state of the art, at the expense of a higher but still contained computation time.","","","10.1109/LGRS.2017.2671922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885056","Convolutional neural networks (CNNs);image multilabeling;Otsu’s algorithm;unmanned aerial vehicles (UAVs);urban monitoring","Unmanned aerial vehicles;Training;Neural networks;Feature extraction;Histograms;Image segmentation;Computer architecture","autonomous aerial vehicles;image classification;image retrieval;learning (artificial intelligence);radial basis function networks","deep learning approach;UAV image multilabeling layer;multilabeling unmanned aerial vehicle imagery;convolutional neural networks;information content;query image description;multilabel classification task;radial basis function neural network;customized thresholding operations;UAV image data sets","","22","19","","","","","IEEE","IEEE Journals"
"Hyperspectral and LiDAR Data Fusion Using Extinction Profiles and Deep Convolutional Neural Network","P. Ghamisi; B. Höfle; X. X. Zhu","Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR) and Signal Processing in Earth Observation, Technischen Universität München, Munich, Germany; GIScience, Institute of Geography, Heidelberg University, Heidelberg, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR) and Signal Processing in Earth Observation, Technischen Universität München, Munich, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","6","3011","3024","This paper proposes a novel framework for the fusion of hyperspectral and light detection and ranging-derived rasterized data using extinction profiles (EPs) and deep learning. In order to extract spatial and elevation information from both the sources, EPs that include different attributes (e.g., height, area, volume, diagonal of the bounding box, and standard deviation) are taken into account. Then, the derived features are fused via either feature stacking or graph-based feature fusion. Finally, the fused features are fed to a deep learning-based classifier (convolutional neural network with logistic regression) to ultimately produce the classification map. The proposed approach is applied to two datasets acquired in Houston, TX, USA, and Trento, Italy. Results indicate that the proposed approach can achieve accurate classification results compared to other approaches. It should be noted that, in this paper, the concept of deep learning has been used for the first time to fuse LiDAR and hyperspectral features, which provides new opportunities for further research.","","","10.1109/JSTARS.2016.2634863","Alexander von Humboldt Fellowship for Postdoctoral Researchers; Helmholtz Young Investigators Group; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7786851","Convolutional neural network (CNN);deep learning;extinction profile (EP);graph-based feature fusion (GBFF);hyperspectral;light detection and ranging (LiDAR);random forest (RF);support vector machines (SVMs)","Feature extraction;Hyperspectral imaging;Laser radar;Data mining;Sensors","hyperspectral imaging;neural nets;optical radar;remote sensing by radar","LiDAR data fusion;extinction profiles;deep convolutional neural network;light detection;ranging-derived rasterized data;graph-based feature fusion;logistic regression;hyperspectral features","","29","63","","","","","IEEE","IEEE Journals"
"Low Complexity Multiply Accumulate Unit for Weight-Sharing Convolutional Neural Networks","J. Garland; D. Gregg","Trinity College Dublin, Dublin 2, Ireland; Trinity College Dublin, Dublin 2, Ireland","IEEE Computer Architecture Letters","","2017","16","2","132","135","Convolutional Neural Networks (CNNs) are one of the most successful deep machine learning technologies for processing image, voice and video data. CNNs require large amounts of processing capacity and memory, which can exceed the resources of low power mobile and embedded systems. Several designs for hardware accelerators have been proposed for CNNs which typically contain large numbers of Multiply Accumulate (MAC) units. One approach to reducing data sizes and memory traffic in CNN accelerators is “weight sharing”, where the full range of values in a trained CNN are put in bins and the bin index is stored instead of the original weight value. In this paper we propose a novel MAC circuit that exploits binning in weight-sharing CNNs. Rather than computing the MAC directly we instead count the frequency of each weight and place it in a bin. We then compute the accumulated value in a subsequent multiply phase. This allows hardware multipliers in the MAC circuit to be replaced with adders and selection logic. Experiments show that for the same clock speed our approach results in fewer gates, smaller logic, and reduced power.","","","10.1109/LCA.2017.2656880","Science Foundation Ireland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829315","Convolutional neural network;power efficiency;multiply accumulate;arithmetic hardware circuits","Energy efficiency;Logic gates;Machine learning;Neural networks;Convolutional neural networks","adders;convolution;embedded systems;feedforward neural nets;learning (artificial intelligence);multiplying circuits","MAC circuit;subsequent multiply phase;hardware multipliers;video data;embedded systems;hardware accelerators;memory traffic;CNN accelerators;bin index;original weight value;weight-sharing CNN;weight-sharing convolutional neural networks;deep machine learning technologies;multiply accumulate units","","4","9","Traditional","","","","IEEE","IEEE Journals"
"Detection and Localization of Robotic Tools in Robot-Assisted Surgery Videos Using Deep Neural Networks for Region Proposal and Detection","D. Sarikaya; J. J. Corso; K. A. Guru","Department of Computer Science and Engineering, SUNY Buffalo, NY, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA; Applied Technology Laboratory for Advanced Surgery, Roswell Park Cancer Institute, Buffalo, NY, USA","IEEE Transactions on Medical Imaging","","2017","36","7","1542","1549","Video understanding of robot-assisted surgery (RAS) videos is an active research area. Modeling the gestures and skill level of surgeons presents an interesting problem. The insights drawn may be applied in effective skill acquisition, objective skill assessment, real-time feedback, and human-robot collaborative surgeries. We propose a solution to the tool detection and localization open problem in RAS video understanding, using a strictly computer vision approach and the recent advances of deep learning. We propose an architecture using multimodal convolutional neural networks for fast detection and localization of tools in RAS videos. To the best of our knowledge, this approach will be the first to incorporate deep neural networks for tool detection and localization in RAS videos. Our architecture applies a region proposal network (RPN) and a multimodal two stream convolutional network for object detection to jointly predict objectness and localization on a fusion of image and temporal motion cues. Our results with an average precision of 91% and a mean computation time of 0.1 s per test frame detection indicate that our study is superior to conventionally used methods for medical imaging while also emphasizing the benefits of using RPN for precision and efficiency. We also introduce a new data set, ATLAS Dione, for RAS video understanding. Our data set provides video data of ten surgeons from Roswell Park Cancer Institute, Buffalo, NY, USA, performing six different surgical tasks on the daVinci Surgical System (dVSS) with annotations of robotic tools per frame.","","","10.1109/TMI.2017.2665671","Roswell Park Alliance Foundation Roswell Park Cancer Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847313","Object detection;multi-layer neural network;image classification;laparoscopes;telerobotics","Videos;Surgery;Proposals;Neural networks;Training;Robots;Object detection","biomedical optical imaging;computer vision;gesture recognition;image fusion;image motion analysis;learning (artificial intelligence);manipulators;medical image processing;medical robotics;neural nets;object detection;surgery;telerobotics;video signal processing","robotic tools;robot-assisted surgery videos;deep neural networks;region detection;gestures;skill level;effective skill acquisition;objective skill assessment;real-time feedback;human-robot collaborative surgeries;tool detection;localization open problem;computer vision approach;deep learning;multimodal convolutional neural networks;RAS videos;tool localization;region proposal network;RPN;multimodal two stream convolutional network;object detection;objectness;image fusion;temporal motion cues;mean computation time;frame detection;medical imaging;ATLAS Dione;Roswell Park Cancer Institute;daVinci Surgical System","Humans;Motion;Neural Networks (Computer);Robotic Surgical Procedures;Robotics","16","39","","","","","IEEE","IEEE Journals"
"Towards Retrieving Force Feedback in Robotic-Assisted Surgery: A Supervised Neuro-Recurrent-Vision Approach","A. I. Aviles; S. M. Alsaleh; J. K. Hahn; A. Casals","Center of Research of Biomedical Engineering, Universitat Politècnica de Cataluya, Barcelona, Spain; Department of Computer Science, Institute for Biomedical Engineering, George Washington University, Washington, DC; Department of Computer Science, Institute for Biomedical Engineering, George Washington University, Washington, DC; Research of Biomedical Engineering, Universitat Politècnica de Cataluya, Barcelona, Spain","IEEE Transactions on Haptics","","2017","10","3","431","443","Robotic-assisted minimally invasive surgeries have gained a lot of popularity over conventional procedures as they offer many benefits to both surgeons and patients. Nonetheless, they still suffer from some limitations that affect their outcome. One of them is the lack of force feedback which restricts the surgeon's sense of touch and might reduce precision during a procedure. To overcome this limitation, we propose a novel force estimation approach that combines a vision based solution with supervised learning to estimate the applied force and provide the surgeon with a suitable representation of it. The proposed solution starts with extracting the geometry of motion of the heart's surface by minimizing an energy functional to recover its 3D deformable structure. A deep network, based on a LSTM-RNN architecture, is then used to learn the relationship between the extracted visual-geometric information and the applied force, and to find accurate mapping between the two. Our proposed force estimation solution avoids the drawbacks usually associated with force sensing devices, such as biocompatibility and integration issues. We evaluate our approach on phantom and realistic tissues in which we report an average root-mean square error of 0.02 N.","","","10.1109/TOH.2016.2640289","FPU national scholarship; Spanish Ministry of Education; MINECO; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7784717","Force estimation;deep networks;visual deformation;computer-assisted surgery","Force;Surgery;Robot sensing systems;Visualization;Estimation;Force feedback","computational geometry;dexterous manipulators;feature extraction;force feedback;learning (artificial intelligence);medical robotics;minimisation;neural net architecture;recurrent neural nets;robot vision;surgery","force feedback;supervised neuro-recurrent-vision approach;robotic-assisted minimally invasive surgeries;force estimation approach;supervised learning;heart surface motion geometry extraction;energy functional minimization;3D deformable structure recovery;deep network;LSTM-RNN architecture;visual-geometric information;average root-mean square error","Feedback;Heart;Humans;Image Interpretation, Computer-Assisted;Mechanical Phenomena;Minimally Invasive Surgical Procedures;Neural Networks (Computer);Robotic Surgical Procedures;Supervised Machine Learning","2","53","Traditional","","","","IEEE","IEEE Journals"
"Visual Importance and Distortion Guided Deep Image Quality Assessment Framework","J. Guan; S. Yi; X. Zeng; W. Cham; X. Wang","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, China","IEEE Transactions on Multimedia","","2017","19","11","2505","2520","In this paper, we tackle the problem of no-reference image quality assessment (IQA). A learning-based IQA framework “VIDGIQA” is proposed, which extracts quality features from the input image and regresses the visual quality on these features. Since different distortions lead to different visual perceptions in the human visual system, distortion information is adopted to guide the feature learning process together with the human quality scores. Besides, a regression method is proposed to model and estimate the visual importance weights of all local regions, which can effectively improve the performance. More importantly, all these operations are integrated into one deep neural network, so that they can be jointly optimized and well cooperate with each other. Experiments were conducted to demonstrate the power of the proposed method on several datasets, including the LIVE dataset [1], the TID 2013 dataset [2], the LIVE multiply distorted IQA dataset [3], CSIQ [4] , and the LIVE in the wild image quality database [5]. The proposed method achieves 0.969 and 0.973 on the LIVE dataset [1] in terms of the spearman rank-order correlation coefficient and the Pearson linear correlation coefficient, respectively, which outperforms the state-of-the-art methods.","","","10.1109/TMM.2017.2703148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7924311","Distortion sensitive features;image quality assessment (IQA);visual importance;visual quality maps","Visualization;Feature extraction;Distortion;Estimation;Boats;Image quality;White noise","image processing;learning (artificial intelligence);neural nets;regression analysis;visual databases;visual perception","visual quality;human visual system;distortion information;feature learning;human quality scores;regression method;visual importance weights;deep neural network;LIVE dataset;TID 2013 dataset;IQA dataset;wild image quality database;deep image quality assessment framework;no-reference image quality assessment;IQA framework VIDGIQA;quality features;visual perceptions;spearman rank-order correlation coefficient;Pearson linear correlation coefficient","","7","77","Traditional","","","","IEEE","IEEE Journals"
"Deep Fusion of Remote Sensing Data for Accurate Classification","Y. Chen; C. Li; P. Ghamisi; X. Jia; Y. Gu","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; German Aerospace Center (DLR), Remote Sensing Technology Institute (IMF), Wessling, Germany; School of Engineering and Information Technology, The University of New South Wales, Canberra, NSW, Australia; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","8","1253","1257","The multisensory fusion of remote sensing data has obtained a great attention in recent years. In this letter, we propose a new feature fusion framework based on deep neural networks (DNNs). The proposed framework employs deep convolutional neural networks (CNNs) to effectively extract features of multi-/hyperspectral and light detection and ranging data. Then, a fully connected DNN is designed to fuse the heterogeneous features obtained by the previous CNNs. Through the aforementioned deep networks, one can extract the discriminant and invariant features of remote sensing data, which are useful for further processing. At last, logistic regression is used to produce the final classification results. Dropout and batch normalization strategies are adopted in the deep fusion framework to further improve classification accuracy. The obtained results reveal that the proposed deep fusion model provides competitive results in terms of classification accuracy. Furthermore, the proposed deep learning idea opens a new window for future remote sensing data fusion.","","","10.1109/LGRS.2017.2704625","State Key Laboratory of Frozen Soil Engineering; Natural Science Foundation of China; National Science Foundation for Excellent Young Scholars; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7940007","Convolutional neural network (CNN);data fusion;deep neural network (DNN);feature extraction (FE);hyperspectral image (HSI);light detection and ranging (LiDAR);multispectral image (MSI)","Feature extraction;Laser radar;Training;Remote sensing;Data mining;Neurons;Fuses","feature extraction;image classification;image fusion;neural nets;optical radar;regression analysis;remote sensing by laser beam","remote sensing data deep fusion;image classification;feature fusion framework;deep neural networks;convolutional neural networks;feature extraction;multihyperspectral data;light detection and ranging data;logistic regression;dropout strategy;batch normalization strategy","","12","15","","","","","IEEE","IEEE Journals"
"Distributed learning of deep feature embeddings for visual recognition tasks","B. Bhattacharjee; M. L. Hill; H. Wu; P. S. Chandakkar; J. R. Smith; M. N. Wegman","NA; NA; NA; NA; NA; NA","IBM Journal of Research and Development","","2017","61","4/5","4:1","4:8","Deep learning has demonstrated an ability to substantially improve the state of the art for understanding the visual content of images. Much of the recent progress has been measured in the context of the ImageNet large-scale visual recognition challenge, with a modest subset of 1.2 million images, labeled according to 1,000 concepts of the full ImageNet dataset. Few published results have applied learning to the full ImageNet dataset of 14 million images over nearly 22,000 concepts. This is partly due to the substantial time and computational resources needed to perform adequate training, from such a large dataset, despite exploitation of graphics processing units. To achieve this scale of training, we use Phalanx, a distributed deep learning framework being developed by IBM. Phalanx is a distributed framework with a parameter server as the hub and multiple learners that employ the open source Caffe platform as spokes. Using Phalanx on the full ImageNet dataset, we performed experiments that demonstrate the impact of large-scale learning on multiple training scenarios. This paper includes fine-tuning, where a pretrained model is used as the basis for further training, as well as use of pretrained models for learning deep feature embeddings.","","","10.1147/JRD.2017.2706118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030315","","Training;Visualization;Servers;Machine learning;Image recognition;Graphics processing units;Computational modeling","","","","","27","","","","","IBM","IBM Journals"
"Deep Conditional Random Field Approach to Transmembrane Topology Prediction and Application to GPCR Three-Dimensional Structure Modeling","H. Wu; K. Wang; L. Lu; Y. Xue; Q. Lyu; M. Jiang","School of Electronic and Information Engineering, Suzhou University of Science and Technology, Suzhou, China; School of Electronic and Information Engineering, Suzhou University of Science and Technology, Suzhou, China; Department of Computer Sciences, University of Minnesota, Minneapolis, MN; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Computer Science and Technology, Soochow University, Suzhou, China; Central Laboratory and Clinical Laboratory, The First Affiliated Hospital of Soochow University, Suzhou, China","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2017","14","5","1106","1114","Transmembrane proteins play important roles in cellular energy production, signal transmission, and metabolism. Many shallow machine learning methods have been applied to transmembrane topology prediction, but the performance was limited by the large size of membrane proteins and the complex biological evolution information behind the sequence. In this paper, we proposed a novel deep approach based on conditional random fields named as dCRF-TM for predicting the topology of transmembrane proteins. Conditional random fields take into account more complicated interrelation between residue labels in full-length sequence than HMM and SVM-based methods. Three widely-used datasets were employed in the benchmark. DCRF-TM had the accuracy 95 percent over helix location prediction and the accuracy 78 percent over helix number prediction. DCRF-TM demonstrated a more robust performance on large size proteins (>350 residues) against 11 state-of-the-art predictors. Further dCRF-TM was applied to ab initio modeling three-dimensional structures of seven-transmembrane receptors, also known as G protein-coupled receptors. The predictions on 24 solved G protein-coupled receptors and unsolved vasopressin V2 receptor illustrated that dCRF-TM helped abGPCR-I-TASSER to improve TM-score 34.3 percent rather than using the random transmembrane definition. Two out of five predicted models caught the experimental verified disulfide bonds in vasopressin V2 receptor.","","","10.1109/TCBB.2016.2602872","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552547","Deep learning;conditional random fields;transmembrane protein;G protein–coupled receptor;vasopressin V2 receptor","Proteins;Hidden Markov models;Topology;Biomembranes;Feature extraction;Evolution (biology)","biomembranes;molecular biophysics;molecular configurations;proteins","deep conditional random field approach;transmembrane topology;GPCR three-dimensional structure modeling;transmembrane proteins;cellular energy production;signal transmission;metabolism;shallow machine learning methods;conditional random fields;dCRF-TM;G protein-coupled receptors;vasopressin V2 receptor;abGPCR-I-TASSER","Computational Biology;Databases, Protein;Humans;Models, Molecular;Models, Statistical;Protein Conformation;Receptors, G-Protein-Coupled;Sequence Analysis, Protein","1","57","Traditional","","","","IEEE","IEEE Journals"
"Weakly Supervised Deep Matrix Factorization for Social Image Understanding","Z. Li; J. Tang","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Image Processing","","2017","26","1","276","288","The number of images associated with weakly supervised user-provided tags has increased dramatically in recent years. User-provided tags are incomplete, subjective and noisy. In this paper, we focus on the problem of social image understanding, i.e., tag refinement, tag assignment, and image retrieval. Different from previous work, we propose a novel weakly supervised deep matrix factorization algorithm, which uncovers the latent image representations and tag representations embedded in the latent subspace by collaboratively exploring the weakly supervised tagging information, the visual structure, and the semantic structure. Due to the well-known semantic gap, the hidden representations of images are learned by a hierarchical model, which are progressively transformed from the visual feature space. It can naturally embed new images into the subspace using the learned deep architecture. The semantic and visual structures are jointly incorporated to learn a semantic subspace without overfitting the noisy, incomplete, or subjective tags. Besides, to remove the noisy or redundant visual features, a sparse model is imposed on the transformation matrix of the first layer in the deep architecture. Finally, a unified optimization problem with a well-defined objective function is developed to formulate the proposed problem and solved by a gradient descent procedure with curvilinear search. Extensive experiments on real-world social image databases are conducted on the tasks of image understanding: image tag refinement, assignment, and retrieval. Encouraging results are achieved with comparison with the state-of-the-art algorithms, which demonstrates the effectiveness of the proposed method.","","","10.1109/TIP.2016.2624140","973 Program; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7728069","Deep architecture;matrix factorization;weakly supervised;image tagging;image retrieval","Visualization;Semantics;Noise measurement;Tagging;Image retrieval;Sparse matrices;Data models","gradient methods;image representation;image retrieval;matrix decomposition;optimisation;search problems;semantic networks;social networking (online)","weakly supervised deep matrix factorization;WDMF algorithm;social image understanding;tag refinement;tag assignment;image retrieval;image representation;tag representation;visual structure;semantic structure;deep architecture;unified optimization problem;objective function;gradient descent procedure;curvilinear search","","95","61","","","","","IEEE","IEEE Journals"
"Yet Another Challenge for the Automotive Software: Deep Learning","F. Falcini; G. Lami; A. Mitidieri","NA; NA; NA","IEEE Software","","2017","PP","99","1","1","In automotive software, the deep learning-based systems, with their peculiar features, are playing an increasingly pervasive role. As a result, inside the automotive software engineering community, an awareness of the need of integrating the deep learning development approach with the “traditional” ones is growing, at technical, methodological and cultural levels. In particular, the innovative and data-intensive phase of deep neural network training, by means of ad-hoc training data, is pivotal in the software development of vehicle functions that rely on such a technology paradigm and thus necessitates a special focus. A development lifecycle fitting deep learning needs is introduced and an improvement initiative, based on Automotive SPICE, for an effective adoption of DNN in the automotive software applications is presented.","","","10.1109/MS.2017.265101102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950845","D Software/Software Engineering;D.2 Software Engineering;D.2.18 Software Engineering Process;D.2.18.a Life cycle;I Computing Methodologies;I.2 Artificial Intelligence;I.2.10 Vision and Scene Understanding;D.2.0 General;D.2.0.d Standards","Machine learning;Automotive engineering;Training;Biological neural networks;Software;Automobiles;Software engineering","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Joint Hierarchical Category Structure Learning and Large-Scale Image Classification","Y. Qu; L. Lin; F. Shen; C. Lu; Y. Wu; Y. Xie; D. Tao","Department of Computer Science, Xiamen University, Xiamen, China; Department of Computer Science, Xiamen University, Xiamen, China; School of Computer Science and Engineering, University of Electric Science and Technology of China, Chengdu, China; Department of Computer Science, Xiamen University, Xiamen, China; Institute for Research Initiatives, Nara Institute of Science and Technology, Nara, Japan; Research Center of Precision Sensing, Control Institute of Automation, Chinese Academy of Sciences, Beijing, China; Center for Artificial Intelligence, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Image Processing","","2017","26","9","4331","4346","We investigate the scalable image classification problem with a large number of categories. Hierarchical visual data structures are helpful for improving the efficiency and performance of large-scale multi-class classification. We propose a novel image classification method based on learning hierarchical inter-class structures. Specifically, we first design a fast algorithm to compute the similarity metric between categories, based on which a visual tree is constructed by hierarchical spectral clustering. Using the learned visual tree, a test sample label is efficiently predicted by searching for the best path over the entire tree. The proposed method is extensively evaluated on the ILSVRC2010 and Caltech 256 benchmark datasets. The experimental results show that our method obtains significantly better category hierarchies than other state-of-the-art visual tree-based methods and, therefore, much more accurate classification.","","","10.1109/TIP.2016.2615423","National Natural Science Foundation of China; Hong Kong Scholar Program; Australian Research Council; JSPS KAKENHI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7583684","Hierarchical learning;large-scale image classification;deep features;visual tree;N-best path","Visualization;Semantics;Prediction algorithms;Measurement;Image representation;Feature extraction;Clustering algorithms","image classification;learning (artificial intelligence)","joint hierarchical category structure learning;large-scale image classification;hierarchical visual data structure;large-scale multiclass classification performance improvement;large-scale multiclass classification efficiency improvement;visual tree;hierarchical spectral clustering;ILSVRC2010 benchmark dataset;Caltech 256 benchmark dataset","","5","63","","","","","IEEE","IEEE Journals"
"Matryoshka Peek: Toward Learning Fine-Grained, Robust, Discriminative Features for Product Search","Z. Kyaw; S. Qi; K. Gao; H. Zhang; L. Zhang; J. Xiao; X. Wang; T. Chua","School of Computing, National University of Singapore, Singapore; Harbin Institute of Technology ShenZhen Graduate School, Shenzhen, China; Institute of Computing Technology, Chinese Academy of Science, Beijing; School of Computing, National University of Singapore, Singapore; Hefei University of Technology, Hefei, China; Zhejiang University, Hangzhou, China; Computer Application Research Center, Harbin Institute of Technology ShenZhen Graduate School, Shenzhen, China; School of Computing, National University of Singapore, Singapore","IEEE Transactions on Multimedia","","2017","19","6","1272","1284","In sharp contrast to the traditional category/subcategory level image retrieval, product image search aims to find the images containing the exact same product. This is a challenging problem because in addition to being robust under different imaging conditions such as varying viewpoints and illumination changes, the features should also be able to distinguish the specific product among many similar products. Consequently, it is important to utilize a large dataset, containing many product classes, to learn a strongly discriminative representation. Building such a dataset requires laborious manual annotation. Toward learning fine-grained, robust, discriminative features for product image search, we present a novel paradigm that can construct the required dataset without any human annotation. Unlike other fine-grained recognition works that rely on high-quality annotated datasets and are very narrowly focused on a specific object category, our method handles multiple object classes and requires minimum human effort. First, an ImageNet pretrained model is used to generate product clusters. As the original features from ImageNet are not discriminative, the clusters generated by this unsupervised procedure contain much noise. We alleviate noise by explicitly modeling noise distribution and automatically detecting errors during learning. The proposed paradigm is general, requires minimum human efforts, and is applicable to any deep learning task where fine-grained discriminative features are desired. Extensive experiments on the ALISC dataset have demonstrated that our approach is sound and effective, surpassing the baseline GoogleNet model by 15.09%.","","","10.1109/TMM.2017.2655422","National Research Foundation; National Nature Science Foundation of China; International Exchange and Cooperation Foundation of Shenzhen City; National University of Singapore (Suzhou) Research Institute; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823008","Feature extraction;image representation;robust learning;image retrieval","Training;Robustness;Convolution;Birds;Search problems;Buildings;Manuals","error detection;feature extraction;image annotation;image representation;image retrieval;lighting;unsupervised learning","Matryoshka peek;fine-grained robust discriminative feature learning;image retrieval;product image search;illumination changes;discriminative representation learning;manual annotation;ImageNet pretrained model;product cluster generation;unsupervised procedure;noise distribution modeling;error detection;ALISC dataset;baseline GoogleNet model","","1","54","","","","","IEEE","IEEE Journals"
"Deep Convolutional Neural Networks for Predominant Instrument Recognition in Polyphonic Music","Y. Han; J. Kim; K. Lee","Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, Seoul, South Korea; Seoul National University, Seoul, South Korea; Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, Seoul, South Korea","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","1","208","221","Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field of music information retrieval. It enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. In this paper, we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music. We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. To obtain the audio-excerpt-wise result, we aggregate multiple outputs from sliding windows over the test audio. In doing so, we investigated two different aggregation methods: one takes the class-wise average followed by normalization, and the other perform temporally local class-wise max-pooling on the output probability prior to averaging and normalization steps to minimize the effect of averaging process suppresses the activation of sporadically appearing instruments. In addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identification threshold, and activation functions for neural networks to find the optimal set of parameters. Our analysis on the instrument-wise performance found that the onset type is a critical factor for recall and precision of each instrument. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.619 for micro and 0.513 for macro, respectively, achieving 23.1% and 18.8% in performance improvement compared with the state-of-the-art algorithm.","","","10.1109/TASLP.2016.2632307","Ministry of Science, ICT and Future Planning (MSIP); Information Technology Research Center; Institute for Information & Communications Technology Promotion; National Research Foundation of Korea; MSIP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7755799","Convolutional neural networks;deep learning;instrument recognition;multi-layer neural network;music information retrieval","Instruments;Music;Speech recognition;Convolution;Neural networks;Speech;Machine learning","audio signal processing;music;neural nets;source separation;support vector machines","music information retrieval;real-world polyphonic music;fixed-length music excerpt;audio signal;sliding window;test audio;temporally local class-wise max-pooling;activation function;identification threshold;analysis window size;source separation;support vector machine;spectral feature exploitation;predominant instrument recognition;deep convolutional neural network","","27","52","","","","","IEEE","IEEE Journals"
"Deep Fully Convolutional Network-Based Spatial Distribution Prediction for Hyperspectral Image Classification","L. Jiao; M. Liang; H. Chen; S. Yang; H. Liu; X. Cao","Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","10","5585","5599","Most of the existing spatial-spectral-based hyperspectral image classification (HSIC) methods mainly extract the spatial-spectral information by combining the pixels in a small neighborhood or aggregating the statistical and morphological characteristics. However, those strategies can only generate shallow appearance features with limited representative ability for classes with high interclass similarity and spatial diversity and therefore reduce the classification accuracy. To this end, we present a novel HSIC framework, named deep multiscale spatial-spectral feature extraction algorithm, which focuses on learning effective discriminant features for HSIC. First, the well pretrained deep fully convolutional network based on VGG-verydeep-16 is introduced to excavate the potential deep multiscale spatial structural information in the proposed hyperspectral imaging framework. Then, the spectral feature and the deep multiscale spatial feature are fused by adopting the weighted fusion method. Finally, the fusion feature is put into a generic classifier to obtain the pixelwise classification. Compared with the existing spectral-spatial-based classification techniques, the proposed method provides the state-of-the-art performance and is much more effective, especially for images with high nonlinear distribution and spatial diversity.","","","10.1109/TGRS.2017.2710079","Major Research Plan of the National Natural Science Foundation of China; National Basic Research Program (973 Program) of China; Program for Cheung Kong Scholars and Innovative Research Team in University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967742","Deep multiscale feature;feature fusion;fully convolutional network (FCN);hyperspectral image classification (HSIC);spatial distribution prediction","Feature extraction;Kernel;Graphical models;Distribution functions;Hyperspectral imaging;Training;Convolution","convolution;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;neural nets;statistics","deep fully convolutional network;spatial distribution prediction;hyperspectral image classification;HSIC;statistical characteristics;morphological characteristics;deep multiscale spatial-spectral feature extraction;weighted fusion","","27","42","Traditional","","","","IEEE","IEEE Journals"
"Single Infrared Image Stripe Noise Removal Using Deep Convolutional Networks","X. Kuang; X. Sui; Q. Chen; G. Gu","Nanjing University of Science and Technology, Nanjing, China; Nanjing University of Science and Technology, Nanjing, China; Nanjing University of Science and Technology, Nanjing, China; Nanjing University of Science and Technology, Nanjing, China","IEEE Photonics Journal","","2017","9","4","1","13","In this paper, we present a deep learning method for single infrared image stripe noise removal. Our method is denoted as a deep convolutional neural network (CNN) that takes the noisy image as the input and outputs the clean image. The deep CNN consists of two components: 1) image denoising, substantially removing the stripe noise but losing details, 2) image denoising and super resolution, completely eliminating the residual stripe noise and restore details. Our deep CNN exhibits excellent image denoising and detail preserving performance. Meanwhile it achieves fast speed for real-time image processing. Experiments study the relationship between model parameter settings and model performance.","","","10.1109/JPHOT.2017.2717948","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Qing Lan Project and Open Research Fund of Jiangsu Key Laboratory of Spectral Imaging & Intelligence Sense; Zijin Intelligent Program, Nanjing University of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954611","Stripe noise removal;infrared image;deep convolutional neural networks.","Image denoising;Neural networks;Convolution;Kernel;Histograms;Training;Image resolution","convolution;image denoising;image resolution;infrared imaging;neural nets","single infrared image stripe noise removal;deep convolutional neural network;image denoising;deep CNN","","8","30","","","","","IEEE","IEEE Journals"
"Ultrasound Aided Vertebral Level Localization for Lumbar Surgery","N. Baka; S. Leenstra; T. van Walsum","Department of Radiology, Biomedical Imaging Group Rotterdam, Erasmus MC, University Medical Center Rotterdam, Rotterdam, GE, The Netherlands; Department of Neurosurgery, Erasmus MC, University Medical Center Rotterdam, Rotterdam, GE, The Netherlands; Department of Radiology, Biomedical Imaging Group Rotterdam, Erasmus MC, University Medical Center Rotterdam, Rotterdam, GE, The Netherlands","IEEE Transactions on Medical Imaging","","2017","36","10","2138","2147","Localization of the correct vertebral level for surgical entry during lumbar hernia surgery is not straightforward. In this paper, we develop and evaluate a solution using free-hand 2-D ultrasound (US) imaging in the operation room (OR). Our system exploits the difference in spinous process shapes of the vertebrae. The spinous processes are pre-operatively outlined and labeled in a lateral lumbar X-ray of the patient. Then, in the OR the spinous processes are imaged with 2-D sagittal US, and are automatically segmented and registered with the X-ray shapes. After a small number of scanned vertebrae, the system robustly matches the shapes, and propagates the X-ray label to the US images. The main contributions of our work are: we propose a deep convolutional neural network-based bone segmentation algorithm from US imaging that outperforms state of the art methods in both performance and speed. We present a matching strategy that determines the levels of the spinal processes being imaged. And lastly, we evaluate the complete procedure on 19 clinical data sets from two hospitals, and two observers. The final labeling was correct in 92% of the cases, demonstrating the feasibility of US-based surgical entry point detection for spinal surgeries.","","","10.1109/TMI.2017.2738612","Dutch Science Foundation STW; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8007292","Bone segmentation;deep learning;machine learning;surgical guidance;lumbar X-ray;computer aided surgery;spine","X-ray imaging;Image segmentation;Bones;Surgery;Shape;Ultrasonic imaging;Two dimensional displays","biomedical ultrasonics;bone;image matching;image registration;image segmentation;medical image processing;neural nets;surgery;ultrasonic imaging","free-hand 2D ultrasound imaging;spinous process shapes;lateral lumbar X-ray;spinous processes;2D sagittal US;automatically segmented X-ray shapes;registered X-ray shapes;scanned vertebrae;X-ray label propagation;US images;deep convolutional neural network-based bone segmentation algorithm;surgical entry;lumbar hernia surgery;correct vertebral level localization;ultrasound aided vertebral level localization;US-based surgical entry point detection;clinical data sets;spinal processes;matching strategy","Algorithms;Humans;Image Processing, Computer-Assisted;Lumbar Vertebrae;Machine Learning;Male;Middle Aged;Surgery, Computer-Assisted;Ultrasonography","3","27","Traditional","","","","IEEE","IEEE Journals"
"AID: A Benchmark Data Set for Performance Evaluation of Aerial Scene Classification","G. Xia; J. Hu; F. Hu; B. Shi; X. Bai; Y. Zhong; L. Zhang; X. Lu","State Key Laboratory of Information Engineering, Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering, Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering, Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Electronics Information, Huazhong University of Science and Technology, Wuhan, China; School of Electronics Information, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Information Engineering, Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering, Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Transient Optics and Photonics, Center for OPTical IMagery Analysis and Learning, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","7","3965","3981","Aerial scene classification, which aims to automatically label an aerial image with a specific semantic category, is a fundamental problem for understanding high-resolution remote sensing imagery. In recent years, it has become an active task in the remote sensing area, and numerous algorithms have been proposed for this task, including many machine learning and data-driven approaches. However, the existing data sets for aerial scene classification, such as UC-Merced data set and WHU-RS19, contain relatively small sizes, and the results on them are already saturated. This largely limits the development of scene classification algorithms. This paper describes the Aerial Image data set (AID): a large-scale data set for aerial scene classification. The goal of AID is to advance the state of the arts in scene classification of remote sensing images. For creating AID, we collect and annotate more than 10000 aerial scene images. In addition, a comprehensive review of the existing aerial scene classification techniques as well as recent widely used deep learning methods is given. Finally, we provide a performance analysis of typical aerial scene classification and deep learning approaches on AID, which can be served as the baseline results on this benchmark.","","","10.1109/TGRS.2017.2685945","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7907303","Aerial images;benchmark;scene classification","Remote sensing;Benchmark testing;Earth;Google;Semantics;Rivers;Performance evaluation","antennas;geophysical image processing;image classification;learning (artificial intelligence);remote sensing","aerial scene classification techniques;semantic category;high-resolution remote sensing imagery;scene classification algorithms;machine learning approach;data-driven approach;UC-Merced data set;WHU-RS19;aerial image data set;AID;deep learning methods","","213","100","","","","","IEEE","IEEE Journals"
"Learning to Classify Fine-Grained Categories with Privileged Visual-Semantic Misalignment","K. Chen; Z. Zhang","Department of Signal Processing, Tampere University of Technology, Tampere, Finland; Institute of Automation, CAS Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Beijing, P.R. China","IEEE Transactions on Big Data","","2017","3","1","37","43","Image categorisation is an active yet challenging research topic in computer vision, which is to classify the images according to their semantic content. Recently, fine-grained object categorisation has attracted wide attention and remains difficult due to feature inconsistency caused by smaller inter-class and larger intra-class variation as well as large varying poses. Most of the existing frameworks focused on exploiting a more discriminative imagery representation or developing a more robust classification framework to mitigate the suffering. The concern has recently been paid to discovering the dependency across fine-grained class labels based on Convolutional Neural Networks. Encouraged by the success of semantic label embedding to discover the fine-grained class labels' correlation, this paper exploits the misalignment between visual feature space and semantic label embedding space and incorporates it as a privileged information into a cost-sensitive learning framework. Owing to capturing both the variation of imagery feature representation and also the label correlation in the semantic label embedding space, such a visual-semantic misalignment can be employed to reflect the importance of instances, which is more informative that conventional cost-sensitivities. Experiment results demonstrate the effectiveness of the proposed framework on public fine-grained benchmarks with achieving superior performance to state-of-the-arts.","","","10.1109/TBDATA.2016.2602231","Academy of Finland; D2I SHOK project; Digile Oy; Nokia Technologies; Tampere; Finland; CSC-IT Center for Science; Finland for generous computational resources; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552556","Fine-grained categorisation;cost-sensitive learning;deep feature;visual-semantic alignment;multiclass classification","Semantics;Visualization;Training;Feature extraction;Correlation;Computer vision;Neural networks","computer vision;feature extraction;image classification;image representation;learning (artificial intelligence)","robust classification framework;visual feature space;semantic label embedding space;cost-sensitive learning framework;image categorisation;semantic content;privileged visual-semantic misalignment;fine-grained categories;feature inconsistency;interclass variation;intraclass variation;semantic label embedding;fine-grained class labels correlation;imagery feature representation variation;imagery feature representation variation","","3","63","","","","","IEEE","IEEE Journals"
"Effective Multi-Query Expansions: Collaborative Deep Networks for Robust Landmark Retrieval","Y. Wang; X. Lin; L. Wu; W. Zhang","University of New South Wales, Sydney, NSW, Australia; University of New South Wales, Sydney, NSW, Australia; Institute of social science research and ITEE, The University of Queensland, St Lucia, QLD, Australia; University of New South Wales, Sydney, NSW, Australia","IEEE Transactions on Image Processing","","2017","26","3","1393","1404","Given a query photo issued by a user (q-user), the landmark retrieval is to return a set of photos with their landmarks similar to those of the query, while the existing studies on the landmark retrieval focus on exploiting geometries of landmarks for similarity matches between candidate photos and a query photo. We observe that the same landmarks provided by different users over social media community may convey different geometry information depending on the viewpoints and/or angles, and may, subsequently, yield very different results. In fact, dealing with the landmarks with low quality shapes caused by the photography of q-users is often nontrivial and has seldom been studied. In this paper, we propose a novel framework, namely, multi-query expansions, to retrieve semantically robust landmarks by two steps. First, we identify the top-k photos regarding the latent topics of a query landmark to construct multi-query set so as to remedy its possible low quality shape. For this purpose, we significantly extend the techniques of Latent Dirichlet Allocation. Then, motivated by the typical collaborative filtering methods, we propose to learn a collaborative deep networks-based semantically, nonlinear, and high-level features over the latent factor for landmark photo as the training set, which is formed by matrix factorization over collaborative user-photo matrix regarding the multi-query set. The learned deep network is further applied to generate the features for all the other photos, meanwhile resulting into a compact multi-query set within such space. Then, the final ranking scores are calculated over the high-level feature space between the multi-query set and all other photos, which are ranked to serve as the final ranking list of landmark retrieval. Extensive experiments are conducted on real-world social media data with both landmark photos together with their user information to show the superior performance over the existing methods, especially our recently proposed multi-query based mid-level pattern representation method [1].","","","10.1109/TIP.2017.2655449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823000","Landmark photo retrieval;multi-query expansions;collaborative deep networks","Collaboration;Robustness;Social network services;Shape;Visualization;Geometry;Poles and towers","collaborative filtering;geometry;image matching;image retrieval;matrix decomposition;photography;query processing;social networking (online)","multiquery expansions;collaborative deep networks;robust landmark retrieval;query photo;landmark geometries;social media community;geometry information;q-users photography;latent Dirichlet allocation;collaborative filtering methods;collaborative deep networks-based semantically nonlinear high-level features;matrix factorization;collaborative user-photo matrix;high-level feature space;user information;multiquery based mid-level pattern representation","","58","56","","","","","IEEE","IEEE Journals"
"Learning to Segment Human by Watching YouTube","X. Liang; Y. Wei; L. Lin; Y. Chen; X. Shen; J. Yang; S. Yan","Sun Yat-sen University, Guangzhou, China; National University of Singapore, Singapore; Sun Yat-sen University, Guangzhou, China; National University of Singapore, Singapore; Adobe Research, San Jose, CA; SnapChat Inc., Venice, CA; 360 AI Institute and National University of Singapore, Singapore","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","7","1462","1468","An intuition on human segmentation is that when a human is moving in a video, the video-context (e.g., appearance and motion clues) may potentially infer reasonable mask information for the whole human body. Inspired by this, based on popular deep convolutional neural networks (CNN), we explore a very-weakly supervised learning framework for human segmentation task, where only an imperfect human detector is available along with massive weakly-labeled YouTube videos. In our solution, the video-context guided human mask inference and CNN based segmentation network learning iterate to mutually enhance each other until no further improvement gains. In the first step, each video is decomposed into supervoxels by the unsupervised video segmentation. The superpixels within the supervoxels are then classified as human or non-human by graph optimization with unary energies from the imperfect human detection results and the predicted confidence maps by the CNN trained in the previous iteration. In the second step, the video-context derived human masks are used as direct labels to train CNN. Extensive experiments on the challenging PASCAL VOC 2012 semantic segmentation benchmark demonstrate that the proposed framework has already achieved superior results than all previous weakly-supervised methods with object class or bounding box annotations. In addition, by augmenting with the annotated masks from PASCAL VOC 2012, our method reaches a new state-of-the-art performance on the human segmentation task.","","","10.1109/TPAMI.2016.2598340","State Key Development Program; National Natural Science Foundation of China; CCF-Tencent Open Fund; National University of Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7534869","Human segmentation;weakly-supervised learning;incremental learning;convolutional neural network","Training;Image segmentation;YouTube;Detectors;Motion segmentation;Semantics;Optimization","graph theory;image segmentation;neural nets;optimisation;social networking (online)","segment human;Watching YouTube;human segmentation;video context;reasonable mask information;deep convolutional neural networks;CNN;supervised learning framework;YouTube videos;human mask inference;graph optimization;video-context","","11","38","","","","","IEEE","IEEE Journals"
"Direct Speech Reconstruction From Articulatory Sensor Data by Machine Learning","J. A. Gonzalez; L. A. Cheah; A. M. Gomez; P. D. Green; J. M. Gilbert; S. R. Ell; R. K. Moore; E. Holdsworth","Department of Computer Science, University of Sheffield, Sheffield, U.K.; School of Engineering, University of Hull, Hull, U.K.; Department of Signal Theory, Telematics and Communications, University of Granada, Granada, Spain; Department of Computer Science, University of Sheffield, Sheffield, U.K.; School of Engineering, University of Hull, Hull, U.K.; Hull and East Yorkshire Hospitals Trust, Castle Hill Hospital, Cottingham, U.K.; Department of Computer Science, University of Sheffield, Sheffield, U.K.; Practical Control Ltd., Sheffield, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","12","2362","2374","This paper describes a technique that generates speech acoustics from articulator movements. Our motivation is to help people who can no longer speak following laryngectomy, a procedure that is carried out tens of thousands of times per year in the Western world. Our method for sensing articulator movement, permanent magnetic articulography, relies on small, unobtrusive magnets attached to the lips and tongue. Changes in magnetic field caused by magnet movements are sensed and form the input to a process that is trained to estimate speech acoustics. In the experiments reported here this “Direct Synthesis” technique is developed for normal speakers, with glued-on magnets, allowing us to train with parallel sensor and acoustic data. We describe three machine learning techniques for this task, based on Gaussian mixture models, deep neural networks, and recurrent neural networks (RNNs). We evaluate our techniques with objective acoustic distortion measures and subjective listening tests over spoken sentences read from novels (the CMU Arctic corpus). Our results show that the best performing technique is a bidirectional RNN (BiRNN), which employs both past and future contexts to predict the acoustics from the sensor data. BiRNNs are not suitable for synthesis in real time but fixed-lag RNNs give similar results and, because they only look a little way into the future, overcome this problem. Listening tests show that the speech produced by this method has a natural quality that preserves the identity of the speaker. Furthermore, we obtain up to 92% intelligibility on the challenging CMU Arctic material. To our knowledge, these are the best results obtained for a silent-speech system without a restricted vocabulary and with an unobtrusive device that delivers audio in close to real time. This work promises to lead to a technology that truly will give people whose larynx has been removed their voices back.","","","10.1109/TASLP.2017.2757263","National Institute for Health Research; Invention for Innovation Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8114382","Silent speech interfaces;articulatory-to-acoustic mapping;speech rehabilitation;permanent magnet articulography;speech synthesis","Biology;Speech recognition;Machine learning;Acoustics;Magnetacoustic effects;Recurrent neural networks;Real-time systems;Sensors","acoustic transducers;Gaussian processes;learning (artificial intelligence);magnetic field measurement;magnetic sensors;mixture models;permanent magnets;recurrent neural nets;speech processing;speech synthesis","Gaussian mixture models;deep neural networks;CMU Arctic corpus;BiRNN;silent-speech system;articulatory sensor data;Western world;permanent magnetic articulography;bidirectional recurrent neural networks;direct speech reconstruction;machine learning techniques;direct synthesis technique;speech acoustic generation;laryngectomy;magnetic field sensing;articulator movement sensing;fixed-lag RNN;objective acoustic distortion;subjective listening testing;CMU Arctic material","","5","52","Traditional","","","","IEEE","IEEE Journals"
"Graph Regularized Auto-Encoders for Image Representation","Y. Liao; Y. Wang; Y. Liu","State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Hangzhou, China; State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Hangzhou, China","IEEE Transactions on Image Processing","","2017","26","6","2839","2852","Image representation has been intensively explored in the domain of computer vision for its significant influence on the relative tasks such as image clustering and classification. It is valuable to learn a low-dimensional representation of an image which preserves its inherent information from the original image space. At the perspective of manifold learning, this is implemented with the local invariant idea to capture the intrinsic low-dimensional manifold embedded in the high-dimensional input space. Inspired by the recent successes of deep architectures, we propose a local invariant deep nonlinear mapping algorithm, called graph regularized auto-encoder (GAE). With the graph regularization, the proposed method preserves the local connectivity from the original image space to the representation space, while the stacked auto-encoders provide explicit encoding model for fast inference and powerful expressive capacity for complex modeling. Theoretical analysis shows that the graph regularizer penalizes the weighted Frobenius norm of the Jacobian matrix of the encoder mapping, where the weight matrix captures the local property in the input space. Furthermore, the underlying effects on the hidden representation space are revealed, providing insightful explanation to the advantage of the proposed method. Finally, the experimental results on both clustering and classification tasks demonstrate the effectiveness of our GAE as well as the correctness of the proposed theoretical analysis, and it also suggests that GAE is a superior solution to the current deep representation learning techniques comparing with variant auto-encoders and existing local invariant methods.","","","10.1109/TIP.2016.2605010","National Natural Science Foundation of China; Natural Science Foundation of Zhejiang Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7556994","Auto-encoders;graph regularization;local invariance","Manifolds;Image reconstruction;Algorithm design and analysis;Jacobian matrices;Decoding;Image representation;Inference algorithms","computer vision;graph theory;image classification;image coding;image representation;Jacobian matrices;learning (artificial intelligence);pattern clustering","graph regularized autoencoders;image representation;computer vision;image clustering;image classification;manifold learning;intrinsic low-dimensional manifold;high-dimensional input space;deep architectures;local invariant deep nonlinear mapping algorithm;GAE;encoding model;complex modeling;weighted Frobenius norm;Jacobian matrix;encoder mapping;weight matrix;hidden representation space;clustering;deep representation learning techniques","","14","48","","","","","IEEE","IEEE Journals"
"DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks","W. Ouyang; X. Zeng; X. Wang; S. Qiu; P. Luo; Y. Tian; H. Li; S. Yang; Z. Wang; H. Li; K. Wang; J. Yan; C. Loy; X. Tang","Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong; Department of Electronic Engineering, Chinese University of Hong Kong, Shatin, NT, Hong Kong","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","7","1320","1334","In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep learning object detection framework has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of model averaging. The proposed approach improves the mean averaged precision obtained by RCNN [1] , which was the state-of-the-art, from $31$  to  $50.3$  percent on the ILSVRC2014 detection test set. It also outperforms the winner of ILSVRC2014, GoogLeNet, by 6.1 percent. Detailed component-wise analysis is also provided through extensive experimental evaluation, which provides a global view for people to understand the deep learning object detection pipeline.","","","10.1109/TPAMI.2016.2587642","General Research Fund; Research Grants Council of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506134","CNN;convolutional neural networks;object detection;deep learning;deep model","Object detection;Context modeling;Deformable models;Machine learning;Visualization;Training;Neural networks","","","","23","84","","","","","IEEE","IEEE Journals"
"3D CNN Based Automatic Diagnosis of Attention Deficit Hyperactivity Disorder Using Functional and Structural MRI","L. Zou; J. Zheng; C. Miao; M. J. Mckeown; Z. J. Wang","Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; School of Computer Engineering, Nanyang Technological University, Singapore; Department of Medicine (Neurology), Pacific Parkinsons Research Centre, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada","IEEE Access","","2017","5","","23626","23636","Attention deficit hyperactivity disorder (ADHD) is one of the most common mental health disorders. As a neuro development disorder, neuroimaging technologies, such as magnetic resonance imaging (MRI), coupled with machine learning algorithms, are being increasingly explored as biomarkers in ADHD. Among various machine learning methods, deep learning has demonstrated excellent performance on many imaging tasks. With the availability of publically-available, large neuroimaging data sets for training purposes, deep learning-based automatic diagnosis of psychiatric disorders can become feasible. In this paper, we develop a deep learning-based ADHD classification method via 3-D convolutional neural networks (CNNs) applied to MRI scans. Since deep neural networks may utilize millions of parameters, even the large number of MRI samples in pooled data sets is still relatively limited if one is to learn discriminative features from the raw data. Instead, here we propose to first extract meaningful 3-D low-level features from functional MRI (fMRI) and structural MRI (sMRI) data. Furthermore, inspired by radiologists' typical approach for examining brain images, we design a 3-D CNN model to investigate the local spatial patterns of MRI features. Finally, we discover that brain functional and structural information are complementary, and design a multi-modality CNN architecture to combine fMRI and sMRI features. Evaluations on the hold-out testing data of the ADHD-200 global competition shows that the proposed multi-modality 3-D CNN approach achieves the state-of-the-art accuracy of 69.15% and outperforms reported classifiers in the literature, even with fewer training samples. We suggest that multi-modality classification will be a promising direction to find potential neuroimaging biomarkers of neuro development disorders.","","","10.1109/ACCESS.2017.2762703","NPRP through the Qatar National Research Fund (a member of Qatar Foundation); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8067637","Attention deficit hyperactive disorder;3D CNN;magnetic resonance imaging;multi-modality analysis","Feature extraction;Three-dimensional displays;Testing;Training;Neuroimaging;Biological neural networks","biomedical MRI;brain;diseases;feature extraction;image classification;learning (artificial intelligence);medical disorders;medical image processing;neural nets;neurophysiology","structural MRI;common mental health disorders;neuro development disorder;neuroimaging technologies;magnetic resonance imaging;machine learning algorithms;psychiatric disorders;deep neural networks;MRI samples;pooled data sets;functional MRI;brain images;MRI features;brain functional;multimodality CNN architecture;fMRI features;sMRI features;ADHD-200 global competition;3D convolutional neural networks;3D CNN based automatic diagnosis;hyperactivity disorder;neuroimaging data sets;3D low-level features;multimodality 3D CNN approach;3D CNN model;neuroimaging biomarkers;deep learning-based ADHD classification method","","17","40","","","","","IEEE","IEEE Journals"
"Hybrid CTC/Attention Architecture for End-to-End Speech Recognition","S. Watanabe; T. Hori; S. Kim; J. R. Hershey; T. Hayashi","Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Nagoya University, Nagoya, Japan","IEEE Journal of Selected Topics in Signal Processing","","2017","11","8","1240","1253","Conventional automatic speech recognition (ASR) based on a hidden Markov model (HMM)/deep neural network (DNN) is a very complicated system consisting of various modules such as acoustic, lexicon, and language models. It also requires linguistic resources, such as a pronunciation dictionary, tokenization, and phonetic context-dependency trees. On the other hand, end-to-end ASR has become a popular alternative to greatly simplify the model-building process of conventional ASR systems by representing complicated modules with a single deep network architecture, and by replacing the use of linguistic resources with a data-driven learning method. There are two major types of end-to-end architectures for ASR; attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC) uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes hybrid CTC/attention end-to-end ASR, which effectively utilizes the advantages of both architectures in training and decoding. During training, we employ the multiobjective learning framework to improve robustness and achieve fast convergence. During decoding, we perform joint decoding by combining both attention-based and CTC scores in a one-pass beam search algorithm to further eliminate irregular alignments. Experiments with English (WSJ and CHiME-4) tasks demonstrate the effectiveness of the proposed multiobjective learning over both the CTC and attention-based encoder-decoder baselines. Moreover, the proposed method is applied to two large-scale ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and exhibits performance that is comparable to conventional DNN/HMM ASR systems based on the advantages of both multiobjective learning and joint decoding without linguistic resources.","","","10.1109/JSTSP.2017.2763455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8068205","Automatic speech recognition;end-to-end;connectionist temporal classification;attention mechanism;hybrid CTC/attention","Hidden Markov models;Neural networks;Machine learning;Markov processes;Automatic speech recognition;Probabilistic logic","computational linguistics;dynamic programming;hidden Markov models;learning (artificial intelligence);neural nets;search problems;signal classification;speech coding;speech recognition","model-building process;conventional ASR systems;single deep network architecture;linguistic resources;end-to-end architectures;attention mechanism;hybrid CTC/attention end-to-end ASR;multiobjective learning framework;joint decoding;large-scale ASR benchmarks;conventional DNN/HMM ASR systems;hybrid CTC/attention architecture;end-to-end speech recognition;conventional automatic speech recognition;hidden Markov model;deep neural network;connectionist temporal classification;acoustic frames;one-pass beam search algorithm","","21","43","Traditional","","","","IEEE","IEEE Journals"
"Toward Domain Independence for Learning-Based Monocular Depth Estimation","M. Mancini; G. Costante; P. Valigi; T. A. Ciarfuglia; J. Delmerico; D. Scaramuzza","Department of Engineering, University of Perugia, Italy; Department of Engineering, University of Perugia, Italy; Department of Engineering, University of Perugia, Italy; Department of Engineering, University of Perugia, Italy; Robotics and Perception Group, University of Zurich, Switzerland; Robotics and Perception Group, University of Zurich, Switzerland","IEEE Robotics and Automation Letters","","2017","2","3","1778","1785","Modern autonomous mobile robots require a strong understanding of their surroundings in order to safely operate in cluttered and dynamic environments. Monocular depth estimation offers a geometry-independent paradigm to detect free, navigable space with minimum space, and power consumption. These represent highly desirable features, especially for microaerial vehicles. In order to guarantee robust operation in real-world scenarios, the estimator is required to generalize well in diverse environments. Most of the existent depth estimators do not consider generalization, and only benchmark their performance on publicly available datasets after specific fine tuning. Generalization can be achieved by training on several heterogeneous datasets, but their collection and labeling is costly. In this letter, we propose a deep neural network for scene depth estimation that is trained on synthetic datasets, which allow inexpensive generation of ground truth data. We show how this approach is able to generalize well across different scenarios. In addition, we show how the addition of long short-term memory layers in the network helps to alleviate, in sequential image streams, some of the intrinsic limitations of monocular vision, such as global scale estimation, with low computational overhead. We demonstrate that the network is able to generalize well with respect to different real-world environments without any fine tuning, achieving comparable performance to state-of-the-art methods on the KITTI dataset.","","","10.1109/LRA.2017.2657002","DARPA FLA Program; M.I.U.R. (Minstero dell’Istruzione dell’Università e della Ricerca); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829276","Collision avoidance;range sensing;visual-based navigation","Estimation;Training;Cameras;Streaming media;Feature extraction;Vehicles;Benchmark testing","learning (artificial intelligence);learning systems;mobile robots;neurocontrollers;recurrent neural nets;robot vision","domain independence;learning-based monocular depth estimation;autonomous mobile robots;geometry-independent paradigm;free navigable space detection;power consumption;microaerial vehicles;heterogeneous datasets;deep neural network;scene depth estimation;synthetic datasets;ground truth data generation;long short-term memory layers;sequential image streams;monocular vision;KITTI dataset","","17","31","","","","","IEEE","IEEE Journals"
"Cyberbullying Detection Based on Semantic-Enhanced Marginalized Denoising Auto-Encoder","R. Zhao; K. Mao","School of Electrical and Electronic Engineering, Nanyang Technological University, Nanyang Avenue, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Nanyang Avenue, Singapore","IEEE Transactions on Affective Computing","","2017","8","3","328","339","As a side effect of increasingly popular social media, cyberbullying has emerged as a serious problem afflicting children, adolescents and young adults. Machine learning techniques make automatic detection of bullying messages in social media possible, and this could help to construct a healthy and safe social media environment. In this meaningful research area, one critical issue is robust and discriminative numerical representation learning of text messages. In this paper, we propose a new representation learning method to tackle this problem. Our method named semantic-enhanced marginalized denoising auto-encoder (smSDA) is developed via semantic extension of the popular deep learning model stacked denoising autoencoder (SDA). The semantic extension consists of semantic dropout noise and sparsity constraints, where the semantic dropout noise is designed based on domain knowledge and the word embedding technique. Our proposed method is able to exploit the hidden feature structure of bullying information and learn a robust and discriminative representation of text. Comprehensive experiments on two public cyberbullying corpora (Twitter and MySpace) are conducted, and the results show that our proposed approaches outperform other baseline text representation learning methods.","","","10.1109/TAFFC.2016.2531682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7412690","Cyberbullying detection;text mining;representation learning;stacked denoising autoencoders;word embedding","Semantics;Noise reduction;Numerical models;Feature extraction;Media;Robustness;Analytical models","hidden feature removal;learning (artificial intelligence);security of data;social aspects of automation;social networking (online);text analysis","cyberbullying detection;semantic-enhanced marginalized denoising auto-encoder;text representation learning;hidden feature structure;deep learning model stacked denoising autoencoder;smSDA;discriminative numerical representation;machine learning;social media","","6","39","Traditional","","","","IEEE","IEEE Journals"
"A Skin Segmentation Algorithm Based on Stacked Autoencoders","Y. Lei; W. Yuan; H. Wang; Y. Wenhu; W. Bo","Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China; Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China; Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China; School of Astronautics, Harbin Institute of Technology, Harbin, China; Education Technology and Information Center, Shenzhen Polytechnic, Shenzhen, China","IEEE Transactions on Multimedia","","2017","19","4","740","749","A good skin detector that is capable of capturing skin tones under different conditions is important for human-machine interaction applications. In a general situation, skin detectors, such as skin probability maps or Gaussian mixture models, achieve acceptable skin segmentation results. However, the false positive rate increases significantly when the skin tones are in shadow or when skin-like background objects are under similar illumination. In this paper, we propose a novel skin feature learning algorithm based on stacked autoencoders, which are deep neural networks. To overcome the problems encountered in skin segmentation that are caused by different ethnicities and varying illumination conditions, the stacked autoencoders are utilized to learn more discriminative representations of the skin area in both the RGB color space and the HSV color space. Unlike traditional machine learning methods, instead of predicting each pixel individually, our algorithm utilizes blocks to learn the representations and detect the skin areas. The algorithm exploits the learning ability of deep neural networks to learn high-level representations of skin tones. Experiments on test images show that the proposed algorithm achieves acceptable results on several publicly available data sets. To reduce the difficulty of detecting skin pixels in these data sets, the ground truths of these data sets are commonly focused on foreground skin area detection. Our skin detector is also able to detect background areas, as shown in our experiments.","","","10.1109/TMM.2016.2638204","Shenzhen IOT Key Technology and Application Systems Integration Engineering Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7779068","Color space;machine learning;skin detection;stacked autoencoders","Skin;Image color analysis;Detectors;Lighting;Image segmentation;Histograms;Training","feature extraction;human computer interaction;image coding;image colour analysis;image representation;image segmentation;learning (artificial intelligence);neural nets;skin","skin segmentation algorithm;stacked autoencoders;skin detector;skin tones;human-machine interaction applications;skin feature learning algorithm;deep neural networks;discriminative representations;RGB color space;HSV color space;machine learning methods","","13","43","","","","","IEEE","IEEE Journals"
"Improving stereo matching by incorporating geometry prior into ConvNet","Z. Liang; H. Liu; L. Qiao; Y. Feng; W. Chen","National University of Defense Technology, People's Republic of China; National University of Defense Technology, People's Republic of China; National University of Defense Technology, People's Republic of China; National University of Defense Technology, People's Republic of China; National University of Defense Technology, People's Republic of China","Electronics Letters","","2017","53","17","1194","1196","Deep learning-based methods for stereo matching have shown superior performance over traditional ones. However, most of them ignore the inherent geometry prior of stereo matching when training, i.e. the reference image can be reconstructed from the second image in the visible regions. The reconstruction can be achieved by backward warping the second image using the disparity map of the reference image, while the visible regions can be calculated by left-right consistency check. This prior is useful especially when the ground truth disparity is sparse (e.g. the outdoor scene such as KITTI 2015). This prior incorporated into a two-stage end-to-end training process, both of which try to minimise the end-point-error with respect to the sparse ground truth disparity (supervised learning), and the reconstruction error (self-supervised learning). The predicted disparity and the reconstruction error of the first stage act as additional information, and are fed to the second stage to make further use of this prior knowledge to improve performance. Experiments on the challenging KITTI 2015 dataset show that the method improves the results in the foreground region, and ranks first among all the published methods on the D1-fg metric.","","","10.1049/el.2017.2418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8024209","","","geometry;image matching;image reconstruction;learning (artificial intelligence);stereo image processing","foreground region;D1-fg metric;predicted disparity;self-supervised learning;reconstruction error;supervised learning;sparse ground truth disparity;end-point-error minimisation;two-stage end-to-end training process;challenging KITTI 2015 dataset;left-right consistency check;reference image;disparity map;backward warping;deep learning-based methods;ConvNet;geometry;stereo matching","","1","12","","","","","IET","IET Journals"
"Small-Footprint Highway Deep Neural Networks for Speech Recognition","L. Lu; S. Renals","Toyota Technological Institute at Chicago, Chicago, IL, USA; The University of Edinburgh, Edinburgh, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","7","1502","1511","State-of-the-art speech recognition systems typically employ neural network acoustic models. However, compared to Gaussian mixture models, deep neural network (DNN) based acoustic models often have many more model parameters, making it challenging for them to be deployed on resource-constrained platforms, such as mobile devices. In this paper, we study the application of the recently proposed highway deep neural network (HDNN) for training small-footprint acoustic models. HDNNs are a depth-gated feedforward neural network, which include two types of gate functions to facilitate the information flow through different layers. Our study demonstrates that HDNNs are more compact than regular DNNs for acoustic modeling, i.e., they can achieve comparable recognition accuracy with many fewer model parameters. Furthermore, HDNNs are more controllable than DNNs: The gate functions of an HDNN can control the behavior of the whole network using a very small number of model parameters. Finally, we show that HDNNs are more adaptable than DNNs. For example, simply updating the gate functions using adaptation data can result in considerable gains in accuracy. We demonstrate these aspects by experiments using the publicly available AMI corpus, which has around 80 h of training data.","","","10.1109/TASLP.2017.2698723","EPSRC; Natural Speech Technology; and in part by the European Union under H2020 Project SUMMA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7913606","Deep learning;highway networks;small-footprint models;speech recognition","Logic gates;Training;Neural networks;Hidden Markov models;Adaptation models;Speech recognition;Road transportation","recurrent neural nets;speech recognition","small-footprint highway deep neural networks;speech recognition systems;neural network acoustic models;Gaussian mixture models;HDNN;resource-constrained platforms;mobile devices;depth-gated feedforward neural network;recognition accuracy;gate functions;adaptation data;AMI corpus","","6","49","","","","","IEEE","IEEE Journals"
"Data-Driven Feature Characterization Techniques for Laser Printer Attribution","A. Ferreira; L. Bondi; L. Baroffio; P. Bestagini; J. Huang; J. A. dos Santos; S. Tubaro; A. Rocha","Shenzhen Key Laboratory of Media Security, College of Information Engineering, Shenzhen University, Shenzhen, China; Dipartimento di Elettronica Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy; Dipartimento di Elettronica Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy; Dipartimento di Elettronica Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy; Shenzhen Key Laboratory of Media Security, College of Information Engineering, Shenzhen University, Shenzhen, China; Department of Computer Science, Universidade Federal de Minas Gerais, Belo Horizonte-MG, Brazil; Dipartimento di Elettronica Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy; Institute of Computing, University of Campinas, Campinas-SP CEP, Brazil","IEEE Transactions on Information Forensics and Security","","2017","12","8","1860","1873","Laser printer attribution is an increasing problem with several applications, such as pointing out the ownership of crime proofs and authentication of printed documents. However, as commonly proposed methods for this task are based on custom-tailored features, they are limited by modeling assumptions about printing artifacts. In this paper, we explore solutions able to learn discriminant-printing patterns directly from the available data during an investigation, without any further feature engineering, proposing the first approach based on deep learning to laser printer attribution. This allows us to avoid any prior assumption about printing artifacts that characterize each printer, thus highlighting almost invisible and difficult printer footprints generated during the printing process. The proposed approach merges, in a synergistic fashion, convolutional neural networks (CNNs) applied on multiple representations of multiple data. Multiple representations, generated through different pre-processing operations, enable the use of the small and lightweight CNNs whilst the use of multiple data enable the use of aggregation procedures to better determine the provenance of a document. Experimental results show that the proposed method is robust to noisy data and outperforms existing counterparts in the literature for this problem.","","","10.1109/TIFS.2017.2692722","NSFC; Shenzhen Research and Development Program; Brazilian National Council for Scientific and Technological Development; São Paulo Research Foundation-FAPESP; DéjàVu Project, Minas Gerais Research Foundation-FAPEMIG; Brazilian Coordination for the Improvement of Higher Level Education Personnel through the DeepEyes Project; Microsoft Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7895220","Laser printer attribution;deep learning-based document provenance analysis;convolutional neural networks;multiple representation;multiple data","Printers;Printing;Feature extraction;Training;Image color analysis;Laser applications","document image processing;feature extraction;feedforward neural nets;laser printers;learning (artificial intelligence);security of data","data representations;CNN;convolutional neural networks;printing process;printing artifacts;deep learning;feature engineering;discriminant-printing patterns;custom-tailored features;laser printer attribution;data-driven feature characterization techniques","","9","53","","","","","IEEE","IEEE Journals"
"Semi-supervised Stacked Label Consistent Autoencoder for Reconstruction and Analysis of Biomedical Signals","A. Gogna; A. Majumdar; R. Ward","Indraprasatha Institute of Information Technology; Indraprasatha Institute of Information Technology, Delhi, India; University of British Columbia","IEEE Transactions on Biomedical Engineering","","2017","64","9","2196","2205","Objective: An autoencoder-based framework that simultaneously reconstruct and classify biomedical signals is proposed. Previous work has treated reconstruction and classification as separate problems. This is the first study that proposes a combined framework to address the issue in a holistic fashion. Methods: For telemonitoring purposes, reconstruction techniques of biomedical signals are largely based on compressed sensing (CS); these are “designed” techniques where the reconstruction formulation is based on some “assumption” regarding the signal. In this study, we propose a new paradigm for reconstruction-the reconstruction is “learned,” using an autoencoder; it does not require any assumption regarding the signal as long as there is sufficiently large training data. But since the final goal is to analyze/classify the signal, the system can also learn a linear classification map that is added inside the autoencoder. The ensuing optimization problem is solved using the Split Bregman technique. Results: Experiments were carried out on reconstructing and classifying electrocardiogram (ECG) (arrhythmia classification) and EEG (seizure classification) signals. Conclusion: Our proposed tool is capable of operating in a semi-supervised fashion. We show that our proposed method is better in reconstruction and more than an order magnitude faster than CS based methods; it is capable of real-time operation. Our method also yields better results than recently proposed classification methods. Significance: This is the first study offering an alternative to CS-based reconstruction. It also shows that the representation learning approach can yield better results than traditional methods that use hand-crafted features for signal analysis.","","","10.1109/TBME.2016.2631620","Qatar National Research Fund; Qatar Foundation; NPRP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752836","Body area network (BAN);classification;deep learning;reconstruction","Electroencephalography;Standards;Correlation;Compressed sensing;Signal analysis;Monitoring;Image reconstruction","bioelectric potentials;biomedical telemetry;body area networks;compressed sensing;electrocardiography;electroencephalography;learning (artificial intelligence);medical disorders;medical signal processing;neurophysiology;signal classification;signal reconstruction;telemedicine","semisupervised stacked label consistent autoencoder;biomedical signal reconstruction;telemonitoring purposes;compressed sensing;linear classification map;optimization problem;Split Bregman technique;ECG signal reconstruction;arrhythmia classification;EEG signal reconstruction;seizure classification","Algorithms;Electrocardiography;Information Storage and Retrieval;Pattern Recognition, Automated;Signal Processing, Computer-Assisted;Supervised Machine Learning","11","64","","","","","IEEE","IEEE Journals"
"Guest Editorial: Deep Learning in Computer Vision","","","IET Computer Vision","","2017","11","8","621","622","","","","10.1049/iet-cvi.2017.0496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8232563","","","","","","","","","","","","IET","IET Journals"
"Airport Detection on Optical Satellite Images Using Deep Convolutional Neural Networks","P. Zhang; X. Niu; Y. Dou; F. Xia","National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, China; National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, China; National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, China; Institute of Electronic Information Warfare, Electronic Engineering College, Naval University of Engineering, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","8","1183","1187","This letter proposes a method using convolutional neural networks (CNNs) for airport detection on optical satellite images. To efficiently build a deep CNN with limited satellite image samples, a transfer learning approach had been employed by sharing the common image features of the natural images. To decrease the computing cost, an efficient region proposal method had been proposed based on the prior knowledge of the line segments distribution in an airport. The transfer learning ability on deep CNN for airport detection on satellite images had been first evaluated in this letter. The proposed method was tested on an image data set, including 170 different airports and 30 nonairports. The detection rate could reach 88.8% in experiments with seconds' computation time, which showed a great improvement over other the state-of-the-art methods.","","","10.1109/LGRS.2017.2673118","National Natural Science Foundation of China; China Postdoctoral Science Foundation; Natural Science Foundation of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7946140","Airport detection;convolutional neural network (CNN);line segment detector (LSD);transfer learning","Airports;Proposals;Satellites;Joining processes;Optical imaging;Feature extraction;Image segmentation","airports;feature extraction;geophysical image processing;neural nets;remote sensing","airport detection;optical satellite image;convolutional neural networks;deep CNN;transfer learning approach;image feature","","12","15","","","","","IEEE","IEEE Journals"
"Complex-Valued Convolutional Neural Network and Its Application in Polarimetric SAR Image Classification","Z. Zhang; H. Wang; F. Xu; Y. Jin","Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","12","7177","7188","Following the great success of deep convolutional neural networks (CNNs) in computer vision, this paper proposes a complex-valued CNN (CV-CNN) specifically for synthetic aperture radar (SAR) image interpretation. It utilizes both amplitude and phase information of complex SAR imagery. All elements of CNN including input-output layer, convolution layer, activation function, and pooling layer are extended to the complex domain. Moreover, a complex backpropagation algorithm based on stochastic gradient descent is derived for CV-CNN training. The proposed CV-CNN is then tested on the typical polarimetric SAR image classification task which classifies each pixel into known terrain types via supervised training. Experiments with the benchmark data sets of Flevoland and Oberpfaffenhofen show that the classification error can be further reduced if employing CV-CNN instead of conventional real-valued CNN with the same degrees of freedom. The performance of CV-CNN is comparable to that of existing state-of-the-art methods in terms of overall classification accuracy.","","","10.1109/TGRS.2017.2743222","National Key R&D Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8039431","Complex-valued convolutional neural network (CV-CNN);deep learning;synthetic aperture radar (SAR);terrain classification","Synthetic aperture radar;Convolutional neural networks;Feature extraction;Neural networks;Computer vision;Training data;Machine learning","computer vision;gradient methods;image classification;learning (artificial intelligence);neural nets;radar computing;radar imaging;radar polarimetry;synthetic aperture radar","typical polarimetric SAR image classification task;supervised training;classification error;CV-CNN training;stochastic gradient descent;complex backpropagation algorithm;pooling layer;activation function;complex SAR imagery;phase information;synthetic aperture radar image interpretation;complex-valued CNN;computer vision;deep convolutional neural networks;convolutional neural network","","46","38","","","","","IEEE","IEEE Journals"
"Wind Turbine Gearbox Failure Identification With Deep Neural Networks","L. Wang; Z. Zhang; H. Long; J. Xu; R. Liu","Department of Systems Engineering and Engineering Management, College of Science and Engineering, City University of Hong Kong, Kowloon, Hong Kong; Department of Systems Engineering and Engineering Management, College of Science and Engineering, City University of Hong Kong, Kowloon, Hong Kong; Department of Systems Engineering and Engineering Management, College of Science and Engineering, City University of Hong Kong, Kowloon, Hong Kong; Centre of Wind Farm Data Analysis and Performance Optimization, China Longyuan Power Group Corporation Ltd., Beijing, China; Centre of Wind Farm Data Analysis and Performance Optimization, China Longyuan Power Group Corporation Ltd., Beijing, China","IEEE Transactions on Industrial Informatics","","2017","13","3","1360","1368","The feasibility of monitoring the health of wind turbine (WT) gearboxes based on the lubricant pressure data in the supervisory control and data acquisition system is investigated in this paper. A deep neural network (DNN)-based framework is developed to monitor conditions of WT gearboxes and identify their impending failures. Six data-mining algorithms, the k-nearest neighbors, least absolute shrinkage and selection operator, ridge regression (Ridge), support vector machines, shallow neural network, as well as DNN, are applied to model the lubricant pressure. A comparative analysis of developed data-driven models is conducted and the DNN model is the most accurate. To prevent the overfitting of the DNN model, a dropout algorithm is applied into the DNN training process. Computational results show that the prediction error will shift before the occurrences of gearbox failures. An exponentially weighted moving average control chart is deployed to derive criteria for detecting the shifts. The effectiveness of the proposed monitoring approach is demonstrated by examining real cases from wind farms in China and benchmarked against the gearbox monitoring based on the oil temperature data.","","","10.1109/TII.2016.2607179","Early Career Scheme Grant from the Research Grants Council of the Hong Kong Special Administrative Region; CityU Strategic Research Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7563362","Condition monitoring;data mining;deep neural network (DNN);lubricant pressure;wind turbine gearbox","Monitoring;Lubricants;Training;Computational modeling;Vibrations;Neural networks;Predictive models","condition monitoring;control charts;data mining;gears;learning (artificial intelligence);lubricants;mechanical engineering computing;moving average processes;neural nets;regression analysis;SCADA systems;support vector machines;wind turbines","wind turbine gearbox failure identification;deep neural networks;health monitoring;WT;lubricant pressure data;supervisory control and data acquisition system;data-mining algorithms;k-nearest neighbors;least absolute shrinkage;selection operator;ridge regression;support vector machines;shallow neural network;dropout algorithm;DNN training process;prediction error;exponentially weighted moving average control chart;wind farms;China;oil temperature data","","61","41","","","","","IEEE","IEEE Journals"
"Radar HRRP Target Recognition Based on t-SNE Segmentation and Discriminant Deep Belief Network","M. Pan; J. Jiang; Q. Kong; J. Shi; Q. Sheng; T. Zhou","School of Electronics and Information, Hangzhou Dianzi University, Hangzhou, China; School of Electronics and Information, Hangzhou Dianzi University, Hangzhou, China; School of Electronics and Information, Hangzhou Dianzi University, Hangzhou, China; School of Electronics and Information, Hangzhou Dianzi University, Hangzhou, China; School of Electronics and Information, Hangzhou Dianzi University, Hangzhou, China; School of Electronics and Information, Hangzhou Dianzi University, Hangzhou, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","9","1609","1613","In radar high-resolution range profile (HRRP)-based target recognition, one of the most challenging tasks is the noncooperative target recognition with imbalanced training data set. This letter presents a novel recognition framework to deal with this problem. The framework is composed of two steps: first, the t-distributed stochastic neighbor embedding (t-SNE) and synthetic sampling are utilized for data preprocessing to provide a well segmented and balanced HRRP data set; second, a discriminant deep belief network (DDBN) is proposed to recognize HRRP data. Compared with the conventional recognition models, the proposed framework not only makes better use of data set inherent structure among HRRP samples for segmentation, but also utilizes high-level features for recognition. Moreover, the DDBN shares latent information of HRRP data globally, which can enhance the ability of modeling the aspect sectors with few HRRP data. The experiments illustrate the meaning of the t-SNE, and validate the effectiveness of the proposed recognition framework with imbalanced HRRP data.","","","10.1109/LGRS.2017.2726098","National Natural Science Foundation of China; Natural Science Foundation of Zhejiang province; “Electronic Science and Technology”–Zhejiang Open Foundation of the Most Important Subjects; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7999232","Discriminant deep belief network (DDBN);high-resolution range profile (HRRP);imbalanced data;noncooperative target recognition;t-distributed stochastic neighbor embedding (t-SNE)","Target recognition;Training data;Radar;Data models;Sensitivity;Stochastic processes;Data preprocessing","belief networks;learning (artificial intelligence);object recognition;radar computing;radar resolution;signal resolution","radar HRRP target recognition;t-SNE segmentation;discriminant deep belief network;high-resolution range profile radar;noncooperative target recognition;imbalanced training data set;t-distributed stochastic neighbor embedding;synthetic sampling;data preprocessing;DDBN","","7","18","","","","","IEEE","IEEE Journals"
"Sequential Deep Trajectory Descriptor for Action Recognition With Three-Stream CNN","Y. Shi; Y. Tian; Y. Wang; T. Huang","National Engineering Laboratory for Video Technology, Cooperative Medianet Innovation Center, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; National Engineering Laboratory for Video Technology, Cooperative Medianet Innovation Center, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; National Engineering Laboratory for Video Technology, Cooperative Medianet Innovation Center, School of Electronics Engineering and Computer Science, Peking University, Beijing, China","IEEE Transactions on Multimedia","","2017","19","7","1510","1520","Learning the spatial-temporal representation of motion information is crucial to human action recognition. Nevertheless, most of the existing features or descriptors cannot capture motion information effectively, especially for long-term motion. To address this problem, this paper proposes a long-term motion descriptor called sequential deep trajectory descriptor (sDTD). Specifically, we project dense trajectories into two-dimensional planes, and subsequently a CNN-RNN network is employed to learn an effective representation for long-term motion. Unlike the popular two-stream ConvNets, the sDTD stream is introduced into a three-stream framework so as to identify actions from a video sequence. Consequently, this three-stream framework can simultaneously capture static spatial features, short-term motion, and long-term motion in the video. Extensive experiments were conducted on three challenging datasets: KTH, HMDB51, and UCF101. Experimental results show that our method achieves state-of-the-art performance on the KTH and UCF101 datasets, and is comparable to the state-of-the-art methods on the HMDB51 dataset.","","","10.1109/TMM.2017.2666540","National Basic Research Program of China; National Natural Science Foundation of China; Beijing Municipal Commission of Science and Technology; Shenzhen Peacock Plan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847353","Action recognition;sequential deep trajectory descriptor (sDTD);three-stream framework;long-term motion","Trajectory;Feature extraction;Optical imaging;Cameras;Streaming media;Neural networks;Histograms","image representation;image sequences;motion estimation;object recognition;recurrent neural nets;spatiotemporal phenomena;video databases;video signal processing","sequential deep-trajectory descriptor;three-stream CNN;spatial-temporal motion information representation learning;human action recognition;long-term motion descriptor;sDTD stream;two-dimensional planes;CNN-RNN network;video sequence;static spatial features;short-term motion;KTH dataset;HMDB51 dataset;UCF101 dataset","","52","59","","","","","IEEE","IEEE Journals"
"Qualitative Action Recognition by Wireless Radio Signals in Human–Machine Systems","S. Lv; Y. Lu; M. Dong; X. Wang; Y. Dou; W. Zhuang","National Laboratory of Parallel and Distributed Processing, National University of Defense Technology, Changsha, China; National Laboratory of Parallel and Distributed Processing, National University of Defense Technology, Changsha, China; Department of Information and Electronic Engineering, Muroran Institute of Technology, Muroran, Japan; National Laboratory of Parallel and Distributed Processing, National University of Defense Technology, Changsha, China; National Laboratory of Parallel and Distributed Processing, National University of Defense Technology, Changsha, China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Human-Machine Systems","","2017","47","6","789","800","Human-machine systems required a deep understanding of human behaviors. Most existing research on action recognition has focused on discriminating between different actions, however, the quality of executing an action has received little attention thus far. In this paper, we study the quality assessment of driving behaviors and present WiQ, a system to assess the quality of actions based on radio signals. This system includes three key components, a deep neural network based learning engine to extract the quality information from the changes of signal strength, a gradient-based method to detect the signal boundary for an individual action, and an activity-based fusion policy to improve the recognition performance in a noisy environment. By using the quality information, WiQ can differentiate a triple body status with an accuracy of 97%, whereas for identification among 15 drivers, the average accuracy is 88%. Our results show that, via dedicated analysis of radio signals, a fine-grained action characterization can be achieved, which can facilitate a large variety of applications, such as smart driving assistants.","","","10.1109/THMS.2017.2693242","National Natural Science Foundation of China; JSPS KAKENHI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917345","Human–computer interaction, machine learning, neural networks","Machine learning;Human computer interaction;Quality assessment;Wireless communication;Sensor systems;Neural networks;Wireless networks","behavioural sciences computing;driver information systems;learning (artificial intelligence);man-machine systems;neural nets;sensor fusion","qualitative action recognition;wireless radio signals;human-machine systems;human behaviors;WiQ;signal strength;gradient-based method;signal boundary;fine-grained action characterization;deep neural network based learning engine;activity-based fusion policy;driving behavior quality assessment","","7","34","Traditional","","","","IEEE","IEEE Journals"
"Retrieval From and Understanding of Large-Scale Multi-modal Medical Datasets: A Review","H. Müller; D. Unay","Information Systems Institute, HES-SO Valais, Sierre, Switzerland; Biomedical Engineering Department, Izmir University of Economics, Izmir, Turkey","IEEE Transactions on Multimedia","","2017","19","9","2093","2104","Content-based multimedia retrieval (CBMR) has been an active research domain since the mid 1990s. In medicine visual retrieval started later and has mostly remained a research instrument and less a clinical tool. The limited size of data sets due to privacy constraints is often mentioned as reason for these limitations. Nevertheless, much work has been done in CBMR, including the availability of increasingly large data sets and scientific challenges. Annotated data sets and clinical data for images have now become available and can be combined for multi-modal retrieval. Much has been learned on user behavior and application scenarios. This text is motivated by the advances in medical image analysis and the availability of public large data sets that often include clinical data. It is a systematic review of recent work (concentrating on the period 2011-2017) on multi-modal CBMR and image understanding in the medical domain, where image understanding includes techniques such as detection, localization, and classification for leveraging visual content. With the objective of summarizing the current state of research for multimedia researchers outside the medical field, the text provides ways to get data sets and identifies current limitations and promising research directions. The text highlights advances in the past six years and a trend to use larger scale training data and deep learning approaches that can replace/complement hand-crafted features. Using images alone will likely only work in limited domains but combining multiple sources of data for multi-modal retrieval has the biggest chances of success, particularly for clinical impact.","","","10.1109/TMM.2017.2729400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984864","Big data;content–based image retrieval;deep learning;large scale datasets;medical images;multi–modality","Medical diagnostic imaging;Multimedia communication;Visualization;Tools;Machine learning","Big Data;data privacy;image retrieval;information retrieval;medical administrative data processing;medical image processing","large-scale multimodal medical datasets;content-based multimedia retrieval;active research domain;medicine visual retrieval;research instrument;clinical tool;privacy constraints;annotated data sets;clinical data;multimodal retrieval;user behavior;medical image analysis;public large data sets;multimodal CBMR;medical domain;multimedia researchers;medical field;training data;deep learning;hand-crafted features;clinical impact","","3","98","","","","","IEEE","IEEE Journals"
"Deep Feature Fusion for VHR Remote Sensing Scene Classification","S. Chaib; H. Liu; Y. Gu; H. Yao","School of Computer Science, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Computer Science, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","8","4775","4784","The rapid development of remote sensing technology allows us to get images with high and very high resolution (VHR). VHR imagery scene classification has become an important and challenging problem. In this paper, we introduce a framework for VHR scene understanding. First, the pretrained visual geometry group network (VGG-Net) model is proposed as deep feature extractors to extract informative features from the original VHR images. Second, we select the fully connected layers constructed by VGG-Net in which each layer is regarded as separated feature descriptors. And then we combine between them to construct final representation of the VHR image scenes. Third, discriminant correlation analysis (DCA) is adopted as feature fusion strategy to further refine the original features extracting from VGG-Net, which allows a more efficient fusion approach with small cost than the traditional feature fusion strategies. We apply our approach to three challenging data sets: 1) UC MERCED data set that contains 21 different areal scene categories with submeter resolution; 2) WHU-RS data set that contains 19 challenging scene categories with various resolutions; and 3) the Aerial Image data set that has a number of 10 000 images within 30 challenging scene categories with various resolutions. The experimental results demonstrate that our proposed method outperforms the state-of-the-art approaches. Using feature fusion technique achieves a higher accuracy than solely using the raw deep features. Moreover, the proposed method based on DCA fusion produces good informative features to describe the images scene with much lower dimension.","","","10.1109/TGRS.2017.2700322","Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934005","Discriminant correlation analysis (DCA);features fusion;scene classification;unsupervised features learning","Feature extraction;Remote sensing;Image resolution;Visualization;Correlation;Principal component analysis;Machine learning","feature extraction;geophysical image processing;image classification;image fusion;remote sensing","deep feature fusion;VHR remote sensing scene classification;remote sensing technology;very high resolution imagery scene classification;VHR imagery scene classification;visual geometry group network;VGG-Net model;feature extractor;VHR image scenes;discriminant correlation analysis;feature fusion strategy;UC MERCED dataset;Aerial Image dataset;feature fusion technique","","53","46","","","","","IEEE","IEEE Journals"
"A Novel Deep Embedding Network for Building Shape Recognition","S. Tian; Y. Zhang; J. Zhang; N. Su","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","11","2127","2131","Building shape, as a key structured element, plays a significant role in various urban remote sensing applications. However, because of high complexity and intraclass variations between building structures, the capability of building shape description and recognition becomes limited or even impoverished. In this letter, a novel deep embedding network is proposed for building shape recognition, which combines the strength of the unsupervised feature learning of convolutional neural networks (CNNs) and a novel triplet loss. Specifically, we take advantage of the strong discriminative power of CNNs to learn an efficient building shape representation for shape recognition. With this deep embedding network, the high-dimensional image space can be mapped into a low-dimensional feature space, and the deep features can effectively reduce the intraclass variations while increasing the interclass variation between different building shape images. Afterward, the derived deep features are exploited for the process of building shape recognition. This method consists of two stages. In the first stage, for standard building shape image queries stored in the shape primitives library and the building shape data set, two sets of deep features are extracted with the deep embedding network. In the second stage, we formulate the shape recognition task into a feature matching problem and the final building shape recognition results can be achieved by set-to-set feature matching method. Experiments on the VHR-10 and UCML data sets demonstrate the effectiveness and precision of the proposed method.","","","10.1109/LGRS.2017.2753821","National Natural Science Foundation of China; Defense Industrial Technology Development Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8065026","Convolutional neural networks (CNNs);object recognition;remote sensing (RS);shape matching","Shape;Buildings;Feature extraction;Training;Libraries;Object recognition","convolution;feature extraction;geophysical image processing;geophysics computing;image matching;image representation;neural nets;remote sensing;shape recognition;unsupervised learning","unsupervised feature learning;convolutional neural networks;shape recognition task;set-to-set feature matching method;building shape representation;building shape images;building shape recognition;urban remote sensing;VHR-10;UCML data sets","","","20","Traditional","","","","IEEE","IEEE Journals"
"Deep Neural Network for Structural Prediction and Lane Detection in Traffic Scene","J. Li; X. Mei; D. Prokhorov; D. Tao","Centre for Quantum Computation Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; Toyota Research Institute, Ann Arbor, MI, USA; Toyota Research Institute, Ann Arbor, MI, USA; Centre for Quantum Computation Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","3","690","703","Hierarchical neural networks have been shown to be effective in learning representative image features and recognizing object classes. However, most existing networks combine the low/middle level cues for classification without accounting for any spatial structures. For applications such as understanding a scene, how the visual cues are spatially distributed in an image becomes essential for successful analysis. This paper extends the framework of deep neural networks by accounting for the structural cues in the visual signals. In particular, two kinds of neural networks have been proposed. First, we develop a multitask deep convolutional network, which simultaneously detects the presence of the target and the geometric attributes (location and orientation) of the target with respect to the region of interest. Second, a recurrent neuron layer is adopted for structured visual detection. The recurrent neurons can deal with the spatial distribution of visible cues belonging to an object whose shape or structure is difficult to explicitly define. Both the networks are demonstrated by the practical task of detecting lane boundaries in traffic scenes. The multitask convolutional neural network provides auxiliary geometric information to help the subsequent modeling of the given lane structures. The recurrent neural network automatically detects lane boundaries, including those areas containing no marks, without any explicit prior knowledge or secondary modeling.","","","10.1109/TNNLS.2016.2522428","Toyota Research Institute Collaborative Project, North America; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407673","Image recognition;pattern analysis;recurrent neural networks","Biological neural networks;Visualization;Neurons;Convolution;Image recognition;Feature extraction;Image analysis","edge detection;recurrent neural nets","recurrent neural network;multitask convolutional neural network;traffic scenes;lane boundary detection;structured visual detection;recurrent neuron layer;region of interest;geometric attributes;multitask deep convolutional network;traffic scene;structural prediction","","90","49","","","","","IEEE","IEEE Journals"
"Deep Convolutional Neural Network for Inverse Problems in Imaging","K. H. Jin; M. T. McCann; E. Froustey; M. Unser","Biomedical Imaging Group, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Center for Biomedical Imaging, Signal Processing Core and the Biomedical Imaging Group, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Biomedical Imaging Group, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Biomedical Imaging Group, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland","IEEE Transactions on Image Processing","","2017","26","9","4509","4522","In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyperparameter selection. The starting point of this paper is the observation that unrolled iterative methods have the form of a CNN (filtering followed by pointwise nonlinearity) when the normal operator (H*H, where H* is the adjoint of the forward imaging operator, H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 × 512 image on the GPU.","","","10.1109/TIP.2017.2713099","European Union’s Horizon 2020 Framework Programme for Research and Innovation (call 2015); Center for Biomedical Imaging of the Geneva-Lausanne Universities and EPFL; European Research Council (H2020-ERC Project GlobalBioIm); National Institute of Biomedical Imaging and Bioengineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949028","Image restoration;image reconstruction;tomography;computed tomography;magnetic resonance imaging;biomedical signal processing;biomedical imaging;reconstruction algorithms","Image reconstruction;Convolution;Inverse problems;Computed tomography;Neural networks;Iterative methods","computerised tomography;feedforward neural nets;image resolution;iterative methods;learning (artificial intelligence);medical image processing","deep convolutional neural network;CNN;ill-posed inverse problems;regularized iterative algorithms;forward operators;adjoint operators;hyperparameter selection;forward model;direct inversion;normal-convolutional inverse problems;multiresolution decomposition;residual learning;image structure;parallel beam X-ray computed tomography;synthetic phantoms;total variation-regularized iterative reconstruction;GPU","","99","65","","","","","IEEE","IEEE Journals"
"Efficient Stereo Matching Leveraging Deep Local and Context Information","X. Ye; J. Li; H. Wang; H. Huang; X. Zhang","Chinese Academy of Sciences, Shanghai Institute of Microsystem and Information Technology, Shanghai, China; Chinese Academy of Sciences, Shanghai Institute of Microsystem and Information Technology, Shanghai, China; Chinese Academy of Sciences, Shanghai Institute of Microsystem and Information Technology, Shanghai, China; Shanghai Open University, Shanghai, China; Chinese Academy of Sciences, Shanghai Institute of Microsystem and Information Technology, Shanghai, China","IEEE Access","","2017","5","","18745","18755","Stereo matching is a challenging problem with respect to weak texture, discontinuities, illumination difference and occlusions. Therefore, a deep learning framework is presented in this paper, which focuses on the first and last stage of typical stereo methods: the matching cost computation and the disparity refinement. For matching cost computation, two patch-based network architectures are exploited to allow the trade-off between speed and accuracy, both of which leverage multi-size and multi-layer pooling unit with no strides to learn cross-scale feature representations. For disparity refinement, unlike traditional handcrafted refinement algorithms, we incorporate the initial optimal and sub-optimal disparity maps before outlier detection. Furthermore, diverse base learners are encouraged to focus on specific replacement tasks, corresponding to the smooth regions and details. Experiments on different datasets demonstrate the effectiveness of our approach, which is able to obtain sub-pixel accuracy and restore occlusions to a great extent. Specifically, our accurate framework attains near-peak accuracy both in non-occluded and occluded region and our fast framework achieves competitive performance against the fast algorithms on Middlebury benchmark.","","","10.1109/ACCESS.2017.2754318","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8047938","Stereo vision;matching cost;disparity refinement;convolutional neural network;occlusion restoration","Computer architecture;Network architecture;Computational modeling;Training;Neural networks;Three-dimensional displays","image matching;image representation;learning (artificial intelligence);stereo image processing","deep learning framework;typical stereo methods;matching cost computation;disparity refinement;patch-based network architectures;cross-scale feature representations;traditional handcrafted refinement algorithms;initial optimal disparity maps;sub-optimal disparity maps;diverse base learners;occlusions;accurate framework attains near-peak accuracy;stereo matching;weak texture;illumination difference;multisize and multilayer pooling unit","","11","43","OAPA","","","","IEEE","IEEE Journals"
"Railway Track Circuit Fault Diagnosis Using Recurrent Neural Networks","T. de Bruin; K. Verbert; R. Babuška","Delft Center for Systems and Control, Delft University of Technology, Delft, CD, The Netherlands; Delft Center for Systems and Control, Delft University of Technology, Delft, CD, The Netherlands; Delft Center for Systems and Control, Delft University of Technology, Delft, CD, The Netherlands","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","3","523","533","Timely detection and identification of faults in railway track circuits are crucial for the safety and availability of railway networks. In this paper, the use of the long-short-term memory (LSTM) recurrent neural network is proposed to accomplish these tasks based on the commonly available measurement signals. By considering the signals from multiple track circuits in a geographic area, faults are diagnosed from their spatial and temporal dependences. A generative model is used to show that the LSTM network can learn these dependences directly from the data. The network correctly classifies 99.7% of the test input sequences, with no false positive fault detections. In addition, the t-Distributed Stochastic Neighbor Embedding (t-SNE) method is used to examine the resulting network, further showing that it has learned the relevant dependences in the data. Finally, we compare our LSTM network with a convolutional network trained on the same task. From this comparison, we conclude that the LSTM network architecture is better suited for the railway track circuit fault detection and identification tasks than the convolutional network.","","","10.1109/TNNLS.2016.2551940","STW/ProRail project “Advanced monitoring of intelligent rail infrastructure (ADMIRE)”; Dutch Technology Foundation STW; Deep Learning for Robust Robot Control (DL-Force); Netherlands Organisation for Scientific Research (NWO); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457692","Fault diagnosis;long-short-term memory (LSTM);recurrent neural network (RNN);track circuit","Circuit faults;Rail transportation;Fault diagnosis;Degradation;Insulation life;Neural networks;Integrated circuit modeling","fault diagnosis;learning (artificial intelligence);neural net architecture;railway safety;recurrent neural nets;stochastic processes","railway track circuit fault diagnosis;railway network safety;railway network availability;long-short-term memory recurrent neural network;spatial dependences;temporal dependences;LSTM recurrent neural network;test input sequences;t-distributed stochastic neighbor embedding method;t-SNE method;railway track circuit fault detection;railway track circuit fault identification","","53","25","","","","","IEEE","IEEE Journals"
"Progressive Shape-Distribution-Encoder for Learning 3D Shape Representation","J. Xie; F. Zhu; G. Dai; L. Shao; Y. Fang","Department of Electrical and Computer Engineering, NYU Multimedia and Visual Computing Laboratory, New York University Abu Dhabi, UAE; Department of Electrical and Computer Engineering, NYU Multimedia and Visual Computing Laboratory, New York University Abu Dhabi, UAE; Department of Electrical and Computer Engineering, NYU Multimedia and Visual Computing Laboratory, New York University Abu Dhabi, UAE; School of Computing Sciences, University of East Anglia, Norwich, U.K.; Department of Electrical and Computer Engineering, NYU Multimedia and Visual Computing Laboratory, New York University Abu Dhabi, UAE","IEEE Transactions on Image Processing","","2017","26","3","1231","1242","Since there are complex geometric variations with 3D shapes, extracting efficient 3D shape features is one of the most challenging tasks in shape matching and retrieval. In this paper, we propose a deep shape descriptor by learning shape distributions at different diffusion time via a progressive shape-distribution-encoder (PSDE). First, we develop a shape distribution representation with the kernel density estimator to characterize the intrinsic geometry structures of 3D shapes. Then, we propose to learn a deep shape feature through an unsupervised PSDE. Specially, the unsupervised PSDE aims at modeling the complex non-linear transform of the estimated shape distributions between consecutive diffusion time. In order to characterize the intrinsic structures of 3D shapes more efficiently, we stack multiple PSDEs to form a network structure. Finally, we concatenate all neurons in the middle hidden layers of the unsupervised PSDE network to form an unsupervised shape descriptor for retrieval. Furthermore, by imposing an additional constraint on the outputs of all hidden layers, we propose a supervised PSDE to form a supervised shape descriptor. For each hidden layer, the similarity between a pair of outputs from the same class is as large as possible and the similarity between a pair of outputs from different classes is as small as possible. The proposed method is evaluated on three benchmark 3D shape data sets with large geometric variations, i.e., McGill, SHREC'10 ShapeGoogle, and SHREC'14 Human data sets, and the experimental results demonstrate the superiority of the proposed method to the existing approaches.","","","10.1109/TIP.2017.2651408","New York University Abu Dhabi; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7812756","3D shape retrieval;shape descriptor;denoising auto-encoder;heat kernel signature;heat diffusion","Shape;Kernel;Heating;Three-dimensional displays;Feature extraction;Noise reduction;Two dimensional displays","computational geometry;feature extraction;image representation;learning (artificial intelligence);shape recognition;transforms","progressive shape-distribution-encoder;3D shape representation;complex geometric variations;3D shape feature extraction;shape matching;shape retrieval;deep shape descriptor;shape distribution learning;kernel density estimator;complex nonlinear transform modeling;diffusion time;network structure;unsupervised PSDE network;supervised PSDE;supervised shape descriptor;McGill data sets;SHREC'10 ShapeGoogle data sets;SHREC'14 Human data sets","","7","35","","","","","IEEE","IEEE Journals"
"Low-Dose CT With a Residual Encoder-Decoder Convolutional Neural Network","H. Chen; Y. Zhang; M. K. Kalra; F. Lin; Y. Chen; P. Liao; J. Zhou; G. Wang","College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; Department of Radiology, Massachusetts General Hospital, Boston, MA, USA; College of Computer Science, Sichuan University, Chengdu, China; Laboratory of Image Science and Technology, Southeast University, Nanjing, China; Department of Scientific Research and Education, The Sixth People’s Hospital of Chengdu, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA","IEEE Transactions on Medical Imaging","","2017","36","12","2524","2535","Given the potential risk of X-ray radiation to the patient, low-dose CT has attracted a considerable interest in the medical imaging field. Currently, the main stream low-dose CT methods include vendor-specific sinogram domain filtration and iterative reconstruction algorithms, but they need to access raw data, whose formats are not transparent to most users. Due to the difficulty of modeling the statistical characteristics in the image domain, the existing methods for directly processing reconstructed images cannot eliminate image noise very well while keeping structural details. Inspired by the idea of deep learning, here we combine the autoencoder, deconvolution network, and shortcut connections into the residual encoder-decoder convolutional neural network (RED-CNN) for low-dose CT imaging. After patch-based training, the proposed RED-CNN achieves a competitive performance relative to the-state-of-art methods in both simulated and clinical cases. Especially, our method has been favorably evaluated in terms of noise suppression, structural preservation, and lesion detection.","","","10.1109/TMI.2017.2715284","National Natural Science Foundation of China; National Institute of Biomedical Imaging and Bioengineering/National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7947200","Low-dose CT;deep learning;auto-encoder;convolutional;deconvolutional;residual neural network","Image reconstruction;Computed tomography;Convolution;Feature extraction;Decoding;X-ray imaging","computerised tomography;deconvolution;image denoising;image reconstruction;iterative methods;learning (artificial intelligence);medical image processing;neural nets;statistical analysis","residual encoder-decoder convolutional neural network;X-ray radiation;medical imaging field;main stream low-dose CT methods;vendor-specific sinogram domain filtration;iterative reconstruction algorithms;raw data;statistical characteristics;image domain;image noise;deep learning;autoencoder;deconvolution network;residual encoder-decoder convolutional neural network;RED-CNN;low-dose CT imaging;patch-based training;noise suppression;structural preservation;lesion detection","Abdomen;Algorithms;Computer Simulation;Humans;Image Processing, Computer-Assisted;Liver Neoplasms;Neural Networks (Computer);Tomography, X-Ray Computed","53","51","","","","","IEEE","IEEE Journals"
"Multitemporal Very High Resolution From Space: Outcome of the 2016 IEEE GRSS Data Fusion Contest","L. Mou; X. Zhu; M. Vakalopoulou; K. Karantzalos; N. Paragios; B. Le Saux; G. Moser; D. Tuia","Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), 82234 Wessling, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), 82234 Wessling, Germany; Remote Sensing Laboratory, National Technical University of Athens, Zografou, Greece; Remote Sensing Laboratory, National Technical University of Athens, Zografou, Greece; Laboratoire de Mathématiques Appliquées aux Systèmes, Centrale Supélec Paris, 92290 Châtenay-Malabry, France; ONERA—The French Aerospace Lab, 91120 Palaiseau, France; Department of Electrical, Electronic, Telecommunications Engineering and Naval Architecture, University of Genoa, 16126 Genova, Italy; Department of Geography, University of Zurich, 8057 Zurich, Switzerland","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","8","3435","3447","In this paper, the scientific outcomes of the 2016 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society are discussed. The 2016 Contest was an open topic competition based on a multitemporal and multimodal dataset, which included a temporal pair of very high resolution panchromatic and multispectral Deimos-2 images and a video captured by the Iris camera on-board the International Space Station. The problems addressed and the techniques proposed by the participants to the Contest spanned across a rather broad range of topics, and mixed ideas and methodologies from the remote sensing, video processing, and computer vision. In particular, the winning team developed a deep learning method to jointly address spatial scene labeling and temporal activity modeling using the available image and video data. The second place team proposed a random field model to simultaneously perform coregistration of multitemporal data, semantic segmentation, and change detection. The methodological key ideas of both these approaches and the main results of the corresponding experimental validation are discussed in this paper.","","","10.1109/JSTARS.2017.2696823","University of Athens; European Research Council (ERC); European Union Horizon 2020 research and innovation programme; Helmholtz Association; SiPEO; Swiss National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7948767","Change detection;convolutional neural networks (CNN);deep learning;image analysis and data fusion;multiresolution;multisource;multimodal;random fields;tracking;video from space","Remote sensing;Iris;Data integration;Cameras;Sensors;Earth;Image resolution","geophysical image processing;image registration;image segmentation;learning (artificial intelligence);remote sensing;sensor fusion;video signal processing","multitemporal very high resolution;AD 2016;IEEE GRSS data fusion contest;Image Analysis and Data Fusion Technical Committee;IEEE Geoscience and Remote Sensing Society;very high resolution panchromatic images;multispectral Deimos-2 images;Iris camera;International Space Station;remote sensing;video processing;computer vision;deep learning method;scene labeling;image coregistration;semantic segmentation;change detection","","15","60","","","","","IEEE","IEEE Journals"
"RSCM: Region Selection and Concurrency Model for Multi-Class Weather Recognition","D. Lin; C. Lu; H. Huang; J. Jia","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Minhang, Qu, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Image Processing","","2017","26","9","4154","4167","Toward weather condition recognition, we emphasize the importance of regional cues in this paper and address a few important problems regarding appropriate representation, its differentiation among regions, and weather-condition feature construction. Our major contribution is, first, to construct a multi-class benchmark data set containing 65 000 images from six common categories for sunny, cloudy, rainy, snowy, haze, and thunder weather. This data set also benefits weather classification and attribute recognition. Second, we propose a deep learning framework named region selection and concurrency model (RSCM) to help discover regional properties and concurrency. We evaluate RSCM on our multi-class benchmark data and another public data set for weather recognition.","","","10.1109/TIP.2017.2695883","Research Grants Council of the Hong Kong SAR; National Natural Science Foundation of China; NSFC; 973 Program; Guangdong Science and Technology Program; Shenzhen Innovation Program; Natural Science Foundation of SZU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7904720","Deep learning;multi-class weather recognition;image classification;attribute recognition","Meteorology;Concurrent computing;Clouds;Image recognition;Computer science;Buildings;Feature extraction","geophysics computing;learning (artificial intelligence);pattern classification;weather forecasting","multiclass benchmark data;deep learning framework;attribute recognition;weather classification;multiclass benchmark data set;weather-condition feature construction;regional cues;weather condition recognition;multiclass weather recognition;region selection and concurrency model;RSCM","","9","45","","","","","IEEE","IEEE Journals"
"ROI-Based Video Transmission in Heterogeneous Wireless Networks With Multi-Homed Terminals","Z. Zhang; T. Jing; J. Han; Y. Xu; X. Li; M. Gao","School of Electronics and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronics and Information Engineering, Beijing Jiaotong University, Beijing, China; Google Inc., Mountain View, CA, USA; Google Inc., Mountain View, CA, USA; School of Electronics and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronics and Information Engineering, Beijing Jiaotong University, Beijing, China","IEEE Access","","2017","5","","26328","26339","We consider the problem of delivering region of interest (ROI)-coded mobile video streams using limited radio resources. Under the conditions of limited bandwidth and time-varying channel status, the goal is to optimize the transmission latency, while ensuring the quality of the ROI parts. Multi-homing support enables the terminals to establish multiple connections for transmission performance improvement. In this paper, we propose a novel framework for ROI-based video transmission in heterogeneous wireless networks with multi-homed terminals. The framework contains the modules of ROI detector and frame splitter, where macroblocks are categorized based on ROI detection and encapsulated into transforming units. It also includes a channel monitor that keeps track of the status of each communication path and sends feedback signals to the streaming controller for packet-scheduling control; a deep learning method is proposed for channel status prediction. To address the delivery problem, we propose a scheduling approach based on the formulated network model and the rate-distortion model. The scheduling method makes a tradeoff between the transmission delay and the distortion. It also guarantees that packets with ROI content are delivered on paths with sufficient bandwidths and low loss rates. Through comparisons with other scheduling methods, we find that the proposed scheme outperforms the other scheduling methods in terms of improving the quality (peak signal-to-noise ratio), balancing the end-to-end delay, and maintaining the playback fluency.","","","10.1109/ACCESS.2017.2748138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030984","Heterogeneous wireless networks;multi-homed communication;region of interest (ROI)-based video coding;deep learning;video transmission","Streaming media;Bandwidth;Wireless networks;Delays;Encoding;Forward error correction;Decoding","learning (artificial intelligence);mobile radio;scheduling;telecommunication computing;time-varying channels;video coding;video communication;video streaming;wireless channels","streaming controller;packet-scheduling control;deep learning method;channel status prediction;delivery problem;formulated network model;rate-distortion model;transmission delay;heterogeneous wireless networks;multihomed terminals;mobile video streams;radio resources;time-varying channel status;frame splitter;region of interest-coded mobile video streams;ROI-based video transmission","","","49","","","","","IEEE","IEEE Journals"
"Predicting MicroRNA-Disease Associations Using Network Topological Similarity Based on DeepWalk","G. Li; J. Luo; Q. Xiao; C. Liang; P. Ding; B. Cao","School of Information Engineering, East China Jiaotong University, Nanchang, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Information Science and Engineering, Shandong Normal University, Jinan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Information and Electronic Engineering, Hunan City University, Yiyang, China","IEEE Access","","2017","5","","24032","24039","Recently, increasing experimental studies have shown that microRNAs (miRNAs) involved in multiple physiological processes are connected with several complex human diseases. Identifying human disease-related miRNAs will be useful in uncovering novel prognostic markers for cancer. Currently, several computational approaches have been developed for miRNA-disease association prediction based on the integration of additional biological information of diseases and miRNAs, such as disease semantic similarity and miRNA functional similarity. However, these methods do not work well when this information is unavailable. In this paper, we present a similarity-based miRNA-disease prediction method that enhances the existing association discovery methods through a topology-based similarity measure. DeepWalk, a deep learning method, is utilized in this paper to calculate similarities within a miRNA-disease association network. It shows superior predictive performance for 22 complex diseases, with area under the ROC curve scores ranging from 0.805 to 0.937 by using five-fold cross-validation. In addition, case studies on breast cancer, lung cancer, and prostatic cancer further justify the use of our method to discover latent miRNA-disease pairs.","","","10.1109/ACCESS.2017.2766758","National Natural Science Foundation of China; Key Project of the Education Department of Hunan Province; Hunan Provincial Innovation Foundation for Postgraduate; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8085107","Deep learning;disease-related microRNAs;microRNA-disease association;similarity measure","Training;Computational modeling;Biology;Databases;Predictive models;Cancer","bioinformatics;cancer;data mining;diseases;learning (artificial intelligence);lung;medical computing;molecular biophysics;patient diagnosis;patient treatment;RNA","network topological similarity;DeepWalk;multiple physiological processes;complex human diseases;disease semantic similarity;miRNA functional similarity;similarity measure;deep learning method;prognostic markers;human disease-related miRNA;association discovery methods;microRNA-disease association prediction","","3","43","","","","","IEEE","IEEE Journals"
"Gaze-Assisted Multi-Stream Deep Neural Network for Action Recognition","Y. Liu; Q. Wu; L. Tang; H. Shi","School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Access","","2017","5","","19432","19441","There are two important aspects in human action recognition. The first one is how to locate the area that better indicates what the subjects in the videos are doing. The second one is how we can utilize the appearance and motion information from the video data. In this paper, we propose a gaze-assisted deep neural network, which performs the action recognition task with the help of human visual attention. Based on the above-mentioned consideration, we first collect a large number of human gaze data by recording the eye movements of human subjects when they watch the video. Then, we employ a fully convolutional network to learn to predict the human gaze. To efficiently utilize the human gaze, inspired by the rank pooling concept, which can encode the video into one image, we design a novel video representation named by dynamic gaze. The proposed dynamic gaze captures both the appearance and motion information from the video, and our human gaze data can better locate the area of interest. Based on the dynamic gaze, we build our dynamic gaze stream. We combine the proposed dynamic gaze stream together with the two-stream architecture as our final multi-stream architecture. We have collected over 300-k human gaze maps for the J-HMDB data set in this paper, and experiments show that the proposed multi-stream architecture can achieve comparable results with the state of the art in the task of action recognition with both collected human gaze data and predicted human gaze data.","","","10.1109/ACCESS.2017.2753830","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8039490","Action recognition;human gaze;convolutional neural network","Videos;Dynamics;Computer architecture;Streaming media;Optical imaging;Trajectory","computer vision;image classification;image motion analysis;image recognition;image representation;image sequences;learning (artificial intelligence);neural nets;robot vision;video signal processing","action recognition task;human visual attention;human subjects;video representation;dynamic gaze captures;motion information;dynamic gaze stream;multistream architecture;multistream deep neural network;human action recognition;video data;gaze-assisted deep neural network;human gaze data collection;J-HMDB data set","","3","37","OAPA","","","","IEEE","IEEE Journals"
"Two-Stage Single-Channel Audio Source Separation Using Deep Neural Networks","E. M. Grais; G. Roma; A. J. R. Simpson; M. D. Plumbley","Centre for Vision, Speech, and Signal Processing, University of Surrey, Guildford, U.K.; Centre for Vision, Speech, and Signal Processing, Georgia Institute of Technology, University of Surrey, Guildford, Atlanta, GA, U.K.USA; Centre for Vision, Speech, and Signal Processing, Netaplex Labs LTD, Sevenoaks TN13 3NJ, U.K., University of Surrey, Guildford, U.K.; Centre for Vision, Speech, and Signal Processing, University of Surrey, Guildford, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","9","1773","1783","Most single channel audio source separation approaches produce separated sources accompanied by interference from other sources and other distortions. To tackle this problem, we propose to separate the sources in two stages. In the first stage, the sources are separated from the mixed signal. In the second stage, the interference between the separated sources and the distortions are reduced using deep neural networks (DNNs). We propose two methods that use DNNs to improve the quality of the separated sources in the second stage. In the first method, each separated source is improved individually using its own trained DNN, while in the second method all the separated sources are improved together using a single DNN. To further improve the quality of the separated sources, the DNNs in the second stage are trained discriminatively to further decrease the interference and the distortions of the separated sources. Our experimental results show that using two stages of separation improves the quality of the separated signals by decreasing the interference between the separated sources and distortions compared to separating the sources using a single stage of separation.","","","10.1109/TASLP.2017.2716443","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7951020","Audio enhancement;discriminative learning;deep neural networks;single channel audio source separation","Matrix decomposition;Speech;Source separation;Interference;Distortion;Training;Training data","audio signal processing;distortion;neural nets","two-stage single-channel audio source separation;deep neural networks;interference;distortions;DNN;single channel audio source separation","","8","39","","","","","IEEE","IEEE Journals"
"VLSI Implementation of Deep Neural Network Using Integral Stochastic Computing","A. Ardakani; F. Leduc-Primeau; N. Onizawa; T. Hanyu; W. J. Gross","Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; Research Institute of Electrical Communication, Tohoku University, Sendai, Japan; Research Institute of Electrical Communication, Tohoku University, Sendai, Japan; Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2017","25","10","2688","2699","The hardware implementation of deep neural networks (DNNs) has recently received tremendous attention: many applications in fact require high-speed operations that suit a hardware implementation. However, numerous elements and complex interconnections are usually required, leading to a large area occupation and copious power consumption. Stochastic computing (SC) has shown promising results for low-power area-efficient hardware implementations, even though existing stochastic algorithms require long streams that cause long latencies. In this paper, we propose an integer form of stochastic computation and introduce some elementary circuits. We then propose an efficient implementation of a DNN based on integral SC. The proposed architecture has been implemented on a Virtex7 field-programmable gate array, resulting in 45% and 62% average reductions in area and latency compared with the best reported architecture in the literature. We also synthesize the circuits in a 65-nm CMOS technology, and we show that the proposed integral stochastic architecture results in up to 21% reduction in energy consumption compared with the binary radix implementation at the same misclassification rate. Due to fault-tolerant nature of stochastic architectures, we also consider a quasi-synchronous implementation that yields 33% reduction in energy consumption with respect to the binary radix implementation without any compromise on performance.","","","10.1109/TVLSI.2017.2654298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839313","Deep neural network (DNN);hardware implementation;integral stochastic computation;machine learning;pattern recognition;VLSI","Adders;Logic gates;Hardware;Very large scale integration;Neural networks;Computer architecture;Engines","CMOS integrated circuits;fault tolerance;field programmable gate arrays;neural nets;stochastic programming;VLSI","deep neural networks;DNN;stochastic computing;low-power area-efficient hardware implementations;stochastic computation;integral SC;Virtex7 field-programmable gate array;CMOS technology;integral stochastic architecture;binary radix implementation;quasi-synchronous implementation;VLSI;size 65 nm","","27","36","Traditional","","","","IEEE","IEEE Journals"
"Modern Computer Vision Techniques for X-Ray Testing in Baggage Inspection","D. Mery; E. Svec; M. Arias; V. Riffo; J. M. Saavedra; S. Banerjee","Department of Computer Science, Pontificia Universidad Católica de Chile, Santiago, Chile; Department of Computer Science, Pontificia Universidad Católica de Chile, Santiago, Chile; Department of Computer Science, Pontificia Universidad Católica de Chile, Santiago, Chile; DIICC-Universidad de Atacama, Casilla, Chile; ORAND S.A., Santiago, Chile; University of Notre Dame, Notre Dame, IN, USA","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2017","47","4","682","692","X-ray screening systems have been used to safeguard environments in which access control is of paramount importance. Security checkpoints have been placed at the entrances to many public places to detect prohibited items, such as handguns and explosives. Generally, human operators are in charge of these tasks as automated recognition in baggage inspection is still far from perfect. Research and development on X-ray testing is, however, exploring new approaches based on computer vision that can be used to aid human operators. This paper attempts to make a contribution to the field of object recognition in X-ray testing by evaluating different computer vision strategies that have been proposed in the last years. We tested ten approaches. They are based on bag of words, sparse representations, deep learning, and classic pattern recognition schemes among others. For each method, we: 1) present a brief explanation; 2) show experimental results on the same database; and 3) provide concluding remarks discussing pros and cons of each method. In order to make fair comparisons, we define a common experimental protocol based on training, validation, and testing data (selected from the public GDXray database). The effectiveness of each method was tested in the recognition of three different threat objects: 1) handguns; 2) shuriken (ninja stars); and 3) razor blades. In our experiments, the highest recognition rate was achieved by methods based on visual vocabularies and deep features with more than 95% of accuracy. We strongly believe that it is possible to design an automated aid for the human inspection task using these computer vision algorithms.","","","10.1109/TSMC.2016.2628381","Fondecyt from CONICYT, Chile; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7775025","Baggage screening;deep learning;implicit shape model (ISM);object categorization;object detection;object recognition;sparse representations;threat objects;X-ray testing","X-ray imaging;Computer vision;Testing;Inspection;Object recognition;Image recognition;Weapons","authorisation;automatic optical inspection;bags;computer vision;object recognition;X-ray imaging","X-ray screening systems;access control;security checkpoints;automated recognition;baggage inspection;X-ray testing;computer vision;object recognition;bag of words;sparse representations;deep learning;pattern recognition;GDXray database;visual vocabularies;deep features;human inspection task","","23","71","","","","","IEEE","IEEE Journals"
"Predicting Transportation Carbon Emission with Urban Big Data","X. Lu; K. Ota; M. Dong; C. Yu; H. Jin","Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Information and Electronic Engineering, Muroran Institute of Technology, Muroran, Hokkaido, Japan; Department of Information and Electronic Engineering, Muroran Institute of Technology, Muroran, Hokkaido, Japan; Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Sustainable Computing","","2017","2","4","333","344","Transportation carbon emission is a significant contributor to the increase of greenhouse gases, which directly threatens the change of climate and human health. Under the pressure of the environment, it is very important to master the information of transportation carbon emission in real time. In the traditional way, we get the information of the transportation carbon emission by calculating the combustion of fossil fuel in the transportation sector. However, it is very difficult to obtain the real-time and accurate fossil fuel combustion in the transportation field. In this paper, we predict the real-time and fine-grained transportation carbon emission information in the whole city, based on the spatio-temporal datasets we observed in the city, that is taxi GPS data, transportation carbon emission data, road networks, points of interests (POIs), and meteorological data. We propose a three-layer perceptron neural network (3-layerPNN) to learn the characteristics of collected data and infer the transportation carbon emission. We evaluate our method with extensive experiments based on five real data sources obtained in Zhuhai, China. The results show that our method has advantages over the well-known three machine learning methods (Gaussian Naive Bayes, Linear Regression, and Logistic Regression) and two deep learning methods (Stacked Denoising Autoencoder and Deep Belief Networks).","","","10.1109/TSUSC.2017.2728805","NSFC; Fundamental Research Funds; Central Universities; National 863 Hi-Tech Research; JSPS KAKENHI; KDDI Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984819","Transportation carbon emission;urban big data;multilayer perceptron neural network;real-time prediction","Carbon dioxide;Urban areas;Smart cities;Real-time systems;Neural networks;Fuels","air pollution;Bayes methods;belief networks;Big Data;combustion;environmental science computing;fossil fuels;learning (artificial intelligence);multilayer perceptrons;regression analysis;transportation","deep learning methods;stacked denoising autoencoder;deep belief networks;logistic regression;linear regression;Gaussian Naive Bayes;machine learning methods;China;Zhuhai;3-layer PNN;three-layer perceptron neural network;meteorological data;POI;points of interests;road networks;taxi GPS data;spatio-temporal datasets;fossil fuel combustion;human health;climate change;greenhouse gases;Urban Big Data;transportation carbon emission prediction;transportation carbon emission data;fine-grained transportation carbon emission information;transportation field;transportation sector","","1","29","Traditional","","","","IEEE","IEEE Journals"
"Mitosis Detection in Phase Contrast Microscopy Image Sequences of Stem Cell Populations: A Critical Review","A. Liu; Y. Lu; M. Chen; Y. Su","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Department of Electrical and Computer Engineering, State University of New York at Albany, Albany, NY; School of Electrical and Information Engineering, Tianjin University, Tianjin, China","IEEE Transactions on Big Data","","2017","3","4","443","457","Detecting mitosis from cell population is a fundamental problem in many biological researches and biomedical applications. In modern researches, advanced imaging technologies have been applied to generate large amount of microscopy images of cells. However, detecting all mitotic cells from these images with human eye is tedious and time-consuming. In recent years, several approaches have been proposed to help humans finish this job automatically with high efficiency and accuracy. In this review paper, we first described some commonly used datasets for mitosis detection, and then discussed different kinds of methods for mitosis detection, like tracking based methods, tracking free methods, hybrid methods, and the most recently proposed works based on deep learning architecture. We compared these methods on same datasets, and found that deep learning based approaches have achieved a great improvement in performance. At last, we discussed the future possible approaches on mitosis detection, to combine the success of previous works and the advantage of big data in modern researches. Considering expertise is highly required in biomedical area, we will further discuss the possibility to learn information from biomedical big data with less expert annotation.","","","10.1109/TBDATA.2017.2721438","National Natural Science Foundation of China; Tianjin Research Program of Application Foundation and Advanced Technology; China Scholarship Council; Elite Scholar Program of Tianjin University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962189","Mitosis detection;computer vision;biomedical image;big data;microscopy image;stem cell","Microscopy;Big Data;Image sequences;Stem cells;Computer vision;Statistics;Computer architecture","cellular biophysics;image sequences;learning (artificial intelligence);medical image processing;microscopy;object detection;object tracking","mitotic cell detection;biomedical applications;biological researches;advanced imaging technologies;human eye;tracking based method;tracking free methods;hybrid methods;deep learning architecture;biomedical Big Data;deep learning based approaches;stem cell populations;phase contrast microscopy image sequences;mitosis detection","","2","41","Traditional","","","","IEEE","IEEE Journals"
"Features for Masking-Based Monaural Speech Separation in Reverberant Conditions","M. Delfarah; D. Wang","Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","5","1085","1094","Monaural speech separation is a fundamental problem in speech and signal processing. This problem can be approached from a supervised learning perspective by predicting an ideal time-frequency mask from features of noisy speech. In reverberant conditions at low signal-to-noise ratios (SNRs), accurate mask prediction is challenging and can benefit from effective features. In this paper, we investigate an extensive set of acoustic-phonetic features extracted in adverse conditions. Deep neural networks are used as the learning machine, and separation performance is evaluated using standard objective speech intelligibility metrics. Separation performance is systematically evaluated in both nonspeech and speech interference, in a variety of SNRs, reverberation times, and direct-to-reverberant energy ratios. Considerable performance improvement is observed by using contextual information, likely due to temporal effects of room reverberation. In addition, we construct feature combination sets using a sequential floating forward selection algorithm, and combined features outperform individual ones. We also find that optimal feature sets in anechoic conditions are different from those in reverberant conditions.","","","10.1109/TASLP.2017.2687829","Air Force Office of Scientific Research; Ohio Supercomputer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7887742","Deep neural networks;feature combination;monaural speech separation;room reverberation;speech intelligibility","Speech;Noise measurement;Feature extraction;Speech processing;Reverberation;Training;Spectrogram","feature extraction;learning (artificial intelligence);neural nets;reverberation;speech processing","monaural speech separation;speech processing;signal processing;supervised learning;noisy speech;signal-to-noise ratios;SNR;acoustic-phonetic feature extraction;deep neural networks;learning machine;reverberation times;direct-to-reverberant energy ratios;contextual information;sequential floating forward selection algorithm","","21","47","","","","","IEEE","IEEE Journals"
"Online Object Tracking, Learning and Parsing with And-Or Graphs","T. Wu; Y. Lu; S. Zhu","University of California, Los Angeles, CA; Department of Statistics, University of California, Los Angeles, CA; Department of Statistics and Computer Science, University of California, Los Angeles, CA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","12","2465","2480","This paper presents a method, called AOGTracker, for simultaneously tracking, learning and parsing (TLP) of unknown objects in video sequences with a hierarchical and compositional And-Or graph (AOG) representation. The TLP method is formulated in the Bayesian framework with a spatial and a temporal dynamic programming (DP) algorithms inferring object bounding boxes on-the-fly. During online learning, the AOG is discriminatively learned using latent SVM [1] to account for appearance (e.g., lighting and partial occlusion) and structural (e.g., different poses and viewpoints) variations of a tracked object, as well as distractors (e.g., similar objects) in background. Three key issues in online inference and learning are addressed: (i) maintaining purity of positive and negative examples collected online, (ii) controling model complexity in latent structure learning, and (iii) identifying critical moments to re-learn the structure of AOG based on its intrackability. The intrackability measures uncertainty of an AOG based on its score maps in a frame. In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks [2] , [3] , and the VOT benchmarks [4] -VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking). In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network [5] , [6] . In the latter, our AOGTracker outperforms all other trackers in VOT2013 and is comparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015.","","","10.1109/TPAMI.2016.2644963","DARPA; ONR; US National Science Foundation; NCSU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797239","Visual tracking;and-or graph;latent SVM;dynamic programming;intrackability","Computational modeling;Hidden Markov models;Object tracking;Benchmark testing;Trajectory;Dynamic programming","Bayes methods;dynamic programming;image representation;image sequences;learning (artificial intelligence);object detection;object tracking;support vector machines","latent structure learning;SVM;parsing;And-Or graphs;AOGTracker;AOG representation;boxes on-the-fly;online inference;online learning;temporal dynamic programming;Bayesian framework;TLP method;video sequences;online object tracking","","1","86","Traditional","","","","IEEE","IEEE Journals"
"A Convolutional Neural Network for Fault Classification and Diagnosis in Semiconductor Manufacturing Processes","K. B. Lee; S. Cheon; C. O. Kim","Department of Industrial Engineering, Yonsei University, Seoul, South Korea; Department of Industrial Engineering, Yonsei University, Seoul, South Korea; Department of Industrial Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Semiconductor Manufacturing","","2017","30","2","135","142","Many studies on the prediction of manufacturing results using sensor signals have been conducted in the field of fault detection and classification (FDC) for semiconductor manufacturing processes. However, fault diagnosis used to find clues as to root causes remains a challenging area. In particular, process monitoring using neural networks has been employed to only a limited extent because it is a black box model, making the relationships between input data and output results difficult to interpret in actual manufacturing settings, despite its high classification performance. In this paper, we propose a convolutional neural network (CNN) model, named FDC-CNN, in which a receptive field tailored to multivariate sensor signals slides along the time axis, to extract fault features. This approach enables the association of the output of the first convolutional layer with the structural meaning of the raw data, making it possible to locate the variable and time information that represents process faults. In an experiment on a chemical vapor deposition process, the proposed method outperformed other deep learning models.","","","10.1109/TSM.2017.2676245","Global Ph.D. Fellowship Program through the National Research Foundation of Korea (NRF); Technology Innovation Program (Development of Big Data-Based Analysis and Control Platform for Semiconductor Manufacturing Plants) through the Ministry of Trade, Industry and Energy, South Korea; NRF through the Ministry of Science, ICT and Future Planning, South Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7867863","Fault detection and classification;fault diagnosis;convolutional neural network;deep learning;multivariate time-series data;semiconductor manufacturing","Feature extraction;Convolution;Fault diagnosis;Semiconductor device modeling;Machine learning;Neural networks;Manufacturing","chemical vapour deposition;convolution;fault diagnosis;feature extraction;neural nets;process monitoring;semiconductor industry;semiconductor technology","convolutional neural network;fault classification;fault diagnosis;FDC-CNN;semiconductor manufacturing processes;process monitoring;black box model;multivariate sensor signals;fault features;chemical vapor deposition process;deep learning models","","49","13","","","","","IEEE","IEEE Journals"
"Nuclear Architecture Analysis of Prostate Cancer via Convolutional Neural Networks","J. T. Kwak; S. M. Hewitt","Department of Computer Science and Engineering, Sejong University, Seoul, South Korea; Experimental Pathology Laboratory, Center for Cancer Research, National Cancer Institute, National Institutes of Health, Bethesda, MD, USA","IEEE Access","","2017","5","","18526","18533","In this paper, we present an approach of convolutional neural networks (CNNs) to identify prostate cancers. Prostate tissue specimen samples were obtained from the tissue microarrays and digitized. For each sample, epithelial nuclear seeds were identified and used to generate a nuclear seed map, i.e., only the location information of epithelial nuclei was utilized. From the nuclear seed maps, CNNs sought to learn the high-level feature representation of nuclear architecture and to detect cancers. Applying data augmentation technique, CNNs were trained on the training data set including 73 benign and 89 cancer samples and validated on the testing data set comprising 217 benign and 274 cancer samples. In detecting cancers, CNNs achieved an AUC of 0.974 (95% CI: 0.961-0.985). In comparison with the approaches of utilizing hand-crafted nuclear architecture features and the state of the art deep learning networks with standard machine learning methods, CNNs were significantly superior to them (p-value <; 5e-2). Moreover, stromal nuclei were incapable of improving the cancer detection performance. The experimental results suggest that our approach offers the ability to aid in improving prostate cancer pathology.","","","10.1109/ACCESS.2017.2747838","National Research Foundation of Korea through the Korea Government (MSIP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8023758","Computer-aided diagnosis;microscopy;artificial neural network;pattern recognition;cancer detection","Biological tissues;Pathology;Prostate cancer;Neurons;Kernel;Machine learning","biological organs;biological tissues;cancer;feature extraction;feedforward neural nets;learning (artificial intelligence);medical image processing;radioisotope imaging;tumours","prostate tissue specimen samples;tissue microarrays;epithelial nuclear seeds;nuclear seed map;epithelial nuclei;high-level feature representation;training data;testing data;hand-crafted nuclear architecture features;standard machine learning methods;cancer detection performance;prostate cancer pathology;nuclear architecture analysis;convolutional neural networks;CNN;deep learning networks;benign cancer samples;AUC;stromal nuclei","","1","56","OAPA","","","","IEEE","IEEE Journals"
"Learning to Predict High-Quality Edge Maps for Room Layout Estimation","W. Zhang; W. Zhang; K. Liu; J. Gu","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China","IEEE Transactions on Multimedia","","2017","19","5","935","943","The goal of room layout estimation is to predict the three-dimensional box that represents the room spatial structure from a monocular image. In this paper, a deconvolution network is trained first to predict the edge map of a room image. Compared to the previous fully convolutional networks, the proposed deconvolution network has a multilayer deconvolution process that can refine the edge map estimate layer by layer. The deconvolution network also has fully connected layers to aggregate the information of every region throughout the entire image. During the layout generation process, an adaptive sampling strategy is introduced based on the obtained high-quality edge maps. Experimental results prove that the learned edge maps are highly reliable and can produce accurate layouts of room images.","","","10.1109/TMM.2016.2642780","NSFC; Major Research Program of Shandong Province; The Fundamental Research Funds of Shandong University; Open Program of Jiangsu Key Laboratory of 3D Printing Equipment and Manufacturing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792744","Deep learning;room layout estimation;scene understanding","Layout;Image edge detection;Deconvolution;Training;Estimation;Three-dimensional displays;Feature extraction","deconvolution;edge detection;image representation;image sampling;learning (artificial intelligence);multilayer perceptrons","high-quality edge map prediction;room layout estimation;three-dimensional box prediction;room spatial structure representation;monocular image;deconvolution network training;multilayer deconvolution process;edge map estimate layer;layout generation process;adaptive sampling strategy","","12","35","","","","","IEEE","IEEE Journals"
"DeepFix: A Fully Convolutional Neural Network for Predicting Human Eye Fixations","S. S. S. Kruthiventi; K. Ayush; R. V. Babu","Department of Computational and Data Sciences, Video Analytics Lab, Indian Institute of Science, Bengaluru, India; Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computational and Data Sciences, Video Analytics Lab, Indian Institute of Science, Bengaluru, India","IEEE Transactions on Image Processing","","2017","26","9","4446","4456","Understanding and predicting the human visual attention mechanism is an active area of research in the fields of neuroscience and computer vision. In this paper, we propose DeepFix, a fully convolutional neural network, which models the bottom-up mechanism of visual attention via saliency prediction. Unlike classical works, which characterize the saliency map using various hand-crafted features, our model automatically learns features in a hierarchical fashion and predicts the saliency map in an end-to-end manner. DeepFix is designed to capture semantics at multiple scales while taking global context into account, by using network layers with very large receptive fields. Generally, fully convolutional nets are spatially invariant-this prevents them from modeling location-dependent patterns (e.g., centre-bias). Our network handles this by incorporating a novel location-biased convolutional layer. We evaluate our model on multiple challenging saliency data sets and show that it achieves the state-of-the-art results.","","","10.1109/TIP.2017.2710620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937829","Saliency prediction;eye fixations;convolutional neural network;deep learning","Visualization;Computational modeling;Feature extraction;Semantics;Predictive models;Biological neural networks;Convolution","computer vision;learning (artificial intelligence);neural nets","saliency data sets;location-biased convolutional layer;location-dependent pattern modelling;receptive fields;network layers;hand-crafted features;saliency map;saliency prediction;bottom-up mechanism;neuroscience;computer vision;human visual attention mechanism;human eye fixation prediction;fully convolutional neural network;DeepFix","Algorithms;Attention;Fixation, Ocular;Humans;Image Processing, Computer-Assisted;Machine Learning;Models, Statistical;Neural Networks (Computer)","31","73","","","","","IEEE","IEEE Journals"
"Exemplar-Based Image and Video Stylization Using Fully Convolutional Semantic Features","F. Zhu; Z. Yan; J. Bu; Y. Yu","Department of Computer Science, The University of Hong Kong, Hong Kong; Facebook AI Research Menlo Park, California; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Department of Computer Science, The University of Hong Kong, Hong Kong","IEEE Transactions on Image Processing","","2017","26","7","3542","3555","Color and tone stylization in images and videos strives to enhance unique themes with artistic color and tone adjustments. It has a broad range of applications from professional image postprocessing to photo sharing over social networks. Mainstream photo enhancement softwares, such as Adobe Lightroom and Instagram, provide users with predefined styles, which are often hand-crafted through a trial-and-error process. Such photo adjustment tools lack a semantic understanding of image contents and the resulting global color transform limits the range of artistic styles it can represent. On the other hand, stylistic enhancement needs to apply distinct adjustments to various semantic regions. Such an ability enables a broader range of visual styles. In this paper, we first propose a novel deep learning architecture for exemplar-based image stylization, which learns local enhancement styles from image pairs. Our deep learning architecture consists of fully convolutional networks for automatic semantics-aware feature extraction and fully connected neural layers for adjustment prediction. Image stylization can be efficiently accomplished with a single forward pass through our deep network. To extend our deep network from image stylization to video stylization, we exploit temporal superpixels to facilitate the transfer of artistic styles from image exemplars to videos. Experiments on a number of data sets for image stylization as well as a diverse set of video clips demonstrate the effectiveness of our deep learning architecture.","","","10.1109/TIP.2017.2703099","Hong Kong Research Grants Council through General Research Funds; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7924216","Image stylization;fully convolutional networks;color transform","Image color analysis;Feature extraction;Semantics;Transforms;Machine learning;Computer architecture;Neural networks","feature extraction;image colour analysis;image enhancement;learning (artificial intelligence)","exemplar-based image stylization;video stylization;convolutional semantic feature;tone stylization;color stylization;image enhancement;video enhancement;image postprocessing;mainstream photo enhancement software;Adobe Lightroom;Instagram;global color transform;deep learning architecture;convolutional network;automatic semantics-aware feature extraction;fully connected neural layer;video clip","","4","47","","","","","IEEE","IEEE Journals"
"DaDianNao: A Neural Network Supercomputer","T. Luo; S. Liu; L. Li; Y. Wang; S. Zhang; T. Chen; Z. Xu; O. Temam; Y. Chen","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Inria Scalay, Palaiseau, France; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Computers","","2017","66","1","73","88","Many companies are deploying services largely based on machine-learning algorithms for sophisticated processing of large amounts of data, either for consumers or industry. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on-chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines, and evaluate performance by integrating electrical and optical inter-chip interconnects separately. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 656.63× over a GPU, and reduce the energy by 184.05× on average for a 64-chip system. We implement the node down to the place and route at 28 nm, containing a combination of custom storage and computational units, with electrical inter-chip interconnects.","","","10.1109/TC.2016.2574353","NSF; 973 Program of China; Strategic Priority Research Program; CAS; International Collaboration Key Program; CAS; 10,000 talent program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7480791","Machine learning;neuron network;supercomputer;multi-chip;interconnect;CNN;DNN","Biological neural networks;Neurons;Kernel;Computer architecture;Hardware;Supercomputers;Graphics processing units","feedforward neural nets;graphics processing units;learning (artificial intelligence);multiprocessor interconnection networks;parallel machines","DaDianNao;neural network supercomputer;machine-learning algorithms;convolutional neural networks;deep neural networks;CNN;DNN;neural network accelerators;computational capacity-area ratio;general-purpose workloads;memory footprint;multichip system;on-chip storage;custom multichip machine-learning architecture;optical interchip interconnects;electrical interchip interconnects;GPU","","35","66","","","","","IEEE","IEEE Journals"
"Toward Fast and Accurate Vehicle Detection in Aerial Images Using Coupled Region-Based Convolutional Neural Networks","Z. Deng; H. Sun; S. Zhou; J. Zhao; H. Zou","College of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","8","3652","3664","Vehicle detection in aerial images, being an interesting but challenging problem, plays an important role for a wide range of applications. Traditional methods are based on sliding-window search and handcrafted or shallow-learning-based features with heavy computational costs and limited representation power. Recently, deep learning algorithms, especially region-based convolutional neural networks (R-CNNs), have achieved state-of-the-art detection performance in computer vision. However, several challenges limit the applications of R-CNNs in vehicle detection from aerial images: 1) vehicles in large-scale aerial images are relatively small in size, and R-CNNs have poor localization performance with small objects; 2) R-CNNs are particularly designed for detecting the bounding box of the targets without extracting attributes; 3) manual annotation is generally expensive and the available manual annotation of vehicles for training R-CNNs are not sufficient in number. To address these problems, this paper proposes a fast and accurate vehicle detection framework. On one hand, to accurately extract vehicle-like targets, we developed an accurate-vehicle-proposal-network (AVPN) based on hyper feature map which combines hierarchical feature maps that are more accurate for small object detection. On the other hand, we propose a coupled R-CNN method, which combines an AVPN and a vehicle attribute learning network to extract the vehicle's location and attributes simultaneously. For original large-scale aerial images with limited manual annotations, we use cropped image blocks for training with data augmentation to avoid overfitting. Comprehensive evaluations on the public Munich vehicle dataset and the collected vehicle dataset demonstrate the accuracy and effectiveness of the proposed method.","","","10.1109/JSTARS.2017.2694890","National Natural Science Foundation of China; NUDT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7921594","Attribute learning;convolutional neural networks;vehicle detection;vehicle proposal network","Feature extraction;Vehicle detection;Object detection;Training;Proposals;Automobiles","computer vision;convolution;learning (artificial intelligence);neural nets;object detection","vehicle detection;aerial images;coupled region-based convolutional neural networks;deep learning algorithms;computer vision;target bounding box detection;vehicle-like target extraction;accurate-vehicle-proposal-network;AVPN;hyperfeature map;hierarchical feature maps;small object detection;coupled R-CNN method;vehicle attribute learning network;cropped image blocks;data augmentation;public Munich vehicle dataset","","29","52","","","","","IEEE","IEEE Journals"
"Facial Age Estimation With Age Difference","Z. Hu; Y. Wen; J. Wang; M. Wang; R. Hong; S. Yan","School of Computer Science and Engineering, Nanyang Technological University, Singapore639798; School of Computer Science and Engineering, Nanyang Technological University, Singapore639798; Azure Storage, Microsoft, Seattle, WA, USA; Hefei University of Technology, Hefei, China; Hefei University of Technology, Hefei, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Image Processing","","2017","26","7","3087","3097","Age estimation based on the human face remains a significant problem in computer vision and pattern recognition. In order to estimate an accurate age or age group of a facial image, most of the existing algorithms require a huge face data set attached with age labels. This imposes a constraint on the utilization of the immensely unlabeled or weakly labeled training data, e.g., the huge amount of human photos in the social networks. These images may provide no age label, but it is easy to derive the age difference for an image pair of the same person. To improve the age estimation accuracy, we propose a novel learning scheme to take advantage of these weakly labeled data through the deep convolutional neural networks. For each image pair, Kullback-Leibler divergence is employed to embed the age difference information. The entropy loss and the cross entropy loss are adaptively applied on each image to make the distribution exhibit a single peak value. The combination of these losses is designed to drive the neural network to understand the age gradually from only the age difference information. We also contribute a data set, including more than 100 000 face images attached with their taken dates. Each image is both labeled with the timestamp and people identity. Experimental results on two aging face databases show the advantages of the proposed age difference learning system, and the state-of-the-art performance is gained.","","","10.1109/TIP.2016.2633868","Nanyang Technological University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762921","Age difference;age estimation;convolutional neural networks;K-L divergence distance","Estimation;Face;Aging;Feature extraction;Machine learning;Neural networks;Entropy","computer vision;entropy;face recognition;learning (artificial intelligence);neural nets","facial age estimation;pattern recognition;computer vision;facial image;face data set;age labels;social networks;learning scheme;deep convolutional neural networks;Kullback-Leibler divergence;age difference information;entropy loss;cross entropy loss;age difference learning system","Adolescent;Adult;Aged;Aging;Algorithms;Child;Child, Preschool;Databases, Factual;Face;Female;Humans;Image Processing, Computer-Assisted;Infant;Infant, Newborn;Male;Middle Aged;Neural Networks (Computer);Pattern Recognition, Automated;Young Adult","25","42","","","","","IEEE","IEEE Journals"
"Age Group and Gender Estimation in the Wild With Deep RoR Architecture","K. Zhang; C. Gao; L. Guo; M. Sun; X. Yuan; T. X. Han; Z. Zhao; B. Li","Department of Electronic and Communication Engineering, North China Electric Power University, Baoding, China; Department of Electronic and Communication Engineering, North China Electric Power University, Baoding, China; Department of Electronic and Communication Engineering, North China Electric Power University, Baoding, China; Department of Electrical and Computer Engineering, University of Missouri, Columbia, MO, USA; Department of Electrical and Computer Engineering, University of Missouri, Columbia, MO, USA; Department of Electrical and Computer Engineering, University of Missouri, Columbia, MO, USA; Department of Electronic and Communication Engineering, North China Electric Power University, Baoding, China; Department of Electronic and Communication Engineering, North China Electric Power University, Baoding, China","IEEE Access","","2017","5","","22492","22503","Automatically predicting age group and gender from face images acquired in unconstrained conditions is an important and challenging task in many real-world applications. Nevertheless, the conventional methods with manually-designed features on in-the-wild benchmarks are unsatisfactory because of incompetency to tackle large variations in unconstrained images. This difficulty is alleviated to some degree through convolutional neural networks (CNN) for its powerful feature representation. In this paper, we propose a new CNN-based method for age group and gender estimation leveraging residual networks of residual networks (RoR), which exhibits better optimization ability for age group and gender classification than other CNN architectures. Moreover, two modest mechanisms based on observation of the characteristics of age group are presented to further improve the performance of age estimation. In order to further improve the performance and alleviate over-fitting problem, RoR model is pre-trained on ImageNet first, and then it is fune-tuned on the IMDB-WIKI-101 data set for further learning the features of face images, finally, it is used to fine-tune on Adience data set. Our experiments illustrate the effectiveness of RoR method for age and gender estimation in the wild, where it achieves better performance than other CNN methods. Finally, the RoR-152+IMDB-WIKI-101 with two mechanisms achieves new state-of-the-art results on Adience benchmark.","","","10.1109/ACCESS.2017.2761849","National Natural Science Foundation of China; Hebei Province Natural Science Foundation; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8063887","Age and gender estimation;Adience;RoR;weighted loss;pre-training;ImageNet;IMDB-WIKI","Estimation;Benchmark testing;Data models;Face;Manuals;Optimization;Power systems","convolution;face recognition;feature extraction;gender issues;image classification;image representation;learning (artificial intelligence);neural nets;pose estimation;Web sites","deep RoR architecture;age group;face images;gender estimation;residual networks;age estimation;deep residual networks of residual network architecture;feature representation;convolutional neural networks;CNN;gender classification","","4","55","","","","","IEEE","IEEE Journals"
"Segmentation- and Annotation-Free License Plate Recognition With Deep Localization and Failure Identification","O. Bulan; V. Kozitsky; P. Ramesh; M. Shreve","PARC, Webster, NY, USA; PARC, Webster, NY, USA; PARC, Webster, NY, USA; PARC, Webster, NY, USA","IEEE Transactions on Intelligent Transportation Systems","","2017","18","9","2351","2363","Automated license plate recognition (ALPR) is essential in several roadway imaging applications. For ALPR systems deployed in the United States, variation between jurisdictions on character width, spacing, and the existence of noise sources (e.g., heavy shadows, non-uniform illumination, various optical geometries, poor contrast, and so on) present in LP images makes it challenging for the recognition accuracy and scalability of ALPR systems. Font and plate-layout variation across jurisdictions further adds to the difficulty of proper character segmentation and increases the level of manual annotation required for training classifiers for each state, which can result in excessive operational overhead and cost. In this paper, we propose a new ALPR workflow that includes novel methods for segmentation- and annotation-free ALPR, as well as improved plate localization and automation for failure identification. Our proposed workflow begins with localizing the LP region in the captured image using a two-stage approach that first extracts a set of candidate regions using a weak sparse network of winnows classifier and then filters them using a strong convolutional neural network (CNN) classifier in the second stage. Images that fail a primary confidence test for plate localization are further classified to identify localization failures, such as LP not present, LP too bright, LP too dark, or no vehicle found. In the localized plate region, we perform segmentation and optical character recognition (OCR) jointly by using a probabilistic inference method based on hidden Markov models (HMMs) where the most likely code sequence is determined by applying the Viterbi algorithm. In order to reduce manual annotation required for training classifiers for OCR, we propose the use of either artificially generated synthetic LP images or character samples acquired by trained ALPR systems already operating in other sites. The performance gap due to differences between training and target domain distributions is minimized using an unsupervised domain adaptation. We evaluated the performance of our proposed methods on LP images captured in several US jurisdictions under realistic conditions.","","","10.1109/TITS.2016.2639020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7819489","Plate localization;deep learning;convolutional neural networks;segmentation;annotation;domain adaptation;image quality assessment;character recognition","Licenses;Training;Image segmentation;Optical character recognition software;Feature extraction;Character recognition;Scalability","hidden Markov models;image classification;image segmentation;neural nets;object recognition;optical character recognition","segmentation-free license plate recognition;annotation-free license plate recognition;deep localization;failure identification;automated license plate recognition;roadway imaging applications;ALPR systems;character width;spacing;noise sources;nonuniform illumination;optical geometries;Font;plate-layout variation;character segmentation;classifiers;ALPR workflow;annotation-free ALPR;plate localization;winnows classifier;convolutional neural network;CNN classifier;optical character recognition;OCR;probabilistic inference method;hidden Markov models;HMM;Viterbi algorithm;synthetic LP images;target domain distributions;unsupervised domain adaptation","","25","43","","","","","IEEE","IEEE Journals"
"Rapid Exact Signal Scanning With Deep Convolutional Neural Networks","M. Thom; F. Gritschneder","driveU/Institute of Measurement, Control, and Microtechnology, Ulm University, Ulm, Germany; driveU/Institute of Measurement, Control, and Microtechnology, Ulm University, Ulm, Germany","IEEE Transactions on Signal Processing","","2017","65","5","1235","1250","A rigorous formulation of the dynamics of a signal processing scheme aimed at dense signal scanning without any loss in accuracy is introduced and analyzed. Related methods proposed in the recent past lack a satisfactory analysis of whether they actually fulfill any exactness constraints. This is improved through an exact characterization of the requirements for a sound sliding window approach. The tools developed in this paper are especially beneficial if Convolutional Neural Networks are employed, but can also be used as a more general framework to validate related approaches to signal scanning. The proposed theory helps to eliminate redundant computations and renders special case treatment unnecessary, resulting in a dramatic boost in efficiency particularly on massively parallel processors. This is demonstrated both theoretically in a computational complexity analysis and empirically on modern parallel processors.","","","10.1109/TSP.2016.2631454","Daimler AG, Germany; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752959","Deep learning techniques;dense signal scanning;sliding window approach;convolutional neural networks","Convolution;Neural networks;Program processors;Computational complexity;Indexes;Computer architecture","neural nets;signal processing","rapid exact signal scanning;deep convolutional neural networks;signal processing scheme;sound sliding window approach;computational complexity analysis;modern parallel processors","","","28","","","","","IEEE","IEEE Journals"
"Adaptation to New Microphones Using Artificial Neural Networks With Trainable Activation Functions","S. M. Siniscalchi; V. M. Salerno","Computer Engineering Faculty, University of Enna Kore, Enna, Italy; University of Enna Kore, Enna, Italy","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","8","1959","1965","Model adaptation is a key technique that enables a modern automatic speech recognition (ASR) system to adjust its parameters, using a small amount of enrolment data, to the nuances in the speech spectrum due to microphone mismatch in the training and test data. In this brief, we investigate four different adaptation schemes for connectionist (also known as hybrid) ASR systems that learn microphone-specific hidden unit contributions, given some adaptation material. This solution is made possible adopting one of the following schemes: 1) the use of Hermite activation functions; 2) the introduction of bias and slope parameters in the sigmoid activation functions; 3) the injection of an amplitude parameter specific for each sigmoid unit; or 4) the combination of 2) and 3). Such a simple yet effective solution allows the adapted model to be stored in a small-sized storage space, a highly desirable property of adaptation algorithms for deep neural networks that are suitable for large-scale online deployment. Experimental results indicate that the investigated approaches reduce word error rates on the standard Spoke 6 task of the Wall Street Journal corpus compared with unadapted ASR systems. Moreover, the proposed adaptation schemes all perform better than simple multicondition training and comparable favorably against conventional linear regression-based approaches while using up to 15 orders of magnitude fewer parameters. The proposed adaptation strategies are also effective when a single adaptation sentence is available.","","","10.1109/TNNLS.2016.2550532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7452664","Deep neural network (DNN);model adaptation;speech recognition","Microphones;Training;Adaptation models;Data models;Speech recognition;Hidden Markov models;Speech","microphones;neural nets;speech recognition","artificial neural networks;microphone adaptation;automatic speech recognition;ASR systems;speech spectrum;microphone-specific hidden unit contribution learning;Hermite activation functions;sigmoid activation functions;deep neural networks;word error rate reduction;Wall Street Journal corpus;standard Spoke 6 task;unadapted ASR systems","","2","26","","","","","IEEE","IEEE Journals"
"Answer Selection in Community Question Answering via Attentive Neural Networks","Y. Xiang; Q. Chen; X. Wang; Y. Qin","Intelligent Computing Research Center, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China; Intelligent Computing Research Center, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China; Intelligent Computing Research Center, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China; Intelligent Computing Research Center, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China","IEEE Signal Processing Letters","","2017","24","4","505","509","Answer selection in community question answering (cQA) is a challenging task in natural language processing. The difficulty lies in that it not only needs the consideration of semantic matching between question answer pairs but also requires a serious modeling of contextual factors. In this letter, we propose an attentive deep neural network architecture so as to learn the deterministic information for answer selection. The architecture can support various input formats through the organization of convolutional neural networks, attention-based long short-term memory, and conditional random fields. Experiments are carried out on the SemEval-2015 cQA dataset. We attain 58.35% on macroaveraged F1, which outperforms the Top-1 system in the shared task by 1.16% and improves the state-of-the-art deep-neural-network-based method by 2.21%.","","","10.1109/LSP.2017.2673123","National 863 Program of China; National Natural Science Foundation of China; Strategic Emerging Industry Development Special Funds of Shenzhen; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7866821","Attention mechanism;answer selection;community question answering (cQA);deep neural networks (DNNs)","Neural networks;Context modeling;Convolution;Computer architecture;Knowledge discovery;Machine learning;Labeling","feedforward neural nets;learning (artificial intelligence);natural language processing;query processing","SemEval-2015 cQA dataset;conditional random fields;attention-based long short-term memory;convolutional neural networks;contextual factors modeling;question answer pairs;semantic matching;natural language processing;attentive deep neural network architecture;community question answering;answer selection","","10","26","","","","","IEEE","IEEE Journals"
"Sparse Bayesian Learning-Based Time-Variant Deconvolution","S. Yuan; S. Wang; M. Ma; Y. Ji; L. Deng","State Key Laboratory of Petroleum Resources and Prospecting, China University of Petroleum, Beijing, China; State Key Laboratory of Petroleum Resources and Prospecting, China University of Petroleum, Beijing, China; State Key Laboratory of Petroleum Resources and Prospecting, China University of Petroleum, Beijing, China; State Key Laboratory of Petroleum Resources and Prospecting, China University of Petroleum, Beijing, China; State Key Laboratory of Petroleum Resources and Prospecting, China University of Petroleum, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","11","6182","6194","In seismic exploration, the wavelet-filtering effect and Q-filtering (amplitude attenuation and velocity dispersion) effect blur the reflection image of subsurface layers. Therefore, both wavelet- and Q-filtering effects should be reduced to retrieve a high-quality subsurface image, which is significant for fine reservoir interpretation. We derive a nonlinear time-variant convolution model to sparsely represent nonstationary seismograms in time domain involving these two effects and present a time-variant deconvolution (TVD) method based on sparse Bayesian learning (SBL) to solve the model to obtain a high-quality reflectivity image. The SBL-based TVD essentially obtains an optimum posterior mean of the reflectivity image, which is regarded as the inverted reflectivity result, by iteratively solving a Bayesian maximum posterior and a type-II maximum likelihood. Because a hierarchical Gaussian prior for reflectivity controlled by model-dependent hyper-parameters is adopted to approximately represent the fact that reflectivity is sparse, SBL-based TVD can retrieve a sparse reflectivity image through the principled sequential addition and deletion of Q-dependent time-variant wavelets. In general, strong reflectors are acquired relatively earlier, whereas weak reflectors and deep reflectors are imaged later. The method has the capacity to avoid false artifacts represented by sequential positive or negative reflectivity spikes with short two-way travel time, which typically occur within stationary deconvolution outcomes. Synthetic, laboratorial, and field data examples are used to demonstrate the effectiveness of the method and illustrate its advantages over SBL-based stationary deconvolution and TVD using an l<sub>2</sub>-norm or an l<sub>1</sub>-norm regularization. The results show that SBL-based TVD is a potentially effective, stable, and high-quality imaging tool.","","","10.1109/TGRS.2017.2722223","National Natural Science Foundation of China; National Key Basic Research Development Program; Major Scientific Research Program of Petrochina Science and Technology Management Department “Comprehensive Seismic Prediction Software Development and Applications of Natural Gas”; Science Foundation of China University of Petroleum, Beijing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7987796","Attenuation;Bayesian framework;deconvolution;inverse problems;sparse Bayesian learning (SBL);sparse representations;thin bed","Deconvolution;Attenuation;Bayes methods;Dispersion;Convolution;Time-frequency analysis;Earth","approximation theory;Bayes methods;deconvolution;filtering theory;Gaussian processes;geophysical signal processing;geophysical techniques;hydrocarbon reservoirs;learning (artificial intelligence);maximum likelihood estimation;seismology;signal representation;vibrational signal processing;wavelet transforms","sequential positive reflectivity spikes;negative reflectivity spikes;two-way travel time;stationary deconvolution outcomes;inverted reflectivity;l<sub>1</sub>-norm regularization;l<sub>2</sub>-norm regularization;strong reflectors;general reflectors;Q-dependent time-variant wavelets;principled sequential addition;sparse reflectivity image;SBL-based TVD;model-dependent hyper-parameters;type-II maximum likelihood;Bayesian maximum posterior;optimum posterior mean;high-quality reflectivity image;time domain;nonlinear time-variant convolution model;fine reservoir interpretation;high-quality subsurface image;Q-filtering effect;subsurface layers;reflection image;velocity dispersion;amplitude attenuation;wavelet-filtering effect;seismic exploration;sparse Bayesian learning;high-quality imaging tool","","18","61","Traditional","","","","IEEE","IEEE Journals"
"Superpixel-Based Difference Representation Learning for Change Detection in Multispectral Remote Sensing Images","M. Gong; T. Zhan; P. Zhang; Q. Miao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","5","2658","2673","With the rapid technological development of various satellite sensors, high-resolution remotely sensed imagery has been an important source of data for change detection in land cover transition. However, it is still a challenging problem to effectively exploit the available spectral information to highlight changes. In this paper, we present a novel change detection framework for high-resolution remote sensing images, which incorporates superpixel-based change feature extraction and hierarchical difference representation learning by neural networks. First, highly homogenous and compact image superpixels are generated using superpixel segmentation, which makes these image blocks adhere well to image boundaries. Second, the change features are extracted to represent the difference information using spectrum, texture, and spatial features between the corresponding superpixels. Third, motivated by the fact that deep neural network has the ability to learn from data sets that have few labeled data, we use it to learn the semantic difference between the changed and unchanged pixels. The labeled data can be selected from the bitemporal multispectral images via a preclassification map generated in advance. And then, a neural network is built to learn the difference and classify the uncertain samples into changed or unchanged ones. Finally, a robust and high-contrast change detection result can be obtained from the network. The experimental results on the real data sets demonstrate its effectiveness, feasibility, and superiority of the proposed technique.","","","10.1109/TGRS.2017.2650198","National Natural Science Foundation of China; National Program for Support of Top-notch Young Professionals of China; Specialized Research Fund for the Doctoral Program of Higher Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839934","Change detection;difference representation learning;multispectral images;neural network;superpixel segmentation","Neural networks;Feature extraction;Remote sensing;Image resolution;Image segmentation;Robustness;Image analysis","feature extraction;geophysical image processing;land cover;neural nets;remote sensing","preclassification map;bitemporal multispectral;semantic difference;neural networks;hierarchical difference representation learning;change feature extraction;land cover transition;high resolution remotely sensed imagery;satellite sensors;multispectral remote sensing images;change detection;superpixel based difference representation learning","","36","55","","","","","IEEE","IEEE Journals"
"Recognizing and Presenting the Storytelling Video Structure With Deep Multimodal Networks","L. Baraldi; C. Grana; R. Cucchiara","Department of Engineering “Enzo Ferrari”, University of Modena and Reggio Emilia, Modena, Italy; Department of Engineering “Enzo Ferrari”, University of Modena and Reggio Emilia, Modena, Italy; Department of Engineering “Enzo Ferrari”, University of Modena and Reggio Emilia, Modena, Italy","IEEE Transactions on Multimedia","","2017","19","5","955","968","In this paper, we propose a novel scene detection algorithm which employs semantic, visual, textual, and audio cues. We also show how the hierarchical decomposition of the storytelling video structure can improve retrieval results presentation with semantically and aesthetically effective thumbnails. Our method is built upon two advancements of the state of the art: first is semantic feature extraction which builds video-specific concept detectors; and second is multimodal feature embedding learning that maps the feature vector of a shot to a space in which the Euclidean distance has task specific semantic properties. The proposed method is able to decompose the video in annotated temporal segments which allow us for a query specific thumbnail extraction. Extensive experiments are performed on different data sets to demonstrate the effectiveness of our algorithm. An in-depth discussion on how to deal with the subjectivity of the task is conducted and a strategy to overcome the problem is suggested.","","","10.1109/TMM.2016.2644872","Città educante; National Technological Cluster on Smart Communities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797131","Deep networks;performance evaluation;scene detection;temporal video segmentation","Feature extraction;Semantics;Visualization;Detection algorithms;Speech;Computational modeling;Detectors","feature extraction;image retrieval;video signal processing","storytelling video structure;deep multimodal networks;novel scene detection algorithm;semantic feature extraction;multimodal feature embedding learning;Euclidean distance;query specific thumbnail extraction","","16","47","","","","","IEEE","IEEE Journals"
"Mixtures of Lightweight Deep Convolutional Neural Networks: Applied to Agricultural Robotics","C. McCool; T. Perez; B. Upcroft","Queensland University of Technology, Brisbane, QLD, Australia; Queensland University of Technology, Brisbane, QLD, Australia; Queensland University of Technology, Brisbane, QLD, Australia","IEEE Robotics and Automation Letters","","2017","2","3","1344","1351","We propose a novel approach for training deep convolutional neural networks (DCNNs) that allows us to tradeoff complexity and accuracy to learn lightweight models suitable for robotic platforms such as AgBot II (which performs automated weed management). Our approach consists of three stages, the first is to adapt a pre-trained model to the task at hand. This provides state-of-the-art performance but at the cost of high computational complexity resulting in a low frame rate of just 0.12 frames per second (fps). Second, we use the adapted model and employ model compression techniques to learn a lightweight DCNN that is less accurate but has two orders of magnitude fewer parameters. Third, K lightweight models are combined as a mixture model to further enhance the performance of the lightweight models. Applied to the challenging task of weed segmentation, we improve the accuracy from 85.9%, using a traditional approach, to 93.9% by adapting a complicated pre-trained DCNN with 25M parameters (Inception-v3). The downside to this adapted model, Adapted-IV3, is that it can only process 0.12 fps. To make this approach fast while still retaining accuracy, we learn lightweight DCNNs which when combined can achieve accuracy greater than 90% while using considerably fewer parameters capable of processing between 1.07 and 1.83 fps, up to an order of magnitude faster and up to an order of magnitude fewer parameters.","","","10.1109/LRA.2017.2667039","Department of Agriculture and Fisheries; Strategic Investment in Farm Robotics Program; Australian Research Council Centre of Excellence for Robotic Vision; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849167","Agricultural automation;computer vision for automation;recognition","Adaptation models;Feature extraction;Agriculture;Mobile robots;Wheels;Shape","agricultural machinery;computational complexity;computer vision;control engineering computing;convolution;image segmentation;industrial robots;learning (artificial intelligence)","lightweight deep convolutional neural networks;agricultural robotics;DCNN training;AgBot II;automated weed management;computational complexity;model compression techniques;weed segmentation;computer vision","","28","29","","","","","IEEE","IEEE Journals"
"FoodNet: Recognizing Foods Using Ensemble of Deep Networks","P. Pandey; A. Deepthi; B. Mandal; N. B. Puhan","School of Electrical Science, Indian Institute of Technology of Bhubaneswar, Bhubaneswar, India; School of Electrical Science, Indian Institute of Technology of Bhubaneswar, Bhubaneswar, India; Kingston University, London, U.K.; School of Electrical Science, Indian Institute of Technology of Bhubaneswar, Bhubaneswar, India","IEEE Signal Processing Letters","","2017","24","12","1758","1762","In this letter, we propose a protocol for an automatic food recognition system that identifies the contents of the meal from the images of the food. We developed a multilayered convolutional neural network (CNN) pipeline that takes advantages of the features from other deep networks and improves the efficiency. Numerous traditional handcrafted features and methods are explored, among which CNNs are chosen as the best performing features. Networks are trained and fine-tuned using preprocessed images and the filter outputs are fused to achieve higher accuracy. Experimental results on the largest real-world food recognition database ETH Food-101 and newly contributed Indian food image database demonstrate the effectiveness of the proposed methodology as compared to many other benchmark deep learned CNN frameworks.","","","10.1109/LSP.2017.2758862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8055608","Deep convolutional neural network;ensemble of networks;food recognition;Indian Food Database","Feature extraction;Databases;Convolutional codes;Image recognition;Image color analysis;Food products;Computer architecture","convolution;food products;image recognition;neural nets","FoodNet;deep networks;automatic food recognition system;multilayered convolutional neural network pipeline;Indian food image database;food recognition database ETH Food-101","","3","31","Traditional","","","","IEEE","IEEE Journals"
"Hyperspectral Image Super-Resolution by Spectral Difference Learning and Spatial Error Correction","J. Hu; Y. Li; W. Xie","State Key Laboratory of Integrated Service Network, Xidian University, Xi&#x2019;an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi&#x2019;an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi&#x2019;an, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","10","1825","1829","A hyperspectral image (HSI) super-resolution (SR) is a highly attractive topic in computer vision. However, most existed methods require an auxiliary high-resolution (HR) image with respect to the input low-resolution (LR) HSI. This limits the practicability of these HSI SR methods. Moreover, these methods often destroy the important spectral information. This letter presents a deep spectral difference convolutional neural network (SDCNN) with the combination of a spatial-error-correction (SEC) model for HSI SR. This method allows for full exploration of the spectral and spatial correlations, which achieves a good spatial information enhancement and spectral information preservation. In the proposed method, the key band is automatically selected and super-resolved with the boundary bands. Meanwhile, spectral difference mapping between the LR and HR HSIs can be learned by the SDCNN, and then be transformed according to the SEC model, which aims at correcting the spatial error while preserving the spectral information. The rest nonkey bands will be super-resolved under the guidance of the transformed spectral difference. Experimental results on synthesized and real-scenario HSIs suggest that the proposed method: (1) achieves comparable performance without requiring any auxiliary images of the same scene and (2) requires less computation time than the state-of-the-art methods.","","","10.1109/LGRS.2017.2737637","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019794","Convolutional neural network;hyperspectral image (HSI);spatial-error-correction (SEC);super-resolution (SR)","Spatial resolution;Correlation;Hyperspectral imaging;Training;Computer vision","feedforward neural nets;hyperspectral imaging;image enhancement;image resolution;learning (artificial intelligence);spectral analysis","hyperspectral image super-resolution;spectral difference learning;Spatial Error Correction;HSI SR methods;computer vision;deep spectral-difference convolutional neural network;SDCNN;spatial-error-correction model;SEC model;spectral correlations;spatial correlations;spatial information enhancement;spectral information preservation;boundary bands;spectral difference mapping;LR HSI;HR HSI;auxiliary images","","9","12","Traditional","","","","IEEE","IEEE Journals"
"Intelligent fault diagnosis approach with unsupervised feature learning by stacked denoising autoencoder","M. Xia; T. Li; L. Liu; L. Xu; C. W. de Silva","The University of British Columbia, Canada; The University of British Columbia, Canada; Istuary Innovation Group, Canada; Istuary Innovation Group, Canada; The University of British Columbia, Canada","IET Science, Measurement & Technology","","2017","11","6","687","695","Condition monitoring and fault diagnosis are important for maintaining the system performance and guaranteeing the operational safety. The traditional data-driven approaches mostly incorporate well-defined features and methodologies such as supervised artificial intelligence algorithms. Prior knowledge of possible features and a large quantity of labelled condition data are needed. Besides, many traditional approaches require rebuilding or a retraining of the original model to diagnosis new conditions. The present study proposes an intelligent fault diagnosis approach that uses a deep neural network (DNN) based on stacked denoising autoencoder. Representative features are learned by applying the denoising autoencoder to the unlabelled data in an unsupervised manner. A DNN is then constructed and fine-tuned with just a few items of labelled data. The trained DNN achieves high performance in fault classification. Furthermore, new conditions can be correctly classified by simply fine-tuning the trained DNN model using a small amount of labelled data under the new conditions. The effectiveness of the proposed approach is evaluated using a case study of fault diagnosis of a bearing unit. The results indicate that the proposed method can extract representative features from massive unlabelled data on the system condition and achieve high performance in fault diagnosis.","","","10.1049/iet-smt.2016.0423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8025441","","","encoding;fault diagnosis;feature extraction;neural nets;signal classification;signal denoising;signal representation;unsupervised learning","intelligent fault diagnosis approach;unsupervised feature learning;stacked denoising autoencoder;condition monitoring;operational safety;data-driven approach;supervised artificial intelligence algorithms;deep neural network;fault classification;trained DNN model;representative feature extraction","","15","36","","","","","IET","IET Journals"
"Personalized Multitask Learning for Predicting Tomorrow's Mood, Stress, and Health","S. A. Taylor; N. Jaques; E. Nosakhare; A. Sano; R. Picard","Media Lab, Massachusetts Institute of Technology, 2167 Cambridge, Massachusetts United States 02139-4307 (e-mail: sataylor@mit.edu); Media Lab, Massachusetts Institute of Technology, Cambridge, Massachusetts United States (e-mail: jaquesn@mit.edu); Electrical Engineering and Computer Science, Massachusetts Institute of Technology, 2167 Cambridge, Massachusetts United States (e-mail: ehinosa@mit.edu); Media Lab, MIT, Cambridge, Massachusetts United States 02142-1322 (e-mail: akanes@media.mit.edu); The Media Laboratory, Massachusetts Institute of Technology, Cambridge, Massachusetts United States (e-mail: picard@media.mit.edu)","IEEE Transactions on Affective Computing","","2017","PP","99","1","1","While accurately predicting mood and wellbeing could have a number of important clinical benefits, traditional machine learning (ML) methods frequently yield low performance in this domain. We posit that this is because a one-size-fits-all machine learning model is inherently ill-suited to predicting outcomes like mood and stress, which vary greatly due to individual differences. Therefore, we employ Multitask Learning (MTL) techniques to train personalized ML models which are customized to the needs of each individual, but still leverage data from across the population. Three formulations of MTL are compared: i) MTL deep neural networks, which share several hidden layers but have final layers unique to each task; ii) Multi-task Multi-Kernel learning, which feeds information across tasks through kernel weights on feature types; and iii) a Hierarchical Bayesian model in which tasks share a common Dirichlet Process prior. We offer the code for this work in open source. These techniques are investigated in the context of predicting future mood, stress, and health using data collected from surveys, wearable sensors, smartphone logs, and the weather. Empirical results demonstrate that using MTL to account for individual differences provides large performance improvements over traditional machine learning methods and provides personalized, actionable insights.","","","10.1109/TAFFC.2017.2784832","Canadas NSERC program; NEC Corporation; Samsung Electronics; Multi-scale modeling of sleep behaviors in social networks; MIT Media Lab Consortium; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8226850","Mood Prediction;Multitask learning;Deep Neural Networks;Multi-Kernel SVM;Hierarchical Bayesian Model","Mood;Stress;Data models;Predictive models;Meteorology;Bayes methods","","","","7","","","","","","IEEE","IEEE Early Access Articles"
"Dense Semantic Labeling of Subdecimeter Resolution Images With Convolutional Neural Networks","M. Volpi; D. Tuia","MultiModal Remote Sensing Group, University of Zurich, Zürich, Switzerland; MultiModal Remote Sensing Group, University of Zurich, Zürich, Switzerland","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","2","881","893","Semantic labeling (or pixel-level land-cover classification) in ultrahigh-resolution imagery (<;10 cm) requires statistical models able to learn high-level concepts from spatial data, with large appearance variations. Convolutional neural networks (CNNs) achieve this goal by learning discriminatively a hierarchy of representations of increasing abstraction. In this paper, we present a CNN-based system relying on a downsample-then-upsample architecture. Specifically, it first learns a rough spatial map of high-level representations by means of convolutions and then learns to upsample them back to the original resolution by deconvolutions. By doing so, the CNN learns to densely label every pixel at the original resolution of the image. This results in many advantages, including: 1) the state-of-the-art numerical accuracy; 2) the improved geometric accuracy of predictions; and 3) high efficiency at inference time. We test the proposed system on the Vaihingen and Potsdam subdecimeter resolution data sets, involving the semantic labeling of aerial images of 9- and 5-cm resolution, respectively. These data sets are composed by many large and fully annotated tiles, allowing an unbiased evaluation of models making use of spatial information. We do so by comparing two standard CNN architectures with the proposed one: standard patch classification, prediction of local label patches by employing only convolutions, and full patch labeling by employing deconvolutions. All the systems compare favorably or outperform a state-of-the-art baseline relying on superpixels and powerful appearance descriptors. The proposed full patch labeling CNN outperforms these models by a large margin, also showing a very appealing inference time.","","","10.1109/TGRS.2016.2616585","Swiss National Science Foundation through the Multimodal Machine Learning for Remote Sensing Information Fusion; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7725499","Aerial images;classification;convolutional neural networks (CNNs);deconvolution networks;deep learning;semantic labeling;subdecimeter resolution","Semantics;Labeling;Feature extraction;Remote sensing;Image resolution;Data models;Machine learning","geophysical image processing;image classification;land cover;neural nets;remote sensing;semantic Web","patch labeling;standard patch classification;aerial images;Potsdam subdecimeter resolution dataset;Vaihingen subdecimeter resolution dataset;CNN-based system;statistical model;ultrahigh-resolution imagery;pixel-level land cover classification;convolutional neural network;subdecimeter resolution images;semantic labeling","","149","45","","","","","IEEE","IEEE Journals"
"Parallel Deep Neural Network Training for Big Data on Blue Gene/Q","I. Chung; T. N. Sainath; B. Ramabhadran; M. Picheny; J. Gunnels; V. Austel; U. Chauhari; B. Kingsbury","IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY","IEEE Transactions on Parallel and Distributed Systems","","2017","28","6","1703","1714","Deep Neural Networks (DNNs) have recently been shown to significantly outperform existing machine learning techniques in several pattern recognition tasks. DNNs are the state-of-the-art models used in image recognition, object detection, classification and tracking, and speech and language processing applications. The biggest drawback to DNNs has been the enormous cost in computation and time taken to train the parameters of the networks-often a tenfold increase relative to conventional technologies. Such training time costs can be mitigated by the application of parallel computing algorithms and architectures. However, these algorithms often run into difficulties because of the cost of inter-processor communication bottlenecks. In this paper, we describe how to enable Parallel Deep Neural Network Training on the IBM Blue Gene/Q (BG/Q) computer system. Specifically, we explore DNN training using the data-parallel Hessian-free 2nd order optimization algorithm. Such an algorithm is particularly well-suited to parallelization across a large set of loosely coupled processors. BG/Q, with its excellent inter-processor communication characteristics, is an ideal match for this type of algorithm. The paper discusses how issues regarding programming model and data-dependent imbalances are addressed. Results on large-scale speech tasks show that the performance on BG/Q scales linearly up to 4,096 processes with no loss in accuracy. This allows us to train neural networks using billions of training examples in a few hours.","","","10.1109/TPDS.2016.2626289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7738586","Big data;speech recognition;high performance computing","Training;Optimization;Neural networks;Speech;Speech recognition;Multicore processing","Big Data;learning (artificial intelligence);optimisation;parallel processing;speech recognition","parallel deep neural network training;Big Data;DNN;parallel computing algorithms;inter-processor communication bottlenecks;IBM Blue Gene-Q computer system;data-parallel Hessian-free 2nd order optimization algorithm;speech recognition","","9","37","","","","","IEEE","IEEE Journals"
"Can surgical simulation be used to train detection and classification of neural networks?","O. Zisimopoulos; E. Flouty; M. Stacey; S. Muscroft; P. Giataganas; J. Nehme; A. Chow; D. Stoyanov","Touch Surgery, Kinosis, Ltd, UK; Touch Surgery, Kinosis, Ltd, UK; Touch Surgery, Kinosis, Ltd, UK; Touch Surgery, Kinosis, Ltd, UK; Touch Surgery, Kinosis, Ltd, UK; Touch Surgery, Kinosis, Ltd, UK; Touch Surgery, Kinosis, Ltd, UK; Touch Surgery, Kinosis, Ltd, UK","Healthcare Technology Letters","","2017","4","5","216","222","Computer-assisted interventions (CAI) aim to increase the effectiveness, precision and repeatability of procedures to improve surgical outcomes. The presence and motion of surgical tools is a key information input for CAI surgical phase recognition algorithms. Vision-based tool detection and recognition approaches are an attractive solution and can be designed to take advantage of the powerful deep learning paradigm that is rapidly advancing image recognition and classification. The challenge for such algorithms is the availability and quality of labelled data used for training. In this Letter, surgical simulation is used to train tool detection and segmentation based on deep convolutional neural networks and generative adversarial networks. The authors experiment with two network architectures for image segmentation in tool classes commonly encountered during cataract surgery. A commercially-available simulator is used to create a simulated cataract dataset for training models prior to performing transfer learning on real surgical data. To the best of authors’ knowledge, this is the first attempt to train deep learning models for surgical instrument detection on simulated data while demonstrating promising results to generalise on real data. Results indicate that simulated data does have some potential for training advanced classification methods for CAI systems.","","","10.1049/htl.2017.0064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107157","","","surgery;neural nets;learning (artificial intelligence);image recognition;image classification;image segmentation;biomedical optical imaging;video signal processing;medical image processing","surgical simulation;computer-assisted interventions;CAI surgical phase recognition algorithms;Vision20 based tool detection;deep learning;image recognition;image classification;tool detection;tool segmentation;deep convolutional neural networks;generative adversarial networks;image segmentation;cataract surgery","","","24","","","","","IET","IET Journals"
"Stable improved softmax using constant normalisation","S. Lim; D. Lee","Kyung Hee University, Republic of Korea; Kyung Hee University, Republic of Korea","Electronics Letters","","2017","53","23","1504","1506","In deep learning architectures, rectified linear unit based functions are widely used as activation functions of hidden layers, and the softmax is used for the output layers. Two critical problems of the softmax are introduced, and an improved softmax method to resolve the problems is proposed. The proposed method minimises instability of the softmax while reducing its losses. Moreover, this method is straightforward so its computation complexity is low, but it is substantially reasonable and operates robustly. Therefore, the proposed method can replace the softmax functions.","","","10.1049/el.2017.3394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8103428","","","computational complexity;learning (artificial intelligence);transfer functions","computation complexity;hidden layers;activation functions;deep learning architectures;rectified linear unit based functions;improved softmax method;constant normalisation","","2","5","","","","","IET","IET Journals"
"Face Search at Scale","D. Wang; C. Otto; A. K. Jain","Department of Computer Science & Engineering, Michigan State University, East Lansing, Michigan; Department of Computer Science & Engineering, Michigan State University, East Lansing, Michigan; Department of Computer Science & Engineering, Michigan State University, East Lansing, Michigan","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","6","1122","1136","Given the prevalence of social media websites, one challenge facing computer vision researchers is to devise methods to search for persons of interest among the billions of shared photos on these websites. Despite significant progress in face recognition, searching a large collection of unconstrained face images remains a difficult problem. To address this challenge, we propose a face search system which combines a fast search procedure, coupled with a state-of-the-art commercial off the shelf (COTS) matcher, in a cascaded framework. Given a probe face, we first filter the large gallery of photos to find the top-k most similar faces using features learned by a convolutional neural network. The k retrieved candidates are re-ranked by combining similarities based on deep features and those output by the COTS matcher. We evaluate the proposed face search system on a gallery containing 80 million web-downloaded face images. Experimental results demonstrate that while the deep features perform worse than the COTS matcher on a mugshot dataset (93.7 percent versus 98.6 percent TAR@FAR of 0.01 percent), fusing the deep features with the COTS matcher improves the overall performance (99.5 percent TAR@FAR of 0.01 percent). This shows that the learned deep features provide complementary information over representations used in state-of-the-art face matchers. On the unconstrained face image benchmarks, the performance of the learned deep features is competitive with reported accuracies. LFW database: 98.20 percent accuracy under the standard protocol and 88.03 percent TAR@FAR of 0.1 percent under the BLUFR protocol; IJB-A benchmark: 51.0 percent TAR@FAR of 0.1 percent (verification), rank 1 retrieval of 82.2 percent (closed-set search), 61.5 percent FNIR@FAR of 1 percent (open-set search). The proposed face search system offers an excellent trade-off between accuracy and scalability on galleries with millions of images. Additionally, in a face search experiment involving photos of the Tsarnaev brothers, convicted of the Boston Marathon bombing, the proposed cascade face search system could find the younger brother's (Dzhokhar Tsarnaev) photo at rank 1 in 1 second on a 5 M gallery and at rank 8 in 7 seconds on an 80 M gallery.","","","10.1109/TPAMI.2016.2582166","National Institute of Justice (NIJ); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7494641","Face search;unconstrained face recognition;deep learning;large face collections;cascaded system;scalability","Face;Face recognition;Protocols;Search problems;Media;Probes;Benchmark testing","face recognition;learning (artificial intelligence);neural nets;social networking (online);Web sites","social media Websites;computer vision;face recognition;commercial-off-the-shelf matcher;COTS matcher;cascaded framework;convolutional neural network;face search system;Web-downloaded face images;mugshot dataset;unconstrained face image benchmarks;LFW database;BLUFR protocol;IJB-A benchmark;face search experiment;FNIR@FAR;TAR@FAR","","31","41","","","","","IEEE","IEEE Journals"
"Spoken Term Detection Automatically Adjusted for a Given Threshold","T. Fuchs; J. Keshet","Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel; Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel","IEEE Journal of Selected Topics in Signal Processing","","2017","11","8","1310","1317","Spoken term detection (STD) is the task of determining whether and where a given word or phrase appears in a given segment of speech. Algorithms for STD are often aimed at maximizing the gap between the scores of positive and negative examples. As such they are focused on ensuring that utterances where the term appears are ranked higher than utterances where the term does not appear. However, they do not determine a detection threshold between the two. In this paper, we propose a new approach for setting an absolute detection threshold for all terms by introducing a new calibrated loss function. The advantage of minimizing this loss function during training is that it aims at maximizing not only the relative ranking scores, but also adjusts the system to use a fixed threshold and thus maximizes the detection accuracy rates. We use the new loss function in the structured prediction setting and extend the discriminative keyword spotting algorithm for learning the spoken term detector with a single threshold for all terms. We further demonstrate the effectiveness of the new loss function by training a deep neural Siamese network in a weakly supervised setting for template-based STD, again with a single fixed threshold. Experiments with the TIMIT, Wall Street Journal (WSJ), and Switchboard corpora showed that our approach not only improved the accuracy rates when a fixed threshold was used but also obtained higher area under curve (AUC).","","","10.1109/JSTSP.2017.2764268","MAGNET program of the Israeli Innovation Authority; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070931","Spoken term detection;keyword spotting;AUC maximization;structured prediction;deep-neural networks","Speech;Training;Machine learning;Predictive models;Algorithm design and analysis","calibration;learning (artificial intelligence);natural language processing;neural nets;speech recognition","absolute detection threshold;calibrated loss function;relative ranking scores;detection accuracy rates;discriminative keyword spotting algorithm;spoken term detector;single threshold;template-based STD;single fixed threshold;spoken term detection;positive examples;negative examples;utterances;structured prediction setting;deep neural Siamese network;TIMIT;Wall Street Journal;WSJ;Switchboard corpora;area under curve;AUC","","","24","Traditional","","","","IEEE","IEEE Journals"
"Skin lesion segmentation using deep convolution networks guided by local unsupervised learning","B. Bozorgtabar; S. Sedai; P. K. Roy; R. Garnavi","NA; NA; NA; NA","IBM Journal of Research and Development","","2017","61","4/5","6:1","6:8","Automatic localization of skin lesions within dermoscopy images is a crucial step toward developing a decision support system for skin cancer detection. However, segmentation of the lesion image can be challenging, as these images possess various artifacts distorting the uniformity of the lesion area. Recently, deep convolution learning-based techniques have drawn great attention for pixel-wise image segmentation. These deep networks produce coarse segmentation, and convolutional filters and pooling layers result in segmentation of a skin lesion at a lower resolution than the original skin image. To overcome these drawbacks, we have proposed a superpixel-based fine-tuning strategy to effectively utilize the characteristics of the skin image pixels to accurately extract the border of the lesion. Our proposed approach not only learns a global map for skin lesions, but also acquires the local contextual information, such as lesion boundary. It can, therefore, accurately segment lesions within a given skin image, even in the presence of fuzzy boundaries and complex textures. To evaluate the performance of our proposed method, experiments have been conducted using the 2016 International Symposium on Biomedical Imaging dataset, and these experiments suggest the effectiveness of the proposed method.","","","10.1147/JRD.2017.2708283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030296","","Lesions;Image segmentation;Skin;Decision support;Biomedical imaging","","","","1","23","","","","","IBM","IBM Journals"
"A Convolutional Neural Network for Automatic Characterization of Plaque Composition in Carotid Ultrasound","K. Lekadir; A. Galimzianova; À. Betriu; M. del Mar Vila; L. Igual; D. L. Rubin; E. Fernández; P. Radeva; S. Napel","Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA; Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA; Unit for the Detection and Treatment of Atherothrombotic Diseases, Institute of Biomedical Research, Lleida, Spain; Cardiovascular Epidemiology and Genetics Research Group, Hospital del Mar Medical Research Institute, Barcelona, Spain; Department of Mathematics and Computer Science, Universitat de Barcelona 08007, Spain; Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA; Unit for the Detection and Treatment of Atherothrombotic Diseases, Institute of Biomedical Research, Lleida, Spain; Department of Mathematics and Computer Science, Universitat de Barcelona 08007, Spain; Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA","IEEE Journal of Biomedical and Health Informatics","","2017","21","1","48","55","Characterization of carotid plaque composition, more specifically the amount of lipid core, fibrous tissue, and calcified tissue, is an important task for the identification of plaques that are prone to rupture, and thus for early risk estimation of cardiovascular and cerebrovascular events. Due to its low costs and wide availability, carotid ultrasound has the potential to become the modality of choice for plaque characterization in clinical practice. However, its significant image noise, coupled with the small size of the plaques and their complex appearance, makes it difficult for automated techniques to discriminate between the different plaque constituents. In this paper, we propose to address this challenging problem by exploiting the unique capabilities of the emerging deep learning framework. More specifically, and unlike existing works which require a priori definition of specific imaging features or thresholding values, we propose to build a convolutional neural network (CNN) that will automatically extract from the images the information that is optimal for the identification of the different plaque constituents. We used approximately 90 000 patches extracted from a database of images and corresponding expert plaque characterizations to train and to validate the proposed CNN. The results of cross-validation experiments show a correlation of about 0.90 with the clinical assessment for the estimation of lipid core, fibrous cap, and calcified tissue areas, indicating the potential of deep learning for the challenging task of automatic characterization of plaque composition in carotid ultrasound.","","","10.1109/JBHI.2016.2631401","Marie-Curie Actions Program of the European Union; REA; NIH; NVIDIA; FIS; European Regions Development; ICREA; NIH; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752798","Atherosclerosis;carotid artery;convolutional neural networks (CNNs);plaque composition;ultrasound","Ultrasonic imaging;Machine learning;Imaging;Feature extraction;Lipidomics;Atherosclerosis;Neural networks","biomedical ultrasonics;blood vessels;cardiovascular system;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;neural nets","convolutional neural network;carotid ultrasound;carotid plaque composition;lipid core;fibrous tissue;calcified tissue;cardiovascular events;cerebrovascular events;image noise;deep learning framework;imaging features;thresholding values;plaque constituents;fibrous cap","Carotid Arteries;Carotid Artery Diseases;Humans;Image Processing, Computer-Assisted;Neural Networks (Computer);Plaque, Atherosclerotic;Ultrasonography","31","37","","","","","IEEE","IEEE Journals"
"DNN-Driven Mixture of PLDA for Robust Speaker Verification","N. Li; M. Mak; J. Chien","Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hong Kong; Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hong Kong; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","6","1371","1383","The mismatch between enrollment and test utterances due to different types of variabilities is a great challenge in speaker verification. Based on the observation that the SNR-level variability or channel-type variability causes heterogeneous clusters in i-vector space, this paper proposes to apply supervised learning to drive or guide the learning of probabilistic linear discriminant analysis (PLDA) mixture models. Specifically, a deep neural network (DNN) is trained to produce the posterior probabilities of different SNR levels or channel types given i-vectors as input. These posteriors then replace the posterior probabilities of indicator variables in the mixture of PLDA. The discriminative training causes the mixture model to perform more reasonable soft divisions of the i-vector space as compared to the conventional mixture of PLDA. During verification, given a test i-vector and a target-speaker's i-vector, the marginal likelihood for the same-speaker hypothesis is obtained by summing the component likelihoods weighted by the component posteriors produced by the DNN, and likewise for the different-speaker hypothesis. Results based on NIST 2012 SRE demonstrate that the proposed scheme leads to better performance under more realistic situations where both training and test utterances cover a wide range of SNRs and different channel types. Unlike the previous SNR-dependent mixture of PLDA which only focuses on SNR mismatch, the proposed model is more general and is potentially applicable to addressing different types of variability in speech.","","","10.1109/TASLP.2017.2692304","RGC of Hong Kong; Taiwan MOST; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894162","Deep neural networks;i-vectors;mixture of PLDA;speaker verification","Signal to noise ratio;Speech;Mixture models;Noise measurement;Training;Robustness;Feature extraction","learning (artificial intelligence);neural nets;pattern clustering;probability;speaker recognition;vectors","DNN-driven mixture;PLDA;robust speaker verification;SNR-level variability;channel-type variability;heterogeneous clusters;i-vector space;supervised learning;probabilistic linear discriminant analysis;mixture models;deep neural network;posterior probabilities;indicator variables;discriminative training;soft divisions;marginal likelihood;same-speaker hypothesis;component posteriors","","10","50","","","","","IEEE","IEEE Journals"
"Sequential Segment Networks for Action Recognition","Q. Chen; Y. Zhang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Signal Processing Letters","","2017","24","5","712","716","Recently, deep convolutional networks (ConvNets) have achieved remarkable progress for action recognition in videos. Most existing deep frameworks treat a video as an unordered frame sequence, and make a prediction by averaging the output of single RGB image, or stacked optical flow field. However, within a video, complex actions may consist of several atomic actions carried out in a sequential manner during its temporal range. To address this issue, we propose a deep learning framework, sequential segment networks (SSN), to model video-level temporal structures in videos. We get several short video snippets by a sparse temporal sampling strategy, and then concatenate the output of ConvNets learned from short snippets; finally, the concatenated consensus vector is fed into a fully connected layer to learn its temporal structure. The sparse sampling strategy and video-level structure enable an efficient and effective training process for SSNs. Extensive empirical studies demonstrate that action recognition performance can be significantly improved by mining temporal structures, and our approach achieves state-of-the-art performance on UCF101 and HMDB51 datasets.","","","10.1109/LSP.2017.2689921","National Nature Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890436","Action recognition;representation learning;sequential segment networks (SSNs);temporal structure","Videos;Image segmentation;Optical imaging;Training;Network architecture;Histograms;Standards","gesture recognition;image sequences;neural nets;video signal processing","sequential segment networks;action recognition;deep convolutional networks;unordered frame sequence;single RGB image;deep learning framework;sparse temporal sampling strategy;video-level structure","","5","34","","","","","IEEE","IEEE Journals"
"Deep Boltzmann Regression With Mimic Features for Oscillometric Blood Pressure Estimation","S. Lee; J. Chang","Department of Electronic Engineering, Hanyang University, Seoul, South Korea; Department of Electronic Engineering, Hanyang University, Seoul, South Korea","IEEE Sensors Journal","","2017","17","18","5982","5993","Oscillometric blood pressure (BP) devices are among the standard automatic monitors, now readily available for the home, office, and hospital. The systolic blood pressure (SBP) and diastolic blood pressure (DBP) are obtained at fixed ratios of the envelope of the maximum amplitude of the oscillometric wave signal. However, these fixed ratios can cause overestimation or underestimation of the real SBP and DBP in oscillometric BP measurements. In this paper, we propose a new regression technique using a deep Boltzmann regression with mimic features based on the bootstrap technique to learn the complex nonlinear relationships between the mimic features vectors acquired from the oscillometric signals and the target BPs. The performance of the proposed model is compared with those of conventional and auscultatory techniques. Our regression model with mimic features provides lower standard deviation of error, mean error, mean absolute error, and standard error of estimates than the conventional techniques, along with a similar fit for the SBP and DBP.","","","10.1109/JSEN.2017.2734104","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7999181","Blood pressure;oscillometric blood pressure estimation;deep neural networks;bootstrap","Blood pressure;Distributed Bragg reflectors;Biomedical monitoring;Estimation;Standards;Training;Feature extraction","blood;blood pressure measurement;bootstrapping;medical signal processing;patient monitoring;regression analysis","oscillometric blood pressure estimation;oscillometric blood pressure devices;standard automatic monitors;diastolic blood pressure;oscillometric wave signal;mimic features vectors;oscillometric signals;conventional techniques;auscultatory techniques;systolic blood pressure;deep Boltzmann regression technique;bootstrap technique","","2","41","","","","","IEEE","IEEE Journals"
"A Reverberation-Time-Aware Approach to Speech Dereverberation Based on Deep Neural Networks","B. Wu; K. Li; M. Yang; C. Lee","National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","1","102","111","A reverberation-time-aware deep-neural-network (DNN)-based speech dereverberation framework is proposed to handle a wide range of reverberation times. There are three key steps in designing a robust system. First, in contrast to sigmoid activation and min-max normalization in state-of-the-art algorithms, a linear activation function at the output layer and global mean-variance normalization of target features are adopted to learn the complicated nonlinear mapping function from reverberant to anechoic speech and to improve the restoration of the low-frequency and intermediate-frequency contents. Next, two key design parameters, namely, frame shift size in speech framing and acoustic context window size at the DNN input, are investigated to show that RT60-dependent parameters are needed in the DNN training stage in order to optimize the system performance in diverse reverberant environments. Finally, the reverberation time is estimated to select the proper frame shift and context window sizes for feature extraction before feeding the log-power spectrum features to the trained DNNs for speech dereverberation. Our experimental results indicate that the proposed framework outperforms the conventional DNNs without taking the reverberation time into account, while achieving a performance only slightly worse than the oracle cases with known reverberation times even for extremely weak and severe reverberant conditions. It also generalizes well to unseen room sizes, loudspeaker and microphone positions, and recorded room impulse responses.","","","10.1109/TASLP.2016.2623559","National Natural Science Foundation of China; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7726012","Acoustic context;deep neural networks (DNNs);frame shift;linear output layer;mean-variance normalization;reverberation-time-aware (RTA);speech dereverberation","Speech;Reverberation;Training;Context;Feature extraction;Speech processing","neural nets;reverberation;speech processing","reverberation-time-aware approach;speech dereverberation framework;deep neural networks;DNN;sigmoid activation;minmax normalization;linear activation function;global mean-variance normalization;nonlinear mapping function;anechoic speech;low-frequency contents;intermediate-frequency contents;design parameters;low-frequency contents;intermediate-frequency contents;frame shift size;speech framing;acoustic context window size;RT60-dependent parameters;DNN training stage;diverse reverberant environments;feature extraction;log-power spectrum features;room sizes;loudspeaker microphone positions;recorded room impulse responses","","24","45","","","","","IEEE","IEEE Journals"
"Automatic Quality Assessment of Echocardiograms Using Convolutional Neural Networks: Feasibility on the Apical Four-Chamber View","A. H. Abdi; C. Luong; T. Tsang; G. Allan; S. Nouranian; J. Jue; D. Hawley; S. Fleming; K. Gin; J. Swift; R. Rohling; P. Abolmaesumi","Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Vancouver General Hospital’s Cardiology Laboratory, Vancouver, BC, Canada; Vancouver General Hospital’s Cardiology Laboratory, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Vancouver General Hospital’s Cardiology Laboratory, Vancouver, BC, Canada; Vancouver General Hospital’s Cardiology Laboratory, Vancouver, BC, Canada; Vancouver General Hospital’s Cardiology Laboratory, Vancouver, BC, Canada; Vancouver General Hospital’s Cardiology Laboratory, Vancouver, BC, Canada; Vancouver General Hospital’s Cardiology Laboratory, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada","IEEE Transactions on Medical Imaging","","2017","36","6","1221","1230","Echocardiography (echo) is a skilled technical procedure that depends on the experience of the operator. The aim of this paper is to reduce user variability in data acquisition by automatically computing a score of echo quality for operator feedback. To do this, a deep convolutional neural network model, trained on a large set of samples, was developed for scoring apical four-chamber (A4C) echo. In this paper, 6,916 end-systolic echo images were manually studied by an expert cardiologist and were assigned a score between 0 (not acceptable) and 5 (excellent). The images were divided into two independent training-validation and test sets. The network architecture and its parameters were based on the stochastic approach of the particle swarm optimization on the training-validation data. The mean absolute error between the scores from the ultimately trained model and the expert's manual scores was 0.71 ± 0.58. The reported error was comparable to the measured intra-rater reliability. The learned features of the network were visually interpretable and could be mapped to the anatomy of the heart in the A4C echo, giving confidence in the training result. The computation time for the proposed network architecture, running on a graphics processing unit, was less than 10 ms per frame, sufficient for real-time deployment. The proposed approach has the potential to facilitate the widespread use of echo at the point-of-care and enable early and timely diagnosis and treatment. Finally, the approach did not use any specific assumptions about the A4C echo, so it could be generalizable to other standard echo views.","","","10.1109/TMI.2017.2690836","Natural Sciences and Engineering Research Council; Canadian Institutes of Health Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892028","Convolutional neural network;deep learning;quality assessment;echocardiography;apical four-chamber;swarm optimization","Machine learning;Quality assessment;Echocardiography;Convolutional neural networks;Particle swarm optimization","data acquisition;echocardiography;medical image processing;neural nets;particle swarm optimisation","automatic quality assessment;echocardiograms;deep convolutional neural networks;apical four-chamber view;data acquisition;echo quality;operator feedback;A4C echo;stochastic approach;particle swarm optimization;training-validation data;intra-rater reliability;graphics processing unit","Echocardiography;Humans;Neural Networks (Computer);Reproducibility of Results","17","41","","","","","IEEE","IEEE Journals"
"Change Detection Based on Deep Siamese Convolutional Network for Optical Aerial Images","Y. Zhan; K. Fu; M. Yan; X. Sun; H. Wang; X. Qiu","Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","10","1845","1849","In this letter, we propose a novel supervised change detection method based on a deep siamese convolutional network for optical aerial images. We train a siamese convolutional network using the weighted contrastive loss. The novelty of the method is that the siamese network is learned to extract features directly from the image pairs. Compared with hand-crafted features used by the conventional change detection method, the extracted features are more abstract and robust. Furthermore, because of the advantage of the weighted contrastive loss function, the features have a unique property: the feature vectors of the changed pixel pair are far away from each other, while the ones of the unchanged pixel pair are close. Therefore, we use the distance of the feature vectors to detect changes between the image pair. Simple threshold segmentation on the distance map can even obtain good performance. For improvement, we use a k-nearest neighbor approach to update the initial result. Experimental results show that the proposed method produces results comparable, even better, with the two state-of-the-art methods in terms of F-measure.","","","10.1109/LGRS.2017.2738149","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8022932","Change detection;deep convolutional network;optical aerial images;siamese network","Feature extraction;Optical sensors;Optical imaging;Training;Optical fiber networks;Synthetic aperture radar;Kernel","feature extraction;geophysical image processing;image segmentation;photogrammetry;radar imaging;synthetic aperture radar;terrain mapping","deep siamese convolutional network;optical aerial image;weighted contrastive loss;hand-crafted feature;conventional change detection method;weighted contrastive loss function;unchanged pixel pair;feature vectors;threshold segmentation;k-nearest neighbor approach;remote sensing;SAR image","","14","15","Traditional","","","","IEEE","IEEE Journals"
"Mispronunciation Detection and Diagnosis in L2 English Speech Using Multidistribution Deep Neural Networks","K. Li; X. Qian; H. Meng","Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong; Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong; Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","1","193","207","This paper investigates the use of multidistribution deep neural networks (DNNs) for mispronunciation detection and diagnosis (MDD), to circumvent the difficulties encountered in an existing approach based on extended recognition networks (ERNs). The ERNs leverage existing automatic speech recognition technology by constraining the search space via including the likely phonetic error patterns of the target words in addition to the canonical transcriptions. MDDs are achieved by comparing the recognized transcriptions with the canonical ones. Although this approach performs reasonably well, it has the following issues: 1) Learning the error patterns of the target words to generate the ERNs remains a challenging task. Phones or phone errors missing from the ERNs cannot be recognized even if we have well-trained acoustic models; and 2) acoustic models and phonological rules are trained independently, and hence, contextual information is lost. To address these issues, we propose an acoustic-graphemic-phonemic model (AGPM) using a multidistribution DNN, whose input features include acoustic features, as well as corresponding graphemes and canonical transcriptions (encoded as binary vectors). The AGPM can implicitly model both grapheme-to-likely-pronunciation and phoneme-to-likely-pronunciation conversions, which are integrated into acoustic modeling. With the AGPM, we develop a unified MDD framework, which works much like free-phone recognition. Experiments show that our method achieves a phone error rate (PER) of 11.1%. The false rejection rate (FRR), false acceptance rate (FAR), and diagnostic error rate (DER) for MDD are 4.6%, 30.5%, and 13.5%, respectively. It outperforms the ERN approach using DNNs as acoustic models, whose PER, FRR, FAR, and DER are 16.8%, 11.0%, 43.6%, and 32.3%, respectively.","","","10.1109/TASLP.2016.2621675","HKSAR Government Research Grants Council General Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752846","Deep neural networks;L2 English speech;mispronunciation detection;mispronunciation diagnosis;speech recognition","Acoustics;Hidden Markov models;Speech;Speech recognition;Neural networks;Context modeling","natural language processing;neural nets;speech processing;speech recognition;voice activity detection;word processing","DER;diagnostic error rate;FAR;false acceptance rate;FRR;false rejection rate;PER;phone error rate;grapheme-to-likely-pronunciation conversion;phoneme-to-likely-pronunciation conversion;AGPM;acoustic-graphemic-phonemic model;well-trained acoustic model;canonical transcription;target word;likely phonetic error pattern;search space;automatic speech recognition technology;MDD;ERN;extended recognition network;mispronunciation detection and diagnosis;multidistribution DNN;multidistribution deep neural network;L2 english speech","","20","98","","","","","IEEE","IEEE Journals"
"Deep Visual-Semantic Alignments for Generating Image Descriptions","A. Karpathy; L. Fei-Fei","Computer Science Department, Stanford University, Stanford, CA; Computer Science Department, Stanford University, Stanford, CA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","4","664","676","We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks (RNN) over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions outperform retrieval baselines on both full images and on a new dataset of region-level annotations. Finally, we conduct large-scale analysis of our RNN language model on the Visual Genome dataset of 4.1 million captions and highlight the differences between image and region-level caption statistics.","","","10.1109/TPAMI.2016.2598339","NVIDIA; GPUs; ONR; MURI; US National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7534740","Image captioning;deep neural networks;visual-semantic embeddings;recurrent neural network;language model","Visualization;Recurrent neural networks;Context;Image segmentation;Analytical models;Natural languages","feedforward neural nets;image processing;inference mechanisms;natural language processing;recurrent neural nets;statistical analysis","deep visual-semantic alignments;natural language image description generation;inter modal correspondences;language data;visual data;convolutional neural networks;image regions;bidirectional recurrent neural networks;bidirectional RNN;multimodal recurrent neural network architecture;multimodal embedding;inferred alignments;Flickr8K dataset;Flickr30K dataset;MSCOCO dataset;region-level annotations;RNN language model;Visual Genome dataset;region-level caption statistics","","124","68","","","","","IEEE","IEEE Journals"
"Encoding Spectral and Spatial Context Information for Hyperspectral Image Classification","X. Sun; F. Zhou; J. Dong; F. Gao; Q. Mu; X. Wang","Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; State Key Laboratory of Applied Optics, Chinese Academy of Sciences, Changchun, China; State Key Laboratory of Applied Optics, Chinese Academy of Sciences, Changchun, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","12","2250","2254","Hyperspectral image (HSI) classification is a popular yet challenging research topic in the remote sensing community. This letter attempts to encode both spectral and spatial information into deep features for HSI classification. We first propose a semisupervised method for training the stacked autoencoder to obtain discriminative deep features. A batch training scheme is introduced to constrain the label consistency on a neighborhood region. Second, a mean pooling procedure is suggested to further fuse the spectral and local spatial information for deep feature generation. The experimental results on two hyperspectral scenes show that the proposed method achieves promising classification performance.","","","10.1109/LGRS.2017.2759168","National Natural Science Foundation of China; Key Research and Development Program of Shandong Province; Open Funding of State Key Laboratory of Applied Optics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8085159","Hyperspectral image (HSI);pattern classification;semisupervised learning;stacked autoencoders (SAE)","Feature extraction;Training;Hyperspectral imaging;Data visualization;Sun","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image representation;learning (artificial intelligence);remote sensing","encoding spectral context information;spatial context information;hyperspectral image classification;spectral information;HSI classification;semisupervised method;stacked autoencoder;discriminative deep features;batch training scheme;local spatial information;deep feature generation","","4","14","","","","","IEEE","IEEE Journals"
"Model-Driven Analysis of Eyeblink Classical Conditioning Reveals the Underlying Structure of Cerebellar Plasticity and Neuronal Activity","A. Antonietti; C. Casellato; E. D’Angelo; A. Pedrocchi","Department of Electronics, Neuroengineering and Medical Robotics Laboratory, Information and Bioengineering, Politecnico di Milano, Milan, Italy; Department of Electronics, Neuroengineering and Medical Robotics Laboratory, Information and Bioengineering, Politecnico di Milano, Milan, Italy; Department of Brain and Behavioral Sciences, Brain Connectivity Center, Istituto di Ricovero e Cura a Carattere Scientifico and the Istituto Neurologico Nazionale C. Mondino, University of Pavia, Pavia, Italy; Department of Electronics, Neuroengineering and Medical Robotics Laboratory, Information and Bioengineering, Politecnico di Milano, Milan, Italy","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","11","2748","2762","The cerebellum plays a critical role in sensorimotor control. However, how the specific circuits and plastic mechanisms of the cerebellum are engaged in closed-loop processing is still unclear. We developed an artificial sensorimotor control system embedding a detailed spiking cerebellar microcircuit with three bidirectional plasticity sites. This proved able to reproduce a cerebellar-driven associative paradigm, the eyeblink classical conditioning (EBCC), in which a precise time relationship between an unconditioned stimulus (US) and a conditioned stimulus (CS) is established. We challenged the spiking model to fit an experimental data set from human subjects. Two subsequent sessions of EBCC acquisition and extinction were recorded and transcranial magnetic stimulation (TMS) was applied on the cerebellum to alter circuit function and plasticity. Evolutionary algorithms were used to find the near-optimal model parameters to reproduce the behaviors of subjects in the different sessions of the protocol. The main finding is that the optimized cerebellar model was able to learn to anticipate (predict) conditioned responses with accurate timing and success rate, demonstrating fast acquisition, memory stabilization, rapid extinction, and faster reacquisition as in EBCC in humans. The firing of Purkinje cells (PCs) and deep cerebellar nuclei (DCN) changed during learning under the control of synaptic plasticity, which evolved at different rates, with a faster acquisition in the cerebellar cortex than in DCN synapses. Eventually, a reduced PC activity released DCN discharge just after the CS, precisely anticipating the US and causing the eyeblink. Moreover, a specific alteration in cortical plasticity explained the EBCC changes induced by cerebellar TMS in humans. In this paper, for the first time, it is shown how closed-loop simulations, using detailed cerebellar microcircuit models, can be successfully used to fit real experimental data sets. Thus, the changes of the model parameters in the different sessions of the protocol unveil how implicit microcircuit mechanisms can generate normal and altered associative behaviors.","","","10.1109/TNNLS.2016.2598190","European Union: REALNET; Human Brain Project, HBP-Regione Lombardia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558111","Cerebellum;distributed plasticity;eyeblink classical conditioning (EBCC);genetic algorithm (GA);long-term plasticity;spiking network model (SNN);transcranial magnetic stimulation (TMS)","Brain modeling;Integrated circuit modeling;Computational modeling;Protocols;Data models;Mathematical model;Neurons","cellular biophysics;data acquisition;learning (artificial intelligence);medical computing;neurophysiology;patient treatment;physiological models","model-driven analysis;eyeblink classical conditioning;cerebellar plasticity;neuronal activity;cerebellum;plastic mechanisms;closed-loop processing;artificial sensorimotor control system;bidirectional plasticity sites;unconditioned stimulus;conditioned stimulus;EBCC acquisition;transcranial magnetic stimulation;circuit function;near-optimal model parameters;optimized cerebellar model;deep cerebellar nuclei;synaptic plasticity;cerebellar cortex;reduced PC activity;cortical plasticity;closed-loop simulations;cerebellar microcircuit;evolutionary algorithm;memory stabilization;Purkinje cells","","1","67","Traditional","","","","IEEE","IEEE Journals"
"Robust Spatial Filtering With Graph Convolutional Neural Networks","F. P. Such; S. Sah; M. A. Dominguez; S. Pillai; C. Zhang; A. Michael; N. D. Cahill; R. Ptucha","Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA; Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA; Department of Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Computer Science, Rochester Institute of Technology, Rochester, NY, USA; Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA; Geisinger Health System, Autism and Developmental Medicine Institute, Danville, PA, USA; Rochester Institute of Technology, School of Mathematical Sciences, Rochester, NY, USA; Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA","IEEE Journal of Selected Topics in Signal Processing","","2017","11","6","884","896","Convolutional neural networks (CNNs) have recently led to incredible breakthroughs on a variety of pattern recognition problems. Banks of finite-impulse response filters are learned on a hierarchy of layers, each contributing more abstract information than the previous layer. The simplicity and elegance of the convolutional filtering process makes them perfect for structured problems, such as image, video, or voice, where vertices are homogeneous in the sense of number, location, and strength of neighbors. The vast majority of classification problems, for example in the pharmaceutical, homeland security, and financial domains are unstructured. As these problems are formulated into unstructured graphs, the heterogeneity of these problems, such as number of vertices, number of connections per vertex, and edge strength, cannot be tackled with standard convolutional techniques. We propose a novel neural learning framework that is capable of handling both homogeneous and heterogeneous data while retaining the benefits of traditional CNN successes. Recently, researchers have proposed variations of CNNs that can handle graph data. In an effort to create learnable filter banks of graphs, these methods either induce constraints on the data or require preprocessing. As opposed to spectral methods, our framework, which we term Graph-CNNs, defines filters as polynomials of functions of the graph adjacency matrix. Graph-CNNs can handle both heterogeneous and homogeneous graph data, including graphs having entirely different vertex or edge sets. We perform experiments to validate the applicability of Graph-CNNs to a variety of structured and unstructured classification problems and demonstrate state-of-the-art results on document and molecule classification problems.","","","10.1109/JSTSP.2017.2726981","National GEM Consortium through the GEM Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979525","Convolutional neural networks;deep learning;graph signal processing","Convolution;Laplace equations;Neural networks;Spectral analysis;Three-dimensional displays;Discrete Fourier transforms","filtering theory;graph theory;learning (artificial intelligence);matrix algebra;neural nets;signal classification","robust spatial filtering;graph convolutional neural network;CNN;pattern recognition problem;finite-impulse response filter;convolutional filtering process;neural learning framework;graph adjacency matrix;structured classification problems;unstructured classification problems;molecule classification problems;document classification problems","","6","58","","","","","IEEE","IEEE Journals"
"RGBD Salient Object Detection via Deep Fusion","L. Qu; S. He; J. Zhang; J. Tian; Y. Tang; Q. Yang","Department of Computer Science, City University of Hong Kong, Hong Kong; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; Department of Computer Science, City University of Hong Kong, Hong Kong; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; University of Science and Technology of China, Hefei, China","IEEE Transactions on Image Processing","","2017","26","5","2274","2285","Numerous efforts have been made to design various low-level saliency cues for RGBD saliency detection, such as color and depth contrast features as well as background and color compactness priors. However, how these low-level saliency cues interact with each other and how they can be effectively incorporated to generate a master saliency map remain challenging problems. In this paper, we design a new convolutional neural network (CNN) to automatically learn the interaction mechanism for RGBD salient object detection. In contrast to existing works, in which raw image pixels are fed directly to the CNN, the proposed method takes advantage of the knowledge obtained in traditional saliency detection by adopting various flexible and interpretable saliency feature vectors as inputs. This guides the CNN to learn a combination of existing features to predict saliency more effectively, which presents a less complex problem than operating on the pixels directly. We then integrate a superpixel-based Laplacian propagation framework with the trained CNN to extract a spatially consistent saliency map by exploiting the intrinsic structure of the input image. Extensive quantitative and qualitative experimental evaluations on three data sets demonstrate that the proposed method consistently outperforms the state-of-the-art methods.","","","10.1109/TIP.2017.2682981","Natural Science Foundation of China; Youth Innovation Promotion Association of the Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7879320","RGBD saliency detection;convolutional neural network;Laplacian propagation","Feature extraction;Image color analysis;Laplace equations;Object detection;Neural networks;Three-dimensional displays;Electronic mail","image colour analysis;image fusion;learning (artificial intelligence);neural nets;object detection","RGBD salient object detection;deep fusion;convolutional neural network;CNN training;raw image pixel;superpixel-based Laplacian propagation framework","","49","55","","","","","IEEE","IEEE Journals"
"Cross-Convolutional-Layer Pooling for Image Recognition","L. Liu; C. Shen; A. v. d. Hengel","School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia; School of Computer Science, The University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","11","2305","2313","Recent studies have shown that a Deep Convolutional Neural Network (DCNN) trained on a large image dataset can be used as a universal image descriptor and that doing so leads to impressive performance for a variety of image recognition tasks. Most of these studies adopt activations from a single DCNN layer, usually a fully-connected layer, as the image representation. In this paper, we proposed a novel way to extract image representations from two consecutive convolutional layers: one layer is used for local feature extraction and the other serves as guidance to pool the extracted features. By taking different viewpoints of convolutional layers, we further develop two schemes to realize this idea. The first directly uses convolutional layers from a DCNN. The second applies the pretrained CNN on densely sampled image regions and treats the fully-connected activations of each image region as a convolutional layer's feature activations. We then train another convolutional layer on top of that as the pooling-guidance convolutional layer. By applying our method to three popular visual classification tasks, we find that our first scheme tends to perform better on applications which demand strong discrimination on lower-level visual patterns while the latter excels in cases that require discrimination on category-level patterns. Overall, the proposed method achieves superior performance over existing approaches for extracting image representations from a DCNN. In addition, we apply cross-layer pooling to the problem of image retrieval and propose schemes to reduce the computational cost. Experimental results suggest that the proposed method achieves promising results for the image retrieval task.","","","10.1109/TPAMI.2016.2637921","Data to Decisions Cooperative Research Centre; Australian Research Council Future Fellowship; Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7779086","Convolutional networks;deep learning;pooling;fine-grained object recognition","Feature extraction;Image representation;Visualization;Image retrieval;Computational efficiency;Image recognition;Neural networks","feature extraction;image recognition;image representation;image retrieval;learning (artificial intelligence);neural nets","densely sampled image regions;image region;pooling-guidance convolutional layer;image representation;cross-layer pooling;image retrieval task;cross-Convolutional-layer pooling;Deep Convolutional Neural Network;image dataset;universal image descriptor;image recognition tasks;single DCNN layer;consecutive convolutional layers;local feature extraction","","11","43","Traditional","","","","IEEE","IEEE Journals"
"Deep learning ensembles for melanoma recognition in dermoscopy images","N. C. F. Codella; Q. -. Nguyen; S. Pankanti; D. A. Gutman; B. Helba; A. C. Halpern; J. R. Smith","NA; NA; NA; NA; NA; NA; NA","IBM Journal of Research and Development","","2017","61","4/5","5:1","5:15","Melanoma is the deadliest form of skin cancer. While curable with early detection, only highly trained specialists are capable of accurately recognizing the disease. As expertise is in limited supply, automated systems capable of identifying disease could save lives, reduce unnecessary biopsies, and reduce costs. Toward this goal, we propose a system that combines recent developments in deep learning with established machine learning approaches, creating ensembles of methods that are capable of segmenting skin lesions, as well as analyzing the detected area and surrounding tissue for melanoma detection. The system is evaluated using the largest publicly available benchmark dataset of dermoscopic images, containing 900 training and 379 testing images. New state-of-the-art performance levels are demonstrated, leading to an improvement in the area under receiver operating characteristic curve of 7.5% (0.843 versus 0.783), in average precision of 4% (0.649 versus 0.624), and in specificity measured at the clinically relevant 95% sensitivity operating point 2.9 times higher than the previous state of the art (36.8% specificity compared to 12.5%). Compared to the average of eight expert dermatologists on a subset of 100 test images, the proposed system produces a higher accuracy (76% versus 70.5%), and specificity (62% versus 59%) evaluated at an equivalent sensitivity (82%).","","","10.1147/JRD.2017.2708299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030303","","Lesions;Malignant tumors;Image segmentation;Skin;Diseases;Cancer","","","","22","51","","","","","IBM","IBM Journals"
"Optimal Transport for Domain Adaptation","N. Courty; R. Flamary; D. Tuia; A. Rakotomamonjy","University of Bretagne Sud, IRISA Laboratory, Rennes, France; University Côte d'Azur, OCA, Lagrange Laboratory, UMR CNRS 7293, Nice, France; Department of Geography, University of Zurich, Zurich, Switzerland; Normandie University, UR, LITIS, Saint-Etienne-du-Rouvray, France","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","9","1853","1865","Domain adaptation is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data representation become more robust when confronted to data depicting the same classes, but described by another observation system. Among the many strategies proposed, finding domain-invariant representations has shown excellent properties, in particular since it allows to train a unique classifier effective in all domains. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labeled samples of the same class in the source domain to remain close during transport. This way, we exploit at the same time the labeled samples in the source and the distributions observed in both domains. Experiments on toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches. In addition, numerical experiments show that our approach leads to better performances on domain invariant deep learning features and can be easily adapted to the semi-supervised case where few labeled samples are available in the target domain.","","","10.1109/TPAMI.2016.2615921","Swiss National Science Foundation; CNRS PEPS Fascido program; Topase project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7586038","Unsupervised domain adaptation;optimal transport;transfer learning;visual adaptation;classification","Transportation;Probability density function;Probability distribution;Training;Feature extraction;Kernel;Data analysis","data analysis;learning (artificial intelligence);pattern classification","domain adaptation;modern data analytics;domain-invariant representations;classifier;regularized unsupervised optimal transportation model;PDFs;domain invariant deep learning features","","21","53","","","","","IEEE","IEEE Journals"
"Time-Frequency Masking in the Complex Domain for Speech Dereverberation and Denoising","D. S. Williamson; D. Wang","Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","7","1492","1501","In real-world situations, speech is masked by both background noise and reverberation, which negatively affect perceptual quality and intelligibility. In this paper, we address monaural speech separation in reverberant and noisy environments. We perform dereverberation and denoising using supervised learning with a deep neural network. Specifically, we enhance the magnitude and phase by performing separation with an estimate of the complex ideal ratio mask. We define the complex ideal ratio mask so that direct speech results after the mask is applied to reverberant and noisy speech. Our approach is evaluated using simulated and real room impulse responses, and with background noises. The proposed approach improves objective speech quality and intelligibility significantly. Evaluations and comparisons show that it outperforms related methods in many reverberant and noisy environments.","","","10.1109/TASLP.2017.2696307","Air Force Office of Scientific Research; National Institute on Deafness and Other Communication Disorders; Ohio Supercomputer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906509","Complex ideal ratio mask;dereverberation;deep neural networks;speech separation;speech quality","Speech;Noise measurement;Reverberation;Speech enhancement;Convolution;Time-frequency analysis","learning (artificial intelligence);neural nets;speech intelligibility;time-frequency analysis;transient response","time-frequency masking;complex domain;speech dereverberation;speech denoising;perceptual quality;perceptual intelligibility;monaural speech separation;reverberant environments;noisy environments;supervised learning;deep neural network;complex ideal ratio mask estimation;real-room impulse responses;simulated impulse responses;background noise;speech quality;speech intelligibility","","35","45","","","","","IEEE","IEEE Journals"
"A Century of Portraits: A Visual Historical Record of American High School Yearbooks","S. Ginosar; K. Rakelly; S. M. Sachs; B. Yin; C. Lee; P. Krähenbühl; A. A. Efros","Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA, USA; Brown University, Providence, RI, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA, USA; Department of Computer Science, University of Texas at Austin, Austin, TX, USA; Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA, USA","IEEE Transactions on Computational Imaging","","2017","3","3","421","431","Imagery offers a rich description of our world and communicates a volume and type of information that cannot be captured by text alone. Since the invention of the camera, an ever-increasing number of photographs document our “visual culture” complementing historical texts. Currently, this treasure trove of knowledge can only be analyzed manually by historians, and only at small scale. In this paper, we perform automated analysis on a large-scale historical image dataset. Our main contributions are: 1) A publicly available dataset of 168,055 (37,921 frontal-facing) American high school yearbook portraits. 2) Weakly supervised data-driven techniques to discover historical visual trends in fashion and identify date-specific visual patterns. 3) A classifier to predict when a portrait was taken, with median error of 4 years for women and 6 for men. 4) A new method for discovering and displaying the visual elements used by the classifier to perform the dating task, finding that they correspond to the tell-tale fashions of each era.","","","10.1109/TCI.2017.2699865","National Science Foundation Graduate Research Fellowship DGE; ONR MURI; NVIDIA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7915779","Data mining;deep learning;image dating;historical data","Visualization;Market research;Social factors;Statistics;Data mining;Machine learning","computer vision;data mining;history;image classification;learning (artificial intelligence)","portraits;visual historical record;American high school yearbooks;imagery;camera;photographs;visual culture;historical text;automated large-scale historical image dataset analysis;supervised data-driven technique;historical visual trend discovery;date-specific visual pattern;classifier;tell-tale fashion","","","43","","","","","IEEE","IEEE Journals"
"Cattle Brand Recognition using Convolutional Neural Network and Support Vector Machines","C. Silva; D. Welfer; F. P. Gioda; C. Dornelles","Univ. Fed. do Pampa, Alegrete, Brazil; Dept. de Comput. Aplic., Univ. Fed. de Santa Maria, Santa Maria, Brazil; Prefeitura Municipal de Sao Francisco de Assis, São Francisco de Assis, Brazil; Prefeitura Municipal de Sao Francisco de Assis, São Francisco de Assis, Brazil","IEEE Latin America Transactions","","2017","15","2","310","316","The recognition images of cattle brand in an automatic way is a necessity to governmental organs responsible for this activity. To help this process, this work presents a method that consists in using Convolutional Neural Network for extracting of characteristics from images of cattle brand and Support Vector Machines for classification. This method consists of six stages: (a) select database of images; (b) select pre-trained CNN; (c) pre-process the images and apply CNN; (d) extract images of features; (e) train and sort images (SVM); (f) evaluate the classification results. The accuracy of the method was tested on database of municipal city hall, where it achieved satisfactory results, comparable to other methods from the literature, reporting 93.28% of accuracy and 12.716 seconds of processing time, respectively.","","","10.1109/TLA.2017.7854627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7854627","Cattle Brands;Convolutional Neural Network;Deep Learning;Recognition Images;Support Vector Machines","Support vector machines;Cows;Image recognition;Neural networks;Feature extraction;Machine learning;Computational modeling","farming;feature extraction;feedforward neural nets;image classification;learning (artificial intelligence);support vector machines;visual databases","municipal city hall;image training;image sorting;image feature extraction;image preprocessing;pretrained CNN;image database;image characteristics extraction;cattle brand image recognition;support vector machines;convolutional neural network","","2","","","","","","IEEE","IEEE Journals"
"STT-RAM Buffer Design for Precision-Tunable General-Purpose Neural Network Accelerator","L. Song; Y. Wang; Y. Han; H. Li; Y. Cheng; X. Li","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2017","25","4","1285","1296","Multilevel spin toque transfer RAM (STT-RAM) is a suitable storage device for energy-efficient neural network accelerators (NNAs), which relies on large-capacity on-chip memory to support brain-inspired large-scale learning models from conventional artificial neural networks to current popular deep convolutional neural networks. In this paper, we investigate the application of multilevel STT-RAM to general-purpose NNAs. First, the error-resilience feature of neural networks is leveraged to tolerate the read/write reliability issue in multilevel cell STT-RAM using approximate computing. The induced read/write failures at the expense of higher storage density can be effectively masked by a wide spectrum of NN applications with intrinsic forgiveness. Second, we present a precision-tunable STT-RAM buffer for the popular general-purpose NNA. The targeted STT-RAM memory design is able to transform between multiple working modes and adaptable to meet the varying quality constraint of approximate applications. Lastly, the reconfigurable STT-RAM buffer not only enables precision scaling in NNA but also provides adaptiveness to the demand for different learning models with distinct working-set sizes. Particularly, we demonstrate the concept of capacity/precision-tunable STT-RAM memory with the emerging reconfigurable deep NNA and elaborate on the data mapping and storage mode switching policy in STT-RAM memory to achieve the best energy efficiency of approximate computing.","","","10.1109/TVLSI.2016.2644279","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835283","Approximate computing;machine learning;neural network;spin toque transfer RAM (STT-RAM)","Artificial neural networks;Computer architecture;Random access memory;Biological neural networks;Approximate computing;Microprocessors;Computational modeling","buffer circuits;energy conservation;integrated circuit reliability;neural nets;random-access storage","multilevel STT-RAM buffer design;multilevel spin toque transfer RAM;precision-tunable general-purpose;energy-efficient neural network accelerators;large-capacity on-chip memory;brain-inspired large-scale learning models;conventional artificial neural networks;deep convolutional neural networks;error-resilience feature;read-write reliability issue;approximate computing;induced read-write failures;precision scaling;capacity-precision-tunable STT-RAM memory;reconfigurable deep NNA;data mapping;storage mode switching policy","","2","33","","","","","IEEE","IEEE Journals"
"An Ensemble of Fine-Tuned Convolutional Neural Networks for Medical Image Classification","A. Kumar; J. Kim; D. Lyndon; M. Fulham; D. Feng","Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Camperdown, NSW, Australia; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Camperdown, NSW, Australia; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Camperdown, NSW, Australia; Department of Molecular Imaging, Royal Prince Alfred Hospital, Australia and Sydney Medical School, The University of Sydney, Camperdown, NSW, Australia; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Camperdown, NSW, Australia","IEEE Journal of Biomedical and Health Informatics","","2017","21","1","31","40","The availability of medical imaging data from clinical archives, research literature, and clinical manuals, coupled with recent advances in computer vision offer the opportunity for image-based diagnosis, teaching, and biomedical research. However, the content and semantics of an image can vary depending on its modality and as such the identification of image modality is an important preliminary step. The key challenge for automatically classifying the modality of a medical image is due to the visual characteristics of different modalities: some are visually distinct while others may have only subtle differences. This challenge is compounded by variations in the appearance of images based on the diseases depicted and a lack of sufficient training data for some modalities. In this paper, we introduce a new method for classifying medical images that uses an ensemble of different convolutional neural network (CNN) architectures. CNNs are a state-of-the-art image classification technique that learns the optimal image features for a given classification task. We hypothesise that different CNN architectures learn different levels of semantic image representation and thus an ensemble of CNNs will enable higher quality features to be extracted. Our method develops a new feature extractor by fine-tuning CNNs that have been initialized on a large dataset of natural images. The fine-tuning process leverages the generic image features from natural images that are fundamental for all images and optimizes them for the variety of medical imaging modalities. These features are used to train numerous multiclass classifiers whose posterior probabilities are fused to predict the modalities of unseen images. Our experiments on the ImageCLEF 2016 medical image public dataset (30 modalities; 6776 training images, and 4166 test images) show that our ensemble of fine-tuned CNNs achieves a higher accuracy than established CNNs. Our ensemble also achieves a higher accuracy than methods in the literature evaluated on the same benchmark dataset and is only overtaken by those methods that source additional training data.","","","10.1109/JBHI.2016.2635663","ARC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7769199","Convolutional neural network (CNN);deep learning;ensembles;fine-tuning;image classification","Biomedical imaging;Training;Feature extraction;Computer architecture;Training data;Informatics;Neural networks","feature extraction;image classification;image representation;medical image processing;neural nets;probability","fine-tuned convolutional neural networks;medical image classification;image features;semantic image representation;feature extraction;natural image dataset;posterior probability;ImageCLEF 2016 medical image public dataset","Diagnostic Imaging;Electronic Health Records;Humans;Image Processing, Computer-Assisted;Machine Learning;Neural Networks (Computer)","84","46","","","","","IEEE","IEEE Journals"
"High-Dimensional Computing as a Nanoscalable Paradigm","A. Rahimi; S. Datta; D. Kleyko; E. P. Frady; B. Olshausen; P. Kanerva; J. M. Rabaey","Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, Berkeley, CA, USA; Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Luleå, Sweden; Helen Wills Neuroscience Institute, University of California at Berkeley, Berkeley, CA, USA; Helen Wills Neuroscience Institute, University of California at Berkeley, Berkeley, CA, USA; Helen Wills Neuroscience Institute, University of California at Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, Berkeley, CA, USA","IEEE Transactions on Circuits and Systems I: Regular Papers","","2017","64","9","2508","2521","We outline a model of computing with high-dimensional (HD) vectors-where the dimensionality is in the thousands. It is built on ideas from traditional (symbolic) computing and artificial neural nets/deep learning, and complements them with ideas from probability theory, statistics, and abstract algebra. Key properties of HD computing include a well-defined set of arithmetic operations on vectors, generality, scalability, robustness, fast learning, and ubiquitous parallel operation, making it possible to develop efficient algorithms for large-scale real-world tasks. We present a 2-D architecture and demonstrate its functionality with examples from text analysis, pattern recognition, and biosignal processing, while achieving high levels of classification accuracy (close to or above conventional machine-learning methods), energy efficiency, and robustness with simple algorithms that learn fast. HD computing is ideally suited for 3-D nanometer circuit technology, vastly increasing circuit density and energy efficiency, and paving a way to systems capable of advanced cognitive tasks.","","","10.1109/TCSI.2017.2705051","Systems on Nanoscale Information fabriCs, one of the six SRC STARnet Centers, sponsored by MARCO and DARPA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7942066","Alternative computing;bio-inspired computing;hyperdimensional computing;vector symbolic architectures;in-memory computing;3D RRAM;pattern recognition","High definition video;Computers;Computer architecture;Nanoscale devices;Robustness;Two dimensional displays;Computational modeling","algebra;biocomputing;learning (artificial intelligence);neural nets;parallel processing","3-D nanometer circuit technology;classification accuracy;biosignal processing;pattern recognition;text analysis;fast learning;ubiquitous parallel operation;arithmetic operations;HD computing;abstract algebra;probability theory;deep learning;artificial neural nets;high-dimensional vectors;nanoscalable paradigm;high-dimensional computing","","14","47","","","","","IEEE","IEEE Journals"
"Violent activity detection with transfer learning method","A. S. Keçeli; A. Kaya","Hacettepe University, Turkey; Hacettepe University, Turkey","Electronics Letters","","2017","53","15","1047","1048","Although action recognition is a widely studied field in computer vision, the recognitions of aggressive activities and crowd violence actions are comparatively less studied. Nowadays, so many surveillance cameras have been installed in the streets and there is a demand for intelligent crowd activity detection systems. A method for violence detection in videos is proposed. The primary contribution is a novel transfer learning-based violence detector that gives promising results compared with the existing detectors. First, the optical flows of the input videos are computed via Lucas-Kanade method. Then, several 2D templates are constructed with overlapping optical flow magnitudes and orientations. These templates are supplied to a pre-trained convolutional neural network as input and deep features of different layers are extracted. Cubic kernel support vector machine and subspace k-nearest neighbours classifiers are trained for prediction and the proposed method is tested with three different datasets that commonly used in violence detection studies.","","","10.1049/el.2017.0970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990292","","","computer vision;image motion analysis;image sensors;neural nets;support vector machines;video surveillance","subspace k-nearest neighbours classifiers;Cubic kernel support vector machine;convolutional neural network;overlapping optical flow magnitudes;Lucas-Kanade method;input videos;video violence detection;surveillance cameras;crowd violence actions;computer vision;action recognition;transfer learning method;violent activity detection","","1","19","","","","","IET","IET Journals"
"Bootstrapping Social Emotion Classification with Semantically Rich Hybrid Neural Networks","X. Li; Y. Rao; H. Xie; R. Y. K. Lau; J. Yin; F. L. Wang","School of Data and Computer Science, Sun Yat-sen University, No. 132 Waihuan East Rd., Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, No. 132 Waihuan East Rd., Guangzhou, China; Department of Mathematics and Information Technology, The Education University of Hong Kong, Tai Po, Hong Kong SAR; Department of Information Systems, City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong SAR; School of Data and Computer Science, Sun Yat-sen University, No. 132 Waihuan East Rd., Guangzhou, China; Caritas Institute of Higher Education, Tseung Kwan O, Hong Kong SAR","IEEE Transactions on Affective Computing","","2017","8","4","428","442","Social emotion classification aims to predict the aggregation of emotional responses embedded in online comments contributed by various users. Such a task is inherently challenging because extracting relevant semantics from free texts is a classical research problem. Moreover, online comments are typically characterized by a sparse feature space, which makes the corresponding emotion classification task very difficult. On the other hand, though deep neural networks have been shown to be effective for speech recognition and image analysis tasks because of their capabilities of transforming sparse low-level features to dense high-level features, their effectiveness on emotion classification requires further investigation. The main contribution of our work reported in this paper is the development of a novel model of semantically rich hybrid neural network (HNN) which leverages unsupervised teaching models to incorporate semantic domain knowledge into the neural network to bootstrap its inference power and interpretability. To our best knowledge, this is the first successful work of incorporating semantics into neural networks to enhance social emotion classification and network interpretability. Through empirical studies based on three real-world social media datasets, our experimental results confirm that the proposed hybrid neural networks outperform other state-of-the-art emotion classification methods.","","","10.1109/TAFFC.2017.2716930","National Natural Science Foundation of China; Internal Research Grant; The Education University of Hong Kong; Research Grants Council of Hong Kong Special Administrative Region, China; Research Grants Council; Hong Kong Special Administrative Region, China; Shenzhen Municipal Science and Technology R&D Funding-Basic Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953530","Social emotion classification;hybrid neural network;sparse encoding;transfer learning","Neural networks;Semantics;Unsupervised learning;Feature extraction;Encoding;Computational modeling;Emotion recognition","cognitive systems;emotion recognition;neural nets;pattern classification;social networking (online);statistical analysis;text analysis;unsupervised learning","hybrid neural networks;bootstrapping social emotion classification;semantically rich hybrid neural network;emotional responses;online comments;classical research problem;sparse feature space;deep neural networks;speech recognition;image analysis tasks;low-level features;high-level features;semantic domain knowledge;network interpretability;real-world social media datasets;unsupervised teaching model","","5","67","Traditional","","","","IEEE","IEEE Journals"
"Multiview Canonical Correlation Analysis Networks for Remote Sensing Image Recognition","X. Yang; W. Liu; D. Tao; J. Cheng; S. Li","School of Information and Control Engineering, China University of Petroleum, Qingdao, China; School of Information and Control Engineering, China University of Petroleum, Qingdao, China; School of Information Science and Engineering, Yunnan University, Kunming, China; Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China; China Aerospace Science and Technology Corporation, 16th Institute, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","10","1855","1859","In the past decade, deep learning (DL) algorithms have been widely used for remote sensing (RS) image recognition tasks. As the most typical DL model, convolutional neural networks (CNNs) achieves outstand performance for big RS data classification. Recently, a variant of CNN, dubbed canonical correlation analysis network (CCANet), was proposed to abstract the two-view image features. Extensive experiments conducted on several benchmark databases validate the effectiveness of CCANet. However, the CCANet structure is powerless when the observations arrive from more than two sources. To serve the multiview purpose, in this letter, we propose multiview CCANets (MCCANets). Particularly, the MCCANet model learns the stacked multiperspective filter banks by the MCCA method and builds a deep convolutional structure. In the output stage, the binarization and the blockwise histogram are employed as nonlinear processing and feature pooling, respectively. To access the effectiveness of the MCCANet, we conduct a host of experiments on the RSSCN7 RS database. Extensive experimental results demonstrate that the MCCANet outperforms the two-view CCANet.","","","10.1109/LGRS.2017.2738671","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Graduate Student Innovation Project Funding of China University of Petroleum; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8023806","Deep learning (DL);image recognition;multiview canonical correlation analysis (MCCA);remote sensing (RS)","Image recognition;Correlation;Remote sensing;Eigenvalues and eigenfunctions;Machine learning;Algorithm design and analysis;Histograms","geophysical image processing;image recognition;remote sensing","multiview canonical correlation analysis networks;remote sensing image recognition;convolutional neural networks;dubbed canonical correlation analysis network;MCCANet model","","2","17","Traditional","","","","IEEE","IEEE Journals"
"The discrimination method as applied to a deteriorated porcelain insulator used in transmission lines on the basis of a convolution neural network","Y. Liu; S. Pei; W. Fu; K. Zhang; X. Ji; Z. Yin","Hebei Provincial Key Laboratory of Power Transmission Equipment Security Defense, North China Electric Power University, Baoding city, Hebei Province, 071003, China; Hebei Provincial Key Laboratory of Power Transmission Equipment Security Defense, North China Electric Power University, Baoding city, Hebei Province, 071003, China; State Grid Hebei Electric Power Company, Shi Jiazhuang, Hebei Province, 050071, China; Hebei Provincial Key Laboratory of Power Transmission Equipment Security Defense, North China Electric Power University, Baoding city, Hebei Province, 071003, China; Hebei Provincial Key Laboratory of Power Transmission Equipment Security Defense, North China Electric Power University, Baoding city, Hebei Province, 071003, China; State Grid Hebei Electric Power Company, Shi Jiazhuang, Hebei Province, 050071, China","IEEE Transactions on Dielectrics and Electrical Insulation","","2017","24","6","3559","3566","Based on the analysis of the principle and structure of a convolutional neural network (CNN) model used for in-depth learning, an intelligent discriminant diagnosis method for porcelain fuselage insulators in transmission lines is proposed. Firstly, the infrared image of a porcelain insulator is extracted, and then Lenet is used to optimize the network structure. Finally, the model of fixed parameters is formed by training. The model has high classification and judgment robustness and offers accuracy under different conditions such as: temperature, humidity, position of deterioration on the insulator, and thermal load, which allows weight-sharing in the CNN model under different environmental conditions. Based on the experimental data from an infrared heating experiment using a porcelain deteriorated insulator, this work uses the back-propagation gradient descent method to train the model, to form an intelligent detection model for deteriorated insulators. This method has the advantages of high accuracy and robustness, and represents a new method for intelligent detection of deteriorated insulators.","","","10.1109/TDEI.2017.006840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8315279","Convolution neural network;deep learning;infrared imaging;asset management;insulators","Insulators;Convolution;Training;Porcelain;Biological neural networks;Feature extraction","backpropagation;feedforward neural nets;gradient methods;image processing;infrared imaging;insulator testing;learning (artificial intelligence);porcelain insulators;power engineering computing;power transmission lines","intelligent detection model;back-propagation gradient descent method;porcelain deteriorated insulator;infrared heating experiment;different environmental conditions;CNN model;judgment robustness;high classification;fixed parameters;network structure;infrared image;porcelain fuselage insulators;intelligent discriminant diagnosis method;in-depth learning;convolutional neural network model;transmission lines;deteriorated porcelain insulator;discrimination method","","4","19","","","","","IEEE","IEEE Journals"
"Fast Classification for Large Polarimetric SAR Data Based on Refined Spatial-Anchor Graph","H. Liu; S. Yang; S. Gou; P. Chen; Y. Wang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","9","1589","1593","The graph model-based semisupervised machine learning is well established. However, its computational complexity is still high in terms of the time consumption especially for large data. In this letter, we propose a fast semisupervised classification algorithm using the recently presented spatial-anchor graph for a large polarimetric synthetic aperture radar (Pol-SAR) data, named as Fast Spatial-Anchor Graph (FSAG) based algorithm. Based on an initial superpixel segmentation on the PolSAR image, the homogenous regions are obtained. The border pixels are reassigned to the most similar superpixel according to majority voting and distance measurement. Then, feature vectors are weighted within local homogenous regions. The refined spatial-anchor graph is constructed with these regions, and the semisupervised classification is conducted. Experimental results on synthesized and real PolSAR data indicate that the proposed FSAG greatly reduces time consumption and maintains the accuracy for terrain classifications compared with state-of-the-art graph-based approaches.","","","10.1109/LGRS.2017.2724844","National Basic Research Program of China; National Natural Science Foundation of China; Ministry of Education of China; National Science Basic Research Plan in Shaanxi province of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7997716","Deep learning;feature extraction;polarimetric synthetic aperture radar (PolSAR)","Image segmentation;Feature extraction;Kernel;Covariance matrices;Polarimetric synthetic aperture radar;Distance measurement","computational complexity;image segmentation;learning (artificial intelligence);radar imaging;radar polarimetry;synthetic aperture radar","fast classification;large polarimetric SAR data;refined spatial-anchor graph;graph model-based semisupervised machine learning;computational complexity;synthetic aperture radar;fast spatial-anchor graph based algorithm;image segmentation;PolSAR image","","3","11","","","","","IEEE","IEEE Journals"
"3D Convolutional Neural Networks for Cross Audio-Visual Matching Recognition","A. Torfi; S. M. Iranmanesh; N. Nasrabadi; J. Dawson","Lane Department of Computer Science and Electrical Engineering, West Virginia University College of Engineering and Mineral Resources, Morgantown, WV, USA; Lane Department of Computer Science and Electrical Engineering, West Virginia University College of Engineering and Mineral Resources, Morgantown, WV, USA; Lane Department of Computer Science and Electrical Engineering, West Virginia University College of Engineering and Mineral Resources, Morgantown, WV, USA; Lane Department of Computer Science and Electrical Engineering, West Virginia University College of Engineering and Mineral Resources, Morgantown, WV, USA","IEEE Access","","2017","5","","22081","22091","Audio-visual recognition (AVR) has been considered as a solution for speech recognition tasks when the audio is corrupted, as well as a visual recognition method used for speaker verification in multispeaker scenarios. The approach of AVR systems is to leverage the extracted information from one modality to improve the recognition ability of the other modality by complementing the missing information. The essential problem is to find the correspondence between the audio and visual streams, which is the goal of this paper. We propose the use of a coupled 3D convolutional neural network (3D CNN) architecture that can map both modalities into a representation space to evaluate the correspondence of audio-visual streams using the learned multimodal features. The proposed architecture will incorporate both spatial and temporal information jointly to effectively find the correlation between temporal information for different modalities. By using a relatively small network architecture and much smaller data set for training, our proposed method surpasses the performance of the existing similar methods for audio-visual matching, which use 3D CNNs for feature representation. We also demonstrate that an effective pair selection method can significantly increase the performance. The proposed method achieves relative improvements over 20% on the equal error rate and over 7% on the average precision in comparison to the state-of-the-art method.","","","10.1109/ACCESS.2017.2761539","Center for Identification Technology Research (CITeR); NSF Industry/University Cooperative Research Center (I/UCRC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8063416","Convolutional networks;3D architecture;deep learning;audio-visual recognition","Feature extraction;Speech;Speech recognition;Visualization;Lips;Three-dimensional displays;Data mining","feature extraction;feedforward neural nets;image recognition;learning (artificial intelligence);neural net architecture;object recognition;speaker recognition","cross audio-visual matching recognition;speech recognition tasks;visual recognition method;speaker verification;multispeaker scenarios;AVR systems;audio streams;coupled 3D convolutional neural network architecture;3D CNN;audio-visual streams;learned multimodal features;temporal information;relatively small network architecture;pair selection method","","13","41","","","","","IEEE","IEEE Journals"
"Clustering Single-Cell Expression Data Using Random Forest Graphs","M. B. Pouyan; M. Nourani","Department of Electrical Engineering, Quality of Life Technology Laboratory, The University of Texas at Dallas, Richardson, TX, USA; Department of Electrical Engineering, Quality of Life Technology Laboratory, The University of Texas at Dallas, Richardson, TX, USA","IEEE Journal of Biomedical and Health Informatics","","2017","21","4","1172","1181","Complex tissues such as brain and bone marrow are made up of multiple cell types. As the study of biological tissue structure progresses, the role of cell-type-specific research becomes increasingly important. Novel sequencing technology such as single-cell cytometry provides researchers access to valuable biological data. Applying machine-learning techniques to these high-throughput datasets provides deep insights into the cellular landscape of the tissue where those cells are a part of. In this paper, we propose the use of random-forest-based single-cell profiling, a new machine-learning-based technique, to profile different cell types of intricate tissues using single-cell cytometry data. Our technique utilizes random forests to capture cell marker dependences and model the cellular populations using the cell network concept. This cellular network helps us discover what cell types are in the tissue. Our experimental results on public-domain datasets indicate promising performance and accuracy of our technique in extracting cell populations of complex tissues.","","","10.1109/JBHI.2016.2565561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7467404","Clustering;random forest (RF);shared nearest neighbor (SNN);single-cell;tissue profiling","Sociology;Statistics;Euclidean distance;Vegetation;Proteins;Bones","bioinformatics;biological techniques;biological tissues;bone;brain;cellular biophysics;learning (artificial intelligence);random processes","single-cell expression data clustering;random forest graphs;complex tissues;brain;bone marrow;multiple cell types;biological tissue structure;cell-type-specific research;sequencing technology;biological data;high-throughput datasets;cellular landscape;random-forest-based single-cell profiling;machine-learning-based technique;single-cell cytometry data;cell marker dependences;cellular populations;cell network concept;public-domain datasets;cell populations","Algorithms;Animals;Bone Marrow Cells;Cluster Analysis;Computational Biology;Databases, Factual;Decision Trees;Gene Expression Profiling;Humans;Machine Learning;Mice;Single-Cell Analysis","2","45","","","","","IEEE","IEEE Journals"
"Adaptive Critic Nonlinear Robust Control: A Survey","D. Wang; H. He; D. Liu","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Electrical, Computer, and Biomedical Engineering, University of Rhode Island, Kingston, RI, USA; School of Automation, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Cybernetics","","2017","47","10","3429","3451","Adaptive dynamic programming (ADP) and reinforcement learning are quite relevant to each other when performing intelligent optimization. They are both regarded as promising methods involving important components of evaluation and improvement, at the background of information technology, such as artificial intelligence, big data, and deep learning. Although great progresses have been achieved and surveyed when addressing nonlinear optimal control problems, the research on robustness of ADP-based control strategies under uncertain environment has not been fully summarized. Hence, this survey reviews the recent main results of adaptive-critic-based robust control design of continuous-time nonlinear systems. The ADP-based nonlinear optimal regulation is reviewed, followed by robust stabilization of nonlinear systems with matched uncertainties, guaranteed cost control design of unmatched plants, and decentralized stabilization of interconnected systems. Additionally, further comprehensive discussions are presented, including event-based robust control design, improvement of the critic learning rule, nonlinear H∞ control design, and several notes on future perspectives. By applying the ADP-based optimal and robust control methods to a practical power system and an overhead crane plant, two typical examples are provided to verify the effectiveness of theoretical results. Overall, this survey is beneficial to promote the development of adaptive critic control methods with robustness guarantee and the construction of higher level intelligent systems.","","","10.1109/TCYB.2017.2712188","National Natural Science Foundation of China; Beijing Natural Science Foundation; U.S. National Science Foundation; Early Career Development Award of SKLMCCS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967695","Adaptive critic designs;adaptive/approximate dynamic programming (ADP);boundedness;convergence;neural networks;optimal control;reinforcement learning;robust control;stability","Robustness;Robust control;Optimal control;Dynamic programming;Nonlinear systems;Learning (artificial intelligence);Uncertainty","adaptive control;continuous time systems;control system synthesis;dynamic programming;H∞ control;interconnected systems;nonlinear control systems;robust control;uncertain systems","optimal control;nonlinear H∞ control design;critic learning rule;event-based robust control design;interconnected systems;decentralized stabilization;unmatched plants;guaranteed cost control design;matched uncertainties;robust stabilization;nonlinear optimal regulation;continuous-time nonlinear systems;uncertain environment;robustness;intelligent optimization;reinforcement learning;ADP;adaptive dynamic programming;adaptive critic nonlinear robust control","","29","208","","","","","IEEE","IEEE Journals"
"Crowded Scene Understanding by Deeply Learned Volumetric Slices","J. Shao; C. C. Loy; K. Kang; X. Wang","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Circuits and Systems for Video Technology","","2017","27","3","613","623","Crowd video analysis is one of the hallmark tasks of crowded scene understanding. While we observe a tremendous progress in image-based tasks with the rise of convolutional neural networks (CNNs), performance on video analysis has not (yet) attained the same level of success. In this paper, we introduce intuitive but effective temporal-aware crowd motion channels by uniformly slicing the video volume from different dimensions. Multiple CNN structures with different data-fusion strategies and weight-sharing schemes are proposed to learn the connectivity both spatially and temporally from these motion channels. To well demonstrate our deep model, we construct a new large-scale Who do What at someWhere crowd data set with 10 000 videos from 8257 crowded scenes, and build an attribute set with 94 attributes. Extensive experiments on crowd video attribute prediction demonstrate the effectiveness of our novel method over the state-of-the-art.","","","10.1109/TCSVT.2016.2593647","General Research Fund through the Research Grants Council, Hong Kong; Hong Kong Innovation and Technology Support Programme; Shenzhen Basic Research Program; NVIDIA Corporation for Hardware Donation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7517290","Crowd database;crowded scene understanding;deep neural network;spatiotemporal features;video analysis","World Wide Web;Neural networks;Feature extraction;Surveillance;Semantics;Image segmentation;Motion segmentation","image fusion;image motion analysis;neural nets;video signal processing","crowded scene understanding;image-based tasks;convolutional neural networks;temporal-aware crowd motion channels;video volume;multiple CNN structures;data-fusion;weight-sharing schemes;motion channels;deep model;large-scale Who do What;someWhere crowd data set;attribute set;crowd video analysis performance;attribute prediction","","14","49","","","","","IEEE","IEEE Journals"
"Learning Hierarchical Decision Trees for Single-Image Super-Resolution","J. Huang; W. Siu","Department of Electronic and Information Engineering, Centre for Signal Processing, The Hong Kong Polytechnic University, Hong Kong; Department of Electronic and Information Engineering, Centre for Signal Processing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Circuits and Systems for Video Technology","","2017","27","5","937","950","Sparse representation has been extensively studied for image super-resolution (SR), and it achieved great improvement. Deep-learning-based SR methods have also emerged in the literature to pursue better SR results. In this paper, we propose to use a set of decision tree strategies for fast and high-quality image SR. Our proposed SR using decision tree (SRDT) method takes the divide-and-conquer strategy, which performs a few simple binary tests to classify an input low-resolution (LR) patch into one of the leaf nodes and directly multiplies this LR patch with the regression model at that leaf node for regression. Both the classification process and the regression process take an extremely small amount of computation. To further boost the SR results, we introduce a SR using hierarchical decision trees (SRHDT) method, which cascades multiple layers of decision trees for SR and progressively refines the estimated high-resolution image. Inspired by the random forests approach, which combines regression models from an ensemble of decision trees, we propose to fuse regression models from relevant leaf nodes within the same decision tree to form a more robust approach. The SRHDT method with fused regression model (SRHDT_f) improves further the SRHDT method by 0.1-dB in PNSR. Our experimental results show that our initial approach, the SRDT method, achieves SR results comparable to those of the sparse-representation-based method and the deep-learning-based method, but our method is much faster. Furthermore, our enhanced version, the SRHDT_f method, achieves more than 0.3-dB higher PSNR than that of the A+ method, which is the state-of-the-art method in SR.","","","10.1109/TCSVT.2015.2513661","Center for Signal Processing, The Hong Kong Polytechnic University; Research Grants Council through the Hong Kong Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7368907","Classification;decision tree;image processing;regression and training;single-image super-resolution (SR)","Image resolution;Regression tree analysis;Dictionaries;Training;Image reconstruction;Data models","decision trees;image representation;image resolution;regression analysis","sparse representation;image super-resolution;deep-learning-based SR methods;decision tree strategies;SRDT method;divide-and-conquer strategy;binary tests;input low-resolution patch;classification process;regression process;hierarchical decision trees method;SRHDT method;high-resolution image;random forests approach;leaf nodes;fused regression model;sparse-representation-based method","","19","51","","","","","IEEE","IEEE Journals"
"Resilience-Aware Frequency Tuning for Neural-Network-Based Approximate Computing Chips","Y. Wang; J. Deng; Y. Fang; H. Li; X. Li","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2017","25","10","2736","2748","Unlike conventional ICs, approximate computing chips are less sensitive to hardware errors. This fascinating feature can be utilized to improve the performance of chip design and even change the timing closure procedure of digital circuit design flow. In this paper, we study the potential of resilience-aware circuit clocking scheme, and demonstrate the methodology with advanced neural network (NN)-based accelerator. We propose a novel timing analysis and frequency setting method for NN-based approximate computing circuits based on in-field NN retraining. With the proposed iterative retiming-and-retraining framework, NN-based accelerator can be retrained to operate safely at aggressive operating frequencies compared with the frequency decided purely by statistical timing analysis or Monto Carlo analysis. For nanometer process technology with increasing threats of timing errors induced by process variation, noises, and so on, our retiming-and-retraining method enables higher circuit operating frequency and enables dynamic precision/frequency adjustment for approximate computing circuits. We evaluate the methodology with both the neural and deep learning accelerators in experiments. The experimental results show that timing errors in neural circuits can be effectively tamed for different applications, so that the circuits can operate at higher clocking rates under the specified quality constraint or be dynamically scaled to work at a wide range of frequency states with only minor accuracy losses.","","","10.1109/TVLSI.2017.2682885","National Natural Science Foundation of China; National Science and Technology Major Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889012","Deep learning;error tolerance;neural network (NN);timing variation","Timing;Artificial neural networks;Approximate computing;Hardware;Clocks;Logic gates;Tuning","iterative methods;neural nets;timing circuits","approximate computing chips;hardware errors;chip design;digital circuit design;resilience-aware circuit clocking scheme;neural network-based accelerator;NN-based accelerator;timing analysis;frequency setting method;NN-based approximate computing circuits;in-field NN retraining;iterative retiming-and-retraining framework;nanometer process technology;timing errors;dynamic precision-frequency adjustment;deep learning accelerators","","3","34","Traditional","","","","IEEE","IEEE Journals"
"Exploiting Depth From Single Monocular Images for Object Detection and Semantic Segmentation","Y. Cao; C. Shen; H. T. Shen","The University of Adelaide, Adelaide, SA, Australia; The University of Adelaide, Adelaide, SA, Australia; The University of Queensland, St Lucia, QLD, Australia","IEEE Transactions on Image Processing","","2017","26","2","836","846","Augmenting RGB data with measured depth has been shown to improve the performance of a range of tasks in computer vision, including object detection and semantic segmentation. Although depth sensors such as the Microsoft Kinect have facilitated easy acquisition of such depth information, the vast majority of images used in vision tasks do not contain depth information. In this paper, we show that augmenting RGB images with estimated depth can also improve the accuracy of both object detection and semantic segmentation. Specifically, we first exploit the recent success of depth estimation from monocular images and learn a deep depth estimation model. Then, we learn deep depth features from the estimated depth and combine with RGB features for object detection and semantic segmentation. In addition, we propose an RGB-D semantic segmentation method, which applies a multi-task training scheme: semantic label prediction and depth value regression. We test our methods on several data sets and demonstrate that incorporating information from estimated depth improves the performance of object detection and semantic segmentation remarkably.","","","10.1109/TIP.2016.2621673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7707416","Object detection;semantic segmentation;depth estimation;deep networks","Semantics;Object detection;Estimation;Image segmentation;Three-dimensional displays;Feature extraction;Image color analysis","estimation theory;feature extraction;image colour analysis;image segmentation;learning (artificial intelligence);object detection;regression analysis","depth value regression;semantic label prediction;multitask training scheme;RGB-D semantic segmentation method;deep depth estimation model;Microsoft Kinect;depth measurement;depth sensor;computer vision;RGB data image augmentation;object detection;single monocular imaging","","17","53","","","","","IEEE","IEEE Journals"
"A Convolutional Neural Network-Based Chinese Text Detection Algorithm via Text Structure Modeling","X. Ren; Y. Zhou; J. He; K. Chen; X. Yang; J. Sun","Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Engineering and Applied Science, Aston University, Birmingham, U.K.; Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Multimedia","","2017","19","3","506","518","Text detection in a natural environment plays an important role in many computer vision applications. While existing text detection methods are focused on English characters, there are strong application demands on text detection in other languages, such as Chinese. In this paper, we present a novel text detection algorithm for Chinese characters based on a specific designed convolutional neural network (CNN). The CNN contains a text structure component detector layer, a spatial pyramid layer, and a multi-input-layer deep belief network (DBN). The CNN is pre-trained via a convolutional sparse auto-encoder, specifically designed for extracting complex features from Chinese characters. In particular, the text structure component detectors enhance the accuracy and uniqueness of feature descriptors by extracting multiple text structure components in various ways. The spatial pyramid layer enhances the scale invariability of the CNN for detecting texts in multiple scales. Finally, the multi-input-layer DBN replaces the fully connected layers in the CNN to ensure features from multiple scales are comparable. A multilingual text detection dataset, in which texts in Chinese, English, and digits are labeled separately, is set up to evaluate the proposed text detection algorithm. The proposed algorithm shows a significant performance improvement over the baseline CNN algorithms. In addition the proposed algorithm is evaluated over a public multilingual benchmark and achieves state-of-the-art result under multiple languages. Furthermore, a simplified version of the proposed algorithm with only general components is evaluated on the ICDAR 2011 and 2013 datasets, showing comparable detection performance to the existing general text detection algorithms.","","","10.1109/TMM.2016.2625259","National Key Research and Development Program of China; National Natural Science Foundation of China; Shanghai Science and Technology Committees of Scientific Research Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733055","Chinese text detection;convolutional neural network (CNN);text structure detector;unsupervised learning","Feature extraction;Detection algorithms;Detectors;Neural networks;Unsupervised learning;Machine learning;Image edge detection","computer vision;natural language processing;neural nets;text analysis","convolutional neural network;chinese text detection algorithm;text structure modeling;computer vision applications;English characters;Chinese characters;CNN;text structure component detector layer;spatial pyramid layer;multi-input-layer deep belief network;DBN;convolutional sparse auto-encoder;complex feature extraction;multiple text structure components;multilingual text detection dataset;Chinese;English;general text detection algorithms","","14","38","","","","","IEEE","IEEE Journals"
"Improving Low-Dose CT Image Using Residual Convolutional Network","W. Yang; H. Zhang; J. Yang; J. Wu; X. Yin; Y. Chen; H. Shu; L. Luo; G. Coatrieux; Z. Gui; Q. Feng","Department of Biomedical Engineering, Guangdong Provincial Key Laboratory of Medical Image Processing, Southern Medical University, Guangzhou, China; Laboratory of Image Science and Technology, Southeast University, Nanjing, China; Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Electronics, Beijing Institute of Technology, Beijing, China; Laboratory of Image Science and Technology, Southeast University, Nanjing, China; Laboratory of Image Science and Technology, Southeast University, Nanjing, China; Laboratory of Image Science and Technology, Southeast University, Nanjing, China; Laboratory of Image Science and Technology, Southeast University, Nanjing, China; Laboratory of Image Science and Technology, Southeast University, Nanjing, China; Institut Mines-Telecom, Telecom Bretagne, INSERM U1101 LaTIM, Brest, France; Shanxi Provincial Key Laboratory for Biomedical Imaging and Big Data, North University of China, Taiyuan, China; Department of Biomedical Engineering, Guangdong Provincial Key Laboratory of Medical Image Processing, Southern Medical University, Guangzhou, China","IEEE Access","","2017","5","","24698","24705","Low-dose CT is an effective solution to alleviate radiation risk to patients, it also introduces additional noise and streak artifacts. In order to maintain a high image quality for low-dose scanned CT data, we propose a post-processing method based on deep learning and using 2-D and 3-D residual convolutional networks. Experimental results and comparisons with other competing methods show that the proposed approach can effectively reduce the low-dose noise and artifacts while preserving tissue details. It is also pointed out that the 3-D model can achieve better performance in both edge-preservation and noise-artifact suppression. Factors that may influence the model performance, such as model width, depth, and dropout, are also examined.","","","10.1109/ACCESS.2017.2766438","State’s Key Project of Research and Development Plan; National Natural Science Foundation; Fundamental Research Funds for the Central Universities; Qing Lan Project in Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8082505","Low-dose CT;convolution neural network;residual learning;3D convolution","Convolution;Computed tomography;Three-dimensional displays;Training;Two dimensional displays;Kernel;Solid modeling","computerised tomography;feedforward neural nets;image denoising;learning (artificial intelligence);medical image processing","low-dose noise;edge-preservation;noise-artifact suppression;low-dose CT image;radiation risk;additional noise;streak artifacts;high image quality;CT data;post-processing method;deep learning;3D residual convolutional networks;2D residual convolutional network;biological tissue;3D model","","12","24","","","","","IEEE","IEEE Journals"
"Deep learning classification in asteroseismology","M. Hon; D. Stello; J. Yu","NA; NA; NA","Monthly Notices of the Royal Astronomical Society","","2017","469","4","4578","4583","In the power spectra of oscillating red giants, there are visually distinct features defining stars ascending the red giant branch from those that have commenced helium core burning. We train a 1D convolutional neural network by supervised learning to automatically learn these visual features from images of folded oscillation spectra. By training and testing on Kepler red giants, we achieve an accuracy of up to 99 per cent in separating helium-burning red giants from those ascending the red giant branch. The convolutional neural network additionally shows capability in accurately predicting the evolutionary states of 5379 previously unclassified Kepler red giants, by which we now have greatly increased the number of classified stars.","","","10.1093/mnras/stx1174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8201335","asteroseismology;methods: data analysis;techniques: image processing;stars: oscillations;stars: statistics","","","","","","","","","","","OUP","OUP Journals"
"Correlated Topic Vector for Scene Classification","P. Wei; F. Qin; F. Wan; Y. Zhu; J. Jiao; Q. Ye","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2017","26","7","3221","3234","Scene images usually involve semantic correlations, particularly when considering large-scale image data sets. This paper proposes a novel generative image representation, correlated topic vector, to model such semantic correlations. Oriented from the correlated topic model, correlated topic vector intends to naturally utilize the correlations among topics, which are seldom considered in the conventional feature encoding, e.g., Fisher vector, but do exist in scene images. It is expected that the involvement of correlations can increase the discriminative capability of the learned generative model and consequently improve the recognition accuracy. Incorporated with the Fisher kernel method, correlated topic vector inherits the advantages of Fisher vector. The contributions to the topics of visual words have been further employed by incorporating the Fisher kernel framework to indicate the differences among scenes. Combined with the deep convolutional neural network (CNN) features and Gibbs sampling solution, correlated topic vector shows great potential when processing large-scale and complex scene image data sets. Experiments on two scene image data sets demonstrate that correlated topic vector improves significantly the deep CNN features, and outperforms existing Fisher kernel-based features.","","","10.1109/TIP.2017.2694320","National Nature Science Foundation of China; Beijing Municipal Science and Technology Commission; Science and Technology Innovation Foundation of Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7898787","Correlated topic vector;Fisher kernel;generative feature learning;semantic correlation","Semantics;Kernel;Correlation;Visualization;Image recognition;Feature extraction;Image coding","image classification;image coding;image recognition;image representation;neural nets","correlated topic vector;scene classification;scene images;semantic correlations;large-scale image data sets;image representation;correlated topic model;feature encoding;Fisher vector;discriminative capability;learned generative model;recognition accuracy;Fisher kernel method;visual words;deep convolutional neural network;Gibbs sampling solution;complex scene image data set;large-scale scene image data set;deep CNN features;Fisher kernel-based features","","","58","","","","","IEEE","IEEE Journals"
"A Radio Resource Virtualization-Based RAT Selection Scheme in Heterogeneous Networks","S. Fan; H. Tian; W. Wang","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Communications Letters","","2017","21","5","1147","1150","In the future heterogeneous wireless networks, heterogeneity of radio resources from different radio access technologies (RATs) still exists. The heterogeneity, especially for networks with the coexistence of non-orthogonal and orthogonal resources, makes the radio resources difficult to be uniformly measured, and thus hinders the efficient utilization of radio resources. To overcome this limitation, this letter firstly proposes a radio resource virtualization approach in heterogeneous networks. Based on the accumulated historical data of resource utilization information, heterogeneous radio resources are virtualized into normalized resources using deep learning method. Secondly, the consumption difference of virtualized resources under different situations of network load and user demand is modeled. Moreover, aiming at efficiently utilizing radio resources and reducing access blocking rate, a RAT selection scheme based on the radio resource virtualization is proposed. Through simulation, the validity of the proposed scheme is evaluated.","","","10.1109/LCOMM.2017.2664808","China Postdoctoral Science Foundation; National Natural Science Foundation of China; Funds for Creative Research Groups of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843637","Heterogeneous wireless networks;radio resource virtualization;RAT selection;deep learning","Rats;Heterogeneous networks;Resource virtualization;Wireless networks;Virtualization;Neural networks","","","","3","10","","","","","IEEE","IEEE Journals"
"Identifying Surface BRDF From a Single 4-D Light Field Image via Deep Neural Network","F. Lu; L. He; S. You; X. Chen; Z. Hao","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, International Research Institute for Multidisciplinary Science, Beihang University, Beihang University, Beijing, Beijing, ChinaChina; School of Computer Science and Engineering, Beihang University, Beijing, China; Data61-CSIRO, Australian National University, Canberra, Canberra, ACT, ACT, AustraliaAustralia; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China","IEEE Journal of Selected Topics in Signal Processing","","2017","11","7","1047","1057","Bidirectional reflectance distribution function (BRDF) defines how light is reflected at a surface patch to produce the surface appearance, and thus, modeling/recognizing BRDFs is of great importance for various tasks in computer vision and graphics. However, such tasks are usually ill-posed or require heavy labor on image capture from different viewing angles. In this paper, we focus on the problem of remote BRDF type identification, by delivering novel techniques that capture and use a single light field image. The key is that a light field image captures both the spatial and angular information by a single shot, and the angular information enables effective samplings of the four-dimensional (4-D) BRDF. To implement the idea, we propose convolutional neural network based architectures for BRDF identification from a single 4-D light field image. Specifically, a StackNet and an Ang-convNet are introduced. The StackNet stacks the angular information of the light field images in an independent dimension, whereas the Ang-convNet uses angular filters to encode the angular information. In addition, we propose a large light field BRDF dataset containing 47 650 high-quality 4-D light field image patches, with different 3-D shapes, BRDFs, and illuminations. Experimental results show significant accuracy improvement in BRDF identification by using the proposed methods.","","","10.1109/JSTSP.2017.2728001","National Natural Science Foundation of China; Joint Funds of NSFC-CARFC; NSFC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7982615","BRDF;CNN;light field","Cameras;Distribution functions;Neural networks;Computer vision;Light field image processing;Light fields","computer graphics;computer vision;convolution;image classification;learning (artificial intelligence);lighting;neural nets","surface BRDF;deep neural network;bidirectional reflectance distribution function;surface patch;surface appearance;computer vision;graphics;image capture;remote BRDF type identification;single light field image;spatial information;angular information;single shot;BRDF identification;StackNet;Ang-convNet;angular filters;light field BRDF dataset;viewing angles;4D light field images;four-dimensional BRDF;4D BRDF;convolutional neural network based architecture","","1","45","Traditional","","","","IEEE","IEEE Journals"
"Being a Supercook: Joint Food Attributes and Multimodal Content Modeling for Recipe Retrieval and Exploration","W. Min; S. Jiang; J. Sang; H. Wang; X. Liu; L. Herranz","Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Multimedia","","2017","19","5","1100","1113","This paper considers the problem of recipe-oriented image-ingredient correlation learning with multi-attributes for recipe retrieval and exploration. Existing methods mainly focus on food visual information for recognition while we model visual information, textual content (e.g., ingredients), and attributes (e.g., cuisine and course) together to solve extended recipe-oriented problems, such as multimodal cuisine classification and attribute-enhanced food image retrieval. As a solution, we propose a multimodal multitask deep belief network (M3TDBN) to learn joint image-ingredient representation regularized by different attributes. By grouping ingredients into visible ingredients (which are visible in the food image, e.g., “chicken” and “mushroom”) and nonvisible ingredients (e.g., “salt” and “oil”), M3TDBN is capable of learning both midlevel visual representation between images and visible ingredients and nonvisual representation. Furthermore, in order to utilize different attributes to improve the intermodality correlation, M3TDBN incorporates multitask learning to make different attributes collaborate each other. Based on the proposed M3TDBN, we exploit the derived deep features and the discovered correlations for three extended novel applications: 1) multimodal cuisine classification; 2) attribute-augmented cross-modal recipe image retrieval; and 3) ingredient and attribute inference from food images. The proposed approach is evaluated on the constructed Yummly dataset and the evaluation results have validated the effectiveness of the proposed approach.","","","10.1109/TMM.2016.2639382","National Natural Science Foundation of China; National High Technology Research and Development 863 Program of China; Beijing Municipal Commission of Science and Technology; Lenovo Outstanding Young Scientists Program; National Program for Special Support of Eminent Professionals; National Program for Support of Top-notch Young Professionals; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782829","Cuisine classification;recipe image retrieval;ingredient inference;multitask deep belief network","Visualization;Correlation;Image recognition;Image retrieval;Oils;Solid modeling;Planning","belief networks;image representation;image retrieval","joint food attributes and multimodal content modeling;recipe-oriented image-ingredient correlation learning;recipe exploration;multimodal multitask deep belief network;joint image-ingredient representation;midlevel visual representation;visible ingredients;nonvisual representation;multimodal cuisine classification;attribute-augmented cross-modal recipe image retrieval;Yummly dataset","","10","52","","","","","IEEE","IEEE Journals"
"Spatiotemporal Joint Mitosis Detection Using CNN-LSTM Network in Time-Lapse Phase Contrast Microscopy Images","Y. Su; Y. Lu; M. Chen; A. Liu","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Department of Electrical and Computer Engineering, State University of New York at Albany, Albany, NY, USA; School of Electrical and Information Engineering, Tianjin University, Tianjin, China","IEEE Access","","2017","5","","18033","18041","We present an approach to jointly detect mitotic events spatially and temporally in time-lapse phase contrast microscopy images. In particular, we combine a convolutional neural network (CNN) and a long short-term memory (LSTM) network to detect mitotic events in patch sequences. The CNN-LSTM network can be trained end-to-end to simultaneously learn convolutional features within each frame and temporal dynamics between frames, without hand-crafted visual or temporal feature design. Owing to the LSTM layer, this approach is able to detect mitotic events in patch sequences of variable length, as well as making use of longer context information among frames in the sequences. To the best of our knowledge, this is the first work to detect mitosis using deep learning in both spatial and temporal domains. Experiments have shown that the CNN-LSTM network can be trained efficiently, and we evaluate this design by applying the network to original raw microscopy image sequences to locate mitotic events both spatially and temporally. The data with which we validate the proposed method include C3H10 mesenchymal and C2C12 myoblastic stem cell populations. Our approach achieved the F score of 98.72% on the C2C12 data set, and the F score of 96.5% on the C3H10 data set. The results on both data sets outperform the traditional graph modelbased approaches by a large margin, both in terms of detection accuracy and frame localization accuracy. Furthermore, we have developed a framework to aid humans in annotating mitosis with high efficiency and accuracy in raw phase contrast microscopy images based on the joint detection results using the proposed method. Under this framework, expert level annotations can be obtained in raw phase contrast microscopy image sequences, and the annotations have shown to further improve the training performance of the CNN-LSTM network.","","","10.1109/ACCESS.2017.2745544","National Natural Science Foundation of China; Tianjin Research Program of Application Foundation and Advanced Technology; China Scholarship Council; Elite Scholar Program of Tianjin University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019789","Biomedical imaging;computer vision;mitosis detection;machine learning;stem cell","Feature extraction;Microscopy;Machine learning;Hidden Markov models;Computer architecture;Spatiotemporal phenomena;Visualization","biological techniques;biology computing;biomedical optical imaging;computer vision;feature extraction;image sequences;learning (artificial intelligence);medical image processing;neural nets;optical microscopy","temporal feature design;mitotic events;patch sequences;spatial domains;temporal domains;CNN-LSTM network;original raw microscopy image sequences;detection accuracy;raw phase contrast microscopy images;joint detection results;raw phase contrast microscopy image sequences;spatiotemporal joint mitosis detection;time-lapse phase contrast microscopy images;convolutional neural network;short-term memory network;visual feature design;F;C<sub>3</sub>H<sub>10</sub>;C<sub>2</sub>C<sub>12</sub>","","9","26","OAPA","","","","IEEE","IEEE Journals"
"Regularized Speaker Adaptation of KL-HMM for Dysarthric Speech Recognition","M. Kim; Y. Kim; J. Yoo; J. Wang; H. Kim","Department of Bioengineering, University of Texas at Dallas, Richardson, TX, USA; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Bioengineering, University of Texas at Dallas, Richardson, TX, USA; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","2017","25","9","1581","1591","This paper addresses the problem of recognizing the speech uttered by patients with dysarthria, which is a motor speech disorder impeding the physical production of speech. Patients with dysarthria have articulatory limitation, and therefore, they often have trouble in pronouncing certain sounds, resulting in undesirable phonetic variation. Modern automatic speech recognition systems designed for regular speakers are ineffective for dysarthric sufferers due to the phonetic variation. To capture the phonetic variation, Kullback-Leibler divergence-based hidden Markov model (KL-HMM) is adopted, where the emission probability of state is parameterized by a categorical distribution using phoneme posterior probabilities obtained from a deep neural network-based acoustic model. To further reflect speaker-specific phonetic variation patterns, a speaker adaptation method based on a combination of L2 regularization and confusion-reducing regularization, which can enhance discriminability between categorical distributions of the KL-HMM states while preserving speaker-specific information is proposed. Evaluation of the proposed speaker adaptation method on a database of several hundred words for 30 speakers consisting of 12 mildly dysarthric, 8 moderately dysarthric, and 10 non-dysarthric control speakers showed that the proposed approach significantly outperformed the conventional deep neural network-based speaker adapted system on dysarthric as well as non-dysarthric speech.","","","10.1109/TNSRE.2017.2681691","National Research Foundation of Korea; National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876719","Dysarthria;speech recognition;speaker adaptation;KL-HMM;regularization","Hidden Markov models;Speech;Adaptation models;Speech recognition;Acoustics;Computational modeling;Silicon","acoustic signal processing;bioacoustics;hidden Markov models;medical disorders;medical signal processing;neural nets;probability;speaker recognition;speech;speech processing","regularized speaker adaptation;dysarthric speech recognition;motor speech disorder;physical production;articulatory limitation;undesirable phonetic variation;modern automatic speech recognition systems;Kullback-Leibler divergence-based hidden Markov model;emission probability;categorical distribution;phoneme posterior probabilities;deep neural network-based acoustic model;speaker-specific phonetic variation patterns;speaker adaptation method;L2 regularization;confusion-reducing regularization;categorical distributions;KL-HMM states;database;nondysarthric control speakers;conventional deep neural network-based speaker adapted system","Adult;Algorithms;Communication Aids for Disabled;Computer Simulation;Dysarthria;Female;Humans;Machine Learning;Male;Markov Chains;Models, Statistical;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Speech Production Measurement","3","50","","","","","IEEE","IEEE Journals"
"Multilevel Contextual 3-D CNNs for False Positive Reduction in Pulmonary Nodule Detection","Q. Dou; H. Chen; L. Yu; J. Qin; P. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Centre for Smart Health, School of NursingThe Hong Kong Polytechnic University; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Biomedical Engineering","","2017","64","7","1558","1567","Objective: False positive reduction is one of the most crucial components in an automated pulmonary nodule detection system, which plays an important role in lung cancer diagnosis and early treatment. The objective of this paper is to effectively address the challenges in this task and therefore to accurately discriminate the true nodules from a large number of candidates. Methods: We propose a novel method employing three-dimensional (3-D) convolutional neural networks (CNNs) for false positive reduction in automated pulmonary nodule detection from volumetric computed tomography (CT) scans. Compared with its 2-D counterparts, the 3-D CNNs can encode richer spatial information and extract more representative features via their hierarchical architecture trained with 3-D samples. More importantly, we further propose a simple yet effective strategy to encode multilevel contextual information to meet the challenges coming with the large variations and hard mimics of pulmonary nodules. Results: The proposed framework has been extensively validated in the LUNA16 challenge held in conjunction with ISBI 2016, where we achieved the highest competition performance metric (CPM) score in the false positive reduction track. Conclusion: Experimental results demonstrated the importance and effectiveness of integrating multilevel contextual information into 3-D CNN framework for automated pulmonary nodule detection in volumetric CT data. Significance: While our method is tailored for pulmonary nodule detection, the proposed framework is general and can be easily extended to many other 3-D object detection tasks from volumetric medical images, where the targeting objects have large variations and are accompanied by a number of hard mimics.","","","10.1109/TBME.2016.2613502","The Hong Kong Special Administrative Region; National Natural Science Foundation of China; Shenzhen-Hong Kong Innovation Circle; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7576695","Computer-aided diagnosis;deep learning;false positive reduction;pulmonary nodule detection;3-D convolutional neural networks","Three-dimensional displays;Feature extraction;Two dimensional displays;Computed tomography;Kernel;Cancer;Lungs","cancer;computerised tomography;feature extraction;lung;medical image processing;neural nets","automated pulmonary nodule detection system;lung cancer diagnosis;3D convolutional neural network;contextual 3D CNN;volumetric computed tomography;volumetric CT scans;feature extraction;LUNA16 challenge;ISBI 2016;competition performance metric score;CPM score;false positive reduction track;multilevel contextual information;3D object detection tasks;volumetric medical images","Algorithms;Diagnostic Errors;False Positive Reactions;Humans;Imaging, Three-Dimensional;Machine Learning;Neural Networks (Computer);Pattern Recognition, Automated;Radiographic Image Interpretation, Computer-Assisted;Reproducibility of Results;Sensitivity and Specificity;Solitary Pulmonary Nodule;Tomography, X-Ray Computed","114","37","","","","","IEEE","IEEE Journals"
"STC: A Simple to Complex Framework for Weakly-Supervised Semantic Segmentation","Y. Wei; X. Liang; Y. Chen; X. Shen; M. Cheng; J. Feng; Y. Zhao; S. Yan","Institute of Information Science, Beijing Jiaotong University, Beijing, China; Sun Yatsen University, Guangdong Sheng, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Adobe Research, San Jose, CA; CCCE, Nankai University, Tianjin, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","11","2314","2320","Recently, significant improvement has been made on semantic object segmentation due to the development of deep convolutional neural networks (DCNNs). Training such a DCNN usually relies on a large number of images with pixel-level segmentation masks, and annotating these images is very costly in terms of both finance and human effort. In this paper, we propose a simple to complex (STC) framework in which only image-level annotations are utilized to learn DCNNs for semantic segmentation. Specifically, we first train an initial segmentation network called Initial-DCNN with the saliency maps of simple images (i.e., those with a single category of major object(s) and clean background). These saliency maps can be automatically obtained by existing bottom-up salient object detection techniques, where no supervision information is needed. Then, a better network called Enhanced-DCNN is learned with supervision from the predicted segmentation masks of simple images based on the Initial-DCNN as well as the image-level annotations. Finally, more pixel-level segmentation masks of complex images (two or more categories of objects with cluttered background), which are inferred by using Enhanced-DCNN and image-level annotations, are utilized as the supervision information to learn the Powerful-DCNN for semantic segmentation. Our method utilizes 40K simple images from Flickr.com and 10K complex images from PASCAL VOC for step-wisely boosting the segmentation network. Extensive experimental results on PASCAL VOC 2012 segmentation benchmark well demonstrate the superiority of the proposed STC framework compared with other state-of-the-arts.","","","10.1109/TPAMI.2016.2636150","National Key Research and Development of China; National Natural Science Foundation of China; CAST young talents plan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7775087","Semantic segmentation;weakly-supervised learning;convolutional neural network","Image segmentation;Semantics;Training;Electronic mail;Object detection;Neural networks;Benchmark testing","image classification;image representation;image segmentation;learning (artificial intelligence);neural nets;object detection","weakly-supervised semantic segmentation;semantic object segmentation;deep convolutional neural networks;pixel-level segmentation masks;image-level annotations;initial segmentation network;saliency maps;simple images;supervision information;Enhanced-DCNN;predicted segmentation masks;complex images;PASCAL VOC 2012 segmentation benchmark;STC framework;DCNN;simple to complex framework;bottom-up salient object detection techniques","","43","47","Traditional","","","","IEEE","IEEE Journals"
"High-Resolution Aerial Image Labeling With Convolutional Neural Networks","E. Maggiori; Y. Tarabalka; G. Charpiat; P. Alliez","Universit&#x00E9; C&#x00F4;te d&#x2019;Azur, Campus Valrose, Nice Cedex, France; Universit&#x00E9; C&#x00F4;te d&#x2019;Azur, Campus Valrose, Nice Cedex, France; Inria Saclay-le-de-France, Palaiseau, France; Universit&#x00E9; C&#x00F4;te d&#x2019;Azur, Campus Valrose, Nice Cedex, France","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","12","7092","7103","The problem of dense semantic labeling consists in assigning semantic labels to every pixel in an image. In the context of aerial image analysis, it is particularly important to yield high-resolution outputs. In order to use convolutional neural networks (CNNs) for this task, it is required to design new specific architectures to provide fine-grained classification maps. Many dense semantic labeling CNNs have been recently proposed. Our first contribution is an in-depth analysis of these architectures. We establish the desired properties of an ideal semantic labeling CNN, and assess how those methods stand with regard to these properties. We observe that even though they provide competitive results, these CNNs often underexploit properties of semantic labeling that could lead to more effective and efficient architectures. Out of these observations, we then derive a CNN framework specifically adapted to the semantic labeling problem. In addition to learning features at different resolutions, it learns how to combine these features. By integrating local and global information in an efficient and flexible manner, it outperforms previous techniques. We evaluate the proposed framework and compare it with state-of-the-art architectures on public benchmarks of high-resolution aerial image labeling.","","","10.1109/TGRS.2017.2740362","Centre National d’Études Spatiales; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8024178","Convolutional neural networks (CNNs);deep learning;high-resolution aerial imagery;semantic labeling","Semantics;Computer architecture;Image analysis;Convolutional neural networks;Convolutional codes","image classification;image resolution;learning (artificial intelligence);neural nets","aerial image analysis;high-resolution outputs;convolutional neural networks;fine-grained classification maps;dense semantic labeling CNNs;ideal semantic labeling CNN;semantic labeling problem;high-resolution aerial image labeling","","13","34","","","","","IEEE","IEEE Journals"
"Diversified Visual Attention Networks for Fine-Grained Object Classification","B. Zhao; X. Wu; J. Feng; Q. Peng; S. Yan","School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Multimedia","","2017","19","6","1245","1256","Fine-grained object classification attracts increasing attention in multimedia applications. However, it is a quite challenging problem due to the subtle interclass difference and large intraclass variation. Recently, visual attention models have been applied to automatically localize the discriminative regions of an image for better capturing critical difference, which have demonstrated promising performance. Unfortunately, without consideration of the diversity in attention process, most of existing attention models perform poorly in classifying fine-grained objects. In this paper, we propose a diversified visual attention network (DVAN) to address the problem of fine-grained object classification, which substantially relieves the dependency on strongly supervised information for learning to localize discriminative regions compared with attention-less models. More importantly, DVAN explicitly pursues the diversity of attention and is able to gather discriminative information to the maximal extent. Multiple attention canvases are generated to extract convolutional features for attention. An LSTM recurrent unit is employed to learn the attentiveness and discrimination of attention canvases. The proposed DVAN has the ability to attend the object from coarse to fine granularity, and a dynamic internal representation for classification is built up by incrementally combining the information from different locations and scales of the image. Extensive experiments conducted on CUB-2011, Stanford Dogs, and Stanford Cars datasets have demonstrated that the pro-posed DVAN achieves competitive performance compared to the state-of-the-art approaches, without using any prior knowledge, user interaction, or external resource in training and testing.","","","10.1109/TMM.2017.2648498","National Natural Science Foundation of China; Program for Sichuan Provincial Science Fund for Distinguished Young Scholars; Fundamental Research Funds for the Central Universities; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7807286","Deep learning;fine-grained object classification;long-short-term-memory (LSTM);visual attention","Visualization;Feature extraction;Birds;Predictive models;Diversity reception;Dogs;Training","computer vision;feature extraction;image capture;image classification;learning (artificial intelligence)","fine-grained object classification;multimedia applications;DVAN;LSTM recurrent unit;feature extraction;dynamic internal representation;CUB-2011;Stanford Cars datasets;Stanford Dogs datasets;diversified visual attention networks","","59","55","","","","","IEEE","IEEE Journals"
"Visual Tracking With Convolutional Random Vector Functional Link Network","L. Zhang; P. N. Suganthan","Advanced Digital Science Center, Singapore; Department of Electrical and Computer Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Cybernetics","","2017","47","10","3243","3253","Deep neural network-based methods have recently achieved excellent performance in visual tracking task. As very few training samples are available in visual tracking task, those approaches rely heavily on extremely large auxiliary dataset such as ImageNet to pretrain the model. In order to address the discrepancy between the source domain (the auxiliary data) and the target domain (the object being tracked), they need to be finetuned during the tracking process. However, those methods suffer from sensitivity to the hyper-parameters such as learning rate, maximum number of epochs, size of mini-batch, and so on. Thus, it is worthy to investigate whether pretraining and fine tuning through conventional back-prop is essential for visual tracking. In this paper, we shed light on this line of research by proposing convolutional random vector functional link (CRVFL) neural network, which can be regarded as a marriage of the convolutional neural network and random vector functional link network, to simplify the visual tracking system. The parameters in the convolutional layer are randomly initialized and kept fixed. Only the parameters in the fully connected layer need to be learned. We further propose an elegant approach to update the tracker. In the widely used visual tracking benchmark, without any auxiliary data, a single CRVFL model achieves 79.0% with a threshold of 20 pixels for the precision plot. Moreover, an ensemble of CRVFL yields comparatively the best result of 86.3%.","","","10.1109/TCYB.2016.2588526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7543468","Convolutional neural network (CNN);convolutional random vector functional link (CRVFL);deep learning;random vector functional link (RVFL);visual tracking","Visualization;Target tracking;Biological neural networks;Training;Data models","convolution;neural nets;object tracking","visual tracking;convolutional random vector functional link neural network;deep neural network;CRVFL neural network;convolutional layer","","11","79","","","","","IEEE","IEEE Journals"
"Design of robust adaptive controller and feedback error learning for rehabilitation in Parkinson's disease: a simulation study","K. Rouhollahi; M. Emadi Andani; S. M. Karbassi; I. Izadi","Yazd University, Iran; University of Isfahan, Iran; Yazd University, Iran; Isfahan University of Technology, Iran","IET Systems Biology","","2017","11","1","19","29","Deep brain stimulation (DBS) is an efficient therapy to control movement disorders of Parkinson's tremor. Stimulation of one area of basal ganglia (BG) by DBS with no feedback is the prevalent opinion. Reduction of additional stimulatory signal delivered to the brain is the advantage of using feedback. This results in reduction of side effects caused by the excessive stimulation intensity. In fact, the stimulatory intensity of controllers is decreased proportional to reduction of hand tremor. The objective of this study is to design a new controller structure to decrease three indicators: (i) the hand tremor; (ii) the level of delivered stimulation in disease condition; and (iii) the ratio of the level of delivered stimulation in health condition to disease condition. For this purpose, the authors offer a new closed-loop control structure to stimulate two areas of BG simultaneously. One area (STN: subthalamic nucleus) is stimulated by an adaptive controller with feedback error learning. The other area (GPi: globus pallidus internal) is stimulated by a partial state feedback (PSF) controller. Considering the three indicators, the results show that, stimulating two areas simultaneously leads to better performance compared with stimulating one area only. It is shown that both PSF and adaptive controllers are robust regarding system parameter uncertainties. In addition, a method is proposed to update the parameters of the BG model in real time. As a result, the parameters of the controllers can be updated based on the new parameters of the BG model.","","","10.1049/iet-syb.2016.0014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850392","","","adaptive control;diseases;medical control systems;medical disorders;neurophysiology;patient rehabilitation;surgery","BG model;robust regarding system parameter uncertainties;partial state feedback controller;adaptive controller;subthalamic nucleus;closed-loop control structure;controller structure;hand tremor;excessive stimulation intensity;side effects;stimulatory signal Reduction;Parkinson's tremor;movement disorders;DBS;deep brain stimulation;Parkinson's disease rehabilitation;feedback error learning;robust adaptive controller design","","4","47","","","","","IET","IET Journals"
"Deep Learning with Dynamic Spiking Neurons and Fixed Feedback Weights","A. Samadi; T. P. Lillicrap; D. B. Tweed","Department of Physiology, University of Toronto, Toronto, Ontario, M5S 1A8, Canada ars2023@med.cornell.edu; Google DeepMind, London, EC4A 3TW, U.K. timothylillicrap@google.com; Department of Physiology, University of Toronto, Toronto, Ontario, M5S 1A8, Canada, and Centre for Vision Research, York University, Toronto, Ontario, M3J 1PC, Canada douglas.tweed@utoronto.ca","Neural Computation","","2017","29","3","578","602","Recent work in computer science has shown the power of deep learning driven by the backpropagation algorithm in networks of artificial neurons. But real neurons in the brain are different from most of these artificial ones in at least three crucial ways: they emit spikes rather than graded outputs, their inputs and outputs are related dynamically rather than by piecewise-smooth functions, and they have no known way to coordinate arrays of synapses in separate forward and feedback pathways so that they change simultaneously and identically, as they do in backpropagation. Given these differences, it is unlikely that current deep learning algorithms can operate in the brain, but we that show these problems can be solved by two simple devices: learning rules can approximate dynamic input-output relations with piecewise-smooth functions, and a variation on the feedback alignment algorithm can train deep networks without having to coordinate forward and feedback synapses. Our results also show that deep spiking networks learn much better if each neuron computes an intracellular teaching signal that reflects that cell’s nonlinearity. With this mechanism, networks of spiking neurons show useful learning in synapses at least nine layers upstream from the output cells and perform well compared to other spiking networks in the literature on the MNIST digit recognition task.","","","10.1162/NECO_a_00929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7864507","","","","","","2","","","","","","MITP",""
"Dimensionality Reduction for Hyperspectral Data Based on Sample-Dependent Repulsion Graph Regularized Auto-encoder","X. Wang; Y. Kong; Y. Cheng","China University of Mining and Technology, China; China University of Mining and Technology, China; China University of Mining and Technology, China","Chinese Journal of Electronics","","2017","26","6","1233","1238","To achieve high classification accuracy of hyperspectral data, a dimensionality reduction algorithm called Sample-dependent repulsion graph regularized auto-encoder (SRGAE) is proposed. Based on the sample-dependent graph, by applying the repulsion force to the samples from different classes but nearby, a sample-dependent repulsion graph is built to make the samples from the same class will be projected to samples that are close-by and the samples from different classes will be projected to samples that are far away. The sample-dependent repulsion graph can avoid the neighborhood parameter selection problem existing in the nearest neighborhood graph. By integrating advantages of deep learning and graph regularization technique, the SRGAE can maintain the learned deep features are consistent with the inherent manifold structure of the original hyperspectral data. Experimental results on two real hyperspectral data show that, when compared with some popular dimensionality reduction algorithms, the proposed SRGAE can yield higher classification accuracy.","","","10.1049/cje.2017.07.012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128700","","","encoding;graph theory;hyperspectral imaging;image classification;learning (artificial intelligence);sampling methods","dimensionality reduction;hyperspectral data classification;sample-dependent repulsion graph regularized autoencoder;SRGAE;deep learning;hyperspectral image","","","","","","","","IET","IET Journals"
"G-CNN: Object Detection via Grid Convolutional Neural Network","Q. Lu; C. Liu; Z. Jiang; A. Men; B. Yang","Multimedia Technology Center, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; China Academy of Space Technology, Beijing, China; Multimedia Technology Center, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Multimedia Technology Center, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Multimedia Technology Center, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Access","","2017","5","","24023","24031","We propose an object detection system that depends on position-sensitive grid feature maps. State-of-the-art object detection networks rely on convolutional neural networks pre-trained on a large auxiliary data set (e.g., ILSVRC 2012) designed for an image-level classification task. The imagelevel classification task favors translation invariance, while the object detection task needs localization representations that are translation variant to an extent. To address this dilemma, we construct position-sensitive convolutional layers, called grid convolutional layers that activate the object's specific locations in the feature maps in the form of grids. With end-to-end training, the region of interesting grid pooling layer shepherds the last set of convolutional layers to learn specialized grid feature maps. Experiments on the PASCAL VOC 2007 data set show that our method outperforms the strong baselines faster region-based convolutional neural network counterpart and region-based fully convolutional networks by a large margin. Our method applied to ResNet-50 improves the mean average precision from 74.8%/74.2% to 79.4% without any other tricks. In addition, our approach achieves similar results on different networks (ResNet-101) and data sets (PASCAL VOC 2012 and MS COCO).","","","10.1109/ACCESS.2017.2770178","Project of National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8097417","Computer vision;deep learning;grid feature map;object detection;region proposal","Object detection;Proposals;Training;Feature extraction;Neural networks;Pipelines","feature extraction;feedforward neural nets;image classification;image recognition;image representation;learning (artificial intelligence);neural nets;object detection","specialized grid feature maps;fully convolutional networks;PASCAL VOC 2012;grid convolutional neural network;object detection system;position-sensitive grid feature maps;auxiliary data set;image-level classification task;translation invariance;object detection task;localization representations;translation variant;position-sensitive convolutional layers;convolutional neural network;G-CNN;PASCAL VOC 2007 data set;ILSVRC 2012 data set","","9","41","","","","","IEEE","IEEE Journals"
"Sea Ice Classification Using Cryosat-2 Altimeter Data by Optimal Classifier–Feature Assembly","X. Shen; J. Zhang; X. Zhang; J. Meng; C. Ke","Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Collaborative Innovation Center of South China Sea Studies, Nanjing University, Nanjing, China; State Oceanic Administration, First Institute of Oceanography, Qingdao, China; State Oceanic Administration, First Institute of Oceanography, Qingdao, China; State Oceanic Administration, First Institute of Oceanography, Qingdao, China; Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, Collaborative Innovation Center of South China Sea Studies, Nanjing University, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","11","1948","1952","Sea ice type is one of the most sensitive variables in Arctic ice monitoring and detailed information about it is essential for ice situation evaluation, vessel navigation, and climate prediction. Many machine-learning methods including deep learning can be employed for ice-type detection, and most classifiers tend to prefer different feature combinations. In order to find the optimal classifier-feature assembly (OCF) for sea ice classification, it is necessary to assess their performance differences. The objective of this letter is to make a recommendation for the OCF for sea ice classification using Cryosat-2 (CS-2) data. Six classifiers including convolutional neural network (CNN), Bayesian, K nearest-neighbor (KNN), support vector machine (SVM), random forest (RF), and back propagation neural network (BPNN) were studied. CS-2 altimeter data of November 2015 and May 2016 in the whole Arctic were used. The overall accuracy was estimated using multivalidation to evaluate the performances of individual classifiers with different feature combinations. Overall, RF achieved a mean accuracy of 89.15%, followed by Bayesian, SVM, and BPNN (~86%), outperforming the worst (CNN and KNN) by 7%. Trailing-edge width (TeW) and leading-edge width (LeW) were the most important features, and feature combination of TeW, LeW, Sigma0, maximum of the returned power waveform (MAX), and pulse peakiness (PP) was the best choice. RF with feature combination of TeW, LeW, Sigma0, MAX, and PP was finally selected as the OCF for sea ice classification and the results that demonstrated this method achieved a mean accuracy of 91.45%, which outperformed the other state-of-art methods by 9%.","","","10.1109/LGRS.2017.2743339","National Key Research and Development Program of China; Research and Development Special Foundation for Public Welfare Industry; National Nature Science Foundation of China; Ministry of Science and Technology of China; European Space Agency through the Dragon-4 Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8027206","Altimeter waveform;classification;Cryosat-2 (CS-2);machine learning;sea ice type","Sea ice;Support vector machines;Feature extraction;Sea surface;Arctic;Radio frequency","altimeters;backpropagation;feature extraction;geophysical image processing;image classification;learning (artificial intelligence);neural nets;oceanographic regions;radar altimetry;remote sensing by radar;sea ice;support vector machines","machine-learning method;convolutional neural network;CNN classifier;Bayesian classifier;K-nearest-neighbor;KNN classifier;support vector machine;SVM classifier;random forest;back propagation neural network;BPNN classifier;AD 2015 11;AD 2016 05;CS-2 altimeter data;ice-type detection;ice situation evaluation;Arctic ice monitoring;sea ice type;optimal classifier-feature assembly;Cryosat-2 altimeter data;sea ice classification","","2","15","Traditional","","","","IEEE","IEEE Journals"
"End-to-End Comparative Attention Networks for Person Re-Identification","H. Liu; J. Feng; M. Qi; J. Jiang; S. Yan","School of Computer and Information, Hefei University of Technology, Hefei, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Computer and Information, Hefei University of Technology, Hefei, China; School of Computer and Information, Hefei University of Technology, Hefei, China; Qihoo 360 Technology Company, Ltd., Artificial Intelligence Institute, Beijing, China","IEEE Transactions on Image Processing","","2017","26","7","3492","3506","Person re-identification across disjoint camera views has been widely applied in video surveillance yet it is still a challenging problem. One of the major challenges lies in the lack of spatial and temporal cues, which makes it difficult to deal with large variations of lighting conditions, viewing angles, body poses, and occlusions. Recently, several deep-learning-based person re-identification approaches have been proposed and achieved remarkable performance. However, most of those approaches extract discriminative features from the whole frame at one glimpse without differentiating various parts of the persons to identify. It is essentially important to examine multiple highly discriminative local regions of the person images in details through multiple glimpses for dealing with the large appearance variance. In this paper, we propose a new soft attention-based model, i.e., the end-to-end comparative attention network (CAN), specifically tailored for the task of person re-identification. The end-to-end CAN learns to selectively focus on parts of pairs of person images after taking a few glimpses of them and adaptively comparing their appearance. The CAN model is able to learn which parts of images are relevant for discerning persons and automatically integrates information from different parts to determine whether a pair of images belongs to the same person. In other words, our proposed CAN model simulates the human perception process to verify whether two images are from the same person. Extensive experiments on four benchmark person re-identification data sets, including CUHK01, CHUHK03, Market-1501, and VIPeR, clearly demonstrate that our proposed end-to-end CAN for person re-identification outperforms well established baselines significantly and offer the new state-of-the-art performance.","","","10.1109/TIP.2017.2700762","National Natural Science Foundation of China; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918589","Person re-identification;comparative attention network;multiple glimpses","Feature extraction;Measurement;Cameras;Adaptation models;Lighting;Robustness;Benchmark testing","feature extraction;learning (artificial intelligence);video surveillance","soft attention-based model;CAN model;feature extraction;deep-learning-based person re-identification approaches;video surveillance;end-to-end comparative attention networks","","125","55","","","","","IEEE","IEEE Journals"
"Online Variable Coding Length Product Quantization for Fast Nearest Neighbor Search in Mobile Retrieval","J. Li; X. Lan; X. Li; J. Wang; N. Zheng; Y. Wu","Xi'an Jiaotong University, Xi'an, China; Xi'an Jiaotong University, Xi'an, China; Xi'an Jiaotong University, Xi'an, China; Baidu Institute of Deep Learning, Sunnyvale, CA, USA; Department of Automation Science and Technology, Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China; Northwestern University, Evanston, IL, USA","IEEE Transactions on Multimedia","","2017","19","3","559","570","Quantization methods are crucial for efficient nearest neighbor search in many applications such as image, music, or product search. As mobile devices are becoming increasingly more popular, the quantization methods on mobile devices are more important, because a large portion of the search queries are becoming performed on mobile devices. One important characteristic of the communication on mobile devices is the inherent unreliability of their communication channels. In order to adapt the quality changes of the communication channels, we need to change the coding length of the quantization accordingly. The existing quantization methods use fixed-length codebooks, and it is expensive to retrain another codebook with different coding length. In this paper, we propose a novel variable length product quantization framework that consists of a set of fast universal scalar quantizers. The framework is capable of producing variable length quantization without retraining the codebook. Each data vector is transformed into a new space to reduce the correlation across dimensions. A proper number of bits is allocated to represent the scalar component in each dimension according to the given coding length. For each component, we estimate its probability density function (PDF) and design an efficient universal scalar quantizer based on the PDF and the allocated bits. To reduce distortion, we learn a Gaussian mixture model for the data. The experimental results show that, compared to state-of-the-art product quantization methods, our approach can construct the codebooks online for variable coding lengths and achieve the comparable performance.","","","10.1109/TMM.2016.2617089","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589007","Approximate nearest neighbor search;Gaussian mixture model;low-complexity quantizer design;variable coding length","Quantization (signal);Encoding;Bit rate;Training;Probability density function;Distortion;Mobile communication","Gaussian processes;mixture models;mobile computing;probability;quantisation (signal);query processing;variable length codes","online variable coding length product quantization;fast nearest neighbor search;mobile retrieval;mobile devices;search query;communication channels;fixed-length codebooks;fast universal scalar quantizers;data vector;correlation reduction;scalar component;probability density function;PDF;efficient universal scalar quantizer;distortion reduction;Gaussian mixture model","","3","53","","","","","IEEE","IEEE Journals"
"CSI-Based Device-Free Wireless Localization and Activity Recognition Using Radio Image Features","Q. Gao; J. Wang; X. Ma; X. Feng; H. Wang","Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Vehicular Technology","","2017","66","11","10346","10356","Device-free wireless localization and activity recognition is an emerging technique, which could estimate the location and activity of a person without equipping him/her with any device. It deduces the state of a person by analyzing his/her influence on surrounding wireless signals. Therefore, how to characterize the influence of human behaviors is the key question. In this paper, we explore and exploit a radio image processing approach to better characterize the influence of human behaviors on Wi-Fi signals. Traditional methods deal with channel state information (CSI) measurements on each channel independently. However, CSI measurements on different channels are correlated, and thus lots of useful information involved with channel correlation may be lost. This motivates us to look on CSI measurements from multiple channels as a radio image and deal with it from the two-dimensional perspective. Specifically, we transform CSI measurements from multiple channels into a radio image, extract color and texture features from the radio image, adopt a deep learning network to learn optimized deep features from image features, and estimate the location and activity of a person using a machine learning approach. Benefits from the informative and discriminative deep image features and experimental results in two clutter laboratories confirm the excellent performance of the proposed system.","","","10.1109/TVT.2017.2737553","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Scientific Research Staring Foundation; Returned Overseas Chinese Scholars; Xinghai Scholars Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004463","Activity recognition;device-free;feature extraction;localization;radio image","Feature extraction;Wireless communication;Wireless sensor networks;Wireless fidelity;Machine learning;Activity recognition","feature extraction;image colour analysis;image recognition;image texture;indoor radio;learning (artificial intelligence)","image features;radio image processing;wireless signals;radio image features;activity recognition;device-free wireless localization;channel state information measurements;human behaviors","","17","38","Traditional","","","","IEEE","IEEE Journals"
"Photo Aesthetics Analysis via DCNN Feature Encoding","H. Lee; K. Hong; H. Kang; S. Lee","Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea; Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea; Department of Mathematics and Computer Science, University of Missouri, St. Louis, MO, USA; Department of Computer Science and Engineering, Pohang University of Science and Technology, Pohang, South Korea","IEEE Transactions on Multimedia","","2017","19","8","1921","1932","We propose an automatic framework for quality assessment of a photograph as well as analysis of its aesthetic attributes. In contrast to the previous methods that rely on manually designed features to account for photo aesthetics, our method automatically extracts such features using a pretrained deep convolutional neural network (DCNN). To make the DCNN-extracted features more suited to our target tasks of photo quality assessment and aesthetic attribute analysis, we propose a novel feature encoding scheme, which supports vector machines-driven sparse restricted Boltzmann machines, which enhances sparseness of features and discrimination between target classes. Experimental results show that our method outperforms the current state-of-the-art methods in automatic photo quality assessment, and gives aesthetic attribute ratings that can be used for photo editing. We demonstrate that our feature encoding scheme can also be applied to general object classification task to achieve performance gains.","","","10.1109/TMM.2017.2687759","Institute for Information and Communications Technology Promotion; National Research Foundation of Korea; Korea Government (MSIP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886320","Aesthetic attributes;deep convolutional neural network (DCNN);feature encoding;photo aesthetics;restricted Boltzmann machines","Feature extraction;Quality assessment;Encoding;Neural networks;Training;Mathematical model;Support vector machines","Boltzmann machines;convolution;feature extraction;image classification;image coding;learning (artificial intelligence);support vector machines","object classification;sparse restricted Boltzmann machines;support vector machines;pretrained deep convolutional neural network;feature extraction;photograph quality assessment;DCNN feature encoding;photo aesthetics analysis","","4","45","","","","","IEEE","IEEE Journals"
"${k}$ -Sparse Autoencoder-Based Automatic Modulation Classification With Low Complexity","A. Ali; F. Yangyu","School of Electronics and Information Engineering, Northwestern Polytechnical University, Xian, China; School of Electronics and Information Engineering, Northwestern Polytechnical University, Xian, China","IEEE Communications Letters","","2017","21","10","2162","2165","How to reduce complexity of the practical automatic modulation classification systems is a very active research area. Moreover, keeping the classification accuracy to a near optimal level is an added challenge. Recently, three new classifiers have been proposed with reduced complexity, mainly: linear support vector machine classifier, approximate maximum likelihood classifier, and backpropogation neural networks classifier. However, these methods include the sorting process of the features z to form an ordered vector z⃗ employing K log(K) comparison operations. Here, we propose a k-sparse autoencoder-based classifier, with unsorted input data features and called it unsorted deep neural network (UDNN). Thus, we strive to omit the K log(K) comparison operations. The results obtained using the UDNN classifier show improved performance when compared with the above three methods. Moreover, using k highest hidden units to reconstruct input data further reduces the overall complexity of the AMC system.","","","10.1109/LCOMM.2017.2717821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954617","Automatic modulation classification;deep neural network;k-sparse autoencoders","Complexity theory;Sorting;Modulation;Neural networks;Feature extraction;Training;Image reconstruction","backpropagation;feature extraction;learning (artificial intelligence);maximum likelihood estimation;modulation;neural nets;pattern classification;support vector machines","unsorted deep neural network;UDNN classifier;AMC system;automatic modulation classification systems;ordered vector;k-sparse autoencoder;sorting process;backpropogation neural networks classifier;approximate maximum likelihood classifier;linear support vector machine classifier","","3","9","Traditional","","","","IEEE","IEEE Journals"
"Convolutional neural networks for time series classification","B. Zhao; H. Lu; S. Chen; J. Liu; D. Wu","College of Electronic Science and Engineering, National University of Defense Technology, Changsha 410073, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha 410073, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha 410073, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha 410073, China; College of Electronic Science and Engineering, National University of Defense Technology, Changsha 410073, China","Journal of Systems Engineering and Electronics","","2017","28","1","162","169","Time series classification is an important task in time series data mining, and has attracted great interests and tremendous efforts during last decades. However, it remains a challenging problem due to the nature of time series data: high dimensionality, large in data size and updating continuously. The deep learning techniques are explored to improve the performance of traditional feature-based approaches. Specifically, a novel convolutional neural network (CNN) framework is proposed for time series classification. Different from other feature-based classification approaches, CNN can discover and extract the suitable internal structure to generate deep features of the input time series automatically by using convolution and pooling operations. Two groups of experiments are conducted on simulated data sets and eight groups of experiments are conducted on real-world data sets from different application domains. The final experimental results show that the proposed method outperforms state-of-the-art methods for time series classification in terms of the classification accuracy and noise tolerance.","","","10.21629/JSEE.2017.01.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7870510","time series;multivariate time series;classification;convolutional neural network (CNN);data mining","Time series analysis;Convolution;Hidden Markov models;Training;Neural networks;Data mining;Neurons","learning (artificial intelligence);neural nets;pattern classification;time series","convolutional neural networks;time series classification;time series data mining;time series data;data size;data updating;deep learning techniques;CNN framework;feature-based classification approaches;pooling operations;noise tolerance","","23","","","","","","BIAI","BIAI Journals"
"Combining Semantic and Geometric Features for Object Class Segmentation of Indoor Scenes","F. Husain; H. Schulz; B. Dellen; C. Torras; S. Behnke","Institut de Robòtica i Informàtica Industrial, CSIC-UPC, Barcelona, Spain; Institute of Computer Science VI, University of Bonn, Bonn, Germany; RheinAhrCampus der Hochschule Koblenz, Remagen, Germany; Institut de Robòtica i Informàtica Industrial, CSIC-UPC, Barcelona, Spain; Institute of Computer Science VI, University of Bonn, Bonn, Germany","IEEE Robotics and Automation Letters","","2017","2","1","49","55","Scene understanding is a necessary prerequisite for robots acting autonomously in complex environments. Low-cost RGB-D cameras such as Microsoft Kinect enabled new methods for analyzing indoor scenes and are now ubiquitously used in indoor robotics. We investigate strategies for efficient pixelwise object class labeling of indoor scenes that combine both pretrained semantic features transferred from a large color image dataset and geometric features, computed relative to the room structures, including a novel distance-from-wall feature, which encodes the proximity of scene points to a detected major wall of the room. We evaluate our approach on the popular NYU v2 dataset. Several deep learning models are tested, which are designed to exploit different characteristics of the data. This includes feature learning with two different pooling sizes. Our results indicate that combining semantic and geometric features yields significantly improved results for the task of object class segmentation.","","","10.1109/LRA.2016.2532927","CSIC; RobInstruct; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7421973","Semantic scene understanding;categorization;segmentation;Semantic scene understanding;categorization;segmentation","Semantics;Feature extraction;Color;Labeling;Image segmentation;Training;Cameras","geometry;image classification;image colour analysis;learning (artificial intelligence);visual databases","geometric features;semantic features;object class segmentation;indoor scenes;RGB-D cameras;Microsoft Kinect;indoor robotics;pixelwise object class labeling;color image dataset;distance-from-wall feature;room structures;NYU v2 dataset;deep learning models","","14","30","","","","","IEEE","IEEE Journals"
"The Many Shades of Negativity","Z. Ma; X. Chang; Y. Yang; N. Sebe; A. G. Hauptmann","Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; University of Technology Sydney, Ultimo, NSW, Australia; University of Trento, Trento, Italy; Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Transactions on Multimedia","","2017","19","7","1558","1568","Complex event detection has been progressively researched in recent years for the broad interest of video indexing and retrieval. To fulfill the purpose of event detection, one needs to train a classifier using both positive and negative examples. Current classifier training treats the negative videos as equally negative. However, we notice that many negative videos resemble the positive videos in different degrees. Intuitively, we may capture more informative cues from the negative videos if we assign them fine-grained labels, thus benefiting the classifier learning. Aiming for this, we use a statistical method on both the positive and negative examples to get the decisive attributes of a specific event. Based on these decisive attributes, we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation. The resulting fine-grained labels may be not optimal to capture the discriminative cues from the negative videos. Hence, we propose to jointly optimize the fine-grained labels with the classifier learning, which brings mutual reciprocality. Meanwhile, the labels of positive examples are supposed to remain unchanged. We thus additionally introduce a constraint for this purpose. On the other hand, the state-of-the-art deep convolutional neural network features are leveraged in our approach for event detection to further boost the performance. Extensive experiments on the challenging TRECVID MED 2014 dataset have validated the efficacy of our proposed approach.","","","10.1109/TMM.2017.2659221","National Science Foundation; Intelligence Advanced Research Projects Activity via Department of Interior National Business Center; Data to Decisions Cooperative Research Centre; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835107","Attribute representation;attribute selection;complex event detection;selective fine-grained labeling","Event detection;Semantics;Animals;Feature extraction;Detectors;Training;Support vector machines","convolution;image classification;learning (artificial intelligence);neural nets;statistical analysis;video retrieval;video signal processing","complex event detection;video indexing;video retrieval;negative videos;positive videos;classifier learning;statistical method;fine-grained labels;deep convolutional neural network features;TRECVID MED 2014 dataset","","54","34","","","","","IEEE","IEEE Journals"
"DNN Filter Bank Cepstral Coefficients for Spoofing Detection","H. Yu; Z. Tan; Y. Zhang; Z. Ma; J. Guo","Pattern Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China; Department of Electronic Systems, Aalborg University, Aalborg, Denmark; International School, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Access","","2017","5","","4779","4787","With the development of speech synthesis techniques, automatic speaker verification systems face the serious challenge of spoofing attack. In order to improve the reliability of speaker verification systems, we develop a new filter bank-based cepstral feature, deep neural network (DNN) filter bank cepstral coefficients, to distinguish between natural and spoofed speech. The DNN filter bank is automatically generated by training a filter bank neural network (FBNN) using natural and synthetic speech. By adding restrictions on the training rules, the learned weight matrix of FBNN is band limited and sorted by frequency, similar to the normal filter bank. Unlike the manually designed filter bank, the learned filter bank has different filter shapes in different channels, which can capture the differences between natural and synthetic speech more effectively. The experimental results on the ASVspoof 2015 database show that the Gaussian mixture model maximum-likelihood classifier trained by the new feature performs better than the state-of-the-art linear frequency triangle filter bank cepstral coefficients-based classifier, especially on detecting unknown attacks.","","","10.1109/ACCESS.2017.2687041","National Natural Science Foundation of China; Beijing Nova Program; Beijing National Science Foundation; Scientific Research Foundation for Returned Scholars, Ministry of Education of China; Chinese 111 program of Advanced Intelligence, Network Service; OCTAVE Project; Research European Agency of the European Commission, in its framework programme Horizon 2020; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886361","Speaker verification;spoofing detection;DNN filter bank cepstral coefficients;filter bank;neural network","Feature extraction;Cepstral analysis;Speech;Neural networks;Shape;Training;Databases","cepstral analysis;channel bank filters;Gaussian processes;learning (artificial intelligence);matrix algebra;maximum likelihood estimation;mixture models;neural nets;reliability;speaker recognition;speech synthesis","DNN filter bank cepstral coefficient;spoofing attack detection;speech synthesis technique;automatic speaker verification system;reliability;deep neural network;training;filter bank neural network;FBNN;learned weight matrix;ASVspoof 2015 database;Gaussian mixture model;maximum-likelihood classifier;linear frequency triangle filter bank cepstral coefficient","","18","41","","","","","IEEE","IEEE Journals"
"SPABox: Safeguarding Privacy During Deep Packet Inspection at a MiddleBox","J. Fan; C. Guan; K. Ren; Y. Cui; C. Qiao","Department of Computer Science and Engineering, University at Buffalo, The State University of New York, Buffalo, NY, USA; Department of Computer Science and Engineering, University at Buffalo, The State University of New York, Buffalo, NY, USA; Department of Computer Science and Engineering, University at Buffalo, The State University of New York, Buffalo, NY, USA; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Engineering, University at Buffalo, The State University of New York, Buffalo, NY, USA","IEEE/ACM Transactions on Networking","","2017","25","6","3753","3766","Widely used over the Internet to encrypt traffic, HTTPS provides secure and private data communication between clients and servers. However, to cope with rapidly changing and sophisticated security attacks, network operators often deploy middleboxes to perform deep packet inspection (DPI) to detect attacks and potential security breaches, using techniques ranging from simple keyword matching to more advanced machine learning and data mining analysis. But this creates a problem: how can middleboxes, which employ DPI, work over HTTPS connections with encrypted traffic while preserving privacy? In this paper, we present SPABox, a middlebox-based system that supports both keyword-based and data analysis-based DPI functions over encrypted traffic. SPABox preserves privacy by using a novel protocol with a limited connection setup overhead. We implement SPABox on a standard server and show that SPABox is practical for both long-lived and short-lived connection. Compared with the state-of-the-art Blindbox system, SPABox is more than five orders of magnitude faster and requires seven orders of magnitude less bandwidth for connection setup while SPABox can achieve a higher security level.","","","10.1109/TNET.2017.2753044","National Science Foundation of China; NSF; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8064195","DPI;middlebox;privacy preserving","Middleboxes;Cryptography;Protocols;Privacy;Malware;Data privacy","computer network security;cryptography;data communication;data privacy;Internet;security of data;telecommunication traffic;transport protocols","SPABox;safeguarding privacy;deep packet inspection;middlebox;secure data communication;private data communication;clients;servers;sophisticated security attacks;DPI;potential security breaches;data mining analysis;HTTPS connections;encrypted traffic;data analysis;connection setup;Internet;traffic encryption;HTTPS;keyword-based DPI function;data analysis-based DPI function","","6","46","","","","","IEEE","IEEE Journals"
"Going Deeper With Contextual CNN for Hyperspectral Image Classification","H. Lee; H. Kwon","Booz Allen Hamilton Inc., McLean, VA, USA; Image Processing Branch, Sensors & Electron Devices Directorate, U.S. Army Research Laboratory, Adalphi, MD, USA","IEEE Transactions on Image Processing","","2017","26","10","4843","4855","In this paper, we describe a novel deep convolutional neural network (CNN) that is deeper and wider than other existing deep networks for hyperspectral image classification. Unlike current state-of-the-art approaches in CNN-based hyperspectral image classification, the proposed network, called contextual deep CNN, can optimally explore local contextual interactions by jointly exploiting local spatio-spectral relationships of neighboring individual pixel vectors. The joint exploitation of the spatio-spectral information is achieved by a multi-scale convolutional filter bank used as an initial component of the proposed CNN pipeline. The initial spatial and spectral feature maps obtained from the multi-scale filter bank are then combined together to form a joint spatio-spectral feature map. The joint feature map representing rich spectral and spatial properties of the hyperspectral image is then fed through a fully convolutional network that eventually predicts the corresponding label of each pixel vector. The proposed approach is tested on three benchmark data sets: the Indian Pines data set, the Salinas data set, and the University of Pavia data set. Performance comparison shows enhanced classification performance of the proposed approach over the current state-of-the-art on the three data sets.","","","10.1109/TIP.2017.2725580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973178","Convolutional neural network (CNN);hyperspectral image classification;residual learning;multi-scale filter bank;fully convolutional network (FCN)","Hyperspectral imaging;Feature extraction;Training;Biological neural networks;Principal component analysis","geophysical image processing;hyperspectral imaging;image classification;neural nets","novel deep convolutional neural network;CNN-based hyperspectral image classification;contextual deep CNN;local contextual interactions;multiscale convolutional filter bank;joint spatio-spectral feature map","","51","44","","","","","IEEE","IEEE Journals"
"Industrial Big Data Analysis in Smart Factory: Current Status and Research Strategies","X. Xu; Q. Hua","Guangdong Mechanical and Electrical College, Guangzhou, China; School of Mechanical and Electrical Engineering, Qingdao University, Qingdao, China","IEEE Access","","2017","5","","17543","17551","Under the background of cyber-physical systems and Industry 4.0, intelligent manufacturing has become an orientation and produced a revolutionary change. Compared with the traditional manufacturing environments, the intelligent manufacturing has the characteristics as highly correlated, deep integration, dynamic integration, and huge volume of data. Accordingly, it still faces various challenges. In this paper, we summarize and analyze the current research status in both domestic and aboard, including industrial big data collection, modeling of the intelligent product lines based on ontology, the predictive diagnosis based on industrial big data, group learning of product line equipment and the product line reconfiguration of intelligent manufacturing. Based on the research status and the problems, we propose the research strategies, including acquisition schemes of industrial big data under the environment of intelligent, ontology modeling and deduction method based intelligent product lines, predictive diagnostic methods on production lines based on deep neural network, deep learning among devices based on cloud supplements and 3-D selforganized reconfiguration mechanism based on the supplements of cloud. In our view, this paper will accelerate the implementation of smart factory.","","","10.1109/ACCESS.2017.2741105","Major Projects for Numerical Control Machine; Water Resource Science and Technology Innovation Program of Guangdong Province; Quality Project of Guangdong Province Office of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8012376","Industrial big data;smart factory;data analysis;cyber-physical systems","Manufacturing;Big Data;Ontologies;Production facilities;Cloud computing;Data models;Machine learning","Big Data;cloud computing;cyber-physical systems;data analysis;factory automation;intelligent manufacturing systems;learning (artificial intelligence);neural nets;ontologies (artificial intelligence);production engineering computing","dynamic integration;current research status;industrial big data collection;intelligent product lines;product line equipment;product line reconfiguration;intelligent manufacturing;ontology modeling;deduction method;smart factory;industrial big data analysis;cyber-physical systems;3-D selforganized reconfiguration mechanism","","14","57","OAPA","","","","IEEE","IEEE Journals"
"Multilinear Principal Component Analysis Network for Tensor Object Classification","J. Wu; S. Qiu; R. Zeng; Y. Kong; L. Senhadji; H. Shu","LIST, Key Laboratory of Computer Network and Information Integration, Southeast University, Ministry of Education, Nanjing, China; LIST, Key Laboratory of Computer Network and Information Integration, Southeast University, Ministry of Education, Nanjing, China; LIST, Key Laboratory of Computer Network and Information Integration, Southeast University, Ministry of Education, Nanjing, China; LIST, Key Laboratory of Computer Network and Information Integration, Southeast University, Ministry of Education, Nanjing, China; Centre de Recherche en Information Biomedicale Sino-Francais, Nanjing, China; LIST, Key Laboratory of Computer Network and Information Integration, Southeast University, Ministry of Education, Nanjing, China","IEEE Access","","2017","5","","3322","3331","The recently proposed principal component analysis network (PCANet) has performed well with respect to the classification of 2-D images. However, feature extraction may perform less well when dealing with multi-dimensional images, since the spatial relationships within the structures of the images are not fully utilized. In this paper, we develop a multilinear principal component analysis network (MPCANet), which is a tensor extension of PCANet, to extract the high-level semantic features from multi-dimensional images. The extracted features largely minimize the intraclass invariance of tensor objects by making efficient use of spatial relationships within multi-dimensional images. The proposed MPCANet outperforms traditional methods on a benchmark composed of three data sets, including the UCF sports action database, the UCF11 database, and a medical image database. It is shown that even a simple one-layer MPCANet may outperform a two-layer PCANet.","","","10.1109/ACCESS.2017.2675478","National Natural Science Foundation of China; Project Sponsored by the Scientific Research Foundation for the Returned Overseas Chinese Scholars; State Education Ministry; Qing Lan Project; 333 Project; Open Fund of China-USA Computer Science Research Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7867073","Deep learning;MPCANet;PCANet;tensor object classification;medical image classification","Tensile stress;Principal component analysis;Feature extraction;Convolution;Visualization;Histograms;Databases","feature extraction;image classification;learning (artificial intelligence);principal component analysis;tensors;visual databases","medical image database;UCF11 database;UCF sports action database;MPCANet;spatial relationships;intraclass invariance minimization;multidimensional images;high-level semantic feature extraction;tensor object classification;multilinear principal component analysis network","","8","36","","","","","IEEE","IEEE Journals"
"Long Short-Term Memory With Quadratic Connections in Recursive Neural Networks for Representing Compositional Semantics","D. Wu; M. Chi","School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China","IEEE Access","","2017","5","","16077","16083","Long short-term memory (LSTM) has been widely used in different applications, such as natural language processing, speech recognition, and computer vision over recurrent neural network (RNN) or recursive neural network (RvNN)-a tree-structured RNN. In addition, the LSTM-RvNN has been used to represent compositional semantics through the connections of hidden vectors over child units. However, the linear connections in the existing LSTM networks are incapable of capturing complex semantic representations of natural language texts. For example, complex structures in natural language texts usually denote intricate relationships between words, such as negated sentiment or sentiment strengths. In this paper, quadratic connections of the LSTM model is proposed in terms of RvNNs (abbreviated as qLSTM-RvNN) in order to attack the problem of representing compositional semantics. The proposed qLSTM-RvNN model is evaluated in the benchmark data sets containing semantic compositionality, i.e., sentiment analysis on Stanford Sentiment Treebank and semantic relatedness on sentences involving compositional knowledge data set. Empirical results show that it outperforms the state-of-the-art RNN, RvNN, and LSTM networks in two semantic compositionality tasks by increasing the classification accuracies and sentence correlation while significantly decreasing computational complexities.","","","10.1109/ACCESS.2016.2647384","Natural Science Foundation of China; State Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7803562","Recurrent neural networks;long short-term memory networks;nonlinear connections;deep learning","Logic gates;Semantics;Recurrent neural networks;Computer architecture;Mathematical model;Cost function","learning (artificial intelligence);recurrent neural nets","long short-term memory;quadratic connections;LSTM;recursive neural networks;compositional semantics representation;recurrent neural network;RNN;LSTM-RvNN;natural language texts;semantic representation;Stanford sentiment treebank","","2","33","","","","","IEEE","IEEE Journals"
"Beyond Frame-level CNN: Saliency-Aware 3-D CNN With LSTM for Video Action Recognition","X. Wang; L. Gao; J. Song; H. Shen","University of Electronic Science and Technology of China, Chengdu; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu; University of Trento, Trento, Italy; School of Information Technology and Electrical Engineering, the University of Queensland, Brisbane, Qld, Australia","IEEE Signal Processing Letters","","2017","24","4","510","514","Human activity recognition in videos with convolutional neural network (CNN) features has received increasing attention in multimedia understanding. Taking videos as a sequence of frames, a new record was recently set on several benchmark datasets by feeding frame-level CNN sequence features to long short-term memory (LSTM) model for video activity recognition. This recurrent model-based visual recognition pipeline is a natural choice for perceptual problems with time-varying visual input or sequential outputs. However, the above-mentioned pipeline takes frame-level CNN sequence features as input for LSTM, which may fail to capture the rich motion information from adjacent frames or maybe multiple clips. Furthermore, an activity is conducted by a subject or multiple subjects. It is important to consider attention that allows for salient features, instead of mapping an entire frame into a static representation. To tackle these issues, we propose a novel pipeline, saliency-aware three-dimensional (3-D) CNN with LSTM, for video action recognition by integrating LSTM with salient-aware deep 3-D CNN features on videos shots. Specifically, we first apply saliency-aware methods to generate saliency-aware videos. Then, we design an end-to-end pipeline by integrating 3-D CNN with LSTM, followed by a time series pooling layer and a softmax layer to predict the activities. Noticeably, we set a new record on two benchmark datasets, i.e., UCF101 with 13 320 videos and HMDB-51 with 6766 videos. Our method outperforms the state-of-the-art end-to-end methods of action recognition by 3.8% and 3.2%, respectively on above two datasets.","","","10.1109/LSP.2016.2611485","Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7572183","Action recognition;deep learning;three-dimensional (3-D) convolution;LSTM;saliency-aware","Three-dimensional displays;Pipelines;Visualization;Time series analysis;Image recognition;Microprocessors;Computer architecture","feature extraction;feedforward neural nets;image representation;image sequences;object recognition;recurrent neural nets;video databases;video signal processing","HMDB-51 dataset;UCF101 dataset;soft-max layer;time series pooling layer;video shots;salient-aware deep 3D CNN features;video action recognition;saliency-aware three-dimensional CNN;static representation;recurrent model-based visual recognition pipeline;video activity recognition;long short-term memory model;frame-level CNN sequence features;video frame sequence;convolutional neural network features;human activity recognition;LSTM","","69","33","","","","","IEEE","IEEE Journals"
"Cross-Modal Retrieval With CNN Visual Features: A New Baseline","Y. Wei; Y. Zhao; C. Lu; S. Wei; L. Liu; Z. Zhu; S. Yan","Institute of Information Science, Beijing Jiaotong University, Beijing, China; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Institute of Information Science, Beijing Jiaotong University, Beijing, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Cybernetics","","2017","47","2","449","460","Recently, convolutional neural network (CNN) visual features have demonstrated their powerful ability as a universal representation for various recognition tasks. In this paper, cross-modal retrieval with CNN visual features is implemented with several classic methods. Specifically, off-the-shelf CNN visual features are extracted from the CNN model, which is pretrained on ImageNet with more than one million images from 1000 object categories, as a generic image representation to tackle cross-modal retrieval. To further enhance the representational ability of CNN visual features, based on the pretrained CNN model on ImageNet, a fine-tuning step is performed by using the open source Caffe CNN library for each target data set. Besides, we propose a deep semantic matching method to address the cross-modal retrieval problem with respect to samples which are annotated with one or multiple labels. Extensive experiments on five popular publicly available data sets well demonstrate the superiority of CNN visual features for cross-modal retrieval.","","","10.1109/TCYB.2016.2519449","National Basic Research Program of China; Fundamental Scientific Research; National Natural Science Foundation of China; Program for Changjiang Scholars and Innovative Research Team in University; National University of Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7428926","Convolutional neural network (CNN) visual features;cross-media;cross-modal;deep learning;multimodal","Visualization;Feature extraction;Semantics;Data models;Correlation;Image recognition;Search problems","feature extraction;image matching;image representation;image retrieval;neural nets;public domain software","convolutional neural network;CNN visual feature extraction;universal representation;cross-modal retrieval;ImageNet;generic image representation;pretrained CNN model;fine-tuning step;open source Caffe CNN library;deep semantic matching","","44","70","","","","","IEEE","IEEE Journals"
"High-Level Feature Extraction for Classification and Person Re-Identification","A. Feizi","Electrical Engineering Department, Damghan University, Damghan, Iran","IEEE Sensors Journal","","2017","17","21","7064","7073","The need to understand behaviors and identify individuals across surveillance cameras has led to a growing interest in research on feature extraction. High-level feature extraction aims at discriminating behaviors and individuals with high accuracy. In this paper, instead of using low-level image features, a novel method for extracting high-level features, which can be used for classification and person re-identification, is proposed. For this purpose, a set of low-level features is first extracted for each local of image. Then, a set of prototypes is sampled in each low-level feature space. These prototypes represent a different category of features which have the maximum inter-distance. Using canonical correlation analysis, new distance criteria are set to measure the mutual distance between each pair of sampled prototypes. This allows the proposed method to evaluate the similarity of the prototypes more accurately. As a result, the sample prototypes have the maximum inter-distance and their discriminative capacity is high. Once the set of the prototypes is achieved in each low-level feature space, the set is used to project the low-level feature space onto a new feature space. To serve this purpose, the correlation between the feature extracted for each local of image in the low-level feature space and each prototype in the set is computed and the resulting correlation is considered as one element of the new feature vector for that local. Now, there are two feature vectors for each local of image which are then combined to obtain a single feature vector. The deep belief network (DBN) is employed as a final stage whereby the single feature vector is fed into the DBN to train it in order to output a discriminant feature with a reduced dimension. This new high-level feature can be efficiently used for such applications as data classification and person re-identification.","","","10.1109/JSEN.2017.2756349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8049283","Person re-identification;feature fusion;deep belief network;canonical correlation analysis;prototype learning","Feature extraction;Prototypes;Correlation;Histograms;Sensors;Computer vision;Computational complexity","belief networks;cameras;correlation methods;distance measurement;feature extraction;image classification;image sampling;video surveillance","DBN;deep belief network;image sampling;mutual distance measurement;canonical correlation analysis;image classification;low-level image feature extraction;surveillance camera;person reidentification;high-level feature extraction","","3","34","Traditional","","","","IEEE","IEEE Journals"
"FreeScup: A Novel Platform for Assisting Sculpture Pose Design","Y. Wu; T. Lu; Z. Yuan; H. Wang","National Key Lab for Novel Software Technology, Nanjing University, Nanjing, China; National Key Lab for Novel Software Technology, Nanjing University, Nanjing, China; National Key Lab for Novel Software Technology, Nanjing University, Nanjing, China; Institute of Deep Learning, Baidu, Beijing, China","IEEE Transactions on Multimedia","","2017","19","1","183","195","Sculpture design is challenging due to its inherent difficulty in characterizing artworks quantitatively; thus, few works have been done to assist sculpture design in the past decades in the multimedia community. We have cooperated with several sculptors on analyzing styles of different artists consisting of Giacometti, Augeuste Rodin, Henry Moore, and Marino Marini from which we find pose editing plays an important role in sculpture design. Motivated by this, we present a novel platform that allows sculptors to edit virtual three-dimensional (3-D) sculptures by a free way. The proposed platform consists of three modules, namely, sculpture initialization, sculptor-sculpture mapping, and interactive pose editing. In sculpture initialization, a virtual 3-D sculpture is first incrementally reconstructed from multiview images. Then, we define Laplace operator and its corresponding spectrum to describe the geometry information of the reconstructed sculpture. During sculptor-sculpture mapping, we apply spectral analysis on the low-frequency parts of the spectrum to search for candidate editing points on the surface of the sculpture. Next, body actions of the sculptor are captured by Kinect and further mapped onto editing points as a predefined configuration set. Finally, during interactive pose editing, a real-time Kinect-driven sculpture pose editing scheme is presented, which not only preserves geometry features of the sculpture but also allows instant changes of sculpture poses. We demonstrate that our platform successfully assists sculptors on real-time pose editing by comparing its performance with those of the existing sculpture assisting methods.","","","10.1109/TMM.2016.2609407","Natural Science Foundation of China; Science Foundation for Distinguished Young Scholars of Jiangsu; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7567515","Artistic design;FreeScup;pose editing;sculpture;spectral","Three-dimensional displays;Image reconstruction;Real-time systems;Shape;Multimedia communication;Solid modeling;Surface reconstruction","art;computational geometry;image reconstruction;pose estimation;spectral analysis;virtual reality","FreeScup;sculpture pose design;artworks;multimedia community;virtual three-dimensional sculptures;virtual 3D sculptures;sculptor-sculpture mapping;interactive pose editing;multiview images;Laplace operator;sculptor-sculpture mapping;spectral analysis;real-time Kinect-driven sculpture pose editing;sculpture geometry features","","1","45","","","","","IEEE","IEEE Journals"
"Efficient Message Passing Methods With Fully Connected Models for Early Vision","X. Tan; C. Sun; F. Shen; K. K. Wong","Baidu, Institute of Deep Learning, Shenzhen, China; CSIRO Data61, Epping, NSW, Australia; University of Electronic Science and Technology of China, Chengdu, China; Department of Computer Science, The University of Hong Kong, Hong Kong","IEEE Transactions on Image Processing","","2017","26","12","5994","6005","Fully connected Markov random fields and conditional random fields have recently been shown to be advantageous in many early vision tasks being formulated as multi-labeling problems, such as stereo matching and image segmentation. The maximum posterior marginal (MPM) inference method in solving fully connected models uses a hybrid framework of mean-field (MF) method and a filtering like approach, and yields excellent results. In this paper, we extend this framework in several aspects. First, we provide an alternative inference method employing fractional belief propagation based method instead of MF. Second, we reformulate the MPM problem into a maximum a posterior (MAP) problem and provide efficient algorithms for solving this. Third, we extend the fully connected model into a multi-resolution approach. Finally, we propose an integral image based approach which makes it possible for efficiently integrating the local linear regression technique into this framework. Comparisons are carried out among different algorithms and different formulations to find the best combination. We demonstrate that the use of our multi-resolution approach with MAP formulation substantially outperforms the ordinary MF-based inference scheme.","","","10.1109/TIP.2017.2750406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030105","Markov random fields;conditional random fields;message passing methods;multi-labeling;stereo matching;image segmentation","Labeling;Message passing;Image segmentation;Belief propagation;Computational modeling;Inference algorithms;Markov random fields","computer vision;image matching;image resolution;image segmentation;inference mechanisms;Markov processes;message passing;regression analysis;stereo image processing","Markov random fields;conditional random fields;multilabeling problems;stereo matching;image segmentation;maximum posterior marginal inference method;mean-field method;alternative inference method;MPM problem;multiresolution approach;integral image based approach","","","43","Traditional","","","","IEEE","IEEE Journals"
"Social Media: New Perspectives to Improve Remote Sensing for Emergency Response","J. Li; Z. He; J. Plaza; S. Li; J. Chen; H. Wu; Y. Wang; Y. Liu","School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Hyperspectral Computing Laboratory Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain; College of Electrical and Information Engineering, Hunan University, Changsha, China; School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing, Wuhan University, Wuhan, China; Institute of Remote Sensing and Geographical Information Systems, Peking University, Beijing, China","Proceedings of the IEEE","","2017","105","10","1900","1912","Remote sensing is a powerful technology for Earth observation (EO), and it plays an essential role in many applications, including environmental monitoring, precision agriculture, resource managing, urban characterization, disaster and emergency response, etc. However, due to limitations in the spectral, spatial, and temporal resolution of EO sensors, there are many situations in which remote sensing data cannot be fully exploited, particularly in the context of emergency response (i.e., applications in which real/near-real-time response is needed). Recently, with the rapid development and availability of social media data, new opportunities have become available to complement and fill the gaps in remote sensing data for emergency response. In this paper, we provide an overview on the integration of social media and remote sensing in time-critical applications. First, we revisit the most recent advances in the integration of social media and remote sensing data. Then, we describe several practical case studies and examples addressing the use of social media data to improve remote sensing data and/or techniques for emergency response.","","","10.1109/JPROC.2017.2684460","Guangdong Provincial Natural Science Foundation; National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7904672","Deep learning;emergency response;remote sensing;social media","Remote sensing;Emergency services;Floods;Geographic information systems;Synthetic aperture radar;Twitter;Spatial resolution;Social network services;Machine learning","Earth;emergency management;remote sensing;social networking (online)","emergency response;Earth observation;EO sensors;remote sensing data;social media data;time-critical applications","","4","127","","","","","IEEE","IEEE Journals"
"Object-Based Land-Cover Supervised Classification for Very-High-Resolution UAV Images Using Stacked Denoising Autoencoders","X. Zhang; G. Chen; W. Wang; Q. Wang; F. Dai","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; College of Science, Wuhan University of Science and Technology, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","7","3373","3385","Over the last decade, object-based image classification (OBIC) has become a mainstream method in remote sensing land-use/land-cover applications. Many supervised classification methods have been proposed in the OBIC framework. However, most did not use deep learning methods. In this paper, a new deep-learning-based OBIC framework is introduced. First, we segment the original image into objects by graph-based minimal-spanning-tree segmentation algorithm. Second, we extract the spectral, spatial, and texture features for each object. Then we put all features into stacked autoencoders (SAE) or stacked denoising autoencoders (SDAE) network, and trained the parameters of the network using training samples. Finally, all objects were classified by the network. Based on our SAE/SDAE OBIC framework, we achieved 97% overall accuracy when classifying an UAV image into five categories. In addition, our experiment shows that our framework increases overall accuracy by approximately 6% when compared to the linear support vector machine (linear SVM) and radial basis function kernel support vector machine (RBF SVM) algorithms when sufficient training samples are lacking.","","","10.1109/JSTARS.2017.2672736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7879207","Deep learning (DL);object-based image classification (OBIC);stacked autoencoders (SAE);stacked denoising autoencoders (SDAE)","Feature extraction;Image segmentation;Remote sensing;Training;Support vector machines;Indexes;Neural networks","geophysical image processing;image classification;image resolution","object-based land-cover supervised classification;very-high-resolution UAV images;stacked denoising autoencoders;mainstream method;remote sensing;OBIC framework;supervised classification methods;graph-based minimal-spanning-tree segmentation algorithm;texture features;training samples;SAE/SDAE OBIC framework;linear support vector machine;radial basis function kernel support vector machine algorithms","","8","37","","","","","IEEE","IEEE Journals"
"Picking Neural Activations for Fine-Grained Recognition","X. Zhang; H. Xiong; W. Zhou; W. Lin; Q. Tian","Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; CAS Key Laboratory of Technology in Geo-spatial Information Processing and Application System, University of Science and Technology of China, Hefei, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA","IEEE Transactions on Multimedia","","2017","19","12","2736","2750","It is a challenging task to recognize fine-grained subcategories due to the highly localized and subtle differences among them. Different from most previous methods that rely on object/part annotations, this paper proposes an automatic fine-grained recognition approach, which is free of any object/part annotation at both training and testing stages. The key idea includes two steps of picking neural activations computed from the convolutional neural networks, one for localization, and the other for description. The first picking step is to find distinctive neurons that are sensitive to specific patterns significantly and consistently. Based on these picked neurons, we initialize positive samples and formulate the localization as a regularized multiple instance learning task, which aims at refining the detectors via iteratively alternating between new positive sample mining and part model retraining. The second picking step is to pool deep neural activations via a spatially weighted combination of Fisher Vectors coding. We conditionally select activations to encode them into the final representation, which considers the importance of each activation. Integrating the above techniques produces a powerful framework, and experiments conducted on several extensive fine-grained benchmarks demonstrate the superiority of our proposed algorithm over the existing methods.","","","10.1109/TMM.2017.2710803","National Science Foundation of China; China Scholarship Council; Program of Shanghai Academic Research Leader; Microsoft Research Aisa Collaborative Research Award; ARO; Faculty Research Gift Awards; NEC Laboratories of America and Blippar; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937818","Fine-grained recognition;regularized multiple instance leaning;spatially weighted Fisher Vectors (SWFV);weakly supervised part discovery","Neurons;Detectors;Training;Object detection;Multimedia databases;Algorithm design and analysis","convolution;data mining;feature extraction;learning (artificial intelligence);neural nets;object detection;object recognition","regularized multiple instance learning task;positive sample mining;deep neural activations;fine-grained recognition approach;testing stages;convolutional neural networks;training stages","","4","53","Traditional","","","","IEEE","IEEE Journals"
"Residual Deconvolutional Networks for Brain Electron Microscopy Image Segmentation","A. Fakhry; T. Zeng; S. Ji","Department of Computer Science, Old Dominion University, Norfolk, VA, USA; School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA; School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA","IEEE Transactions on Medical Imaging","","2017","36","2","447","456","Accurate reconstruction of anatomical connections between neurons in the brain using electron microscopy (EM) images is considered to be the gold standard for circuit mapping. A key step in obtaining the reconstruction is the ability to automatically segment neurons with a precision close to human-level performance. Despite the recent technical advances in EM image segmentation, most of them rely on hand-crafted features to some extent that are specific to the data, limiting their ability to generalize. Here, we propose a simple yet powerful technique for EM image segmentation that is trained end-to-end and does not rely on prior knowledge of the data. Our proposed residual deconvolutional network consists of two information pathways that capture full-resolution features and contextual information, respectively. We showed that the proposed model is very effective in achieving the conflicting goals in dense output prediction; namely preserving full-resolution predictions and including sufficient contextual information. We applied our method to the ongoing open challenge of 3D neurite segmentation in EM images. Our method achieved one of the top results on this open challenge. We demonstrated the generality of our technique by evaluating it on the 2D neurite segmentation challenge dataset where consistently high performance was obtained. We thus expect our method to generalize well to other dense output prediction problems.","","","10.1109/TMI.2016.2613019","National Science Foundation, Old Dominion University, and Washington State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575638","Brain circuit reconstruction;deconvolutional networks;deep learning;electron microscopy;image segmentation;residual learning","Image segmentation;Deconvolution;Three-dimensional displays;Image reconstruction;Feature extraction;Predictive models;Convolution","brain;deconvolution;electron microscopy;image segmentation;medical image processing;neural nets;neurophysiology","residual deconvolutional networks;brain electron microscopy;anatomical connection reconstruction;neurons;electron microscopy images;circuit mapping;human-level performance;EM image segmentation;hand-crafted features;information pathways;full-resolution features;contextual information;dense output prediction;full-resolution predictions;3D neurite segmentation;neurite segmentation challenge dataset","Brain;Humans;Microscopy, Electron;Neurons","30","46","","","","","IEEE","IEEE Journals"
"Assessment of PD severity in gas-insulated switchgear with an SSAE","J. Tang; M. Jin; F. Zeng; X. Zhang; R. Huang","Wuhan University, People's Republic of China; Wuhan University, People's Republic of China; Wuhan University, People's Republic of China; Wuhan University, People's Republic of China; Shandong Electric Power Research Institute, People's Republic of China","IET Science, Measurement & Technology","","2017","11","4","423","430","Scientific partial discharge (PD) severity evaluation is highly important to the safe operation of gas-insulated switchgear. However, describing PD severity with only a few statistical features such as discharge time and discharge amplitude is unreliable. Hence, a deep-learning neural network model called stacked sparse auto-encoder (SSAE) is proposed to realise feature extraction from the middle layer with a small number of nodes. The output feature that is almost similar to the input PD information is produced in the model. The features extracted from PD data are then fed into a soft-max classifier to be classified into one of four defined PD severity states. In addition, unsupervised greedy layer-wise pre-training and supervised fine-tuning are utilised to train the SSAE network during evaluation. Results of testing and simulation analysis show that the features extracted by the SSAE model effectively characterise PD severity. The performance of the SSAE model, which possesses an average assessment accuracy of up to 92.2%, is better than that of the support vector machine algorithm based on statistical features. According to the tested number of SSAE layers and features and the training sample size, the SSAE model possesses good expansibility and can be useful in practical applications.","","","10.1049/iet-smt.2016.0326","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7959678","","","computerised instrumentation;encoding;feature extraction;gas insulated switchgear;learning (artificial intelligence);neural nets;partial discharge measurement;statistical analysis","PD severity assessment;gas-insulated switchgear;SSAE;partial discharge severity assessment;discharge time;discharge amplitude;deep-learning neural network model;stacked sparse autoencoder;feature extraction;soft-max classifier;unsupervised greedy layer-wise pre-training method;supervised fine-tuning method;support vector machine algorithm","","5","24","","","","","IET","IET Journals"
"Overview of Passive Optical Multispectral and Hyperspectral Image Simulation Techniques","S. Han; J. P. Kerekes","Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA; Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","10","11","4794","4804","The simulation of optical images can playkey roles in the development of new instruments, the quantitative evaluation of algorithms and in the training of both image analysis software and human analysts. Methods for image simulation include surrogate data collections, operations on empirical imagery, statistical generation techniques, and full physical modeling approaches. Each method offers advantages or disadvantages in terms of time, cost, and realism. Current state of the art suggests three-dimensional radiative transfer models capture most of the significant characteristics of real imagery and find valuable use in system development and evaluation programs. Emerging computational power available from multithreading, graphical processing units, and techniques from deep learning will continue to enable even more realistic simulations in the near future.","","","10.1109/JSTARS.2017.2759240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8074733","Hyperspectral imaging;image simulation;multispectral imaging;optical remote sensing","Atmospheric modeling;Analytical models;Computational modeling;Land surface;Solid modeling;Remote sensing;Optical imaging","image processing;learning (artificial intelligence);optical images;statistical analysis","passive optical multispectral image simulation techniques;hyperspectral image simulation techniques;image analysis software;statistical generation techniques;physical modeling approaches;three-dimensional radiative transfer models;deep learning","","1","109","Traditional","","","","IEEE","IEEE Journals"
"Enhancing short-term probabilistic residential load forecasting with quantile long–short-term memory","D. Gan; Y. Wang; N. Zhang; W. Zhu","Tsinghua University, People's Republic of China; Tsinghua University, People's Republic of China; Tsinghua University, People's Republic of China; Electric Power Research Institute, Guangdong Power Grid Corporation, China Southern Power Grid, People's Republic of China","The Journal of Engineering","","2017","2017","14","2622","2627","In the study of load forecasting, short-term (ST) load forecasting in the horizon of individuals is prone to manifest non-stationary and stochastic features compared with predicting the aggregated loads. Hence, better methodologies should be proposed to forecast ST residential loads more accurately, and refined representation of forecasting results should be reconsidered to make the prediction more reliable. A format of ST probabilistic forecasting results in terms of quantiles is offered, which can better describe the uncertainty of residential loads, and a deep-learning-based method, quantile long–ST memory, to implement probabilistic residential load forecasting. Experiments are conducted on an open dataset. Results show that the proposed method overrides traditional methods significantly in terms of average quantile score.","","","10.1049/joe.2017.0833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8310533","","","power engineering computing;stochastic processes;learning (artificial intelligence);probability;load forecasting","average quantile score;probabilistic residual load forecasting;deep-learning-based method;residual loads uncertainty;ST probabilistic forecasting;aggregated loads;stochastic features;nonstationary features;short-term ST load forecasting;quantile long-short-term memory;short-term probabilistic residential load forecasting","","4","22","","","","","IET","IET Journals"
"Pedestrian Movement Direction Recognition Using Convolutional Neural Networks","A. Dominguez-Sanchez; M. Cazorla; S. Orts-Escolano","University of Alicante, Alicante, Spain; University of Alicante, Alicante, Spain; University of Alicante, Alicante, Spain","IEEE Transactions on Intelligent Transportation Systems","","2017","18","12","3540","3548","Pedestrian movement direction recognition is an important factor in autonomous driver assistance and security surveillance systems. Pedestrians are the most crucial and fragile moving objects in streets, roads, and events, where thousands of people may gather on a regular basis. People flow analysis on zebra crossings and in shopping centers or events such as demonstrations are a key element to improve safety and to enable autonomous cars to drive in real life environments. This paper focuses on deep learning techniques such as convolutional neural networks (CNN) to achieve a reliable detection of pedestrians moving in a particular direction. We propose a CNN-based technique that leverages current pedestrian detection techniques (histograms of oriented gradients-linSVM) to generate a sum of subtracted frames (flow estimation around the detected pedestrian), which are used as an input for the proposed modified versions of various state-of-the-art CNN networks, such as AlexNet, GoogleNet, and ResNet. Moreover, we have also created a new data set for this purpose, and analyzed the importance of training in a known data set for the neural networks to achieve reliable results.","","","10.1109/TITS.2017.2726140","Feder funds, Spanish Government through the COMBAHO Project; University of Alicante Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8006277","Pedestrian detection;advance driver assistance system;convolutional neural networks;pedestrian intention recognition","Biological neural networks;Trajectory;Autonomous automobiles;Histograms;Convolutional neural networks;Training","driver information systems;image recognition;learning (artificial intelligence);neural nets;object detection;pedestrians;support vector machines;traffic engineering computing","security surveillance systems;pedestrians;crucial moving objects;fragile moving objects;autonomous cars;deep learning techniques;convolutional neural networks;CNN networks;histograms of oriented gradients-linSVM;pedestrian detection techniques;autonomous driver assistance;pedestrian movement direction recognition","","7","30","","","","","IEEE","IEEE Journals"
"Joint Distance Maps Based Action Recognition With Convolutional Neural Networks","C. Li; Y. Hou; P. Wang; W. Li","School of Electronic Information Engineering, Tianjin University, Tianjin, China; School of Electronic Information Engineering, Tianjin University, Tianjin, China; Advanced Multimedia Research Lab, University of Wollongong, Wollongong, NSW, Australia; Advanced Multimedia Research Lab, University of Wollongong, Wollongong, NSW, Australia","IEEE Signal Processing Letters","","2017","24","5","624","628","Motivated by the promising performance achieved by deep learning, an effective yet simple method is proposed to encode the spatio-temporal information of skeleton sequences into color texture images, referred to as joint distance maps (JDMs), and convolutional neural networks are employed to exploit the discriminative features from the JDMs for human action and interaction recognition. The pair-wise distances between joints over a sequence of single or multiple person skeletons are encoded into color variations to capture temporal information. The efficacy of the proposed method has been verified by the state-of-the-art results on the large RGB+D Dataset and small UTD-MHAD Dataset in both single-view and cross-view settings.","","","10.1109/LSP.2017.2678539","National Natural Science Foundation of China; Tianjin Science and Technology Pillar Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872453","Action recognition;convolutional neural networks (ConvNets);joint distance maps (JDM)","Skeleton;Training;Legged locomotion;Image color analysis;Image recognition;Neural networks;Three-dimensional displays","image colour analysis;image recognition;image sequences;image texture;learning (artificial intelligence);neural nets","UTD-MHAD dataset;RGB+D dataset;pairwise distance;interaction recognition;human action recognition;discriminative features;JDM;color texture images;skeleton sequences;spatio-temporal information;deep learning;convolutional neural networks;joint distance maps based action recognition","","59","28","","","","","IEEE","IEEE Journals"
"Data Fusion and IoT for Smart Ubiquitous Environments: A Survey","F. Alam; R. Mehmood; I. Katib; N. N. Albogami; A. Albeshri","Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; High Performance Computing Center, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia","IEEE Access","","2017","5","","9533","9554","The Internet of Things (IoT) is set to become one of the key technological developments of our times provided we are able to realize its full potential. The number of objects connected to IoT is expected to reach 50 billion by 2020 due to the massive influx of diverse objects emerging progressively. IoT, hence, is expected to be a major producer of big data. Sharing and collaboration of data and other resources would be the key for enabling sustainable ubiquitous environments, such as smart cities and societies. A timely fusion and analysis of big data, acquired from IoT and other sources, to enable highly efficient, reliable, and accurate decision making and management of ubiquitous environments would be a grand future challenge. Computational intelligence would play a key role in this challenge. A number of surveys exist on data fusion. However, these are mainly focused on specific application areas or classifications. The aim of this paper is to review literature on data fusion for IoT with a particular focus on mathematical methods (including probabilistic methods, artificial intelligence, and theory of belief) and specific IoT environments (distributed, heterogeneous, nonlinear, and object tracking environments). The opportunities and challenges for each of the mathematical methods and environments are given. Future developments, including emerging areas that would intrinsically benefit from data fusion and IoT, autonomous vehicles, deep learning for data fusion, and smart cities, are discussed.","","","10.1109/ACCESS.2017.2697839","Deanship of Scientific Research (DSR) at the King Abdulaziz University (KAU), Jeddah, Saudi Arabia; High Performance Computing Center at the King AbdulAziz University, Jeddah; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7911293","Internet of Things;big data;data fusion;computational and artificial intelligence;high performance computing;smart cities;smart societies;ubiquitous environments","Data integration;Smart cities;Internet of Things;Big Data;Decision making;Computer science;Information technology","Big Data;data analysis;Internet of Things;learning (artificial intelligence);sensor fusion","data collaboration;big data analysis;mathematical methods;probabilistic methods;artificial intelligence;theory-of-belief;autonomous vehicles;deep learning;smart cities;data sharing;smart ubiquitous environments;Internet-of-Things;IoT;big data fusion","","67","214","","","","","IEEE","IEEE Journals"
"Automatic ear detection and feature extraction using Geometric Morphometrics and convolutional neural networks","C. Cintas; M. Quinto-Sánchez; V. Acuña; C. Paschetta; S. de Azevedo; C. Cesar Silva de Cerqueira; V. Ramallo; C. Gallo; G. Poletti; M. C. Bortolini; S. Canizales-Quinteros; F. Rothhammer; G. Bedoya; A. Ruiz-Linares; R. Gonzalez-José; C. Delrieux","Centro Nacional Patagónico, Consejo Nacional de Investigaciones Científicas y Técnicas, Argentina; Universidad Nacional Autónoma de México, México; University College London, UK; Centro Nacional Patagónico, Consejo Nacional de Investigaciones Científicas y Técnicas, Argentina; Centro Nacional Patagónico, Consejo Nacional de Investigaciones Científicas y Técnicas, Argentina; Superintendência da Polícia Técnico-Científica do Estado de São Paulo, Brazil; Centro Nacional Patagónico, Consejo Nacional de Investigaciones Científicas y Técnicas, Argentina; Universidad Peruana Cayetano Heredia, Perú; Universidad Peruana Cayetano Heredia, Perú; Universidade Federal do Rio Grande do Sul, Brasil; UNAM, México; Instituto de Alta Investigación Universidad de Tarapacá, Chile; Universidad de Antioquia, Colombia; Fudan University, China; Centro Nacional Patagónico, Consejo Nacional de Investigaciones Científicas y Técnicas, Argentina; Universidad Nacional del Sur, and Consejo Nacional de Investigaciones Científicas y Técnicas, Argentina","IET Biometrics","","2017","6","3","211","223","Accurate gathering of phenotypic information is a key aspect in several subject matters, including biometrics, biomedical analysis, forensics, and many other. Automatic identification of anatomical structures of biometric interest, such as fingerprints, iris patterns, or facial traits, are extensively used in applications like access control and anthropological research, all having in common the drawback of requiring intrusive means for acquiring the required information. In this regard, the ear structure has multiple advantages. Not only the ear's biometric markers can be easily captured from the distance with non intrusive methods, but also they experiment almost no changes over time, and are not influenced by facial expressions. Here we present a new method based on Geometric Morphometrics and Deep Learning for automatic ear detection and feature extraction in the form of landmarks. A convolutional neural network was trained with a set of manually landmarked examples. The network is able to provide morphometric landmarks on ears' images automatically, with a performance that matches human landmarking. The feasibility of using ear landmarks as feature vectors opens a novel spectrum of biometrics applications.","","","10.1049/iet-bmt.2016.0002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7898901","","","biometrics (access control);computational geometry;feature extraction;image matching;learning (artificial intelligence);neural nets","people identification;feature vectors;human-assisted landmark matching;morphometric landmarks;convolutional neural network training;2D landmarks;automatic ear detection;deep-learning algorithms;phenotypic attributes;facial expressions;nonintrusive method;ear biometric markers;ear structure;facial traits;iris patterns;fingerprints;anatomical structure identification;phenotypic information;geometric morphometrics;feature extraction","","2","53","","","","","IET","IET Journals"
"Beautiful and Damned. Combined Effect of Content Quality and Social Ties on User Engagement","L. M. Aiello; R. Schifanella; M. Redi; S. Svetlichnaya; F. Liu; S. Osindero","Nokia Bell Labs, Cambridge, United Kingdom; University of Turin, Torino, Italy; Nokia Bell Labs, Cambridge, United Kingdom; Flickr, Sunnyvale, CA; Flickr, Sunnyvale, CA; Flickr, Sunnyvale, CA","IEEE Transactions on Knowledge and Data Engineering","","2017","29","12","2682","2695","User participation in online communities is driven by the intertwinement of the social network structure with the crowd-generated content that flows along its links. These aspects are rarely explored jointly and at scale. By looking at how users generate and access pictures of varying beauty on Flickr, we investigate how the production of quality impacts the dynamics of online social systems. We develop a deep learning computer vision model to score images according to their aesthetic value and we validate its output through crowdsourcing. By applying it to over 15 B Flickr photos, we study for the first time how image beauty is distributed over a large-scale social system. Beautiful images are evenly distributed in the network, although only a small core of people get social recognition for them. To study the impact of exposure to quality on user engagement, we set up matching experiments aimed at detecting causality from observational data. Exposure to beauty is double-edged: following people who produce high-quality content increases one's probability of uploading better photos; however, an excessive imbalance between the quality generated by a user and the user's neighbors leads to a decline in engagement. Our analysis has practical implications for improving link recommender systems.","","","10.1109/TKDE.2017.2747552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8023828","Content quality;image aesthetics;network effects;causal inference;influence;matching;flickr","Flickr;Production;Visualization;Content-based retrieval;Computer vision;Social network services","computer vision;learning (artificial intelligence);recommender systems;social networking (online);user interfaces","Flickr;online social systems;aesthetic value;image beauty;large-scale social system;user engagement;link recommender systems;content quality;social ties;user participation;online communities;social network structure;deep learning;computer vision model","","","72","Traditional","","","","IEEE","IEEE Journals"
"EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos","A. P. Twinanda; S. Shehata; D. Mutter; J. Marescaux; M. de Mathelin; N. Padoy","ICube, University of Strasbourg, CNRS, IHU, Strasbourg, France; ICube, University of Strasbourg, CNRS, IHU, Strasbourg, France; University Hospital of Strasbourg, IRCAD and IHU, Strasbourg, France; University Hospital of Strasbourg, IRCAD and IHU, Strasbourg, France; ICube, University of Strasbourg, CNRS, IHU, Strasbourg, France; ICube, University of Strasbourg, CNRS, IHU, Strasbourg, France","IEEE Transactions on Medical Imaging","","2017","36","1","86","97","Surgical workflow recognition has numerous potential medical applications, such as the automatic indexing of surgical video databases and the optimization of real-time operating room scheduling, among others. As a result, surgical phase recognition has been studied in the context of several kinds of surgeries, such as cataract, neurological, and laparoscopic surgeries. In the literature, two types of features are typically used to perform this task: visual features and tool usage signals. However, the used visual features are mostly handcrafted. Furthermore, the tool usage signals are usually collected via a manual annotation process or by using additional equipment. In this paper, we propose a novel method for phase recognition that uses a convolutional neural network (CNN) to automatically learn features from cholecystectomy videos and that relies uniquely on visual information. In previous studies, it has been shown that the tool usage signals can provide valuable information in performing the phase recognition task. Thus, we present a novel CNN architecture, called EndoNet, that is designed to carry out the phase recognition and tool presence detection tasks in a multi-task manner. To the best of our knowledge, this is the first work proposing to use a CNN for multiple recognition tasks on laparoscopic videos. Experimental comparisons to other methods show that EndoNet yields state-of-the-art results for both tasks.","","","10.1109/TMI.2016.2593957","French state funds managed by the ANR within the Investissements d’Avenir program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7519080","Laparoscopic videos;cholecystectomy;convolutional neural network;tool presence detection;phase recognition","Videos;Surgery;Visualization;Feature extraction;Laparoscopes;Computer architecture;Image recognition","biomedical optical imaging;endoscopes;feature extraction;medical image processing;neural nets;optimisation;surgery","EndoNet;deep architecture;recognition tasks;laparoscopic videos;surgical workflow recognition;medical applications;automatic indexing;tool presence detection tasks;CNN architecture;phase recognition task;cholecystectomy videos;convolutional neural network;manual annotation process;tool usage signals;visual features;laparoscopic surgeries;neurological surgeries;cataract surgeries;surgical phase recognition;real-time operating room scheduling;optimization;surgical video databases","Algorithms;Databases, Factual;Laparoscopy;Neural Networks (Computer)","93","35","","","","","IEEE","IEEE Journals"
"DNN-Based Feature Extraction for Conflict Intensity Estimation From Speech","G. Gosztolya; L. Tóth","MTA-SZTE Research Group on Artificial Intelligence, Szeged, Hungary; MTA-SZTE Research Group on Artificial Intelligence, Szeged, Hungary","IEEE Signal Processing Letters","","2017","24","12","1837","1841","Over past few years, there has been an increasing need to extract nonlinguistic information from audio sources. This trend has created a new area in speech technology known as computational paralinguistics. A task belonging to this area is to estimate the intensity of conflicts arising in speech recordings, based only on the audio information. It was shown that the human comprehension of conflict intensity is closely related to speaker overlap; that is, when multiple persons are speaking at the same time. This type of information can also aid automated conflict intensity estimation. In this study, we propose a simple, DNN-based feature extraction step, and show that this approach is superior to those introduced in the literature so far: By combining our results with an efficient greedy feature selection algorithm, we were able to outperform all previous results on the SSPNet conflict dataset, achieving a correlation coefficient of 0.856 on the test set.","","","10.1109/LSP.2017.2736008","János Bolyai Research Scholarship of the Hungarian Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8002607","Computational paralinguistics;conflict intensity estimation;deep neural networks (DNNs);feature extraction","Feature extraction;Estimation;Speech processing;Correlation;Neural networks;Learning systems;Neural networks","estimation theory;feature extraction;greedy algorithms;neural nets;speech processing","computational paralinguistics;speech recordings;audio information;conflict intensity estimation;simple based feature extraction step;DNN-based feature extraction step;efficient greedy feature selection algorithm;SSPNet conflict dataset;audio sources;speech technology;nonlinguistic information extraction;correlation coefficient;deep neural network","","2","35","Traditional","","","","IEEE","IEEE Journals"
"Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review","W. Rawat; Z. Wang","Department of Electrical and Mining Engineering, University of South Africa, Florida 1710, South Africa; Department of Electrical and Mining Engineering, University of South Africa, Florida 1710, South Africa","Neural Computation","","2017","29","9","2352","2449","<para>Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art deep learning systems. Along the way, we analyze (1) their early successes, (2) their role in the deep learning renaissance, (3) selected symbolic works that have contributed to their recent popularity, and (4) several improvement attempts by reviewing contributions and challenges of over 300 publications. We also introduce some of their current trends and remaining challenges.</para>","","","10.1162/neco_a_00990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8016501","","","","","","67","","Traditional","","","","MITP",""
"Identifying infrastructure change in the fourth and fifth dimensions","M. Coleman; P. Carberry; R. Hoddenbach","Fugro Roames, Brisbane, Australia; Fugro Inc., Fredrick, USA; Fugro Geospatial, Leidschendam, Netherlands","CIRED - Open Access Proceedings Journal","","2017","2017","1","2772","2774","As humans we see the world in three dimension, many will recognise the fourth dimension (4D) as time, which can be represented by the detectable change in state of a given object between two points in time. In that sense we cannot see in 4D but as asset managers we experience asset performance and risk implications from it. While 3D addresses the questions ‘what is the object and where is it?’, 4D asks ‘How did it change?’. Identifying change at scale has been made possible by the advent and adoption of cloud computing and use of increasingly sophisticated deep learning computer algorithms which can automatically identify and quantify change in timeframes that are a fraction of those achieved through traditional human-based methods. We are now able to move modelling into the fifth dimension (5D), which postulates all possible scenarios, now we are asking ‘How could it change?’. This is extremely valuable because it allows us to quickly run complex simulations on extremely large datasets. For example, a utility can optimise their vegetation management strategy to meet certain cost or risk targets by virtually testing every possible combination of clearance standards.","","","10.1049/oap-cired.2017.1361","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8315881","","","Big Data;cloud computing;asset management;telecommunication computing","asset managers;asset performance;risk implications;change detection;cloud computing;cloud processing data;deep learning computer algorithms;human-based methods;big data;fifth dimension;vegetation management strategy;clearance standards;mapped objects","","","","","","","","IET","IET Journals"
"Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge","O. Vinyals; A. Toshev; S. Bengio; D. Erhan","Department of Research, Google, Mountain View, CA; Department of Research, Google, Mountain View, CA; Department of Research, Google, Mountain View, CA; Department of Research, Google, Mountain View, CA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","4","652","663","Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.","","","10.1109/TPAMI.2016.2587640","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636","Image captioning;recurrent neural network;sequence-to-sequence;language model","Logic gates;Training;Recurrent neural networks;Visualization;Computer vision;Computational modeling;Natural languages","artificial intelligence;computer vision;language translation;natural language processing;optimisation;recurrent neural nets","image captioning;artificial intelligence;computer vision;natural language processing;deep recurrent architecture;machine translation;natural sentence generation;likelihood maximization;target description sentence","","140","49","","","","","IEEE","IEEE Journals"
"Task Intelligence of Robots: Neural Model-Based Mechanism of Thought and Online Motion Planning","I. Jeong; W. Ko; G. Park; D. Kim; Y. Yoo; J. Kim","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejon, South Korea","IEEE Transactions on Emerging Topics in Computational Intelligence","","2017","1","1","41","50","The crux of the realization of task intelligence for robots is to design the memory module for storing temporal event sequences of tasks, the mechanism of thought for reasoning, and motion planning methodology for execution, among others. In this paper, task intelligence is realized using episodic memory, neural model-based mechanism of thought, and an online motion planning algorithm. Robots are taught either by demonstration or symbolic description. A behavior appropriate to the current situation is selected by the developmental episodic memory-based mechanism of thought, while a proper task is retrieved from Deep adaptive resonance theory (ART). The behaviors are executed safely and quickly with the proposed motion planning algorithm. The effectiveness and applicability of task intelligence are demonstrated through experiments with the humanoid robot, Mybot, developed in the Robot Intelligence Technology Laboratory at KAIST.","","","10.1109/TETCI.2016.2645720","Technology Innovation Program; Ministry of Trade, Industry & Energy (MOTIE); National Research Foundation of Korea (NRF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801006","Episodic memory;mechanism of thought;motion planning;task intelligence","Subspace constraints;Planning;Trajectory;Service robots;Cognition;Context","adaptive resonance theory;humanoid robots;learning (artificial intelligence);motion control;neurocontrollers;path planning;robot programming","task intelligence;neural model;memory module;online motion planning algorithm;developmental episodic memory;Robot Intelligence Technology Laboratory;Deep adaptive resonance theory;humanoid robot;Mybot;robot learning","","11","30","","","","","IEEE","IEEE Journals"
"Automatic Road Detection and Centerline Extraction via Cascaded End-to-End Convolutional Neural Network","G. Cheng; Y. Wang; S. Xu; H. Wang; S. Xiang; C. Pan","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","6","3322","3337","Accurate road detection and centerline extraction from very high resolution (VHR) remote sensing imagery are of central importance in a wide range of applications. Due to the complex backgrounds and occlusions of trees and cars, most road detection methods bring in the heterogeneous segments; besides for the centerline extraction task, most current approaches fail to extract a wonderful centerline network that appears smooth, complete, as well as single-pixel width. To address the above-mentioned complex issues, we propose a novel deep model, i.e., a cascaded end-to-end convolutional neural network (CasNet), to simultaneously cope with the road detection and centerline extraction tasks. Specifically, CasNet consists of two networks. One aims at the road detection task, whose strong representation ability is well able to tackle the complex backgrounds and occlusions of trees and cars. The other is cascaded to the former one, making full use of the feature maps produced formerly, to obtain the good centerline extraction. Finally, a thinning algorithm is proposed to obtain smooth, complete, and single-pixel width road centerline network. Extensive experiments demonstrate that CasNet outperforms the state-of-the-art methods greatly in learning quality and learning speed. That is, CasNet exceeds the comparing methods by a large margin in quantitative performance, and it is nearly 25 times faster than the comparing methods. Moreover, as another contribution, a large and challenging road centerline data set for the VHR remote sensing image will be publicly available for further studies.","","","10.1109/TGRS.2017.2669341","National Natural Science Foundation of China; Beijing Natural Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7873262","Cascaded convolutional neural network (CasNet);end-to-end;road centerline extraction;road detection","Roads;Feature extraction;Remote sensing;Automobiles;Neural networks;Image segmentation;Data mining","geophysical techniques;neural nets;remote sensing","automatic road detection methods;centerline extraction;complex backgrounds;smooth centerline network;single-pixel width;deep model;CasNet;cascaded end-to-end convolutional neural network;tree occlusion;car occlusion;thinning algorithm;road centerline network;learning quality;learning speed;VHR remote sensing image;very high resolution remote sensing imagery","","65","54","","","","","IEEE","IEEE Journals"
"Closed-Loop Modulation of the Pathological Disorders of the Basal Ganglia Network","C. Liu; J. Wang; H. Li; M. Lu; B. Deng; H. Yu; X. Wei; C. Fietkiewicz; K. A. Loparo","School of Electrical Engineering and Automation, Tianjin University, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University, Tianjin, China; School of Automation and Electrical Engineering, Tianjin University of Technology and Education, Tianjin, China; School of Informational Technology and Engineering, Tianjin University of Technology and Education, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University, Tianjin, China; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, OH, USA; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, OH, USA","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","2","371","382","A generalized predictive closed-loop control strategy to improve the basal ganglia activity patterns in Parkinson's disease (PD) is explored in this paper. Based on system identification, an input-output model is established to reveal the relationship between external stimulation and neuronal responses. The model contributes to the implementation of the generalized predictive control (GPC) algorithm that generates the optimal stimulation waveform to modulate the activities of neuronal nuclei. By analyzing the roles of two critical control parameters within the GPC law, optimal closed-loop control that has the capability of restoring the normal relay reliability of the thalamus with the least stimulation energy expenditure can be achieved. In comparison with open-loop deep brain stimulation and traditional static control schemes, the generalized predictive closed-loop control strategy can optimize the stimulation waveform without requiring any particular knowledge of the physiological properties of the system. This type of closed-loop control strategy generates an adaptive stimulation waveform with low energy expenditure with the potential to improve the treatments for PD.","","","10.1109/TNNLS.2015.2508599","China Scholarship Council’s Study Abroad Project; Natural Science Foundation of Tianjin, China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381666","Basal ganglia (BG) network;energy expenditure;generalized predictive closed-loop control;Parkinsonian state;system identification","Neurons;Computational modeling;Pathology;Data models;Adaptation models;Modulation;Basal ganglia","brain;closed loop systems;diseases;identification;medical disorders;neurophysiology;open loop systems;optimal control;patient treatment;predictive control","generalized predictive closed-loop control;basal ganglia activity patterns;Parkinson's disease;system identification;input-output model;external stimulation;neuronal responses;optimal stimulation waveform;neuronal nuclei;GPC law;optimal closed-loop control;normal relay reliability;thalamus;least stimulation energy expenditure;open-loop deep brain stimulation;static control;physiological properties;PD treatment;pathological disorders;basal ganglia network;closed-loop modulation","","6","48","","","","","IEEE","IEEE Journals"
"Recent progress in deep end-to-end models for spoken language processing","K. Audhkhasi; A. Rosenberg; G. Saon; A. Sethy; B. Ramabhadran; S. Chen; M. Picheny","NA; NA; NA; NA; NA; NA; NA","IBM Journal of Research and Development","","2017","61","4/5","2:1","2:10","End-to-end models (or sequence-to-sequence models) based on deep neural networks have recently become popular within the machine learning community. These techniques are also increasingly used in automatic speech recognition as an alternative to the state-of-the-art, hybrid HMM-DNN (hidden Markov model, deep neural network) system. The end-to-end systems contain a purely neural architecture that eliminates the need of any time alignment between the input acoustic feature vector sequence and output phone sequence. In this paper, we present progress within the IBM Watson Multimodal Group on end-to-end models for spoken language processing. We present our work on two types of end-to-end models applied to speech-to-text and keyword search tasks, namely, 1) recurrent neural networks (RNNs) based on connectionist temporal classification loss, and 2) attention-based encoder–decoder RNNs. We present results on several languages (such as Pashto, Mongolian, Javanese, Amharic, Guarani, Dholuo, Igbo, and Georgian) from the Intelligence Advanced Research Projects Activity funded Babel Program. We also present a detailed analysis of some salient characteristics of these models compared with the state-of-the-art HMM-DNN hybrid systems, and also discuss future challenges in using such models for spoken language processing.","","","10.1147/JRD.2017.2701207","Intelligence Advanced Research Projects; Department of Defense; U.S. Army Research Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030300","","Hidden Markov models;Acoustics;Decoding;Training;Recurrent neural networks;Mathematical model","","","","","15","","","","","IBM","IBM Journals"
"Neurofeedback Control in Parkinsonian Patients Using Electrocorticography Signals Accessed Wirelessly With a Chronic, Fully Implanted Device","P. Khanna; N. C. Swann; C. de Hemptinne; S. Miocinovic; A. Miller; P. A. Starr; J. M. Carmena","The UC Berkeley-UCSF Graduate Program in Bioengineering, Berkeley, CA, USA; Department of Neurological Surgery, University of California, San Francisco, CA, USA; Department of Neurological Surgery, University of California, San Francisco, CA, USA; Department of Neurology, University of California, San Francisco, CA, USA; Department of Neurological Surgery, University of California, San Francisco, CA, USA; Department of Neurological Surgery, University of California, San Francisco, CA, USA; The UC Berkeley-UCSF Graduate Program in Bioengineering, Berkeley, CA, USA","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","2017","25","10","1715","1724","Parkinson's disease (PD) is characterized by motor symptoms such as rigidity and bradykinesia that prevent normal movement. Beta band oscillations (13-30 Hz) in neural local field potentials (LFPs) have been associated with these motor symptoms. Here, three PD patients implanted with a therapeutic deep brain neural stimulator that can also record and wirelessly stream neural data played a neurofeedback game where they modulated their beta band power from sensorimotor cortical areas. Patients' beta band power was streamed in real-time to update the position of a cursor that they tried to drive into a cued target. After playing the game for 1-2 hours each, all three patients exhibited above chance-level performance regardless of subcortical stimulation levels. This study, for the first time, demonstrates using an invasive neural recording system for at-home neurofeedback training. Future work will investigate chronic neurofeedback training as a potentially therapeutic tool for patients with neurological disorders.","","","10.1109/TNSRE.2016.2597243","National Science Foundation Graduate Research Fellowship; Defense Advanced Research Projects Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7549053","Brain–computer interfaces;neurofeedback;Parkinson’s disease","Neurofeedback;Training;Time-domain analysis;Real-time systems;Medical treatment;Lead;Channel estimation","bioelectric potentials;diseases;feedback;patient care;prosthetics","neurofeedback control;parkinsonian patients;electrocorticography signals;fully implanted device;Parkinson's disease;motor symptoms;rigidity;bradykinesia;beta band oscillations;local field potentials;therapeutic deep brain neural stimulator;sensorimotor cortical areas;invasive neural recording system;at-home neurofeedback training;neurological disorders;time 1 hour to 2 hour","Algorithms;Beta Rhythm;Brain-Computer Interfaces;Electrocorticography;Electrodes, Implanted;Equipment Design;Games, Experimental;Humans;Learning;Male;Middle Aged;Neurofeedback;Parkinsonian Disorders;Sensorimotor Cortex;Wireless Technology","3","43","Traditional","","","","IEEE","IEEE Journals"
"Hierarchical Context Modeling for Video Event Recognition","X. Wang; Q. Ji","Nokia Bell Labs, Murray Hill, NJ; Department of Electrical, Computer and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","9","1770","1782","Current video event recognition research remains largely target-centered. For real-world surveillance videos, target-centered event recognition faces great challenges due to large intra-class target variation, limited image resolution, and poor detection and tracking results. To mitigate these challenges, we introduced a context-augmented video event recognition approach. Specifically, we explicitly capture different types of contexts from three levels including image level, semantic level, and prior level. At the image level, we introduce two types of contextual features including the appearance context features and interaction context features to capture the appearance of context objects and their interactions with the target objects. At the semantic level, we propose a deep model based on deep Boltzmann machine to learn event object representations and their interactions. At the prior level, we utilize two types of prior-level contexts including scene priming and dynamic cueing. Finally, we introduce a hierarchical context model that systematically integrates the contextual information at different levels. Through the hierarchical context model, contexts at different levels jointly contribute to the event recognition. We evaluate the hierarchical context model for event recognition on benchmark surveillance video datasets. Results show that incorporating contexts in each level can improve event recognition performance, and jointly integrating three levels of contexts through our hierarchical model achieves the best performance.","","","10.1109/TPAMI.2016.2616308","Defense Advanced Research Projects Agency; Army Research Office; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7588132","Hierarchical context model;event recognition;image context;semantic context;priming context","Context;Context modeling;Semantics;Hidden Markov models;Image recognition;Target recognition","Boltzmann machines;image recognition;image representation;learning (artificial intelligence);video signal processing;video surveillance","hierarchical context modeling;video event recognition;surveillance videos;image resolution;image level;semantic level;prior level;deep Boltzmann machine;object representations;scene priming;dynamic cueing","","4","62","","","","","IEEE","IEEE Journals"
"An Ensemble of Invariant Features for Person Reidentification","Y. Lee; S. Chen; J. Hwang; Y. Hung","Department of Electrical Engineering, University of Washington, Seattle, WA, USA; Huawei Research and Development, Santa Clara, CA, USA; Department of Electrical Engineering, University of Washington, Seattle, WA, USA; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan","IEEE Transactions on Circuits and Systems for Video Technology","","2017","27","3","470","483","This paper proposes an ensemble of invariant features (EIFs), which can properly handle the variations of color difference and human poses/viewpoints for matching pedestrian images observed in different cameras with nonoverlapping field of views. Our proposed method is a direct reidentification (re-id) method, which requires no prior domain learning based on prelabeled corresponding training data. The novel features consist of the holistic and region-based features. The holistic features are extracted by using a publicly available pretrained deep convolutional neural network used in generic object classification. In contrast, the region-based features are extracted based on our proposed two-way Gaussian mixture model fitting, which overcomes the self-occlusion and pose variations. To make a better generalization during recognizing identities without additional learning, the ensemble scheme aggregates all the feature distances using the similarity normalization. The proposed framework achieves robustness against partial occlusion, pose, and viewpoint changes. Moreover, the evaluation results show that our method outperforms the state-of-the-art direct re-id methods on the challenging benchmark viewpoint invariant pedestrian recognition and 3D people surveillance data sets.","","","10.1109/TCSVT.2016.2637818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7779129","3D people surveillance (3DPeS);deep convolutional neural network (DCNN);ensembles of invariant features (EIFs);person reidentification (re-id);two-way Gaussian mixture model fitting (2WGMMF);viewpoint invariant pedestrian recognition (VIPeR)","Feature extraction;Image color analysis;Cameras;Training data;Measurement;Three-dimensional displays;Visualization","feature extraction;Gaussian processes;image classification;image colour analysis;image matching;mixture models;neural nets;pedestrians","person reidentification;ensemble of invariant features;EIFs;color difference variations;pedestrian image matching;direct reidentification method;region-based feature extraction;holistic feature extraction;deep convolutional neural network;generic object classification;two-way Gaussian mixture model;pose variations;self-occlusion;identity recognition;feature distances;similarity normalization;partial occlusion;benchmark viewpoint invariant pedestrian recognition;3D people surveillance data sets;cameras;nonoverlapping field of views","","8","48","","","","","IEEE","IEEE Journals"
"Super Resolution of Light Field Images Using Linear Subspace Projection of Patch-Volumes","R. A. Farrugia; C. Galea; C. Guillemot","Department of Communications and Computer Engineering, University of Malta, Msida, Malta; Department of Communications and Computer Engineering, University of Malta, Msida, Malta; Institut National de Recherche en Informatique et en Automatique, Rennes, France","IEEE Journal of Selected Topics in Signal Processing","","2017","11","7","1058","1071","Light field imaging has emerged as a very promising technology in the field of computational photography. Cameras are becoming commercially available for capturing real-world light fields. However, capturing high spatial resolution light fields remains technologically challenging, and the images rendered from real light fields have today a significantly lower spatial resolution compared to traditional two-dimensional (2-D) cameras. This paper describes an example-based super-resolution algorithm for light fields, which allows the increase of the spatial resolution of the different views in a consistent manner across all subaperture images of the light field. The algorithm learns linear projections between subspaces of reduced dimension in which reside patch-volumes extracted from the light field. The method is extended to cope with angular super-resolution, where 2-D patches of intermediate subaperture images are approximated from neighboring subaperture images using multivariate ridge regression. Experimental results show significant quality improvement when compared to state-of-the-art single-image super-resolution methods applied on each view separately, as well as when compared to a recent light field super-resolution techniques based on deep learning.","","","10.1109/JSTSP.2017.2747127","EU H2020 Research and Innovation Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8022880","Dimensionality reduction;learning;light-fields;super-resolution","Spatial resolution;Algorithm design and analysis;Two dimensional displays;Cameras;Light fields;Signal processing algorithms;Machine learning","image resolution;rendering (computer graphics)","light field images;light field imaging;high spatial resolution light fields;super-resolution algorithm;angular super-resolution;intermediate subaperture images;single-image super-resolution methods;light field super-resolution techniques","","11","30","Traditional","","","","IEEE","IEEE Journals"
"Deep Pain: Exploiting Long Short-Term Memory Networks for Facial Expression Classification","P. Rodriguez; G. Cucurull; J. Gonzàlez; J. M. Gonfaus; K. Nasrollahi; T. B. Moeslund; F. X. Roca","Computer Vision Center, Universitat Autònoma de Barcelona, 08193 Barcelona, Spain.; Computer Vision Center, Universitat Autònoma de Barcelona, 08193 Barcelona, Spain.; Computer Vision Center, Universitat Autònoma de Barcelona, 08193 Barcelona, Spain, and also with the Visual Tagging Services, Campus UAB Barcelona, 08193 Barcelona, Spain.; Visual Tagging Services, Campus UAB Barcelona, Barcelona, Spain.; Aalborg University, 9100 Aalborg, Denmark.; Aalborg University, 9100 Aalborg, Denmark.; Computer Vision Center, Universitat Autònoma de Barcelona, 08193 Barcelona, Spain, and also with the Visual Tagging Services, Campus UAB Barcelona, 08193 Barcelona, Spain.","IEEE Transactions on Cybernetics","","2017","PP","99","1","11","Pain is an unpleasant feeling that has been shown to be an important factor for the recovery of patients. Since this is costly in human resources and difficult to do objectively, there is the need for automatic systems to measure it. In this paper, contrary to current state-of-the-art techniques in pain assessment, which are based on facial features only, we suggest that the performance can be enhanced by feeding the raw frames to deep learning models, outperforming the latest state-of-the-art results while also directly facing the problem of imbalanced data. As a baseline, our approach first uses convolutional neural networks (CNNs) to learn facial features from VGG_Faces, which are then linked to a long short-term memory to exploit the temporal relation between video frames. We further compare the performances of using the so popular schema based on the canonically normalized appearance versus taking into account the whole image. As a result, we outperform current state-of-the-art area under the curve performance in the UNBC-McMaster Shoulder Pain Expression Archive Database. In addition, to evaluate the generalization properties of our proposed methodology on facial motion recognition, we also report competitive results in the Cohn Kanade+ facial expression database.","","","10.1109/TCYB.2017.2662199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849133","Affective computing;computer applications;cybercare industry applications;human factors engineering in medicine and biology;medical services;monitoring;patient monitoring computers and information processing;pattern recognition","Pain;Feature extraction;Hidden Markov models;Face;Estimation;Databases;Face recognition","","","","22","","","","","","IEEE","IEEE Early Access Articles"
"Universum Autoencoder-Based Domain Adaptation for Speech Emotion Recognition","J. Deng; X. Xu; Z. Zhang; S. Frühholz; B. Schuller","Chair of Complex and Intelligent Systems, University of Passau, Passau, Germany; Machine Intelligence and Signal Processing Group, Technische Universität München, Munich, Germany; Chair of Complex and Intelligent Systems, University of Passau, Passau, Germany; Institute of Psychology, University of Zurich, Zurich, Switzerland; Chair of Complex and Intelligent Systems, University of Passau, Passau, Germany","IEEE Signal Processing Letters","","2017","24","4","500","504","One of the serious obstacles to the applications of speech emotion recognition systems in real-life settings is the lack of generalization of the emotion classifiers. Many recognition systems often present a dramatic drop in performance when tested on speech data obtained from different speakers, acoustic environments, linguistic content, and domain conditions. In this letter, we propose a novel unsupervised domain adaptation model, called Universum autoencoders, to improve the performance of the systems evaluated in mismatched training and test conditions. To address the mismatch, our proposed model not only learns discriminative information from labeled data, but also learns to incorporate the prior knowledge from unlabeled data into the learning. Experimental results on the labeled Geneva Whispered Emotion Corpus database plus other three unlabeled databases demonstrate the effectiveness of the proposed method when compared to other domain adaptation methods.","","","10.1109/LSP.2017.2672753","BMBF; Swiss National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862157","Deep learning;domain adaptation;speech emotion recognition;universum autoencoders (  $\mathfrak {U}$  -AE)","Speech;Speech recognition;Training;Emotion recognition;Databases;Decoding;Neural networks","emotion recognition;speech recognition;vocoders","universum autoencoder-based domain adaptation;speech emotion recognition systems;acoustic environments;domain adaptation methods;Geneva whispered emotion corpus database;Universum autoencoders;unsupervised domain adaptation model","","27","27","","","","","IEEE","IEEE Journals"
"Recurrent Neural Networks to Correct Satellite Image Classification Maps","E. Maggiori; G. Charpiat; Y. Tarabalka; P. Alliez","TITANE Team, Inria, Université Côte d’Azur, Sophia Antipolis, France; Tao Team, Inria, Inria Saclay–Île-de-France, Palaiseau, France; TITANE Team, Inria, Université Côte d’Azur, Sophia Antipolis, France; TITANE Team, Inria, Université Côte d’Azur, Sophia Antipolis, France","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","9","4962","4971","While initially devised for image categorization, convolutional neural networks (CNNs) are being increasingly used for the pixelwise semantic labeling of images. However, the proper nature of the most common CNN architectures makes them good at recognizing but poor at localizing objects precisely. This problem is magnified in the context of aerial and satellite image labeling, where a spatially fine object outlining is of paramount importance. Different iterative enhancement algorithms have been presented in the literature to progressively improve the coarse CNN outputs, seeking to sharpen object boundaries around real image edges. However, one must carefully design, choose, and tune such algorithms. Instead, our goal is to directly learn the iterative process itself. For this, we formulate a generic iterative enhancement process inspired from partial differential equations, and observe that it can be expressed as a recurrent neural network (RNN). Consequently, we train such a network from manually labeled data for our enhancement task. In a series of experiments, we show that our RNN effectively learns an iterative process that significantly improves the quality of satellite image classification maps.","","","10.1109/TGRS.2017.2697453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7938635","Deep learning;image classification;recurrent neural networks","Labeling;Context;Satellites;Algorithm design and analysis;Image edge detection;Recurrent neural networks;Semantics","geophysical image processing;image classification;neural nets","recurrent neural networks;satellite image classification maps;image categorization;convolutional neural networks;pixelwise semantic labeling;aerial image labeling;satellite image labeling;iterative process;generic iterative enhancement process;partial differential equations;recurrent neural network","","12","34","","","","","IEEE","IEEE Journals"
"Robust Face Super-Resolution via Locality-Constrained Low-Rank Representation","T. Lu; Z. Xiong; Y. Zhang; B. Wang; T. Lu","Wuhan Institute of Technology, Wuhan, China; Texas A&M University, College Station, TX, USA; Wuhan Institute of Technology, Wuhan, China; Texas A&M University, College Station, TX, USA; Wuhan Institute of Technology, Wuhan, China","IEEE Access","","2017","5","","13103","13117","Learning-based face super-resolution relies on obtaining accurate a priori knowledge from the training data. Representation-based approaches (e.g., sparse representation-based and neighbor embedding-based schemes) decompose the input images using sophisticated regularization techniques. They give reasonably good reconstruction performance. However, in real application scenarios, the input images are often noisy, blurry, or suffer from other unknown degradations. Traditional face super-resolution techniques treat image noise at the pixel level without considering the underlying image structures. In order to rectify this shortcoming, we propose in this paper a unified framework for representation-based face super-resolution by introducing a locality-constrained low-rank representation (LLR) scheme to reveal the intrinsic structures of input images. The low-rank representation part of LLR clusters an input image into the most accurate subspace from a global dictionary of atoms, while the locality constraint enables recovery of local manifold structures from local patches. In addition, low-rank, sparsity, locality, accuracy, and robustness of the representation coefficients are exploited in LLR via regularization. Experiments on the FEI, CMU face database, and real surveillance scenario show that LLR outperforms the state-of-the-art face super-resolution algorithms (e.g., convolutional neural network-based deep learning) both objectively and subjectively.","","","10.1109/ACCESS.2017.2717963","National Natural Science Foundation of China; Natural Science Foundation of Hubei Province of China; Scientific Research Foundation of Wuhan Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954766","Face super-resolution;low-rank representation;locality constraint;alternating direction method of multiplier","Face;Image resolution;Dictionaries;Manifolds;Image reconstruction;Robustness;Testing","face recognition;image representation;image resolution;learning (artificial intelligence);minimisation;visual databases","robust face super-resolution;learning-based face super-resolution;unified framework;representation-based face super-resolution;locality-constrained low-rank representation scheme;locality-constrained LLR clusters;intrinsic structures;input images;input image;global dictionary;local manifold structures;local patches;representation coefficients;FEI face database;CMU face database;surveillance scenario;locality constraint","","11","45","","","","","IEEE","IEEE Journals"
"3-D BLE Indoor Localization Based on Denoising Autoencoder","C. Xiao; D. Yang; Z. Chen; G. Tan","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Chinese Academy of Sciences, SIAT, Shenzhen, China","IEEE Access","","2017","5","","12751","12760","Bluetooth low energy (BLE)-based indoor localization has attracted increasing interests for its low-cost, low-power consumption, and ubiquitous availability in mobile devices. In this paper, a novel denoising autoencoder-based BLE indoor localization (DABIL) method is proposed to provide high-performance 3-D positioning in large indoor places. A deep learning model, called denoising autoencoder, is adopted to extract robust fingerprint patterns from received signal strength indicator measurements, and a fingerprint database is constructed with reference locations in 3-D space, rather than traditional 2-D plane. Field experiments show that 3-D space fingerprinting can effectively increase positioning accuracy, and DABIL performs the best in terms of both horizontal accuracy and vertical accuracy, comparing with a traditional fingerprinting method and a deep learning-based method. Moreover, it can achieve stable performance with incomplete beacon measurements due to unpredictable BLE beacon lost.","","","10.1109/ACCESS.2017.2720164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7959171","Bluetooth low energy;indoor localization;fingerprinting;denoising autoencoder","Noise reduction;Three-dimensional displays;Databases;Wireless fidelity;Robustness;Wireless communication;Receivers","Bluetooth;indoor radio;RSSI;signal denoising;telecommunication power management","3D BLE indoor localization;denoising autoencoder;Bluetooth low energy;low-power consumption;mobile devices;deep learning model;fingerprint patterns;received signal strength indicator;fingerprint database","","15","30","","","","","IEEE","IEEE Journals"
"Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval","X. Wei; J. Luo; J. Wu; Z. Zhou","National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Image Processing","","2017","26","6","2868","2881","Deep convolutional neural network models pre-trained for the ImageNet classification task have been successfully adopted to tasks in other domains, such as texture description and object proposal generation, but these tasks require annotations for images in the new domain. In this paper, we focus on a novel and challenging task in the pure unsupervised setting: fine-grained image retrieval. Even with image labels, fine-grained images are difficult to classify, letting alone the unsupervised retrieval task. We propose the selective convolutional descriptor aggregation (SCDA) method. The SCDA first localizes the main object in fine-grained images, a step that discards the noisy background and keeps useful deep descriptors. The selected descriptors are then aggregated and the dimensionality is reduced into a short feature vector using the best practices we found. The SCDA is unsupervised, using no image label or bounding box annotation. Experiments on six fine-grained data sets confirm the effectiveness of the SCDA for fine-grained image retrieval. Besides, visualization of the SCDA features shows that they correspond to visual attributes (even subtle ones), which might explain SCDA's high-mean average precision in fine-grained retrieval. Moreover, on general image retrieval data sets, the SCDA achieves comparable retrieval results with the state-of-the-art general image retrieval approaches.","","","10.1109/TIP.2017.2688133","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7887720","Fine-grained image retrieval;selection and aggregation;unsupervised object localization","Image retrieval;Machine learning;Dogs;Convolution;Automobiles;Buildings;Birds","feature extraction;image classification;image retrieval;neural nets","selective convolutional descriptor aggregation;fine-grained image retrieval;deep convolutional neural network model;imageNet classification;unsupervised retrieval task;selective convolutional descriptor aggregation method;SCDA method;SCDA feature visualization;visual attribute;state-of-the-art general image retrieval approach","","63","46","","","","","IEEE","IEEE Journals"
"Separation of a Mixture of Simultaneous Dual-Tracer PET Signals: A Data-Driven Approach","D. Ruan; H. Liu","State Key Laboratory of Modern Optical Instrumentation, College of Optical Science and Engineering, Zhejiang University, Hangzhou, China; State Key Laboratory of Modern Optical Instrumentation, College of Optical Science and Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Nuclear Science","","2017","64","9","2588","2597","In developing a signal separation algorithm, an obvious challenge relates to the ways in which the photon measurements from dual-tracer positron emission tomography (PET) are overlapped. Aside from this issue, the ill-conditioning and the noisy measures are still obstacles to achieve accurate and robust result. In this paper, we develop a novel approach from a data-driven perspective that separates dynamic dual-tracer PET signals into individual-tracer components explicitly with a simultaneous-injection single-scan. Our approach is divided into two phases: training and reconstruction. In the training phase, our proposed framework will learn the potential relationship of different frames for dynamic dual-tracer PET data. In the reconstruction phase, given the sinogram of a dual tracer, the PET activity map of an individual tracer can be obtained directly. Experiments on Monte Carlo simulation data sets are conducted as validation. The experimental results have demonstrated the superior performance of the proposed approach by comparing with the ground truth of the individual-tracer.","","","10.1109/TNS.2017.2736644","National Natural Science Foundation of China; National Key Technology Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8003306","Deep learning;dual-tracer;positron emission tomography (PET);signal separation","Image reconstruction;Imaging;Training;Kinetic theory;Source separation;Photonics;Heuristic algorithms","Monte Carlo methods;positron emission tomography","data-driven approach;signal separation algorithm;photon measurements;dual-tracer positron emission tomography signals;individual-tracer components;dynamic dual-tracer PET signals;simultaneous-injection single-scan;dynamic dual-tracer PET data;Monte Carlo simulation data","","1","30","OAPA","","","","IEEE","IEEE Journals"
"Turn Signal Detection During Nighttime by CNN Detector and Perceptual Hashing Tracking","L. Chen; X. Hu; T. Xu; H. Kuang; Q. Li","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Information Engineering, Hubei University, Wuhan, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; City University of Hong Kong, Hong Kong; Shenzhen University, Shenzhen, China","IEEE Transactions on Intelligent Transportation Systems","","2017","18","12","3303","3314","Detecting vehicle turn signals at night is critical for both assistant driving systems and autonomous driving systems. In this paper, we propose a novel method that consists of detection and tracking modules to achieve a high level of robustness. For nighttime vehicle detection, a Nakagami-image-based method is used to locate the regions containing vehicle lights. At the same time, a set of vehicle object proposals is generated using a region proposal network based on convolutional neural network (CNN) feature maps. Then, the light regions and proposals are combined to generate the regions of interest (ROIs) for the further detection. Vehicle candidates are extracted from the ROIs using a softmax classifier with CNN-based features. For the tracking module, we propose a perceptional hashing algorithm to track these vehicle candidates. During the tracking, turn signals are detected by analyzing the continuous intensity variation of the vehicle box sequences. Experimental results for typical sequences show that the proposed method can robustly detect and track a vehicle in front with over 95% accuracy and recognize the turning signals in night scenes with a detection rate of over 90%. The vehicle detection method improves the miss rate of state-of-the-art systems by more than 20%. In addition, the proposed vehicle tracking method outperforms other state-of-the-art systems.","","","10.1109/TITS.2017.2683641","National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; CCFTencent Open Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891988","Intelligent transportation system;assistant and autonomous driving;turn signals detection;deep convolutional neural network;hash perceptual tracking","Vehicle detection;Intelligent transportation systems;Feature extraction;Cameras;Nakagami distribution;Signal detection;Object detection;Autonomous vehicles","convolution;driver information systems;feature extraction;learning (artificial intelligence);neural nets;object detection;object tracking;road vehicles;signal detection;vehicles","turning signals;vehicle detection method;vehicle tracking method;turn signal detection;assistant driving systems;autonomous driving systems;tracking module;nighttime vehicle detection;Nakagami-image;vehicle lights;region proposal network;convolutional neural network;CNN;ROIs;perceptional hashing algorithm;vehicle box sequences;vehicle turn signal detection;vehicle object;regions of interest;softmax classifier","","13","49","","","","","IEEE","IEEE Journals"
"Influence of Arduino on the Development of Advanced Microcontrollers Courses","J. C. Martínez-Santos; O. Acevedo-Patino; S. H. Contreras-Ortiz","Program of Electrical and Electronic Engineering, Universidad Tecnológica de Bolívar, Cartagena de Indias, Colombia; Program of Electrical and Electronic Engineering, Universidad Tecnológica de Bolívar, Cartagena de Indias, Colombia; Program of Electrical and Electronic Engineering, Universidad Tecnológica de Bolívar, Cartagena de Indias, Colombia","IEEE Revista Iberoamericana de Tecnologias del Aprendizaje","","2017","12","4","208","217","This paper describes the development of courses in the field of digital design that use Arduino boards as their main platforms. Arduino offers an intuitive development environment and multiple hardware and software resources that allow rapid development of microcontroller-based projects. However, due to the vast amount of information available, students were losing the capability to design their own prototypes. We propose a methodology that introduces the study of microcontrollers using Arduino to develop different types of projects and proceeds to study the system architecture to gain control on the device. This methodology has been used in an undergraduate course in microcontrollers and a graduate course in advanced techniques in digital design. The students of the microcontrollers course showed improved design skills and motivation compared to the students from previous versions of the course. With respect to the advanced techniques in digital design course, the students were able to take advantage of Arduino platform to gain a deep understanding of hardware/software co-design of embedded systems.","","","10.1109/RITA.2017.2776444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8123929","Education courses;engineering education;embedded systems;microcontrollers;Arduino;project-based learning","Microcontrollers;Education courses;Learning systems;Embedded systems;Programming","computer aided instruction;computer science education;educational courses;electrical engineering education;embedded systems;further education;hardware-software codesign;microcontrollers","Arduino boards;multiple hardware;software resources;microcontroller-based projects;undergraduate course;graduate course;design skills;digital design course;hardware/software co-design;microcontroller courses;embedded systems","","6","44","","","","","IEEE","IEEE Journals"
"Class-Specific Kernel Discriminant Analysis Revisited: Further Analysis and Extensions","A. Iosifidis; M. Gabbouj","Department of Signal Processing, Tampere University of Technology, Tampere, Finland; Department of Signal Processing, Tampere University of Technology, Tampere, Finland","IEEE Transactions on Cybernetics","","2017","47","12","4485","4496","In this paper, we revisit class-specific kernel discriminant analysis (KDA) formulation, which has been applied in various problems, such as human face verification and human action recognition. We show that the original optimization problem solved for the determination of class-specific discriminant projections is equivalent to a low-rank kernel regression (LRKR) problem using training data-independent target vectors. In addition, we show that the regularized version of class-specific KDA is equivalent to a regularized LRKR problem, exploiting the same targets. This analysis allows us to devise a novel fast solution. Furthermore, we devise novel incremental, approximate and deep (hierarchical) variants. The proposed methods are tested in human facial image and action video verification problems, where their effectiveness and efficiency is shown.","","","10.1109/TCYB.2016.2612479","Academy of Finland Post-Doctoral Research Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590015","Approximation;class-specific kernel discriminant analysis (CSKDA);incremental learning;low-rank kernel regression (LRKR);regularization","Kernel;Training data;Optimization;Training;Data models;Principal component analysis;Learning systems","face recognition;regression analysis","class-specific KDA;regularized LRKR problem;human facial image;action video verification problems;class-specific kernel discriminant analysis formulation;human face verification;human action recognition;original optimization problem;class-specific discriminant projections;low-rank kernel regression problem;training data-independent target vectors","","2","58","","","","","IEEE","IEEE Journals"
"Shape Synthesis from Sketches via Procedural Models and Convolutional Networks","H. Huang; E. Kalogerakis; E. Yumer; R. Mech","University of Massachusetts Amherst, Amherst, MA; University of Massachusetts Amherst, Amherst, MA; Adobe Research, San Jose, CA; Adobe Research, San Jose, CA","IEEE Transactions on Visualization and Computer Graphics","","2017","23","8","2003","2013","Procedural modeling techniques can produce high quality visual content through complex rule sets. However, controlling the outputs of these techniques for design purposes is often notoriously difficult for users due to the large number of parameters involved in these rule sets and also their non-linear relationship to the resulting content. To circumvent this problem, we present a sketch-based approach to procedural modeling. Given an approximate and abstract hand-drawn 2D sketch provided by a user, our algorithm automatically computes a set of procedural model parameters, which in turn yield multiple, detailed output shapes that resemble the user's input sketch. The user can then select an output shape, or further modify the sketch to explore alternative ones. At the heart of our approach is a deep Convolutional Neural Network (CNN) that is trained to map sketches to procedural model parameters. The network is trained by large amounts of automatically generated synthetic line drawings. By using an intuitive medium, i.e., freehand sketching as input, users are set free from manually adjusting procedural model parameters, yet they are still able to create high quality content. We demonstrate the accuracy and efficacy of our method in a variety of procedural modeling scenarios including design of man-made and organic shapes.","","","10.1109/TVCG.2016.2597830","US National Science Foundation; NVidia for GPU donations; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530838","Shape synthesis;convolutional neural networks;procedural modeling;sketch-based modeling","Shape;Computational modeling;Three-dimensional displays;Two dimensional displays;Neural networks;Solid modeling;Computer architecture","convolution;data visualisation;learning (artificial intelligence);shape recognition","shape synthesis;procedural models;high quality visual content;complex rule sets;hand-drawn 2D sketch;deep convolutional neural network;CNN;network training;automatically generated synthetic line drawings;intuitive medium","","1","48","","","","","IEEE","IEEE Journals"
"Single Sideband Frequency Offset Estimation and Correction for Quality Enhancement and Speaker Recognition","H. Xing; J. H. L. Hansen","CRSS: Center for Robust Speech Systems, Jonsson School of Engineering and Computer Science, University of Texas at Dallas, Richardson, TX, USA; CRSS: Center for Robust Speech Systems, Jonsson School of Engineering and Computer Science, University of Texas at Dallas, Richardson, TX, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","1","124","136","Communication system mismatch represents a major influence in the losses of both speech quality and speaker recognition system performance. Although microphone and handset differences have been considered for speaker recognition (e.g., NIST SRE), nonlinear communication system differences, such as modulation/demodulation (Mod/DeMod) carrier mismatch, have yet to be explored. While such mismatch was common in traditional analog communications, today, with the diversity and blending of communication technologies, it is reconsidered as a major distortion. This paper is focused on estimating and correcting the frequency-shift distortion resulting from Mod/DeMod carrier frequency mismatch in high-frequency single sideband (HF-SSB) speech. To overcome the drawbacks of existing solutions, a two-step algorithm is proposed to improve estimation performance. In the first step, the offset of speech is scaled to a small frequency interval, which eliminates or reduces the nonuniqueness issue due to the periodicity within the spectrum; the second step performs fine tuning within the estimated predetermined uniqueness interval (UI). For the first time, a statistical framework is developed for UI detection, where an innovative acoustic feature is proposed to represent alternative frequency shifts. Additionally, in the estimation process, statistical techniques such as GMM-SVM, i-Vector, and deep neural networks are applied in the first step to improve the estimation accuracy. An evaluation using DARPA RATS HF-SSB data shows that the proposed algorithm achieves a significant improvement in the estimation performance (up to +35.6% improvement in accuracy), speech quality measurement (up to +27.3% relative improvement in the PESQ score), and speaker verification (up to +59.9% relative improvement in equal error rate).","","","10.1109/TASLP.2016.2623563","NIH; Air Force Research Laboratory; University of Texas at Dallas from the Distinguished University Chair in Telecommunications Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727986","Single sideband (SSB);frequency offset;speech enhancement;speaker recognition","Speech;Estimation;Frequency estimation;Amplitude modulation;Speech recognition;Harmonic analysis;Distortion","demodulation;Gaussian processes;learning (artificial intelligence);mixture models;modulation;speaker recognition;speech processing;support vector machines","single sideband frequency offset estimation;single sideband frequency offset correction;quality enhancement;speaker recognition;microphone difference;handset difference;NIST SRE;nonlinear communication system differences;modulation carrier mismatch;demodulation carrier mismatch;Mod carrier mismatch;DeMod carrier mismatch;frequency-shift distortion estimation;frequency-shift distortion correction;high-frequency single sideband speech;uniqueness interval;statistical framework;GMM-SVM;i-Vector;deep neural networks;DARPA RATS HF-SSB data;speech quality measurement;speaker verification","","2","40","","","","","IEEE","IEEE Journals"
"Accurate Cervical Cell Segmentation from Overlapping Clumps in Pap Smear Images","Y. Song; E. Tan; X. Jiang; J. Cheng; D. Ni; S. Chen; B. Lei; T. Wang","National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Shenzhen University, Shenzhen, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Shenzhen University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Shenzhen University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Shenzhen University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Shenzhen University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Medical Imaging","","2017","36","1","288","300","Accurate segmentation of cervical cells in Pap smear images is an important step in automatic pre-cancer identification in the uterine cervix. One of the major segmentation challenges is overlapping of cytoplasm, which has not been well-addressed in previous studies. To tackle the overlapping issue, this paper proposes a learning-based method with robust shape priors to segment individual cell in Pap smear images to support automatic monitoring of changes in cells, which is a vital prerequisite of early detection of cervical cancer. We define this splitting problem as a discrete labeling task for multiple cells with a suitable cost function. The labeling results are then fed into our dynamic multi-template deformation model for further boundary refinement. Multi-scale deep convolutional networks are adopted to learn the diverse cell appearance features. We also incorporated high-level shape information to guide segmentation where cell boundary might be weak or lost due to cell overlapping. An evaluation carried out using two different datasets demonstrates the superiority of our proposed method over the state-of-the-art methods in terms of segmentation accuracy.","","","10.1109/TMI.2016.2606380","National Natural Science Foundation of China; National Key Research and Development Project; (Key) Project of Department of Education of Guangdong Province; Shenzhen Key Basic Research Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7562400","Cervical cancer;dynamic multi-template deformation model;multi-scale convolutional networks;overlapping cells splitting;Pap smear screening","Image segmentation;Shape;Labeling;Cervical cancer;Deformable models;Image edge detection;Image color analysis","biomedical optical imaging;cancer;cellular biophysics;feature extraction;gynaecology;image segmentation;medical image processing;tumours","accurate cervical cell segmentation;overlapping clumps;Pap smear images;cervical cells;automatic precancer identification;uterine cervix;cytoplasm;learning-based method;automatic monitoring;cervical cancer detection;discrete labeling task;multiple cells;dynamic multitemplate deformation model;boundary refinement;multiscale deep convolutional networks;diverse cell appearance features;high-level shape information;cell boundary;cell overlapping;datasets","Algorithms;Female;Humans;Papanicolaou Test;Uterine Cervical Neoplasms","51","58","","","","","IEEE","IEEE Journals"
"Image-Based Appraisal of Real Estate Properties","Q. You; R. Pang; L. Cao; J. Luo","Department of Computer Science, University of Rochester, Rochester, NY, USA; PayPaL, San Jose, CA, USA; Electrical Engineering and Computer Sciences Department, Columbia University, New York, NY, USA; Department of Computer Science, University of Rochester, Rochester, NY, USA","IEEE Transactions on Multimedia","","2017","19","12","2751","2759","Real estate appraisal, which is the process of estimating the price for real estate properties, is crucial for both buyers and sellers as the basis for negotiation and transaction. Traditionally, the repeat sales model has been widely adopted to estimate real estate prices. However, it depends on the design and calculation of a complex economic-related index, which is challenging to estimate accurately. Today, real estate brokers provide easy access to detailed online information on real estate properties to their clients. We are interested in estimating the real estate price from these large amounts of easily accessed data. In particular, we analyze the prediction power of online house pictures, which is one of the key factors for online users to make a potential visiting decision. The development of robust computer vision algorithms makes the analysis of visual content possible. In this paper, we employ a recurrent neural network to predict real estate prices using the state-of-the-art visual features. The experimental results indicate that our model outperforms several other state-of-the-art baseline algorithms in terms of both mean absolute error and mean absolute percentage error.","","","10.1109/TMM.2017.2710804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937942","Deep neural networks;real estate;visual content analysis","Recurrent neural networks;Appraisal;Algorithm design and analysis;Prediction methods;Machine learning","computer vision;decision making;estimation theory;pricing;property market;real estate data processing;recurrent neural nets","recurrent neural network;computer vision algorithms;visiting decision making;real estate brokers;real estate price estimation;image-based appraisal;real estate properties","","1","43","Traditional","","","","IEEE","IEEE Journals"
"Video Satellite Imagery Super Resolution via Convolutional Neural Networks","Y. Luo; L. Zhou; S. Wang; Z. Wang","Collaborative Innovation Centre of Geospatial Technology, School of Remote Sensing Information Engineering, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, Computer School, Wuhan University, Wuhan, China; Division of Imaging Sciences and Biomedical Engineering Research, King’s College London, London, U.K; Collaborative Innovation Centre of Geospatial Technology, School of Remote Sensing Information Engineering, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","12","2398","2402","Video satellite imagery is a new technique for earth dynamic observation and has a wide range of uses in environmental fields. Despite its capability of dynamic targets' detection, it sustains a serious restriction of the image quality due to the degradation and compression in its imaging process. Hence, the super-resolution (SR) reconstruction on these compressed low-spatial-resolution images is of significance to afterward ground objects recognition and detection tasks. Based on the recent proposed state-of-the-art convolutional neural networks (CNNs) SR methods, we proposed an SR method which could get more precise reconstructed high-spatial-resolution images. Trained with Gaofen-2 satellite images, a robust CNN model specified in satellite image SR is obtained. Experimentally, the reconstruction results on Jilin-1 mission satellite images validate the effectiveness of our method.","","","10.1109/LGRS.2017.2766204","National Natural Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8101498","Convolutional neural network (CNN);deep learning;super resolution (SR);video satellite","Satellites;Image reconstruction;Training;Spatial resolution;Neural networks;Remote sensing","cellular neural nets;data compression;geophysical image processing;image classification;image coding;image reconstruction;image resolution;neural nets;object recognition;remote sensing","earth dynamic observation;environmental fields;serious restriction;image quality;degradation;compression;imaging process;afterward ground objects recognition;detection tasks;convolutional neural networks SR methods;SR method;high-spatial-resolution images;Gaofen-2 satellite images;satellite image SR;reconstruction results;Jilin-1 mission satellite images;dynamic target detection;video satellite imagery superresolution;superresolution reconstruction;low-spatial-resolution image compression;reconstructed high-spatial-resolution images;robust CNN model","","5","24","","","","","IEEE","IEEE Journals"
"Single-View and Multi-View Depth Fusion","J. M. Fácil; A. Concha; L. Montesano; J. Civera","I3A, University of Zaragoza, 50018 Zaragoza, Spain; I3A, University of Zaragoza, 50018 Zaragoza, Spain; I3A, Bit&Brain Technologies, University of Zaragoza, Zaragoza, Zaragoza, SpainSpain; I3A, University of Zaragoza, 50018 Zaragoza, Spain","IEEE Robotics and Automation Letters","","2017","2","4","1994","2001","Dense and accurate 3-D mapping from a monocular sequence is a key technology for several applications and still an open research area. This letter leverages recent results on single-view convolutional network (CNN)-based depth estimation and fuses them with multiview depth estimation. Both approaches present complementary strengths. Multiview depth is highly accurate but only in high-texture areas and high-parallax cases. Single-view depth captures the local structure of midlevel regions, including texture-less areas, but the estimated depth lacks global coherence. The single and multiview fusion we propose is challenging in several aspects. First, both depths are related by a deformation that depends on the image content. Second, the selection of multiview points of high accuracy might be difficult for low-parallax configurations. We present contributions for both problems. Our results in the public datasets of NYUv2 and TUM shows that our algorithm outperforms the individual single and multiview approaches. A video showing the key aspects of mapping in our single and multiview depth proposal is available at https://youtu.be/ipc5HukTb4k.","","","10.1109/LRA.2017.2715400","NVIDIA Corporation; Spanish government; Aragon regional government; University of Zaragoza; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949041","Deep learning in robotics and automation;mapping;SLAM","Estimation;Image reconstruction;Fuses;Proposals;Training;Three-dimensional displays;Periodic structures","image fusion;image sequences;image texture;neural nets","low-parallax configurations;multiview depth estimation;CNN-based depth estimation;single-view convolutional network;monocular sequence;3D mapping;single-view depth fusion;multiview depth fusion","","4","29","Traditional","","","","IEEE","IEEE Journals"
"Multispectral and Hyperspectral Image Fusion Using a 3-D-Convolutional Neural Network","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Geoscience and Remote Sensing Letters","","2017","14","5","639","643","In this letter, we propose a method using a 3-D convolutional neural network to fuse together multispectral and hyperspectral (HS) images to obtain a high resolution HS image. Dimensionality reduction of the HS image is performed prior to fusion in order to significantly reduce the computational time and make the method more robust to noise. Experiments are performed on a data set simulated using a real HS image. The results obtained show that the proposed approach is very promising when compared with conventional methods. This is especially true when the HS image is corrupted by additive noise.","","","10.1109/LGRS.2017.2668299","Icelandic Research Fund; University of Iceland Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869277","Convolutional neural networks (CNNs);deep learning (DL);hyperspectral (HS);image fusion;multispectral (MS)","Spatial resolution;Neurons;Loading;Principal component analysis;Neural networks;Training","geophysical techniques;hyperspectral imaging;neural nets","multispectral image fusion;hyperspectral image fusion;3-D-convolutional neural network;high resolution image;additive noise","","46","21","","","","","IEEE","IEEE Journals"
"Accurate Periocular Recognition Under Less Constrained Environment Using Semantics-Assisted Convolutional Neural Network","Z. Zhao; A. Kumar","Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Information Forensics and Security","","2017","12","5","1017","1030","Accurate biometric identification under real environments is one of the most critical and challenging tasks to meet growing demand for higher security. This paper proposes a new framework to efficiently and accurately match periocular images that are automatically acquired under less-constrained environments. Our framework, referred to as semantics-assisted convolutional neural networks (SCNNs) in this paper, incorporates explicit semantic information to automatically recover comprehensive periocular features. This strategy enables superior matching accuracy with the usage of relatively smaller number of training samples, which is often an issue with several biometrics. Our reproducible experimental results on four different publicly available databases suggest that the SCNN-based periocular recognition approach can achieve outperforming results, both in achievable accuracy and matching time, for less-constrained periocular matching. Additional experimental results presented in this paper also indicate that the effectiveness of proposed SCNN architecture is not only limited to periocular recognition but it can also be useful for generalized image classification. Without increasing the volume of training data, the SCNN is able to automatically extract more discriminative features from the input data than a single CNN, therefore can consistently improve the recognition performance. The experimental results presented in this paper validate such an approach to enable faster and more accurate periocular recognition under less constrained environments.","","","10.1109/TIFS.2016.2636093","General Research Fund from the Hong Kong Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7775081","Periocular recognition;deep learning;convolution neural network;training data augmentation","Training;Feature extraction;Convolution;Training data;Iris recognition;Face recognition;Neural networks","biometrics (access control);feature extraction;image recognition;neural nets","semantics-assisted convolutional neural networks;biometrics;SCNN-based periocular recognition approach;feature extraction","","27","44","","","","","IEEE","IEEE Journals"
"Convolutional Neural Networks for Large-Scale Remote-Sensing Image Classification","E. Maggiori; Y. Tarabalka; G. Charpiat; P. Alliez","TITANE Team, Inria, Universit&#x00E9; C&#x00F4;te d&#x2019;Azur, Sophia Antipolis, France; TITANE Team, Inria, Universit&#x00E9; C&#x00F4;te d&#x2019;Azur, Sophia Antipolis, France; Tao Team, Inria Saclay–Île-de-France, Laboratoire de Recherche en Informatique, Universit&#x00E9; Paris-Sud, Orsay, France; TITANE Team, Inria, Universit&#x00E9; C&#x00F4;te d&#x2019;Azur, Sophia Antipolis, France","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","2","645","657","We propose an end-to-end framework for the dense, pixelwise classification of satellite imagery with convolutional neural networks (CNNs). In our framework, CNNs are directly trained to produce classification maps out of the input images. We first devise a fully convolutional architecture and demonstrate its relevance to the dense classification problem. We then address the issue of imperfect training data through a two-step training approach: CNNs are first initialized by using a large amount of possibly inaccurate reference data, and then refined on a small amount of accurately labeled data. To complete our framework, we design a multiscale neuron module that alleviates the common tradeoff between recognition and precise localization. A series of experiments show that our networks consider a large amount of context to provide fine-grained classification maps.","","","10.1109/TGRS.2016.2612821","Centre National d’Etudes Spatiales; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7592858","Classification;convolutional neural networks (CNNs);deep learning;satellite images","Satellites;Biological neural networks;Neurons;Context;Remote sensing;Training;Training data","geophysical image processing;image classification;neural nets;remote sensing","convolutional neural network;remote sensing image classification;satellite imagery classification;neuron module","","258","39","","","","","IEEE","IEEE Journals"
"Generative Adversarial Networks for Noise Reduction in Low-Dose CT","J. M. Wolterink; T. Leiner; M. A. Viergever; I. Išgum","Image Sciences Institute, University Medical Center Utrecht, Utrecht, CX, The Netherlands; Department of Radiology, University Medical Center Utrecht, Utrecht, CX, The Netherlands; Image Sciences Institute, University Medical Center Utrecht, Utrecht, CX, The Netherlands; Image Sciences Institute, University Medical Center Utrecht, Utrecht, CX, The Netherlands","IEEE Transactions on Medical Imaging","","2017","36","12","2536","2545","Noise is inherent to low-dose CT acquisition. We propose to train a convolutional neural network (CNN) jointly with an adversarial CNN to estimate routine-dose CT images from low-dose CT images and hence reduce noise. A generator CNN was trained to transform low-dose CT images into routine-dose CT images using voxelwise loss minimization. An adversarial discriminator CNN was simultaneously trained to distinguish the output of the generator from routine-dose CT images. The performance of this discriminator was used as an adversarial loss for the generator. Experiments were performed using CT images of an anthropomorphic phantom containing calcium inserts, as well as patient non-contrast-enhanced cardiac CT images. The phantom and patients were scanned at 20% and 100% routine clinical dose. Three training strategies were compared: the first used only voxelwise loss, the second combined voxelwise loss and adversarial loss, and the third used only adversarial loss. The results showed that training with only voxelwise loss resulted in the highest peak signal-to-noise ratio with respect to reference routine-dose images. However, CNNs trained with adversarial loss captured image statistics of routine-dose images better. Noise reduction improved quantification of low-density calcified inserts in phantom CT images and allowed coronary calcium scoring in low-dose patient CT images with high noise levels. Testing took less than 10 s per CT volume. CNN-based low-dose CT noise reduction in the image domain is feasible. Training with an adversarial network improves the CNNs ability to generate images with an appearance similar to that of reference routine-dose CT images.","","","10.1109/TMI.2017.2708987","Netherlands Organization for Health Research and Development (ZonMw) in the framework of the Innovative Medical Devices Initiative research programme, through the project FSCAD; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934380","Coronary calcium scoring;deep learning;generative adversarial networks;low-dose cardiac CT;noise reduction","Computed tomography;Generators;Noise reduction;Convolution;Training;Transforms;Calcium","calcium;cardiology;computerised tomography;image capture;image denoising;medical image processing;neural nets;phantoms;statistical analysis","generative adversarial networks;low-dose CT acquisition;convolutional neural network;routine-dose CT image estimation;voxelwise loss minimization;adversarial discriminator CNN;anthropomorphic phantom;patient noncontrast-enhanced cardiac CT images;signal-to-noise ratio;adversarial loss captured image statistics;coronary calcium scoring;Ca","Arteries;Calcinosis;Humans;Image Processing, Computer-Assisted;Phantoms, Imaging;Signal-To-Noise Ratio;Tomography, X-Ray Computed","55","49","","","","","IEEE","IEEE Journals"
"Instant Object Detection in Lidar Point Clouds","A. Börcs; B. Nagy; C. Benedek","Machine Perception Research Laboratory, Institute for Computer Science and Control, Hungarian Academy of Sciences, Budapest, Hungary; Machine Perception Research, MTASZTAKI, Budapest, Hungary; Machine Perception Research, MTASZTAKI, Budapest, Hungary","IEEE Geoscience and Remote Sensing Letters","","2017","14","7","992","996","In this letter, we present a new approach for object classification in continuously streamed Lidar point clouds collected from urban areas. The input of our framework is raw 3-D point cloud sequences captured by a Velodyne HDL-64 Lidar, and we aim to extract all vehicles and pedestrians in the neighborhood of the moving sensor. We propose a complete pipeline developed especially for distinguishing outdoor 3-D urban objects. First, we segment the point cloud into regions of ground, short objects (i.e., low foreground), and tall objects (high foreground). Then, using our novel two-layer grid structure, we perform efficient connected component analysis on the foreground regions, for producing distinct groups of points, which represent different urban objects. Next, we create depth images from the object candidates, and apply an appearance-based preliminary classification by a convolutional neural network. Finally, we refine the classification with contextual features considering the possible expected scene topologies. We tested our algorithm in real Lidar measurements, containing 1485 objects captured from different urban scenarios.","","","10.1109/LGRS.2017.2674799","National Research, Development and Innovation Fund; János Bolyai Research Scholarship of the Hungarian Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927715","Deep learning;Lidar;object classification","Three-dimensional displays;Laser radar;Object detection;Object recognition;Feature extraction;Merging;Neural networks","feature extraction;image classification;neural nets;object detection;optical radar;radar computing;radar imaging","real Lidar measurements;scene topologies;contextual features;convolutional neural network;appearance-based preliminary classification;depth images;foreground regions;connected component analysis;two-layer grid structure;outdoor 3D urban objects;Velodyne HDL-64 Lidar;raw 3-D point cloud sequences;object classification;Lidar point clouds;instant object detection","","16","15","","","","","IEEE","IEEE Journals"
"Analysis of Recurrent Neural Networks for Probabilistic Modeling of Driver Behavior","J. Morton; T. A. Wheeler; M. J. Kochenderfer","Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA; Department of Aeronautics and Astronautics, Stanford University, Stanford, CA, USA","IEEE Transactions on Intelligent Transportation Systems","","2017","18","5","1289","1298","The validity of any traffic simulation model depends on its ability to generate representative driver acceleration profiles. This paper studies the effectiveness of recurrent neural networks in predicting the acceleration distributions for car following on highways. The long short-term memory recurrent networks are trained and used to propagate the simulated vehicle trajectories over 10-s horizons. On the basis of several performance metrics, the recurrent networks are shown to generally match or outperform baseline methods in replicating driver behavior, including smoothness and oscillatory characteristics present in real trajectories. This paper reveals that the strong performance is due to the ability of the recurrent network to identify recent trends in the ego-vehicle's state, and recurrent networks are shown to perform as, well as feedforward networks with longer histories as inputs.","","","10.1109/TITS.2016.2603007","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7565491","Recurrent neural networks;car-following models;prediction methods;autonomous vehicles;deep learning","Vehicles;Acceleration;Hidden Markov models;Predictive models;Mathematical model;Recurrent neural networks","feedforward neural nets;human factors;probability;recurrent neural nets;road vehicles;traffic engineering computing","recurrent neural networks;probabilistic modeling;driver behavior;traffic simulation model;driver acceleration profiles;acceleration distributions;long short-term memory recurrent network training;simulated vehicle trajectory propagation;oscillatory characteristics;smoothness characteristics;ego-vehicle state;feedforward networks;car-following models","","53","49","","","","","IEEE","IEEE Journals"
"ConvNet-Based Localization of Anatomical Structures in 3-D Medical Images","B. D. de Vos; J. M. Wolterink; P. A. de Jong; T. Leiner; M. A. Viergever; I. Išgum","Image Sciences Institute, University Medical Center Utrecht, Utrecht, The Netherlands; Image Sciences Institute, University Medical Center Utrecht, Utrecht, The Netherlands; Department of Radiology, University Medical Center, The Netherlands; Department of Radiology, University Medical Center, The Netherlands; Image Sciences Institute, University Medical Center Utrecht, Utrecht, The Netherlands; Image Sciences Institute, University Medical Center Utrecht, Utrecht, The Netherlands","IEEE Transactions on Medical Imaging","","2017","36","7","1470","1481","Localization of anatomical structures is a prerequisite for many tasks in a medical image analysis. We propose a method for automatic localization of one or more anatomical structures in 3-D medical images through detection of their presence in 2-D image slices using a convolutional neural network (ConvNet). A single ConvNet is trained to detect the presence of the anatomical structure of interest in axial, coronal, and sagittal slices extracted from a 3-D image. To allow the ConvNet to analyze slices of different sizes, spatial pyramid pooling is applied. After detection, 3-D bounding boxes are created by combining the output of the ConvNet in all slices. In the experiments, 200 chest CT, 100 cardiac CT angiography (CTA), and 100 abdomen CT scans were used. The heart, ascending aorta, aortic arch, and descending aorta were localized in chest CT scans, the left cardiac ventricle in cardiac CTA scans, and the liver in abdomen CT scans. Localization was evaluated using the distances between automatically and manually defined reference bounding box centroids and walls. The best results were achieved in the localization of structures with clearly defined boundaries (e.g., aortic arch) and the worst when the structure boundary was not clearly visible (e.g., liver). The method was more robust and accurate in localization multiple structures.","","","10.1109/TMI.2017.2673121","Netherlands Organization for Scientific Research Foundation for Technology Sciences Project 12726; NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862905","Localization;detection;convolutional neural networks;CT;deep learning","Computed tomography;Three-dimensional displays;Anatomical structure;Two dimensional displays;Abdomen;Heart","angiocardiography;computerised tomography;feature extraction;liver;medical image processing;neural nets;stereo image processing","3D medical images;convolutional neural network;ConvNet-based localization;anatomical structure;cardiac CT angiography;abdomen CT scans;chest CT;heart;ascending aorta;aortic arch;descending aorta;left cardiac ventricle;liver","Angiography;Humans;Imaging, Three-Dimensional;Liver;Neural Networks (Computer);Tomography, X-Ray Computed","13","38","","","","","IEEE","IEEE Journals"
"DySPAN Spectrum Challenge: Situational Awareness and Opportunistic Spectrum Access Benchmarked","F. Wunsch; F. Paisana; S. Rajendran; A. Selim; P. Alvarez; S. Müller; S. Koslowski; B. Van den Bergh; S. Pollin","Karlsruhe Institute of Technology, Karlsruhe, Germany; Trinity College, Dublin, Dublin, Ireland; Katholieke Universiteit Leuven Faculteit Wetenschappen, Leuven, Belgium; Trinity College, Dublin, Dublin, Ireland; Trinity College, Dublin, Dublin, Ireland; Karlsruhe Institute of Technology, Karlsruhe, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany; Katholieke Universiteit Leuven Faculteit Wetenschappen, Leuven, Belgium; Katholieke Universiteit Leuven Faculteit Wetenschappen, Leuven, Belgium","IEEE Transactions on Cognitive Communications and Networking","","2017","3","3","550","562","In this paper, we describe the original problem statement and the two winning solutions to the IEEE DySPAN Challenge, organized in Baltimore in 2017. The idea of the challenge was to invite teams to propose as diverse as possible solutions to a well defined problem, and evaluate the performance of the proposed solutions in a realistic environment. The challenge is defined to enable benchmarking and comparison of multiple teams, possibly working on different parts of the system, in a real environment. The winning solutions represented a complete and working system, working robustly and adapting to both anticipated scenario changes, as well as random effects caused by the conference setting. The code for running the challenge along with the winning solutions is publicly available, so that interested teams can start from the code when designing or benchmarking solutions, as well as when setting up own challenges and competitions. As a result, the challenge can serve as a milestone toward the creation of a benchmarking series. This paper contains all the necessary details about the software repositories so that it becomes possible to rerun the challenge and start building novel solutions based on the winners in IEEE DySPAN 2017.","","","10.1109/TCCN.2017.2745682","European Union’s Horizon 2020; Science Foundation Ireland (SFI); IWT SBO project SAMURAI; European Union’s Horizon 2020; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8017499","Spectrum challenge;cognitive radio;deep learning;FBMC","Wireless communication;Databases;Receivers;OFDM;Benchmark testing;Throughput;Delays","cognitive radio;radio spectrum management","IEEE DySPAN 2017;DySPAN spectrum Challenge;situational awareness;performance evaluation;software repositories;opportunistic spectrum access benchmark","","2","19","Traditional","","","","IEEE","IEEE Journals"
"Multi-level cross-lingual attentive neural architecture for low resource name tagging","X. Feng; L. Huang; B. Qin; Y. Lin; H. Ji; T. Liu","College of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China; College of Computer Science, Rensselaer Polytechnic Institute, Troy 12180, USA; College of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China; College of Computer Science, Rensselaer Polytechnic Institute, Troy 12180, USA; College of Computer Science, Rensselaer Polytechnic Institute, Troy 12180, USA; College of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China","Tsinghua Science and Technology","","2017","22","6","633","645","Neural networks have been widely used for English name tagging and have delivered state-of-the-art results. However, for low resource languages, due to the limited resources and lack of training data, taggers tend to have lower performance, in comparison to the English language. In this paper, we tackle this challenging issue by incorporating multi-level cross-lingual knowledge as attention into a neural architecture, which guides low resource name tagging to achieve a better performance. Specifically, we regard entity type distribution as language independent and use bilingual lexicons to bridge cross-lingual semantic mapping. Then, we jointly apply word-level cross-lingual mutual influence and entity-type level monolingual word distributions to enhance low resource name tagging. Experiments on three languages demonstrate the effectiveness of this neural architecture: for Chinese, Uzbek, and Turkish, we are able to yield significant improvements in name tagging over all previous baselines.","","","10.23919/TST.2017.8195346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8195346","name tagging;deep learning;recurrent neural network;cross-lingual information extraction","Tagging;Neural networks;Semantics;Training data;Dictionaries;Computer architecture","information retrieval;natural language processing;neural nets;text analysis","English language;multilevel cross-lingual knowledge;low resource name tagging;cross-lingual semantic mapping;word-level cross-lingual mutual influence;entity-type level monolingual word distributions;multilevel cross-lingual attentive neural architecture;neural networks;English name tagging;low resource languages;bilingual lexicons","","","","","","","","TUP","TUP Journals"
"Segmentation Mask Refinement Using Image Transformations","T. D. Nguyen; A. Shinya; T. Harada; R. Thawonmas","Graduate School of Information Science and Engineering, Ritsumeikan University, Shiga, Japan; Graduate School of Information Science and Engineering, Ritsumeikan University, Shiga, Japan; College of Information Science and Engineering, Ritsumeikan University, Shiga, Japan; College of Information Science and Engineering, Ritsumeikan University, Shiga, Japan","IEEE Access","","2017","5","","26409","26418","This paper discusses object proposal generation, which is a crucial step of instance-level semantic segmentation (instance segmentation). Known as a challenging computer vision task, the instance segmentation requires jointly detecting and segmenting individual instances of objects in an image. A common approach to this task is first to propose a set of class-agnostic object candidates in the forms of segmentation masks, which represent both object locations and boundaries, and then to perform classification on each object candidate. In this paper, we propose an effective refinement process that employs image transformations and mask matching to increase the accuracy of object segmentation masks. The proposed refinement process is applied to three state-of-the-art object proposal methods (DeepMask, SharpMask, and FastMask), and is evaluated on two standard benchmarks (Microsoft COCO and PASCAL VOC). Both the quantitative and qualitative results show the effectiveness of the process across various experimental settings.","","","10.1109/ACCESS.2017.2772269","Strategic Research Foundation Grant-aided Project for Private Universities, Japan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8103741","Instance segmentation;object proposal;segmentation mask;convolutional neural networks;deep learning","Image segmentation;Proposals;Feature extraction;Object segmentation;Semantics;Head","computer vision;image segmentation;object detection","object segmentation mask refinement;object locations;class-agnostic object candidates;challenging computer vision task;instance-level semantic segmentation;image transformations","","1","14","","","","","IEEE","IEEE Journals"
"A Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition","X. Ren; Y. Zhou; Z. Huang; J. Sun; X. Yang; K. Chen","Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Information Security and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Access","","2017","5","","3193","3204","Scene text information extraction plays an important role in many computer vision applications. Most features in existing text extraction algorithms are only applicable to one text extraction stage (text detection or recognition), which significantly weakens the consistency in an end-to-end system, especially for the complex Chinese texts. To tackle this challenging problem, we propose a novel text structure feature extractor based on a text structure component detector (TSCD) layer and residual network for Chinese texts. Inspired by the three-layer Chinese text cognition model of a human, we combine the TSCD layer and the residual network to extract features suitable for both text extraction stages. The specialized modeling for Chinese characters in the TSCD layer simulates the key structure component cognition layer in the psychological model. And the residual mechanism in the residual network simulates the key bidirectional connection among the layers in the psychological model. Through the organic combination of the TSCD layer and the residual network, the extracted features are applicable to both text detection and recognition, as humans do. In evaluation, both text detection and recognition models based on our proposed text structure feature extractor achieve great improvements over baseline CNN models. And an end-to-end Chinese text information extraction system is experimentally designed and evaluated, showing the advantage of the proposed feature extractor as a unified feature extractor.","","","10.1109/ACCESS.2017.2676158","National Key Research and Development Program of China; National Natural Science Foundation of Chinaina; Shanghai Science and Technology Committees of Scientific Research Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7870570","Text structure feature;Chinese text;deep learning;residual network;unified model","Feature extraction;Text recognition;Data mining;Information retrieval;Character recognition;Cognition;Engines","cognition;computer vision;feature extraction;natural language processing;text detection","text structure feature extractor;Chinese scene text detection;Chinese scene text recognition;scene text information extraction;computer vision;text structure component detector;TSCD layer;three-layer Chinese text cognition model;Chinese characters;key structure component cognition layer;psychological model;key bidirectional connection;residual network;end-to-end Chinese text information extraction system","","14","45","","","","","IEEE","IEEE Journals"
"An Energy-Efficient Precision-Scalable ConvNet Processor in 40-nm CMOS","B. Moons; M. Verhelst","Department of Electrical Engineering, ESAT-MICAS, KU Leuven, Leuven, Belgium; Department of Electrical Engineering, ESAT-MICAS, KU Leuven, Leuven, Belgium","IEEE Journal of Solid-State Circuits","","2017","52","4","903","914","A precision-scalable processor for low-power ConvNets or convolutional neural networks is implemented in a 40-nm CMOS technology. To minimize energy consumption while maintaining throughput, this paper is the first to implement dynamic precision and energy scaling and exploit the sparsity of convolutions in a dedicated processor architecture. The processor's 256 parallel processing units achieve a peak 102 GOPS running at 204 MHz and 1.1 V. It is fully C-programmable through a custom generated compiler and consumes 25-287 mW at 204 MHz and a scaling efficiency between 0.3 and 2.7 effective TOPS/W. It achieves 47 frames/s on the convolutional layers of the AlexNet benchmark, consuming only 76 mW. This system hereby outperforms the state-of-the-art up to five times in energy efficiency.","","","10.1109/JSSC.2016.2636225","Research Foundation – Flanders; Intel Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801877","Approximate computing;ConvNet;convolutional neural network (CNN);deep learning;Dynamic-Voltage-Accuracy-Scaling;processor architecture;voltage scaling","Computer architecture;Hardware;Neural networks;Switches;Benchmark testing;Face recognition;Acceleration","CMOS integrated circuits;convolution;microprocessor chips;neural chips;parallel processing","convolutional neural networks;energy efficient precision scalable ConvNet processor;CMOS processor;parallel processing units;AlexNet benchmark","","45","44","","","","","IEEE","IEEE Journals"
"Recent advances in conversational speech recognition using convolutional and recurrent neural networks","G. Saon; M. Picheny","NA; NA","IBM Journal of Research and Development","","2017","61","4/5","1:1","1:10","Deep learning methodologies have had a major impact on performance across a wide variety of machine learning tasks, and speech recognition is no exception. We describe a set of deep learning techniques that proved to be particularly successful in achieving performance gains in word error rate on a popular large vocabulary conversational speech recognition benchmark task (“Switchboard”). We found that the best performance is achieved by combining features from both recurrent and convolutional neural networks. We compare two recurrent architectures: partially unfolded nets with max-out activations and bidirectional long short-term memory nets. In addition, inspired by the success of convolutional networks for image classification, we designed a convolutional net with many convolutional layers and small kernels that create a receptive field with more nonlinearity and fewer parameters than standard configurations. When combined, these neural networks achieve a word error rate of 6.2% on this difficult task; this was the best reported rate at the time of this writing and is even more remarkable given that human performance itself is estimated to be 4% on this data.","","","10.1147/JRD.2017.2701178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030294","","Speech recognition;Hidden Markov models;Error analysis;Feature extraction;Machine learning;Neural networks","","","","1","44","","","","","IBM","IBM Journals"
"Cascade recurrent neural network for image caption generation","J. Wu; H. Hu","Sun Yat-sen University, People's Republic of China; Sun Yat-sen University, People's Republic of China","Electronics Letters","","2017","53","25","1642","1643","A new cascade recurrent neural network (CRNN) for image caption generation is proposed. Different from the classical multimodal recurrent neural network, which only uses a single network for extracting unidirectional syntactic features, CRNN adopts a cascade network for learning visual-language interactions from forward and backward directions, which can exploit the deep semantic contexts contained in the image. In the proposed framework, two embedding layers for dense word expression are constructed. A new stacked Gated Recurrent Unit is designed for learning image-word mappings. The effectiveness of the CRNN model is verified with adopting the commonly used MSCOCO datasets, where the results indicate CRNN can achieve better performance compared with the state-of-the-art image captioning methods such as Google NIC, multimodal recurrent neural network and so on.","","","10.1049/el.2017.3159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8221809","","","feature extraction;recurrent neural nets","Google NIC;image captioning method;MSCOCO datasets;CRNN model;image-word mappings;stacked gated recurrent unit;dense word expression;embedding layer;deep semantic contexts;visual-language interaction;cascade network;unidirectional syntactic feature;classical multimodal recurrent neural network;image caption generation;cascade recurrent neural network","","","12","","","","","IET","IET Journals"
"Segmentation and semantic labelling of RGBD data with convolutional neural networks and surface fitting","G. Pagnutti; L. Minto; P. Zanuttigh","University of Padova, Italy; University of Padova, Italy; University of Padova, Italy","IET Computer Vision","","2017","11","8","633","642","We present an approach for segmentation and semantic labelling of RGBD data exploiting together geometrical cues and deep learning techniques. An initial over-segmentation is performed using spectral clustering and a set of non-uniform rational B-spline surfaces is fitted on the extracted segments. Then a convolutional neural network (CNN) receives in input colour and geometry data together with surface fitting parameters. The network is made of nine convolutional stages followed by a softmax classifier and produces a vector of descriptors for each sample. In the next step, an iterative merging algorithm recombines the output of the over-segmentation into larger regions matching the various elements of the scene. The couples of adjacent segments with higher similarity according to the CNN features are candidate to be merged and the surface fitting accuracy is used to detect which couples of segments belong to the same surface. Finally, a set of labelled segments is obtained by combining the segmentation output with the descriptors from the CNN. Experimental results show how the proposed approach outperforms state-of-the-art methods and provides an accurate segmentation and labelling.","","","10.1049/iet-cvi.2016.0502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8120042","","","feature extraction;image colour analysis;image segmentation;iterative methods;neural nets;surface fitting","segmentation output;labelled segments;surface fitting accuracy;CNN features;over-segmentation;iterative merging algorithm;softmax classifier;semantic labelling;RGBD data;convolutional neural networks;surface fitting;geometrical cues;deep learning technique;initial over-segmentation;spectral clustering;nonuniform rational B-spline surfaces;convolutional neural network;CNN;input colour;geometry data;surface fitting parameters","","3","40","","","","","IET","IET Journals"
"Combining Convolutional and Recurrent Neural Networks for Human Skin Detection","H. Zuo; H. Fan; E. Blasch; H. Ling","Department of Chemical Equipment and Control Engineering, China University of Petroleum, Qingdao, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA; Air Force Research Laboratory, Rome, NY, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA","IEEE Signal Processing Letters","","2017","24","3","289","293","Skin detection from images, typically used as a preprocessing step, has a wide range of applications such as dermatology diagnostics, human computer interaction designs, and etc. It is a challenging problem due to many factors such as variation in pigment melanin, uneven illumination, and differences in ethnicity geographics. Besides, age and gender introduce additional difficulties to the detection process. It is hard to determine whether a single pixel is skin or nonskin without considering the context. An efficient traditional hand-engineered skin color detection algorithm requires extensive work by domain experts. Recently, deep learning algorithms, especially convolutional neural networks (CNNs), have achieved great success in pixel-wise labeling tasks. However, CNN-based architectures are not sufficient for modeling the relationship between pixels and their neighbors. In this letter, we integrate recurrent neural networks (RNNs) layers into the fully convolutional neural networks (FCNs), and develop an end-to-end network for human skin detection. In particular, FCN layers capture generic local features, while RNN layers model the semantic contextual dependencies in images. Experimental results on the COMPAQ and ECU skin datasets validate the effectiveness of the proposed approach, where RNN layers enhance the discriminative power of skin detection in complex background situations.","","","10.1109/LSP.2017.2654803","National Science Foundation; National Natural Science Foundation of China; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820144","Convolutional neural networks (CNNs);recurrent neural networks (RNNs);skin classification;skin detection;skin segmentation","Skin;Recurrent neural networks;Convolution;Image color analysis;Detection algorithms;Lighting","image colour analysis;image recognition;recurrent neural nets;skin","human skin detection;convolutional neural networks;recurrent neural networks;preprocessing step;dermatology diagnostics;human computer interaction;pigment melanin;uneven illumination;ethnicity geographics;hand-engineered skin color detection;domain experts;deep learning algorithms;pixel-wise labeling tasks;end-to-end network;FCN layers;generic local features;semantic contextual dependency;COMPAQ skin datasets;ECU skin datasets;discriminative power;complex background situations","","36","45","","","","","IEEE","IEEE Journals"
"Fast and Exact Newton and Bidirectional Fitting of Active Appearance Models","J. Kossaifi; G. Tzimiropoulos; M. Pantic","Department of Computing, Imperial College London, London, U.K.; School of Computer Science, The University of Notthingham, Nottingham, U.K.; Department of Computing, Imperial College London, London, U.K.","IEEE Transactions on Image Processing","","2017","26","2","1040","1053","Active appearance models (AAMs) are generative models of shape and appearance that have proven very attractive for their ability to handle wide changes in illumination, pose, and occlusion when trained in the wild, while not requiring large training data set like regression-based or deep learning methods. The problem of fitting an AAM is usually formulated as a non-linear least squares one and the main way of solving it is a standard Gauss-Newton algorithm. In this paper, we extend AAMs in two ways: we first extend the Gauss-Newton framework by formulating a bidirectional fitting method that deforms both the image and the template to fit a new instance. We then formulate a second order method by deriving an efficient Newton method for AAMs fitting. We derive both methods in a unified framework for two types of AAMs, holistic and part-based, and additionally show how to exploit the structure in the problem to derive fast yet exact solutions. We perform a thorough evaluation of all algorithms on three challenging and recently annotated in-the-wild data sets, and investigate fitting accuracy, convergence properties, and the influence of noise in the initialization. We compare our proposed methods to other algorithms and show that they yield state-of-the-art results, out-performing other methods while having superior convergence properties.","","","10.1109/TIP.2016.2642828","European Community Horizon 2020 [H2020/2014-2020]; European Community 7th Framework Programme [FP7/2007-2013]; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792677","Active appearance models;newton method;bidirectional image alignment;inverse compositional;forward additive","Active appearance model;Shape;Computational modeling;Optimization;Training;Silicon carbide;Robustness","convergence;image processing;least squares approximations;Newton method","bidirectional fitting;active appearance model;deep learning method;training data set like regression;nonlinear least squares;Gauss-Newton algorithm;Newton method;AAMs fitting;convergence properties","","4","30","","","","","IEEE","IEEE Journals"
"Can a Machine Generate Humanlike Language Descriptions for a Remote Sensing Image?","Z. Shi; Z. Zou","Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2017","55","6","3623","3634","This paper investigates an intriguing question in the remote sensing field: “can a machine generate humanlike language descriptions for a remote sensing image?” The automatic description of a remote sensing image (namely, remote sensing image captioning) is an important but rarely studied task for artificial intelligence. It is more challenging as the description must not only capture the ground elements of different scales, but also express their attributes as well as how these elements interact with each other. Despite the difficulties, we have proposed a remote sensing image captioning framework by leveraging the techniques of the recent fast development of deep learning and fully convolutional networks. The experimental results on a set of high-resolution optical images including Google Earth images and GaoFen-2 satellite images demonstrate that the proposed method is able to generate robust and comprehensive sentence description with desirable speed performance.","","","10.1109/TGRS.2017.2677464","National Natural Science Foundation of China; Beijing Natural Science Foundation; funding project of the State Key Laboratory of Virtual Reality Technology and Systems, Beihang University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891049","Fully convolutional networks (FCNs);high-resolution optical remote sensing image;image understanding;remote sensing image captioning","Remote sensing;Semantics;Feature extraction;Airports;Airplanes;Convolution;Optical imaging","geophysical techniques;remote sensing","humanlike language descriptions;artificial intelligence;remote sensing image captioning framework;deep learning network;fully convolutional network;high-resolution optical images;comprehensive sentence description method;GaoFen-2 satellite images;Google Earth images","","22","42","","","","","IEEE","IEEE Journals"
"SparseConnect: regularising CNNs on fully connected layers","Q. Xu; G. Pan","Zhejiang University, People's Republic of China; Zhejiang University, People's Republic of China","Electronics Letters","","2017","53","18","1246","1248","Deep convolutional neural networks (CNNs) have achieved unprecedented success in many domains. The numerous parameters allow CNNs to learn complex features, but also tend to hinder generalisation by over-fitting training data. Despite many previously proposed regularisation methods, over-fitting is one of many problems in training a robust CNN. Among many factors that may lead to over-fitting, the numerous parameters of fully connected layers (FCLs) of a typical CNN are a contributor to the over-fitting problem. The authors propose SparseConnect, which alleviates over-fitting by sparsifying connections to FCLs. Experimental results on three benchmark datasets MNIST and CIFAR10 show SparseConnect outperforms several state-of-the-art regularisation methods.","","","10.1049/el.2017.2621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030447","","","computer vision;neural nets","SparseConnect;CNN;deep convolutional neural networks;over-fitting training data;fully connected layers;FCL;computer vision","","","9","","","","","IET","IET Journals"
"Near-Threshold RISC-V Core With DSP Extensions for Scalable IoT Endpoint Devices","M. Gautschi; P. D. Schiavone; A. Traber; I. Loi; A. Pullini; D. Rossi; E. Flamand; F. K. Gürkaynak; L. Benini","Integrated Systems Laboratory, ETH Zürich, Zurich, Switzerland; Integrated Systems Laboratory, ETH Zürich, Zurich, Switzerland; Integrated Systems Laboratory, Electrical engineering and information technology, ETH Zürich, Zollikon, Switzerland; GreenWaves Technologies, Villard-Bonnot, France; Integrated Systems Laboratory, ETH Zürich, Zurich, Switzerland; GreenWaves Technologies, Villard-Bonnot, France; Integrated Systems Laboratory, ETH Zürich, Zurich, Switzerland; Integrated Systems Laboratory, ETH Zürich, Zurich, Switzerland; Integrated Systems Laboratory, ETH Zürich, Zurich, Switzerland","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2017","25","10","2700","2713","Endpoint devices for Internet-of-Things not only need to work under extremely tight power envelope of a few milliwatts, but also need to be flexible in their computing capabilities, from a few kOPS to GOPS. Near-threshold (NT) operation can achieve higher energy efficiency, and the performance scalability can be gained through parallelism. In this paper, we describe the design of an open-source RISC-V processor core specifically designed for NT operation in tightly coupled multicore clusters. We introduce instruction extensions and microarchitectural optimizations to increase the computational density and to minimize the pressure toward the shared-memory hierarchy. For typical data-intensive sensor processing workloads, the proposed core is, on average, 3.5× faster and 3.2× more energy efficient, thanks to a smart L0 buffer to reduce cache access contentions and support for compressed instructions. Single Instruction Multiple Data extensions, such as dot products, and a built-in L0 storage further reduce the shared-memory accesses by 8× reducing contentions by 3.2×. With four NT-optimized cores, the cluster is operational from 0.6 to 1.2 V, achieving a peak efficiency of 67 MOPS/mW in a low-cost 65-nm bulk CMOS technology. In a low-power 28-nm FD-SOI process, a peak efficiency of 193 MOPS/mW (40 MHz and 1 mW) can be achieved.","","","10.1109/TVLSI.2017.2654506","FP7 ERC Advance Project MULTITHERMAN; Micropower Deep Learning; Swiss NSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7864441","Instruction set architecture (ISA) extensions;Internet-of-Things;multicore;RISC-V;ultralow power (ULP)","","buffer circuits;CMOS integrated circuits;digital signal processing chips;Internet of Things;reduced instruction set computing;silicon-on-insulator","endpoint devices;Internet-of-Things;near-threshold operation;open-source RISC-V processor core;tightly coupled multicore clusters;instruction extensions;microarchitectural optimizations;computational density;shared-memory hierarchy;data-intensive sensor processing workloads;smart L0 buffer;cache access contentions;compressed instructions;single instruction multiple data extensions;bulk CMOS technology;FD-SOI process;DSP extensions;voltage 0.6 V to 1.2 V;size 65 nm;size 28 nm;frequency 40 MHz;power 1 mW","","46","46","Traditional","","","","IEEE","IEEE Journals"
"A Sub-mW IoT-Endnode for Always-On Visual Monitoring and Smart Triggering","M. Rusci; D. Rossi; E. Farella; L. Benini","ICT Center, Fondazione Bruno Kessler, Trento, Italy; Department of Electrical, Electronic and Information Engineering “Guglielmo Marconi,”, Energy Efficient Embedded Systems Laboratory, University of Bologna, Bologna, Italy; ICT Center, Fondazione Bruno Kessler, Trento, Italy; ICT Center, Fondazione Bruno Kessler, Trento, Italy","IEEE Internet of Things Journal","","2017","4","5","1284","1295","This paper presents a fully programmable Internet of Things visual sensing node that targets sub-mW power consumption in always-on monitoring scenarios. The system features a spatial-contrast 128 × 64 binary pixel imager with focal-plane processing. The sensor, when working at its lowest power mode (10 μW at 10 frames/s), provides as output the number of changed pixels. Based on this information, a dedicated camera interface, implemented on a low-power field-programmable gate array, wakes up an ultralow-power parallel processing unit to extract context-aware visual information. We evaluate the smart sensor on three always-on visual triggering application scenarios. Triggering accuracy comparable to RGB image sensors is achieved at nominal lighting conditions, while consuming an average power between 193 and 277 μW, depending on context activity. The digital subsystem is extremely flexible, thanks to a fully programmable digital signal processing engine, but still achieves 19× lower power consumption compared to MCU-based cameras with significantly lower on-board computing capabilities.","","","10.1109/JIOT.2017.2731301","Swiss National Science Foundation (Transient Computing Systems) (MicroLearn: Micropower Deep Learning); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990119","Always-on;context-aware;event-driven;smart camera;ultralow-power","Visualization;Power demand;Smart cameras;Field programmable gate arrays;Robot sensing systems","cameras;digital signal processing chips;field programmable gate arrays;image resolution;image sensors;intelligent sensors;Internet of Things;low-power electronics;parallel processing","RGB image sensors;nominal lighting conditions;context activity;fully programmable digital signal processing engine;sub-mW IoT-endnode;visual monitoring;smart triggering;fully programmable Internet;sub-mW power consumption;monitoring scenarios;focal-plane processing;lowest power mode;output the number;low-power field-programmable gate array;ultralow-power parallel processing unit;context-aware visual information;smart sensor;visual triggering application scenarios;lower power consumption;camera interface;spatial-contrast binary pixel imager;Internet of Things visual sensing node;power 10.0 muW;power 193.0 muW to 277.0 muW","","4","40","Traditional","","","","IEEE","IEEE Journals"
"A Full Density Stereo Matching System Based on the Combination of CNNs and Slanted-Planes","L. Chen; L. Fan; J. Chen; D. Cao; F. Wang","School of Data and Computer Science, Sun Yat-sen University, Guangzhou 510275, China.(e-mail: chenl46@mail.sysu.edu.cn); School of Data and Computer Science, Sun Yat-sen University, Guangzhou 510275, China.; School of Computer Science and Engineering, Nanyang Technological University, Singapore.; Advanced Vehicle Engineering Center, Cranfield University, Cranfield MK43 0AL, U.K.; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100080, China.","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2017","PP","99","1","12","Stereo matching methods consist of matching cost computation and several post processing steps. Deep learning methods have greatly raised the accuracy of matching cost and achieved the lowest error rate on several public datasets. However, their generality capabilities are not the best due to potential overfitting, which is the common problem of supervised learning approaches. This paper proposes a convolutional neural network (CNN) based cost estimation method for computing the similarity of image patches. In consideration of accuracy and generalization capability, small size convolution kernels are chosen in the convolution layer and dropout in the decision layer is used for preventing overfitting. After obtaining stereo matching cost from the output of the CNN, several post-processing operations are adopted for disparity optimization, which includes semi-global matching in 1-D from different directions, a left-right consistency check, and the slanted plane smoothing method. The method is evaluated on KITTI 2012, KITTI 2015, and Middlebury stereo datasets and the experimental results on the KITTI benchmark demonstrate the competitive accuracy performance of the approach. Additionally, to test the generalization of the method, a series of extended crossover experiments are conducted in which the training samples and testing samples come from different datasets. The results indicate superior generalization capability of our method than other supervised learning methods.","","","10.1109/TSMC.2017.2767823","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8103909","Image reconstruction;stereo matching;stereo vision","Machine learning;Computer architecture;Computational modeling;Estimation;Kernel;Neural networks;Convolution","","","","7","","","","","","IEEE","IEEE Early Access Articles"
"Earthquake Prediction based on Spatio-Temporal Data Mining: An LSTM Network Approach","Q. Wang; Y. Guo; L. Yu; P. Li","NA; NA; NA; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, OH 44106 (e-mail: lipan@case.edu).","IEEE Transactions on Emerging Topics in Computing","","2017","PP","99","1","1","Earthquake prediction is a very important problem in seismology, the success of which can potentially save many human lives. Various kinds of technologies have been proposed to address this problem, such as mathematical analysis, machine learning algorithms like decision trees and support vector machines, and precursors signal study. Unfortunately, they usually do not have very good results due to the seemingly dynamic and unpredictable nature of earthquakes. In contrast, we notice that earthquakes are spatially and temporally correlated because of the crust movement. Therefore, earthquake prediction for a particular location should not be conducted only based on the history data in that location, but according to the history data in a larger area. In this paper, we employ a deep learning technique called long short-term memory (LSTM) networks to learn the spatio-temporal relationship among earthquakes in different locations and make predictions by taking advantage of that relationship. Simulation results show that the LSTM network with two-dimensional input developed in this paper is able to discover and exploit the spatio-temporal correlations among earthquakes to make better predictions than before.","","","10.1109/TETC.2017.2699169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7913634","Earthquake Prediction;Spatio-Temporal Data mining;LSTM","Earthquakes;Correlation;Neural networks;Machine learning;Prediction algorithms;Data mining;Support vector machines","","","","6","","","","","","IEEE","IEEE Early Access Articles"
"Statistical Features-Based Real-Time Detection of Drifted Twitter Spam","C. Chen; Y. Wang; J. Zhang; Y. Xiang; W. Zhou; G. Min","University of Electronic Science and Technology of China, Chengdu, China; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K.","IEEE Transactions on Information Forensics and Security","","2017","12","4","914","925","Twitter spam has become a critical problem nowadays. Recent works focus on applying machine learning techniques for Twitter spam detection, which make use of the statistical features of tweets. In our labeled tweets data set, however, we observe that the statistical properties of spam tweets vary over time, and thus, the performance of existing machine learning-based classifiers decreases. This issue is referred to as “Twitter Spam Drift”. In order to tackle this problem, we first carry out a deep analysis on the statistical features of one million spam tweets and one million non-spam tweets, and then propose a novel Lfun scheme. The proposed scheme can discover “changed” spam tweets from unlabeled tweets and incorporate them into classifier's training process. A number of experiments are performed to evaluate the proposed scheme. The results show that our proposed Lfun scheme can significantly improve the spam detection accuracy in real-world scenarios.","","","10.1109/TIFS.2016.2621888","ARC Linkage Project; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7707341","Social network security;twitter spam detection;machine learning","Twitter;Feature extraction;Uniform resource locators;Security;Market research;Malware","pattern classification;security of data;social networking (online)","statistical features-based real-time detection;drifted Twitter spam;Lfun scheme;classifiers training process","","40","44","","","","","IEEE","IEEE Journals"
"Product Propagation: A Backup Rule Better Than Minimaxing?","H. Kaindl; H. Horacek; A. Scheucher","Institute of Computer Technology, TU Wien, Vienna, Austria; German Research Center for AI, Saarbrücken, Germany; Raiffeisen Bank International, Vienna, Austria","IEEE Transactions on Computational Intelligence and AI in Games","","2017","9","2","109","122","There is a gap between theory and practice regarding the assessment of minimaxing versus product propagation. The use of minimaxing in real programs for certain two-player games like chess is more or less ubiquitous, due to the substantial search space reductions enabled by several pruning algorithms. In stark contrast, some theoretical work supported the view that product propagation could be a viable alternative, or even superior on theoretical grounds. In fact, these rules have different conceptual problems. While minimaxing treats heuristic values as true values, product propagation interprets them as independent probabilities. So, which is the better rule for backing up heuristic values in game trees, and under which circumstances? We present a systematic analysis and results of simulation studies that compare these backup rules in synthetic trees with properties found in certain real game trees, for a variety of situations with characteristic properties. Our results show yet unobserved complementary strengths in their respective capabilities, depending on the size of node score changes (“quiet” versus “nonquiet” positions), and on the degree of advantage of any player over the opponent. In particular, exhaustive analyses for shallow depths show that product propagation can indeed be better than minimaxing when both approaches search to the same depth, especially for making decisions from a huge amount of alternatives, where deep searches are still prohibitive. However, our results also provide some justification for the more or less ubiquitous use of minimaxing in chess programs, where deep searches prevail and the pruning algorithms available for minimaxing make the difference.","","","10.1109/TCIAIG.2015.2508966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358032","Game trees;minimaxing;multivalued evaluation functions;product propagation;simulation studies","Games;Computers;Error probability;Systematics;Analytical models;Indexes;Artificial intelligence","game theory;heuristic programming;learning (artificial intelligence);minimax techniques","product propagation;minimaxing;heuristic values;game trees;backup rules;synthetic trees","","","27","","","","","IEEE","IEEE Journals"
"Facial Expression Recognition Using Salient Features and Convolutional Neural Network","M. Z. Uddin; W. Khaksar; J. Torresen","Department of Informatics, University of Oslo, Oslo, Norway; Department of Informatics, University of Oslo, Oslo, Norway; Department of Informatics, University of Oslo, Oslo, Norway","IEEE Access","","2017","5","","26146","26161","A depth camera-based novel method is proposed here for efficient facial expression recognition. For each pixel in a depth image, eight local directional strengths are obtained and ranked. Once the rank of all pixels is obtained, eight histograms are developed for the eight surrounding directions. The histograms are then concatenated to represent features for a depth image of a face. This approach is named local directional rank histogram pattern (LDRHP). To combine with LDRHP features, one more robust feature extraction technique named local directional strength pattern (LDSP) is proposed. Typical local directional pattern (LDP) considers only absolute values of edge strengths for a pixel. This generalization in LDP may generate the same patterns for two different kinds of edge pixels. LDSP can overcome this problem. It considers the binary values of the position with the directions representing the highest and lowest original strengths. The highest strength indicates the strongest direction on the bright side of a pixel and the lowest one indicates the strongest direction in the dark side of that pixel. Hence, combining binary positions of these two directions can generate more robust patterns than LDP. Besides, LDSP pattern of a pixel is of six bits, whereas traditional LDP-based patterns are of eight bits (e.g., local directional deviation-based pattern and local directional position pattern). Thus, LDSP reduces the dimension of features with the same time adding robustness. For a depth image in a depth video, LDSP features are augmented with LDRHP features followed by Kernel principal component analysis and generalized discriminant analysis to generate more robust features. At last, the features are trained with a deep learning approach and convolutional neural network for successful facial expression recognition. The proposed approach is compared and shown to outperform the traditional expression recognition methods.","","","10.1109/ACCESS.2017.2777003","Research Council of Norway as a part of the Multimodal Elderly Care Systems Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119492","CNN;GDA;LDRHP;LDSP;KPCA","Face recognition;Face;Robustness;Histograms;Feature extraction;Image edge detection;Principal component analysis","face recognition;feature extraction;feedforward neural nets;learning (artificial intelligence);principal component analysis","salient features;convolutional neural network;depth camera;efficient facial expression recognition;depth image;local directional strengths;surrounding directions;local directional rank histogram pattern;LDRHP features;robust feature extraction technique;local directional strength pattern;typical local directional pattern;edge strengths;edge pixels;binary values;highest strengths;lowest original strengths;strongest direction;binary positions;robust patterns;LDSP pattern;traditional LDP-based patterns;local directional deviation-based pattern;local directional position pattern;depth video;LDSP features;robust features;successful facial expression recognition;traditional expression recognition methods","","15","44","","","","","IEEE","IEEE Journals"
"Nonrecurrent Neural Structure for Long-Term Dependence","S. Zhang; C. Liu; H. Jiang; S. Wei; L. Dai; Y. Hu","National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; IFLYTEK Research, Hefei, China; Department of Electrical Engineering and Computer Science, Lassonde School of Engineering, York University, Toronto, ON, Canada; IFLYTEK Research, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; IFLYTEK Research, Hefei, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","4","871","884","In this paper, we propose a novel neural network structure, namely feedforward sequential memory networks (FSMN), to model long-term dependence in time series without using recurrent feedback. The proposed FSMN is a standard fully connected feedforward neural network equipped with some learnable memory blocks in its hidden layers. The memory blocks use a tapped-delay line structure to encode the long context information into a fixed-size representation as short-term memory mechanism which are somehow similar to the time-delay neural networks layers. We have evaluated the FSMNs in several standard benchmark tasks, including speech recognition and language modeling. Experimental results have shown that FSMNs outperform the conventional recurrent neural networks (RNN) while can be learned much more reliably and faster in modeling sequential signals like speech or language. Moreover, we also propose a compact feedforward sequential memory networks (cFSMN) by combining FSMN with low-rank matrix factorization and make a slight modification to the encoding method used in FSMNs in order to further simplify the network architecture. On the speech recognition Switchboard task, the proposed cFSMN structures can reduce the model size by 60% and speed up the learning by more than seven times while the model can still significantly outperform the popular bidirectional LSTMs for both frame-level cross-entropy criterion-based training and MMI-based sequence training.","","","10.1109/TASLP.2017.2672398","Science and Technology Department of Anhui Province; Chinese Academy of Sciences; National Key Research and Development Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859324","CFSMN;Deep neural networks;feedforward sequential memory networks;language modeling;speech recognition","Feedforward neural networks;Standards;Speech recognition;Encoding;Speech;Recurrent neural networks","encoding;feedforward neural nets;matrix decomposition;speech recognition;time series","nonrecurrent neural structure;long-term dependence;neural network structure;time series;feedforward neural network;learnable memory blocks;hidden layers;tapped-delay line structure;fixed-size representation;short-term memory mechanism;time-delay neural networks layers;speech recognition;language modeling;sequential signals;compact feedforward sequential memory networks;cFSMN;low-rank matrix factorization;encoding method;switchboard task;frame-level cross-entropy criterion;MMI-based sequence training","","7","64","","","","","IEEE","IEEE Journals"
"Joint Human Detection and Head Pose Estimation via Multistream Networks for RGB-D Videos","G. Zhang; J. Liu; H. Li; Y. Q. Chen; L. S. Davis","University of Maryland, School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, College Park, Shanghai, MD, USAChina; Nanyang Technological University, Singapore; School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China; Center for Automation Research, University of Maryland, College Park, MD, USA","IEEE Signal Processing Letters","","2017","24","11","1666","1670","We propose a multistream multitask deep network for joint human detection and head pose estimation in RGB-D videos. To achieve high accuracy, we jointly utilize appearance, shape, and motion information as inputs. Based on the depth information, we generate scale invariant proposals, which are then fed into a novel contextual region of interest pooling (CRP) layer in our deep network. This CRP has two branches to deal with contextual information for each subject. The proposed method outperforms state-of-the-art approaches on three public datasets.","","","10.1109/LSP.2017.2731952","Science and Technology Commission of Shanghai Municipality; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7993051","Contextual ROI pooling (CRP);head pose estimation;human detection;RGB-D image;scale invariant proposals","Head;Proposals;Pose estimation;Shape;Videos;Machine learning;Clothing","image colour analysis;neural nets;object detection;pose estimation;video signal processing","CRP;novel contextual region of interest pooling layer;RGB-D videos;multistream networks;joint human detection and head pose estimation","","7","51","Traditional","","","","IEEE","IEEE Journals"
"Performance of natural language classifiers in a question-answering system","R. Bakis; D. P. Connors; P. Dube; P. Kapanipathi; A. Kumar; D. Malioutov; C. Venkatramani","NA; NA; NA; NA; NA; NA; NA","IBM Journal of Research and Development","","2017","61","4/5","14:1","14:10","Deep-learning algorithms are being used extensively in question–answering systems based on natural language classifiers to classify an incoming user question into a set of classes with the same answer. We treat a natural language classifier as a black box and study its performance with respect to the ground truth that is used to train and test the system. We have observed that maintaining ground truth is challenging; for example, 1) the number of answer classes can be large (in the several hundreds), 2) manual mapping of questions to answers can result in inconsistent mappings, leading to overlap and confusion among them, and 3) users ask questions within a context that is not apparent by examining the question standalone, leading to erroneous mappings. We propose a methodology for guided evolution of the ground truth, from its initial creation to its ongoing maintenance in the deployed production environment. We measure performance using two metrics: accuracy and confidence. Accuracy measures how many classifications are correct, based on an assessment, while confidence is a raw metric, output by the classifier, which correlates with accuracy. Confidence can further be used to effectively manage the perceived accuracy of the system from a user's perspective, appropriately trading off accuracy versus coverage.","","","10.1147/JRD.2017.2711719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030301","","Measurement;Natural language processing;Machine learning;Algorithm design and analysis","","","","1","33","","","","","IBM","IBM Journals"
"Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection","E. Çakır; G. Parascandolo; T. Heittola; H. Huttunen; T. Virtanen","Department of Signal Processing, Tampere University of Technology, Tampere, Finland; Department of Signal Processing, Tampere University of Technology, Tampere, Finland; Department of Signal Processing, Tampere University of Technology, Tampere, Finland; Department of Signal Processing, Tampere University of Technology, Tampere, Finland; Department of Signal Processing, Tampere University of Technology, Tampere, Finland","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","6","1291","1303","Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNNs) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a convolutional recurrent neural network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.","","","10.1109/TASLP.2017.2690575","European Research Council; European Unions H2020 Framework Programme; ERC; EVERYSOUND; Google's Faculty Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7933050","Convolutional neural networks (CNNs);deep neural networks;recurrent neural networks (RNNs);sound event detection","Recurrent neural networks;Event detection;Context;Feature extraction;Hidden Markov models;Speech","audio signals;recurrent neural nets;signal detection","convolutional recurrent neural networks;local spectral variation;local temporal variation;audio signals;sound recognition tasks;CRNN;polyphonic sound event detection task","","31","57","","","","","IEEE","IEEE Journals"
"Tactics to Directly Map CNN Graphs on Embedded FPGAs","K. Abdelouahab; M. Pelcat; J. Sérot; C. Bourrasset; F. Berry","Institut Pascal, Université Clermont Auvergne, Clermont-Ferrand, France; Institut Pascal, Université Clermont Auvergne, Clermont-Ferrand, France; Institut Pascal, Université Clermont Auvergne, Clermont-Ferrand, France; Atos/Bull, Montpellier, France; Institut Pascal, Université Clermont Auvergne, Clermont-Ferrand, France","IEEE Embedded Systems Letters","","2017","9","4","113","116","Deep convolutional neural networks (CNNs) are the state-of-the-art in image classification. Since CNN feed forward propagation involves highly regular parallel computation, it benefits from a significant speed-up when running on fine grain parallel programmable logic devices. As a consequence, several studies have proposed field-programmable gate array (FPGA)-based accelerators for CNNs. However, because of the large computational power required by CNNs, none of the previous studies has proposed a direct mapping of the CNN onto the physical resources of an FPGA, allocating each processing actor to its own hardware instance. In this letter, we demonstrate the feasibility of the so called direct hardware mapping (DHM) and discuss several tactics we explore to make DHM usable in practice. As a proof of concept, we introduce the HADDOC2 open source tool, that automatically transforms a CNN description into a synthesizable hardware description with platform-independent DHM.","","","10.1109/LES.2017.2743247","French Ministry of Higher Education MESR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8015156","Convolutional neural network (CNN);dataflow;field-programmable gate array (FPGA);VHSIC hardware description language (VHDL)","Field programmable gate arrays;Convolutional neural networks;Image classification;Computational modeling;Feature extraction","feedforward neural nets;field programmable gate arrays;graph theory;hardware description languages;image classification;learning (artificial intelligence);programmable logic devices;public domain software","computational power;direct mapping;physical resources;FPGA;DHM usable;HADDOC2 open source tool;synthesizable hardware description;embedded FPGAs;convolutional neural networks;image classification;CNN feed forward propagation;fine grain parallel programmable logic devices;field-programmable gate array;parallel computation;direct hardware mapping;CNN graphs","","6","17","","","","","IEEE","IEEE Journals"
"A Data-Driven Model of Tonal Chord Sequence Complexity","B. D. Giorgi; S. Dixon; M. Zanoni; A. Sarti","Dipartimento di Elettronica ed Informazione, Politecnico di Milano, Milan, Italy; Centre for Digital Music, School of Electronic Engineering and Computer Science, Queen Mary University of London, London, U.K.; Dipartimento di Elettronica ed Informazione, Politecnico di Milano, Milan, Italy; Dipartimento di Elettronica ed Informazione, Politecnico di Milano, Milan, Italy","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","11","2237","2250","We present a compound language model of tonal chord sequences, and evaluate its capability to estimate perceived harmonic complexity. In order to build the compound model, we trained three different models: prediction by partial matching, a hidden Markov model and a deep recurrent neural network on a novel large dataset containing half a million annotated chord sequences. We describe the training process and propose an interpretation of the harmonic patterns that are learned by the hidden states of these models. We use the compound model to generate new chord sequences and estimate their probability, which we then relate to perceived harmonic complexity. In order to collect subjective ratings of complexity, we devised a listening test comprising two different experiments. In the first, subjects choose the more complex chord sequence between two. In the second, subjects rate with a continuous scale the complexity of a single chord sequence. The results of both experiments show a strong relation between negative log probability, given by our language model, and the perceived complexity ratings. The relation is stronger for subjects with high musical sophistication index, acquired through the GoldMSI standard questionnaire. The analysis of the results also includes the preference ratings that have been collected along with the complexity ratings; a weak negative correlation emerged between preference and log probability.","","","10.1109/TASLP.2017.2756443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8049362","Harmonic complexity;chord sequences;language models","Hidden Markov models;Complexity theory;Harmonic analysis;Predictive models;Computational modeling;Training;Mathematical model","hidden Markov models;music;probability;recurrent neural nets","complex chord sequence;harmonic patterns;deep recurrent neural network;hidden Markov model;perceived harmonic complexity;compound language model;tonal chord sequence complexity;GoldMSI standard questionnaire;musical sophistication index;negative log probability;partial matching","","","52","Traditional","","","","IEEE","IEEE Journals"
"Convolution Neural Networks With Two Pathways for Image Style Recognition","T. Sun; Y. Wang; J. Yang; X. Hu","Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science, Nanjing University of Science and Technology, Nanjing, China; Department of Computer Science and Technology, Tsinghua National Laboratory for Information Science and Technology, Center for Brain-Inspired Computing Research, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","","2017","26","9","4102","4113","Automatic recognition of an image's style is important for many applications, including artwork analysis, photo organization, and image retrieval. Traditional convolution neural network (CNN) approach uses only object features for image style recognition. This approach may not be optimal, because the same object in two images may have different styles. We propose a CNN architecture with two pathways extracting object features and texture features, respectively. The object pathway represents the standard CNN architecture and the texture pathway intermixes the object pathway by outputting the gram matrices of intermediate features in the object pathway. The two pathways are jointly trained. In experiments, two deep CNNs, AlexNet and VGG-19, pretrained on the ImageNet classification data set are fine-tuned for this task. For any model, the two-pathway architecture performs much better than individual pathways, which indicates that the two pathways contain complementary information of an image's style. In particular, the model based on VGG-19 achieves the state-of-the-art results on three benchmark data sets, WikiPaintings, Flickr Style, and AVA Style.","","","10.1109/TIP.2017.2710631","National Basic Research Program (973 Program) of China; National Natural Science Foundation of China; Program for Changjiang Scholars; Grant from Sensetime; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7945535","Image style recognition;neural network","Image recognition;Feature extraction;Painting;Machine learning;Convolution;Computer architecture;Neural networks","feature extraction;image classification;image retrieval;image texture;neural nets","convolution neural network;image style recognition;artwork analysis;photo organization;image retrieval;standard CNN architecture;two pathways;object feature extraction;texture feature extraction;gram matrices;AlexNet CNN;VGG-19 CNN;ImageNet classification data set","","20","38","","","","","IEEE","IEEE Journals"
"Origami: A 803-GOp/s/W Convolutional Network Accelerator","L. Cavigelli; L. Benini","Department of Electrical Engineering and Information Technology, ETH Zurich, Zürich, Switzerland; Department of Electrical Engineering and Information Technology, ETH Zurich, Zürich, Switzerland","IEEE Transactions on Circuits and Systems for Video Technology","","2017","27","11","2461","2475","An ever-increasing number of computer vision and image/video processing challenges are being approached using deep convolutional neural networks, obtaining state-of-the-art results in object recognition and detection, semantic segmentation, action recognition, optical flow, and super resolution. Hardware acceleration of these algorithms is essential to adopt these improvements in embedded and mobile computer vision systems. We present a new architecture, design, and implementation, as well as the first reported silicon measurements of such an accelerator, outperforming previous work in terms of power, area, and I/O efficiency. The manufactured device provides up to 196 GOp/s on 3.09$\text {mm}^{2}$of silicon in UMC 65-nm technology and can achieve a power efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it the first architecture scalable to TOp/s performance.","","","10.1109/TCSVT.2016.2592330","Armasuisse Science and Technology; European Research Council through the MultiTherman Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7514941","Computer vision;convolutional networks (ConvNets);very large scale integration","Throughput;Computer architecture;Feature extraction;Neural networks;Machine learning;Computer vision;Mobile communication","","","","25","54","","","","","IEEE","IEEE Journals"
"Models for Music Analysis From a Markov Logic Networks Perspective","H. Papadopoulos; G. Tzanetakis","CNRS, Laboratoire des Signaux et Systèmes, France; Computer Science Department, University of Victoria, Victoria, BC, Canada","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2017","25","1","19","34","Analyzing and formalizing the intricate mechanisms of music is a very challenging goal for Artificial Intelligence. Dealing with real audio recordings requires the ability to handle both uncertainty and complex relational structure at multiple levels of representation. Until now, these two aspects have been generally treated separately, probability being the standard way to represent uncertainty in knowledge, while logical representation being the standard way to represent knowledge and complex relational information. Several approaches attempting a unification of logic and probability have recently been proposed. In particular, Markov logic networks (MLNs), which combine first-order logic and probabilistic graphical models, have attracted increasing attention in recent years in many domains. This paper introduces MLNs as a highly flexible and expressive formalism for the analysis of music that encompasses most of the commonly used probabilistic and logic-based models. We first review and discuss existing approaches for music analysis. We then introduce MLNs in the context of music signal processing by providing a deep understanding of how they specifically relate to traditional models, specifically hidden Markov models and conditional random fields. We then present a detailed application of MLNs for tonal harmony music analysis that illustrates the potential of this framework for music processing.","","","10.1109/TASLP.2016.2614351","Marie Curie International Outgoing Fellowship within the 7th European Community Framework Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7579173","Conditional random fields;chord;hidden Markov models;key;Markov logic networks;music information retrieval;musical structure;Statistical relational learning;tonal harmony","Hidden Markov models;Music;Probabilistic logic;Multiple signal classification;Computational modeling;Estimation;Analytical models","artificial intelligence;knowledge representation;Markov processes;music","Markov logic networks perspective;music analysis;artificial intelligence;first-order logic model;probabilistic graphical model;logic-based model;knowledge representation","","3","106","","","","","IEEE","IEEE Journals"
"Toward on-chip acceleration of the backpropagation algorithm using nonvolatile memory","P. Narayanan; A. Fumarola; L. L. Sanches; K. Hosokawa; S. C. Lewis; R. M. Shelby; G. W. Burr","NA; NA; NA; NA; NA; NA; NA","IBM Journal of Research and Development","","2017","61","4/5","11:1","11:11","By performing computation at the location of data, non-Von Neumann (VN) computing should provide power and speed benefits over conventional (e.g., VN-based) approaches to data-centric workloads such as deep learning. For the on-chip training of large-scale deep neural networks using nonvolatile memory (NVM) based synapses, success will require performance levels (e.g., deep neural network classification accuracies) that are competitive with conventional approaches despite the inherent imperfections of such NVM devices, and will also require massively parallel yet low-power read and write access. In this paper, we focus on the latter requirement, and outline the engineering tradeoffs in performing parallel reads and writes to large arrays of NVM devices to implement this acceleration through what is, at least locally, analog computing. We address how the circuit requirements for this new neuromorphic computing approach are somewhat reminiscent of, yet significantly different from, the well-known requirements found in conventional memory applications. We discuss tradeoffs that can influence both the effective acceleration factor (“speed”) and power requirements of such on-chip learning accelerators.","","","10.1147/JRD.2017.2716579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030206","","Nonvolatile memory;Training;System-on-chip;Performance evaluation;Biological neural networks","","","","8","29","","","","","IBM","IBM Journals"
"Active Inference, Curiosity and Insight","K. J. Friston; M. Lin; C. D. Frith; G. Pezzulo; J. A. Hobson; S. Ondobaka","Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London WC1N 3BG, U.K.k.friston@ucl.ac.uk; Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London WC1N 3BG, U.K.marco.lin91@gmail.com; Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London WC1N 3BG, and Institute of Philosophy, School of Advanced Studies, University of London EC1E 7HU, U.K.c.frith@ucl.ac.uk; Institute of Cognitive Sciences and Technologies, National Research Council, 7-00185 Rome, Italygiovanni.pezzulo@gmail.com; Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London WC1N 3BG, U.K., and Division of Sleep Medicine, Harvard Medical School, Boston, MA 02215, U.S.A.allan_hobson@hms.harvard.edu; Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London WC1N 3BG, U.K.s.ondobaka@ucl.ac.uk","Neural Computation","","2017","29","10","2633","2683","<para>This article offers a formal account of curiosity and insight in terms of active (Bayesian) inference. It deals with the dual problem of inferring states of the world and learning its statistical structure. In contrast to current trends in machine learning (e.g., deep learning), we focus on how people attain insight and understanding using just a handful of observations, which are solicited through curious behavior. We use simulations of abstract rule learning and approximate Bayesian inference to show that minimizing (expected) variational free energy leads to active sampling of novel contingencies. This epistemic behavior closes explanatory gaps in generative models of the world, thereby reducing uncertainty and satisfying curiosity. We then move from epistemic learning to model selection or structure learning to show how abductive processes emerge when agents test plausible hypotheses about symmetries (i.e., invariances or rules) in their generative models. The ensuing Bayesian model reduction evinces mechanisms associated with sleep and has all the hallmarks of “aha” moments. This formulation moves toward a computational account of consciousness in the pre-Cartesian sense of sharable knowledge (i.e., <italic toggle=""yes"">con</italic>: “together”; <italic toggle=""yes"">scire</italic>: “to know”).</para>","","","10.1162/neco_a_00999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8032552","","","","","","","","Traditional","","","","MITP",""
"Analysis of sentence embedding models using prediction tasks in natural language processing","Y. Adi; E. Kermany; Y. Belinkov; O. Lavi; Y. Goldberg","NA; NA; NA; NA; NA","IBM Journal of Research and Development","","2017","61","4/5","3:1","3:9","The tremendous success of word embeddings in improving the ability of computers to perform natural language tasks has shifted the research on language representation from word representation to focus on sentence representation. This shift introduced a plethora of methods for learning vector representations of sentences, many of them based on compositional methods over word embeddings. These vectors are used as features for subsequent machine learning tasks or for pretraining in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they encapsulate. Recent studies analyze the encoded representations and the kind of information they capture. In this paper, we analyze results from a previous study on the ability of models to encode basic properties such as content, order, and length. Our analysis led to new insights, such as the effect of word frequency or word distance on the ability to encode content and order.","","","10.1147/JRD.2017.2702858","CONSENSUS project; European Commission within its FP7 Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030297","","Decoding;Encoding;Semantics;Syntactics;Computational modeling;Analytical models;Predictive models","","","","1","32","","","","","IBM","IBM Journals"
"Rank Pooling for Action Recognition","B. Fernando; E. Gavves; J. Oramas M.; A. Ghodrati; T. Tuytelaars","ESAT-PSI, Australia National University, iMinds, Belgium; QUVA Lab, ESAT-PSI, University of Amsterdam, iMinds, NetherlandsBelgium; KU Leuven, ESAT-PSI, iMinds, Belgium; KU Leuven, ESAT-PSI, iMinds, Belgium; KU Leuven, ESAT-PSI, iMinds, Belgium","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","39","4","773","787","We propose a function-based temporal pooling method that captures the latent structure of the video sequence data - e.g., how frame-level features evolve over time in a video. We show how the parameters of a function that has been fit to the video data can serve as a robust new video representation. As a specific example, we learn a pooling function via ranking machines. By learning to rank the frame-level features of a video in chronological order, we obtain a new representation that captures the video-wide temporal dynamics of a video, suitable for action recognition. Other than ranking functions, we explore different parametric models that could also explain the temporal changes in videos. The proposed functional pooling methods, and rank pooling in particular, is easy to interpret and implement, fast to compute and effective in recognizing a wide variety of actions. We evaluate our method on various benchmarks for generic action, fine-grained action and gesture recognition. Results show that rank pooling brings an absolute improvement of 7-10 average pooling baseline. At the same time, rank pooling is compatible with and complementary to several appearance and local motion based methods and features, such as improved trajectories and deep learning features.","","","10.1109/TPAMI.2016.2558148","FP7; ERC; COGNIMUND; KU Leuven; DBOF; FWO; iMinds; High-Tech Visualization project; Australian Research Council Centre of Excellence for Robotic Vision; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7458903","Action recognition;temporal encoding;temporal pooling;rank pooling;video dynamics","Hidden Markov models;Dynamics;Training;Data models;Visualization;Feature extraction;Recurrent neural networks","gesture recognition;image representation;image sequences;video signal processing","rank pooling function;action recognition;function-based temporal pooling method;video sequence data latent structure;frame-level features;video representation;ranking machines;video-wide temporal dynamics;gesture recognition;local motion based methods","","85","75","","","","","IEEE","IEEE Journals"
"Super-Resolution for Remote Sensing Images via Local–Global Combined Network","S. Lei; Z. Shi; Z. Zou","Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2017","14","8","1243","1247","Super-resolution is an image processing technology that recovers a high-resolution image from a single or sequential low-resolution images. Recently deep convolutional neural networks (CNNs) have made a huge breakthrough in many tasks including super-resolution. In this letter, we propose a new single-image super-resolution algorithm named local-global combined networks (LGCNet) for remote sensing images based on the deep CNNs. Our LGCNet is elaborately designed with its “multifork” structure to learn multilevel representations of remote sensing images including both local details and global environmental priors. Experimental results on a public remote sensing data set (UC Merced) demonstrate an overall improvement of both accuracy and visual performance over several state-of-the-art algorithms.","","","10.1109/LGRS.2017.2704122","National Natural Science Foundation of China; Beijing Natural Science Foundation; funding project of the State Key Laboratory of Virtual Reality Technology and Systems, Beihang University; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937881","Convolutional neural networks (CNNs);local–global combined network (LGCNet);remote sensing images;super-resolution","Remote sensing;Spatial resolution;Training;Convolution;Image reconstruction","image processing;neural nets;remote sensing","remote sensing images;local-global combined network;image processing technology;high-resolution image;convolutional neural networks;single-image super-resolution algorithm","","18","22","","","","","IEEE","IEEE Journals"
"Unified Architecture for Multichannel End-to-End Speech Recognition With Neural Beamforming","T. Ochiai; S. Watanabe; T. Hori; J. R. Hershey; X. Xiao","Doshisha University, Kyotanabe, Japan; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Nanyang Technological University, Singapore","IEEE Journal of Selected Topics in Signal Processing","","2017","11","8","1274","1288","This paper proposes a unified architecture for end-to-end automatic speech recognition (ASR) to encompass microphone-array signal processing such as a state-of-the-art neural beamformer within the end-to-end framework. Recently, the end-to-end ASR paradigm has attracted great research interest as an alternative to conventional hybrid paradigms with deep neural networks and hidden Markov models. Using this novel paradigm, we simplify ASR architecture by integrating such ASR components as acoustic, phonetic, and language models with a single neural network and optimize the overall components for the end-to-end ASR objective: generating a correct label sequence. Although most existing end-to-end frameworks have mainly focused on ASR in clean environments, our aim is to build more realistic end-to-end systems in noisy environments. To handle such challenging noisy ASR tasks, we study multichannel end-to-end ASR architecture, which directly converts multichannel speech signal to text through speech enhancement. This architecture allows speech enhancement and ASR components to be jointly optimized to improve the end-to-end ASR objective and leads to an end-to-end framework that works well in the presence of strong background noise. We elaborate the effectiveness of our proposed method on the multichannel ASR benchmarks in noisy environments (CHiME-4 and AMI). The experimental results show that our proposed multichannel end-to-end system obtained performance gains over the conventional end-to-end baseline with enhanced inputs from a delay-and-sum beamformer (i.e., BeamformIT) in terms of character error rate. In addition, further analysis shows that our neural beamformer, which is optimized only with the end-to-end ASR objective, successfully learned a noise suppression function.","","","10.1109/JSTSP.2017.2764276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070987","Multichannel end-to-end ASR;neural beamformer;encoder-decoder network","Speech enhancement;Automatic speech recognition;Noise measurement;Array signal processing;Neural networks;Hidden Markov models;Microphones","acoustic signal processing;array signal processing;interference suppression;microphone arrays;neural nets;optimisation;speech enhancement;speech recognition","multichannel end-to-end speech recognition;neural beamforming;end-to-end automatic speech recognition;end-to-end ASR objective;multichannel end-to-end ASR architecture;speech enhancement;neural network;delay-and-sum beamformer;character error rate;microphone-array signal processing;noise suppression function","","10","47","Traditional","","","","IEEE","IEEE Journals"
"A 16-Gb/s 14.7-mW Tri-Band Cognitive Serial Link Transmitter With Forwarded Clock to Enable PAM-16/256-QAM and Channel Response Detection","Y. Du; W. Cho; P. Huang; Y. Li; C. Wong; J. Du; Y. Kim; B. Hu; L. Du; C. Liu; S. J. Lee; M. F. Chang","High Speed Electronics Laboratory, University of California, Los Angeles, CA, USA; High Speed Electronics Laboratory, University of California, Los Angeles, CA, USA; High Speed Electronics Laboratory, University of California, Los Angeles, CA, USA; High Speed Electronics Laboratory, University of California, Los Angeles, CA, USA; High Speed Electronics Laboratory, University of California, Los Angeles, CA, USA; High Speed Electronics Laboratory, University of California, Los Angeles, CA, USA; High Speed Electronics Laboratory, University of California, Los Angeles, CA, USA; High Speed Electronics Laboratory, University of California, Los Angeles, CA, USA; High Speed Electronics Laboratory, University of California, Los Angeles, CA, USA; High Speed Electronics Laboratory, University of California, Los Angeles, CA, USA; High Speed Electronics Laboratory, University of California, Los Angeles, CA, USA; High Speed Electronics Laboratory, University of California, Los Angeles, CA, USA","IEEE Journal of Solid-State Circuits","","2017","52","4","1111","1122","A cognitive tri-band transmitter (TX) with a forwarded clock using multiband signaling and high-order digital signal modulations is presented for serial link applications. The TX features learning an arbitrary channel response by sending a sweep of continuous wave, detecting power level at the receiver side, and then adapting modulation scheme, data bandwidth, and carrier frequencies accordingly based on detected channel information. The supported modulation scheme ranges from nonreturn to zero/Quadrature phase shift keying (QPSK) to Pulse-amplitude modulation (PAM) 16/256-Quadrature amplitude modulation(QAM). The proposed highly reconfigurable TX is capable of dealing with low-cost serial channels, such as low-cost connectors, cables, or multidrop buses with deep and narrow notches in the frequency domain (e.g., a 40-dB loss at notches). The adaptive multiband scheme mitigates equalization requirements and enhances the energy efficiency by avoiding frequency notches and utilizing the maximum available signal-to-noise ratio and channel bandwidth. The implemented TX prototype consumes a 14.7-mW power and occupies 0.016 mm2 in a 28-nm CMOS. It achieves a maximum data rate of 16 Gb/s with forwarded clock through one differential pair and the most energy efficient figure of merit of 20.4 μW/Gb/s/dB, which is calculated based on power consumption of transmitting per gigabits per second data and simultaneously overcoming per decibel worst case channel loss within the Nyquist frequency.","","","10.1109/JSSC.2016.2628049","Broadcom Foundation; Air Force Research Laboratory; Defense Advanced Research Projects Agency (Darpa); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7765148","Cognitive;continuous-time linear equalization (CTLE);decision feedback equalization (DFE);digital modulation;energy efficiency;feedforward equalization (FFE);forwarded clock;Inter-Symbol Interference (ISI);memory interface;uW/Gb/s/dB;multiband signaling;multidrop bus (MDB);multilevel signaling;nonreturn to zero (NRZ);pulse-amplitude modulation (PAM);quadrature amplitude modulation (QAM);serial link;source synchronous;transmitter (TX);wireline","Clocks;Frequency modulation;Baseband;OFDM;Decision feedback equalizers;Optical signal processing","channel estimation;clocks;CMOS digital integrated circuits;cognitive radio;decision feedback equalisers;energy conservation;power aware computing;power consumption;pulse amplitude modulation;quadrature amplitude modulation;radio transmitters","tri-band cognitive serial link transmitter;forwarded clock;PAM-16/256-QAM;channel response detection;multiband signaling;high-order digital signal modulations;continuous wave;power level detection;data bandwidth;carrier frequencies;channel information;nonreturn to zero-quadrature phase shift keying;QPSK;pulse-amplitude modulation;16/256-quadrature amplitude modulation;adaptive multiband scheme;energy efficiency enhancement;signal-to-noise ratio;channel bandwidth;CMOS technology;power consumption;worst case channel;decision feedback equalization;bit rate 16 Gbit/s;power 14.7 mW;size 28 nm","","12","25","","","","","IEEE","IEEE Journals"
"High-Order Local Pooling and Encoding Gaussians Over a Dictionary of Gaussians","P. Li; H. Zeng; Q. Wang; S. C. K. Shiu; L. Zhang","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Image Processing","","2017","26","7","3372","3384","Local pooling (LP) in configuration (feature) space proposed by Boureau et al. explicitly restricts similar features to be aggregated, which can preserve as much discriminative information as possible. At the time it appeared, this method combined with sparse coding achieved competitive classification results with only a small dictionary. However, its performance lags far behind the state-of-the-art results as only the zero-order information is exploited. Inspired by the success of high-order statistical information in existing advanced feature coding or pooling methods, we make an attempt to address the limitation of LP. To this end, we present a novel method called high-order LP (HO-LP) to leverage the information higher than the zero-order one. Our idea is intuitively simple: we compute the first- and second-order statistics per configuration bin and model them as a Gaussian. Accordingly, we employ a collection of Gaussians as visual words to represent the universal probability distribution of features from all classes. Our problem is naturally formulated as encoding Gaussians over a dictionary of Gaussians as visual words. This problem, however, is challenging since the space of Gaussians is not a Euclidean space but forms a Riemannian manifold. We address this challenge by mapping Gaussians into the Euclidean space, which enables us to perform coding with common Euclidean operations rather than complex and often expensive Riemannian operations. Our HO-LP preserves the advantages of the original LP: pooling only similar features and using a small dictionary. Meanwhile, it achieves very promising performance on standard benchmarks, with either conventional, hand-engineered features or deep learning-based features.","","","10.1109/TIP.2017.2695884","National Natural Science Foundation of China; Hong Kong RGC GRF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7904668","Image classification;high-order local pooling (HO-LP);manifold of Gaussians","Image coding;Dictionaries;Feature extraction;Encoding;Covariance matrices;Manifolds;Kernel","encoding;Gaussian processes","encoding Gaussians;high order local pooling;dictionary of Gaussians;configuration feature space;discriminative information;sparse coding;zero-order information;high-order statistical information;advanced feature coding;visual words;Riemannian manifold;Euclidean space;expensive Riemannian operations","","5","64","","","","","IEEE","IEEE Journals"
"Low-Resource Footprint, Data-Driven Malware Detection on Android","S. Aonzo; A. Merlo; M. Migliardi; L. Oneto; F. Palmieri","DIBRIS, University of Genova, Genova, GE Italy (e-mail: simone.aonzo@unige.it); DIBRIS, University of Genova, Genova, GE Italy 16145 (e-mail: alessio.merlo@unige.it); Dipartimento di Ingegneria dell Informazione, University of Padua, Italy, Padova, PD Italy (e-mail: mauro.migliardi@unipd.it); DIBRIS, University of Genova, Genova, GE Italy (e-mail: loca.oneto@unige.it); Department of Computer Science, University of Salerno, Fisciano, Salerno Italy (e-mail: fpalmieri@unisa.it)","IEEE Transactions on Sustainable Computing","","2017","PP","99","1","1","Resource-constrained systems are becoming more and more common as users migrate from PCs to mobile devices and as IoT systems enter the mainstream. At the same time, it is not acceptable to reduce the level of security hence it is necessary to accommodate the required security into the system-imposed resource constraints. This paper introduces BAdDroIds, a mobile application leveraging machine learning for detecting malware on resource constrained devices. BAdDroIds executes in background and transparently analyzes the applications as soon as they are installed, i.e., before infecting the device. BAdDroIds relies on static analysis techniques and features provided by the Android OS to build up sound and complete models of Android apps in terms of permissions and API invocations. It uses ad-hoc supervised classification techniques to allow resource-efficient malware detection. By exploiting the intrinsic nature of data, it has been possible to implement a state-of-the-art data-driven model which provides deep insights on the detection problem and can be efficiently executed on the device itself as it requires a very limited computational effort. Besides its limited resource footprint, BAdDroIds is extremely effective: an extensive experimental evaluation shows that BAdDroIds outperforms the currently available solutions in terms of accuracy, which is around 99%.","","","10.1109/TSUSC.2017.2774184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8113505","","Malware;Androids;Humanoid robots;Computational modeling;Feature extraction;Security","","","","","","","","","","IEEE","IEEE Early Access Articles"
"SAR Image Despeckling Using a Convolutional Neural Network","P. Wang; H. Zhang; V. M. Patel","Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ, USA; Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ, USA; Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ, USA","IEEE Signal Processing Letters","","2017","24","12","1763","1767","Synthetic aperture radar (SAR) images are often contaminated by a multiplicative noise known as speckle. Speckle makes the processing and interpretation of SAR images difficult. We propose a deep-learning-based approach called, image despeckling convolutional neural network (ID-CNN), for automatically removing speckle from the input noisy images. In particular, ID-CNN uses a set of convolutional layers along with batch normalization and rectified linear unit activation function and a componentwise division residual layer to estimate speckle and it is trained in an end-to-end fashion using a combination of Euclidean loss and total variation loss. Extensive experiments on synthetic and real SAR images show that the proposed method achieves significant improvements over the state-of-the-art speckle reduction methods.","","","10.1109/LSP.2017.2758203","ARO; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8053792","Denoising;despecking;image restoration;synthetic aperture radar (SAR)","Synthetic aperture radar;Noise measurement;TV;Convolution;Image restoration;Training;Speckle","image denoising;neural nets;radar imaging;speckle;synthetic aperture radar","synthetic aperture radar images;ID-CNN;input noisy images;SAR image despeckling;convolutional neural network;speckle reduction methods;batch normalization;rectified linear unit activation function;Euclidean loss","","27","34","Traditional","","","","IEEE","IEEE Journals"
"Structure-Aware Linear Solver for Realtime Convex Optimization for Embedded Systems","I. Yamazaki; S. Nooshabadi; S. Tomov; J. Dongarra","Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Department of Electrical and Computer Engineering, Michigan Tech University, Houghton, MI, USA; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA","IEEE Embedded Systems Letters","","2017","9","3","61","64","With the increasing sophistication in the use of optimization algorithms such as deep learning on embedded systems, the convex optimization solvers on embedded systems have found widespread use. This letter presents a novel linear solver technique to reduce the run-time of convex optimization solver by using the property that some parameters are fixed during the solution iterations of a solve instance. Our experimental results show that the run-time can be reduced by two orders of magnitude.","","","10.1109/LES.2017.2700401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917357","Karush Kuhn Tucker (KKT);realtime embedded convex optimization solver","Convex functions;Linear systems;Matrices;Embedded systems;Optimization;Symmetric matrices;Linear matrix inequalities","convex programming;embedded systems","structure-aware linear solver;realtime convex optimization;embedded systems;convex optimization solvers;linear solver technique","","","16","","","","","IEEE","IEEE Journals"
"Reliability of nand-Based SSDs: What Field Studies Tell Us","B. Schroeder; A. Merchant; R. Lagisetty","Department of Computer Science, University of Toronto, Toronto, ON, Canada; Google Inc., Mountain View, CA, USA; Google Inc., Mountain View, CA, USA","Proceedings of the IEEE","","2017","105","9","1751","1769","Solid-state drives (SSDs) based on NAND flash are making deep inroads into data centers as well as the consumer market. In 2016, manufacturers shipped more than 130 million units totaling around 50 Exabytes of storage capacity. As the amount of data stored on solid state drives keeps increasing, it is important to understand the reliability characteristics of these devices. For a long time, our knowledge about flash reliability was derived from controlled experiments in lab environments under synthetic workloads, often using methods for accelerated testing. However, within the last two years, three large-scale field studies have been published that report on the failure behavior of flash devices in production environments subjected to real workloads and operating conditions. The goal of this paper is to provide an overview of what we have learned about flash reliability in production, and where appropriate contrasting it with prior studies performing controlled experiments.","","","10.1109/JPROC.2017.2735969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8013175","Failure;field study;flash technology;production systems;reliability;solid-state drives (SSDs);uncorrectable errors","Reliability;Hardware;Maintenance engineering;Microprogramming;Production;Drives;Solid-state circuits","failure analysis;flash memories;integrated circuit reliability;NAND circuits","solid-state drives;SSD;NAND flash;reliability characteristics;flash reliability;accelerated testing;failure behavior;field studies","","6","41","","","","","IEEE","IEEE Journals"
"Preface: Deep learning","","","IBM Journal of Research and Development","","2017","61","4/5","1","5","","","","10.1147/JRD.2017.2712370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036353","","","","","","","","","","","","IBM","IBM Journals"
"PDP: parallel dynamic programming","F. Wang; J. Zhang; Q. Wei; X. Zheng; L. Li","The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences U+0028 SKL-MCCS, CASIA U+0029, Beijing 100190, China; The State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences U+0028 SKL-MCCS, CASIA U+0029, Beijing 100190, China; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN 55414, USA; Department of Automation, Tsinghua University, Beijing 100084, China","IEEE/CAA Journal of Automatica Sinica","","2017","4","1","1","5","Deep reinforcement learning is a focus research area in artificial intelligence. The principle of optimality in dynamic programming is a key to the success of reinforcement learning methods. The principle of adaptive dynamic programming U+0028 ADP U+0029 is first presented instead of direct dynamic programming U+0028 DP U+0029, and the inherent relationship between ADP and deep reinforcement learning is developed. Next, analytics intelligence, as the necessary requirement, for the real reinforcement learning, is discussed. Finally, the principle of the parallel dynamic programming, which integrates dynamic programming and analytics intelligence, is presented as the future computational intelligence.","","","10.1109/JAS.2017.7510310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815546","","","dynamic programming;learning (artificial intelligence);parallel programming","PDP;parallel dynamic programming;deep reinforcement learning;artificial intelligence;optimality principle;adaptive dynamic programming U+0028 ADP U+0029;analytics intelligence;real reinforcement learning;computational intelligence","","14","","","","","","IEEE","IEEE Journals"
"Special Issue on Biomedical Big Data: Understanding, Learning and Applications","J. Zhu; A. Liu; M. Chen; T. Tasdizen; H. Su","NA; NA; NA; NA; NA","IEEE Transactions on Big Data","","2017","3","4","375","377","The papers in this special issue focus on biomedical Big Data. Biomedical imaging is an essential component in various fields of biomedical research and clinical practice. The study of biologists requires continuous monitoring of cell behavior under microscope. Neuroscientists detect regional metabolic brain activity from positron emission tomography (PET), functional magnetic resonance imaging (MRI), and magnetic resonance spectrum imaging (MRSI) scans. During these researching process, large amount of biomedical data will be produced for processing. The development of advanced imaging equipment and diverse applications also have driven the generation of biomedical big data. The main challenge and bottleneck for the related research is the conversion of “biomedical big data” into interpretable information and hence discoveries. Computer vision theory has a huge potential in many aspects for automated understanding of biomedical data and has been used successfully to speed up and improve applications such as large-scale cell image analysis (image preconditioning, cell segmentation and detection, cell tracking, and cell behavior identification), image reconstruction and registration, organ segmentation and disease classification. Considering the recent advance in machine learning technique, deep learning has revolutionized multiple fields of computer vision, significantly pushing the state of arts of computer vision systems in a broad array of high-level tasks. Hopefully these technique advance will help to deal problems in biomedical big data.","","","10.1109/TBDATA.2017.2772930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8124126","","Special issues and sections;Big Data;Biomedical imaging;Brain computer interfaces;Neuroimaging;Computer vision;Image segmentation","","","","1","","Traditional","","","","IEEE","IEEE Journals"
"IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming","","","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","4","1016","1016","Advertisement, IEEE.","","","10.1109/TNNLS.2017.2678899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7879500","","","","","","","","","","","","IEEE","IEEE Journals"
"IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming","","","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","1","244","244","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","","","10.1109/TNNLS.2016.2640821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797566","","","","","","","","","","","","IEEE","IEEE Journals"
"IEEE Transactions on Neural Networks and Learning Systems special section on deep reinforcement learning and adaptive dynamic programming","","","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","2","490","490","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","","","10.1109/TNNLS.2017.2652580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823114","","","","","","","","","","","","IEEE","IEEE Journals"
"IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming","","","IEEE Transactions on Neural Networks and Learning Systems","","2017","28","3","772","772","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","","","10.1109/TNNLS.2017.2655663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7857119","","","","","","","","","","","","IEEE","IEEE Journals"
"IEEE Transactions on Sustainable Computing Call for Papers for Special Issue on “Smart Data and Deep Learning in Sustainable Computing” (SDDL)","","","IEEE Transactions on Sustainable Computing","","2017","2","2","228","229","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","","","10.1109/TSUSC.2017.2721858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979655","","","","","","","","","","","","IEEE","IEEE Journals"
"An Introduction to the October-December issue","P. Brusilovsky; M. Sharples","NA; NA","IEEE Transactions on Learning Technologies","","2017","10","4","403","404","Welcome to the last issue of 2017, the second extended issue of this year which brings together 12 papers. We start the issue with two survey papers that offer deep analyses of popular research areas. All 12 papers are briefly summarized.","","","10.1109/TLT.2017.2776818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8207729","","","","","","","","Traditional","","","","IEEE","IEEE Journals"
"Photo essay [Automation Robotics]","T. Fryer","NA","Engineering & Technology","","2017","12","4","40","41","The quest for ping pong perfection goes on. Since Forpheus made its debut in 2014, the developer Omron has deployed sequential deep learning to assess an opponent's level of skill as a rally unfolds, adapting returns to the ability of its human counterpart.","","","10.1049/et.2017.0424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8386400","","","","","","","","","","","","IET","IET Journals"
