"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,"Reference Count","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Deep multiple instance learning for image classification and auto-annotation","J. Wu; Yinan Yu; Chang Huang; Kai Yu","Massachusetts Institute of Technology, USA; Institute of Deep Learning, Baidu, USA; Institute of Deep Learning, Baidu, USA; Institute of Deep Learning, Baidu, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3460","3469","The recent development in learning deep representations has demonstrated its wide applications in traditional vision tasks like classification and detection. However, there has been little investigation on how we could build up a deep learning framework in a weakly supervised setting. In this paper, we attempt to model deep learning in a weakly supervised learning (multiple instance learning) framework. In our setting, each image follows a dual multi-instance assumption, where its object proposals and possible text annotations can be regarded as two instance sets. We thus design effective systems to exploit the MIL property with deep learning strategies from the two ends; we also try to jointly learn the relationship between object and annotation proposals. We conduct extensive experiments and prove that our weakly supervised deep learning framework not only achieves convincing performance in vision tasks including classification and image annotation, but also extracts reasonable region-keyword pairs with little supervision, on both widely used benchmarks like PASCAL VOC and MIT Indoor Scene 67, and also a dataset for image-and patch-level annotations.","","","10.1109/CVPR.2015.7298968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298968","","Machine learning;Proposals;Visualization;Feature extraction;Noise measurement;Neural networks;Supervised learning","image classification;learning (artificial intelligence)","deep multiple instance learning;MIL;image classification;image annotation;supervised deep learning framework","","111","49","","","","","IEEE","IEEE Conferences"
"Deep supervised learning for hyperspectral data classification through convolutional neural networks","K. Makantasis; K. Karantzalos; A. Doulamis; N. Doulamis","Technical University of Crete, University campus, Kounoupidiana, 73100, Chania, Greece; National Technical University of Athens, Zographou campus, 15780, Athens, Greece; National Technical University of Athens, Zographou campus, 15780, Athens, Greece; National Technical University of Athens, Zographou campus, 15780, Athens, Greece","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","4959","4962","Spectral observations along the spectrum in many narrow spectral bands through hyperspectral imaging provides valuable information towards material and object recognition, which can be consider as a classification task. Most of the existing studies and research efforts are following the conventional pattern recognition paradigm, which is based on the construction of complex handcrafted features. However, it is rarely known which features are important for the problem at hand. In contrast to these approaches, we propose a deep learning based classification method that hierarchically constructs high-level features in an automated way. Our method exploits a Convolutional Neural Network to encode pixels' spectral and spatial information and a Multi-Layer Perceptron to conduct the classification task. Experimental results and quantitative validation on widely used datasets showcasing the potential of the developed approach for accurate hyperspectral data classification.","","","10.1109/IGARSS.2015.7326945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326945","Machine learning;Earth observation;Imaging spectroscopy;Object Recognition","Hyperspectral imaging;Training;Machine learning;Accuracy;Neural networks;Support vector machines","geophysical techniques;hyperspectral imaging;image classification;neural nets","deep supervised learning;hyperspectral data classification;convolutional neural networks;spectral observation;narrow spectral bands;hyperspectral imaging;conventional pattern recognition paradigm;complex handcrafted construction;deep learning based classification method","","147","11","","","","","IEEE","IEEE Conferences"
"Improving deep convolutional neural networks with unsupervised feature learning","K. Nguyen; C. Fookes; S. Sridharan","Image and Video lab, SAIVT Research Program, Queensland University of Technology, Brisbane, Australia; Image and Video lab, SAIVT Research Program, Queensland University of Technology, Brisbane, Australia; Image and Video lab, SAIVT Research Program, Queensland University of Technology, Brisbane, Australia","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","2270","2274","The latest generation of Deep Convolutional Neural Networks (DCNN) have dramatically advanced challenging computer vision tasks, especially in object detection and object classification, achieving state-of-the-art performance in several computer vision tasks including text recognition, sign recognition, face recognition and scene understanding. The depth of these supervised networks has enabled learning deeper and hierarchical representation of features. In parallel, unsupervised deep learning such as Convolutional Deep Belief Network (CDBN) has also achieved state-of-the-art in many computer vision tasks. However, there is very limited research on jointly exploiting the strength of these two approaches. In this paper, we investigate the learning capability of both methods. We compare the output of individual layers and show that many learnt filters and outputs of the corresponding level layer are almost similar for both approaches. Stacking the DCNN on top of unsupervised layers or replacing layers in the DCNN with the corresponding learnt layers in the CDBN can improve the recognition/classification accuracy and training computational expense. We demonstrate the validity of the proposal on ImageNet dataset.","","","10.1109/ICIP.2015.7351206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351206","Deep learning;Convolutional Neural Network;Deep Convolutional Belief Network;Unsupervised deep learning;Supervised deep learning","Training;Machine learning;Computer architecture;Unsupervised learning;Neural networks;Supervised learning;Convolutional codes","computer vision;feature extraction;image classification;image representation;learning (artificial intelligence);neural nets;object detection","ImageNet dataset;hierarchical feature representation;scene understanding;face recognition;sign recognition;text recognition;object classification;object detection;computer vision;DCNN;unsupervised feature learning;deep convolutional neural network","","11","21","","","","","IEEE","IEEE Conferences"
"Scene Recognition by Manifold Regularized Deep Learning Architecture","Y. Yuan; L. Mou; X. Lu","Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, Shaanxi, P. R. China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, Shaanxi, P. R. China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, Shaanxi, P. R. China","IEEE Transactions on Neural Networks and Learning Systems","","2015","26","10","2222","2233","Scene recognition is an important problem in the field of computer vision, because it helps to narrow the gap between the computer and the human beings on scene understanding. Semantic modeling is a popular technique used to fill the semantic gap in scene recognition. However, most of the semantic modeling approaches learn shallow, one-layer representations for scene recognition, while ignoring the structural information related between images, often resulting in poor performance. Modeled after our own human visual system, as it is intended to inherit humanlike judgment, a manifold regularized deep architecture is proposed for scene recognition. The proposed deep architecture exploits the structural information of the data, making for a mapping between visible layer and hidden layer. By the proposed approach, a deep architecture could be designed to learn the high-level features for scene recognition in an unsupervised fashion. Experiments on standard data sets show that our method outperforms the state-of-the-art used for scene recognition.","","","10.1109/TNNLS.2014.2359471","National Basic Research Program (973 Program) of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018034","Deep architecture;machine learning;manifold kernel;manifold regularization;scene recognition.;Deep architecture;machine learning;manifold kernel;manifold regularization;scene recognition","Kernel;Manifolds;Computer architecture;Semantics;Visualization;Sparse matrices;Feature extraction","computer vision;image recognition;learning (artificial intelligence)","scene recognition;manifold regularized deep learning architecture;computer vision;semantic modeling;semantic modeling approach;structural information;human visual system","Algorithms;Humans;Learning;Neural Networks (Computer);Pattern Recognition, Automated;Pattern Recognition, Visual;Photic Stimulation","104","53","","","","","IEEE","IEEE Journals"
"An Energy-Efficient and Scalable Deep Learning/Inference Processor With Tetra-Parallel MIMD Architecture for Big Data Applications","S. Park; J. Park; K. Bong; D. Shin; J. Lee; S. Choi; H. Yoo","Department of Electrical Engineering, KAIST, Daejeon, Korea (the Republic of); School of Electrical Engineering, KAIST, Daejeon, Korea (the Republic of); School of Electrical Engineering, KAIST, Daejeon, Korea (the Republic of); KAIST, Daejeon, Korea (the Republic of); School of Electrical Engineering, KAIST, Daejeon, Korea (the Republic of); School of Electrical Engineering, KAIST, Daejeon, Korea (the Republic of); KAIST, Daejeon, Korea, Republic of","IEEE Transactions on Biomedical Circuits and Systems","","2015","9","6","838","848","Deep Learning algorithm is widely used for various pattern recognition applications such as text recognition, object recognition and action recognition because of its best-in-class recognition accuracy compared to hand-crafted algorithm and shallow learning based algorithms. Long learning time caused by its complex structure, however, limits its usage only in high-cost servers or many-core GPU platforms so far. On the other hand, the demand on customized pattern recognition within personal devices will grow gradually as more deep learning applications will be developed. This paper presents a SoC implementation to enable deep learning applications to run with low cost platforms such as mobile or portable devices. Different from conventional works which have adopted massively-parallel architecture, this work adopts task-flexible architecture and exploits multiple parallelism to cover complex functions of convolutional deep belief network which is one of popular deep learning/inference algorithms. In this paper, we implement the most energy-efficient deep learning and inference processor for wearable system. The implemented 2.5 mm ×4.0 mm deep learning/inference processor is fabricated using 65 nm 8-metal CMOS technology for a battery-powered platform with real-time deep inference and deep learning operation. It consumes 185 mW average power, and 213.1 mW peak power at 200 MHz operating frequency and 1.2 V supply voltage. It achieves 411.3 GOPS peak performance and 1.93 TOPS/W energy efficiency, which is 2.07× higher than the state-of-the-art.","","","10.1109/TBCAS.2015.2504563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7384530","Convolutional deep belief networks;deep inference;deep learning;fog computing;semi-supervised learning","Machine learning;Big data;Neurons;Convolution;Pattern recognition;Graphics processing units;Parallel architectures","belief networks;Big Data;biomedical engineering;learning (artificial intelligence);program processors;system-on-chip","scalable deep learning-inference processor;tetraparallel MIMD architecture;Big data applications;deep Learning algorithm;pattern recognition applications;text recognition;object recognition;action recognition;best-in-class recognition accuracy;hand-crafted algorithm;shallow learning based algorithms;GPU platforms;SoC implementation;mobile device;portable device;energy-efficient deep learning;wearable system;CMOS technology;battery-powered platform","Algorithms;Equipment and Supplies;Learning;Pattern Recognition, Physiological;Signal Processing, Computer-Assisted;Software","8","27","","","","","IEEE","IEEE Journals"
"The Research of Feedback-Feedforward Iterative Learning Control in Hydrodynamic Deep Drawing Process","S. Shi","NA","2015 14th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES)","","2015","","","423","426","This paper firstly introduces the characteristics of hydrodynamic deep drawing (HDD), which is an important sheet metal forming technology, then points out the necessity of the chamber pressure control. Secondly considering the characteristics of the drawing action is repeated, iterative learning control (ILC) is the proper algorithm. Then introduces the concept of iterative learning control and feedback - feed forward iterative learning control to solve the delay and improve system robustness. Finally, the computer iterative learning control algorithm implementation process is given and the effectiveness of the algorithm is verified by simulation.","","","10.1109/DCABES.2015.112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429646","Feedback;feedforward;Iterative Learning Control;hydrodynamic deep drawing;pressure control","Iterative learning control;Feedforward neural networks;Hydrodynamics;Process control;Trajectory;Robustness;Algorithm design and analysis","deep drawing;feedback;feedforward;hydrodynamics;iterative learning control;pressure control;sheet metal processing","feedback-feedforward iterative learning control;hydrodynamic deep drawing process;HDD;sheet metal forming technology;chamber pressure control;ILC","","","5","","","","","IEEE","IEEE Conferences"
"Chest pathology detection using deep learning with non-medical training","Y. Bar; I. Diamant; L. Wolf; S. Lieberman; E. Konen; H. Greenspan","The Blavatnik School of Computer Science, Tel-Aviv University, Tel Aviv 69978, Israel; Department of Biomedical Engineering, Tel-Aviv University, Tel Aviv 69978, Israel; The Blavatnik School of Computer Science, Tel-Aviv University, Tel Aviv 69978, Israel; Diagnostic Imaging Department, Sheba Medical Center, Tel Hashomer, Israel; Diagnostic Imaging Department, Sheba Medical Center, Tel Hashomer, Israel; Department of Biomedical Engineering, Tel-Aviv University, Tel Aviv 69978, Israel","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","","2015","","","294","297","In this work, we examine the strength of deep learning approaches for pathology detection in chest radiographs. Convolutional neural networks (CNN) deep architecture classification approaches have gained popularity due to their ability to learn mid and high level image representations. We explore the ability of CNN learned from a non-medical dataset to identify different types of pathologies in chest x-rays. We tested our algorithm on a 433 image dataset. The best performance was achieved using CNN and GIST features. We obtained an area under curve (AUC) of 0.87-0.94 for the different pathologies. The results demonstrate the feasibility of detecting pathology in chest x-rays using deep learning approaches based on non-medical learning. This is a first-of-its-kind experiment that shows that Deep learning with ImageNet, a large scale non-medical image database may be a good substitute to domain specific representations, which are yet to be available, for general medical image recognition tasks.","","","10.1109/ISBI.2015.7163871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163871","Chest Radiography;Computer-Aided Diagnosis Disease Categorization;Deep Learning;Deep Networks;CNN","Pathology;Machine learning;Visualization;X-rays;Biomedical imaging;Diagnostic radiography;Feature extraction","convolution;diagnostic radiography;diseases;feature extraction;image classification;image representation;learning (artificial intelligence);medical image processing;neural nets","chest pathology detection;deep learning;nonmedical training;chest radiograph;convolutional neural network;CNN deep architecture classification;mid level image representation learning;high level image representation learning;CNN learning;pathology identification;pathology type;chest X-ray image dataset;CNN algorithm;GIST feature;area under curve;AUC;nonmedical learning;ImageNet;large scale nonmedical image database;domain specific representation;general medical image recognition task","","86","15","","","","","IEEE","IEEE Conferences"
"Deep Belief Networks and deep learning","Yuming Hua; Junhai Guo; Hua Zhao","Beijing Institute of Tracking and Telecommunications Technology, China; Beijing Institute of Tracking and Telecommunications Technology, China; Beijing Institute of Tracking and Telecommunications Technology, China","Proceedings of 2015 International Conference on Intelligent Computing and Internet of Things","","2015","","","1","4","Deep Belief Network is an algorithm among deep learning. It is an effective method of solving the problems from neural network with deep layers, such as low velocity and the overfitting phenomenon in learning. In this paper, we will introduce how to process a Deep Belief Network by using Restricted Boltzmann Machines. What is more, we will combine the Deep Belief Network together with softmax classifier, and use it in the recognition of handwritten numbers.","","","10.1109/ICAIOT.2015.7111524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7111524","Deep learning;Deep Belief Network;classify Introduction","Neural networks;Training;Feature extraction;Unsupervised learning;Classification algorithms;Fitting;Mathematical model","belief networks;Boltzmann machines;learning (artificial intelligence)","deep belief networks;deep learning;neural network;deep layers;restricted Boltzmann machines;softmax classifier;handwritten number recognition","","2","15","","","","","IEEE","IEEE Conferences"
"Deep learning for pattern learning and recognition","C. L. P. Chen","Faculty of Science and Technology, The University of Macau, Macau SAR, China","2015 IEEE 10th Jubilee International Symposium on Applied Computational Intelligence and Informatics","","2015","","","17","17","Summary form only given. Deep learning is a set of algorithms in machine learning that attempt to learn in multiple levels, corresponding to different levels of abstraction. It is typically used to abstract useful information from data. The levels in these learned statistical models correspond to distinct levels of concepts, where higher-level concepts are defined from lower-level ones, and the same lower level concepts can help to define many higher-level concepts. Alternatively, the main advantage of deep learning is about learning multiple levels of representation and abstraction that help to make sense of data such as images, sound, and text. This talk is to overview the foundationa, data representation capability of deep networks, and to investigate efficient deep learning algorithms, and meaningful applications.","","","10.1109/SACI.2015.7208200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208200","","Machine learning;Computational intelligence;Informatics;Pattern recognition;Machine learning algorithms;Algorithm design and analysis","data structures;learning (artificial intelligence);statistical analysis","pattern learning;pattern recognition;machine learning;statistical models;higher-level concepts;data representation capability;deep networks;deep learning algorithms","","1","","","","","","IEEE","IEEE Conferences"
"Deep learning based primary user classification in Cognitive Radios","Y. Cui; X. j. Jing; S. Sun; X. Wang; D. Cheng; H. Huang","Key Laboratory of Trustworthy Distributed Computing and service (BUPT), Ministry of Education, Beijing University of Posts and Telecommunications, China; Key Laboratory of Trustworthy Distributed Computing and service (BUPT), Ministry of Education, Beijing University of Posts and Telecommunications, China; Key Laboratory of Trustworthy Distributed Computing and service (BUPT), Ministry of Education, Beijing University of Posts and Telecommunications, China; Key Laboratory of Trustworthy Distributed Computing and service (BUPT), Ministry of Education, School of Software and Microelectronics, Beijing China; Key Laboratory of Trustworthy Distributed Computing and service (BUPT), Ministry of Education, 305 Hospital of PLA, Beijing China; Key Laboratory of Trustworthy Distributed Computing and service (BUPT), Ministry of Education, Beijing University of Posts and Telecommunications, China","2015 15th International Symposium on Communications and Information Technologies (ISCIT)","","2015","","","165","168","Deep Belief Networks (DBN) is a very powerful algorithm in deep learning. The DBN has been effectively applied in many areas of machine learning, such as computer vision (CV) and natural language processing (NLP). With the help of deep architecture, their accuracy has been largely improved and their human annotation data which traditional machine learning algorithm extremely rely on could be reduced. In Cognitive Radios (CRs), learning is necessary for its cognition, while two of the key challenges are how to classify primary user agents with their performances and predict their behaviors. The CRs' performance has a positive correlation with the hit rate of learning algorithm's classification and prediction results. In this paper, we study the questions of classification and prediction of user agents. We apply the DBN model to improve accuracy rating of user agent's recognition in CRs with user-centered model, it's the first application of deep learning structure in CRs. The DBN model provides a primary user agent's classification, which is the foundation of the prediction to both idle frequency spectrums and time slots. Experimental results show that the cognitive engine finds a much better detection rate than the CRs engine with shallow learning and other traditional strategy. The simulation results are also tested on the WIFI channel with 5GHz and 2.4GHz.","","","10.1109/ISCIT.2015.7458333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7458333","deep learning;cognitive radio;deep belief networks;spectrum sensing","Training;Machine learning algorithms;Classification algorithms;Prediction algorithms;Machine learning;Sensors;Artificial neural networks","cognitive radio;learning (artificial intelligence)","deep belief learning based primary user classification;cognitive radios;machine learning;computer vision;natural language processing;DBN model;user-centered model;idle frequency spectrums;time slots;WIFI channel","","3","12","","","","","IEEE","IEEE Conferences"
"Blind Image Quality Assessment via Deep Learning","W. Hou; X. Gao; D. Tao; X. Li","School of Electronic Engineering, Xidian University, 2 South Taibai Road, Xi’an, Shaanxi, P. R. China; School of Electronic Engineering, Xidian University, 2 South Taibai Road, Xi’an, Shaanxi, P. R. China; Centre for Quantum Computation and Intelligent Systems and the Faculty of Engineering and Information Technology, University of Technology, Sydney, 235 Jones Street, Ultimo, NSW, Australia; Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, Shaanxi, P. R. China","IEEE Transactions on Neural Networks and Learning Systems","","2015","26","6","1275","1286","This paper investigates how to blindly evaluate the visual quality of an image by learning rules from linguistic descriptions. Extensive psychological evidence shows that humans prefer to conduct evaluations qualitatively rather than numerically. The qualitative evaluations are then converted into the numerical scores to fairly benchmark objective image quality assessment (IQA) metrics. Recently, lots of learning-based IQA models are proposed by analyzing the mapping from the images to numerical ratings. However, the learnt mapping can hardly be accurate enough because some information has been lost in such an irreversible conversion from the linguistic descriptions to numerical scores. In this paper, we propose a blind IQA model, which learns qualitative evaluations directly and outputs numerical scores for general utilization and fair comparison. Images are represented by natural scene statistics features. A discriminative deep model is trained to classify the features into five grades, corresponding to five explicit mental concepts, i.e., excellent, good, fair, poor, and bad. A newly designed quality pooling is then applied to convert the qualitative labels into scores. The classification framework is not only much more natural than the regression-based models, but also robust to the small sample size problem. Thorough experiments are conducted on popular databases to verify the model's effectiveness, efficiency, and robustness.","","","10.1109/TNNLS.2014.2336852","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Program for Changjiang Scholars and Innovative Research Team with the University of China; Shaanxi Innovative Research Team for Key Science and Technology; Key Research Program, Chinese Academy of Sciences, Beijing, China; Australian Research Counci; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6872541","Deep learning;image quality assessment (IQA);natural scene statistics (NSS);no reference;Deep learning;image quality assessment (IQA);natural scene statistics (NSS);no reference","Image quality;Databases;Numerical models;Training;Visualization;Image representation;Measurement","image classification;learning (artificial intelligence);numerical analysis;regression analysis;visual databases","blind image quality assessment;deep learning;visual quality;linguistic descriptions;extensive psychological evidence;qualitative evaluations;IQA metrics;numerical ratings;numerical scores;general utilization;fair comparison;natural scene statistics features;discriminative deep model;explicit mental concepts;quality pooling;classification framework;databases;model effectiveness;model efficiency;model robustness","Algorithms;Biomimetics;Databases, Factual;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Learning;Models, Theoretical;Pattern Recognition, Automated;Visual Perception","137","43","","","","","IEEE","IEEE Journals"
"Fuzzy Restricted Boltzmann Machine for the Enhancement of Deep Learning","C. L. P. Chen; C. Zhang; L. Chen; M. Gan","Department of Computer and Infor-mation Science, Faculty of Science and Technology, University of Macau, Macau, China; Department of Computer and Information Science, Faculty of Science and Technology, University of Macau, Macau, China; Department of Computer and Infor-mation Science, Faculty of Science and Technology, University of Macau, Macau, China; Department of Computer and Information Science, Hefei University of Technology, Hefei, China","IEEE Transactions on Fuzzy Systems","","2015","23","6","2163","2173","In recent years, deep learning caves out a research wave in machine learning. With outstanding performance, more and more applications of deep learning in pattern recognition, image recognition, speech recognition, and video processing have been developed. Restricted Boltzmann machine (RBM) plays an important role in current deep learning techniques, as most of existing deep networks are based on or related to it. For regular RBM, the relationships between visible units and hidden units are restricted to be constants. This restriction will certainly downgrade the representation capability of the RBM. To avoid this flaw and enhance deep learning capability, the fuzzy restricted Boltzmann machine (FRBM) and its learning algorithm are proposed in this paper, in which the parameters governing the model are replaced by fuzzy numbers. This way, the original RBM becomes a special case in the FRBM, when there is no fuzziness in the FRBM model. In the process of learning FRBM, the fuzzy free energy function is defuzzified before the probability is defined. The experimental results based on bar-and-stripe benchmark inpainting and MNIST handwritten digits classification problems show that the representation capability of FRBM model is significantly better than the traditional RBM. Additionally, the FRBM also reveals better robustness property compared with RBM when the training data are contaminated by noises.","","","10.1109/TFUZZ.2015.2406889","Macau Science and Technology Development Fund; UM Multiyear Research; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7047917","Deep learning;Fuzzy restricted Boltzmann machine;RBM;Fuzzy deep networks;Image inpainting;Image classification;Deep learning;fuzzy deep networks;fuzzy restricted Boltzmann machine;image classification;image inpainting;restricted Boltzmann machine (RBM)","Approximation methods;Markov processes;Optimization;Robustness;Probability distribution;Training;Linear programming","Boltzmann machines;learning (artificial intelligence);pattern classification;probability","fuzzy restricted Boltzmann machine;deep learning enhancement;research wave;machine learning;pattern recognition;image recognition;speech recognition;video processing;deep learning technique;deep network;representation capability;deep learning capability;learning algorithm;fuzzy number;fuzziness;FRBM model;fuzzy free energy function;probability;bar-and-stripe benchmark inpainting;MNIST handwritten digits classification problem;robustness property;training data","","76","37","","","","","IEEE","IEEE Journals"
"Joint Structural Learning to Rank with Deep Linear Feature Learning","X. Zhao; X. Li; Z. Zhang","Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Knowledge and Data Engineering","","2015","27","10","2756","2769","Multimedia information retrieval usually involves two key modules including effective feature representation and ranking model construction. Most existing approaches are incapable of well modeling the inherent correlations and interactions between them, resulting in the loss of the latent consensus structure information. To alleviate this problem, we propose a learning to rank approach that simultaneously obtains a set of deep linear features and constructs structure-aware ranking models in a joint learning framework. Specifically, the deep linear feature learning corresponds to a series of matrix factorization tasks in a hierarchical manner, while the learning-to-rank part concentrates on building a ranking model that effectively encodes the intrinsic ranking information by structural SVM learning. Through a joint learning mechanism, the two parts are mutually reinforced in our approach, and meanwhile their underlying interaction relationships are implicitly reflected by solving an alternating optimization problem. Due to the intrinsic correlations among different queries (i.e., similar queries for similar ranking lists), we further formulate the learning-to-rank problem as a multi-task problem, which is associated with a set of mutually related query-specific learning-to-rank subproblems. For computational efficiency and scalability, we design a MapReduce-based parallelization approach to speed up the learning processes. Experimental results demonstrate the efficiency, effectiveness, and scalability of the proposed approach in multimedia information retrieval.","","","10.1109/TKDE.2015.2426707","National Natural Science Foundation of China; National Basic Research Program of China; US National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7095607","Learning to rank;information retrieval;deep feature learning;matrix factorization;structural SVM;joint learning;Learning to rank;information retrieval;deep feature learning;matrix factorization;structural SVM;joint learning","Support vector machines;Joints;Information retrieval;Multimedia communication;Training;Correlation;Optimization","learning (artificial intelligence);matrix decomposition;multimedia computing;parallel processing;query processing;support vector machines","joint structural learning;deep linear feature learning;multimedia information retrieval;feature representation;ranking model construction;structure-aware ranking model;matrix factorization;intrinsic ranking information;structural SVM learning;joint learning mechanism;query-specific learning-to-rank subproblem;MapReduce-based parallelization approach","","4","49","","","","","IEEE","IEEE Journals"
"Analog inference circuits for deep learning","J. Holleman; I. Arel; S. Young; J. Lu","Department of Electrical Engineering & Computer Science, University of Tennessee, Knoxville, Knoxville, TN, USA; Department of Electrical Engineering & Computer Science, University of Tennessee, Knoxville, Knoxville, TN, USA; Department of Electrical Engineering & Computer Science, University of Tennessee, Knoxville, Knoxville, TN, USA; BroadCom Corporation, Irvine, CA, USA","2015 IEEE Biomedical Circuits and Systems Conference (BioCAS)","","2015","","","1","4","Deep Machine Learning (DML) algorithms have proven to be highly successful at challenging, high-dimensional learning problems, but their widespread deployment is limited by their heavy computational requirements and the associated power consumption. Analog computational circuits offer the potential for large improvements in power efficiency, but noise, mismatch, and other effects cause deviations from ideal computations. In this paper we describe circuits useful for DML algorithms, including a tunable-width bump circuit and a configurable distance calculator. We also discuss the impacts of computational errors on learning performance. Finally we will describe a complete deep learning engine implemented using current-mode analog circuits and compare its performance to digital equivalents.","","","10.1109/BioCAS.2015.7348310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348310","analog CMOS;deep learning;neuromorphic computing;error modeling","Computational modeling;Feature extraction;Training;Machine learning algorithms;Integrated circuit modeling;Computational efficiency;Clustering algorithms","biomedical electronics;CMOS analogue integrated circuits;learning (artificial intelligence);medical computing","analog inference circuits;deep machine learning algorithms;high-dimensional learning problems;heavy computational requirements;power consumption;analog computational circuits;noise;DML algorithms;tunable-width bump circuit;configurable distance calculator;computational errors;deep learning engine;current-mode analog circuits;digital equivalents","","2","8","","","","","IEEE","IEEE Conferences"
"Simplicity of Kmeans Versus Deepness of Deep Learning: A Case of Unsupervised Feature Learning with Limited Data","M. Dundar; Q. Kou; B. Zhang; Y. He; B. Rajwa","Dept. of Comput. & Inf. Sci., Indiana Univ. - Purdue Univ., Indianapolis, IN, USA; Dept. of Biohealth Inf., Indiana Univ. - Purdue Univ., Indianapolis, IN, USA; Dept. of Comput. & Inf. Sci., Indiana Univ. - Purdue Univ., Indianapolis, IN, USA; Dept. of Comput. & Inf. Sci., Indiana Univ. - Purdue Univ., Indianapolis, IN, USA; Bindley Biosci. Center, Purdue Univ., West Lafayette, IN, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","883","888","We study a bio-detection application as a case study to demonstrate that Kmeans -- based unsupervised feature learning can be a simple yet effective alternative to deep learning techniques for small data sets with limited intra-as well as inter-class diversity. We investigate the effect on the classifier performance of data augmentation as well as feature extraction with multiple patch sizes and at different image scales. Our data set includes 1833 images from four different classes of bacteria, each bacterial culture captured at three different wavelengths and overall data collected during a three-day period. The limited number and diversity of images present, potential random effects across multiple days, and the multi-mode nature of class distributions pose a challenging setting for representation learning. Using images collected on the first day for training, on the second day for validation, and on the third day for testing Kmeans -- based representation learning achieves 97% classification accuracy on the test data. This compares very favorably to 56% accuracy achieved by deep learning and 74% accuracy achieved by handcrafted features. Our results suggest that data augmentation or dropping connections between units offers little help for deep-learning algorithms, whereas significant boost can be achieved by Kmeans -- based representation learning by augmenting data and by concatenating features obtained at multiple patch sizes or image scales.","","","10.1109/ICMLA.2015.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424433","kmeans clustering;unsupervised feature learning;deep learning;bio-detection","Microorganisms;Feature extraction;Training;Neural networks;Encoding;Machine learning;Testing","biology computing;data handling;feature extraction;image classification;image representation;microorganisms;unsupervised learning","limited data;biodetection application;K-means-based unsupervised feature learning;deep learning techniques;limited intraclass diversity;limited interclass diversity;data augmentation classifier performance;feature extraction;bacterial culture;K-means-based representation learning","","9","14","","","","","IEEE","IEEE Conferences"
"An application of deep learning for trade signal prediction in financial markets","A. C. Türkmen; A. T. Cemgil","Bilgisayar Mühendisliği Bölümü, Boğaziçi Üniversitesi, Istanbul, Türkiye; Bilgisayar Mühendisliği Bölümü, Boğaziçi Üniversitesi, Istanbul, Türkiye","2015 23nd Signal Processing and Communications Applications Conference (SIU)","","2015","","","2521","2524","We know algorithms for predicting price movement direction and time are in practical use, despite being disputed at a theoretical level. In this study, we analyze the benefits of various machine learning algorithms to the price movement direction prediction problem, on selected stocks from the U.S. stock markets. To this end, we generate an array of features known to be beneficial in technical analysis of securities, and show the efficacy of several supervised learning methods. Lastly, we demonstrate that Stacked Denoising Auto-Encoders, an example of “deep learning” that has grown popular in recent years, yields significant prediction power.","","","10.1109/SIU.2015.7130397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7130397","Machine Learning;Deep Learning;Finance","Machine learning algorithms;Market research;Stock markets;Noise reduction;Neural networks;Time series analysis;Forecasting","economic forecasting;learning (artificial intelligence);pricing;stock markets","trade signal prediction;financial markets;machine learning algorithms;price movement direction prediction problem;US stock markets;securities;supervised learning methods;stacked denoising auto-encoders;deep learning","","1","14","","","","","IEEE","IEEE Conferences"
"Estimating crop yields with deep learning and remotely sensed data","K. Kuwata; R. Shibasaki","The University of Tokyo IIS, The University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan; The University of Tokyo IIS, The University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","858","861","This paper describes Illinois corn yield estimation using deep learning and another machine learning, SVR. Deep learning is a technique that has been attracting attention in recent years of machine learning, it is possible to implement using the Caffe. High accuracy estimation of crop yield is very important from the viewpoint of food security. However, since every country prepare data inhomogeneously, the implementation of the crop model in all regions is difficult. Deep learning is possible to extract important features for estimating the object from the input data, so it can be expected to reduce dependency of input data. The network model of two InnerProductLayer was the best algorithm in this study, achieving RMSE of 6.298 (standard value). This study highlights the advantages of deep learning for agricultural yield estimating.","","","10.1109/IGARSS.2015.7325900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325900","Deep Learning;Caffe;MODIS;crop yield;EVI","Agriculture;Machine learning;Remote sensing;Feature extraction;Meteorology;Indexes;Data models","estimation theory;learning (artificial intelligence);regression analysis;remote sensing;support vector machines;vegetation mapping","crop yield estimatoin;remotely sensed data;deep learning data;machine learning;SVR;Caffe;crop yield estimation;food security;crop model;inner product layer;RMSE;agricultural yield;support vector regression","","14","13","","","","","IEEE","IEEE Conferences"
"Supporting software product line testing by optimizing code configuration coverage","L. Vidács; F. Horváth; J. Mihalicza; B. Vancsics; À. Beszédes","MTA-SZTE Research Group on Artificial Intelligence, Szeged, Hungary; University of Szeged, Szeged, Hungary; NNG LLC, Budapest, Hungary; University of Szeged, Szeged, Hungary; University of Szeged, Szeged, Hungary","2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","2015","","","1","7","Software product lines achieve much shorter time to market by system level reuse and code variability. A possible way to achieve this flexibility is to use generic components, including the core system, in different products in alternative configurations. The focus of testing efforts for such complex and highly variable systems often shifts from testing specific products to assessing the overall quality of the core system or potential new configurations. As a complementary approach to feature models and related combinatorial testing methods optimizing for feature coverage, we apply a source code oriented analysis of variability. We present two algorithms that optimize for high coverage of the common code base in terms of C++ preprocessor-based configurations with a limited set of actual configurations selected for testing. The methods have been evaluated on iGO Navigation, a large industrial system with typical configuration support for product lines, hence we believe the approach can be generalized to other systems as well.","","","10.1109/ICSTW.2015.7107478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7107478","Variability;Preprocessor;Configurations;Software Product Line;White box testing","Testing;Navigation;Algorithm design and analysis;Software;Frequency modulation;Electronic mail;Analytical models","C++ language;program testing;software product lines;source code (software)","iGO navigation;C++ preprocessor-based configuration;source code oriented analysis;combinatorial testing method;core system quality;code variability;code configuration coverage optimization;software product line testing","","1","33","","","","","IEEE","IEEE Conferences"
"Tendencies towards DEEP or SURFACE learning for participants taking a large massive open online course (MOOC)","A. Kemppainen; J. Sticklen; B. Oakley; D. Chung","Department of Engineering Fundamentals, Michigan Technological University, Houghton, MI USA; Department of Engineering Fundamentals, Michigan Technological University, Houghton, MI USA; Industrial and Systems Engineering Department, Oakland University, Rochester, MI USA; School of Engineering, Temasek Polytechnic, Singapore","2015 IEEE Frontiers in Education Conference (FIE)","","2015","","","1","4","In this report we will describe our first steps in understanding the characteristics of individuals enrolling and completing MOOCs. The learner characteristic we focus on is the deep versus shallow learning dimension. We will use the revised two-factor study process questionnaire of Biggs in our study [1]. To our knowledge, there is no comparable research either reported in the literature or currently under way. Our focus is on Learning How to Learn (LHTL), currently the most heavily subscribed course on the Coursera platform. The last offering of LHTL, completed in January, 2015, attracted just under a quarter million learners. In the fourth and final week of the course, the R-SPQ-2F survey instrument was made available to all students on the LHTL site. Approximately 1,600 students completed the survey. We believe our research to be of interest widely because of the confluence in our research of (a) MOOCs, (b) the deep versus surface learning dimension, and (c) a methodology that can lead to better understanding of MOOCs.","","","10.1109/FIE.2015.7344144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344144","MOOC;online learning;deep learning;surface learning","Sociology;Videos;Instruments;Correlation;Planning;Machine learning","computer aided instruction;educational courses","DEEP learning;SURFACE learning;massive open online course;MOOC;learner characteristic;learning how to learn;Coursera platform;R-SPQ-2F survey instrument;LHTL site;surface learning dimension","","1","7","","","","","IEEE","IEEE Conferences"
"A Deep Learning Method Combined Sparse Autoencoder with SVM","Y. Ju; J. Guo; S. Liu","Comput. Center Dept., East China Normal Univ., Shanghai, China; Comput. Center Dept., East China Normal Univ., Shanghai, China; Comput. Center Dept., East China Normal Univ., Shanghai, China","2015 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery","","2015","","","257","260","In this paper, a novel unsupervised method for learning sparse features combined with support vector machines for classification is proposed. The classical SVM method has restrictions on the large-scale applications. This model uses sparse auto encoder, a deep learning algorithm, to improve the performance. Firstly, we use multiple layers of sparse auto encoder to learn the features of the data. Secondly, we use SVM to classify. Many experimental results show that compared with SVM, our proposed method can improve the classification rate. In particular, it can effectively deal with large-scale data sets.","","","10.1109/CyberC.2015.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307823","sparse autoencoder;unsupervised learning;deep learning;SVM","Support vector machines;Training;Kernel;Classification algorithms;Machine learning;Feature extraction;Data models","encoding;pattern classification;support vector machines;unsupervised learning","deep learning algorithm;sparse autoencoder;support vector machine;SVM;unsupervised learning method;data classification","","12","13","","","","","IEEE","IEEE Conferences"
"Image recognition based on deep learning","Meiyin Wu; Li Chen","College of Computer Science and Technology, Wuhan University of Science and Technology, Key Laboratory of Intelligent Information Processing and Real-time Industrial System, China; College of Computer Science and Technology, Wuhan University of Science and Technology, Key Laboratory of Intelligent Information Processing and Real-time Industrial System, China","2015 Chinese Automation Congress (CAC)","","2015","","","542","546","Deep learning is a multilayer neural network learning algorithm which emerged in recent years. It has brought a new wave to machine learning, and making artificial intelligence and human-computer interaction advance with big strides. We applied deep learning to handwritten character recognition, and explored the two mainstream algorithm of deep learning: the Convolutional Neural Network (CNN) and the Deep Belief NetWork (DBN). We conduct the performance evaluation for CNN and DBN on the MNIST database and the real-world handwritten character database. The classification accuracy rate of CNN and DBN on the MNIST database is 99.28% and 98.12% respectively, and on the real-world handwritten character database is 92.91% and 91.66% respectively. The experiment results show that deep learning does have an excellent feature learning ability. It don't need to extract features manually. Deep learning can learn more nature features of the data.","","","10.1109/CAC.2015.7382560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382560","Deep learning;Artificial intelligence;Convolutional Neural Network;Deep Belief Network;Handwritten Character recognition","Databases;Feature extraction;Machine learning;Training;Convolution;Kernel;Character recognition","belief networks;handwritten character recognition;image recognition;multilayer perceptrons","image recognition;deep learning;multilayer neural network learning algorithm;handwritten character recognition;convolutional neural network;deep belief network;CNN;DBN;MNIST database;classification accuracy rate","","2","19","","","","","IEEE","IEEE Conferences"
"Automatic detection of cerebral microbleeds via deep learning based 3D feature representation","H. Chen; L. Yu; Q. Dou; L. Shi; V. C. T. Mok; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Technology, Zhejiang University, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Medicine and Therapeutics, The Chinese University of Hong Kong; Department of Medicine and Therapeutics, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","","2015","","","764","767","Clinical identification and rating of the cerebral microbleeds (CMBs) are important in vascular diseases and dementia diagnosis. However, manual labeling is time-consuming with low reproducibility. In this paper, we present an automatic method via deep learning based 3D feature representation, which solves this detection problem with three steps: candidates localization with high sensitivity, feature representation, and precise classification for reducing false positives. Different from previous methods by exploiting low-level features, e.g., shape features and intensity values, we utilize the deep learning based high-level feature representation. Experimental results validate the efficacy of our approach, which outperforms other methods by a large margin with a high sensitivity while significantly reducing false positives per subject.","","","10.1109/ISBI.2015.7163984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163984","cerebral microbleeds;feature representation;deep learning;object detection","Feature extraction;Three-dimensional displays;Training;Biomedical imaging;Machine learning;Radio frequency;Sensitivity","biomedical MRI;blood vessels;brain;diseases;image representation;learning (artificial intelligence);medical image processing;object detection","automatic detection;deep learning based 3D feature representation;clinical identification;cerebral microbleed rating;CMB;vascular diseases;dementia diagnosis;manual labeling;detection problem;candidate localization;precise classification;false positive reduction;low-level features;shape features;intensity values;deep learning based high-level feature representation","","17","13","","","","","IEEE","IEEE Conferences"
"Deep Neural Networks with Parallel Autoencoders for Learning Pairwise Relations: Handwritten Digits Subtraction","T. Du; L. Liao","NA; NA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","582","587","Modelling relational data is a common task for many machine learning problems. In this work, we focus on learning pairwise relations between two entities, with deep neural networks. To incorporate the structural properties in the data that represent two entities concatenated together, two separate stacked autoencoders are introduced in parallel to extract individual features, which are then fed into a deep neural network for classification. The method is applied to a specific problem: whether two input handwritten digits differ by one. We tested the performance on a dataset generated from handwritten digits in MNIST, which is a widely used dataset for testing different machine learning techniques and pattern recognition methods. We compared with several different machine learning algorithms, including logistic regression and support vector machines, on this handwritten digit subtraction (HDS) dataset. The results showed that deep neural networks outperformed other methods, and in particular, the deep neural networks fitted with two separate autoencoders in parallel increased the prediction accuracy from 85.83%, which was achieved by a standard neural network with a single stacked autoencoder, to 88.27%.","","","10.1109/ICMLA.2015.175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424379","Deep learning;relational learning;parallel autoencoders;neural networks;machine learning","Feature extraction;Machine learning;Biological neural networks;Kernel;Machine learning algorithms;Support vector machines","handwritten character recognition;image classification;image coding;learning (artificial intelligence);neural nets;regression analysis;relational databases;support vector machines","pairwise relation learning;deep neural networks;relational data modelling;structural properties;stacked autoencoders;parallel stacked autoencoders;classification;MNIST;machine learning techniques;pattern recognition method;logistic regression;support vector machines;handwritten digit subtraction dataset;HDS dataset","","1","22","","","","","IEEE","IEEE Conferences"
"Sentiment analysis of a document using deep learning approach and decision trees","A. S. Zharmagambetov; A. A. Pak","LLC AlemResearch, Almaty, Kazakhstan; Institute of ICT, Almaty, Kazakhstan","2015 Twelve International Conference on Electronics Computer and Computation (ICECCO)","","2015","","","1","4","The given paper describes modern approach to the task of sentiment analysis of movie reviews by using deep learning recurrent neural networks and decision trees. These methods are based on statistical models, which are in a nutshell of machine learning algorithms. The fertile area of research is the application of Google's algorithm Word2Vec presented by Tomas Mikolov, Kai Chen, Greg Corrado and Jeffrey Dean in 2013. The main idea of Word2Vec is the representations of words with the help of vectors in such manner that semantic relationships between words preserved as basic linear algebra operations. The extra advantage of the mentioned algorithm above the alternatives is computational efficiency. This paper focuses on using Word2Vec model for text classification by their sentiment type.","","","10.1109/ICECCO.2015.7416902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7416902","NLP;sentiment analysis;deep learning;machine learning;text classification","Sentiment analysis;Semantics;Text categorization;Motion pictures;Classification algorithms;Computational modeling;Machine learning","decision trees;document handling;learning (artificial intelligence);sentiment analysis","sentiment analysis;document;movie reviews;deep learning recurrent neural networks;statistical models;nutshell;machine learning algorithms;fertile area;Google algorithm Word2Vec;basic linear algebra operations;computational efficiency;Word2Vec model;text classification;sentiment type","","9","18","","","","","IEEE","IEEE Conferences"
"The evolution of e-learning platforms from content to activity based learning: The case of Learn@WU","M. Andergassen; G. Ernst; V. Guerra; F. Mödritscher; M. Moser; G. Neumann; T. Renner","Information Systems and New Media, Vienna University of Economics and Business (WU), Vienna, Austria; Information Systems and New Media, Vienna University of Economics and Business (WU), Vienna, Austria; Information Systems and New Media, Vienna University of Economics and Business (WU), Vienna, Austria; Information Systems and New Media, Vienna University of Economics and Business (WU), Vienna, Austria; Information Systems and New Media, Vienna University of Economics and Business (WU), Vienna, Austria; Information Systems and New Media, Vienna University of Economics and Business (WU), Vienna, Austria; Information Systems and New Media, Vienna University of Economics and Business (WU), Vienna, Austria","2015 International Conference on Interactive Collaborative Learning (ICL)","","2015","","","779","784","Over the last decades e-learning platforms evolved from tools for distributing materials and providing quiz-based assessments to complex software systems that enable didactically enriched learning experiences. This paper describes the metamorphosis of the Learn@WU system, one of the most intensely used e-learning platforms. The platform is in use since 2001, when it was designed for learning content management, complemented by community management functions over the years. One of the major usages in the beginning was the support of blended learning in large classes up to 1000 students and the provision of self-assessment in the initial phase of the study programs. From the start, the platform supported all beginner courses of all subjects without much instructional design, to give students complete freedom for self-directed learning. Today, the platform contains more than 160,000 learning materials used in the 4,000 courses offered each year. However, studies showed indications that most students prefer to use the self-assessments in order to memorize questions instead of deep learning of content, i.e. by carelessly submitting results for many exercises to receive the correct solutions provided as a feedback from the system. As a consequence, the success rates in self-assessments did not serve as good predictors for knowledge and exam success. It was also found that spaced learning and overall learning time have a positive influence on final exam results [1]. Based on these findings, the objectives for the further development of the platform included efforts to increase the validity of the self-assessment data and to better support instructional designs. The new implementation allows to assign different learning activities to resources and to organize activities in thematic modules in order to support, e.g., flipped-classroom designs. This focus on learning activities was achieved through the implementation of learning design functionality and the possibility to connect resources to several so-called `content flows', which are short workflow sequences that specify series of user interactions with the resource within a certain context. They define, for instance, if a resource has a training or self-assessment purpose (which are different learning activities), in order to improve the validity of knowledge assessment. After one month of productive use of the new features, more than 8,000 learning activities have been created in more than 100 courses of the university, which indicates good acceptance rates. According to our knowledge, Learn@WU is the first large-scale e-learning platform which has adopted a workflow-based approach for realizing differentiated learning activities.","","","10.1109/ICL.2015.7318127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318127","Learning Management System;Data-Driven Development;Learning Activities;Content Flows;Learning Progress;Learning Workflows","Navigation;Electronic learning;Least squares approximations;Collaborative work;Sequential analysis;Conferences","educational courses;educational institutions;learning management systems","e-learning platforms;activity based learning;content based learning;Learn@WU system;learning content management;blended learning;courses;self-directed learning;self-assessment data;spaced learning;overall learning time;learning design functionality;content flows;short workflow sequences;knowledge assessment;university;differentiated learning activities","","3","20","","","","","IEEE","IEEE Conferences"
"Deep learning network for face detection","Xueyi Ye; X. Chen; H. Chen; Yafeng Gu; Qiuyun Lv","Lab of Pattern Recognition & Information Security, Hangzhou Dianzi University, 310018, China; Lab of Pattern Recognition & Information Security, Hangzhou Dianzi University, 310018, China; Lab of Pattern Recognition & Information Security, Hangzhou Dianzi University, 310018, China; Lab of Pattern Recognition & Information Security, Hangzhou Dianzi University, 310018, China; Lab of Pattern Recognition & Information Security, Hangzhou Dianzi University, 310018, China","2015 IEEE 16th International Conference on Communication Technology (ICCT)","","2015","","","504","509","By the multi-layer nonlinear mapping and the semantic feature extraction of the deep learning, a deep learning network is proposed for face detection to overcome the challenge of detecting faces accurately and rapidly in the non-ideal case. Key to this deep network is that, to better simulate the response for information in the human brain, the status probability of the neuron is used to model the status of the human brain neuron which is a continuous distribution from the most active to the least active. Moreover, the number of the hidden layer's neuron decreases layer-by-layer to eliminate the redundant information of the input data and accelerate the detection speed combining with the skin color detection. Experimental results show that, besides the fast detection speed and strong robustness to face rotation, the proposed method possesses lower false detection rate and lower missing detection rate.","","","10.1109/ICCT.2015.7399887","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7399887","Face detection;Deep learning network;Pre-training;Greedy layer-wise learning;The status probability of the neuron","Machine learning;Optimization;Libraries;Robustness","face recognition;learning (artificial intelligence);neural nets;probability","deep learning network;face detection;multilayer nonlinear mapping;semantic feature extraction;response simulation;neuron status probability;human brain neuron status;hidden layer neuron;skin color detection;detection speed;face rotation;low-false detection rate;low-missing detection rate","","","14","","","","","IEEE","IEEE Conferences"
"Learning deep features for image emotion classification","M. Chen; L. Zhang; J. P. Allebach","School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN 47906, USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN 47906, USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN 47906, USA","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","4491","4495","Images can both express and affect people's emotions. It is intriguing and important to understand what emotions are conveyed and how they are implied by the visual content of images. Inspired by the recent success of deep convolutional neural networks (CNN) in visual recognition, we explore two simple, yet effective deep learning-based methods for image emotion analysis. The first method uses off-the-shelf CNN features directly for classification. For the second method, we fine-tune a CNN that is pre-trained on a large dataset, i.e. ImageNet, on our target dataset first. Then we extract features using the fine-tuned CNN at different location at multiple levels to capture both the global and local information. The features at different location are aggregated using the Fisher Vector for each level and concatenated to form a compact representation. From our experimental results, both the deep learning-based methods outperforms traditional methods based on generic image descriptors and hand-crafted features.","","","10.1109/ICIP.2015.7351656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351656","Image emotion;image classification;deep learning;convolutional neural network","Feature extraction;Training;Visualization;Image recognition;Support vector machines;Neural networks;Machine learning","emotion recognition;feature extraction;image classification;image representation;learning (artificial intelligence);neural nets;vectors","deep feature learning;image emotion classification;deep convolutional neural networks;visual recognition;off-the-shelf CNN features;feature extraction;Fisher vector;compact representation","","14","18","","","","","IEEE","IEEE Conferences"
"Multimedia Analysis with Deep Learning","Q. Wu; H. Zhang; S. Liu; X. Cao","State Key Lab. of Inf. Security, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; State Key Lab. of Inf. Security, China; State Key Lab. of Inf. Security, China","2015 IEEE International Conference on Multimedia Big Data","","2015","","","20","23","Recently, deep learning method has been attracting more and more researchers due to its great success in various computer vision tasks. Particularly, some researchers focus on the study of multimedia analysis by deep learning method, and the research tasks mainly include the following six aspects: classification, retrieval, segmentation, tracking, detection and recommendation. As far as we know, there is not any literature conducting on survey of these studies, and it is of great significance for the community to review this subject. In this paper, we discuss the application of deep learning method in the six multimedia analysis tasks, and also point out the future directions of deep learning in multimedia analysis.","","","10.1109/BigMM.2015.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153770","deep learning;multimedia analysis;classification;retrieval;segmentation;tracking;detection;recommendation","Multimedia communication;Brain models;Learning systems;Neural networks;Feature extraction;Tracking","computer vision;image classification;image retrieval;image segmentation;learning (artificial intelligence);multimedia systems;object detection;object tracking","multimedia analysis;deep learning method;computer vision;image classification;image retrieval;image segmentation;image tracking;image detection;image recommendation","","2","23","","","","","IEEE","IEEE Conferences"
"A Deep Learning Prediction Process Accelerator Based FPGA","Q. Yu; C. Wang; X. Ma; X. Li; X. Zhou","Sch. of Comput. Sci., Univ. of Sci. & Technol. of China, Hefei, China; Sch. of Comput. Sci., Univ. of Sci. & Technol. of China, Hefei, China; Sch. of Comput. Sci., Univ. of Sci. & Technol. of China, Hefei, China; Sch. of Comput. Sci., Univ. of Sci. & Technol. of China, Hefei, China; Sch. of Comput. Sci., Univ. of Sci. & Technol. of China, Hefei, China","2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing","","2015","","","1159","1162","Recently, machine learning is widely used in applications and cloud services. And as the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. To give users better experience, high performance implementations of deep learning applications seem very important. As a common means to accelerate algorithms, FPGA has high performance, low power consumption, small size and other characteristics. So we use FPGA to design a deep learning accelerator, the accelerator focuses on the implementation of the prediction process, data access optimization and pipeline structure. Compared with Core 2 CPU 2.3GHz, our accelerator can achieve promising result.","","","10.1109/CCGrid.2015.114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7152611","FPGA;deep learning;prediction process;accelerator","Field programmable gate arrays;Neural networks;Computer architecture;Training;Interpolation;Hardware;Clocks","field programmable gate arrays;learning (artificial intelligence)","field-programmable gate array;pipeline structure;data access optimization;prediction process implementation;deep learning accelerator design;complex learning problems;machine learning;deep learning prediction process accelerator-based FPGA","","14","10","","","","","IEEE","IEEE Conferences"
"Deep learning in exploring semantic relatedness for microblog dimensionality reduction","L. Xu; C. Jiang; Y. Ren","Department of Electronic Engineering, Tsinghua University, Beijing, 100084, China; Department of Electronic Engineering, Tsinghua University, Beijing, 100084, China; Department of Electronic Engineering, Tsinghua University, Beijing, 100084, China","2015 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","","2015","","","98","102","Mining the large volume textual data produced by microblogging services has attracted much attention in recent years. An important preprocessing step of microblog text mining is to convert natural language texts into proper numerical representations. Due to the short-length characteristic, finding proper representations of microblog texts is nontrivial. In this paper, we propose to build deep network-based models to learn low-dimensional representations of microblog texts. The proposed models take advantage of the semantic relatedness derived from two types of microblog-specific information, namely the retweet relationship and hashtags. Experiment results show that the deep models perform better than traditional dimensionality reduction methods such as latent semantic analysis and latent Dirichlet allocation topic model, and the use of microblog-specific information can help to learn better representations.","","","10.1109/GlobalSIP.2015.7418164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7418164","microblog mining;dimensionality reduction;text representation;deep learning;autoencoder","Training;Tagging;Twitter;Semantics;Conferences;Information processing;Machine learning","data mining;data reduction;learning (artificial intelligence);natural language processing;social networking (online);text analysis","semantic relatedness;microblog dimensionality reduction;microblogging services;natural language texts;numerical representations;short-length characteristic;deep network-based models;low-dimensional representations;microblog-specific information;retweet relationship;hashtags;microblog text mining;large volume textual data mining;deep learning","","1","15","","","","","IEEE","IEEE Conferences"
"Beyond code coverage — An approach for test suite assessment and improvement","D. Tengeri; Á. Beszédes; T. Gergely; L. Vidács; D. Havas; T. Gyimóthy","Department of Software Engineering, University of Szeged, Szeged, Hungary; Department of Software Engineering, University of Szeged, Szeged, Hungary; Department of Software Engineering, University of Szeged, Szeged, Hungary; MTA-SZTE Research Group on Artificial Intelligence, University of Szeged, Szeged, Hungary; Department of Software Engineering, University of Szeged, Szeged, Hungary; Department of Software Engineering, University of Szeged, Szeged, Hungary","2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","2015","","","1","7","Code coverage is successfully used to guide white box test design and evaluate the respective test completeness. However, simple overall coverage ratios are often not precise enough to effectively help when a (regression) test suite needs to be reassessed and evolved after software change. We present an approach for test suite assessment and improvement that utilizes code coverage information, but on a more detailed level and adds further evaluation aspects derived from the coverage. The main use of the method is to aid various test suite evolution situations such as removal, refactoring and extension of test cases as a result of code change or test suite efficiency enhancement. We define various metrics to express different properties of test suites beyond simple code coverage ratios, and present the assessment and improvement process as an iterative application of different improvement goals and more specific sub-activities. The method is demonstrated by applying it to improve the tests of one of our experimental systems.","","","10.1109/ICSTW.2015.7107476","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7107476","code coverage;regression testing;test suite quality;test suite refactoring;test suite evolution;white box testing metrics","Measurement;Libraries;Software;Testing;Systematics;Data structures;Conferences","program testing;source code (software)","code coverage;test suite assessment;test suite improvement;white box test design;test completeness evaluation;coverage ratio;test suite evolution situation;test suite efficiency enhancement","","7","25","","","","","IEEE","IEEE Conferences"
"Learning of Generic Vision Features Using Deep CNN","K. N. D.; B. S. P.","NA; NA","2015 Fifth International Conference on Advances in Computing and Communications (ICACC)","","2015","","","54","57","Eminence of learning algorithm applied for computer vision tasks depends on the features engineered from image. It's premise that different representations can interweave and ensnare most of the elucidative genes that are responsible for variations in images, be it rigid, affine or projective. Hence researches give at most attention in hand-engineering features that capture these variations. But problem is, we need subtle domain knowledge to do that. Thereby making researchers elude epitome of representations. Hence learning algorithms never reach their full potential. In recent times there has been a shift from hand-crafting features to representation learning. The resulting features are not only optimal but also generic as in they can be used as off the shelf features for visual recognition tasks. In this paper we design and experiment with a basic deep convolution neural nets for learning generic vision features with an variant of convolving kernels. They operate by giving importance to individual uncorrelated color channels in a color model by convolving each channel with channel specific kernels. We were able to achieve considerable improvement in performance even when using smaller dataset.","","","10.1109/ICACC.2015.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7433775","Feature learning;Image classification;Deep learning;Supervised learning;Convolution Neural networks","Kernel;Convolution;Feature extraction;Training;Neurons;Neural networks;Linearity","computer vision;learning (artificial intelligence);neural nets","generic vision features;deep CNN;computer vision tasks;elucidative genes;hand-engineering features;learning algorithms;hand-crafting features;representation learning;visual recognition tasks;basic deep convolution neural nets;convolving kernels;color channels;channel specific kernels","","","27","","","","","IEEE","IEEE Conferences"
"A Robust Deep Model for Improved Classification of AD/MCI Patients","F. Li; L. Tran; K. Thung; S. Ji; D. Shen; J. Li","Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA; Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA; Department of Radiology, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Computer Science, Old Dominion University, Norfolk, VA, USA; Department of Radiology, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA","IEEE Journal of Biomedical and Health Informatics","","2015","19","5","1610","1616","Accurate classification of Alzheimer's disease (AD) and its prodromal stage, mild cognitive impairment (MCI), plays a critical role in possibly preventing progression of memory impairment and improving quality of life for AD patients. Among many research tasks, it is of a particular interest to identify noninvasive imaging biomarkers for AD diagnosis. In this paper, we present a robust deep learning system to identify different progression stages of AD patients based on MRI and PET scans. We utilized the dropout technique to improve classical deep learning by preventing its weight coadaptation, which is a typical cause of overfitting in deep learning. In addition, we incorporated stability selection, an adaptive learning factor, and a multitask learning strategy into the deep learning framework. We applied the proposed method to the ADNI dataset, and conducted experiments for AD and MCI conversion diagnosis. Experimental results showed that the dropout technique is very effective in AD diagnosis, improving the classification accuracies by 5.9% on average as compared to the classical deep learning methods.","","","10.1109/JBHI.2015.2429556","NIH grants; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7101222","Alzheimer’s Disease;Early Diagnosis;MRI;PET;Deep Learning;Alzheimer's disease (AD);deep learning;early diagnosis;magnetic resonance imaging (MRI);positron emission tomography (PET)","Principal component analysis;Support vector machines;Magnetic resonance imaging;Computational modeling;Positron emission tomography;Training;Feature extraction","biomedical MRI;cognition;diseases;learning (artificial intelligence);medical computing;neurophysiology;positron emission tomography","improved classification;AD patients;MCI patients;Alzheimer's disease;prodromal stage;mild cognitive impairment;memory impairment;quality of life;noninvasive imaging biomarkers;AD diagnosis;robust deep learning system;progression stages;MRI;PET scans;dropout technique;weight coadaptation;stability selection;adaptive learning factor;multitask learning strategy;deep learning framework;ADNI dataset;AD conversion diagnosis;MCI conversion diagnosis;classification accuracy;classical deep learning method","Alzheimer Disease;Early Diagnosis;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Magnetic Resonance Imaging;Mild Cognitive Impairment;Models, Theoretical;Positron-Emission Tomography;Principal Component Analysis;Support Vector Machine","66","29","","","","","IEEE","IEEE Journals"
"Development of deep learning-based facial expression recognition system","H. Jung; S. Lee; S. Park; B. Kim; J. Kim; I. Lee; C. Ahn","Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Broadcasting & Telecommunications, Convergence Media Research Department, Electronics and Telecommunications Research Institute, Daejeon, Korea; Broadcasting & Telecommunications, Convergence Media Research Department, Electronics and Telecommunications Research Institute, Daejeon, Korea","2015 21st Korea-Japan Joint Workshop on Frontiers of Computer Vision (FCV)","","2015","","","1","4","Deep learning is considered to be a breakthrough in the field of computer vision, since most of the world records of the recognition tasks are being broken. In this paper, we try to apply such deep learning techniques to recognizing facial expressions that represent human emotions. The procedure of our facial expression recognition system is as follows: First, face is detected from input image using Haar-like features. Second, the deep network is used for recognizing facial expression using detected faces. In this step, two different deep networks can be used such as deep neural network and convolutional neural network. Consequently, we compared experimentally two types of deep networks, and the convolutional neural network had better performance than deep neural network.","","","10.1109/FCV.2015.7103729","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7103729","Facial expression recognition;deep neural network;deep learning;convolutional neural network","Face recognition;Face;Neural networks;Databases;Training;Face detection;Convolution","computer vision;emotion recognition;face recognition;Haar transforms;learning (artificial intelligence);neural nets;object detection","facial expression recognition system;computer vision;deep learning techniques;human emotion;face detection;Haar-like features;deep neural network;convolutional neural network","","12","9","","","","","IEEE","IEEE Conferences"
"Deep learning and its application to general image classification","P. Liu; S. Su; M. Chen; C. Hsiao","Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, Kau Yuan University, Kaohsiung City, Taiwan","2015 International Conference on Informative and Cybernetics for Computational Social Systems (ICCSS)","","2015","","","7","10","Deep learning has recently exhibited good performance in many applications. The convolution neural network is an often-used architecture for deep learning and has been widely used in computer vision and audio recognition, and outperformed other related handcraft designed feature in recent years. These techniques compared to other artificial intelligence algorithms and handcraft features need extremely much more time in training and testing and then were not widely used in the early days. Our study is about the impacts of different factors used in the convolution neural network. The considered factors are network depth, numbers of filters, and filter sizes. The used data set is the CIFAR dataset. According to our experiments, some suggestions about those factors are recommended in this study.","","","10.1109/ICCSS.2015.7281139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7281139","Deep learning;CIFAR dataset;convolution neural network","Accuracy;Convolution;Training;Machine learning;Computer architecture;Biological neural networks","filtering theory;image classification;learning (artificial intelligence);multilayer perceptrons","deep learning;image classification;convolution neural network;network depth;filters;CIFAR dataset;multilayer perceptron","","5","17","","","","","IEEE","IEEE Conferences"
"Deep learning with shallow architecture for image classification","A. ElAdel; R. Ejbali; M. Zaied; C. Ben Amar","Research Group in Intelligent Machines, National School of Engineers of sfax, B.P 1173, Tunisia; Research Group in Intelligent Machines, National School of Engineers of sfax, B.P 1173, Tunisia; Research Group in Intelligent Machines, National School of Engineers of sfax, B.P 1173, Tunisia; Research Group in Intelligent Machines, National School of Engineers of sfax, B.P 1173, Tunisia","2015 International Conference on High Performance Computing & Simulation (HPCS)","","2015","","","408","412","This paper presents a new scheme for image classification. The proposed scheme depicts a shallow architecture of Convolutional Neural Network (CNN) providing deep learning: For each image, we calculated the connection weights between the input layer and the hidden layer based on MultiResolution Analysis (MRA) at different levels of abstraction. Then, we selected the best features, representing well each class of images, with their corresponding weights using Adaboost algorithm. These weights are used as the connection weights between the hidden layer and the output layer, and will be used in the test phase to classify a given query image. The proposed approach was tested on different datasets and the obtained results prove the efficiency and the speed of the proposed approach.","","","10.1109/HPCSim.2015.7237069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7237069","multiresolution analysis;Adaboost;deep learning;wavelet;image classification","Neural networks;Feature extraction;Computer architecture;Databases;Machine learning;Multiresolution analysis;Algorithm design and analysis","image classification;image representation;image resolution;image retrieval;learning (artificial intelligence);neural net architecture","deep-learning;shallow-architecture;convolutional neural network;CNN;connection weights;input layer;hidden layer;multiresolution analysis;MRA;abstraction levels;feature selection;image representation;Adaboost algorithm;query image classification","","3","22","","","","","IEEE","IEEE Conferences"
"Retinal vessel landmark detection using deep learning and hessian matrix","T. Fang; R. Su; L. Xie; Q. Gu; Q. Li; P. Liang; T. Wang","Department of Ophthalmology, Affiliated Nanshan people's Hospital of Shenzhen University, Shenzhen University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound; Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging; Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound; Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging; Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound; Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging; Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound; Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging; Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound; Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging; Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound; Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging; Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen, China","2015 8th International Congress on Image and Signal Processing (CISP)","","2015","","","387","392","The purpose of retinal image registration is to establish the coherent correspondences between the multi-model retinal image for applying into the ophthalmological surgery. Vessel landmarks detection in retinal image is the vital step in the retinal image registration. In this paper, a novel approach is proposed, firstly, a deep learning technology is used to vessel segmentation to generate the probability map of the retinal image, which is more reliable for optimizing the feature detection in retinal image. Secondly, we detect the landmarks using the multi-scale Hessian response on the probability map of the retinal image. Compared to the traditional methods, the results show that our method enable a majority of the bifurcation points, crossover points and curvature extreme points to be detected out simultaneously. Moreover, the impact of image noise and pathology can be reduced significantly.","","","10.1109/CISP.2015.7407910","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407910","Image registration;Landmark detection;Vessel Segmentation;Deep learning;Hessian response;The probability map;The retinal image","Retina;Machine learning;Image segmentation;Feature extraction;Image color analysis;Neural networks;Image registration","eye;feature extraction;Hessian matrices;image registration;image segmentation;learning (artificial intelligence);medical image processing;object detection;probability;surgery","retinal vessel landmark detection;deep learning;Hessian matrix;retinal image registration;multimodel retinal image;ophthalmological surgery;vessel segmentation;retinal image probability map;feature detection;multiscale Hessian response;bifurcation points;crossover points;curvature extreme points;image noise;pathology","","2","9","","","","","IEEE","IEEE Conferences"
"Research on Chinese Micro-Blog Sentiment Analysis Based on Deep Learning","L. Yanmei; C. Yuda","Wuhan Inst. of Design & Sci., Wuhan, China; Nat. Eng. Res. Center for Geographic Inf. Syst., Wuhan, China","2015 8th International Symposium on Computational Intelligence and Design (ISCID)","","2015","1","","358","361","Micro-blog sentiment analysis aims to find user's attitude and opinion of hot events. Most of studies have used SVM, CRF and other traditional algorithms, which based on manual tagging of a lot of emotional characteristics, but paid a high price. To improve this situation, further studied deep learning and Micro-blog sentiment analysis, and proposed a new technical solution. It firstly crawled some data from Micro-blog through crawler, then after corpus pretreatment, as the input sample of Convolutional Neural Network, and built the classifier based on SVM/RNN, finally judged the emotional orientation of each sentence in a given test set. Verified by examples, experimental results show that this solution can effectively improve the accuracy of emotional orientation, validation result is ideal.","","","10.1109/ISCID.2015.217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7468968","Micro-blog;sentiment analysis;deep learning;convolutional Neural Network;classifier","Sentiment analysis;Blogs;Machine learning;Training;Feature extraction;Crawlers;Internet","learning (artificial intelligence);neural nets;support vector machines;Web sites","Chinese microblog sentiment analysis;deep learning;data crawler;corpus pretreatment;convolutional neural network;SVM;RNN;emotional orientation","","3","10","","","","","IEEE","IEEE Conferences"
"Joint Feature Learning for Face Recognition","J. Lu; V. E. Liong; G. Wang; P. Moulin","Advanced Digital Sciences Center, Singapore; Advanced Digital Sciences Center, Singapore; Advanced Digital Sciences Center, Singapore; Advanced Digital Sciences Center, Singapore","IEEE Transactions on Information Forensics and Security","","2015","10","7","1371","1383","This paper presents a new joint feature learning (JFL) approach to automatically learn feature representation from raw pixels for face recognition. Unlike many existing face recognition systems, where conventional feature descriptors, such as local binary patterns and Gabor features, are used for face representation, we propose an unsupervised feature learning method to learn hierarchical feature representation. Since different face regions have different physical characteristics, we propose to use different feature dictionaries to represent them, and to learn multiple yet related feature projection matrices for these regions simultaneously. Hence position-specific discriminative information can be exploited for face representation. Having learned these feature projections for different face regions, we perform spatial pooling for face patches within each region to enhance the representative power of the learned features. Moreover, we stack our JFL model into a deep architecture to exploit hierarchical information for feature representation and further improve the recognition performance. Experimental results on five widely used face data sets show the effectiveness of our proposed approach.","","","10.1109/TIFS.2015.2408431","Human Cyber Security Systems Program within the Advanced Digital Sciences Center, Singapore, through the Agency for Science, Technology and Research, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7053922","Face recognition;feature learning;joint learning;deep learning.;Face recognition;feature learning;joint learning;deep learning","Feature extraction;Face recognition;Learning systems;Deep learning;Dictionaries","face recognition;feature extraction;image representation;unsupervised learning","face representation;position-specific discriminative information;feature dictionaries;unsupervised feature learning method;Gabor feature;local binary patterns feature;hierarchical feature representation learning;JFL approach;face recognition;joint feature learning","","46","70","","","","","IEEE","IEEE Journals"
"Palmprint recognition based on deep learning","D. Zhao; X. Pan; X. Luo; X. Gao","College of computer and Information Engineering, Inner Mongolia Agricultural University Hohhot, China; College of computer and Information Engineering, Inner Mongolia Agricultural University Hohhot, China; College of computer and Information Engineering, Inner Mongolia Agricultural University Hohhot, China; College of computer and Information Engineering, Inner Mongolia Agricultural University Hohhot, China","6th International Conference on Wireless, Mobile and Multi-Media (ICWMMN 2015)","","2015","","","214","216","Deep learning method has been considered as a breakthrough in computer vision, successfully aplied in many domains, including biometrics. Palmprint recognition has been accepted with high acceptability and low intrusion. In this study, deep learning was introduced into palmprint recognition for a better performance. Three concrete steps were involved in the application. First, a deep belief net was built by top-to-down unsupervised training with training samples. Second, the optimum parameters were chosen to adapt the model for a robust performance. Third, the testing samples were labeled by employing the deep learning models. Compared with traditional recognition methods, such as PCA, LBP, the experimental results show that deep learning method has a higher recognition rate for palmprint recognition.","","","10.1049/cp.2015.0942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453906","palmprint recognition;deep learning;deep belief nets","","computer vision;learning (artificial intelligence);neural nets;palmprint recognition","palmprint recognition rate;deep-learning method;computer vision;top-to-down unsupervised training;optimum parameters;robust performance","","","","","","","","IET","IET Conferences"
"Hyperparameter search for deep convolutional neural network using effect factors","Z. Li; L. Jin; C. Yang; Z. Zhong","South China University of Technology; South China University of Technology; South China University of Technology; South China University of Technology","2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)","","2015","","","782","786","Learning a deep architecture involves a tough issue called hyperparameter search. This is especially the case for convolutional neural networks with a large number of hyperparameters. To solve this problem, we propose a tensor completion method to predict the best architecture configurations for convolutional neural networks. This method is based on a hypothesis that the generalization performance of a deep architecture is controlled by several effect factors, each of which is a function of hyperparameter of the deep architecture. Predicted generalization accuracy of the best configurations are checked by carrying out deep learning computation. Since generalization performance for a practical recognition task is always data- and code-dependent, we tried out our method on an open deep learning platform named Caffe, and we increased the generalization accuracy from 98.97% to around 99.25% on MNIST by replacing only five numbers.","","","10.1109/ChinaSIP.2015.7230511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230511","Architecture hyperparameter search;convolutional neural network;deep learning;artificial neural network;machine learning","Accuracy;Tensile stress;Convolutional codes;Noise;Neural networks;Computer architecture;Machine learning","learning (artificial intelligence);neural nets;tensors","hyperparameter search;deep convolutional neural network;deep architecture learning;tensor completion method;Caffe open deep learning platform;MNIST","","2","12","","","","","IEEE","IEEE Conferences"
"Lung segmentation in chest radiographs using distance regularized level set and deep-structured learning and inference","T. A. Ngo; G. Carneiro","Australia Centre for Visual Technologies, The University of Adelaide, Australia; Australia Centre for Visual Technologies, The University of Adelaide, Australia","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","2140","2143","Computer-aided diagnosis of digital chest X-ray (CXR) images critically depends on the automated segmentation of the lungs, which is a challenging problem due to the presence of strong edges at the rib cage and clavicle, the lack of a consistent lung shape among different individuals, and the appearance of the lung apex. From recently published results in this area, hybrid methodologies based on a combination of different techniques (e.g., pixel classification and deformable models) are producing the most accurate lung segmentation results. In this paper, we propose a new methodology for lung segmentation in CXR using a hybrid method based on a combination of distance regularized level set and deep structured inference. This combination brings together the advantages of deep learning methods (robust training with few annotated samples and top-down segmentation with structured inference and learning) and level set methods (use of shape and appearance priors and efficient optimization techniques). Using the publicly available Japanese Society of Radiological Technology (JSRT) dataset, we show that our approach produces the most accurate lung segmentation results in the field. In particular, depending on the initialization used, our methodology produces an average accuracy on JSTR that varies from 94.8% to 98.5%.","","","10.1109/ICIP.2015.7351179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351179","Lung segmentation;Deep learning;Level set methods","Lungs;Image segmentation;Shape;Level set;Training;Databases;Optimization","diagnostic radiography;image segmentation;inference mechanisms;learning (artificial intelligence);lung;medical image processing;set theory","JSRT dataset;Japanese Society of Radiological Technology dataset;optimization techniques;top-down segmentation;deep learning methods;deformable models;pixel classification;lung apex appearance;lung shape;clavicle;rib cage;automated lung segmentation;CXR images;digital chest X-ray images;computer-aided diagnosis;deep-structured inference;deep-structured learning;distance regularized level set method;chest radiographs","","8","14","","","","","IEEE","IEEE Conferences"
"A New Method Based on Deep Belief Networks for Learning Features from Symbolic Music","Q. Huang; Z. Huang; Y. Yuan; M. Tian","Sch. of Comput. & Inf. Sci., Southwest Univ., Chongqing, China; NA; Sch. of Comput. & Inf. Sci., Southwest Univ., Chongqing, China; NA","2015 11th International Conference on Semantics, Knowledge and Grids (SKG)","","2015","","","231","234","As the rapid increase of music data, Music Information Retrieval (MIR) have been receiving increasing attention in both the academic and commercial spheres. Feature extraction is a crucial part of many Music Information Retrieval (MIR) tasks. In recent years, deep learning approaches have gained significant interest as a way of learning a higher abstract representation from unlabeled data. In this paper, we present a system that can automatically extract relevant from symbolic music data. Firstly, The lower level features are extracted by using toolbox Music21, the higher level feature are then learned by a Deep Belief Network (DBN), finally the activations of the trained network as inputs for a non-linear Support Vector Machine (SVM) classifier. The experiment results demonstrate that the learned features obtain a better classification accuracy than other classical methods.","","","10.1109/SKG.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429384","Feature Extraction;Deep Boltzmann Machine;Support Vector Machine (SVM)","Feature extraction;Support vector machines;Training;Machine learning;Semantics;Music;Data mining","belief networks;content-based retrieval;feature extraction;learning (artificial intelligence);music;pattern classification","deep belief networks;feature learning;music information retrieval;MIR;feature extraction;deep learning approach;abstract representation;symbolic music data extraction;Music21 toolbox;DBN;nonlinear support vector machine classifier;nonlinear SVM classifier","","","25","","","","","IEEE","IEEE Conferences"
"Robust visual tracking through deep learning-based confidence evaluation","a. Euntae Hong; a. Juhan Bae; a. Jongwoo Lim","Division of Computer Science and Engineering, Hanyang University, Seoul, 133-791, Korea; Division of Computer Science and Engineering, Hanyang University, Seoul, 133-791, Korea; Division of Computer Science and Engineering, Hanyang University, Seoul, 133-791, Korea","2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","","2015","","","581","584","In this paper, we propose an object tracking method through deep learning-based confidence evaluation, aiming at correctly updating an object template and on-line training a deep neural network. Our method updats both a deep neural network and a detector in Tracking-Learning-Detection(TLD) framework by robustly finding object regions highly similar to the target. We detect tracking failure points by measuring spatiotemporal similarity from Forward-Backward Error and output of the deep neural network. In addition, the proposed method adaptively updates the templates of tracker by finding the region with highest confidence of neural network within both tracking and detection results. Our experiment results demonstrate the effectiveness of the proposed method in severe environmental changes.","","","10.1109/URAI.2015.7358836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358836","tracking;detection;deep learning tracking","Target tracking;Neural networks;Detectors;Robustness;Visualization","learning (artificial intelligence);neural nets;object tracking","forward-backward error;spatiotemporal similarity measurement;tracking failure point detection;TLD framework;tracking-learning-detection framework;deep neural network;online training;object tracking method;confidence evaluation;deep learning;robust visual tracking","","1","9","","","","","IEEE","IEEE Conferences"
"Deep Learning Based Feature Selection for Remote Sensing Scene Classification","Q. Zou; L. Ni; T. Zhang; Q. Wang","Sch. of Comput. Sci., Wuhan Univ., Wuhan, China; Sch. of Comput. Sci., Wuhan Univ., Wuhan, China; State Key Lab. of Inf. Eng. in Surveying, Wuhan Univ., Wuhan, China; Sch. of Comput. Sci., Wuhan Univ., Wuhan, China","IEEE Geoscience and Remote Sensing Letters","","2015","12","11","2321","2325","With the popular use of high-resolution satellite images, more and more research efforts have been placed on remote sensing scene classification/recognition. In scene classification, effective feature selection can significantly boost the final performance. In this letter, a novel deep-learning-based feature-selection method is proposed, which formulates the feature-selection problem as a feature reconstruction problem. Note that the popular deep-learning technique, i.e., the deep belief network (DBN), achieves feature abstraction by minimizing the reconstruction error over the whole feature set, and features with smaller reconstruction errors would hold more feature intrinsics for image representation. Therefore, the proposed method selects features that are more reconstructible as the discriminative features. Specifically, an iterative algorithm is developed to adapt the DBN to produce the inquired reconstruction weights. In the experiments, 2800 remote sensing scene images of seven categories are collected for performance evaluation. Experimental results demonstrate the effectiveness of the proposed method.","","","10.1109/LGRS.2015.2475299","National Natural Science Foundation of China; National Basic Research Program of China; 3551 Optics Valley Talents Scheme of Wuhan East Lake High-Tech Zone; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272047","Deep belief network (DBN);feature learning;iterative deep learning;scene recognition;scene understanding;Deep belief network (DBN);feature learning;iterative deep learning;scene recognition;scene understanding","Image reconstruction;Remote sensing;Machine learning;Training;Satellites;Feature extraction;Testing","belief networks;feature selection;geophysical image processing;image classification;image representation;iterative methods;remote sensing","deep learning based feature selection;remote sensing scene classification;high-resolution satellite images;remote sensing scene recognition;feature reconstruction problem;deep belief network;feature abstraction;image representation;iterative algorithm;remote sensing scene images","","175","17","","","","","IEEE","IEEE Journals"
"Deep semi-supervised learning for domain adaptation","H. Chen; J. Chien","Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan 30010, ROC; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan 30010, ROC","2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)","","2015","","","1","6","Domain adaptation aims to adapt a classifier from source domain to target domain through learning a good feature representation that allows knowledge to be shared and transferred across domains. Most of previous studies are restricted to extract features and train classifier separately under a shallow model structure. In this paper, we propose a semi-supervised domain adaptation method which co-trains the feature representation and pattern classification under deep neural network (DNN) framework. The labeling in target domain is not required. We treat the hidden layers in DNN as feature extraction and construct the output layer consisting of classification and regression. Our idea is to conduct the feature-based domain adaptation which jointly minimizes the divergence between the distributions from labeled and unlabeled data in both domains, the reconstruction errors due to an auto-encoder, and the classification errors due to the labeled data in source domain. Experiments on image recognition and sentiment classification show the superiority of DNN co-training for domain adaptation.","","","10.1109/MLSP.2015.7324325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324325","Deep learning;domain adaptation;semi-supervised learning;co-training;auto-encoder","Adaptation models;Feature extraction;Training;Neurons;Machine learning;Linear programming;Semisupervised learning","feature extraction;image classification;image reconstruction;image representation;learning (artificial intelligence);neural nets;regression analysis","deep semisupervised learning;classifier;feature representation;knowledge sharing;shallow model structure;semisupervised domain adaptation method;pattern classification;deep neural network;DNN framework;feature extraction;regression;feature-based domain adaptation;unlabeled data distributions;reconstruction errors;auto-encoder;classification errors;image recognition;sentiment classification","","6","19","","","","","IEEE","IEEE Conferences"
"Automatic Feature Learning to Grade Nuclear Cataracts Based on Deep Learning","X. Gao; S. Lin; T. Y. Wong","Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Microsoft Research Asia; Singapore Eye Research Institute","IEEE Transactions on Biomedical Engineering","","2015","62","11","2693","2701","Goal: Cataracts are a clouding of the lens and the leading cause of blindness worldwide. Assessing the presence and severity of cataracts is essential for diagnosis and progression monitoring, as well as to facilitate clinical research and management of the disease. Methods: Existing automatic methods for cataract grading utilize a predefined set of image features that may provide an incomplete, redundant, or even noisy representation. In this study, we propose a system to automatically learn features for grading the severity of nuclear cataracts from slit-lamp images. Local filters are first acquired through clustering of image patches from lenses within the same grading class. The learned filters are fed into a convolutional neural network, followed by a set of recursive neural networks, to further extract higher order features. With these features, support vector regression is applied to determine the cataract grade. Results: The proposed system is validated on a large population-based dataset of 5378 images, where it outperforms the state of the art by yielding with respect to clinical grading a mean absolute error (ε) of 0.304, a 70.7% exact integral agreement ratio (R0), an 88.4% decimal grading error ≤0.5 (Re0.5), and a 99.0% decimal grading error ≤1.0 (Re1.0). Significance: The proposed method is useful for assisting and improving clinical management of the disease in the context of large-population screening and has the potential to be applied to other eye diseases.","","","10.1109/TBME.2015.2444389","Agency for Science, Technology and Research, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7122265","Cataract Grading;Automatic Feature Learning;Deep Learning;Automatic feature learning;cataract grading;deep learning","Lenses;Feature extraction;Neural networks;Visualization;Image color analysis;Training;Standards","biomedical optical imaging;diseases;eye;image denoising;image representation;learning (artificial intelligence);medical image processing;regression analysis;support vector machines;vision defects","automatic feature learning;nuclear cataracts;deep learning;lens;clouding;progression monitoring;diagnosis monitoring;noisy representation;slit-lamp imaging;local filters;image patch clustering;population-based dataset;clinical grading;mean absolute error;integral agreement ratio;decimal grading error;clinical management;large-population screening;eye diseases","Adult;Algorithms;Cataract;Diagnostic Techniques, Ophthalmological;Humans;Lens, Crystalline;Neural Networks (Computer)","41","38","","","","","IEEE","IEEE Journals"
"A Multiobjective Sparse Feature Learning Model for Deep Neural Networks","M. Gong; J. Liu; H. Li; Q. Cai; L. Su","Key Laboratory of Intelligent Perception and Image Understanding, International Research Center for Intelligent Perception and Computation, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, International Research Center for Intelligent Perception and Computation, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, International Research Center for Intelligent Perception and Computation, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, International Research Center for Intelligent Perception and Computation, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, International Research Center for Intelligent Perception and Computation, Ministry of Education, Xidian University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","","2015","26","12","3263","3277","Hierarchical deep neural networks are currently popular learning models for imitating the hierarchical architecture of human brain. Single-layer feature extractors are the bricks to build deep networks. Sparse feature learning models are popular models that can learn useful representations. But most of those models need a user-defined constant to control the sparsity of representations. In this paper, we propose a multiobjective sparse feature learning model based on the autoencoder. The parameters of the model are learnt by optimizing two objectives, reconstruction error and the sparsity of hidden units simultaneously to find a reasonable compromise between them automatically. We design a multiobjective induced learning procedure for this model based on a multiobjective evolutionary algorithm. In the experiments, we demonstrate that the learning procedure is effective, and the proposed multiobjective model can learn useful sparse features.","","","10.1109/TNNLS.2015.2469673","National Natural Science Foundation of China; National Program for Top-Notch Young Professionals of China; Specialized Research Fund for the Doctoral Program of Higher Education; Fundamental Research Funds through the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230284","Deep neural networks;evolutionary algorithm;multiobjective optimization;sparse feature learning.;Deep neural networks;evolutionary algorithm;multiobjective optimization;sparse feature learning","Feature extraction;Pareto optimization;Brain modeling;Neural networks;Linear programming;Evolutionary computation","bioinformatics;brain;feature extraction;neural nets","hierarchical deep neural network;human brain;single-layer feature extractor;reconstruction error;multiobjective evolutionary algorithm;multiobjective sparse feature learning model","Algorithms;Artificial Intelligence;Computer Simulation;Humans;Learning;Models, Theoretical;Neural Networks (Computer)","73","52","","","","","IEEE","IEEE Journals"
"FPGA-Accelerated Hadoop Cluster for Deep Learning Computations","A. Alhamali; N. Salha; R. Morcel; M. Ezzeddine; O. Hamdan; H. Akkary; H. Hajj","NA; NA; NA; NA; NA; NA; NA","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","","2015","","","565","574","Deep learning algorithms have received significant attention in the last few years. Their popularity is due to their ability to achieve higher accuracy than conventional machine learning in many research areas such as speech recognition, image processing and natural language processing. Deep learning algorithms rely on multiple cascaded layers of non-linear processing units, typically composed of hidden artificial neural networks for feature extraction and transformation. However, deep learning algorithms require a large amount of computational power and significant amount of time to train. Fortunately, the training and inference algorithms of deep learning architectures expose abundant data-parallelism. We aim in this work to develop technology that exploits deep learning data parallelism in 2 ways: 1) by distributing deep computation into a Hadoop cluster or cloud of computing nodes, and 2) by using field programmable gate arrays (FPGA) hardware acceleration to speed up computationally intensive deep learning kernels. In this paper, we describe a hardware prototype of our accelerated Hadoop deep learning system architecture and report initial performance and energy reduction results. By accelerating the convolutional layers of deep learning Convolutional Neural Network, we have observed a potential speed-up of 12.6 times and an energy reduction of 87.5% on a 6-node FPGA accelerated Hadoop cluster.","","","10.1109/ICDMW.2015.148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395718","deep learning;convolutional neural network;Hadoop;FPGA;map-reduce","Machine learning;Computer architecture;Neural networks;Feature extraction;Training;Hardware;Field programmable gate arrays","data handling;feature extraction;field programmable gate arrays;inference mechanisms;learning (artificial intelligence);neural nets;parallel processing;software architecture","FPGA-accelerated Hadoop cluster;deep learning computations;machine learning;nonlinear processing units;hidden artificial neural networks;feature extraction;transformation;inference algorithms;deep learning architectures;deep learning data parallelism;field programmable gate arrays;convolutional neural network","","6","37","","","","","IEEE","IEEE Conferences"
"Deep Belief Networks Oriented Clustering","Q. Yang; H. Wang; T. Li; Y. Yang","NA; NA; NA; NA","2015 10th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)","","2015","","","58","65","Deep learning has been popular for a few years, and it shows great capability on unsupervised leaning of representation. Deep belief network consists of multi layers of restricted Boltzmann machine(RBM) and a deep auto-encoder, which uses a stack architecture learning feature layer by layer. The learning rule is that one deeper layer learns more complex representations, which are the high level features of the input data, from the representations learnt by the layer before. Fuzzy C-Means(FCM) is one of the most popular clustering algorithms, which allows one piece of data belong to several clusters. In this paper the authors propose a novel clustering model, and introduce a novel clustering technique(DBNOC) which combines deep belief network and fuzzy c-means. The main idea is that: first, it clusters with the high level representations learnt by stacked RBM to produce the initial cluster center, then it uses the fine-tune step including one center holding clustering algorithm and deep auto-encoder to optimize the cluster center and membership between input data and every cluster by cross iteration. The authors use FCM clustering algorithm to fulfill the model and do experiment on both low dimensional datasets and high dimensional datasets. The experiment results suggest that the proposed deep belief network oriented clustering method is better than the standard K-Means and FCM algorithm on the test datasets. Even on high dimensional datasets, the DBNOC clustering method show more generalization. What's more, the proposed model is suitable both in theoretical and practical.","","","10.1109/ISKE.2015.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383025","deep learning;unsupervised learning;deep belief network;clustering;fuzzy c-means","Clustering algorithms;Clustering methods;Unsupervised learning;Machine learning;Neural networks;Data models;Training","Boltzmann machines;fuzzy set theory;learning (artificial intelligence);optimisation;pattern clustering","deep belief network oriented clustering;DBNOC;deep learning;restricted Boltzmann machine;RBM;deep autoencoder;fuzzy c-means algorithm;FCM algorithm;cluster center optimization;membership optimization","","1","17","","","","","IEEE","IEEE Conferences"
"Multi-staged deep learning with created coarse and appended fine categories","R. Hagawa; Y. Ishii; S. Tsukizawa","Panasonic Corporation, 1006 Kadoma, Kadoma City, Osaka 571-8501, Japan; Panasonic Corporation, 1006 Kadoma, Kadoma City, Osaka 571-8501, Japan; Panasonic Corporation, 1006 Kadoma, Kadoma City, Osaka 571-8501, Japan","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","","2015","","","036","040","This paper proposes a new learning method for Deep Learning based on the concept of a Coarse-to-Fine approach. The Coarse-to-Fine classification improves Deep Learning performance, but it increases network size and presents the problem of close dependence on the accuracy of coarse classification. We tried to avoid this problem by adopting the concept of Curriculum Learning and succeeded in improving the accuracy of Deep Learning. This technique uses learning that employs a single closed image dataset several times in the same network except for the last layer. In this process, coarse labels are given to the images during the pre-training stages and fine labels are given to the same images at the fine-tuning stage. This coarse category pre-training method makes it possible to obtain those features that commonly exist in multiple fine categories. To demonstrate the advantage of this technique, several patterns of a dataset in the quantity of several tens of classes and a single dataset of 100 classes were produced using the ImageNet dataset and compared with the previous technique. The results showed a 5.7% improvement of TOP1 accuracy, with the best case confirmed in the 100-class dataset.","","","10.1109/ACPR.2015.7486461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486461","","Decision support systems;Machine learning;Neural networks;Image recognition;Pattern recognition;Learning systems;Convolutional codes","image classification;learning (artificial intelligence)","ImageNet dataset;fine-tuning stage;pretraining stages;single closed image dataset;curriculum learning;deep learning performance improvement;coarse-to-fine classification;multistaged deep learning","","","16","","","","","IEEE","IEEE Conferences"
"Audio recapture detection using deep learning","D. Luo; H. Wu; J. Huang","College of Information Engineering, Shenzhen University, Shenzhen, 518060, P.R. China, Shenzhen Key Laboratory of Media Security, Shenzhen, 518060, China; School of Information Science and Technology, Sun Yat-Sen University, Guangzhou, 510006, P.R. China; College of Information Engineering, Shenzhen University, Shenzhen, 518060, P.R. China, Shenzhen Key Laboratory of Media Security, Shenzhen, 518060, China","2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)","","2015","","","478","482","Since the audio recapture can be used to assist audio splicing, it is important to identify whether a suspected audio recording is recaptured or not. However, few works on such detection have been reported. In this paper, we propose an method to detect the recaptured audio based on deep learning and we investigate two deep learning techniques, i.e., neural network with dropout method and stack auto-encoders (SAE). The waveform samples of audio frame is directly used as the input for the deep neural network. The experimental results show that error rate around 7.5% can be achieved, which indicates that our proposed method can successfully discriminate recaptured audio and original audio.","","","10.1109/ChinaSIP.2015.7230448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230448","Audio recapture detection;Deep learning;Dropout;SAE","Machine learning;Error analysis;Speech;Artificial neural networks;Training;Feature extraction","audio coding;audio recording;neural nets;waveform analysis","audio recapture detection;deep learning;audio splicing;audio recording;dropout method;stack auto-encoders;SAE;waveform samples;deep neural network","","7","23","","","","","IEEE","IEEE Conferences"
"Application of deep-learning algorithms to MSTAR data","H. Wang; S. Chen; F. Xu; Y. Jin","Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai 200433, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai 200433, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai 200433, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai 200433, China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","3743","3745","In this paper, a new All-Convolutional Networks (A-ConvNets) is proposed and applied to Moving and Stationary Target Acquisition and Recognition (MSTAR) data. Conventional deep learning algorithms, especially the deep convolutional networks (ConvNets) have achieved many success state-of-art results. However, directly applying ConvNets to SAR data will yield severe overfitting because of limited data availability. The proposed A-ConvNets can significantly reduce the number of free parameters and the degree of overfitting. Average accuracy of 99.1% on classification of 10-class targets was obtained by applying A-ConvNets to MSTAR datasets.","","","10.1109/IGARSS.2015.7326637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326637","deep learning;synthetic aperture radar;automatic target recognition","Synthetic aperture radar;Feature extraction;Target recognition;Accuracy;Training;Support vector machines;Convolution","convolution;learning (artificial intelligence);radar signal processing;radar target recognition","all-convolutional networks;A-ConvNets;moving and stationary target acquisition and recognition data;MSTAR data;deep learning algorithms;deep convolutional networks;SAR data;free parameters;overfitting","","25","14","","","","","IEEE","IEEE Conferences"
"FPGA Design for PCANet Deep Learning Network","Y. Zhou; W. Wang; X. Huang","NA; NA; NA","2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines","","2015","","","232","232","In recent years, deep learning has attracted lots of research interests for pattern recognition and artificial intelligence. PCA Network (PCANet) is a simple deep learning network with highly competitive performance for texture classification and object recognition. When compared to other deep neural networks such as convolutional neural network (CNN), PCANet has much simpler structure, which makes it attractive for hardware design on an FPGA. In this paper, an efficient, high-throughput, pipeline architecture is proposed for the PCANet classifier. The implementation on an FPGA is more than 1,000 times faster than software execution on a general purpose processor. When evaluated using the MNIST handwritten digits dataset, the PCANet design results an accuracy of about 99.46%.","","","10.1109/FCCM.2015.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160078","Deep Learning;PCANet;FPGA;MNIST","Field programmable gate arrays;Principal component analysis;Neural networks;Hardware;Convolutional codes;Pipelines;Computer architecture","field programmable gate arrays;handwritten character recognition;image classification;learning (artificial intelligence);neural nets","FPGA design;field programmable gate array;PCANet deep learning network;PCA network;texture classification;object recognition;convolutional neural network;CNN;PCANet classifier;software execution;general purpose processor;MNIST handwritten digits dataset","","6","3","","","","","IEEE","IEEE Conferences"
"Urban classification using PolSAR data and deep learning","S. De; A. Bhattacharya","Indian Institute of Technology Bombay; Indian Institute of Technology Bombay","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","353","356","The urban classification of PolSAR images is made difficult by the characteristic of a rotated target to exhibit volume scattering. In this paper we use a deep learning technique in conjunction with some statistical parameters to learn to classify urban areas irrespective of the rotation. The learning algorithm was trained to differentiate urban from non-urban areas and was able to achieve a 8.5834% validation accuracy and 6.554% test accuracy.","","","10.1109/IGARSS.2015.7325773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325773","Deep Learning;Classification;POLSAR","Urban areas;Training;Machine learning;Accuracy;Synthetic aperture radar;Scattering","geophysical image processing;geophysical techniques;remote sensing by radar;synthetic aperture radar","urban classification;PolSAR data;PolSAR images;rotated target;volume scattering;deep learning technique;learning algorithm","","8","10","","","","","IEEE","IEEE Conferences"
"Transfer learning method using multi-prediction deep Boltzmann machines for a small scale dataset","Y. Sawada; K. Kozuka","Panasonic Corporation, 3-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan; Panasonic Corporation, 3-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan","2015 14th IAPR International Conference on Machine Vision Applications (MVA)","","2015","","","110","113","In this article, we propose a transfer learning method using the multi-prediction deep Boltzmann machine (MPDBM). In recent years, deep learning has been widely used in many applications such as image classification and object detection. However, it is hard to apply a deep learning method to medical images because the deep learning method needs a large number of training data to train the deep neural network. Medical image datasets such as X-ray CT image datasets do not have enough training data because of privacy. In this article, we propose a method that re-uses the network trained on non-medical images (source domain) to improve performance even if we have a small number of medical images (target domain). Our proposed method firstly trains the deep neural network for solving the source task using the MPDBM. Secondly, we evaluate the relation between the source domain and the target domain. To evaluate the relation, we input the target domain into the deep neural network trained on the source domain. Then, we compute the histograms based on the response of the output layer. After computing the histograms, we select the variables of the output layer corresponding to the target domain. Then, we tune the parameters in such a way that the selected variables respond as the outputs of the target domain. In this article, we use the MNIST dataset as the source domain and the lung dataset of the X-ray CT images as the target domain. Experimental results show that our proposed method can improve classification performance.","","","10.1109/MVA.2015.7153145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153145","","Neural networks;Histograms;Lesions;Learning systems;Training;Biomedical imaging;Training data","Boltzmann machines;computerised tomography;image classification;learning (artificial intelligence);lung;medical image processing;neural nets;X-ray imaging","small scale dataset;transfer learning method;multiprediction deep Boltzmann machine;deep learning method;deep neural network;medical image datasets;network reuse;nonmedical image;source task solving;MPDBM;source domain;target domain;MNIST dataset;X-ray CT image lung dataset;image classification performance improvement","","7","10","","","","","IEEE","IEEE Conferences"
"Deep neural network language model research and application overview","Fu-Lian Yin; Xing-Yi Pan; Xiao-Wei Liu; Hui-Xin Liu","Department of Information and Engineering, Faculty of Science and Technology, Communication University of China, Beijing, 100024, China; Department of Information and Engineering, Faculty of Science and Technology, Communication University of China, Beijing, 100024, China; Department of Information and Engineering, Faculty of Science and Technology, Communication University of China, Beijing, 100024, China; Department of Information and Engineering, Faculty of Science and Technology, Communication University of China, Beijing, 100024, China","2015 12th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)","","2015","","","55","60","Research of the neural network language model in NLP is reviewed. In this paper, the neural network language models are classified into early shallow language models and deep neural network models based on deep learning. This paper emphatically introduces progress of the deep neural network language models, and summarizes the status of deep neural network research's development. Finally, the existing problems and deficiencies are put forward.","","","10.1109/ICCWAMTIP.2015.7493906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493906","Deep Neural Network Language Model;Natural Language Processing;Word Embedding;Sentiment Analysis;Deep Learning","Neural networks;Training;Computational modeling;Machine learning;Syntactics;Semantics;Analytical models","learning (artificial intelligence);natural language processing;neural nets","deep neural network language model;NLP;shallow language model;deep learning","","1","24","","","","","IEEE","IEEE Conferences"
"Using deep learning for robustness to parapapillary atrophy in optic disc segmentation","R. Srivastava; J. Cheng; D. W. K. Wong; J. Liu","Institute for Infocomm Research, Singapore 138632; Institute for Infocomm Research, Singapore 138632; Institute for Infocomm Research, Singapore 138632; Institute for Infocomm Research, Singapore 138632","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","","2015","","","768","771","Optic Disc (OD) segmentation from retinal fundus images is important for many applications such as detecting other optic structures and early detection of glaucoma. One of the major problems in segmenting OD is the presence of Para-papillary Atrophy (PPA) which in many cases looks similar to the OD. Researchers have used many different features to distinguish between PPA and OD, however each of these features has some limitation or the other. In this paper, we propose to use a deep neural network for OD segmentation which can learn features to distinguish PPA from OD. Using simple image intensity based features, the proposed method has the least mean overlapping error (9.7%) among the state-of-the-art works for OD segmentation in fundus images with PPA.","","","10.1109/ISBI.2015.7163985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163985","Optic Disc segmentation;deep learning;parapapillary atrophy;retinal fundus images","Image segmentation;Optical imaging;Adaptive optics;Feature extraction;Retina;Training;Atrophy","eye;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;neural nets;neurophysiology","deep learning;parapapillary atrophy;optic disc segmentation;retinal fundus images;optic structures;glaucoma detection;deep neural network;image intensity based features;least mean overlapping error","","8","10","","","","","IEEE","IEEE Conferences"
"Pose-Robust and Discriminative Feature Representation by Multi-task Deep Learning for Multi-view Face Recognition","J. Seo; H. Kim; Y. M. Ro","Sch. of Electr. Eng., KAIST, Daejeon, South Korea; Sch. of Electr. Eng., KAIST, Daejeon, South Korea; Sch. of Electr. Eng., KAIST, Daejeon, South Korea","2015 IEEE International Symposium on Multimedia (ISM)","","2015","","","166","171","Automatic face recognition (FR) under uncontrolled environments has attracted considerable research attention. In the uncontrolled environments, pose variation is known as one of the crucial factors that influences FR performance. In this paper, we propose a discriminative and pose-robust feature representation using the multi-task learning in deep convolutional neural networks (ConvNet). We introduce four tasks (i.e., maximizing inter-class variation, minimizing intraclass variation, minimizing intra-pose variation, and preserving pose continuity) to learn the ConvNet. Moreover, two-stage learning strategy is proposed to minimize the error functions in learning the deep ConvNet. The extensive experimental results (with the challenging CMU MultiPIE dataset containing pose variations) show that the proposed method outperform stateof-the-art in terms of FR accuracy. Furthermore, the proposed method shows significant improvement even for the face images whose poses are not included in training set.","","","10.1109/ISM.2015.93","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442319","multi-view face recognition;deep learning;deep convolutional neural networks;deep feature representation;multi-task learning","Face;Training;Lighting;Face recognition;Machine learning;Neural networks;Image resolution","face recognition;feature extraction;image representation;neural nets;pose estimation","discriminative feature representation;pose robust representation;multitask deep learning;multiview face recognition;automatic face recognition;FR;uncontrolled environments;pose variation;pose robust feature representation;multitask learning;deep convolutional neural networks;error functions;MultiPIE dataset;pose variations","","2","22","","","","","IEEE","IEEE Conferences"
"Training word embeddings for deep learning in biomedical text mining tasks","Z. Jiang; L. Li; D. Huang; Liuke Jin","School of Computer Science and Technology, Dalian University of Technology, China; School of Computer Science and Technology, Dalian University of Technology, China; School of Computer Science and Technology, Dalian University of Technology, China; School of Computer Science and Technology, Dalian University of Technology, China","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","625","628","Most word embedding methods are proposed with general purpose which take a word as a basic unit and learn embeddings according to words' external contexts. However, in biomedical text mining, there are many biomedical entities and syntactic chunks which contain rich domain information, and the semantic meaning of a word is also strongly related to those information. Hence, we present a biomedical domain-specific word embedding model by incorporating stem, chunk and entity to train word embeddings. We also present two deep learning architectures respectively for two biomedical text mining tasks, by which we evaluate our word embeddings and compare them with other models. Experimental results show that our biomedical domain-specific word embeddings overall outperform other general-purpose word embeddings in these deep learning methods for biomedical text mining tasks.","","","10.1109/BIBM.2015.7359756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359756","biomedical word embeddings;word representation;deep learning;biomedical text mining","Syntactics;Training;Proteins;Protein engineering","bioinformatics;data mining;learning (artificial intelligence);medical computing;natural language processing;text analysis","training word embedding;biomedical text mining tasks;word external context;word semantic meaning;biomedical domain-specific word embedding model;deep learning architecture;biomedical entities;syntactic chunks","","21","20","","","","","IEEE","IEEE Conferences"
"Bit-Scalable Deep Hashing With Regularized Similarity Learning for Image Retrieval and Person Re-Identification","R. Zhang; L. Lin; R. Zhang; W. Zuo; L. Zhang","Sun Yat-sen University, Guangzhou, China; Sun Yat-sen University, Guangzhou, China; Sun Yat-sen University, Guangzhou, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Image Processing","","2015","24","12","4766","4779","Extracting informative image features and learning effective approximate hashing functions are two crucial steps in image retrieval. Conventional methods often study these two steps separately, e.g., learning hash functions from a predefined hand-crafted feature space. Meanwhile, the bit lengths of output hashing codes are preset in the most previous methods, neglecting the significance level of different bits and restricting their practical flexibility. To address these issues, we propose a supervised learning framework to generate compact and bit-scalable hashing codes directly from raw images. We pose hashing learning as a problem of regularized similarity learning. In particular, we organize the training images into a batch of triplet samples, each sample containing two images with the same label and one with a different label. With these triplet samples, we maximize the margin between the matched pairs and the mismatched pairs in the Hamming space. In addition, a regularization term is introduced to enforce the adjacency consistency, i.e., images of similar appearances should have similar codes. The deep convolutional neural network is utilized to train the model in an end-to-end fashion, where discriminative image features and hash functions are simultaneously optimized. Furthermore, each bit of our hashing codes is unequally weighted, so that we can manipulate the code lengths by truncating the insignificant bits. Our framework outperforms state-of-the-arts on public benchmarks of similar image search and also achieves promising results in the application of person re-identification in surveillance. It is also shown that the generated bit-scalable hashing codes well preserve the discriminative powers with shorter code lengths.","","","10.1109/TIP.2015.2467315","Hong Kong Scholar Program; Guangdong Natural Science Foundation; Program of Guangzhou Zhujiang Star of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7185403","Image Retrieval;Hashing Learning;Similarity Comparison;Deep Model;Person Re-identification;Image retrieval;hashing learning;similarity comparison;deep model;person re-identification","Training;Image retrieval;Convolutional codes;Neural networks;Approximation methods;Optimization;Convolution","cryptography;feature extraction;image coding;image matching;image retrieval;learning (artificial intelligence);neural nets","bit-scalable deep hashing;regularized similarity learning;image retrieval;person reidentification;informative image feature extraction;approximate hashing functions;hand-crafted feature space;hash function learning;hashing code bit lengths;supervised learning framework;bit-scalable hashing codes;regularized similarity learning;Hamming space;adjacency consistency;deep convolutional neural network;discriminative image features","Algorithms;Animals;Biometric Identification;Databases, Factual;Humans;Image Processing, Computer-Assisted;Neural Networks (Computer)","221","44","","","","","IEEE","IEEE Journals"
"PCANet: A Simple Deep Learning Baseline for Image Classification?","T. Chan; K. Jia; S. Gao; J. Lu; Z. Zeng; Y. Ma","MediaTek Inc., Hsinchu, Taiwan; Department of Computer and Information ScienceFaculty of Science and Technology, University of Macau, Macau, SAR, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Department of Automation, Tsinghua University, Beijing, China; Advanced Digital Sciences Center, Singapore; School of Information Science and Technology, ShanghaiTech University, Shanghai, China","IEEE Transactions on Image Processing","","2015","24","12","5017","5032","In this paper, we propose a very simple deep learning network for image classification that is based on very basic data processing components: 1) cascaded principal component analysis (PCA); 2) binary hashing; and 3) blockwise histograms. In the proposed architecture, the PCA is employed to learn multistage filter banks. This is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus called the PCA network (PCANet) and can be extremely easily and efficiently designed and learned. For comparison and to provide a better understanding, we also introduce and study two simple variations of PCANet: 1) RandNet and 2) LDANet. They share the same topology as PCANet, but their cascaded filters are either randomly selected or learned from linear discriminant analysis. We have extensively tested these basic networks on many benchmark visual data sets for different tasks, including Labeled Faces in the Wild (LFW) for face verification; the MultiPIE, Extended Yale B, AR, Facial Recognition Technology (FERET) data sets for face recognition; and MNIST for hand-written digit recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state-of-the-art features either prefixed, highly hand-crafted, or carefully learned [by deep neural networks (DNNs)]. Even more surprisingly, the model sets new records for many classification tasks on the Extended Yale B, AR, and FERET data sets and on MNIST variations. Additional experiments on other public data sets also demonstrate the potential of PCANet to serve as a simple but highly competitive baseline for texture classification and object recognition.","","","10.1109/TIP.2015.2475625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7234886","Convolution Neural Network;Deep Learning;PCA Network;Random Network;LDA Network;Face Recognition;Handwritten Digit Recognition;Object Classification;Convolution neural network;deep learning;PCA network;random network;LDA network;face recognition;handwritten digit recognition;object classification","Principal component analysis;Histograms;Face;Feature extraction;Machine learning;Training;Face recognition","channel bank filters;face recognition;handwriting recognition;image classification;image texture;learning (artificial intelligence);neural nets;object recognition;principal component analysis","deep learning baseline;image classification;data processing components;cascaded principal component analysis;binary hashing;blockwise histograms;multistage filter banks;PCA network;PCANet;RandNet;LDANet;linear discriminant analysis;visual data sets;labeled faces;wild;LFW;face verification;multiPIE;extended Yale B;AR;facial recognition technology;FERET data sets;handwritten digit recognition;MNIST;deep neural networks;DNN;public data sets;texture classification;object recognition","","445","65","","","","","IEEE","IEEE Journals"
"Restricted Boltzmann Machine for Nonlinear System Modeling","E. D. l. Rosa; W. Yu","NA; Dept. de Control Automatico, CINVESTAV-IPN (Nat. Polytech. Inst.), Mexico City, Mexico","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","443","446","In this paper, we use a deep learning method, restricted Boltzmann machine, for nonlinear system identification. The neural model has deep architecture and is generated by a random search method. The initial weights of this deep neural model are obtained from the restricted Boltzmann machines. To identify nonlinear systems, we propose special unsupervised learning methods with input data. The normal supervised learning is used to train the weights with the output data. The modified algorithm is validated by modeling two benchmark systems.","","","10.1109/ICMLA.2015.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424354","deep learning;modeling;restricted Boltzmann machine","Nonlinear systems;Machine learning;Probability distribution;Training;Search methods;Unsupervised learning;Benchmark testing","Boltzmann machines;identification;nonlinear systems;search problems;unsupervised learning","restricted Boltzmann machine;nonlinear system modeling;deep learning method;nonlinear system identification;neural model;random search method;unsupervised learning methods;normal supervised learning","","1","18","","","","","IEEE","IEEE Conferences"
"Weather forecasting using deep learning techniques","A. G. Salman; B. Kanigoro; Y. Heryadi","School of Computer Science, Bina Nusantara University, Jakarta, Indonesia; School of Computer Science, Bina Nusantara University, Jakarta, Indonesia; School of Computer Science, Bina Nusantara University, Jakarta, Indonesia","2015 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","","2015","","","281","285","Weather forecasting has gained attention many researchers from various research communities due to its effect to the global human life. The emerging deep learning techniques in the last decade coupled with the wide availability of massive weather observation data and the advent of information and computer technology have motivated many researches to explore hidden hierarchical pattern in the large volume of weather dataset for weather forecasting. This study investigates deep learning techniques for weather forecasting. In particular, this study will compare prediction performance of Recurrence Neural Network (RNN), Conditional Restricted Boltzmann Machine (CRBM), and Convolutional Network (CN) models. Those models are tested using weather dataset provided by BMKG (Indonesian Agency for Meteorology, Climatology, and Geophysics) which are collected from a number of weather stations in Aceh area from 1973 to 2009 and El-Nino Southern Oscilation (ENSO) data set provided by International Institution such as National Weather Service Center for Environmental Prediction Climate (NOAA). Forecasting accuracy of each model is evaluated using Frobenius norm. The result of this study expected to contribute to weather forecasting for wide application domains including flight navigation to agriculture and tourism.","","","10.1109/ICACSIS.2015.7415154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415154","deep learning;recurrent neural network;conditional restricted boltzmann machines;convolutional networks;weather","Computational modeling;Meteorology;Artificial neural networks;Bismuth;US Government agencies;Read only memory","geophysics computing;learning (artificial intelligence);weather forecasting","weather forecasting;deep learning technique;weather observation data;information technology;weather station;El-Nino Southern Oscilation;ENSO;Frobenius norm","","12","22","","","","","IEEE","IEEE Conferences"
"A Deep Learning Neural Network for Number Cognition: A bi-cultural study with the iCub","A. Di Nuovo; V. M. De La Cruz; A. Cangelosi","Centre for Robotics and Neural Systems, Plymouth University, UK; Department of Cognitive Processes, University of Catania, Italy; Centre for Robotics and Neural Systems, Plymouth University, UK","2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)","","2015","","","320","325","The novel deep learning paradigm offers a highly biologically plausible way to train neural network architectures with many layers, inspired by the hierarchical organization of the human brain. Indeed, deep learning gives a new dimension to research modeling human cognitive behaviors, and provides new opportunities for applications in cognitive robotics. In this paper, we present a novel deep neural network architecture for number cognition by means of finger counting and number words. The architecture is composed of 5 layers and is designed in a way that allows it to learn numbers from one to ten by associating the sensory inputs (motor and auditory) coming from the iCub humanoid robotic platform. The architecture performance is validated and tested in two developmental experiments. In the first experiment, standard backpropagation is compared with a deep learning approach, in which weights and biases are pre-trained by means of a greedy algorithm and then refined with backpropagation. In the second experiment, six bi-cultural number learning conditions are compared to explore the impact of different languages (for number words) and finger counting strategies. The developmental experiments confirm the validity of the model and the increase in efficiency given by the deep learning approach. Results of the bi-cultural study are presented and discussed with respect to the neuro-psychological literature and implications of the results for learning situations are briefly outlined.","","","10.1109/DEVLRN.2015.7346165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346165","Deep Learning;Number Cognition;Cognitive Developmental Robotics;Embodiment;Bi-Cultural learning","Thumb;Machine learning;Cognition;Computer architecture;Robot sensing systems","artificial intelligence;backpropagation;brain;dexterous manipulators;humanoid robots;intelligent robots;neural nets","deep learning neural network;number cognition;neural network architecture;human brain;human cognitive behavior;cognitive robotics;iCub humanoid robotic platform;backpropagation;bi-cultural number learning condition","","10","26","","","","","IEEE","IEEE Conferences"
"Regularized Deep Learning for Face Recognition With Weight Variations","S. Nagpal; M. Singh; R. Singh; M. Vatsa","Indraprastha Institute of Information Technology Delhi, New Delhi, India; Indraprastha Institute of Information Technology Delhi, New Delhi, India; Indraprastha Institute of Information Technology Delhi, New Delhi, India; Indraprastha Institute of Information Technology Delhi, New Delhi, India","IEEE Access","","2015","3","","3010","3018","Body weight variations are an integral part of a person's aging process. However, the lack of association between the age and the weight of an individual makes it challenging to model these variations for automatic face recognition. In this paper, we propose a regularizer-based approach to learn weight invariant facial representations using two different deep learning architectures, namely, sparse-stacked denoising autoencoders and deep Boltzmann machines. We incorporate a body-weight aware regularization parameter in the loss function of these architectures to help learn weight-aware features. The experiments performed on the extended WIT database show that the introduction of weight aware regularization improves the identification accuracy of the architectures both with and without dropout.","","","10.1109/ACCESS.2015.2510865","Department of Electronics and Information Technology, Government of India; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7361971",";Face recognition;biometrics;body-weight variations;facial aging","Machine learning;Face recognition;Deep learning;Training data;Noise reduction","face recognition;feature extraction;learning (artificial intelligence)","regularized deep learning;body weight variations;person aging process;automatic face recognition;regularizer-based approach;learn weight invariant facial representations;deep learning architectures;sparse-stacked denoising autoencoders;deep Boltzmann machines;body-weight aware regularization parameter;weight-aware feature learning;extended WIT database","","11","22","","","","","IEEE","IEEE Journals"
"Weakly Semi-Supervised Deep Learning for Multi-Label Image Annotation","F. Wu; Z. Wang; Z. Zhang; Y. Yang; J. Luo; W. Zhu; Y. Zhuang","College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; University of Technology Sydney, Sydney, Australia; Department of Computer Science, 611 Computer Studies Building, University of Rochester, Rochester, NY; Tsinghua National Laboratory for Information Science and Technology, Department of Computer Science and Technology, Tsinghua University, Beijing, China; College of Computer Science, Zhejiang University, Hangzhou, China","IEEE Transactions on Big Data","","2015","1","3","109","122","In this paper, we study leveraging both weakly labeled images and unlabeled images for multi-label image annotation. Motivated by the recent advance in deep learning, we propose an approach called weakly semi-supervised deep learning for multi-label image annotation (WeSed). In WeSed, a novel weakly weighted pairwise ranking loss is effectively utilized to handle weakly labeled images, while a triplet similarity loss is employed to harness unlabeled images. WeSed enables us to train deep convolutional neural network (CNN) with images from social networks where images are either only weakly labeled with several labels or unlabeled. We also design an efficient algorithm to sample high-quality image triplets from large image datasets to fine-tune the CNN. WeSed is evaluated on benchmark datasets for multi-label annotation. The experiments demonstrate the effectiveness of our proposed approach and show that the leverage of the weakly labeled images and unlabeled images leads to a significantly better performance.","","","10.1109/TBDATA.2015.2497270","National Basic Research Program of China; U.S. National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7317747","Weakly labeled image;unlabeled image;deep learning;ranking loss;Weakly labeled image;unlabeled image;deep learning;ranking loss","Training;Semantics;Machine learning;Visualization;Big data;Neural networks;Social network services","image retrieval;image sampling;learning (artificial intelligence);neural nets;visual databases","weakly-semisupervised deep-learning;multilabel image annotation;weakly-labeled image leveraging;benchmark datasets;large image datasets;high-quality image triplet sampling;social network images;CNN training;deep-convolutional neural network training;triplet similarity loss;weakly-weighted pairwise ranking loss;WeSed;unlabeled image leveraging","","44","35","","","","","IEEE","IEEE Journals"
"Study on signal recognition and diagnosis for spacecraft based on deep learning method","Ke Li; Quanxin Wang","School of Aeronautic Science and Engineering, Beijing University of Aeronautics and Astronautics, China; School of Aeronautic Science and Engineering, Beijing University of Aeronautics and Astronautics, China","2015 Prognostics and System Health Management Conference (PHM)","","2015","","","1","5","According to the large variety of data generated during the spacecraft test and fault diagnosis, this paper designs a multi class classification algorithm based on deep learning method. The algorithm uses the stack auto-Encoder to initialize the initial weights and offsets of the multi-layer neural network, and then monitor the parameters after the initialization with the gradient descent method. The algorithm can overcome many weaknesses of SVM, for example, it is too complex and occupied more space when data is large or the categories are huge. By studying the measured data, the expert knowledge can be provided for the spacecraft health management platform. Experimental data show that this depth learning algorithm can get a high accuracy in the classification of multi class signals of spacecraft.","","","10.1109/PHM.2015.7380040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7380040","Fault Diagnosis;deep learning;Data Compression;Pattern Recognition;Auto-Encoder","Training;Artificial neural networks;Space vehicles;Data mining;Yttrium;Logistics","aerospace computing;aerospace testing;fault diagnosis;gradient methods;learning (artificial intelligence);neural nets;signal classification;space vehicles;support vector machines","signal recognition;deep learning method;spacecraft test;fault diagnosis;multiclass classification algorithm;stack auto-encoder;multilayer neural network;gradient descent method;SVM;expert knowledge;spacecraft health management platform;multiclass signal classification","","1","16","","","","","IEEE","IEEE Conferences"
"A Hierarchical Deep Neural Network for Fault Diagnosis on Tennessee-Eastman Process","D. Xie; L. Bai","NA; NA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","745","748","This paper proposes a hierarchical deep neural network (HDNN) for diagnosing the faults on the Tennessee-Eastman process (TEP). The TEP process is a benchmark simulation model for evaluating process control and monitoring method. A supervisory deep neural network is trained to categorize the whole faults into a few groups. For each group of faults, a special deep neural network which is trained for the particular group is triggered for further diagnosis. The training and test data is generated from the Tennessee Eastman process simulation. The performance of the proposed method is evaluated and compared to single neural network (SNN) and duty-oriented hierarchical artificial neural network (DOHANN) methods. The results of experiment demonstrate that our method outperforms the SNN and DOHANN methods.","","","10.1109/ICMLA.2015.208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424410","Deep neural network;Tennessee-Eastman process;fault diagnosis;chemical engineering","Fault diagnosis;Biological neural networks;Training;Artificial neural networks;Machine learning;Machine learning algorithms","chemical engineering computing;fault diagnosis;learning (artificial intelligence);neural nets","hierarchical deep neural network;fault diagnosis;Tennessee-Eastman process;supervisory deep neural network;single neural network;duty-oriented hierarchical artificial neural network","","4","7","","","","","IEEE","IEEE Conferences"
"Multimodal deep network learning-based image annotation","S. Zhu; X. Li; S. Shen","Nanjing University of Posts and Telecommunications at Xianlin Campus, People's Republic of China; Nanjing University of Posts and Telecommunications at Xianlin Campus, People's Republic of China; Nanjing University of Posts and Telecommunications at Xianlin Campus, People's Republic of China","Electronics Letters","","2015","51","12","905","906","Multilabel image annotation is one of the most important open problems in the computer vision field. Unlike existing works that usually use conventional visual features to annotate images, features based on deep learning have shown potential to achieve outstanding performance. A multimodal deep learning framework is proposed, which aims to optimally integrate multiple deep neural networks pretrained with convolutional neural networks. In particular, the proposed framework explores a unified two-stage learning scheme that consists of (i) learning to fune-tune the parameters of the deep neural network with respect to each individual modality and (ii) learning to find the optimal combination of diverse modalities simultaneously in a coherent process. Experiments conducted on a variety of public datasets.","","","10.1049/el.2015.0258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7122435","","","image processing;learning (artificial intelligence);neural nets","unified two-stage learning scheme;convolutional neural networks;multiple deep neural networks;multimodal deep learning framework;visual features;computer vision;multilabel image annotation;multimodal deep network learning-based image annotation","","1","5","","","","","IET","IET Journals"
"Privacy-preserving deep learning","R. Shokri; V. Shmatikov","UT Austin, United States; Cornell Tech., NY, United States","2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)","","2015","","","909","910","Deep learning based on artificial neural networks is a very popular approach to modeling, classifying, and recognizing complex data such as images, speech, and text. The unprecedented accuracy of deep learning methods has turned them into the foundation of new AI-based services on the Internet. Commercial companies that collect user data on a large scale have been the main beneficiaries of this trend since the success of deep learning techniques is directly proportional to the amount of data available for training. Massive data collection required for deep learning presents obvious privacy issues. Users' personal, highly sensitive data such as photos and voice recordings is kept indefinitely by the companies that collect it. Users can neither delete it, nor restrict the purposes for which it is used. Furthermore, centrally kept data is subject to legal subpoenas and extrajudicial surveillance. Many data owners-for example, medical institutions that may want to apply deep learning methods to clinical records-are prevented by privacy and confidentiality concerns from sharing the data and thus benefitting from large-scale deep learning. In this paper, we present a practical system that enables multiple parties to jointly learn an accurate neural-network model for a given objective without sharing their input datasets. We exploit the fact that the optimization algorithms used in modern deep learning, namely, those based on stochastic gradient descent, can be parallelized and executed asynchronously. Our system lets participants train independently on their own datasets and selectively share small subsets of their models' key parameters during training. This offers an attractive point in the utility/privacy tradeoff space: participants preserve the privacy of their respective data while still benefitting from other participants' models and thus boosting their learning accuracy beyond what is achievable solely on their own inputs. We demonstrate the accuracy of our privacy-preserving deep learning on benchmark datasets.","","","10.1109/ALLERTON.2015.7447103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7447103","","Training;Privacy;Machine learning;Data models;Decision support systems;Companies;Data privacy","data privacy;gradient methods;learning (artificial intelligence);neural nets;optimisation","privacy-preserving deep learning;artificial neural networks;data modeling;data classification;data recognition;AI-based services;artificial intelligence;Internet;data collection;optimization algorithms;stochastic gradient descent;data privacy","","13","1","","","","","IEEE","IEEE Conferences"
"Deep Learning for Just-in-Time Defect Prediction","X. Yang; D. Lo; X. Xia; Y. Zhang; J. Sun","Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China; Sch. of Inf. Syst., Singapore Manage. Univ., Singapore, Singapore; Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China","2015 IEEE International Conference on Software Quality, Reliability and Security","","2015","","","17","26","Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time. Nowadays, deep learning is a hot topic in the machine learning literature. Whether deep learning can be used to improve the performance of just-in-time defect prediction is still uninvestigated. In this paper, to bridge this research gap, we propose an approach Deeper which leverages deep learning techniques to predict defect-prone changes. We first build a set of expressive features from a set of initial change features by leveraging a deep belief network algorithm. Next, a machine learning classifier is built on the selected features. To evaluate the performance of our approach, we use datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. We compare our approach with the approach proposed by Kamei et al. The experimental results show that on average across the 6 projects, Deeper could discover 32.22% more bugs than Kamei et al's approach (51.04% versus 18.82% on average). In addition, Deeper can achieve F1-scores of 0.22-0.63, which are statistically significantly higher than those of Kamei et al.'s approach on 4 out of the 6 projects.","","","10.1109/QRS.2015.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272910","Deep Learning;Just-In-Time Defect Prediction;Deep Belief Network;Cost Effectiveness","Feature extraction;Logistics;Machine learning;Computer bugs;Measurement;Training;Software quality","just-in-time;learning (artificial intelligence);pattern classification;software quality","deep learning;just-in-time defect prediction;change-level defect prediction;software quality;machine learning literature;machine learning classifier","","47","40","","","","","IEEE","IEEE Conferences"
"Deep neural networks for understanding and diagnosing partial discharge data","V. M. Catterson; B. Sheng","Institute for Energy and Environment, University of Strathclyde, Glasgow, United Kingdom; Institute for Energy and Environment, University of Strathclyde, Glasgow, United Kingdom","2015 IEEE Electrical Insulation Conference (EIC)","","2015","","","218","221","Artificial neural networks have been investigated for many years as a technique for automated diagnosis of defects causing partial discharge (PD). While good levels of accuracy have been reported, disadvantages include the difficulty of explaining results, and the need to hand-craft appropriate features for standard two-layer networks. Recent advances in the design and training of deep neural networks, which contain more than two layers of hidden neurons, have resulted in improved results in speech and image recognition tasks. This paper investigates the use of deep neural networks for PD diagnosis. Defect samples constructed in mineral oil were used to generate data for training and testing. The paper demonstrates the improvements in accuracy and visualization of learning which can be gained from deep learning.","","","10.1109/ICACACT.2014.7223616","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223616","Artificial neural networks;deep learning;partial discharge;diagnostics;UHF monitoring;defects in oil","Neurons;Partial discharges;Accuracy;Biological neural networks;Training;Computer architecture;Machine learning","data analysis;electrical maintenance;fault diagnosis;learning (artificial intelligence);neural nets;partial discharges;power engineering computing","deep neural networks;partial discharge data;hidden neurons;partial discharge diagnosis;mineral oil;deep learning","","3","16","","","","","IEEE","IEEE Conferences"
"Multimedia Retrieval via Deep Learning to Rank","X. Zhao; X. Li; Z. Zhang","Department of Information Scienceand Electronic Engineering, Zhejiang University, Hangzhou, China; College of Computer Science, ZhejiangUniversity, Hangzhou, China; Department of Information Scienceand Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Signal Processing Letters","","2015","22","9","1487","1491","Many existing learning-to-rank approaches are incapable of effectively modeling the intrinsic interaction relationships between the feature-level and ranking-level components of a ranking model. To address this problem, we propose a novel joint learning-to-rank approach called Deep Latent Structural SVM (DL-SSVM), which jointly learns deep neural networks and latent structural SVM (connected by a set of latent feature grouping variables) to effectively model the interaction relationships at two levels (i.e., feature-level and ranking-level). To make the joint learning problem easier to optimize, we present an effective auxiliary variable-based alternating optimization approach with respect to deep neural network learning and structural latent SVM learning. Experimental results on several challenging datasets have demonstrated the effectiveness of the proposed learning to rank approach in real-world information retrieval.","","","10.1109/LSP.2015.2410134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7054452","Deep neural network;joint learning;latent variable;learning to rank;structural SVM","Neural networks;Support vector machines;Vectors;Feature extraction;Joints;Data models;Adaptation models","information retrieval;learning (artificial intelligence);multimedia systems;neural nets;support vector machines","multimedia retrieval;deep learning-to-rank approach;feature-level component;ranking-level component;deep latent structural SVM;DL-SSVM approach;auxiliary variable-based alternating optimization approach;structural latent SVM learning;deep neural network learning;information retrieval","","8","24","","","","","IEEE","IEEE Journals"
"Toward Deep Learning Software Repositories","M. White; C. Vendome; M. Linares-Vasquez; D. Poshyvanyk","Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA; Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA; Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA; Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","","2015","","","334","345","Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields such as natural language processing (NLP). Recent research in the software engineering (SE) community has demonstrated the usefulness of applying NLP techniques to software corpora. Hence, we motivate deep learning for software language modeling, highlighting fundamental differences between state-of-the-practice software language models and connectionist models. Our deep learning models are applicable to source code files (since they only require lexically analyzed source code written in any programming language) and other types of artifacts. We show how a particular deep learning model can remember its state to effectively model sequential data, e.g., Streaming software tokens, and the state is shown to be much more expressive than discrete tokens in a prefix. Then we instantiate deep learning models and show that deep learning induces high-quality models compared to n-grams and cache-based n-grams on a corpus of Java projects. We experiment with two of the models' hyper parameters, which govern their capacity and the amount of context they use to inform predictions, before building several committees of software language models to aid generalization. Then we apply the deep learning models to code suggestion and demonstrate their effectiveness at a real SE task compared to state-of-the-practice models. Finally, we propose avenues for future work, where deep learning can be brought to bear to support model-based testing, improve software lexicons, and conceptualize software artifacts. Thus, our work serves as the first step toward deep learning software repositories.","","","10.1109/MSR.2015.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180092","Software repositories;machine learning;deep learning;software language models;n-grams;neural networks","Software;Machine learning;Computational modeling;Context;Training;Context modeling;Computer architecture","Java;learning (artificial intelligence);natural language processing;program testing;project management;source code (software)","deep-learning software repositories;automatic compositional representation learning;natural language processing;NLP techniques;software engineering community;SE community;software corpora;software language modeling;connectionist models;lexically analyzed source code files;programming language;sequential data model;software token streaming;high-quality models;Java project corpus;hyperparameters;code suggestion;SE task;software lexicon improvement;software artifact conceptualization","","50","76","","","","","IEEE","IEEE Conferences"
"Vehicle Color Recognition With Spatial Pyramid Deep Learning","C. Hu; X. Bai; L. Qi; P. Chen; G. Xue; L. Mei","Third Res. Inst., Minist. of Public Security, Shanghai, China; Sch. of Electron. Inf. & Commun., Huazhong Univ. of Sci. & Technol., Wuhan, China; Third Res. Inst., Minist. of Public Security, Shanghai, China; Sch. of Electron. Inf. & Commun., Huazhong Univ. of Sci. & Technol., Wuhan, China; Third Res. Inst., Minist. of Public Security, Shanghai, China; Third Res. Inst., Minist. of Public Security, Shanghai, China","IEEE Transactions on Intelligent Transportation Systems","","2015","16","5","2925","2934","Color, as a notable and stable attribute of vehicles, can serve as a useful and reliable cue in a variety of applications in intelligent transportation systems. Therefore, vehicle color recognition in natural scenes has become an important research topic in this area. In this paper, we propose a deep-learning-based algorithm for automatic vehicle color recognition. Different from conventional methods, which usually adopt manually designed features, the proposed algorithm is able to adaptively learn representation that is more effective for the task of vehicle color recognition, which leads to higher recognition accuracy and avoids preprocessing. Moreover, we combine the widely used spatial pyramid strategy with the original convolutional neural network architecture, which further boosts the recognition accuracy. To the best of our knowledge, this is the first work that employs deep learning in the context of vehicle color recognition. The experiments demonstrate that the proposed approach achieves superior performance over conventional methods.","","","10.1109/TITS.2015.2430892","National 863 Project; National Science and Technology Major Project; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7118723","Color recognition;deep learning;convolutional neural network (CNN);spatial pyramid (SP);intelligent transportation;Color recognition;deep learning;convolutional neural network (CNN);spatial pyramid (SP);intelligent transportation","Image color analysis;Vehicles;Feature extraction;Support vector machines;Training;Histograms;Computer architecture","convolution;image colour analysis;image recognition;intelligent transportation systems;learning (artificial intelligence);natural scenes;neural net architecture;road vehicles","spatial pyramid deep learning;intelligent transportation systems;natural scenes;deep-learning-based algorithm;automatic vehicle color recognition;spatial pyramid strategy;convolutional neural network architecture;recognition accuracy","","28","52","","","","","IEEE","IEEE Journals"
"Deep learning in acoustic modeling for Automatic Speech Recognition and Understanding - an overview -","I. Gavat; D. Militaru","Department of Electronics, Telecommunications and Information Technology, University POLITEHNICA, Bucharest, Romania; Department of Electronics, Telecommunications and Information Technology, University POLITEHNICA, Bucharest, Romania","2015 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)","","2015","","","1","8","This paper will discuss the progress made in Automatic Speech Recognition and Understanding (ASRU) by applying Deep Learning (DL) in the frame of acoustic modeling. After explaining the concept of DL, specific algorithms like Restricted Bolzmann Machine (RBM), Convolutional Neural Network (CNN), Autoencoder (AE), Deep Belief Network (DBN), will be presented and evaluated. Experiments in the academic research but also in the industry with DL structures concerning Phone Recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) will be highlighted, confirming the usefulness of the DL framework in ASRU. Some considerations about the future of this new and effective machine learning paradigm will conclude the paper.","","","10.1109/SPED.2015.7343074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7343074","ASRU;LVCSR;deep learning;restricted Bolzmann machine;autoencoder;deep belief network;convolutional neural network;continuous speech recognition","Speech recognition;Speech;Hidden Markov models;Acoustics;Neural networks;Feature extraction;Machine learning","acoustic signal processing;belief networks;Boltzmann machines;learning (artificial intelligence);neural nets;speech recognition","deep learning;acoustic modeling;automatic speech recognition and understanding;ASRU;DL;restricted Boltzmann machine;RBM;convolutional neural network;CNN;autoencoder;AE;deep belief network;DBN;phone recognition;large vocabulary continuous speech recognition;LVCSR;machine learning paradigm","","1","33","","","","","IEEE","IEEE Conferences"
"Query-Dependent Aesthetic Model With Deep Learning for Photo Quality Assessment","X. Tian; Z. Dong; K. Yang; T. Mei","CAS Key Laboratory of Technology in Geo-spatial Information Processing and Application System, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Technology in Geo-spatial Information Processing and Application System, University of Science and Technology of China, Hefei, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China","IEEE Transactions on Multimedia","","2015","17","11","2035","2048","The automatic assessment of photo quality from an aesthetic perspective is a very challenging problem. Most existing research has predominantly focused on the learning of a universal aesthetic model based on hand-crafted visual descriptors . However, this research paradigm can achieve only limited success because (1) such hand-crafted descriptors cannot well preserve abstract aesthetic properties , and (2) such a universal model cannot always capture the full diversity of visual content. To address these challenges, we propose in this paper a novel query-dependent aesthetic model with deep learning for photo quality assessment. In our method, deep aesthetic abstractions are discovered from massive images , whereas the aesthetic assessment model is learned in a query- dependent manner. Our work addresses the first problem by learning mid-level aesthetic feature abstractions via powerful deep convolutional neural networks to automatically capture the underlying aesthetic characteristics of the massive training images . Regarding the second problem, because photographers tend to employ different rules of photography for capturing different images , the aesthetic model should also be query- dependent . Specifically, given an image to be assessed, we first identify which aesthetic model should be applied for this particular image. Then, we build a unique aesthetic model of this type to assess its aesthetic quality. We conducted extensive experiments on two large-scale datasets and demonstrated that the proposed query-dependent model equipped with learned deep aesthetic abstractions significantly and consistently outperforms state-of-the-art hand-crafted feature -based and universal model-based methods.","","","10.1109/TMM.2015.2479916","973 project; NSFC; Fundamental Research Funds for the Central Universities; Specialized Research Fund for the Doctoral Program of Higher Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271097","Deep aesthetic visual abstraction;deep learning;quality assessment","Training;Kernel;Visualization;Quality assessment;Feature extraction;Adaptation models;Image color analysis","image capture;image retrieval;learning (artificial intelligence);neural nets","deep-learning;automatic photoquality assessment;universal aesthetic model learning;hand-crafted visual descriptors;abstract aesthetic properties;visual content;query-dependent aesthetic model;deep aesthetic abstractions;massive images;aesthetic assessment model;midlevel aesthetic feature abstractions;deep convolutional neural networks;massive training images;large-scale datasets","","35","38","","","","","IEEE","IEEE Journals"
"Integrative Data Analysis of Multi-Platform Cancer Data with a Multimodal Deep Learning Approach","M. Liang; Z. Li; T. Chen; J. Zeng","Department of Mathematical Sciences, Tsinghua University, Beijing, P. R. China; Drug Discovery Oncology Group, Genomics Institute of the Novartis Research Foundation, 75 John Jay Hopkins Drive, San Diego, CA; Bioinformatics Division, TNLIST and Department of Computer Science and Technology, Tsinghua University, Beijing, P. R. China; Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, P. R. China","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2015","12","4","928","937","Identification of cancer subtypes plays an important role in revealing useful insights into disease pathogenesis and advancing personalized therapy. The recent development of high-throughput sequencing technologies has enabled the rapid collection of multi-platform genomic data (e.g., gene expression, miRNA expression, and DNA methylation) for the same set of tumor samples. Although numerous integrative clustering approaches have been developed to analyze cancer data, few of them are particularly designed to exploit both deep intrinsic statistical properties of each input modality and complex cross-modality correlations among multi-platform input data. In this paper, we propose a new machine learning model, called multimodal deep belief network (DBN), to cluster cancer patients from multi-platform observation data. In our integrative clustering framework, relationships among inherent features of each single modality are first encoded into multiple layers of hidden variables, and then a joint latent model is employed to fuse common features derived from multiple input modalities. A practical learning algorithm, called contrastive divergence (CD), is applied to infer the parameters of our multimodal DBN model in an unsupervised manner. Tests on two available cancer datasets show that our integrative data analysis approach can effectively extract a unified representation of latent features to capture both intra- and cross-modality correlations, and identify meaningful disease subtypes from multi-platform cancer data. In addition, our approach can identify key genes and miRNAs that may play distinct roles in the pathogenesis of different cancer subtypes. Among those key miRNAs, we found that the expression level of miR-29a is highly correlated with survival time in ovarian cancer patients. These results indicate that our multimodal DBN based data analysis approach may have practical applications in cancer pathogenesis studies and provide useful guidelines for personalized cancer therapy.","","","10.1109/TCBB.2014.2377729","National Basic Research Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977954","Multi-platform cancer data analysis;restricted Boltzmann machine;multimodal deep belief network;identification of cancer subtypes;genomic data;clinical data;Multi-platform cancer data analysis;restricted Boltzmann machine;multimodal deep belief network;identification of cancer subtypes;genomic data;clinical data","Cancer;Bioinformatics;Genomics;Data models;Data analysis;Computational biology;DNA","belief networks;cancer;data analysis;feature extraction;genetics;genomics;medical computing;molecular biophysics;pattern clustering;RNA;tumours;unsupervised learning","integrative data analysis;multiplatform cancer data;multimodal deep learning approach;cancer subtype identification;disease pathogenesis;advancing personalized therapy;high-throughput sequencing technologies;multiplatform genomic data;gene expression;miRNA expression;DNA methylation;tumor samples;integrative clustering approaches;cancer data analysis;intrinsic statistical properties;input modality;complex cross-modality correlations;machine learning model;multimodal deep belief network;cancer patient clustering;joint latent model;multiple input modalities;practical learning algorithm;contrastive divergence;multimodal DBN model;unsupervised manner;integrative data analysis approach;latent feature extraction;intramodality correlations;cross-modality correlations;key genes;miR-29a;ovarian cancer patients;multimodal DBN based data analysis;cancer pathogenesis;personalized cancer therapy","Computational Biology;Gene Expression Profiling;Humans;Kaplan-Meier Estimate;Machine Learning;MicroRNAs;Neoplasms","76","30","","","","","IEEE","IEEE Journals"
"Cognition-Based Networks: A New Perspective on Network Optimization Using Learning and Distributed Intelligence","M. Zorzi; A. Zanella; A. Testolin; M. De Filippo De Grazia; M. Zorzi","Department of Information Engineering, University of Padua, Padua, Italy; Department of Information Engineering, University of Padua, Padua, Italy; Department of General Psychology, University of Padua, Padua, Italy; Department of General Psychology, University of Padua, Padua, Italy; Department of General Psychology, University of Padua, Padua, Italy","IEEE Access","","2015","3","","1512","1530","In response to the new challenges in the design and operation of communication networks, and taking inspiration from how living beings deal with complexity and scalability, in this paper we introduce an innovative system concept called COgnition-BAsed NETworkS (COBANETS). The proposed approach develops around the systematic application of advanced machine learning techniques and, in particular, unsupervised deep learning and probabilistic generative models for system-wide learning, modeling, optimization, and data representation. Moreover, in COBANETS, we propose to combine this learning architecture with the emerging network virtualization paradigms, which make it possible to actuate automatic optimization and reconfiguration strategies at the system level, thus fully unleashing the potential of the learning approach. Compared with the past and current research efforts in this area, the technical approach outlined in this paper is deeply interdisciplinary and more comprehensive, calling for the synergic combination of expertise of computer scientists, communications and networking engineers, and cognitive scientists, with the ultimate aim of breaking new ground through a profound rethinking of how the modern understanding of cognition can be used in the management and optimization of telecommunication networks.","","","10.1109/ACCESS.2015.2471178","Fondazione CaRiPaRo through the Project A Novel Approach to Wireless Networking based on Cognitive Science and Distributed Intelligence within the framework “Progetti di Eccellenza 2012.”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7217798","Cognitive networks;deep learning;hierarchical generative models;optimization;Cognitive networks;deep learning;hierarchical generative models;optimization","Communication networks;Cognitive networks;Hierarchical networks;Optimization;Deep learning","cognitive radio;data structures;learning (artificial intelligence);telecommunication computing","cognitive network;reconfiguration strategy;automatic optimization;network virtualization paradigm;system-wide optimization;system-wide modeling;probabilistic generative model;unsupervised deep learning;machine learning technique;network optimization;cognition-based network;system-wide learning;data representation;COBANET","","44","107","","","","","IEEE","IEEE Journals"
"SWAP-NODE: A regularization approach for deep convolutional neural networks","T. Yamashita; M. Tanaka; Y. Yamauchi; H. Fujiyoshi","Chubu University 1200 Matsumoto-cho, Kasugai, Aichi, Japan; Tokyo Institute of Technology 2-12-1 Ookayama, Meguro-ku, Tokyo, Japan; Chubu University 1200 Matsumoto-cho, Kasugai, Aichi, Japan; Chubu University 1200 Matsumoto-cho, Kasugai, Aichi, Japan","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","2475","2479","The regularization is important for training of a deep network. One of breakthrough approach is dropout. It randomly deletes a certain number of activations in each layer in the feed-forward step of the training process. The dropout significantly reduces an effect of over-fitting and improves test performance. We introduce a new regularization approach for deep learning, called the swap-node. The swap-node, which is applied to a fully connected layer, swaps the activation values of two nodes randomly selected with a certain probability. Empirical evaluation shows that the network using the swap-node performs the best on MNIST, CIFAR-10, and SVHN. We also demonstrate superior performance of a combination of the swap-node and dropout on these datasets.","","","10.1109/ICIP.2015.7351247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351247","deep learning;dropout;swap-node;regularization","Training;Error analysis;Training data;Convolution;Neural networks;Machine learning;Computer vision","learning (artificial intelligence);neural nets","SWAP-NODE;regularization approach;deep convolutional neural networks;training;feed-forward step;deep learning;swap-node;activation values;MNIST;CIFAR-10;SVHN","","","12","","","","","IEEE","IEEE Conferences"
"Smile recognition based on deep Auto-Encoders","Shufen Liang; Xiangqun Liang; Min Guo","School of Information Engineering, Wuyi University, Jiangmen, China; School of Information Engineering, Wuyi University, Jiangmen, China; School of Information Engineering, Wuyi University, Jiangmen, China","2015 11th International Conference on Natural Computation (ICNC)","","2015","","","176","181","Most of smile recognition methods are based on constrained databases. Thus there are a lot of limitations when applying those algorithms into the real-world smile recognition. For the purpose of improving the accuracy in real-world smile recognition, we conducted our experiments on two databases (GENKI-4K database and our own built database). Depending on deep learning theory, we constructed a new deep model by stacking Contractive Auto-Encoder (CAE) on Contractive Denoising Auto-Encoder (CDAE) to extract useful features. Firstly, we pre-trained a CDAE to extract the feature of the first layer, then the extracted feature were used as input of the next basic model CAE, by pre-training the CAE model, we got more abstract feature, then the feature were used to classification. Experiments showed that our approach was useful for smile recognition. On the other hand, we also explored the influence of different number of training samples.","","","10.1109/ICNC.2015.7377986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7377986","Smile Recogniton;Deep Learning Algorithms;Contractive Auto-Encoder;Contactive Denoising Auto-Encoder","Databases;Feature extraction;Training;Computer aided engineering;Robustness;Error analysis;Machine learning","emotion recognition;face recognition;feature extraction;image classification;image coding;image denoising;learning (artificial intelligence);visual databases","smile recognition methods;constrained databases;GENKI-4K database;deep learning theory;contractive autoencoder;stacking;contractive denoising autoencoder;feature extraction;CDAE;CAE model;pretraining;classification;training samples;deep autoencoders","","","15","","","","","IEEE","IEEE Conferences"
"Spectral–Spatial Classification of Hyperspectral Data Based on Deep Belief Network","Y. Chen; X. Zhao; X. Jia","Department of Information Engineering, School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Engineering and Information Technology, The University of New South Wales, Sydney, NSW, Australia","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2015","8","6","2381","2392","Hyperspectral data classification is a hot topic in remote sensing community. In recent years, significant effort has been focused on this issue. However, most of the methods extract the features of original data in a shallow manner. In this paper, we introduce a deep learning approach into hyperspectral image classification. A new feature extraction (FE) and image classification framework are proposed for hyperspectral data analysis based on deep belief network (DBN). First, we verify the eligibility of restricted Boltzmann machine (RBM) and DBN by the following spectral information-based classification. Then, we propose a novel deep architecture, which combines the spectral-spatial FE and classification together to get high classification accuracy. The framework is a hybrid of principal component analysis (PCA), hierarchical learning-based FE, and logistic regression (LR). Experimental results with hyperspectral data indicate that the classifier provide competitive solution with the state-of-the-art methods. In addition, this paper reveals that deep learning system has huge potential for hyperspectral data classification.","","","10.1109/JSTARS.2015.2388577","Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018910","Deep belief network (DBN);deep learning;feature extraction (FE);hyperspectral data classification;logistic regression (LR);restricted Boltzmann machine (RBM);support vector machine (SVM);Deep belief network (DBN);deep learning;feature extraction (FE);hyperspectral data classification;logistic regression (LR);restricted Boltzmann machine (RBM);support vector machine (SVM)","Hyperspectral imaging;Feature extraction;Iron;Training;Vectors;Support vector machines","belief networks;Boltzmann machines;data analysis;feature extraction;hyperspectral imaging;image classification;learning (artificial intelligence);principal component analysis;regression analysis;remote sensing","spectral-spatial hyperspectral data classification;deep belief network;DBN;remote sensing;feature extraction;deep learning approach;hyperspectral image classification;hyperspectral data analysis;restricted Boltzmann machine;RBM;spectral information-based classification;spectral-spatial FE;principal component analysis;PCA;hierarchical learning-based FE;logistic regression;LR","","347","54","","","","","IEEE","IEEE Journals"
"Boosting compound-protein interaction prediction by deep learning","Kai Tian; Mingyu Shao; Shuigeng Zhou; Jihong Guan","Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, 200433, China; Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, 200433, China; Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, 200433, China; Department of Computer Science and Technology, Tongji University, Shanghai 201804, China","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","29","34","The identification of interactions between compounds and proteins plays an important role in network pharmacology and drug discovery. However, experimentally identifying compound-protein interactions (CPIs) is generally expensive and time-consuming, computational approaches are thus introduced. Among these, machine-learning based methods have achieved a considerable success. However, due to the nonlinear and imbalanced nature of biological data, many machine learning approaches have their own limitations. Recently, deep learning techniques show advantages over many state-of-the-art machine learning methods in many applications. In this study, we aim at improving the performance of CPI prediction based on deep learning, and propose a method called DL-CPI (the abbreviation of Deep Learning for Compound-Protein Interactions prediction), which employs deep neural network (DNN) to effectively learn the representations of compound-protein pairs. Extensive experiments show that DL-CPI can learn useful features of compound-protein pairs by a layerwise abstraction, and thus achieves better prediction performance than existing methods on both balanced and imbalanced datasets.","","","10.1109/BIBM.2015.7359651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359651","Compound-protein interaction;Deep learning;Deep neural network (DNN)","Radio frequency;Genomics;Bioinformatics;Training","biology computing;drugs;health care;learning (artificial intelligence);neural nets;proteins","compound-protein interaction prediction;network pharmacology;drug discovery;machine-learning;deep learning techniques;DL-CPI;deep neural network;layerwise abstraction","","2","22","","","","","IEEE","IEEE Conferences"
"Deep Convolutional Neural Networks for Multi-instance Multi-task Learning","T. Zeng; S. Ji","Sch. of Electr. Eng. & Comput. Sci., Washington State Univ., Pullman, WA, USA; Sch. of Electr. Eng. & Comput. Sci., Washington State Univ., Pullman, WA, USA","2015 IEEE International Conference on Data Mining","","2015","","","579","588","Multi-instance learning studies problems in which labels are assigned to bags that contain multiple instances. In these settings, the relations between instances and labels are usually ambiguous. In contrast, multi-task learning focuses on the output space in which an input sample is associated with multiple labels. In real world, a sample may be associated with multiple labels that are derived from observing multiple aspects of the problem. Thus many real world applications are naturally formulated as multi-instance multi-task (MIMT) problems. A common approach to MIMT is to solve it task-by-task independently under the multi-instance learning framework. On the other hand, convolutional neural networks (CNN) have demonstrated promising performance in single-instance single-label image classification tasks. However, how CNN deals with multi-instance multi-label tasks still remains an open problem. This is mainly due to the complex multiple-to-multiple relations between the input and output space. In this work, we propose a deep leaning model, known as multi-instance multi-task convolutional neural networks (MIMT-CNN), where a number of images representing a multi-task problem is taken as the inputs. Then a shared sub-CNN is connected with each input image to form instance representations. Those sub-CNN outputs are subsequently aggregated as inputs to additional convolutional layers and full connection layers to produce the ultimate multi-label predictions. This CNN model, through transfer learning from other domains, enables transfer of prior knowledge at image level learned from large single-label single-task data sets. The bag level representations in this model are hierarchically abstracted by multiple layers from instance level representations. Experimental results on mouse brain gene expression pattern annotation data show that the proposed MIMT-CNN model achieves superior performance.","","","10.1109/ICDM.2015.92","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373362","Deep learning;multi-instance learning;multi-task learning;transfer learning;bioinformatics","Brain models;Gene expression;Data models;Biological system modeling;Standards","image classification;image representation;learning (artificial intelligence);neural nets","deep convolutional neural networks;multiinstance multitask learning;single-instance single-label image classification tasks;multiinstance multilabel tasks;deep leaning model;multiinstance multitask convolutional neural networks;MIMT-CNN problem;complex multiple-to-multiple relations;image representation;ultimate multilabel predictions;transfer learning;single-label single-task data sets;bag level representations;mouse brain gene expression pattern annotation data","","17","30","","","","","IEEE","IEEE Conferences"
"Study on Deep Learning and Its Application in Visual Tracking","D. Hu; X. Zhou; X. Yu; Z. Hou","Sch. of Comput. Sci., Northwestern Polytech. Univ., Xi'an, China; Sch. of Comput. Sci., Northwestern Polytech. Univ., Xi'an, China; Gen. Res. Inst., Equip. Acad. of Air Force, Beijing, China; Inf. & Navig. Coll., Air Force Eng. Univ., Xi'an, China","2015 10th International Conference on Broadband and Wireless Computing, Communication and Applications (BWCCA)","","2015","","","240","246","Inspired by recent advances in deep learning, this paper reviews the deep learning methodologies and its applications in object tracking. To overcome the complexity and low-efficiency of existing full-connected deep learning based tracker, we use a novel convolutional deep belief network (CDBN) with convolution, weights sharing and pooling to have much fewer parameters, in addition to gain translation invariance which would benefit the tracker performance. Empirical evaluation demonstrates our CDBN based tracker outperforms several state-of-the-art methods on an open tracker benchmark.","","","10.1109/BWCCA.2015.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424831","deep learning;object tracking;convolutional deep belief network","Machine learning;Visualization;Convolution;Feature extraction;Kernel;Training;Neural networks","belief networks;convolution;learning (artificial intelligence);object tracking","visual tracking;deep learning methodologies;convolutional deep belief network;CDBN;weights sharing;pooling;gain translation invariance;open tracker benchmark;object tracking","","1","21","","","","","IEEE","IEEE Conferences"
"A Deep Learning Network Approach to ab initio Protein Secondary Structure Prediction","M. Spencer; J. Eickholt; J. Cheng","Informatics Institute, University of Missouri, Columbia, MO, USA; Department of Computer Science, Central Michigan University, Mount Pleasant, MI, USA; Department of Computer Science, University of Missouri, Columbia, MO, USA","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2015","12","1","103","112","Ab initio protein secondary structure (SS) predictions are utilized to generate tertiary structure predictions, which are increasingly demanded due to the rapid discovery of proteins. Although recent developments have slightly exceeded previous methods of SS prediction, accuracy has stagnated around 80 percent and many wonder if prediction cannot be advanced beyond this ceiling. Disciplines that have traditionally employed neural networks are experimenting with novel deep learning techniques in attempts to stimulate progress. Since neural networks have historically played an important role in SS prediction, we wanted to determine whether deep learning could contribute to the advancement of this field as well. We developed an SS predictor that makes use of the position-specific scoring matrix generated by PSI-BLAST and deep learning network architectures, which we call DNSS. Graphical processing units and CUDA software optimize the deep network architecture and efficiently train the deep networks. Optimal parameters for the training process were determined, and a workflow comprising three separately trained deep networks was constructed in order to make refined predictions. This deep learning network approach was used to predict SS for a fully independent test dataset of 198 proteins, achieving a Q3 accuracy of 80.7 percent and a Sov accuracy of 74.2 percent.","","","10.1109/TCBB.2014.2343960","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6872810","Machine learning;neural nets;protein structure prediction;deep learning","Proteins;Accuracy;Training;Neural networks;Testing;Bioinformatics;Computational biology","ab initio calculations;biology computing;graphics processing units;learning (artificial intelligence);molecular biophysics;molecular configurations;neural nets;parallel architectures;proteins","deep learning network approach;ab initio protein secondary structure prediction;tertiary structure;SS prediction;neural networks;position-specific scoring matrix;PSI-BLAST;DNSS;graphical processing units;CUDA software","Databases, Protein;Machine Learning;Neural Networks (Computer);Protein Structure, Secondary;Proteins","115","52","","","","","IEEE","IEEE Journals"
"A Novel Sparse Representation Classification Face Recognition Based on Deep Learning","J. Zeng; Y. Zhai; J. Gan","Sch. of Inf. Eng., Wuyi Univ., Jiangmen, China; Sch. of Inf. Eng., Wuyi Univ., Jiangmen, China; Sch. of Inf. Eng., Wuyi Univ., Jiangmen, China","2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)","","2015","","","1520","1523","The existing face recognition under pose and illumination variations is a challenging problem. A novel sparse recognition face recognition algorithm based on deep learning is presented in this paper. The deep learning network extracted global and local information, the deep learning network adopted the supervised Convolution restricted Boltzmann machine. The features extracted could recover the face image and reduce intraidentity variances, while maintaining discriminativeness between identities. The algorithm obtained the feature by the deep network and realized fast sparse classification by smoothed l0 norm. Experimental results on FERET face database show that the proposed algorithm can improve recognition rate and recognition speed when dealing with various conditions such as pose variation.","","","10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518453","face recognition; deep learning; feature extraction;","Feature extraction;Face;Face recognition;Machine learning;Classification algorithms;Lighting;Image recognition","face recognition;image representation;learning (artificial intelligence);pose estimation","novel sparse representation classification face recognition;deep learning;illumination variations;pose variations;deep learning network;Boltzmann machine;face image;deep network;sparse classification;FERET face database;pose variation","","","13","","","","","IEEE","IEEE Conferences"
"DeepSign: Deep learning for automatic malware signature generation and classification","O. E. David; N. S. Netanyahu","Dept. of Computer Science, Bar-Ilan University, Ramat-Gan 52900, Israel; Dept. of Computer Science, Bar-Ilan University, Ramat-Gan 52900, Israel","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","This paper presents a novel deep learning based method for automatic malware signature generation and classification. The method uses a deep belief network (DBN), implemented with a deep stack of denoising autoencoders, generating an invariant compact representation of the malware behavior. While conventional signature and token based methods for malware detection do not detect a majority of new variants for existing malware, the results presented in this paper show that signatures generated by the DBN allow for an accurate classification of new malware variants. Using a dataset containing hundreds of variants for several major malware families, our method achieves 98.6% classification accuracy using the signatures generated by the DBN. The presented method is completely agnostic to the type of malware behavior that is logged (e.g., API calls and their parameters, registry entries, websites and ports accessed, etc.), and can use any raw input from a sandbox to successfully train the deep neural network which is used to generate malware signatures.","","","10.1109/IJCNN.2015.7280815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280815","Deep Learning;Deep Belief Network;Autoencoders;Malware;Automatic Signature Generation","Training;Malware","belief networks;invasive software;learning (artificial intelligence);neural nets;pattern classification","automatic malware signature generation;automatic malware signature classification;deep learning;deep belief network;deep stack denoising autoencoders;malware behavior invariant compact representation;DBN;malware families;deep neural network;deep unsupervised neural network;DeepSign","","43","30","","","","","IEEE","IEEE Conferences"
"A new technique for restricted Boltzmann machine learning","V. Golovko; A. Kroshchanka; V. Turchenko; S. Jankowski; D. Treadwell","Brest State Technical University, Moskowskaja 267, Brest, 224017, Belarus; Brest State Technical University, Moskowskaja 267, Brest, 224017, Belarus; University of Lethbridge, University Drive 4401, Lethbridge, AB, T1K 3M4, Canada; Warsaw University of Technology, Nowowiejska 15/19, Warsaw, 00-665, Poland; Beckon, 107 South B Street, Suite 300, San Mateo, CA, 94401, USA","2015 IEEE 8th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)","","2015","1","","182","186","Over the last decade, deep belief neural networks have been a hot topic in machine learning. Such networks can perform a deep hierarchical representation of input data. The first layer can extract low-level features, the second layer can extract high-level features and so on. In general, deep belief neural network represents many-layered perceptron and permits to overcome some limitations of conventional multilayer perceptron due to deep architecture. In this work we propose a new training technique called Reconstruction Error-Based Approach (REBA) for deep belief neural network based on restricted Boltzmann machine. In contrast to classical Hinton's training approach, which is based on a linear training rule, the proposed technique is based on a nonlinear learning rule. We demonstrate the performance of REBA technique for the MNIST dataset visualization. The main contribution of this paper is a novel view on the training of a restricted Boltzmann machine.","","","10.1109/IDAACS.2015.7340725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340725","Restricted Boltzmann machine;deep learning;data visualization;machine learning","Training;Feature extraction;Mathematical model;Data visualization;Biological neural networks;Mean square error methods","belief networks;Boltzmann machines;data visualisation;feature extraction;learning (artificial intelligence);multilayer perceptrons","restricted Boltzmann machine learning;deep belief neural networks;deep hierarchical representation;low-level feature extraction;high-level feature extraction;many-layered perceptron;multilayer perceptron;deep architecture;reconstruction error-based approach;REBA;Hinton training approach;nonlinear learning rule;MNIST dataset visualization","","1","9","","","","","IEEE","IEEE Conferences"
"When underwater imagery analysis meets deep learning: A solution at the age of big visual data","H. Qin; X. Li; Zhixiong Yang; M. Shang","Graduate School at Shenzhen, Tsinghua University, China; Graduate School at Shenzhen, Tsinghua University, China; Graduate School at Shenzhen, Tsinghua University, China; Graduate School at Shenzhen, Tsinghua University, China","OCEANS 2015 - MTS/IEEE Washington","","2015","","","1","5","Underwater imagery processing is in great demand, while the research is far from enough. The unrestricted natural environment makes it a challenging task. On the other hand, prior to the advent of cabled observatories, the majority of deep-sea video data was acquired by remotely operated vehicles (ROVs), and was analyzed and annotated manually. In contrast, seafloor cabled observatories such as the NEPTUNE and VENUS observatories offer a 24/7 presence, resulting in unprecedented volumes of visual data. The analysis of underwater imagery imposes a series of unique challenges, which need to be tackled by the computer vision community in collaboration with biologists and ocean scientists. In this paper, we introduce how deep learning, the state-of-the-art machine learning technique, can benefit underwater imagery understanding at the age of big data.","","","10.23919/OCEANS.2015.7404463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404463","","Machine learning;Feature extraction;Computer vision;Kernel;Visualization;Neural networks;Computer architecture","Big Data;learning (artificial intelligence);mobile robots;remotely operated vehicles;robot vision","machine learning technique;ocean scientists;computer vision community;visual data;seafloor cabled observatories;ROV;remotely operated vehicles;deep-sea video;cabled observatories;underwater imagery processing;big visual data;deep learning;underwater imagery analysis","","4","23","","","","","IEEE","IEEE Conferences"
"Automatic Recognition of Books Based on Machine Learning","B. Zhu; L. Yang; X. Wu; T. Guo","NA; NA; NA; NA","2015 3rd International Symposium on Computational and Business Intelligence (ISCBI)","","2015","","","74","78","The content-based image recognition is a research focus in the field of computer vision. Machine learning especially deep learning has a great potential in the field of image recognition. This paper adopts the support vector machine algorithm and deep learning method convolutional neural network to recognize books in the digital image library and compares their performance. Experiments show that both methods used in this paper realize a fast and efficient image classification and help improve the intelligence of books retrieval.","","","10.1109/ISCBI.2015.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383540","image recognition;recognition of books;support vector machine;deep learning","Image recognition;Training;Support vector machines;Libraries;Image resolution;Machine learning;Feature extraction","content-based retrieval;digital libraries;electronic publishing;image classification;learning (artificial intelligence);neural nets;support vector machines","automatic recognition;books;machine learning;content-based image recognition;computer vision;support vector machine algorithm;deep learning method;convolutional neural network;digital image library;image classification","","1","9","","","","","IEEE","IEEE Conferences"
"Terminal Replacement Prediction Based on Deep Belief Networks","Z. Zhao; J. Guo; E. Ding; Z. Zhu; D. Zhao","IOT Perception Mine Res. Center, China Univ. of Min. & Technol., Xuzhou, China; IOT Perception Mine Res. Center, China Univ. of Min. & Technol., Xuzhou, China; IOT Perception Mine Res. Center, China Univ. of Min. & Technol., Xuzhou, China; IOT Perception Mine Res. Center, China Univ. of Min. & Technol., Xuzhou, China; IOT Perception Mine Res. Center, China Univ. of Min. & Technol., Xuzhou, China","2015 International Conference on Network and Information Systems for Computers","","2015","","","255","258","To help telecommunications operators accurately predict the terminal replacement behavior, and improve the success rate of marketing and the accuracy of resources devoting, huge user consumption data are used to build Deep Belief Network. The deep features that characterize the terminal replacement behavior are learned, through which a terminal replacement prediction model is conducted. Experiments are carried out on real data set, and the prediction accuracy is over 82%. It is better than three others models based on 1 Nearest Neighbors, Support Vector Machines and Neural Network. The experiments results show that the features obtained by deep learning are more descriptive for predicting terminal replacement behavior.","","","10.1109/ICNISC.2015.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7311880","terminal replacement;deep belief networks;feature learning;deep learning","Training;Data models;Predictive models;Prediction algorithms;Classification algorithms;Neurons;Machine learning","belief networks;neural nets;pattern classification;support vector machines","terminal replacement prediction;deep belief network;terminal replacement behavior;terminal replacement prediction model;1-nearest neighbor;support vector machine;neural network;deep learning","","1","11","","","","","IEEE","IEEE Conferences"
"Traffic Flow Prediction With Big Data: A Deep Learning Approach","Y. Lv; Y. Duan; W. Kang; Z. Li; F. Wang","State Key Lab. of Manage. & Control for Complex Syst., Inst. of Autom., Beijing, China; State Key Lab. of Manage. & Control for Complex Syst., Inst. of Autom., Beijing, China; State Key Lab. of Manage. & Control for Complex Syst., Inst. of Autom., Beijing, China; North China Univ. of Technol., Beijing, China; State Key Lab. of Manage. & Control for Complex Syst., Inst. of Autom., Beijing, China","IEEE Transactions on Intelligent Transportation Systems","","2015","16","2","865","873","Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.","","","10.1109/TITS.2014.2345663","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6894591","Deep learning;stacked autoencoders (SAEs);traffic flow prediction;Deep learning;stacked autoencoders (SAEs);traffic flow prediction","Predictive models;Autoregressive processes;Training;Biological system modeling;Adaptation models;Traffic control;Artificial neural networks","Big Data;intelligent transportation systems;learning (artificial intelligence);traffic engineering computing","traffic flow prediction method;Big Data;deep learning approach;traffic flow information;intelligent transportation system;spatial correlation;temporal correlation;stacked autoencoder model;deep architecture model","","223","61","","","","","IEEE","IEEE Journals"
"Learning structure in gene expression data using deep architectures, with an application to gene clustering","A. Gupta; H. Wang; M. Ganapathiraju","Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, USA; Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, USA; Department of Biomedical Informatics, University of Pittsburgh, USA","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","1328","1335","Genes play a central role in all biological processes. DNA microarray technology has made it possible to study the expression behavior of thousands of genes in one go. Often, gene expression data is used to generate features for supervised and unsupervised learning tasks. At the same time, advances in the field of deep learning have made available a plethora of architectures. In this paper, we use deep architectures pre-trained in an unsupervised manner using denoising autoencoders as a preprocessing step for a popular unsupervised learning task. Denoising autoencoders (DA) can be used to learn a compact representation of input, and have been used to generate features for further supervised learning tasks. We propose that our deep architectures can be treated as empirical versions of Deep Belief Networks (DBNs). We use our deep architectures to regenerate gene expression time series data for two different data sets. We test our hypothesis on two popular datasets for the unsupervised learning task of clustering and find promising improvements in performance.","","","10.1109/BIBM.2015.7359871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359871","gene expression;autoencoders;deep learning;gene clustering","Noise reduction","genetics;learning (artificial intelligence);neural nets;pattern clustering","learning structure;deep architectures;gene clustering;biological processes;DNA microarray technology;deep learning;denoising autoencoders;learning tasks;Deep Belief Networks;gene expression time series data","","13","20","","","","","IEEE","IEEE Conferences"
"Deep learning for credit card data analysis","A. Niimi","Faculty of Systems Information Science, Future University Hakodate, 2-116 Kamedanakano, Hakodate, Hokkaido 041-8655, Japan","2015 World Congress on Internet Security (WorldCIS)","","2015","","","73","77","In this paper, two major applications are introduced to develop advanced deep learning methods for credit-card data analysis. The proposed methods are validated using benchmark experiments with other machine learnings. The experiments confirm that deep learning exhibits similar accuracy to the Gaussian kernel SVM.","","","10.1109/WorldCIS.2015.7359417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359417","","Machine learning;Credit cards;Machine learning algorithms;Sparks;Libraries;Data analysis;Data mining","credit transactions;data analysis;financial data processing;learning (artificial intelligence);support vector machines","credit card data analysis;advanced deep learning methods;machine learnings;Gaussian kernel SVM","","4","17","","","","","IEEE","IEEE Conferences"
"A Continuous Learning Framework for Activity Recognition Using Deep Hybrid Feature Models","M. Hasan; A. K. Roy-Chowdhury","Department of CSE, University of California, Riverside, CA, USA; Department of ECE, University of California, Riverside, CA, USA","IEEE Transactions on Multimedia","","2015","17","11","1909","1922","Most of the research on human activity recognition has focused on learning a static model, considering that all the training instances are labeled and present in advance, while in streaming videos new instances continuously arrive and are not labeled. Moreover, these methods generally use application- specific hand-engineered and static feature models, which are not suitable for continuous learning. Some recent approaches on activity recognition use deep-learning-based hierarchical feature models, but the large size of these networks constrain them from being used in continuous learning scenarios. In this work, we propose a continuous activity learning framework for streaming videos by intricately tying together deep hybrid feature models and active learning. This allows us to automatically select the most suitable features and take the advantage of incoming unlabeled instances to improve the existing model incrementally. Given the segmented activities from streaming videos, we learn features in an unsupervised manner using deep hybrid networks, which have the ability to take the advantage of both the local hand-engineered features and the deep model in an efficient way. Additionally, we use active learning to train the activity classifier using a reduced amount of manually labeled instances. Retraining the models with a huge amount of accumulated examples is computationally expensive and not suitable for continuous learning. Hence, we propose a method to select the best subset of these examples to update the models incrementally. We conduct rigorous experiments on four challenging human activity datasets to demonstrate the effectiveness of our framework.","","","10.1109/TMM.2015.2477242","NSF; ONR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7244231","Active learning;activity recognition;autoencoder;hybrid feature model;incremental learning","Training;Videos;Computational modeling;Machine learning;Feature extraction;Data models;Labeling","image classification;image motion analysis;learning (artificial intelligence);video streaming","activity recognition;deep hybrid feature model;videos streaming;static feature model;deep-learning-based hierarchical feature model;continuous activity learning framework;local hand-engineered feature;activity classifier","","28","62","","","","","IEEE","IEEE Journals"
"Application of deep learning to polarimetric SAR classification","C. Liu; J. Yin; J. Yang","Dept. Electronic Engineering, Tsinghua University, Beijing 100084, China; Dept. Electronic Engineering, Tsinghua University, Beijing 100084, China; Dept. Electronic Engineering, Tsinghua University, Beijing 100084, China","IET International Radar Conference 2015","","2015","","","1","4","It is difficult to extract effective scattering features or describe data using simple statistical distribution for classification of polarimetric SAR. Deep learning is effective in generating complex data model since it use network graph to model data. In this paper, a classification scheme is proposed based on deep learning algorithm, and a hierarchical structure is used to classify data based on the deep learning classifier and polarimetric feature selection when there are a variety of land covers. The classification scheme is accessed using polarimetric SAR images acquired by RADARSAT-2 sensor over Singapore-area. Experiment results demonstrate the effectiveness of the proposed classification method.","","","10.1049/cp.2015.1182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455404","classification;polarimetric SAR;deep learning","","radar imaging;radar polarimetry;synthetic aperture radar","polarimetric SAR classification;classification scheme;hierarchical structure;deep learning classifier;polarimetric feature selection;land covers;polarimetric SAR images;RADARSAT-2 sensor;Singapore","","","","","","","","IET","IET Conferences"
"Disease Inference from Health-Related Questions via Sparse Deep Learning","L. Nie; M. Wang; L. Zhang; S. Yan; B. Zhang; T. Chua","School of Computing, National University of Singapore, Singapore; Hefei University of Technology, China; School of Computing, National University of Singapore, Singapore; Department of Electrical and computer Engineering, National University of Singapore, Singapore; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computing, National University of Singapore, Singapore","IEEE Transactions on Knowledge and Data Engineering","","2015","27","8","2107","2119","Automatic disease inference is of importance to bridge the gap between what online health seekers with unusual symptoms need and what busy human doctors with biased expertise can offer. However, accurately and efficiently inferring diseases is non-trivial, especially for community-based health services due to the vocabulary gap, incomplete information, correlated medical concepts, and limited high quality training samples. In this paper, we first report a user study on the information needs of health seekers in terms of questions and then select those that ask for possible diseases of their manifested symptoms for further analytic. We next propose a novel deep learning scheme to infer the possible diseases given the questions of health seekers. The proposed scheme is comprised of two key components. The first globally mines the discriminant medical signatures from raw features. The second deems the raw features and their signatures as input nodes in one layer and hidden nodes in the subsequent layer, respectively. Meanwhile, it learns the inter-relations between these two layers via pre-training with pseudo-labeled data. Following that, the hidden nodes serve as raw features for the more abstract signature mining. With incremental and alternative repeating of these two components, our scheme builds a sparsely connected deep architecture with three hidden layers. Overall, it well fits specific tasks with fine-tuning. Extensive experiments on a real-world dataset labeled by online doctors show the significant performance gains of our scheme.","","","10.1109/TKDE.2015.2399298","NUS-Tsinghua Extreme Search Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029673","Community-based Health Services;Question Answering;Disease Inference;Deep Learning;Community-based health services;question answering;disease inference;deep learning","Diseases;Medical diagnostic imaging;Cancer;Educational institutions;Training","data mining;diseases;health care;inference mechanisms;information needs;learning (artificial intelligence);medical information systems","pseudolabeled data;abstract signature mining;sparsely connected deep architecture;discriminant medical signatures;information needs;community-based health services;online health seekers;automatic disease inference;sparse deep learning;health-related questions","","82","51","","","","","IEEE","IEEE Journals"
"Cross-modal correlation learning with deep convolutional architecture","Y. Hua; H. Tian; A. Cai; P. Shi","Communication University of China; Fujitsu Research & Development Center; Beijing University of Posts and Telecommunications; Communication University of China","2015 Visual Communications and Image Processing (VCIP)","","2015","","","1","4","With the explosive growth of online multi-media data, methodologies of retrieving documents from heterogeneous modalities are indispensable to facilitate information acquisition in real applications. Most of existing research efforts are focused on building correlation learning models on hand-crafted features for visual and textual modalities. However, they lack the ability to capture the meaningful patterns from complicated visual modality, and are not able to identify the true correlation between modalities during feature learning process. In this paper, we propose a novel cross-modal correlation learning method with well-designed deep convolutional network to learn representations from visual modality. A cross-modal correlation layer with a linear projection is added on the top of the network by maximizing semantic consistency with large margin principle. All the parameters are jointly optimized with stochastic gradient descent. With the deep architecture, our model is able to disentangle the complex visual information, and learn the semantically consistent patterns in a layer-by-layer fashion. Experimental results on widely used NUS-WIDE dataset show that our model outperforms state-of-the-art correlation learning methods built on 6 hand-crafted visual features for image-text retrieval.","","","10.1109/VCIP.2015.7457841","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457841","Deep architecture;Convolution;Correlation learning;Large margin;Cross-modal retrieval","Correlation;Visualization;Semantics;Convolution;Analytical models;Learning systems;Training","image retrieval;learning (artificial intelligence);neural nets;text analysis","cross-modal correlation learning method;deep convolutional architecture;document retrieval;information acquisition;visual modality;textual modality;feature learning process;deep convolutional network;cross-modal correlation layer;linear projection;semantic consistency;large margin principle;NUS-WIDE dataset;image-text retrieval;visual features","","1","22","","","","","IEEE","IEEE Conferences"
"Predicting the success of bank telemarketing using deep convolutional neural network","K. Kim; C. Lee; S. Jo; S. Cho","Department of Computer Science, Yonsei University, Seoul, Republic of Korea; Department of Computer Science, Yonsei University, Seoul, Republic of Korea; Department of Computer Science, Yonsei University, Seoul, Republic of Korea; Department of Computer Science, Yonsei University, Seoul, Republic of Korea","2015 7th International Conference of Soft Computing and Pattern Recognition (SoCPaR)","","2015","","","314","317","Recently, exploitations of the financial big data to solve the real world problems have been to the fore. Deep neural networks are one of the famous machine learning classifiers as their automatic feature extractions are useful, and even more, their performance is impressive in practical problems. Deep convolutional neural network, one of the promising deep neural networks, can handle the local relationship between their nodes which can make this model powerful in the area of image and speech recognition. In this paper, we propose the deep convolutional neural network architecture that predicts whether a given customer is proper for bank telemarketing or not. The number of layers, learning rate, initial value of nodes, and other parameters that should be set to construct deep convolutional neural network are analyzed and proposed. To validate the proposed model, we use the bank marketing data of 45,211 phone calls collected during 30 months, and attain 76.70% of accuracy which outperforms other conventional classifiers.","","","10.1109/SOCPAR.2015.7492828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7492828","Deep learning;Convolutional neural network;Bank telemarketing;Fintech;Big data","Neural networks;Feature extraction;Kernel;Convolution;Instruments;Correlation;Support vector machines","banking;Big Data;learning (artificial intelligence);marketing;neural net architecture","bank telemarketing data;financial Big Data;machine learning classifiers;automatic feature extraction;deep neural networks;deep-convolutional neural network architecture;learning rate;node value;layer number;phone calls","","1","10","","","","","IEEE","IEEE Conferences"
"Stretching deep architectures for text recognition","Y. Zheng; Y. Cai; G. Zhong; Y. Chherawala; Y. Shi; J. Dong","Department of Computer Science and Technology, Ocean University of China, 238 Songling Road, Qingdao, China 266100; Department of Computer Science and Technology, Ocean University of China, 238 Songling Road, Qingdao, China 266100; Department of Computer Science and Technology, Ocean University of China, 238 Songling Road, Qingdao, China 266100; Synchromedia Laboratory for Multimedia Communication in Telepresence, École de Technologie Supérieure, Montréal, Québec H3C 1K3, Canada; Department of Computer Science and Technology, Ocean University of China, 238 Songling Road, Qingdao, China 266100; Department of Computer Science and Technology, Ocean University of China, 238 Songling Road, Qingdao, China 266100","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","236","240","In recent years, many deep architectures have been proposed for handwritten text recognition. However, most of the previous deep models need large scale training data and a long training time to obtain good results. In this paper, we propose a novel deep learning method based on “stretching” the projection matrices of stacked feature learning models. We call the proposed method “stretching deep architectures” (or SDA). In the implementation of SDA, stacked feature learning models are first learned layer by layer, and then the stretching technique is applied on the weight matrices between successive layers. As the feature learning models can be efficiently optimized and the stretching results can be easily computed, the training of SDA is very fast and no back propagation is needed. We have tested SDA on handwritten digits recognition, Arabic subword recognition and English letter recognition tasks. Extensive experiments demonstrate that SDA performs not only better than shallow feature learning models, but also state-of-the-art deep learning models.","","","10.1109/ICDAR.2015.7333759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333759","Text recognition;feature learning;deep learning;deep architectures;stretching","Biological system modeling;Principal component analysis;Adaptation models;Weaving;Computational modeling","handwritten character recognition;learning (artificial intelligence);matrix algebra;text detection","deep learning method;projection matrices;stacked feature learning models;stretching deep architectures;SDA;weight matrices;handwritten digits recognition;Arabic subword recognition;English letter recognition","","7","28","","","","","IEEE","IEEE Conferences"
"Multimodal Deep Autoencoder for Human Pose Recovery","C. Hong; J. Yu; J. Wan; D. Tao; M. Wang","College of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science, Hangzhou Dianzi University, Hangzhou, China; Centre for Quantum Computation and Intelligent Systems and the Faculty of Engineering and Information Technology, University of Technology, Sydney, 81 Broadway Street, Ultimo, NSW, Australia; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China","IEEE Transactions on Image Processing","","2015","24","12","5659","5670","Video-based human pose recovery is usually conducted by retrieving relevant poses using image features. In the retrieving process, the mapping between 2D images and 3D poses is assumed to be linear in most of the traditional methods. However, their relationships are inherently non-linear, which limits recovery performance of these methods. In this paper, we propose a novel pose recovery method using non-linear mapping with multi-layered deep neural network. It is based on feature extraction with multimodal fusion and back-propagation deep learning. In multimodal fusion, we construct hypergraph Laplacian with low-rank representation. In this way, we obtain a unified feature description by standard eigen-decomposition of the hypergraph Laplacian matrix. In back-propagation deep learning, we learn a non-linear mapping from 2D images to 3D poses with parameter fine-tuning. The experimental results on three data sets show that the recovery error has been reduced by 20%-25%, which demonstrates the effectiveness of the proposed method.","","","10.1109/TIP.2015.2487860","National Natural Science Foundation of China; National 973 Program of China; Natural Science Foundation of Fujian Province, China; Zhejiang Provincial Natural Science Foundation of China; Hong Kong Scholar Programme; Australian Research Council Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293666","Human pose recovery;deep learning;multi-modal learning;hypergraph, back propagation;Human pose recovery;deep learning;multi-modal learning;hypergraph;back propagation","Three-dimensional displays;Feature extraction;Visualization;Machine learning;Neural networks;Hidden Markov models;Electronic mail","backpropagation;eigenvalues and eigenfunctions;feature extraction;graph theory;image fusion;image representation;matrix decomposition;neural nets;pose estimation;video signal processing","parameter fine tuning;hypergraph Laplacian matrix standard eigen-decomposition;low-rank representation;backpropagation deep learning;multimodal fusion;image feature extraction;multilayered deep neural network;nonlinear mapping;video-based human pose recovery process;pose retrieving process;multimodal deep autoencoder","Artificial Intelligence;Female;Humans;Image Processing, Computer-Assisted;Male;Posture;Video Recording","252","43","","","","","IEEE","IEEE Journals"
"Learning Compact Hash Codes for Multimodal Representations Using Orthogonal Deep Structure","D. Wang; P. Cui; M. Ou; W. Zhu","Tsinghua National Laboratory for Information Science and Technology, the Department of Computer Science and Technology, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology, the Department of Computer Science and Technology, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology, the Department of Computer Science and Technology, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology, the Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Multimedia","","2015","17","9","1404","1416","As large-scale multimodal data are ubiquitous in many real-world applications, learning multimodal representations for efficient retrieval is a fundamental problem. Most existing methods adopt shallow structures to perform multimodal representation learning. Due to a limitation of learning ability of shallow structures, they fail to capture the correlation of multiple modalities. Recently, multimodal deep learning was proposed and had proven its superiority in representing multimodal data due to its high nonlinearity. However, in order to learn compact and accurate representations, how to reduce the redundant information lying in the multimodal representations and incorporate different complexities of different modalities in the deep models is still an open problem. In order to address the aforementioned problem, in this paper we propose a hashing-based orthogonal deep model to learn accurate and compact multimodal representations. The method can better capture the intra-modality and inter-modality correlations to learn accurate representations. Meanwhile, in order to make the representations compact, the hashing-based model can generate compact hash codes and the proposed orthogonal structure can reduce the redundant information lying in the codes by imposing orthogonal regularizer on the weighting matrices. We also theoretically prove that, in this case, the learned codes are guaranteed to be approximately orthogonal. Moreover, considering the different characteristics of different modalities, effective representations can be attained with different number of layers for different modalities. Comprehensive experiments on three real-world datasets demonstrate a substantial gain of our method on retrieval tasks compared with existing algorithms.","","","10.1109/TMM.2015.2455415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7154455","Deep learning;multimodal hashing;similarity search","Correlation;Computer aided engineering;Joints;Redundancy;Complexity theory;Binary codes;Semantics","data structures;information retrieval;learning (artificial intelligence);matrix algebra","compact hash codes learning;multimodal representation learning;orthogonal deep structure;multimodal data;multimodal deep learning;hashing-based orthogonal deep model;intra-modality correlation;inter-modality correlation;orthogonal regularizer;weighting matrix","","27","55","","","","","IEEE","IEEE Journals"
"Analog Neural Circuit with Switched Capacitor and Design of Deep Learning Model","M. Kawaguchi; M. Umeno; N. Ishii","Dept. of Electr. & Electron. Eng., Suzuka Coll., Suzuka, Japan; Dept. of Electron. Eng., Chubu Univ., Kasugai, Japan; Dept. of Inf. Sci., Aichi Inst. of Technol., Toyota, Japan","2015 3rd International Conference on Applied Computing and Information Technology/2nd International Conference on Computational Science and Intelligence","","2015","","","322","327","In the neural network field, many application models have been proposed. Previous analog neural network models were composed of the operational amplifier and fixed resistance. It is difficult to change the connecting weight of network. In this study, we used analog electronic multiple and sample hold circuits. The connecting weights describe the input voltage. It is easy to change the connection coefficient. This model works only on analog electronic circuits. It can finish the learning process in a very short time and this model will enable more flexible learning. However, the structure of this model is only one input and one output network. We improved the number of unit and network layer. Moreover, we suggest the possibility of realization about the hardware implementation of the deep learning model.","","","10.1109/ACIT-CSI.2015.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336082","electronic circuit;neural network;multiple circuit;deep learning","Biological neural networks;Joining processes;Neurons;Integrated circuit modeling;Solid modeling;Biological system modeling;SPICE","analogue circuits;neural nets","analog neural circuit;switched capacitor;deep learning model;neural network field;analog neural network models;operational amplifier;fixed resistance;sample hold circuits;connection coefficient;analog electronic circuits;learning process","","3","15","","","","","IEEE","IEEE Conferences"
"A spectral filtering based deep learning for detection of logo and stamp","A. V. Nandedkar; J. Mukherjee; S. Sural","School of Information Technology, IIT Kharagpur, India; Dept. of Computer Science & Engg., IIT Kharagpur, India; School of Information Technology, IIT Kharagpur, India","2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)","","2015","","","1","4","This paper presents a novel spectral filtering based deep learning algorithm (SFDL) for detecting logos and stamps in a scanned document image. In a document image, textual contents are main source of high spatial frequency components. Accordingly, the high frequency filtering is used to suppress the text symbols. In the next step, segmentation process is used for localizing the candidate regions of interests such as logos and stamps. Preprocessing of these candidate regions is essential before classification. The proposed preprocessing includes steps such as region fusion, resizing and key point based pooling. Finally, the preprocessed candidate regions are classified using deep convolutional neural network. The main advantage of the SFDL is its capability to detect logos without prior information or assumption about their locations in a document. The performance of the proposed SFDL algorithm is evaluated using publicly accessible document image database StaVer. It is observed that SFDL performs satisfactorily for detecting logo and stamp. The precision and recall measures of the proposed SFDL are compared with existing techniques. Experimental results show that recall and precision of logo detection are 86.8%, 97.2%, respectively. Similarly, recall and precision for stamp detection are 85.3% and 94.8%.","","","10.1109/NCVPRIPG.2015.7490053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490053","","Image color analysis;Machine learning;Neural networks;Feature extraction;Training;Image segmentation;Colored noise","convolution;document image processing;feature extraction;image classification;image filtering;image segmentation;learning (artificial intelligence);neural nets;text detection;visual databases","spectral filtering based deep learning algorithm;SFDL algorithm;document image logo detection;document image stamp detection;text symbol suppression;segmentation process;candidate region classification;deep convolutional neural network;document image database","","1","13","","","","","IEEE","IEEE Conferences"
"Toward accelerating deep learning at scale using specialized hardware in the datacenter","K. Ovtcharov; O. Ruwase; J. Kim; J. Fowers; K. Strauss; E. S. Chung","NA; NA; NA; NA; NA; NA","2015 IEEE Hot Chips 27 Symposium (HCS)","","2015","","","1","38","Presents a collection of slides covering the following topics: FPGA; data center; deep learning; cloud specialization tradeoff; and deep convolutional neural network.","","","10.1109/HOTCHIPS.2015.7477459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477459","","Acceleration;Machine learning;Hardware;Field programmable gate arrays;Neural networks;Energy efficiency;Reconfigurable architectures","cloud computing;computer centres;convolution;field programmable gate arrays;learning (artificial intelligence);neural nets","deep convolutional neural network;cloud specialization tradeoff;data center;FPGA;deep learning","","10","","","","","","IEEE","IEEE Conferences"
"Review Sentiment Analysis Based on Deep Learning","Z. Hu; J. Hu; W. Ding; X. Zheng","Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China","2015 IEEE 12th International Conference on e-Business Engineering","","2015","","","87","94","With rapid development of E-commerce platforms, automated review sentiment analysis for commodities becomes a research focus, with main purpose to extract potential information within reviews for decision making of consumers. Traditional methods have made some progress on document level sentiment analysis, but with tremendous increasing of data scale, how to process high dimension of data fast and effectively becomes the largest limitation. In this paper, we import deep neural network which is appropriate for high dimension data analysis, and propose a framework of sentiment analysis based on deep learning. Experiments on different data scale and different domains show that the proposed method can solve high dimensional problem with good performance.","","","10.1109/ICEBE.2015.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349950","deep learning;deep nerual network;sentiment analysis","Feature extraction;Sentiment analysis;Machine learning;Context;Neural networks;Semantics;Algorithm design and analysis","consumer behaviour;data analysis;decision making;electronic commerce;feature extraction;learning (artificial intelligence);neural nets;text analysis","review sentiment analysis;deep learning;deep neural network;e-commerce platform;information extraction;consumer decision making;data analysis","","13","20","","","","","IEEE","IEEE Conferences"
"Object Detection in Optical Remote Sensing Images Based on Weakly Supervised Learning and High-Level Feature Learning","J. Han; D. Zhang; G. Cheng; L. Guo; J. Ren","Sch. of Autom., Northwestern Polytech. Univ., Xi'an, China; Sch. of Autom., Northwestern Polytech. Univ., Xi'an, China; Sch. of Autom., Northwestern Polytech. Univ., Xi'an, China; Sch. of Autom., Northwestern Polytech. Univ., Xi'an, China; Dept. of Electron. & Electr. Eng., Univ. of Strathclyde, Glasgow, UK","IEEE Transactions on Geoscience and Remote Sensing","","2015","53","6","3325","3337","The abundant spatial and contextual information provided by the advanced remote sensing technology has facilitated subsequent automatic interpretation of the optical remote sensing images (RSIs). In this paper, a novel and effective geospatial object detection framework is proposed by combining the weakly supervised learning (WSL) and high-level feature learning. First, deep Boltzmann machine is adopted to infer the spatial and structural information encoded in the low-level and middle-level features to effectively describe objects in optical RSIs. Then, a novel WSL approach is presented to object detection where the training sets require only binary labels indicating whether an image contains the target object or not. Based on the learnt high-level features, it jointly integrates saliency, intraclass compactness, and interclass separability in a Bayesian framework to initialize a set of training examples from weakly labeled images and start iterative learning of the object detector. A novel evaluation criterion is also developed to detect model drift and cease the iterative learning. Comprehensive experiments on three optical RSI data sets have demonstrated the efficacy of the proposed approach in benchmarking with several state-of-the-art supervised-learning-based object detection approaches.","","","10.1109/TGRS.2014.2374218","National Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991537","Bayesian framework;deep Boltzmann machine (DBM);object detection;weakly supervised learning (WSL);Bayesian framework;deep Boltzmann machine (DBM);object detection;weakly supervised learning (WSL)","Feature extraction;Training;Optical imaging;Object detection;Detectors;Optical sensors;Supervised learning","feature extraction;geophysical image processing;remote sensing","state-of-the-art supervised-learning-based object detection;Bayesian framework;intraclass compactness;WSL approach;deep Boltzmann machine;effective geospatial object detection framework;advanced remote sensing technology;contextual information;spatial information;high-level feature learning;weakly supervised learning;optical remote sensing images;object detection","","345","40","","","","","IEEE","IEEE Journals"
"Rating Image Aesthetics Using Deep Learning","X. Lu; Z. Lin; H. Jin; J. Yang; J. Z. Wang","College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA; Adobe Research, Adobe Systems Inc., San Jose, CA, USA; Adobe Research, Adobe Systems Inc., San Jose, CA, USA; Adobe Research, Adobe Systems Inc., San Jose, CA, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA","IEEE Transactions on Multimedia","","2015","17","11","2021","2034","This paper investigates unified feature learning and classifier training approaches for image aesthetics assessment . Existing methods built upon handcrafted or generic image features and developed machine learning and statistical modeling techniques utilizing training examples. We adopt a novel deep neural network approach to allow unified feature learning and classifier training to estimate image aesthetics. In particular, we develop a double-column deep convolutional neural network to support heterogeneous inputs, i.e., global and local views, in order to capture both global and local characteristics of images . In addition, we employ the style and semantic attributes of images to further boost the aesthetics categorization performance . Experimental results show that our approach produces significantly better results than the earlier reported results on the AVA dataset for both the generic image aesthetics and content -based image aesthetics. Moreover, we introduce a 1.5-million image dataset (IAD) for image aesthetics assessment and we further boost the performance on the AVA test set by training the proposed deep neural networks on the IAD dataset.","","","10.1109/TMM.2015.2477040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7243357","Automatic feature learning;deep neural networks;image aesthetics","Neural networks;Training;Visualization;Computer architecture;Image color analysis;Machine learning;Semantics","convolution;image classification;learning (artificial intelligence);neural nets","unified feature learning;classifier training approaches;image aesthetics assessment;statistical modeling techniques;machine learning;novel deep neural network approach;double-column deep convolutional neural network;AVA dataset;content -based image aesthetics;generic image aesthetics;IAD","","72","32","","","","","IEEE","IEEE Journals"
"Probabilistic visual search for masses within mammography images using deep learning","M. G. Ertosun; D. L. Rubin","Department of Radiology, Stanford School of Medicine, CA USA; Departments of Radiology and Medicine (Biomedical Informatics), Stanford School of Medicine, CA USA","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","1310","1315","We developed a deep learning-based visual search system for the task of automated search and localization of masses in whole mammography images. The system consists of two modules: a classification engine and a localization engine. It first classifies mammograms as containing a mass or no mass using a deep learning classifier, and then localizes the mass(es) within the image using a regional probabilistic approach based on a deep learning network. We obtained 85% accuracy for the task of identifying images that contain a mass, and we were able to localize 85% of the masses at an average of 0.9 false positives per image. Our system has the advantages of being able to work with an entire mammography image as input without the need for image segmentation or other pre-processing steps, such as cropping or tiling the image, and it is based on deep learning with unsupervised feature discovery, so it does not require pre-defined and hand-crafted image features.","","","10.1109/BIBM.2015.7359868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359868","Mammography;Visual Search;CAD;Breast Cancer;Deep Learning;Classification;Detection","Breast;Informatics;Engines;Mammography;Visualization","biomedical engineering;learning (artificial intelligence);mammography","deep learning network;deep learning classifier;localization engine;classification engine;mass localization;automated search;deep learning-based visual search system;mammography images","","17","21","","","","","IEEE","IEEE Conferences"
"Large-Margin Multi-Modal Deep Learning for RGB-D Object Recognition","A. Wang; J. Lu; J. Cai; T. Cham; G. Wang","School of Computer Engineering, Nanyang Technological University (NTU), Singapore; Department of Automation, Tsinghua University, Beijing, China; School of Computer Engineering, Nanyang Technological University (NTU), Singapore; School of Computer Engineering, Nanyang Technological University (NTU), Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University (NTU), Singapore","IEEE Transactions on Multimedia","","2015","17","11","1887","1898","Most existing feature learning-based methods for RGB-D object recognition either combine RGB and depth data in an undifferentiated manner from the outset, or learn features from color and depth separately, which do not adequately exploit different characteristics of the two modalities or utilize the shared relationship between the modalities. In this paper, we propose a general CNN-based multi-modal learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, which are then connected with a carefully designed multi-modal layer. This layer is designed to not only discover the most discriminative features for each modality, but is also able to harness the complementary relationship between the two modalities. The results of the multi-modal layer are back-propagated to update parameters of the CNN layers, and the multi-modal feature learning and the back-propagation are iteratively performed until convergence. Experimental results on two widely used RGB-D object datasets show that our method for general multi-modal learning achieves comparable performance to state-of-the-art methods specifically designed for RGB-D data.","","","10.1109/TMM.2015.2476655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7258382","Deep learning;large-margin feature learning;multi-modality;RGB-D object recognition","Image color analysis;Feature extraction;Object recognition;Correlation;Machine learning;Labeling;Neural networks","backpropagation;convergence;convolution;learning (artificial intelligence);neural nets;object recognition","large-margin multimodal deep learning;RGB-D object recognition;feature learning-based methods;RGB data;depth data;CNN layers;backpropagation;convergence;convolutional neural networks","","69","53","","","","","IEEE","IEEE Journals"
"Deep learning for gender recognition","Q. Deng; Y. Xu; J. Wang; K. Sun","Bio-Computing Research Center, Shenzhen Graduate School, Harbin Institute of Technology; Bio-Computing Research Center, Shenzhen Graduate School, Harbin Institute of Technology; School of Electrical and Electronic Engineering, Nanyang Technological University; Shenzhen Sunwin Intelligent Corporation, Shenzhen, China","2015 International Conference on Computers, Communications, and Systems (ICCCS)","","2015","","","206","209","The secondary sex characteristics in faces of people are quite different due to variations of age, sex hormone, race and dress-up style. It is a very challenging work to build a gender recognition model for all kinds of people. This paper proposes to train a gender recognition model based on the deep convolutional network on a complete dataset. Our newly built complete dataset contains as many common variations of face images as possible. Based on this complete dataset, we design a very deep convolutional network as our gender classifier. We achieve an accuracy of 98.67% on the most challenging public database, labelled faces in the wild (LFW) [1]· We collect 10000 images from Internet and build a new dataset - Chinese wild database. Our model achieves the accuracy of 97.51% This indicates our model is robust to racial variation. In the above two experiments, our model achieves the state-of-the-art performances in the wild.","","","10.1109/CCOMS.2015.7562902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7562902","gender recognition;complete dataset;deep convolutional network;adverse variations","Training;Convolution;Databases;Face recognition;Biochemistry;Object recognition;Network architecture","face recognition;image classification;learning (artificial intelligence);neural nets","secondary sex characteristics;deep learning;deep convolutional network;face images;gender classifier;public database;Internet;Chinese wild database;gender recognition model training","","3","15","","","","","IEEE","IEEE Conferences"
"Temporal Pattern and Association Discovery of Diagnosis Codes Using Deep Learning","S. Mehrabi; S. Sohn; D. Li; J. J. Pankratz; T. Therneau; J. L. S. Sauver; H. Liu; M. Palakal","Dept. of Health Sci. Res., Mayo Clinic, Rochester, MN, USA; Dept. of Health Sci. Res., Mayo Clinic, Rochester, MN, USA; Dept. of Health Sci. Res., Mayo Clinic, Rochester, MN, USA; Div. of Inf. Manage. & Analytics, Mayo Clinic, Rochester, MN, USA; Dept. of Health Sci. Res., Mayo Clinic, Rochester, MN, USA; NA; Dept. of Health Sci. Res., Mayo Clinic, Rochester, MN, USA; Sch. of Inf. & Comput., Indiana Univ., Indianapolis, IN, USA","2015 International Conference on Healthcare Informatics","","2015","","","408","416","Longitudinal health records contain data on patients' visits, condition, treatment, and test results representing progression of their health status over time. In poorly understood patient populations, such data are particularly helpful in characterizing disease progression and early detection. In this work we developed a deep learning algorithm for temporal pattern discovery over Rochester Epidemiology Project data. We modeled each patient's records as a matrix of temporal clinical events with ICD9 and HCUP CSS diagnosis codes as rows and years of diagnosis as columns. Patients aged 18 or younger at the time of diagnosis were selected. A deep Boltzmann machine network with three hidden layers was constructed with each patient's diagnosis matrix values as visible nodes. The final weights of the network model were analyzed as the common features among patients' records.","","","10.1109/ICHI.2015.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349719","Deep Learning;Temporal Pattern Discovery;Rochester Epidemiology Project","Cascading style sheets;Machine learning;Medical diagnostic imaging;Diseases;Sociology;Statistics","Boltzmann machines;data mining;electronic health records;learning (artificial intelligence);matrix algebra;patient diagnosis","deep learning;longitudinal health record;disease progression;deep learning algorithm;temporal pattern discovery;temporal association discovery;Rochester Epidemiology Project data;HCUP CSS diagnosis;ICD9 diagnosis code;deep Boltzmann machine network;patient diagnosis matrix value","","14","45","","","","","IEEE","IEEE Conferences"
"Deep learninig of EEG signals for emotion recognition","Y. Gao; H. J. Lee; R. M. Mehmood","Division of Computer Science and Engineering, Chonbuk National University, Jeonju 561-756, Korea; Division of Computer Science and Engineering, Chonbuk National University, Jeonju 561-756, Korea; Division of Computer Science and Engineering, Chonbuk National University, Jeonju 561-756, Korea","2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","","2015","","","1","5","Emotion recognition is an important task for computer to understand the human status in brain computer interface (BCI) systems. It is difficult to perceive the emotion of some disabled people through their facial expression, such as functional autism patient. EEG signal provides us a non-invasive way to recognize the emotion of these disable people through EEG headset electrodes placed on their scalp. In this paper, we propose a deep learning algorithm to simultaneously learn the features and classify the emotions of EEG signals. It differs from the conventional methods as we apply deep learning on the raw signal without explicit hand-crafted feature extraction. Because the EEG signal has subject dependency, it is better to train the emotion model subject-wise, while there is not much epochs available for each subject. Deep learning algorithm provides a solution with a pre-training way using three layers of restricted Boltzmann machines (RBMs). Thus, we can use epochs of all subjects to pre-training the deep network, and use back-propagation to fine tuning the network subject by subject. Experiment results show that our proposed framework achieves better recognition accuracy than conventional algorithms.","","","10.1109/ICMEW.2015.7169796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169796","EEG;Emotion Recognition;Deep learning;RBM","Electroencephalography;Support vector machines;Brain modeling;Feature extraction;Biomedical imaging;Image recognition;Artificial neural networks","behavioural sciences computing;Boltzmann machines;electrodes;electroencephalography;emotion recognition;handicapped aids;learning (artificial intelligence);medical signal processing","EEG signals;emotion recognition;brain computer interface systems;human status;BCI;facial expression;disabled people;functional autism patient;EEG headset electrodes;deep learning algorithm;subject dependency;emotion model;restricted Boltzmann machines;RBM;deep network;backpropagation","","11","21","","","","","IEEE","IEEE Conferences"
"Single Sample Face Recognition via Learning Deep Supervised Autoencoders","S. Gao; Y. Zhang; K. Jia; J. Lu; Y. Zhang","ShanghaiTech University, Shanghai, China; ShanghaiTech University, Shanghai, China; University of Macau, Macau, China; Advanced Digital Sciences Center, Singapore; Zhejiang University, Hangzhou, China","IEEE Transactions on Information Forensics and Security","","2015","10","10","2108","2118","This paper targets learning robust image representation for single training sample per person face recognition. Motivated by the success of deep learning in image representation, we propose a supervised autoencoder, which is a new type of building block for deep architectures. There are two features distinct our supervised autoencoder from standard autoencoder. First, we enforce the faces with variants to be mapped with the canonical face of the person, for example, frontal face with neutral expression and normal illumination; Second, we enforce features corresponding to the same person to be similar. As a result, our supervised autoencoder extracts the features which are robust to variances in illumination, expression, occlusion, and pose, and facilitates the face recognition. We stack such supervised autoencoders to get the deep architecture and use it for extracting features in image representation. Experimental results on the AR, Extended Yale B, CMU-PIE, and Multi-PIE data sets demonstrate that by coupling with the commonly used sparse representation-based classification, our stacked supervised autoencoders-based face representation significantly outperforms the commonly used image representations in single sample per person face recognition, and it achieves higher recognition accuracy compared with other deep learning models, including the deep Lambertian network, in spite of much less training data and without any domain information. Moreover, supervised autoencoder can also be used for face verification, which further demonstrates its effectiveness for face representation.","","","10.1109/TIFS.2015.2446438","Shanghai Pujiang Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7124463","Single training sample per person;Face recognition;Supervised Auto-encoder;Deep architecture;Single training sample per person;face recognition;supervised auto-encoder;deep architecture","Face;Feature extraction;Face recognition;Noise reduction;Training;Robustness;Image representation","face recognition;feature extraction;image classification;image representation;learning (artificial intelligence)","learning deep supervised autoencoders;feature extraction;image representation;sparse representation-based classification;stacked supervised autoencoders-based face representation;single sample per person face recognition;deep learning models;deep Lambertian network","","84","57","","","","","IEEE","IEEE Journals"
"Efficient Multi-training Framework of Image Deep Learning on GPU Cluster","C. R. Chen; G. G. C. Lee; Y. Xia; W. S. Lin; T. Suzumura; C. Lin","Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan","2015 IEEE International Symposium on Multimedia (ISM)","","2015","","","489","494","In this paper, we develop a pipelining schema for image deep learning on GPU cluster to leverage heavy workload of training procedure. In addition, it is usually necessary to train multiple models to obtain a good deep learning model due to the limited a priori knowledge on deep neural network structure. Therefore, adopting parallel and distributed computing appears is an obvious path forward, but the mileage varies depending on how amenable a deep network can be parallelized and the availability of rapid prototyping capabilities with low cost of entry. In this work, we propose a framework to organize the training procedures of multiple deep learning models into a pipeline on a GPU cluster, where each stage is handled by a particular GPU with a partition of the training dataset. Instead of frequently migrating data among the disks, CPUs, and GPUs, our framework only moves partially trained models to reduce bandwidth consumption and to leverage the full computation capability of the cluster. In this paper, we deploy the proposed framework on popular image recognition tasks using deep learning, and the experiments show that the proposed method reduces overall training time up to dozens of hours compared to the baseline method.","","","10.1109/ISM.2015.119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442384","deep learning;GPU cluster;software pipeline","Training;Computational modeling;Data models;Machine learning;Graphics processing units;Load modeling;Pipeline processing","graphics processing units;image recognition;learning (artificial intelligence);neural nets;pipeline processing;software prototyping","image deep learning multitraining framework efficiency;GPU cluster;pipelining schema;multiple model training;deep neural network structure;distributed computing;parallel computing;rapid prototyping capability;training dataset;data frequent migration;disk;CPU;bandwidth consumption reduction;image recognition","","1","22","","","","","IEEE","IEEE Conferences"
"Deep-plant: Plant identification with convolutional neural networks","S. H. Lee; C. S. Chan; P. Wilkin; P. Remagnino","Centre of Image & Signal Processing, Fac. Comp. Sci. & Info. Tech., University of Malaya, Malaysia; Centre of Image & Signal Processing, Fac. Comp. Sci. & Info. Tech., University of Malaya, Malaysia; Dept. Natural Capital & Plant Health, Royal Botanic Gardens, Kew, United Kingdom; Comp. & Info. Sys., Kingston University, United Kingdom","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","452","456","This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England. To gain intuition on the chosen features from the CNN model (opposed to a `black box' solution), a visualisation technique based on the deconvolutional networks (DN) is utilized. It is found that venations of different order have been chosen to uniquely represent each of the plant species. Experimental results using these CNN features with different classifiers show consistency and superiority compared to the state-of-the art solutions which rely on hand-crafted features.","","","10.1109/ICIP.2015.7350839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350839","plant classification;deep learning;feature visualisation","Shape;Support vector machines;Visualization;Machine learning;Training;Yttrium;Failure analysis","biology computing;botany;data visualisation;feature extraction;image classification;image representation;learning (artificial intelligence)","plant identification;convolutional neural network;unsupervised feature representation;Royal Botanic Garden;Kew;England;CNN model;visualisation technique;deconvolutional networks;plant species;deep learning","","70","22","","","","","IEEE","IEEE Conferences"
"A Deep Learning Approach to Human Activity Recognition Based on Single Accelerometer","Y. Chen; Y. Xue","NA; NA","2015 IEEE International Conference on Systems, Man, and Cybernetics","","2015","","","1488","1492","In this paper, we propose an acceleration-based human activity recognition method using popular deep architecture, Convolution Neural Network (CNN). In particular, we construct a CNN model and modify the convolution kernel to adapt the characteristics of tri-axial acceleration signals. Also, for comparison, we use some widely used methods to accomplish the recognition task on the same dataset. The large dataset we constructed consists of 31688 samples from eight typical activities. The experiment results show that the CNN works well, which can reach an average accuracy of 93.8% without any feature extraction methods.","","","10.1109/SMC.2015.263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379395","tri-axial acceleration signal;human activity recognition;deep architecture;convolution kernel","Acceleration;Feature extraction;Convolution;Kernel;Legged locomotion;Training;Discrete cosine transforms","accelerometers;feature extraction;gesture recognition;learning (artificial intelligence);neural nets","deep learning approach;single accelerometer;acceleration-based human activity recognition method;popular deep architecture;convolution neural network;CNN model;convolution kernel;tri-axial acceleration signal;recognition task;feature extraction method","","47","23","","","","","IEEE","IEEE Conferences"
"Deep learning EEG response representation for brain computer interface","L. Jingwei; C. Yin; Z. Weidong","Department of Automation, Shanghai Jiao Tong University, Shanghai 200240, PRC; Department of Automation, Shanghai Jiao Tong University, Shanghai 200240, PRC; Department of Automation, Shanghai Jiao Tong University, Shanghai 200240, PRC","2015 34th Chinese Control Conference (CCC)","","2015","","","3518","3523","In this paper, the multi-scale deep convolutional neural networks are introduced to deal with the representation for imagined motor Electroencephalography (EEG) signals. We propose to learn a set of high-level feature representations through deep learning algorithm, referred to as Deep Motor Features (DeepMF), for brain computer interface (BCI) with imagined motor tasks. As the extracted DeepMF are dissimilar for different tasks and alike for the same tasks, it is convenient to separate the diverse EEG signals for imagined motor tasks apart. Our approach achieves 100% accuracy for 4 classes imagined motor EEG signals classification on Project BCI - EEG motor activity dataset. Moreover, thanks to the highly abstract features DeepMF learned, only 4.125 seconds trials of training data are needed, compared with the conventional BLDA algorithm for 8.75 seconds trials demand to achieve the same accuracy, accordingly the BCI response time and the required trials for training are almost declined by half. Experiments are provided to illustrate the effectiveness of the proposed design approach.","","","10.1109/ChiCC.2015.7260182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7260182","deep learning;electroencephalography (EEG);brain computer interface (BCI);convolutional neural networks (CNNs)","Electroencephalography;Feature extraction;Convolution;Biological neural networks;Accuracy;Convergence;Brain-computer interfaces","brain-computer interfaces;electroencephalography;learning (artificial intelligence);medical signal processing;neural nets;signal classification;signal representation","deep learning EEG response representation;brain computer interface;multiscale deep convolutional neural network;imagined motor electroencephalography signal;high-level feature representation;deep learning algorithm;deep motor features;DeepMF;EEG signal classification;Project BCI-EEG motor activity dataset;BLDA algorithm;BCI response time","","6","25","","","","","IEEE","IEEE Conferences"
"Implementation of a smartphone wireless accelerometer platform for establishing deep brain stimulation treatment efficacy of essential tremor with machine learning","R. LeMoyne; N. Tomycz; T. Mastroianni; C. McCandless; M. Cozza; D. Peduto","Department of Biological Sciences, Northern Arizona University, Flagstaff, 86011-5640 USA; Neurosurgeon, West Penn Allegheny Health System, Pittsburgh, PA 15212 USA; Independent, Pittsburgh, PA 15243 USA; Behavioral Science, Sentient Decision Science, Portsmouth, NH 03801 USA; Engineering, RE2, Pittsburgh, PA 15201 USA; Special Psychological Applications, Sewickley, PA 15143 USA","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","6772","6775","Essential tremor (ET) is a highly prevalent movement disorder. Patients with ET exhibit a complex progressive and disabling tremor, and medical management often fails. Deep brain stimulation (DBS) has been successfully applied to this disorder, however there has been no quantifiable way to measure tremor severity or treatment efficacy in this patient population. The quantified amelioration of kinetic tremor via DBS is herein demonstrated through the application of a smartphone (iPhone) as a wireless accelerometer platform. The recorded acceleration signal can be obtained at a setting of the subject's convenience and conveyed by wireless transmission through the Internet for post-processing anywhere in the world. Further post-processing of the acceleration signal can be classified through a machine learning application, such as the support vector machine. Preliminary application of deep brain stimulation with a smartphone for acquisition of a feature set and machine learning for classification has been successfully applied. The support vector machine achieved 100% classification between deep brain stimulation in `on' and `off' mode based on the recording of an accelerometer signal through a smartphone as a wireless accelerometer platform.","","","10.1109/EMBC.2015.7319948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319948","Essential Tremor;Deep Brain Stimulation;Smartphone;iPhone;Accelerometer;Wireless Accelerometer;Feedback;Support Vector Machine;Machine Learning","Accelerometers;Wireless communication;Acceleration;Brain stimulation;Support vector machines;Diseases;Wireless sensor networks","accelerometers;diseases;Internet;patient treatment;smart phones","smartphone;wireless accelerometer platform;deep brain stimulation treatment efficacy;essential tremor;machine learning;movement disorder;medical management;tremor severity;iPhone;Internet","Accelerometry;Deep Brain Stimulation;Essential Tremor;Humans;Internet;Machine Learning;Signal Processing, Computer-Assisted;Smartphone;Treatment Outcome","17","20","","","","","IEEE","IEEE Conferences"
"Polarimetric SAR images classification using deep belief networks with learning features","B. Hou; X. Luo; S. Wang; L. Jiao; X. Zhang","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an 710071, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an 710071, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an 710071, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an 710071, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an 710071, P. R. China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","2366","2369","A novel polarimetric synthetic aperture radar (PolSAR) image classification method based on Deep Belief Networks (DBNs) is proposed in this paper. First, the coherency matrix data are converted to a 9-dimentional data. Second, many patches are randomly selected from each dimension in the 9-dimentional data, and many filters can be obtained from a Restricted Boltzmann Machine (RBM) trained by using these patches. Thus we can get the features for each pixel from each dimension in the 9-dimentional space. Finally, the learned features and the elements of coherent matrix are combined to train a 3-layers DBNs for PolSAR image classification. Experimental results show that the proposed method is efficient and effective for PolSAR image classification.","","","10.1109/IGARSS.2015.7326284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326284","Deep Belief Networks;RBM;PolSAR;Image classification","Accuracy;Image classification;Artificial neural networks;Classification algorithms;Synthetic aperture radar;Support vector machines;Yttrium","belief networks;Boltzmann machines;feature extraction;image classification;image filtering;learning (artificial intelligence);matrix algebra;radar computing;radar imaging;radar polarimetry;synthetic aperture radar","polarimetric SAR image classification;deep belief network;learning feature;polarimetric synthetic aperture radar image classification method;coherency matrix data;restricted Boltzmann machine;RBM;efficient PolSAR image classification;PolSAR;DBN;9-dimentional data","","5","8","","","","","IEEE","IEEE Conferences"
"Weakly Supervised Deep Metric Learning for Community-Contributed Image Retrieval","Z. Li; J. Tang","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Multimedia","","2015","17","11","1989","1999","Recent years have witnessed the explosive growth of community-contributed images with rich context information, which is beneficial to the task of image retrieval. It can help us to learn a suitable metric to alleviate the semantic gap. In this paper, we propose a new distance metric learning algorithm, namely weakly-supervised deep metric learning (WDML), under the deep learning framework. It utilizes a progressive learning manner to discover knowledge by jointly exploiting the heterogeneous data structures from visual contents and user-provided tags of social images. The semantic structure in the textual space is expected to be well preserved while the problem of the noisy, incomplete or subjective tags is addressed by leveraging the visual structure in the original visual space. Besides, a sparse model with the l2,1 mixed norm is imposed on the transformation matrix of the first layer in the deep architecture to compress the noisy or redundant visual features. The proposed problem is formulated as an optimization problem with a well-defined objective function and a simple yet efficient iterative algorithm is proposed to solve it. Extensive experiments on real-world social image datasets are conducted to verify the effectiveness of the proposed method for image retrieval. Encouraging experimental results are achieved compared with several representative metric learning methods.","","","10.1109/TMM.2015.2477035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7243340","Deep;image retrieval;metric learning;weakly supervised","Visualization;Semantics;Image retrieval;Noise measurement;Learning systems;Data structures","data mining;image retrieval;iterative methods;learning (artificial intelligence);matrix algebra;optimisation","weakly supervised deep metric learning;community-contributed image retrieval;rich context information;distance metric learning algorithm;WDML;progressive learning manner;knowledge discovery;heterogeneous data structures;transformation matrix;optimization problem;iterative algorithm;real-world social image datasets","","78","39","","","","","IEEE","IEEE Journals"
"A novel pruning model of deep learning for large-scale distributed data processing","Y. Sheng; C. Li; J. Wang; H. Deng; Z. Zhao","National Network New Media Engineering Research Center, Chinese Academy of Sciences, Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China; National Network New Media Engineering Research Center, Chinese Academy of Sciences, Beijing 100190, China; National Network New Media Engineering Research Center, Chinese Academy of Sciences, Beijing 100190, China; University of Science and Technology of China, Anhui 230026, China","2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","","2015","","","314","319","In this paper, we propose a novel pruning model of deep learning for large-scale distributed data processing to simulate a potential application in the geographical neighbor of Internet of Things. We formulate a general model of pruning learning, and we investigate the procedure of pruning learning to satisfy hard constraint and soft constraint. The hard constraint is a class of non-flexible setting without parameter learning to match the structure of distributed data. The soft constraint is a process of adaptive parameter learning to satisfy an inequality without any degradation of accuracy if the size of training data is large enough. Based on the simulation using distributed MNIST image database with large-scale samples, the performance of the proposed pruning model is better than that of a state-of-the-art model of deep learning in case of big data processing.","","","10.1109/APSIPA.2015.7415528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415528","deep learning;big data;distributed data;cloud computing;internet of things","Data models;Cloud computing;Machine learning;Neurons;Big data;Internet of things","Big Data;cloud computing;Internet of Things;learning (artificial intelligence)","pruning model;deep learning;large-scale distributed data processing;geographical neighbor;Internet of Things;pruning learning;hard constraint;soft constraint;nonflexible setting;adaptive parameter learning;distributed MNIST image database;big data processing","","","23","","","","","IEEE","IEEE Conferences"
"Deep learning based large scale handwritten Devanagari character recognition","S. Acharya; A. K. Pant; P. K. Gyawali","Institute Of Engineering, Tribhuvan University, Kathmandu, Nepal; Institute Of Science and Technology, Tribhuvan University, Kathmandu, Nepal; Institute Of Engineering, Tribhuvan University, Kathmandu, Nepal","2015 9th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)","","2015","","","1","6","In this paper, we introduce a new public image dataset for Devanagari script: Devanagari Handwritten Character Dataset (DHCD). Our dataset consists of 92 thousand images of 46 different classes of characters of Devanagari script segmented from handwritten documents. We also explore the challenges in recognition of Devanagari characters. Along with the dataset, we also propose a deep learning architecture for recognition of those characters. Deep Convolutional Neural Network (CNN) have shown superior results to traditional shallow networks in many recognition tasks. Keeping distance with the regular approach of character recognition by Deep CNN, we focus the use of Dropout and dataset increment approach to improve test accuracy. By implementing these techniques in Deep CNN, we were able to increase test accuracy by nearly 1 percent. The proposed architecture scored highest test accuracy of 98.47% on our dataset.","","","10.1109/SKIMA.2015.7400041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7400041","Devanagari Handwritten Character Dataset;Image processing;Computer Vision;Deep learning;Deep Convolutional Neural Network;Optical Character Recognition;Dropout","Training;Character recognition;Testing;Neural networks;Convolution;Kernel","document image processing;handwritten character recognition;learning (artificial intelligence);neural nets;optical character recognition","optical character recognition;dataset increment approach;dropout approach;deep CNN;deep convolutional neural network;deep learning architecture;handwritten documents;Devanagari script;DHCD;Devanagari handwritten character dataset;public image dataset;large scale handwritten Devanagari character recognition","","22","16","","","","","IEEE","IEEE Conferences"
"Deep learning: Architectures, algorithms, applications","R. Memisevic","University of Montreal","2015 IEEE Hot Chips 27 Symposium (HCS)","","2015","","","1","127","This article consists of a collection of slides from the author's conference presentation. Some of the topics covered include: Machine learning 101: Neural nets, backprop, RNNs; Applications; Structured prediction; Unsupervised learning; ""Neural Programs""; Architecture exploration; Towards hardware-friendlier DL; and Software.","","","10.1109/HOTCHIPS.2015.7477319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477319","","Machine learning;Unsupervised learning;Artificial neural networks;Recurrent neural networks;Learning (artificial intelligence)","neural nets;recurrent neural nets;unsupervised learning","hardware-friendlier DL;deep learning applications;deep learning algorithms;neural programs;unsupervised learning;RNN;deep learning architecture","","1","","","","","","IEEE","IEEE Conferences"
"Combining Deep Learning with Information Retrieval to Localize Buggy Files for Bug Reports (N)","A. N. Lam; A. T. Nguyen; H. A. Nguyen; T. N. Nguyen","NA; NA; NA; NA","2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2015","","","476","481","Bug localization refers to the automated process of locating the potential buggy files for a given bug report. To help developers focus their attention to those files is crucial. Several existing automated approaches for bug localization from a bug report face a key challenge, called lexical mismatch, in which the terms used in bug reports to describe a bug are different from the terms and code tokens used in source files. This paper presents a novel approach that uses deep neural network (DNN) in combination with rVSM, an information retrieval (IR) technique. rVSM collects the feature on the textual similarity between bug reports and source files. DNN is used to learn to relate the terms in bug reports to potentially different code tokens and terms in source files and documentation if they appear frequently enough in the pairs of reports and buggy files. Our empirical evaluation on real-world projects shows that DNN and IR complement well to each other to achieve higher bug localization accuracy than individual models. Importantly, our new model, HyLoc, with a combination of the features built from DNN, rVSM, and project's bug-fixing history, achieves higher accuracy than the state-of-the-art IR and machine learning techniques. In half of the cases, it is correct with just a single suggested file. Two out of three cases, a correct buggy file is in the list of three suggested files.","","","10.1109/ASE.2015.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372035","Deep Neural Network;Deep Learning;Bug Localization;Information Retrieval;Bug Reports","Feature extraction;History;Metadata;Computer bugs;Software;Bridges;Information retrieval","information retrieval;learning (artificial intelligence);program debugging;support vector machines","deep learning;buggy file localization;bug reports;lexical mismatch;source files;deep neural network;DNN;rVSM;information retrieval technique;IR technique;code tokens;HyLoc;projec bug-fixing history;machine learning techniques","","28","10","","","","","IEEE","IEEE Conferences"
"A knowledge-in-the-loop approach to integrated safety&security for cooperative system-of-systems","D. Chen; K. Meinke; K. Östberg; F. Asplund; C. Baumann","Mechatronics, Machine Design, ITM, KTH Royal Institute of Technology, SE-100 44 Stockholm, Sweden; Theoretical Computer Science, CSC, KTH Royal Institute of Technology, SE-100 44 Stockholm, Sweden; Electronics / Software, SP Technical Research Institute, SE-501 15 Borås, Sweden; Mechatronics, Machine Design, ITM, KTH Royal Institute of Technology, SE-100 44 Stockholm, Sweden; Theoretical Computer Science, CSC, KTH Royal Institute of Technology, SE-100 44 Stockholm, Sweden","2015 IEEE Seventh International Conference on Intelligent Computing and Information Systems (ICICIS)","","2015","","","13","20","A system-of-systems (SoS) is inherently open in configuration and evolutionary in lifecycle. For the next generation of cooperative cyber-physical system-of-systems, safety and security constitute two key issues of public concern that affect the deployment and acceptance. In engineering, the openness and evolutionary nature also entail radical paradigm shifts. This paper presents one novel approach to the development of qualified cyber-physical system-of-systems, with Cooperative Intelligent Transport Systems (C-ITS) as one target. The approach, referred to as knowledge-in-the-loop, aims to allow a synergy of well-managed lifecycles, formal quality assurance, and smart system features. One research goal is to enable an evolutionary development with continuous and traceable flows of system rationale from design-time to post-deployment time and back, supporting automated knowledge inference and enrichment. Another research goal is to develop a formal approach to risk-aware dynamic treatment of safety and security as a whole in the context of system-of-systems. Key base technologies include: (1) EAST-ADL for the consolidation of system-wide concerns and for the creation of an ontology for advanced run-time decisions, (2) Learning Based-Testing for run-time and post-deployment model inference, safety monitoring and testing, (3) Provable Isolation for run-time attack detection and enforcement of security in real-time operating systems.","","","10.1109/IntelCIS.2015.7397237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7397237","systems-of-systems;cyber-physical system;ontology;knowledge modeling;machine learning;safety;security;model-based development;verification and validation;quality-of-service","Ontologies;Analytical models;Roads;Organizations;Security;System analysis and design;Risk management","cyber-physical systems;evolutionary computation;formal verification;intelligent transportation systems;learning (artificial intelligence);ontologies (artificial intelligence);security of data","knowledge-in-the-loop approach;integrated safety and security;cooperative system-of-systems;cyber-physical system-of-systems;cooperative intelligent transport systems;C-ITS;formal quality assurance;smart system feature;evolutionary development;risk-aware dynamic treatment;EAST-ADL;ontology;learning based-testing;safety monitoring;run-time attack detection","","3","53","","","","","IEEE","IEEE Conferences"
"Building detection in very high resolution multispectral data with deep learning features","M. Vakalopoulou; K. Karantzalos; N. Komodakis; N. Paragios","Remote Sensing Lab., National Technical University of Athens, Athens, Greece; Remote Sensing Lab., National Technical University of Athens, Athens, Greece; Ecole des Ponts ParisTech, Universite Paris Est, France; Center for Visual Computing, Ecole Centrale de Paris, Paris, France","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","1873","1876","The automated man-made object detection and building extraction from single satellite images is, still, one of the most challenging tasks for various urban planning and monitoring engineering applications. To this end, in this paper we propose an automated building detection framework from very high resolution remote sensing data based on deep convolutional neural networks. The core of the developed method is based on a supervised classification procedure employing a very large training dataset. An MRF model is then responsible for obtaining the optimal labels regarding the detection of scene buildings. The experimental results and the performed quantitative validation indicate the quite promising potentials of the developed approach.","","","10.1109/IGARSS.2015.7326158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326158","Machine learning;deep convolutional networks;ImageNet;man made objects;extraction","Buildings;Feature extraction;Training;Satellites;Remote sensing;Support vector machines;Image resolution","buildings (structures);image classification;image resolution;neural nets;object detection;remote sensing;town and country planning","automated man-made object detection;building extraction;satellite images;urban planning applications;urban monitoring engineering applications;automated building detection framework;very high resolution remote sensing data;deep convolutional neural networks;supervised classification procedure;training dataset;MRF model;optimal labels;scene building detection;quantitative validation;very high resolution multispectral data;deep learning features","","59","14","","","","","IEEE","IEEE Conferences"
"Multimedia data mining using deep learning","P. Wlodarczak; J. Soar; M. Ally","Faculty of Business, Education, Law and Arts, University of Southern Queensland, Toowoomba, Australia; Faculty of Business, Education, Law and Arts, University of Southern Queensland, Toowoomba, Australia; Faculty of Business, Education, Law and Arts, University of Southern Queensland, Toowoomba, Australia","2015 Fifth International Conference on Digital Information Processing and Communications (ICDIPC)","","2015","","","190","196","Due to the large amounts of Multimedia data on the Internet, Multimedia mining has become a very active area of research. Multimedia mining is a form of data mining. Data mining uses algorithms to segment data to identify useful patterns and to make predictions. Despite the successes in many areas, data mining remains a challenging task. In the past, multimedia mining was one of the fields where the results were often not satisfactory. Multimedia Data Mining extracts relevant data from multimedia files such as audio, video and still images to perform similarity searches, identify associations, entity resolution and for classification. As the mining techniques have matured, new techniques were developed. A lot of progress has been made in areas such as visual data mining and natural language processing using deep learning techniques. Deep learning is a branch of machine learning and has been used among other on Smartphones for face recognition and voice commands. Deep learners are a type of artificial neural networks with multiple data processing layers that learn representations by increasing the level of abstraction from one layer to the next. These methods have improved the state-of-the-art in multimedia mining, in speech recognition, visual object recognition, natural language processing and other areas such as genome mining and predicting the efficacy of drug molecules. This paper describes some of the deep learning techniques that have been used in recent research for multimedia data mining.","","","10.1109/ICDIPC.2015.7323027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7323027","data mining;multimedia data mining;deep learning;artificial neural networks;natural language processing;visual data mining","Data mining;Multimedia communication;Feature extraction;Machine learning;Training;Visualization;Backpropagation","data mining;information retrieval;Internet;learning (artificial intelligence);multimedia computing;neural nets;pattern classification","multimedia data mining;deep learning;Internet;data segmentation;useful pattern identification;relevant data extraction;multimedia files;audio files;video files;still images;similarity search;association identification;entity resolution;classification;machine learning;artificial neural networks;multiple data processing layers","","12","29","","","","","IEEE","IEEE Conferences"
"Bridge deep learning to the physical world: An efficient method to quantize network","P. Hung; C. Lee; S. Yang; V. S. Somayazulu; Y. Chen; S. Chien","Media IC and System Lab, Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Intel Corporation, USA Intel-NTU Connected Context Computing Center, Taipei, Taiwan; Intel Corporation, USA Intel-NTU Connected Context Computing Center, Taipei, Taiwan; Intel Corporation, USA Intel-NTU Connected Context Computing Center, Taipei, Taiwan; Media IC and System Lab, Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","2015 IEEE Workshop on Signal Processing Systems (SiPS)","","2015","","","1","6","As better performance is achieved by deep convolutional network with more and more layers, the increasing number of weighting and bias parameters makes it only possible to be implemented on servers in cyber space but infeasible to be deployed in physical-world embedded systems because of huge storage and memory bandwidth requirements. In this paper, we proposed an efficient method to quantize the model parameters. Instead of taking the quantization process as a negative effect on precision, we regarded it as a regularize problem to prevent overfitting, and a two-stage quantization technique including soft- and hard-quantization is developed. With the help of our quantization method, not only 93.75% of the parameter memory size can be reduced by replacing the word length from 32-bit to 2-bit, but the testing accuracy after quantization is also better than previous approaches in some dataset, and the additional training overhead is only 3% of the ordinary one.","","","10.1109/SiPS.2015.7345005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345005","Quantize;Regularize;Deep Neural Network;Convolutional Network;Deep Learning","Quantization (signal);Training;Neural networks;Machine learning;Computational modeling;Graphics processing units;Memory management","convolution;embedded systems;image classification;learning (artificial intelligence);quantisation (signal)","deep learning;physical world embedded system;network quantization;deep convolutional network;image classification","","6","27","","","","","IEEE","IEEE Conferences"
"Research on Gas Recognition Based on Stacked Denoising Autoencoders","W. Yu; C. Gan; W. Lu","Sch. of Comput. Sci. & Inf. Eng., Shanghai Inst. of Technol., Shanghai, China; Sch. of Comput. Sci. & Inf. Eng., Shanghai Inst. of Technol., Shanghai, China; Sch. of Comput. Sci. & Inf. Eng., Shanghai Inst. of Technol., Shanghai, China","2015 8th International Symposium on Computational Intelligence and Design (ISCID)","","2015","1","","301","304","The gas data coming from an array of chemical gas sensors is a kind of multivariate time-series. This data set is extremely difficult and complex to interpret for human experts. It needs designing hand-made features when applying traditional shallow machine learning algorithms in gas recognition. A new gas recognition method based on Deep Learning were proposed in this paper. It is one of unsupervised feature learning methods that can extract self-adapting features from the gas data, overcoming the complex process in designing features by hands and making the features more general. In this work, two methods based on UCI Machine learning database respectively were compared in the experiments. One of them is a two-hidden-layer structure of deep neural network-Stacked denoising Autoencoders and another is a kind of shallow machine learning algorithms. The results show that extracting features automaticly using Deep Learning is a simpler and more universal way in gas recognition. The method proposed in this paper not only improves the gas classification accuracy, but also reduces complexity of the process in shallow machine learning alogithms, so it is valuable to be applied in practice.","","","10.1109/ISCID.2015.226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7468955","gas recognition;time-series signal;high-dimensional;Deep Learning;Stacked denoising Autoencoders","Feature extraction;Machine learning;Support vector machines;Noise reduction;Gas detectors;Data mining","computerised instrumentation;feature extraction;gas sensors;neural nets;sensor arrays;time series;unsupervised learning","gas classification;deep neural network;two-hidden-layer structure;UCI machine learning database;self-adapting feature extraction;unsupervised feature learning method;deep learning;gas recognition method;shallow machine learning algorithm;multivariate time-series;chemical gas sensor;stacked denoising autoencoder","","","9","","","","","IEEE","IEEE Conferences"
"Layer-Specific Adaptive Learning Rates for Deep Networks","B. Singh; S. De; Y. Zhang; T. Goldstein; G. Taylor","NA; NA; NA; NA; NA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","364","368","The increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. This slow training is due in part to ""vanishing gradients,"" in which the gradients used by back-propagation are extremely large for weights connecting deep layers (layers near the output layer), and extremely small for shallow layers (near the input layer), this results in slow learning in the shallow layers. Additionally, it has also been shown that in highly non-convex problems, such as deep neural networks, there is a proliferation of high-error low curvature saddle points, which slows down learning dramatically [1]. In this paper, we attempt to overcome the two above problems by proposing an optimization method for training deep neural networks which uses learning rates which are both specific to each layer in the network and adaptive to the curvature of the function, increasing the learning rate at low curvature points. This enables us to speed up learning in the shallow layers of the network and quickly escape high-error low curvature saddle points. We test our method on standard image classification datasets such as MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy as well as reduces the required training time over standard algorithms.","","","10.1109/ICMLA.2015.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424337","deep networks;adaptive learing rate","Training;Standards;Adaptive systems;Neural networks;Optimization methods;Training data","backpropagation;concave programming;image classification;neural nets","layer-specific adaptive learning rates;deep networks;deep learning architectures;vanishing gradients;backpropagation;highly nonconvex problems;deep neural networks;high-error low curvature saddle points;optimization method;image classification datasets;MNIST dataset;CIFAR10 dataset;ImageNet dataset","","9","16","","","","","IEEE","IEEE Conferences"
"Development of a UAV-type jellyfish monitoring system using deep learning","H. Kim; D. Kim; Sungwook Jung; Jungmo Koo; J. Shin; H. Myung","Urban Robotics Laboratory (URL), Civil & Environmental Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291 Daehak-ro (373-1 Guseong-dong), Yuseong-gu, Daejeon 34141, Korea; Urban Robotics Laboratory (URL), Robotics Program, Korea Advanced Institute of Science and Technology (KAIST), 291 Daehak-ro (373-1 Guseong-dong), Yuseong-gu, Daejeon 34141, Korea; Urban Robotics Laboratory (URL), Robotics Program, Korea Advanced Institute of Science and Technology (KAIST), 291 Daehak-ro (373-1 Guseong-dong), Yuseong-gu, Daejeon 34141, Korea; Urban Robotics Laboratory (URL), Civil & Environmental Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291 Daehak-ro (373-1 Guseong-dong), Yuseong-gu, Daejeon 34141, Korea; Rastech Inc., 964 Tamnip-dong, Yuseong-gu, Daejeon, 305-510, Korea; Urban Robotics Laboratory (URL), Civil & Environmental Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291 Daehak-ro (373-1 Guseong-dong), Yuseong-gu, Daejeon 34141, Korea","2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","","2015","","","495","497","At present, unmanned aerial vehicles (UAVs) are the primary platforms widely used for environmental monitoring. The advantage of the UAV-type surveillance system is its low-cost with high observation performance. Using this system, we can extend the workable area of the jellyfish removal system. The proposed system observes jellyfish on the surface of the sea while flying, and can recognize a herd of jellyfish using deep learning. The preliminary results of the proposed system show that the proposed system improves the jellyfish removal system for efficient operation.","","","10.1109/URAI.2015.7358813","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358813","Jellyfish removal;monitoring system;unmanned aerial vehicle","Robots;Sea surface;Surveillance;Machine learning;Global Positioning System;Clustering algorithms","autonomous aerial vehicles;environmental management;intelligent robots;learning (artificial intelligence)","jellyfish removal system;UAV-type surveillance system;environmental monitoring;unmanned aerial vehicles;deep learning;UAV-type jellyfish monitoring system","","1","8","","","","","IEEE","IEEE Conferences"
"A Preliminary Study on Deep-Learning Based Screaming Sound Detection","M. Z. Zaheer; J. Y. Kim; H. Kim; S. Y. Na","Dept. of Electron. & Comput. Eng., Chonnam Nat. Univ., Gwangju, South Korea; Dept. of Electron. & Comput. Eng., Chonnam Nat. Univ., Gwangju, South Korea; Dept. of Electron. Convergence Eng., Kwangwoon Univ., Seoul, South Korea; Dept. of Electron. & Comput. Eng., Chonnam Nat. Univ., Gwangju, South Korea","2015 5th International Conference on IT Convergence and Security (ICITCS)","","2015","","","1","4","In addition to the traditional video surveillance, various audio processing techniques can also be added to the existing CCTV cameras. They can be used as additional features to help in analyzing the scene better and autonomously detecting violence or any unwanted activity in the scene. For this purpose, a deep learning based scream sound detection approach is proposed in this paper. MFCC features after interpolation are used as input of the system. The proposed system is experimented using a self-recorded scream database and with controlled and calculated parameters 100 % accuracy is achieved.","","","10.1109/ICITCS.2015.7292925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7292925","","Mel frequency cepstral coefficient;Feature extraction;Training;Accuracy;Surveillance;Machine learning;Cameras","audio signal processing;feature extraction;interpolation;learning (artificial intelligence);signal detection","video surveillance;audio processing techniques;CCTV cameras;deep learning based scream sound detection approach;MFCC features;interpolation;self-recorded scream database","","2","11","","","","","IEEE","IEEE Conferences"
"Hyper-class augmented and regularized deep learning for fine-grained image classification","S. Xie; T. Yang; Xiaoyu Wang; Yuanqing Lin","University of California, San Diego, USA; Department of Computer Science, University of Iowa, USA; Snapchat Research, USA; NEC Laboratories America, Inc., USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","2645","2654","Deep convolutional neural networks (CNN) have seen tremendous success in large-scale generic object recognition. In comparison with generic object recognition, fine-grained image classification (FGIC) is much more challenging because (i) fine-grained labeled data is much more expensive to acquire (usually requiring domain expertise); (ii) there exists large intra-class and small inter-class variance. Most recent work exploiting deep CNN for image recognition with small training data adopts a simple strategy: pre-train a deep CNN on a large-scale external dataset (e.g., ImageNet) and fine-tune on the small-scale target data to fit the specific classification task. In this paper, beyond the fine-tuning strategy, we propose a systematic framework of learning a deep CNN that addresses the challenges from two new perspectives: (i) identifying easily annotated hyper-classes inherent in the fine-grained data and acquiring a large number of hyper-class-labeled images from readily available external sources (e.g., image search engines), and formulating the problem into multitask learning; (ii) a novel learning model by exploiting a regularization between the fine-grained recognition model and the hyper-class recognition model. We demonstrate the success of the proposed framework on two small-scale fine-grained datasets (Stanford Dogs and Stanford Cars) and on a large-scale car dataset that we collected.","","","10.1109/CVPR.2015.7298880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298880","","Machine learning;Image recognition;Feature extraction;Neural networks;Search engines;Visualization;Solid modeling","image classification;learning (artificial intelligence)","hyper-class augmented deep learning;regularized deep learning;fine-grained image classification;deep convolutional neural networks;large-scale generic object recognition;FGIC;fine-grained labeled data;large intra-class variance;small inter-class variance;deep CNN;image recognition;large-scale external dataset;fine-tuning strategy;hyper-class-labeled images;multitask learning;learning model;fine-grained recognition model;hyper-class recognition model;small-scale fine-grained datasets;Stanford Dogs;Stanford Cars","","53","43","","","","","IEEE","IEEE Conferences"
"Nonlinear system identification using deep learning and randomized algorithms","E. de la Rosa; W. Yu; X. Li","Departamento de Control Automatico, CINVESTAV-IPN (National Polytechnic Institute), Mexico City, Mexico; Departamento de Control Automatico, CINVESTAV-IPN (National Polytechnic Institute), Mexico City, Mexico; Departamento de Computacion, CINVESTAV-IPN (National Polytechnic Institute), Mexico City, Mexico","2015 IEEE International Conference on Information and Automation","","2015","","","274","279","Randomized algorithms have good performances for regression and classification problems by using random hidden weights and pseudoinverse computing for the output weights. They have one single hidden layer structure. On the other hand, deep learning techniques have been successfully used for pattern recognition due to their deep structure and effective unsupervised learning. In this paper, the randomized algorithm is modified by the deep learning method. There are multiple hidden layers, and the hidden weights are decided by the input data and modified restricted Boltzmann machines. The output weights are trained by normal randomized algorithms. The proposed deep learning with the randomized algorithms are validated with three benchmark datasets.","","","10.1109/ICInfA.2015.7279298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279298","deep learning;randomized algorithms;system identification","Machine learning;Training;Nonlinear systems;Computational modeling;Accuracy;Neural networks;Probability distribution","Boltzmann machines;pattern classification;randomised algorithms;regression analysis;unsupervised learning","nonlinear system identification;deep learning;regression problem;classification problem;random hidden weights;pseudoinverse computing;pattern recognition;unsupervised learning;restricted Boltzmann machines;normal randomized algorithms","","7","42","","","","","IEEE","IEEE Conferences"
"Deep learning vs. kernel methods: Performance for emotion prediction in videos","Y. Baveye; E. Dellandréa; C. Chamaret; L. Chen","Technicolor 975, avenue des Champs Blancs, 35576 Cesson Sévigné, France; Université de Lyon, CNRS, Ecole Centrale de Lyon, LIRIS, UMR5205, F-69134, France; Technicolor 975, avenue des Champs Blancs, 35576 Cesson Sévigné, France; Université de Lyon, CNRS, Ecole Centrale de Lyon, LIRIS, UMR5205, F-69134, France","2015 International Conference on Affective Computing and Intelligent Interaction (ACII)","","2015","","","77","83","Recently, mainly due to the advances of deep learning, the performances in scene and object recognition have been progressing intensively. On the other hand, more subjective recognition tasks, such as emotion prediction, stagnate at moderate levels. In such context, is it possible to make affective computational models benefit from the breakthroughs in deep learning? This paper proposes to introduce the strength of deep learning in the context of emotion prediction in videos. The two main contributions are as follow: (i) a new dataset, composed of 30 movies under Creative Commons licenses, continuously annotated along the induced valence and arousal axes (publicly available) is introduced, for which (ii) the performance of the Convolutional Neural Networks (CNN) through supervised fine-tuning, the Support Vector Machines for Regression (SVR) and the combination of both (Transfer Learning) are computed and discussed. To the best of our knowledge, it is the first approach in the literature using CNNs to predict dimensional affective scores from videos. The experimental results show that the limited size of the dataset prevents the learning or finetuning of CNN-based frameworks but that transfer learning is a promising solution to improve the performance of affective movie content analysis frameworks as long as very large datasets annotated along affective dimensions are not available.","","","10.1109/ACII.2015.7344554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344554","continuous emotion prediction;deep learning;benchmarking;affective computing","Motion pictures;Videos;Hidden Markov models;Computational modeling;Machine learning;Kernel;Correlation","emotion recognition;learning (artificial intelligence);object recognition;regression analysis;support vector machines;video signal processing","deep learning;kernel methods;object recognition;computational models;video emotion prediction;creative common licenses;convolutional neural networks;supervised fine-tuning;support vector machines for regression;SVR;transfer learning;dimensional affective scores;CNN-based frameworks;affective movie content analysis frameworks","","20","27","","","","","IEEE","IEEE Conferences"
"A Hierarchical Oil Tank Detector With Deep Surrounding Features for High-Resolution Optical Satellite Imagery","L. Zhang; Z. Shi; J. Wu","State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; Department of Remote Sensing and Mapping, Space Star Technology Co., Ltd., Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2015","8","10","4895","4909","Automatic oil tank detection plays a very important role for remote sensing image processing. To accomplish the task, a hierarchical oil tank detector with deep surrounding features is proposed in this paper. The surrounding features extracted by the deep learning model aim at making the oil tanks more easily to recognize, since the appearance of oil tanks is a circle and this information is not enough to separate targets from the complex background. The proposed method is divided into three modules: 1) candidate selection; 2) feature extraction; and 3) classification. First, a modified ellipse and line segment detector (ELSD) based on gradient orientation is used to select candidates in the image. Afterward, the feature combing local and surrounding information together is extracted to represent the target. Histogram of oriented gradients (HOG) which can reliably capture the shape information is extracted to characterize the local patch. For the surrounding area, the convolutional neural network (CNN) trained in ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012) contest is applied as a blackbox feature extractor to extract rich surrounding feature. Then, the linear support vector machine (SVM) is utilized as the classifier to give the final output. Experimental results indicate that the proposed method is robust under different complex backgrounds and has high detection rate with low false alarm.","","","10.1109/JSTARS.2015.2467377","National Natural Science Foundation of China; Beijing Natural Science Foundation; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University; Fundamental Research Funds for the Central Universities; Open Research Fund of the State Key Laboratory of Space-Ground Integrated Information Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7229258","Convolutional neural network (CNN);deep learning;ellipse and line segment detector (ELSD);oil tank detection;surrounding information;Convolutional neural network (CNN);deep learning;ellipse and line segment detector (ELSD);oil tank detection;surrounding information","Feature extraction;Fuel storage;Support vector machines;Detectors;Machine learning;Remote sensing;Data mining;Neural networks","feature extraction;fuel storage;geophysical image processing;image classification;learning (artificial intelligence);neural nets;remote sensing by laser beam;support vector machines;tanks (containers)","hierarchical oil tank detector;deep surrounding feature;high-resolution optical satellite imagery;automatic oil tank detection;remote sensing image processing;feature extraction;deep learning model;modified ELSD;Ellipse and Line Segment Detector;gradient orientation;histogram-of-oriented gradient;shape information;convolutional neural network;ImageNet Large Scale Visual Recognition Challenge;AD 2012;blackbox feature extractor;linear support vector machine","","48","43","","","","","IEEE","IEEE Journals"
"Regularization of deep neural networks using a novel companion objective function","W. Sun; F. Su","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","2865","2869","A novel objective function of deep neuron networks with companion losses of both convolutional layers and non-linear activation functions is proposed, aiming to obtain more discriminative features. Conventional deep neuron networks were generally trained by the end-to-end supervised learning framework, whose performance is restricted by the training problems, such as the gradient vanishing problem, leading to less discriminative features, especially in lower layers. Instead, we build a novel objective function with two kinds of companion losses. The advantages of this framework are as follows: Firstly, it facilities the optimization by solving the gradient vanishing problem. Secondly, both kinds of companion supervised information contribute to obtain more discriminative features. Finally, a good initialization for fine-tuning could be obtained with the aid of the companion supervised training. Experimental results demonstrate the proposed model yielding better performances on the image classification benchmark dataset.","","","10.1109/ICIP.2015.7351326","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351326","Deep neuron networks;Regularization;Companion objective function;Image classification","Training;Linear programming;Support vector machines;Computational modeling;Neurons;Fasteners;Supervised learning","learning (artificial intelligence);neural nets;optimisation","image classification benchmark dataset;companion supervised training;optimization;companion losses;gradient vanishing problem;training problems;end-to-end supervised learning framework;deep neuron networks;nonlinear activation functions;convolutional layers;companion objective function;deep neural network regularization","","","20","","","","","IEEE","IEEE Conferences"
"Deep transfer metric learning","J. Hu; J. Lu; Y. Tan","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Advanced Digital Sciences Center, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","325","333","Conventional metric learning methods usually assume that the training and test samples are captured in similar scenarios so that their distributions are assumed to be the same. This assumption doesn't hold in many real visual recognition applications, especially when samples are captured across different datasets. In this paper, we propose a new deep transfer metric learning (DTML) method to learn a set of hierarchical nonlinear transformations for cross-domain visual recognition by transferring discriminative knowledge from the labeled source domain to the unlabeled target domain. Specifically, our DTML learns a deep metric network by maximizing the inter-class variations and minimizing the intra-class variations, and minimizing the distribution divergence between the source domain and the target domain at the top layer of the network. To better exploit the discriminative information from the source domain, we further develop a deeply supervised transfer metric learning (DSTML) method by including an additional objective on DTML where the output of both the hidden layers and the top layer are optimized jointly. Experimental results on cross-dataset face verification and person re-identification validate the effectiveness of the proposed methods.","","","10.1109/CVPR.2015.7298629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298629","","Measurement;Training;Face;Learning systems;Visualization;Machine learning;Face recognition","face recognition;learning (artificial intelligence)","conventional metric learning method;real visual recognition;deep transfer metric learning method;DTML method;hierarchical nonlinear transformation;cross-domain visual recognition;discriminative knowledge;labeled source domain;unlabeled target domain;deep metric network;interclass variation;intraclass variation;distribution divergence;discriminative information;deeply supervised transfer metric learning method;DSTML method;hidden layer;cross-dataset face verification;person reidentification","","73","40","","","","","IEEE","IEEE Conferences"
"Interweaving deep learning and semantic techniques for emotion analysis in human-machine interaction","D. Kollias; G. Marandianos; A. Raouzaiou; A. Stafylopatis","School of Electrical and Computer Engineering National Technical University of Athens 9, Iroon Politechniou street, Athens, Greece; School of Electrical and Computer Engineering National Technical University of Athens 9, Iroon Politechniou street, Athens, Greece; School of Electrical and Computer Engineering National Technical University of Athens 9, Iroon Politechniou street, Athens, Greece; School of Electrical and Computer Engineering National Technical University of Athens 9, Iroon Politechniou street, Athens, Greece","2015 10th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP)","","2015","","","1","6","This paper presents a new data classification approach which is based on the one hand on deep learning neural networks for effectively extracting well defined categorical information from data and on the other hand on an adaptable support vector machine, which appropriately represents existing related knowledge about user and context specific data. The proposed approach is implemented and successfully tested experimentally for emotion analysis in human machine interaction.","","","10.1109/SMAP.2015.7370086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7370086","deep learning;convolutional networks;kernel based semantic classification;emotion analysis;human computer interaction","Kernel;Support vector machines;Machine learning;Semantics;Emotion recognition;Convolution;Computer architecture","human computer interaction;learning (artificial intelligence);neural nets;pattern classification;support vector machines","interweaving deep learning;semantic technique;emotion analysis;human-machine interaction;data classification approach;deep learning neural network;categorical information;adaptable support vector machine;human machine interaction","","2","33","","","","","IEEE","IEEE Conferences"
"A comparative study for chest radiograph image retrieval using binary texture and deep learning classification","Y. Anavi; I. Kogan; E. Gelbart; O. Geva; H. Greenspan","Medical Image Processing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel Aviv University, Israel; Medical Image Processing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel Aviv University, Israel; Medical Image Processing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel Aviv University, Israel; Medical Image Processing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel Aviv University, Israel; Medical Image Processing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel Aviv University, Israel","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","2940","2943","In this work various approaches are investigated for X-ray image retrieval and specifically chest pathology retrieval. Given a query image taken from a data set of 443 images, the objective is to rank images according to similarity. Different features, including binary features, texture features, and deep learning (CNN) features are examined. In addition, two approaches are investigated for the retrieval task. One approach is based on the distance of image descriptors using the above features (hereon termed the “descriptor”-based approach); the second approach (“classification”-based approach) is based on a probability descriptor, generated by a pair-wise classification of each two classes (pathologies) and their decision values using an SVM classifier. Best results are achieved using deep learning features in a classification scheme.","","","10.1109/EMBC.2015.7319008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319008","","Pathology;Heart;Feature extraction;Machine learning;Measurement;Biomedical imaging;Support vector machines","diagnostic radiography;image classification;image retrieval;image texture;learning (artificial intelligence);medical image processing;probability;support vector machines","chest radiograph image retrieval;binary texture;deep learning classification;X-ray image retrieval;chest pathology retrieval;query image;binary features;texture features;deep learning features;CNN;image descriptors;descriptor-based approach;classification-based approach;probability descriptor;pair-wise classification;decision values;SVM classifier","Machine Learning;Thorax","14","12","","","","","IEEE","IEEE Conferences"
"4.6 A1.93TOPS/W scalable deep learning/inference processor with tetra-parallel MIMD architecture for big-data applications","S. Park; K. Bong; D. Shin; J. Lee; S. Choi; H. Yoo","KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea","2015 IEEE International Solid-State Circuits Conference - (ISSCC) Digest of Technical Papers","","2015","","","1","3","Recently, deep learning (DL) has become a popular approach for big-data analysis in image retrieval with high accuracy [1]. As Fig. 4.6.1 shows, various applications, such as text, 2D image and motion recognition use DL due to its best-in-class recognition accuracy. There are 2 types of DL: supervised DL with labeled data and unsupervised DL with unlabeled data. With unsupervised DL, most of learning time is spent in massively iterative weight updates for a restricted Boltzmann machine [2]. For a -100MB training dataset, >100 TOP computational capability and ~40GB/s IO and SRAM data bandwidth is required. So, a 3.4GHz CPU needs >10 hours learning time with a -100K input-vector dataset and takes ~1 second for recognition, which is far from real-time processing. Thus, DL is typically done using cloud servers or high-performance GPU environments with learning-on-server capability. However, the wide use of smart portable devices, such as smartphones and tablets, results in many applications which need big-data processing with machine learning, such as tagging private photos in personal devices. A high-performance and energy-efficient DL/DI (deep inference) processor is required to realize user-centric pattern recognition in portable devices.","","","10.1109/ISSCC.2015.7062935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7062935","","Program processors;Bandwidth;Parallel processing;Multicore processing;Pipelines;Scalability","Big Data;Boltzmann machines;graphics processing units;image retrieval;inference mechanisms;learning (artificial intelligence);parallel architectures;SRAM chips","inference processor;scalable deep learning processor;tetra-parallel MIMD architecture;big data applications;image retrieval;supervised DL;labeled data;unsupervised DL;unlabeled data;iterative weight update;Boltzmann machine;IO;SRAM;CPU;input vector dataset;GPU environment;learning-on-server capability;smart portable devices;machine learning;personal devices;private photos tagging;user centric pattern recognition;frequency 3.4 GHz","","27","7","","","","","IEEE","IEEE Conferences"
"Deep architecture using Multi-Kernel Learning and multi-classifier methods","I. Rebai; Y. BenAyed; W. Mahdi","Multimedia InfoRmation system and Advanced Computing Laboratory, University of Sfax, Tunisia; Multimedia InfoRmation system and Advanced Computing Laboratory, University of Sfax, Tunisia; Multimedia InfoRmation system and Advanced Computing Laboratory, University of Sfax, Tunisia","2015 IEEE/ACS 12th International Conference of Computer Systems and Applications (AICCSA)","","2015","","","1","6","Kernel Methods have been successfully applied in different tasks and used on a variety of data sample sizes. Multiple Kernel Learning (MKL) and Multilayer Multiple Kernel Learning (MLMKL), as new families of kernel methods, consist of learning the optimal kernel from a set of predefined kernels by using an optimization algorithm. However, learning this optimal combination is considered to be an arduous task. Furthermore, existing algorithms often do not converge to the optimal solution (i.e., weight distribution). They achieve worse results than the simplest method, which is based on the average combination of base kernels, for some real-world applications. In this paper, we present a hybrid model that integrates two methods: Support Vector Machine (SVM) and Multiple Classifier (MC) methods. More precisely, we propose a multiple classifier framework of deep SVMs for classification tasks. We adopt the MC approach to train multiple SVMs based on multiple kernel in a multi-layer structure in order to avoid solving the complicated optimization tasks. Since the average combination of kernels gives high performance, we train multiple models with a predefined combination of kernels. Indeed, we apply a specific distribution of weights for each model. To evaluate the performance of the proposed method, we conducted an extensive set of classification experiments on a number of benchmark data sets. Experimental results show the effectiveness and efficiency of the proposed method as compared to various state-of-the-art MKL and MLMKL algorithms.","","","10.1109/AICCSA.2015.7507155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507155","Support Vector Machine;Kernel methods;Deep Multiple Kernel Learning;Multiple Classifier","Kernel;Support vector machines;Optimization;Training;Computer architecture;Nonhomogeneous media;Machine learning","learning (artificial intelligence);optimisation;pattern classification;support vector machines","data sample sizes;multilayer structure;MC methods;multiple classifier methods;SVM;support vector machine;optimization algorithm;MLMKL;multilayer multiple kernel learning;MKL;multiple kernel learning;multiclassifier methods;multikernel learning;deep architecture","","","28","","","","","IEEE","IEEE Conferences"
"Composite sketch recognition via deep network - a transfer learning approach","P. Mittal; M. Vatsa; R. Singh","IIIT-Delhi, India; IIIT-Delhi, India; IIIT-Delhi, India","2015 International Conference on Biometrics (ICB)","","2015","","","251","256","Sketch recognition is one of the integral components used by law enforcement agencies in solving crime. In recent past, software generated composite sketches are being preferred as they are more consistent and faster to construct than hand drawn sketches. Matching these composite sketches to face photographs is a complex task because the composite sketches are drawn based on the witness description and lack minute details which are present in photographs. This paper presents a novel algorithm for matching composite sketches with photographs using transfer learning with deep learning representation. In the proposed algorithm, first the deep learning architecture based facial representation is learned using large face database of photos and then the representation is updated using small problem-specific training database. Experiments are performed on the extended PRIP database and it is observed that the proposed algorithm outperforms recently proposed approach and a commercial face recognition system.","","","10.1109/ICB.2015.7139092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139092","","Face;Databases;Training;Accuracy;Feature extraction;Face recognition;Software","face recognition;image matching;image representation;learning (artificial intelligence);police data processing","composite sketch recognition;deep network;transfer learning approach;law enforcement agencies;software generated composite sketches;image matching;face photographs;deep learning architecture based facial representation;extended PRIP database","","19","25","","","","","IEEE","IEEE Conferences"
"EvoAE -- A New Evolutionary Method for Training Autoencoders for Deep Learning Networks","S. Lander; Y. Shang","Comput. Sci. Dept., Univ. of Missouri, Columbia, MO, USA; Comput. Sci. Dept., Univ. of Missouri, Columbia, MO, USA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","2","","790","795","Although deep learning has achieved outstanding performances on several difficult machine learning applications, there are multiple issues that make its application on new problems difficult: speed of training, local minima, and manual selection of hyper-parameters. To overcome these problems, this paper proposes a new evolutionary method, EvoAE, to train auto encoders for deep learning networks. By evolving a population of auto encoders, EvoAE learns multiple features in each auto encoder in the form of hidden nodes, evaluates the auto encoders based on their reconstruction quality, and generates new auto encoders using crossover and mutation with chromosomes made up of hidden nodes and associated connections and weights. EvoAE optimizes network weights and structures of auto encoders simultaneously and employs a mini-batch variant, called Evo-batch, to speed up auto encoder search on large datasets. Furthermore, EvoAE supports different training methods in data partitioning and selection, requires little manual intervention, and reduces overall training time drastically over traditional methods on large datasets.","","","10.1109/COMPSAC.2015.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273701","autoencoder;neural networks;deep learning;evolutionary algorithm","Training;Backpropagation;Sociology;Statistics;Machine learning;Optimization;Testing","evolutionary computation;learning (artificial intelligence)","machine learning applications;evolutionary method;EvoAE;deep learning networks;reconstruction quality;crossover;mutation;chromosomes;network weights optimization;minibatch variant;Evo-batch;autoencoder search;large datasets;training methods;data partitioning;data selection","","9","10","","","","","IEEE","IEEE Conferences"
"DeepEmo: Real-world facial expression analysis via deep learning","W. Deng; J. Hu; S. Zhang; J. Guo","Beijing University of Posts and Telecommunications, Beijing, 100876, China; Beijing University of Posts and Telecommunications, Beijing, 100876, China; Beijing University of Posts and Telecommunications, Beijing, 100876, China; Beijing University of Posts and Telecommunications, Beijing, 100876, China","2015 Visual Communications and Image Processing (VCIP)","","2015","","","1","4","Recent automatic facial expression recognition research has focused on optimizing performance on a few databases that were collected under controlled pose and lighting conditions, and has produced nearly perfect accuracy. This paper explores the necessary characteristics of the training dataset, feature representations and machine learning algorithms for a system that operates reliably in more realistic conditions. A new database, Real-world Affective Face Database (RAF-DB), is presented which contains about 30,000 greatly-diverse facial images from social networks. Crowdsourcing results suggest that real-world expression recognition problem is a typical imbalanced multi-label classification problem, and the balanced, single-label datasets currently used in the literature could potentially lead research into misleading algorithmic solutions. A deep learning architecture, DeepEmo, is proposed to address the real-world challenge of emotion recognition by learning the highlevel feature representations which are highly effective for discriminating realistic facial expressions. Extensive experimental results show that the deep learning method is significantly superior to handcrafted features, and with the near-frontal pose constraint, human-level recognition accuracy is achievable.","","","10.1109/VCIP.2015.7457876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457876","Expression recognition;Deep learning;face analysis;emotion recognition;human-computer interaction","Databases;Machine learning;Face recognition;Emotion recognition;Training;Face;Support vector machines","emotion recognition;face recognition;feature extraction;human computer interaction;image classification;image representation;learning (artificial intelligence);social networking (online)","DeepEmo;deep learning architecture;facial expression analysis;facial expression recognition;feature representation;machine learning algorithm;social network;multilabel classification problem;emotion recognition;near-frontal pose constraint;human-level recognition;human-computer interaction","","3","13","","","","","IEEE","IEEE Conferences"
"A robust PHD filter with deep learning updating for multiple human tracking","P. Feng; W. Wang; S. M. Naqvi; J. A. Chambers","Center for Vision Speech and Signal Processing, Department of Electronic Engineering, University of Surrey, UK; Center for Vision Speech and Signal Processing, Department of Electronic Engineering, University of Surrey, UK; Advanced Signal Processing Group, Loughborough University, UK; Advanced Signal Processing Group, Loughborough University, UK","2015 IEEE International Conference on Digital Signal Processing (DSP)","","2015","","","1227","1231","We propose a novel robust probability hypothesis density (PHD) filter for multiple target tracking in an enclosed environment, where a deep learning method is used in the update step for combining different human features to mitigate the effect of measurement noise on the calculation of particle weights. Deep belief networks (DBNs) are trained based on both colour and oriented gradient (HOG) histogram features and then used to mitigate the measurement noise from the particle selection and PHD update step, thereby improving the tracking performance. To evaluate the proposed PHD filter, two sequences with 383 frames from the CAVIAR dataset are employed and both the optimal subpattern assignment (OSPA) and mean of error from each target method are used as objective measures. The results show that the proposed robust PHD filter outperforms the traditional PHD filter.","","","10.1109/ICDSP.2015.7252076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7252076","Multiple human tracking;PHD filter;deep learning;deep belief networks","Target tracking;Feature extraction;Noise;Robustness;Atmospheric measurements;Particle measurements;Training","feature extraction;filters;gradient methods;learning (artificial intelligence);probability;target tracking","probability hypothesis density filter;PHD filter;multiple human tracking;multiple target tracking;deep learning method;deep belief networks;DBN;histogram oriented gradient features;HOG features;noise measurement;CAVIAR dataset;optimal subpattern assignment;OSPA","","2","17","","","","","IEEE","IEEE Conferences"
"A Novel Method to Fix Numbers of Hidden Neurons in Deep Neural Networks","J. Li; Y. Wu; J. Zhang; G. Zhao","Coll. of Electron. & Inf. Eng., Tongji Univ., Shanghai, China; Coll. of Electron. & Inf. Eng., Tongji Univ., Shanghai, China; Coll. of Electron. & Inf. Eng., Tongji Univ., Shanghai, China; Coll. of Electron. & Inf. Eng., Tongji Univ., Shanghai, China","2015 8th International Symposium on Computational Intelligence and Design (ISCID)","","2015","2","","523","526","In this paper, we propose a novel method which can automatically find the preferable hidden neuron numbers in deep neural networks. This method is completed by two cooperating algorithms: Principle Components Analysis (PCA) and Reinforcement Learning (RL). PCA is used to find a range of hidden neuron numbers, and RL is applied to search a better number of hidden neurons and update the searching points. The training process is layer wisely conducted and finally formed a deep neural network. Testing on the MNIST dataset shows, the algorithm can automatically fix the number of hidden neurons layer wisely in deep neural networks and achieve an accuracy of 98.24%, which shows that our method is effective in selection of hidden neuron numbers.","","","10.1109/ISCID.2015.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7469188","deep neural network;hidden neuron numbers selection;reinforcement learning;principle components analysis;autoencoders","Neurons;Principal component analysis;Biological neural networks;Training;Testing;Artificial neural networks;Distortion","learning (artificial intelligence);neural nets;principal component analysis","deep neural networks;hidden neuron numbers;principle component analysis;PCA;reinforcement learning;RL;MNIST dataset","","1","12","","","","","IEEE","IEEE Conferences"
"Combining Newton interpolation and deep learning for image classification","Y. Zhang; C. Shang","Aberystwyth University, United Kingdom; Aberystwyth University, United Kingdom","Electronics Letters","","2015","51","1","40","42","A novel approach for image classification, by integrating deep learning and feature interpolation, supported with advanced learning classification techniques, is presented. The recently introduced deep spatiotemporal inference network (DeSTIN) is employed to carry out limited original feature extraction. Newton interpolation is then used to artificially increase the dimensionality of the extracted feature sets for accurate classification, without incurring heavy computational cost. Support vector machines are utilised for image classification. The proposed approach is tested against the popular MNIST dataset of handwritten digits, demonstrating the potential of the approach.","","","10.1049/el.2014.3223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7006843","","","feature extraction;image classification;interpolation;learning (artificial intelligence);Newton method;spatiotemporal phenomena;support vector machines","MNIST dataset;SVM;support vector machine;feature extraction;DeSTIN;deep spatiotemporal inference network;advanced learning classification technique;feature interpolation;image classification;deep learning;Newton interpolation","","3","5","","","","","IET","IET Journals"
"Learning Deep Trajectory Descriptor for action recognition in videos using deep neural networks","Yemin Shi; Wei Zeng; Tiejun Huang; Yaowei Wang","School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China; School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China; School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China; School of Information and Electronics, Beijing Institute of Technology, 100081, China","2015 IEEE International Conference on Multimedia and Expo (ICME)","","2015","","","1","6","Human action recognition is widely recognized as a challenging task due to the difficulty of effectively characterizing human action in a complex scene. Recent studies have shown that the dense-trajectory-based methods can achieve state-of-the-art recognition results on some challenging datasets. However, in these methods, each dense trajectory is often represented as a vector of coordinates, consequently losing the structural relationship between different trajectories. To address the problem, this paper proposes a novel Deep Trajectory Descriptor (DTD) for action recognition. First, we extract dense trajectories from multiple consecutive frames and then project them onto a canvas. This will result in a “trajectory texture” image which can effectively characterize the relative motion in these frames. Based on these trajectory texture images, a deep neural network (DNN) is utilized to learn a more compact and powerful representation of dense trajectories. In the action recognition system, the DTD descriptor, together with other non-trajectory features such as HOG, HOF and MBH, can provide an effective way to characterize human action from various aspects. Experimental results show that our system can statistically outperform several state-of-the-art approaches, with an average accuracy of 95:6% on KTH and an accuracy of 92.14% on UCF50.","","","10.1109/ICME.2015.7177461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177461","Deep Neural Network;Deep Trajectory Descriptor;action recognition","Trajectory;Videos;Feature extraction;Accuracy;Neural networks;Convolution;Kernel","image recognition;image texture;neural nets","learning deep trajectory descriptor;human action recognition;deep neural networks;dense-trajectory-based methods;DTD descriptor;trajectory texture images;DNN;HOG;HOF;MBH;UCF50;KTH","","2","17","","","","","IEEE","IEEE Conferences"
"Deep belief networks for predicting corporate defaults","S. Yeh; C. Wang; M. Tsai","Department of Computer Science, University of Taipei, Taipei, Taiwan; Department of Computer Science, University of Taipei, Taipei, Taiwan; Department of Computer Science, National Chengchi University, Taipei, Taiwan","2015 24th Wireless and Optical Communication Conference (WOCC)","","2015","","","159","163","This paper provides a new perspective on the default prediction problem using deep learning algorithms. Via the advantages of deep learning, the representable factors of input data will no longer need to be explicitly extracted, but can be implicitly learned by the deep learning algorithms. We consider the stock returns of both default and solvent companies as input signals and adopt one of the deep learning architecture, Deep Belief Networks (DBN), to train the prediction models. The preliminary results show that the proposed approach outperforms traditional machine learning algorithms.","","","10.1109/WOCC.2015.7346197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346197","","Companies;Machine learning;Predictive models;Training;Feature extraction;Prediction algorithms;Time series analysis","belief networks;business data processing;financial data processing;learning (artificial intelligence);prediction theory;stock markets","deep belief network;corporate default prediction;deep learning algorithm;stock return;DBN;prediction model;machine learning algorithm","","1","27","","","","","IEEE","IEEE Conferences"
"Compressed-Domain Ship Detection on Spaceborne Optical Image Using Deep Neural Network and Extreme Learning Machine","J. Tang; C. Deng; G. Huang; B. Zhao","Sch. of Inf. & Electron., Beijing Inst. of Technol., Beijing, China; Sch. of Inf. & Electron., Beijing Inst. of Technol., Beijing, China; Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Inf. & Electron., Beijing Inst. of Technol., Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2015","53","3","1174","1185","Ship detection on spaceborne images has attracted great interest in the applications of maritime security and traffic control. Optical images stand out from other remote sensing images in object detection due to their higher resolution and more visualized contents. However, most of the popular techniques for ship detection from optical spaceborne images have two shortcomings: 1) Compared with infrared and synthetic aperture radar images, their results are affected by weather conditions, like clouds and ocean waves, and 2) the higher resolution results in larger data volume, which makes processing more difficult. Most of the previous works mainly focus on solving the first problem by improving segmentation or classification with complicated algorithms. These methods face difficulty in efficiently balancing performance and complexity. In this paper, we propose a ship detection approach to solving the aforementioned two issues using wavelet coefficients extracted from JPEG2000 compressed domain combined with deep neural network (DNN) and extreme learning machine (ELM). Compressed domain is adopted for fast ship candidate extraction, DNN is exploited for high-level feature representation and classification, and ELM is used for efficient feature pooling and decision making. Extensive experiments demonstrate that, in comparison with the existing relevant state-of-the-art approaches, the proposed method requires less detection time and achieves higher detection accuracy.","","","10.1109/TGRS.2014.2335751","National Natural Science Foundation of China; Beijing Excellent Talent Fund; Excellent Young Scholars Research Fund of Beijing Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6866146","Compressed domain;deep neural network (DNN);extreme learning machine (ELM);JPEG2000;optical spaceborne image;remote sensing;ship detection;Compressed domain;deep neural network (DNN);extreme learning machine (ELM);JPEG2000;optical spaceborne image;remote sensing;ship detection","Marine vehicles;Feature extraction;Image coding;Optical imaging;Transform coding;Training;Optical sensors","compressed sensing;data compression;geophysical image processing;image classification;image coding;learning (artificial intelligence);neural nets;object detection;oceanographic techniques;remote sensing;ships;wavelet transforms","compressed domain ship detection;spaceborne optical images;deep neural network;extreme learning machine;maritime security;maritime traffic control;remote sensing images;object detection;weather conditions;high image resolution;data volume;wavelet coefficients;JPEG2000 compressed domain;DNN;ELM;high level feature representation;high level feature classification","","189","53","","","","","IEEE","IEEE Journals"
"Smart city and geospatiality: Hobart deeply learned","J. Aryal; R. Dutta","University of Tasmania, Discipline of Geography and Spatial Sciences, Hobart, Australia; Commonwealth Scientific and Industrial Research, Organisation (CSIRO), Digital Productivity Flagship, Hobart, Australia","2015 31st IEEE International Conference on Data Engineering Workshops","","2015","","","108","109","We propose a cloud computing based big data framework using Deep Neural Networks, to learn urban objects from very high-resolution image in an abstract optimized manner. Automatic recognition of such objects would be essential to minimize big data accessibility issues and increase efficiency of urban dynamics monitoring and planning. We have shown that deep learning could be a way forward towards that complex aim with very high accuracy rates.","","","10.1109/ICDEW.2015.7129557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7129557","smart cities;ultra-high resolution;geospatiality;Hobart;IKONOS;GEOBIA;Deep Learning","Cities and towns;Feature extraction;Big data;Data mining;Spatial resolution;Accuracy;Knowledge based systems","Big Data;cloud computing;image resolution;learning (artificial intelligence);neural nets;object recognition;remote sensing;smart cities;town and country planning","smart city;geospatiality;Hobart IKONOS data;cloud computing;Big Data framework;deep neural network;deep learning;image resolution;object recognition;urban dynamics monitoring;urban planning","","4","4","","","","","IEEE","IEEE Conferences"
"A review on advances in deep learning","Soniya; S. Paul; L. Singh","Dept. of Physics and Computer Science, Dayalbagh Educational Institute, Dayalbagh, Agra 282005; Dept. of Physics and Computer Science, Dayalbagh Educational Institute Dayalbagh, Agra 282005; Dept. of Physics and Computer Science, Dayalbagh Educational Institute, Dayalbagh, Agra 282005","2015 IEEE Workshop on Computational Intelligence: Theories, Applications and Future Directions (WCI)","","2015","","","1","6","Over the years conventional neural networks has shown state-of-art performance on many problems. However, their performance on recognition system is still not widely accepted in the machine learning community because these networks are unable to handle selectivity-invariance dilemma and also suffer from the problem of vanishing gradients. Some of these issues have been addressed by deep learning. Deep learning approaches attempt to disentangle intricate aspects of input by creating multiple levels of representation. These approaches have shown astonishing results in problem domains like recognition system, natural language processing, medical sciences, and in many other fields. The paper presents an overview of different deep learning approaches in a nutshell and also highlights some limitations which are restricting performance of deep neural networks in order to handle more realistic problems.","","","10.1109/WCI.2015.7495514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495514","conventional neural networks;deep learning;deep neural networks","Feature extraction;Biological neural networks;Computer architecture;Machine learning;Unsupervised learning;Supervised learning","","","","12","80","","","","","IEEE","IEEE Conferences"
"Deep neural network and random forest hybrid architecture for learning to detect retinal vessels in fundus images","D. Maji; A. Santara; S. Ghosh; D. Sheet; P. Mitra","Indian Institute of Technology Kharagpur, WB 721302, India; Indian Institute of Technology Kharagpur, WB 721302, India; Department of Ophthalmology, North Bengal Medical College and Hospital, Darjeeling, India; Indian Institute of Technology Kharagpur, WB 721302, India; Indian Institute of Technology Kharagpur, WB 721302, India","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","3029","3032","Vision impairment due to pathological damage of the retina can largely be prevented through periodic screening using fundus color imaging. However the challenge with large-scale screening is the inability to exhaustively detect fine blood vessels crucial to disease diagnosis. In this work we present a computational imaging framework using deep and ensemble learning based hybrid architecture for reliable detection of blood vessels in fundus color images. A deep neural network (DNN) is used for unsupervised learning of vesselness dictionaries using sparse trained denoising auto-encoders (DAE), followed by supervised learning of the DNN response using a random forest for detecting vessels in color fundus images. In experimental evaluation with the DRIVE database, we achieve the objective of vessel detection with max. avg. accuracy of 0.9327 and area under ROC curve of 0.9195.","","","10.1109/EMBC.2015.7319030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319030","Computational imaging;deep learning;denoising auto-encoder;random forests;vessel detection","Biomedical imaging;Vegetation;Retinal vessels;Image analysis;Radio frequency","biomedical optical imaging;blood vessels;diseases;eye;image denoising;learning (artificial intelligence);medical image processing;vision defects","deep network;random forest hybrid architecture;retinal vessel detection;vision impairment;pathological damage;periodic screening;fundus color imaging;large-scale screening;blood vessels;disease diagnosis;computational imaging framework;learning based hybrid architecture;deep neural network;unsupervised learning;sparse trained denoising autoencoders;DNN response;DRIVE database;ROC curve","Fundus Oculi;Humans;Neural Networks (Computer);Retina;Retinal Vessels","13","16","","","","","IEEE","IEEE Conferences"
"A novel deep learning by combining discriminative model with generative model","S. Kim; M. Lee; J. Shen","School of Electronics Engineering, Kyungpook National University, Taegu, Republic of Korea; School of Electronics Engineering, Kyungpook National University, Taegu, Republic of Korea; Network Management Center, China Mobile Group Guangdong Co., Ltd., Shenzhen, China","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","6","Deep learning methods allow a classifier to learn features automatically through multiple layers of training. In a deep learning process, low-level features are abstracted into high-level features. In this paper, we propose a new probabilistic deep learning method that combines a discriminative model, namely, Support Vector Machine (SVM), with a generative model, namely, Gaussian Mixture Model (GMM). Combining the SVM with the GMM, we can represent a new input feature for deeper layer training of uncertain data in current layer construction. Bayesian rule is used to re-represent the output data of the previous layer of the SVM with GMM to serve as the input data for the next deep layer. As a result, deep features are reliably extracted without additional feature extraction efforts, using multiple layers of the SVM with GMM. Experimental results show that the proposed deep structure model allows for an easier classification of the uncertain data through multiple-layer training and it gives more accurate results.","","","10.1109/IJCNN.2015.7280589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280589","deep learning;hybrid model;discriminative method;generative method","Support vector machines;Feature extraction;Ionosphere;Diabetes;Adaptation models;Data models;Data mining","Bayes methods;Gaussian processes;learning (artificial intelligence);mixture models;support vector machines","deep learning methods;discriminative model;generative model;low-level features;support vector machine;Gaussian mixture model;SVM;GMM;current layer construction;Bayesian rule;feature extraction efforts;deep structure model;multiple layer training","","1","29","","","","","IEEE","IEEE Conferences"
"The Challenges and Feasibility of Societal Risk Classification Based on Deep Learning of Representations","J. Chen; X. Tang","NA; NA","2015 IEEE International Conference on Systems, Man, and Cybernetics","","2015","","","569","574","Using the posts of Tianya Forum as the data source and adopting the socio psychology study results on societal risks perception, we analyze the challenges and feasibility of the document-level multiple societal risk classification of BBS posts. To effectively capture the semantics and word order of documents, a deep learning model as Post Vector is applied to realize the distributed vector representations of the posts in the vector space. Based on the distributed vector representations, cross-validated classification of the posts labeled by different annotators with KNN method and pair wise similarities comparisons of the posts between risk categories are implemented. The big variance of the results of cross validation shows the differences of individual risk perceptions, which reflects the challenges of societal risk classification. Furthermore, the higher similarities of posts in same societal risk category manifest the feasibility of the classification of societal risks, and indicate the possibility to improve the performance of the societal risk classification of BBS posts.","","","10.1109/SMC.2015.110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379242","societal risk classification;Tianya forum;deep learning;individual risk perception;pairwise similarity","Training;Machine learning;Semantics;Learning systems;Context;Text categorization;Psychology","learning (artificial intelligence);pattern classification;psychology;risk management;social networking (online)","Tianya Forum;socio psychology study;societal risks perception;document-level multiple societal risk classification;BBS posts;semantics;word order;deep learning model;post vector;distributed vector representations;vector space;cross-validated post classification;KNN method;pairwise similarities;risk perceptions;societal risk category","","1","18","","","","","IEEE","IEEE Conferences"
"Exploring the use of deep learning for feature location","C. S. Corley; K. Damevski; N. A. Kraft","The University of Alabama, Tuscaloosa, USA; Virginia Commonwealth University, Richmond, USA; ABB Corporate Research, Raleigh, NC, USA","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","","2015","","","556","560","Deep learning models can infer complex patterns present in natural language text. Relative to n-gram models, deep learning models can capture more complex statistical patterns based on smaller training corpora. In this paper we explore the use of a particular deep learning model, document vectors (DVs), for feature location. DVs seem well suited to use with source code, because they both capture the influence of context on each term in a corpus and map terms into a continuous semantic space that encodes semantic relationships such as synonymy. We present preliminary results that show that a feature location technique (FLT) based on DVs can outperform an analogous FLT based on latent Dirichlet allocation (LDA) and then suggest several directions for future work on the use of deep learning models to improve developer effectiveness in feature location.","","","10.1109/ICSM.2015.7332513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332513","deep learning;neural networks;document vectors;feature location","Semantics;Machine learning;Natural languages;Voltage control;Neural networks;Training;Context","learning (artificial intelligence);neural nets;statistical analysis;text analysis","natural language text;deep learning model;n-gram model;statistical pattern;document vector;source code;continuous semantic space;feature location technique;FLT;latent Dirichlet allocation;LDA","","5","23","","","","","IEEE","IEEE Conferences"
"Deep learning using heterogeneous feature maps for maxout networks","Y. Ishii; R. Hagawa; S. Tsukizawa","Panasonic Corporation, 1006 Kadoma, Kadoma City, Osaka 571-8501, Japan; Panasonic Corporation, 1006 Kadoma, Kadoma City, Osaka 571-8501, Japan; Panasonic Corporation, 1006 Kadoma, Kadoma City, Osaka 571-8501, Japan","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","","2015","","","459","463","We propose a novel type of maxout that uses filters with kernels of multiple sizes for generating convolved maps. These filters extract the most effective features for recognition from many different variations of texture patterns. A convolved map is generated by convolution between feature maps and filters. If the size of filters is varied, the size of the convolved map will also vary; in which case, since there are no correspondences among the positions of convolved maps, maxout will not work for these kinds of filters. To align the sizes of convolved maps, we converted, in advance, feature maps, which we term `heterogeneous feature maps,' using zero padding. Converting the size of feature maps in this way allows maxout to function, even with filters of different sizes. In this study we demonstrate the classification performances using our proposed maxout on MNIST, CIFAR-10, CIFAR-100, SVHN datasets, and show a 13.17% improvement of accuracy with augmented data.","","","10.1109/ACPR.2015.7486545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486545","","Kernel;Feature extraction;Convolution;Error analysis;Neural networks;Training;Machine learning","feature extraction;image classification;image filtering;image texture;learning (artificial intelligence)","deep learning;heterogeneous feature map;maxout network;feature extraction;texture pattern variation;feature filter;zero padding;classification performances;MNIST;CIFAR-10;CIFAR-100;SVHN dataset","","","13","","","","","IEEE","IEEE Conferences"
"Multi-input topology of deep belief networks for image segmentation","A. M. Nickfarjam; H. Ebrahimpour-komleh","Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Kashan, Kashan, Iran; Department of Computer Engineering, Faculty of Electrical and Computer Engineering, University of Kashan, Kashan, Iran","2015 International Congress on Technology, Communication and Knowledge (ICTCK)","","2015","","","482","485","We propose a novel approach for image segmentation by taking the advantages of a 5-layer Deep Belief Network (DBN). DBN composed of multiple layers of latent variables (“hidden units”) which used to extract abstract and robust features for image segmentation. However, it processes images with intricate background, hardly. In order to overcome this limitation, we aim to segment only a few pixels by obtaining their local and regional features at each step. The proposed DBN topology utilizes two different components in two different layers as input to extract local and regional features. Both input components are defined on two different scales of the image. Experimental results show 81.93% accuracy on test images which is the result of providing more information for DBN architecture to learn images with intricate background.","","","10.1109/ICTCK.2015.7582716","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582716","Image Segmentation;Deep Belief Network;Contrastive Divergence Learning;Restricted Boltzmann Machine;Multi-input Topology","Image segmentation;Feature extraction;Topology;Network topology;Stochastic processes;Training;Computers","belief networks;feature extraction;image segmentation;learning (artificial intelligence)","multiinput topology;deep belief networks;image segmentation;latent variables;hidden units;abstract feature extraction;robust feature extraction;image processing;local feature extraction;regional feature extraction;DBN topology;test images;image learning","","1","11","","","","","IEEE","IEEE Conferences"
"Deep learning of tissue specific speckle representations in optical coherence tomography and deeper exploration for in situ histology","D. Sheet; S. P. K. Karri; A. Katouzian; N. Navab; A. K. Ray; J. Chatterjee","Department of Electrical Engineering, Indian Institute of Technology Kharagpur, India; School of Medical Science and Technology, Indian Institute of Technology Kharagpur, India; Computer Aided Medical Procedures, Technische Universita¨t Mu¨nchen, Germany; Computer Aided Medical Procedures, Technische Universita¨t Mu¨nchen, Germany; Electronics and Electrical Comm. Engg., Indian Institute of Technology Kharagpur, India; School of Medical Science and Technology, Indian Institute of Technology Kharagpur, India","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","","2015","","","777","780","Optical coherence tomography (OCT) relies on speckle image formation by coherent sensing of photons diffracted from a broadband laser source incident on tissues. Its non-ionizing nature and tissue specific speckle appearance has leveraged rapid clinical translation for non-invasive high-resolution in situ imaging of critical organs and tissue viz. coronary vessels, healing wounds, retina and choroid. However the stochastic nature of speckles introduces inter- and intra-observer reporting variability challenges. This paper proposes a deep neural network (DNN) based architecture for unsupervised learning of speckle representations in swept-source OCT using denoising auto-encoders (DAE) and supervised learning of tissue specifics using stacked DAEs for histologically characterizing healthy skin and healing wounds with the aim of reducing clinical reporting variability. Performance of our deep learning based tissue characterization method in comparison with conventional histology of healthy and wounded mice skin strongly advocates its use for in situ histology of live tissues.","","","10.1109/ISBI.2015.7163987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163987","Representation learning;denoising autoencoders;optical coherence tomography;tissue characterization;in situ histology;cutaneous wounds","Speckle;Optical imaging;Biomedical optical imaging;Adaptive optics;Wounds;Skin;Machine learning","biomedical optical imaging;image coding;image denoising;medical image processing;neural nets;optical tomography;skin;speckle;tissue engineering;unsupervised learning;wounds","deep learning;tissue specific speckle representations;optical coherence tomography;in situ histology;OCT;speckle image formation;coherent sensing;photon diffraction;laser source incident;tissue specific speckle appearance;leveraged rapid clinical translation;noninvasive high-resolution in situ imaging;critical organs;coronary vessels;healing wounds;retina;choroid;stochastic nature;interobserver reporting variability challenges;intraobserver reporting variability challenges;deep neural network-based architecture;unsupervised learning;speckle representations;swept-source OCT;denoising autoencoders;histologically characterizing healthy skin;clinical reporting variability;conventional histology;wounded mice skin;live tissues","","2","11","","","","","IEEE","IEEE Conferences"
"Predictive Deep Boltzmann Machine for Multiperiod Wind Speed Forecasting","C. Zhang; C. L. P. Chen; M. Gan; L. Chen","Department of Computer and Information Science, Faculty of Science and Technology, University of Macau, Macau, China; Department of Computer and Information Science, Faculty of Science and Technology, University of Macau, Macau, China; School of Electrical Engineering and Automation, Hefei University of Technology, Hefei, China; Department of Computer and Information Science, Faculty of Science and Technology, University of Macau, Macau, China","IEEE Transactions on Sustainable Energy","","2015","6","4","1416","1425","It is important to forecast the wind speed for managing operations in wind power plants. However, wind speed prediction is extremely complex and difficult due to the volatility and deviation of the wind. As existing forecasting methods directly model the raw wind speed data, it is difficult for them to provide higher inference accuracy. Differently, this paper presents a sophisticated deep-learning technique for short-term and long-term wind speed forecast, i.e., the predictive deep Boltzmann machine (PDBM) and corresponding learning algorithm. The proposed deep model forecasts wind speed by analyzing the higher level features abstracted from lower level features of the wind speed data. These automatically learnt features are very informative and appropriate for the prediction. The proposed PDBM is a deep stochastic model that can represent the wind speed very well, and is inspired by two aspects. 1) The stochastic model is suitable to capture the probabilistic characteristics of wind speed. 2) Recent developments in neural networks with deep architectures show that deep generative models have competitive capability to approximate nonlinear and nonsmooth functions. The evaluation of the proposed PDBM model is depicted by both hour-ahead and day-ahead prediction experiments based on real wind speed datasets. The prediction accuracy of the PDBM model outperforms existing methods by more than 10%.","","","10.1109/TSTE.2015.2434387","Macau Science and Technology development fund; UM Multiyear Research; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7145470","Deep Boltzmann machine (DBM);deep learning;time series;wind speed prediction;Deep Boltzmann machine (DBM);deep learning;time series;wind speed prediction","Wind speed;Wind forecasting;Predictive models;Training;Time series analysis;Wind power generation;Machine learning","Boltzmann machines;load forecasting;power system management;stochastic processes;wind power plants","predictive deep Boltzmann machine;multiperiod wind speed forecasting;wind power plants;wind speed prediction;short-term wind speed forecast;long-term wind speed forecast;stochastic model","","67","40","","","","","IEEE","IEEE Journals"
"Learning deep compact descriptor with bagging auto-encoders for object retrieval","H. Guo; J. Wang; H. Lu","National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, Beijing, China, 100190; National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, Beijing, China, 100190; National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, Beijing, China, 100190","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","3175","3179","Content based object retrieval across large scale surveillance video dataset is a significant and challenging task, in which learning an effective compact object descriptor plays a critical role. In this paper, we propose an efficient deep compact descriptor with bagging auto-encoders. Specifically, we take advantage of discriminative CNN to extract efficient deep features, which not only involve rich semantic information but also can filter background noise. Besides, to boost the retrieval speed, auto-encoders are used to map the high-dimensional real-valued CNN features into short binary codes. Considering the instability of auto-encoder, we adopt a bagging strategy to fuse multiple auto-encoders to reduce the generalization error, thus further improving the retrieval accuracy. In addition, bagging is easy for parallel computing, so retrieval efficiency can be guaranteed. Retrieval experimental results on the dataset of 100k visual objects extracted from multi-camera surveillance videos demonstrate the effectiveness of the proposed deep compact descriptor.","","","10.1109/ICIP.2015.7351389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351389","Object retrieval;bagging;auto-encoder","Bagging;Feature extraction;Training;Surveillance;Visualization;Binary codes;Vehicles","binary codes;content-based retrieval;convolution;feature extraction;learning (artificial intelligence);neural nets;video coding;video retrieval;video surveillance","deep compact descriptor learning;bagging autoencoders;content based object retrieval;large scale surveillance video dataset;discriminative CNN;convolutional neural network;deep feature extraction;background noise filtering;high-dimensional real-valued CNN features;short binary codes;100k visual object dataset","","2","17","","","","","IEEE","IEEE Conferences"
"Deep Learning for Imbalanced Multimedia Data Classification","Y. Yan; M. Chen; M. Shyu; S. Chen","Dept. of Electr. & Comput. Eng., Univ. of Miami, Coral Gables, FL, USA; Sch. of Sci., Technol., Eng. & Math., Univ. of Washington Bothell, Bothell, WA, USA; Dept. of Electr. & Comput. Eng., Univ. of Miami, Coral Gables, FL, USA; Sch. of Comput. & Inf. Sci., Florida Int. Univ., Miami, FL, USA","2015 IEEE International Symposium on Multimedia (ISM)","","2015","","","483","488","Classification of imbalanced data is an important research problem as lots of real-world data sets have skewed class distributions in which the majority of data instances (examples) belong to one class and far fewer instances belong to others. While in many applications, the minority instances actually represent the concept of interest (e.g., fraud in banking operations, abnormal cell in medical data, etc.), a classifier induced from an imbalanced data set is more likely to be biased towards the majority class and show very poor classification accuracy on the minority class. Despite extensive research efforts, imbalanced data classification remains one of the most challenging problems in data mining and machine learning, especially for multimedia data. To tackle this challenge, in this paper, we propose an extended deep learning approach to achieve promising performance in classifying skewed multimedia data sets. Specifically, we investigate the integration of bootstrapping methods and a state-of-the-art deep learning approach, Convolutional Neural Networks (CNNs), with extensive empirical studies. Considering the fact that deep learning approaches such as CNNs are usually computationally expensive, we propose to feed low-level features to CNNs and prove its feasibility in achieving promising performance while saving a lot of training time. The experimental results show the effectiveness of our framework in classifying severely imbalanced data in the TRECVID data set.","","","10.1109/ISM.2015.126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442383","classification;deep learning;imbalanced data;semantic indexing;convolutional neural network (CNN)","Machine learning;Multimedia communication;Training;Data models;Classification algorithms;Neural networks;Convolution","convolution;data analysis;learning (artificial intelligence);multimedia computing;neural nets","TRECVID data set;low-level features;deep learning approaches;CNN;convolutional neural networks;banking operations;data instances;real-world data sets;imbalanced multimedia data classification","","28","41","","","","","IEEE","IEEE Conferences"
"Deep independence network analysis of structural brain imaging: A simulation study","E. Castro; D. Hjelm; S. Plis; L. Dinh; J. Turner; V. Calhoun","The Mind Research Network, NM, USA; The Mind Research Network, NM, USA; The Mind Research Network, NM, USA; Université de Montréal, QC, CAN; The Mind Research Network, NM, USA; The Mind Research Network, NM, USA","2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)","","2015","","","1","6","The objective of this paper is to further validate theoretically and empirically a nonlinear independent component analysis (ICA) algorithm implemented with a deep learning architecture. We first revisited its formulation to verify its consistency with the criterion of minimization of mutual information. Then, we applied the nonlinear independent component estimation algorithm (NICE) to synthetic 2D images that resemble structural magnetic resonance imaging (sMRI) data. This data was generated by mixing spatial components that represent axial slices of sMRI tissue concentration images. Next, we generated the images under linear and mildly nonlinear mixtures, being able to show that NICE matches ICA when the data is generated by using the conventional linear mixture and outperforms ICA for the nonlinear mixture of components. The obtained results are promising and suggest that NICE has potential to find richer brain networks if applied to real sMRI data, provided that small conditioning adjustments are performed along with this approach.","","","10.1109/MLSP.2015.7324318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324318","Nonlinear ICA;deep learning;structural MRI;simulation;NICE","Imaging;Jacobian matrices;Couplings;Mutual information;Brain;Independent component analysis;Machine learning","biological tissues;biomedical MRI;brain;independent component analysis;learning (artificial intelligence);medical image processing;minimisation;network analysis;neurophysiology","deep independence network analysis;structural brain imaging;nonlinear independent component analysis;ICA;deep learning architecture;minimization criterion;synthetic 2D imaging;structural magnetic resonance imaging data;MRI data;spatial components;axial slices;MRI tissue concentration imaging;mildly nonlinear mixtures;NICE matches;conventional linear mixture;brain networks","","1","18","","","","","IEEE","IEEE Conferences"
"Object Recognition Base on Deep Belief Network","Y. Zhang; Z. Liu; W. Zhou; Y. Zhang","NA; NA; NA; NA","2015 10th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)","","2015","","","268","273","Event ontology is a general knowledge base constructed by event as the basic knowledge unit for computer communication. Event contains six elements which are action, object, time, environment, assertion and language performance. In this paper, we mainly discuss object elements recognition. There are several mainly existing way to recognize object: methods based on rule, statistical and shallow machine learning. Although these methods can get better recognition results in a particular environment, they have nature defects. For instance, it is difficult for them to do feature extraction and they can not achieve complex function approximation, leading to low recognition accuracy and scalability. Aiming at problems of existing object recognition methods, we present a Chinese emergency object recognition model based on deep learning (CEORM). Firstly, we use word segmentation system (LTP) to segment sentence, and classify words according to annotating elements in CEC2.0 corpus, and then obtain each word's vectorization of multiple features, which include part of speech, dependency grammar, length, location. We obtain word's deep semantic characteristics from the collection after vectorization using deep belief network, finally, object elements are classified and recognized. Extensive testing analysis shows that our proposed method can achieve better recognition effect.","","","10.1109/ISKE.2015.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383058","event ontology;deep learning;event recognition;DBN;CEORM","Semantics;Object oriented modeling;Feature extraction;Training;Object recognition;Machine learning;Grammar","belief networks;learning (artificial intelligence);natural language processing;object recognition","deep belief network;event ontology;action element;object element;time element;environment element;assertion element;language performance element;rule learning;statistical learning;shallow machine learning;Chinese emergency object recognition model;CEORM model;word segmentation system;LTP;CEC2.0 corpus;deep semantic characteristics","","","10","","","","","IEEE","IEEE Conferences"
"Transfer Learning in Hierarchical Feature Spaces","H. Zuo; G. Zhang; V. Behbood; J. Lu; X. Meng","Fac. of Eng. & Inf. Technol., Univ. of Technol., Sydney, NSW, Australia; Fac. of Eng. & Inf. Technol., Univ. of Technol., Sydney, NSW, Australia; Fac. of Eng. & Inf. Technol., Univ. of Technol., Sydney, NSW, Australia; Fac. of Eng. & Inf. Technol., Univ. of Technol., Sydney, NSW, Australia; Coll. of Math. & Inf. Sci., Hebei Univ., Baoding, China","2015 10th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)","","2015","","","183","188","Transfer learning provides an approach to solve target tasks more quickly and effectively by using previously acquired knowledge learned from source tasks. As one category of transfer learning approaches, feature-based transfer learning approaches aim to find a latent feature space shared between source and target domains. The issue is that the sole feature space can't exploit the relationship of source domain and target domain fully. To deal with this issue, this paper proposes a transfer learning method that uses deep learning to extract hierarchical feature spaces, so knowledge of source domain can be exploited and transferred in multiple feature spaces with different levels of abstraction. In the experiment, the effectiveness of transfer learning in multiple feature spaces is compared and this can help us find the optimal feature space for transfer learning.","","","10.1109/ISKE.2015.86","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383046","transfer learning;deep learning;feature extraction","Feature extraction;Machine learning;Data models;Predictive models;Training;Noise reduction;Learning systems","feature extraction;learning (artificial intelligence);neural nets;pattern classification","transfer learning method;hierarchical feature space extraction;feature space classification;source domain;target domain;deep neural network","","2","17","","","","","IEEE","IEEE Conferences"
"Deep Learning for Wind Speed Forecasting in Northeastern Region of Brazil","A. T. Sergio; T. B. Ludermir","NA; NA","2015 Brazilian Conference on Intelligent Systems (BRACIS)","","2015","","","322","327","Deep Learning is one of the latest approaches in the field of artificial neural networks. Since they were first proposed in mid-2006, Deep Learning models have obtained state-of-art results in some problems with classification and pattern recognition. However, such models have been little used in time series forecasting. This work aims to investigate the use of some of these architectures in this kind of problem, specifically in predicting the hourly average speed of winds in the Northeastern region of Brazil. The results showed that Deep Learning offers a good alternative for performing this task, overcoming some results of previous works.","","","10.1109/BRACIS.2015.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424040","deep learning;neural networks;time series forecasting;wind forecasting","Machine learning;Time series analysis;Training;Biological neural networks;Forecasting;Neurons;Predictive models","learning (artificial intelligence);neural nets;power engineering computing;wind power plants","deep learning;wind speed forecasting;Northeastern Region;Brazil;artificial neural networks;time series forecasting","","4","31","","","","","IEEE","IEEE Conferences"
"Tree RE-weighted belief propagation using deep learning potentials for mass segmentation from mammograms","N. Dhungel; G. Carneiro; A. P. Bradley","ACVT, School of Computer Science, The University of Adelaide; ACVT, School of Computer Science, The University of Adelaide; School of ITEE, The University of Queensland","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","","2015","","","760","763","In this paper, we propose a new method for the segmentation of breast masses from mammograms using a conditional random field (CRF) model that combines several types of potential functions, including one that classifies image regions using deep learning. The inference method used in this model is the tree re-weighted (TRW) belief propagation, which allows a learning mechanism that directly minimizes the mass segmentation error and an inference approach that produces an optimal result under the approximations of the TRW formulation. We show that the use of these inference and learning mechanisms and the deep learning potential functions provides gains in terms of accuracy and efficiency in comparison with the current state of the art using the publicly available datasets INbreast and DDSM-BCRP.","","","10.1109/ISBI.2015.7163983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163983","Mammograms;mass segmentation;tree re-weighted belief propagation;Deep learning;Gaussian Mixture model","Training;Mammography;Machine learning;Image segmentation;Shape;Belief propagation;Solid modeling","belief networks;cancer;image segmentation;inference mechanisms;learning (artificial intelligence);mammography;medical image processing;minimisation;tumours","tree re-weighted belief propagation;deep learning potentials;mass segmentation;mammograms;breast masses;conditional random field;CRF model;error minimization;inference approach;TRW formulation;INbreast dataset;DDSM-BCRP dataset","","9","20","","","","","IEEE","IEEE Conferences"
"Deep CCA based super vector for action recognition","D. Cai; F. Su","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","1945","1949","Super vector based feature encoding methods have recently produced state-of-the-art performance in video based action recognition. Inspired by the idea of multi-view super vector (MVSV), we propose a novel global representation, deep canonical correlation analysis based super vector (DCCA-SV), which is composed of shared components and relatively independent components derived from a pair of descriptors. The shared parts are based on the representations learned by DCCA model and the independent parts are constructed by the first and second order statistics of the reconstruction errors. Compared with the existing feature encoding strategies, DCCA-SV takes advantages of deep learning models in describing complex data distribution. It can learn the complex nonlinear transformations of two views of data with a single model, which is more efficient in capturing the overall correlations between feature pairs. Furthermore, in DCCA-SV, there is no need to compute the inner product and matrix multiplication, which is more efficient in real applications. Experiments on Non-Human Primates' (NHPs') surveillance video action recognition dataset show that DCCA-SV achieves promising results compared with state-of-the-art methods.<sup>1</sup>","","","10.1109/ICIP.2015.7351140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351140","Action recognition;feature encoding;deep CCA;super vector;non-human primates","Encoding;Correlation;Computational modeling;Data models;Feature extraction;Histograms;Computational efficiency","correlation methods;feature extraction;image representation;learning (artificial intelligence);object recognition;statistics;video coding;video surveillance","deep CCA based supervector;supervector based feature encoding method;video based action recognition;multiview supervector;MVSV;global representation;deep canonical correlation analysis based supervector;DCCA-SV;shared components;relatively independent components;first order statistics;second order statistics;reconstruction errors;deep learning model;complex data distribution;complex nonlinear transformation;feature pairs;nonhuman primates surveillance video action recognition dataset;NHP","","","16","","","","","IEEE","IEEE Conferences"
"Deep Representations for Software Engineering","M. White","Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","","2015","2","","781","783","Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields. We propose that software engineering (SE) research is a unique opportunity to use these transformative approaches. Our research examines applications of deep architectures such as recurrent neural networks and stacked restricted Boltzmann machines to SE tasks.","","","10.1109/ICSE.2015.248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203069","","Software;Computational modeling;Machine learning;Conferences;Software engineering;Computer architecture;Context","Boltzmann machines;learning (artificial intelligence);recurrent neural nets;software engineering","deep representation;software engineering;deep learning subsumes algorithm;compositional representation learning;transformative approach;deep architecture;recurrent neural networks;stacked restricted Boltzmann machine","","1","45","","","","","IEEE","IEEE Conferences"
"Standard Plane Localization in Fetal Ultrasound via Domain Transferred Deep Neural Networks","H. Chen; D. Ni; J. Qin; S. Li; X. Yang; T. Wang; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Medicine, Shenzhen University, Guangdong, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Medicine, Shenzhen University, Guangdong, China; Department of Ultrasound, Affiliated Shenzhen Maternal and Child Healthcare, Hospital of Nanfang Medical University, Shenzhen, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Medicine, Shenzhen University, Guangdong, China; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Medicine, Shenzhen University, Guangdong, China; Department of Computer Science and Engineering, Chinese University of Hong Kong","IEEE Journal of Biomedical and Health Informatics","","2015","19","5","1627","1636","Automatic localization of the standard plane containing complicated anatomical structures in ultrasound (US) videos remains a challenging problem. In this paper, we present a learning-based approach to locate the fetal abdominal standard plane (FASP) in US videos by constructing a domain transferred deep convolutional neural network (CNN). Compared with previous works based on low-level features, our approach is able to represent the complicated appearance of the FASP and hence achieve better classification performance. More importantly, in order to reduce the overfitting problem caused by the small amount of training samples, we propose a transfer learning strategy, which transfers the knowledge in the low layers of a base CNN trained from a large database of natural images to our task-specific CNN. Extensive experiments demonstrate that our approach outperforms the state-of-the-art method for the FASP localization as well as the CNN only trained on the limited US training samples. The proposed approach can be easily extended to other similar medical image computing problems, which often suffer from the insufficient training samples when exploiting the deep CNN to represent high-level features.","","","10.1109/JBHI.2015.2425041","National Natural Science Foundation of China; Shenzhen Key Basic Research Project; Shenzhen-Hong Kong Innovation Circle Funding Program; Hong Kong Innovation and Technology Fund; Research Grants Council of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7090943","Ultrasound;standard plane;deep learning;domain transfer;knowledge transfer;convolutional neural network;Convolutional neural network (CNN);deep learning;domain transfer;knowledge transfer;standard plane;ultrasound (US)","Training;Videos;Biomedical imaging;Standards;Feature extraction;Dictionaries;Informatics","biomedical ultrasonics;image classification;learning (artificial intelligence);medical image processing;neural nets;object detection;obstetrics","fetal ultrasound;automatic standard plane localization;ultrasound videos;learning-based approach;fetal abdominal standard plane;US videos;domain transferred deep convolutional neural network;low-level features;classification performance;overfitting problem;transfer learning strategy;natural images;task-specific CNN;FASP localization;medical image computing problems;high-level features","Abdomen;Female;Fetus;Humans;Image Processing, Computer-Assisted;Neural Networks (Computer);Pregnancy;ROC Curve;Ultrasonography, Prenatal","124","37","","","","","IEEE","IEEE Journals"
"Multiview Deep Learning for Land-Use Classification","F. P. S. Luus; B. P. Salmon; F. van den Bergh; B. T. J. Maharaj","Dept. of Electr., Univ. of Pretoria, Pretoria, South Africa; Sch. of Eng. & ICT, Univ. of Tasmania, Hobart, TAS, Australia; Meraka Inst., Council for Sci. & Ind. Res., Pretoria, South Africa; Dept. of Electr., Univ. of Pretoria, Pretoria, South Africa","IEEE Geoscience and Remote Sensing Letters","","2015","12","12","2448","2452","A multiscale input strategy for multiview deep learning is proposed for supervised multispectral land-use classification, and it is validated on a well-known data set. The hypothesis that simultaneous multiscale views can improve composition-based inference of classes containing size-varying objects compared to single-scale multiview is investigated. The end-to-end learning system learns a hierarchical feature representation with the aid of convolutional layers to shift the burden of feature determination from hand-engineering to a deep convolutional neural network (DCNN). This allows the classifier to obtain problem-specific features that are optimal for minimizing the multinomial logistic regression objective, as opposed to user-defined features which trade optimality for generality. A heuristic approach to the optimization of the DCNN hyperparameters is used, based on empirical performance evidence. It is shown that a single DCNN can be trained simultaneously with multiscale views to improve prediction accuracy over multiple single-scale views. Competitive performance is achieved for the UC Merced data set, where the 93.48% accuracy of multiview deep learning outperforms the 85.37% accuracy of SIFT-based methods and the 90.26% accuracy of unsupervised feature learning.","","","10.1109/LGRS.2015.2483680","National Research Foundation of South Africa; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307121","Feature extraction;neural network applications;neural network architecture;remote sensing;urban areas;Feature extraction;neural network applications;neural network architecture;remote sensing;urban areas","Accuracy;Machine learning;Storage tanks;Remote sensing;Neurons;Training;Neural networks","geophysical techniques;geophysics computing;land use;learning (artificial intelligence);neural nets;pattern classification","Multiview Deep Learning;multiscale input strategy;supervised multispectral land-use classification;well-known data set;composition-based in- ference;size-varying objects;end-to-end learning sys- tem;hierarchical feature representation;convolutional layers;feature determination;hand-engineering;deep convolutional neural network;problem-specific fea- tures;multinomial logistic re- gression objective;user-defined features;heuristic approach;DCNN hyperparameters optimization;multiple single-scale views;UC Merced data set;SIFT-based methods;unsupervised feature learning","","104","15","","","","","IEEE","IEEE Journals"
"The role of the facilitator in a project/design based learning environment","S. Chandrasekaran; G. Littlefair; A. Stojcevski","School of Engineering, Deakin University, Geelong, Victoria, Australia; School of Engineering, Deakin University, Geelong, Victoria, Australia; Centre of Technology, RMIT University, Ho Chi Minh, Vietnam","2015 International Conference on Interactive Collaborative Learning (ICL)","","2015","","","21","24","The Project Oriented Design Based Learning (PODBL) is a teaching and learning approach (TLA) that is based on engineering design activities undertaken during a project. PODBL encourages independent learning and a deep approach to student learning outcomes. The Centre for Advanced Design in Engineering Training (CADET) is a new engineering building with cutting edge technology facilities at Deakin University. It acts as a catalyst for a pedagogical change in the way that Engineering is delivered. This paper is focused on the role of the facilitator in a project/design based learning environment and it also looks in to appropriate staff development training for a project/design based learning environment.","","","10.1109/ICL.2015.7318224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318224","Project oriented design based learning;Professional development;Students learning outcomes","Engineering education;Industries;Problem-solving;Conferences;Collaborative work","continuing professional development;design engineering;educational institutions;engineering education;teaching","facilitator;project/design based learning environment;project oriented design based learning;PODBL;teaching and learning approach;TLA;engineering design activities;independent learning;student learning outcomes;Centre for Advanced Design in Engineering Training;CADET;engineering building;technology facilities;Deakin University;pedagogical change;staff development training;professional development","","2","19","","","","","IEEE","IEEE Conferences"
"Multi-layer feature extractions for image classification — Knowledge from deep CNNs","K. Ueki; T. Kobayashi","Faculty of Science and Engineering, Waseda University, Room 40-701, Waseda-machi 27, Shinjuku-ku, Tokyo, 162-0042 Japan; Faculty of Science and Engineering, Waseda University, Room 40-701, Waseda-machi 27, Shinjuku-ku, Tokyo, 162-0042 Japan","2015 International Conference on Systems, Signals and Image Processing (IWSSIP)","","2015","","","9","12","Recently, there has been considerable research into the application of deep learning to image recognition. Notably, deep convolutional neural networks (CNNs) have achieved excellent performance in a number of image classification tasks, compared with conventional methods based on techniques such as Bag-of-Features (BoF) using local descriptors. In this paper, to cultivate a better understanding of the structure of CNN, we focus on the characteristics of deep CNNs, and adapt them to SIFT+BoF-based methods to improve the classification accuracy. We introduce the multi-layer structure of CNNs into the classification pipeline of the BoF framework, and conduct experiments to confirm the effectiveness of this approach using a fine-grained visual categorization dataset. The results show that the average classification rate is improved from 52.4% to 69.8%.","","","10.1109/IWSSIP.2015.7313924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313924","Deep learning;Feature extraction;Bag-of-Features;Generic object recognition;Fine-grained visual categorization","Principal component analysis;Feature extraction;Training;Computer vision;Neural networks;Visualization;Conferences","feature extraction;image classification;learning (artificial intelligence);neural nets","multilayer feature extraction;image classification;deep CNN;deep convolutional neural networks;BoF technique;bag-of-features;fine-grained visual categorization dataset","","1","24","","","","","IEEE","IEEE Conferences"
"An Associative Generated Model for Multi-signals Based on Deep Learning","D. Guo; Y. Hao; M. Liu","Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun, China; Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun, China; Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun, China","2015 7th International Conference on Intelligent Human-Machine Systems and Cybernetics","","2015","2","","280","283","During exploring the emergence of language, we found that the brain can extract some common features from the same thing in different representations by pattern recognition and association. Consequently, the brain would establish a connection for identical concept from multi-signals. An associative generated model primarily based on Restricted Boltzmann Machine (RBM) and Deep Belief Network (DBN) is set up for multi-signals to simulate the brain's ability. The first step is to use the DBNs for extracting features from multiple input signal sources. The second is using top-level RBM to achieve the goal of associating and generating mutually by fusing each feature. Finally, we verify the feasibility of the model through the realization of generating Arabic digital pictures and Chinese characters digital images reciprocally.","","","10.1109/IHMSC.2015.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334969","association;generation;feature extraction;Deep Belief network","Feature extraction;Brain modeling;Biological neural networks;Computational modeling;Digital images;Training;Yttrium","belief networks;Boltzmann machines;brain;character recognition;feature extraction;learning (artificial intelligence);natural language processing","feature extraction;pattern recognition;pattern association;associative generated model;restricted Boltzmann machine;deep belief network;multisignals;brain ability;DBN;input signal sources;top-level RBM;Arabic digital picture generation;Chinese character digital images","","","12","","","","","IEEE","IEEE Conferences"
"Video Tracking Using Learned Hierarchical Features","L. Wang; T. Liu; G. Wang; K. L. Chan; Q. Yang","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Computer Science, City University of Hong Kong, Hong Kong","IEEE Transactions on Image Processing","","2015","24","4","1424","1435","In this paper, we propose an approach to learn hierarchical features for visual object tracking. First, we offline learn features robust to diverse motion patterns from auxiliary video sequences. The hierarchical features are learned via a two-layer convolutional neural network. Embedding the temporal slowness constraint in the stacked architecture makes the learned features robust to complicated motion transformations, which is important for visual object tracking. Then, given a target video sequence, we propose a domain adaptation module to online adapt the pre-learned features according to the specific target object. The adaptation is conducted in both layers of the deep feature learning module so as to include appearance information of the specific target object. As a result, the learned hierarchical features can be robust to both complicated motion transformations and appearance changes of target objects. We integrate our feature learning algorithm into three tracking methods. Experimental results demonstrate that significant improvement can be achieved using our learned hierarchical features, especially on video sequences with complicated motion transformations.","","","10.1109/TIP.2015.2403231","Ministry of Education (MOE) Tier 1; MOE Tier 2; Agency for Science, Technology and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7041176","Object tracking;deep feature learning;domain adaptation;Object tracking;deep feature learning;domain adaptation","Target tracking;Visualization;Object tracking;Video sequences;Robustness;Feature extraction","feature extraction;image motion analysis;image sequences;learning (artificial intelligence);neural nets;object tracking;video signal processing","video tracking;visual object tracking;diverse motion pattern;video sequence;two-layer convolutional neural network;complicated motion transformation;deep feature learning module;hierarchical feature learning algorithm","","88","63","","","","","IEEE","IEEE Journals"
"The hidden layer design for staked denoising autoencoder","Qianqian Hao; Hua Zhang; Jinkou Ding","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China; Beijing School of Science, Beijing University of Posts and Telecommunications, China","2015 12th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)","","2015","","","150","153","Deep learning can achieve the complex function approximation and the characteristics of the input data by studying a deep nonlinear network. At present, one of the most important problems in the study of deep learning is how to construct a reasonable structure. This paper studies the deep learning model of stacked denoising autoencoder (SDA) and the remaining task is to construct its reasonable model. We introduce three effective methods to construct the structure of the SDA. Numerical experiments imply that the structure obtained by the golden section principle performs the best.","","","10.1109/ICCWAMTIP.2015.7493964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493964","Deep Learning;SDA;golden section;Hidden Layer;nonlinear network","Machine learning;Mathematical model;Training;Data models;Noise reduction;Numerical models;Neural networks","learning (artificial intelligence);neural nets","hidden layer design;staked denoising autoencoder;complex function approximation;input data characteristics;deep nonlinear network;deep learning model;stacked denoising autoencoder;SDA structure;golden section principle;multilayer neural network","","","12","","","","","IEEE","IEEE Conferences"
"Collaborative learning and virtual laboratories: A new teaching and learning model in the International Telematic University UNINETTUNO's 3D Island of Knowledge","","","2015 International Conference on Interactive Collaborative Learning (ICL)","","2015","","","1194","1197","This paper illustrates the results of research activities on collaborative learning and the virtual laboratories that are available in the International Telematic University UNINETTUNO's platform. In particular, the interaction models, created in UNINETTUNO' Island of Knowledge where the avatars of teachers and students contribute to realize environments characterized by a strong sense of reality that promote collaborative learning and learning-by-doing, are analyzed. Concrete applications, such as virtual laboratories controlled from remote and through smart phones, based on a deep study of the complex interrelations existing between communication technologies and cognitive models and between technologies and the theoretical models on which distance teaching and learning processes rely on, are illustrated. Additionally, I describe the UNINETTUNO University's tridimensional virtual laboratories that are realized in the framework of international research programs allowing to connect with other laboratories worldwide as well as to redefine the geography itself of research and innovation.","","","10.1109/ICL.2015.7318205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318205","e-learning;distance teaching;virtual laboratories;remote laboratories","Laboratories;Collaborative work;Smart phones;Education;Avatars;Solid modeling;Conferences","avatars;computer aided instruction;educational institutions;groupware;teaching","learning model;UNINETTUNO 3D island of knowledge;research activities;collaborative learning;International Telematic University;UNINETTUNO platform;interaction models;avatars;learning-by-doing;smart phones;communication technologies;cognitive models;distance teaching;learning processes;tridimensional virtual laboratories;international research programs","","","","","","","","IEEE","IEEE Conferences"
"A 1 TOPS/W Analog Deep Machine-Learning Engine With Floating-Gate Storage in 0.13 µm CMOS","J. Lu; S. Young; I. Arel; J. Holleman","Electrical Engineering and Computer Science, The University of Tennessee, Knoxville, United States; Electrical Engineering and Computer Science, The University of Tennessee, Knoxville, United States; Electrical Engineering and Computer Science, The University of Tennessee, Knoxville, United States; Electrical Engineering and Computer Science, The University of Tennessee, Knoxville, United States","IEEE Journal of Solid-State Circuits","","2015","50","1","270","281","An analog implementation of a deep machine-learning system for efficient feature extraction is presented in this work. It features online unsupervised trainability and non-volatile floating-gate analog storage. It utilizes a massively parallel reconfigurable current-mode analog architecture to realize efficient computation, and leverages algorithm-level feedback to provide robustness to circuit imperfections in analog signal processing. A 3-layer, 7-node analog deep machine-learning engine was fabricated in a 0.13 μm standard CMOS process, occupying 0.36 mm 2 active area. At a processing speed of 8300 input vectors per second, it consumes 11.4 μW from the 3 V supply, achieving 1×10 12 operation per second per Watt of peak energy efficiency. Measurement demonstrates real-time cluster analysis, and feature extraction for pattern recognition with 8-fold dimension reduction with an accuracy comparable to the floating-point software simulation baseline.","","","10.1109/JSSC.2014.2356197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6919341","Analog signal processing;current mode arithmetic;deep machine learning;floating gate;neuromorphic engineering;translinear circuits","Feature extraction;Logic gates;Training;Computer architecture;Tunneling;Engines;Learning systems","CMOS digital integrated circuits;feature extraction;pattern clustering;random-access storage;real-time systems;unsupervised learning","floating-point software simulation baseline;8-fold dimension reduction;pattern recognition;real-time cluster analysis;analog signal processing;algorithm-level feedback;massively parallel reconfigurable current-mode analog architecture;nonvolatile floating-gate analog storage;online unsupervised trainability;feature extraction;standard CMOS process;deep machine-learning engine;size 0.13 mum;power 11.4 muW;voltage 3 V","","46","30","","","","","IEEE","IEEE Journals"
"Deep Learning and Music Adversaries","C. Kereliuk; B. L. Sturm; J. Larsen","DTU Compute, The Technical University of Denmark, Frederiksberg, Denmark; School of Electronic Engineering and Computer Science Eng. 111, Queen Mary University of London, London, United Kingdom; Richard Petersens Plads, Denmark’s Technical University, Building 321, Kogens Lyngby, Denmark","IEEE Transactions on Multimedia","","2015","17","11","2059","2071","An adversary is an agent designed to make a classification system perform in some particular way, e.g., increase the probability of a false negative. Recent work builds adversaries for deep learning systems applied to image object recognition, exploiting the parameters of the system to find the minimal perturbation of the input image such that the system misclassifies it with high confidence. We adapt this approach to construct and deploy an adversary of deep learning systems applied to music content analysis. In our case, however, the system inputs are magnitude spectral frames, which require special care in order to produce valid input audio signals from network- derived perturbations . For two different train-test partitionings of two benchmark datasets, and two different architectures , we find that this adversary is very effective. We find that convolutional architectures are more robust compared to systems based on a majority vote over individually classified audio frames. Furthermore , we experiment with a new system that integrates an adversary into the training loop, but do not find that this improves the resilience of the system to new adversaries.","","","10.1109/TMM.2015.2478068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7254179","AEA-MIR content-based processing and music information retrieval;deep learning","Machine learning;Computer architecture;Training;Benchmark testing;Rhythm;Neural networks","audio signal processing;content management;convolution;image classification;information retrieval;learning (artificial intelligence);music;object recognition","classification system;deep learning system;image object recognition;music content analysis;magnitude spectral frame;audio signal;network derived perturbation;convolutional architecture;audio frame classification;training loop;music adversary","","29","65","","","","","IEEE","IEEE Journals"
"Controlling the hidden layers' output to optimizing the training process in the Deep Neural Network algorithm","Andreas; M. H. Purnomo; M. Hariadi","Information System, Universitas Pelita Harapan Surabaya, Surabaya, Indonesia; Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Electrical Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)","","2015","","","1028","1032","Deep learning is one of the most recent development form of Artificial Neural Network (ANN) in machine learning. Deep Neural Network (DNN) algorithm is usually used in image and speech recognition applications. As the development of Artificial Neural Network, very possible there are so many hidden layers in Deep Neural Network. In DNN, the output of each node is a quadratic function of its inputs. The DNN training process is very difficult. In this paper, we try to optimizing the training process by slightly construct of the deep architecture and combines several existing algorithms. Output's error of each unit in the previous layer will be calculated. The weight of the unit with the smallest error will be maintained in the next iteration. This paper uses MNIST handwriting images as its data training and data test. After doing some tests, it can be concluded that the optimization by selecting any output in each hidden layer, the DNN training process will be faster approximately 8%.","","","10.1109/CYBER.2015.7288086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7288086","machine learning;deep neural network;image and voice recognition","Training;Artificial neural networks;Computer architecture;Speech recognition;Image recognition;Backpropagation","handwriting recognition;learning (artificial intelligence);neural nets","DNN training process;MNIST handwriting images;deep neural network algorithm","","","16","","","","","IEEE","IEEE Conferences"
"Deep neural network for face recognition based on sparse autoencoder","Z. Zhang; J. Li; R. Zhu","Department of Computer Science and Technology, Nanjing University, Nanjing, China; Faculty of Printing, Packaging Engineering and Digital Media Technology, Xi'an University of Technology, Xi'an, China; Faculty of Printing, Packaging Engineering and Digital Media Technology, Xi'an University of Technology, Xi'an, China","2015 8th International Congress on Image and Signal Processing (CISP)","","2015","","","594","598","Face recognition is a very important research topic in computer vision because of its many potential applications. In this paper, we investigated a face recognition method based on deep neural network. The sparse coding neural network and the softmax classifiers were used in this paper to build and train the deep hierarchical network after the face image preprocessing. The method is evaluated on the ORL, Yale, Yale-B and PERET face database, respectively. The experimental results show that the deep learning method can abstractly express the original data with efficiency and accuracy, and achieve a good performance in the conditions of illumination, expression, posture and low resolution.","","","10.1109/CISP.2015.7407948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407948","deep learning;sparse coding;face recognition;softmax classifier","Face recognition;Machine learning;Classification algorithms;Biological neural networks;Face;Feature extraction","computer vision;face recognition;neural nets;visual databases","deep neural network;sparse autoencoder;computer vision;face recognition method;sparse coding neural network;softmax classifiers;face image preprocessing;deep hierarchical network;ORL;Yale-B;PERET face database","","4","15","","","","","IEEE","IEEE Conferences"
"Geometry-Aware Deep Transform","J. Huang; Q. Qiu; R. Calderbank; G. Sapiro","Duke Univ., Durham, NC, USA; Duke Univ., Durham, NC, USA; Duke Univ., Durham, NC, USA; Duke Univ., Durham, NC, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","4139","4147","Many recent efforts have been devoted to designing sophisticated deep learning structures, obtaining revolutionary results on benchmark datasets. The success of these deep learning methods mostly relies on an enormous volume of labeled training samples to learn a huge number of parameters in a network; therefore, understanding the generalization ability of a learned deep network cannot be overlooked, especially when restricted to a small training set, which is the case for many applications. In this paper, we propose a novel deep learning objective formulation that unifies both the classification and metric learning criteria. We then introduce a geometry-aware deep transform to enable a non-linear discriminative and robust feature transform, which shows competitive performance on small training sets for both synthetic and real-world data. We further support the proposed framework with a formal (K, ϵ)-robustness analysis.","","","10.1109/ICCV.2015.471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410828","","Training;Measurement;Transforms;Testing;Robustness;Machine learning;Neural networks","feature extraction;generalisation (artificial intelligence);geometry;image classification;learning (artificial intelligence);transforms","geometry-aware deep transform;deep learning structures;deep learning methods;generalization ability;deep learning objective formulation;metric learning criteria;nonlinear discriminative transform;robust feature transform;formal (K,ϵ)-robustness analysis","","4","18","","","","","IEEE","IEEE Conferences"
"Tutorial 1: NVIDIA's platform for Deep Neural Networks","G. Röth","Solution Architect @ NVIDIA","2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)","","2015","","","XXXVII","XXXIX","Summary form only given. In that session, we will present the NVIDIA's platform for Deep Neural Networks (DNN). NVIDIA designs and produces GPUs and processors that are at the origin of breakthroughs in Deep Learning. NVIDIA also develops CuDNN, a library of primitives for Deep Learning which is integrated in leading frameworks like Theano, Torch and Caffe and DIGITS, an interactive Deep Learning GPU Training System. During that session, you will learn more about those different components to help you make your DNN more powerful.","","","10.1109/DSAA.2015.7344778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344778","","Tutorials;Machine learning;Machine learning algorithms;Neural networks;Training;Biological system modeling;Computer science","graphics processing units;neural nets","NVIDIA;deep neural networks;GPUs;CuDNN;Theano;Torch;Caffe;DIGITS;interactive deep learning GPU training system","","","","","","","","IEEE","IEEE Conferences"
"The Potential of the Intel (R) Xeon Phi for Supervised Deep Learning","A. Viebke; S. Pllana","Dept. of Comput. Sci., Linnaeus Univ., Vaxjo, Sweden; Dept. of Comput. Sci., Linnaeus Univ., Vaxjo, Sweden","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","758","765","Supervised learning of Convolutional Neural Networks (CNNs), also known as supervised Deep Learning, is a computationally demanding process. To find the most suitable parameters of a network for a given application, numerous training sessions are required. Therefore, reducing the training time per session is essential to fully utilize CNNs in practice. While numerous research groups have addressed the training of CNNs using GPUs, so far not much attention has been paid to the Intel Xeon Phi coprocessor. In this paper we investigate empirically and theoretically the potential of the Intel Xeon Phi for supervised learning of CNNs. We design and implement a parallelization scheme named CHAOS that exploits both the thread-and SIMD-parallelism of the coprocessor. Our approach is evaluated on the Intel Xeon Phi 7120P using the MNIST dataset of handwritten digits for various thread counts and CNN architectures. Results show a 103.5x speed up when training our large network for 15 epochs using 244 threads, compared to one thread on the coprocessor. Moreover, we develop a performance model and use it to assess our implementation and answer what-if questions.","","","10.1109/HPCC-CSS-ICESS.2015.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336249","Many-core;Parallel Computing;Intel Xeon Phi;Supervised Deep Learning;Convolutional Neural Networks","Training;Coprocessors;Neurons;Chaos;Biological neural networks;Instruction sets;Machine learning","graphics processing units;learning (artificial intelligence);neural nets;parallel processing","Intel Xeon Phi 7120P;thread-parallelism;SIMD-parallelism;CHAOS;parallelization scheme;Intel Xeon Phi coprocessor;GPU;CNN;convolutional neural network;supervised deep learning","","19","33","","","","","IEEE","IEEE Conferences"
"Multi-view deep learning for image-based pose recovery","C. Hong; J. Yu; Yong Xie; X. Chen","Xiamen University of Technology, 361024, China; Hangzhou Dianzi University, 310018, China; Xiamen University of Technology, 361024, China; Xiamen University of Technology, 361024, China","2015 IEEE 16th International Conference on Communication Technology (ICCT)","","2015","","","897","902","Image-based human pose recovery is usually conducted by retrieving relevant poses with image features. However, semantic gap exists for current feature extractors, which limits recovery performance. In this paper, we propose a novel method to recover 3D human poses from silhouettes. It is based on multiple feature fusion and deep learning. First, to fuse different types of features, we introduce manifold alignment with hypergraph Laplacian. Hypergraph Laplacian matrix is constructed with patch alignment framework. Second, multi-view description is applied to deep neural networks. In this way, the non-linear mapping from 2D images to 3D poses is learned and pose recovery can be achieved. Experimental results on the widely-used Human3.6m dataset show that the recovery error has been reduced by 10% to 20%, which demonstrates the effectiveness of the proposed method.","","","10.1109/ICCT.2015.7399969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7399969","Pose recovery;Multi-view fusion;Manifold alignment;Deep learning","","feature extraction;image fusion;image retrieval;Laplace equations;learning (artificial intelligence);matrix algebra;neural nets;pose estimation","multiview deep learning;image-based human pose recovery;relevant pose retrieval;image features;feature extractors;3D human pose recovery;silhouettes;multiple feature fusion;manifold alignment;hypergraph Laplacian matrix;patch alignment framework;multiview description;deep neural networks;nonlinear mapping;2D images;Human3.6m dataset;recovery error reduction","","2","21","","","","","IEEE","IEEE Conferences"
"Automatic fusion and classification using random forests and features extracted with deep learning","A. Merentitis; C. Debes","AGT International, 64295 Darmstadt, Germany; AGT International, 64295 Darmstadt, Germany","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","2943","2946","Fusion of different sensor modalities has proven very effective in numerous remote sensing applications. However, in order to benefit from fusion, advanced feature extraction mechanisms that rely on domain expertise are typically required. In this paper we present an automated feature extraction scheme based on deep learning. The feature extraction is unsupervised and hierarchical. Furthermore, computational efficiency (often a challenge for deep learning methods) is a primary goal in order to make certain that the method can be applied in large remote sensing datasets. Promising classification results show the applicability of the approach for both reducing the gap between naive feature extraction and methods relying on domain expertise, as well as further improving the performance of the latter in two challenging datasets.","","","10.1109/IGARSS.2015.7326432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326432","","Feature extraction;Machine learning;Laser radar;Hyperspectral imaging;Data integration;Correlation","feature extraction;geophysical image processing;image classification;image fusion;remote sensing by laser beam","automatic fusion;automatic classification;random forests;deep learning feature extraction;sensor modality fusion;remote sensing applications;advanced feature extraction mechanisms;automated feature extraction scheme;remote sensing datasets","","6","7","","","","","IEEE","IEEE Conferences"
"Does diversity improve deep learning?","R. F. Alvear-Sandoval; A. R. Figueiras-Vidal","GAMMA-L+/DTSC, Universidad Carlos III de Madrid; GAMMA-L+/DTSC, Universidad Carlos III de Madrid","2015 23rd European Signal Processing Conference (EUSIPCO)","","2015","","","2496","2500","In this work, we carry out a first exploration of the possibility of increasing the performance of Deep Neural Networks (DNNs) by applying diversity techniques to them. Since DNNs are usually very strong, weakening them can be important for this purpose. This paper includes experimental evidence of the effectiveness of binarizing multi-class problems to make beneficial the application of bagging to Denoising Auto-Encoding-Based DNNs for solving the classical MNIST problem. Many research opportunities appear following the diversification idea: We mention some of the most relevant lines at the end of this contribution.","","","10.1109/EUSIPCO.2015.7362834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362834","Auto-encoding;classification;depth;diversity","Training;Bagging;Error analysis;Standards;Europe;Signal processing;Neural networks","database management systems;learning (artificial intelligence);neural nets","deep learning;deep neural network;diversity technique;multiclass problem binarization;bagging application;auto-encoding-based DNN denoising application;MNIST problem","","2","30","","","","","IEEE","IEEE Conferences"
"Learning to Detect Saliency with Deep Structure","Y. Hu; Z. Chen; Z. Chi; H. Fu","Dept. of Electron. & Inf. Eng., Hong Kong Polytech. Univ., Hong Kong, China; Dept. of Electron. & Inf. Eng., Hong Kong Polytech. Univ., Hong Kong, China; Dept. of Electron. & Inf. Eng., Hong Kong Polytech. Univ., Hong Kong, China; Dept. of Comput. Sci., Chu Hai Coll. of Higher Educ., Hong Kong, China","2015 IEEE International Conference on Systems, Man, and Cybernetics","","2015","","","1770","1775","Deep learning has shown great successes in solving various problems of computer vision. To the best of our knowledge, however, little existing work applies deep learning to saliency modeling. In this paper, a new saliency model based on convolutional neural network is proposed. The proposed model is able to produce a saliency map directly from an image's pixels. In the model, multi-level output values are adopted to simulate continuous values in a saliency map. Differing from most neural networks that use a relatively small number of output nodes, the output layer of our model has a large number of nodes. To make the training more efficient, an improved learning algorithm is adopted to train the model. Experimental results show that the proposed model succeeds in generating acceptable saliency maps after proper training.","","","10.1109/SMC.2015.310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379442","saliency detection;saliency map;deep learning;convolutional neural network","Neural networks;Training;Visualization;Computational modeling;Kernel;Feature extraction;Image resolution","image resolution;learning (artificial intelligence);neural nets;object detection","saliency detection;deep learning;computer vision;saliency modeling;convolutional neural network;image pixels;saliency map;improved learning algorithm","","3","25","","","","","IEEE","IEEE Conferences"
"Investigating deep learning for fNIRS based BCI","J. Hennrich; C. Herff; D. Heger; T. Schultz","Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany; Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany; Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany; Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","2844","2847","Functional Near infrared Spectroscopy (fNIRS) is a relatively young modality for measuring brain activity which has recently shown promising results for building Brain Computer Interfaces (BCI). Due to its infancy, there are still no standard approaches for meaningful features and classifiers for single trial analysis of fNIRS. Most studies are limited to established classifiers from EEG-based BCIs and very simple features. The feasibility of more complex and powerful classification approaches like Deep Neural Networks has, to the best of our knowledge, not been investigated for fNIRS based BCI. These networks have recently become increasingly popular, as they outperformed conventional machine learning methods for a variety of tasks, due in part to advances in training methods for neural networks. In this paper, we show how Deep Neural Networks can be used to classify brain activation patterns measured by fNIRS and compare them with previously used methods.","","","10.1109/EMBC.2015.7318984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318984","","Training;Accuracy;Biological neural networks;Feature extraction;Standards;Yttrium","biomedical optical imaging;brain-computer interfaces;electroencephalography;feature extraction;infrared spectroscopy;learning (artificial intelligence);medical signal processing;neural nets;neurophysiology;signal classification","deep learning;fNIRS based BCI;functional near infrared spectroscopy;brain activity measurement;young modality;brain-computer interfaces;single trial analysis features;single trial analysis classifiers;EEG-based BCIs;deep neural networks;conventional machine learning methods;brain activation pattern classification","Brain;Brain-Computer Interfaces;Machine Learning;Neural Networks (Computer);Spectroscopy, Near-Infrared","16","23","","","","","IEEE","IEEE Conferences"
"Discrimination of ADHD children based on Deep Bayesian Network","A. J. Hao; B. L. He; C. H. Yin","Tongji University, Department of Computer Science and Technology; Tongji University, Department of Computer Science and Technology; Tongji University, Department of Computer Science and Technology","2015 IET International Conference on Biomedical Image and Signal Processing (ICBISP 2015)","","2015","","","1","6","Attention deficit hyperactivity disorder (ADHD) is a threat for the public health all the time, so the effective discrimination of it is significant and meaningful. In current research, Functional Magnetic Resonance Imaging (fMRI) data has become a popular tool for the analysis of ADHD. In this paper, we introduce the Deep Bayesian Network, a combination of Deep Belief Network and Bayesian Network, to classify the ADHD children from the normal. In Deep Bayesian Network, The Deep Belief Network is applied to normalize and reduce dimension of the fMRI data in every brodmann area. And the Bayesian Network is used to extract the feature of relationships between several well-performed brain areas by structure learning. According to the information of structure and probability in Bayesian Network, we predicted the subjects as control, combined, inattentive or hyperactive using SVM classifier. The final results perform better than using single Deep Belief Network and the best results in ADHD-200 competition.","","","10.1049/cp.2015.0764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7450340","ADHD;Deep Learning;Bayesian Network;SVM","","Bayes methods;belief networks;biomedical MRI;image classification;medical disorders;medical image processing;neurophysiology;paediatrics;support vector machines","ADHD children;deep Bayesian network;attention deficit hyperactivity disorder;functional MRI;magnetic resonance imaging;fMRI data;ADHD analysis;deep belief network;brain area;structure learning;SVM classifier;support vector machine","","1","","","","","","IET","IET Conferences"
"A deep-learning approach to facial expression recognition with candid images","W. Li; M. Li; Z. Su; Z. Zhu","CUNY City College; Alibaba. Inc; IBM China Research Lab; CUNY Graduate Center and City College","2015 14th IAPR International Conference on Machine Vision Applications (MVA)","","2015","","","279","282","To recognize facial expression from candid, non-posed images, we propose a deep-learning based approach using convolutional neural networks (CNNs). In order to evaluate the performance in real-time candid facial expression recognition, we have created a candid image facial expression (CIFE) dataset, with seven types of expression in more than 10,000 images gathered from the Web. As baselines, two feature-based approaches (LBP+SVM, SIFT+SVM) are tested on the dataset. The structure of our proposed CNN-based approach is described, and a data augmentation technique is provided in order to generate sufficient number of training samples. The performance using the feature-based approaches is close to the state of the art when tested with standard datasets, but fails to function well when dealing with candid images. Our experiments show that the CNN-based approach is very effective in candid image expression recognition, significantly outperforming the baseline approaches, by a 20% margin.","","","10.1109/MVA.2015.7153185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153185","","Face recognition;Image recognition;Training;Face;Neural networks;Feature extraction;Computational modeling","face recognition;feature extraction;learning (artificial intelligence);neural nets","deep-learning approach;convolutional neural network;CNN;candid image facial expression recognition;CIFE recognition;feature-based approach;data augmentation technique","","20","19","","","","","IEEE","IEEE Conferences"
"License Plate Detection Based on Sparse Auto-Encoder","R. Yang; H. Yin; X. Chen","Dept. of Electron. Eng. & Inf. Sci., Univ. of Sci. & Technol. of China, Hefei, China; Dept. of Electron. Eng. & Inf. Sci., Univ. of Sci. & Technol. of China, Hefei, China; Dept. of Electron. Eng. & Inf. Sci., Univ. of Sci. & Technol. of China, Hefei, China","2015 8th International Symposium on Computational Intelligence and Design (ISCID)","","2015","2","","465","469","In modern society, automatic license plate recognition (ALPR) plays an important role in the field of Intelligent Transport Systems (ITS). In order to recognize the license plate efficiently, the location of the license plate must be detected first. In consequence, the detection of the license plate becomes a crucial stage in an ALPR system, affecting the performance of the whole system enormously. In this paper, we propose a novel method based on Sparse Auto-Encoder (SAE) to detect the vehicle license plate. The proposed method consists of three main stages: (1) A block-based image segmentation technique used for dividing the image into several blocks. (2) Deep learning model (SAE) trained for candidate block selection. (3) Accurate extraction of the license plate. Unlike other existing license plate detection methods, the proposed algorithm use a deep learning model to learn the features of the license plate. Experiment results demonstrate that our method can detect various types of license plates with a high accuracy and a relatively short running time.","","","10.1109/ISCID.2015.151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7469174","license plate detection;deep learning;Sparse AutoEncoder","Licenses;Feature extraction;Vehicles;Image segmentation;Image color analysis;Training;Machine learning","automobiles;compressed sensing;feature extraction;image classification;image coding;image segmentation;intelligent transportation systems;learning (artificial intelligence);object detection","sparse auto-encoder;automatic license plate recognition;intelligent transport systems;ITS;license plate location detection;ALPR system;SAE;block-based image segmentation technique;deep-learning model;block selection;license plate extraction;feature learning","","2","15","","","","","IEEE","IEEE Conferences"
"A deep convolutional neural wavelet network to supervised Arabic letter image classification","S. Hassairi; R. Ejbali; M. Zaied","REGIM-Lab: REsearch Groups in Intelligent Machines, University of Sfax, National Engineering School of Sfax (ENIS), BP 1173, 3038, Tunisia; REGIM-Lab: REsearch Groups in Intelligent Machines, University of Sfax, National Engineering School of Sfax (ENIS), BP 1173, 3038, Tunisia; REGIM-Lab: REsearch Groups in Intelligent Machines, University of Sfax, National Engineering School of Sfax (ENIS), BP 1173, 3038, Tunisia","2015 15th International Conference on Intelligent Systems Design and Applications (ISDA)","","2015","","","207","212","In this paper, a new approach to supervised image classification is suggested. It's conducted by the combination of two techniques of learning: the wavelet network and the deep learning. This new approach consists of performing the classification of one class versus all the other classes of the dataset by the reconstruction of a convolutional deep neural wavelet network. This network is obtained using a series of stacked auto-encoders and a linear classifier. Finally, a local contrast normalization and an intelligent pooling are applied to our network. The experimental test of our approach performed on Arabic Printed Text Image (APTI) dataset demonstrates that our model is remarkably efficient for image classification compared to a known classifier.","","","10.1109/ISDA.2015.7489226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489226","Wavelet Network;Deep learning;Stacked Auto-Encoders;Convolutional Neural Network","Neurons;Image resolution;Semantics","image classification;learning (artificial intelligence);natural language processing;wavelet neural nets","deep convolutional neural wavelet network;supervised Arabic letter image classification;stacked auto-encoders;linear classifier;Arabic printed text image dataset;APTI","","6","20","","","","","IEEE","IEEE Conferences"
"Deep neural network based protein-protein interaction extraction from biomedical literature","Zhehuan Zhao; Zhihao Yang; Ling Luo; Hongfei Lin; Jian Wang; Song Gao","College of Computer Science and Technology, Dalian University of Technology, 116024, China; College of Computer Science and Technology, Dalian University of Technology, 116024, China; College of Computer Science and Technology, Dalian University of Technology, 116024, China; College of Computer Science and Technology, Dalian University of Technology, 116024, China; College of Computer Science and Technology, Dalian University of Technology, 116024, China; Department of pharmacy, First Affiliated Hospital of Dalian Medical University, 116023, China","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","1156","1156","This paper presents a deep neural network-based protein-protein interactions (PPIs) information extraction approach which can learn complex and abstract features automatically from unlabeled data by unsupervised representation learning methods. This approach first employs the training algorithm of auto-encoders to initialize the parameters of a deep multilayer neural network. Then the gradient descent method using back-propagation is applied to train this deep multilayer neural network model. Experimental results on five public PPI corpora show that our method can achieve better performance than can a multilayer neural network. In addition, the performance comparison with APG also verifies the effectiveness of our method.","","","10.1109/BIBM.2015.7359845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359845","Deep learning;biomedical text mining;interaction extraction;neural network","Data mining;Biological system modeling;Proteins;Protein engineering;Nonhomogeneous media;Neural networks","backpropagation;medical computing;molecular biophysics;neural nets;proteins","deep neural network-based PPI extraction;protein-protein interaction;biomedical literature;PPI information extraction approach;unsupervised representation learning method;training algorithm;deep multilayer neural network;gradient descent method;backpropagation;public PPI corpora","","","6","","","","","IEEE","IEEE Conferences"
"Modeling local and global deformations in Deep Learning: Epitomic convolution, Multiple Instance Learning, and sliding window detection","G. Papandreou; I. Kokkinos; P. Savalle","Google, USA; CentraleSupélec and INRIA, France; CentraleSupélec and INRIA, France","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","390","399","Deep Convolutional Neural Networks (DCNNs) achieve invariance to domain transformations (deformations) by using multiple `max-pooling' (MP) layers. In this work we show that alternative methods of modeling deformations can improve the accuracy and efficiency of DCNNs. First, we introduce epitomic convolution as an alternative to the common convolution-MP cascade of DCNNs, that comes with the same computational cost but favorable learning properties. Second, we introduce a Multiple Instance Learning algorithm to accommodate global translation and scaling in image classification, yielding an efficient algorithm that trains and tests a DCNN in a consistent manner. Third we develop a DCNN sliding window detector that explicitly, but efficiently, searches over the object's position, scale, and aspect ratio. We provide competitive image classification and localization results on the ImageNet dataset and object detection results on Pascal VOC2007.","","","10.1109/CVPR.2015.7298636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298636","","Convolution;Training;Computational modeling;Accuracy;Deformable models;Data structures","image classification;learning (artificial intelligence);neural nets;object detection","local deformation;global deformation;deep learning;epitomic convolution;sliding window detection;deep convolutional neural networks;DCNNs;domain transformations;max-pooling layers;MP layers;deformation modeling;multiple instance learning algorithm;DCNN sliding window detector;competitive image classification;localization results;ImageNet dataset;object detection;Pascal VOC2007","","40","45","","","","","IEEE","IEEE Conferences"
"A nonmonotone learning rate strategy for SGD training of deep neural networks","N. S. Keskar; G. Saon","Northwestern University, Evanston, IL, USA; IBM T.J. Watson Research Center, Yorktown Heights, NY, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4974","4978","The algorithm of choice for cross-entropy training of deep neural network (DNN) acoustic models is mini-batch stochastic gradient descent (SGD). One of the important decisions for this algorithm is the learning rate strategy (also called stepsize selection). We investigate several existing schemes and propose a new learning rate strategy which is inspired by nonmonotone linesearch techniques in nonlinear optimization and the NewBob algorithm. This strategy was found to be relatively insensitive to poorly tuned parameters and resulted in lower word error rates compared to Newbob on two different LVCSR tasks (English broadcast news transcription 50 hours and Switchboard telephone conversations 300 hours). Further, we discuss some justifications for the method by briefly linking it to results in optimization theory.","","","10.1109/ICASSP.2015.7178917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178917","deep learning;speech recognition;stepsize;learning rate;nonmonotonicity","Training;Artificial neural networks","acoustic signal processing;entropy;gradient methods;learning (artificial intelligence);nonlinear programming;speech recognition;stochastic processes","automatic speech recognition system;word error rate;NewBob algorithm;nonlinear optimization;nonmonotone linesearch technique;mini-batch stochastic gradient descent;(DNN) acoustic models;deep neural network SGD training;nonmonotone learning rate strategy","","3","22","","","","","IEEE","IEEE Conferences"
"A Deep Awareness Framework for Pervasive Video Cloud","W. Zhang; P. Duan; Z. Li; Q. Lu; W. Gong; S. Yang","Department of Software Engineering, China University of Petroleum, Qingdao, China; Department of Software Engineering, China University of Petroleum, Qingdao, China; Department of Software Engineering, China University of Petroleum, Qingdao, China; Department of Software Engineering, China University of Petroleum, Qingdao, China; Department of Software Engineering, China University of Petroleum, Qingdao, China; College of Computer Science and Technology, Fudan University, Shanghai, China","IEEE Access","","2015","3","","2227","2237","Context-awareness for big data applications is different from that of traditional applications in that it is getting challenging to obtain the contexts from big data due to the complexity, velocity, variety, and other aspects of big data, especially big video data. The awareness of contexts in big data is more difficult, and should be more in-depth than that of classical applications. Therefore, in this paper, we propose an in-depth context-awareness framework for a pervasive video cloud in order to obtain underlying contexts in big video data. In this framework, we propose an approach that combines the historical view with the current view to obtain meaningful in-depth contexts, where deep learning techniques are used to obtain raw context data. We have conducted initial evaluations to show the effectiveness of the proposed approach in terms of performance and also the accuracy of obtaining the contexts. The evaluation results show that the proposed approach is effective for real-time context-awareness in a pervasive video cloud.","","","10.1109/ACCESS.2015.2497278","National Natural Science Foundation of China; Natural Science Foundation of Shandong Province; Key Technologies Development Plan of Qingdao Technical Economic Development Area; Start-Up Funds for Academic Top-Notch Professors through the China University of Petroleum; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7315021","Pervasive Video Cloud;Deep Learning;Framework;Context Awareness;Cloud Computing;Pervasive video cloud;deep learning;framework;context awareness;cloud computing","Cloud computing;Video communication;Context awareness;Pervasive computing;Deep learning","cloud computing;ubiquitous computing;video communication","big data applications;context awareness;pervasive video cloud;deep awareness framework","","5","27","","","","","IEEE","IEEE Journals"
"Deep Multimodal Learning for Affective Analysis and Retrieval","L. Pang; S. Zhu; C. Ngo","Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong; Multimedia Communications Research Laboratory (MCRLab), School of Electrical Engineering and Computer Science, University of Ottawa, Ontario, Canada; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong","IEEE Transactions on Multimedia","","2015","17","11","2008","2020","Social media has been a convenient platform for voicing opinions through posting messages, ranging from tweeting a short text to uploading a media file, or any combination of messages. Understanding the perceived emotions inherently underlying these user-generated contents (UGC) could bring light to emerging applications such as advertising and media analytics. Existing research efforts on affective computation are mostly dedicated to single media, either text captions or visual content. Few attempts for combined analysis of multiple media are made, despite that emotion can be viewed as an expression of multimodal experience. In this paper, we explore the learning of highly non-linear relationships that exist among low-level features across different modalities for emotion prediction. Using the deep Bolzmann machine (DBM), a joint density model over the space of multimodal inputs, including visual, auditory, and textual modalities, is developed. The model is trained directly using UGC data without any labeling efforts. While the model learns a joint representation over multimodal inputs, training samples in absence of certain modalities can also be leveraged. More importantly, the joint representation enables emotion-oriented cross-modal retrieval, for example, retrieval of videos using the text query “crazy cat”. The model does not restrict the types of input and output, and hence, in principle, emotion prediction and retrieval on any combinations of media are feasible. Extensive experiments on web videos and images show that the learnt joint representation could be very compact and be complementary to hand-crafted features, leading to performance improvement in both emotion classification and cross-modal retrieval.","","","10.1109/TMM.2015.2482228","Research Grants Council of the Hong Kong Special Administrative Region; National Hi-Tech Research and Development Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7277066","Cross-modal retrieval;deep Boltzmann machine;emotion analysis;multimodal learning","Visualization;Joints;Media;Videos;Semantics;Training;Feature extraction","Boltzmann machines;image retrieval;learning (artificial intelligence);social networking (online)","deep multimodal learning;affective analysis;affective retrieval;social media;voicing opinions;posting messages;media file;user generated contents;UGC;text captions;visual content;emotion prediction;deep Bolzmann machine;DBM;joint density model;cross-modal retrieval;emotion classification","","34","43","","","","","IEEE","IEEE Journals"
"Deep transfer learning with ontology for image classification","U. Gupta; S. Chaudhury","Dept. of Electrical Engineering, Indian Institute of Technology Delhi, India; Dept. of Electrical Engineering, Indian Institute of Technology Delhi, India","2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)","","2015","","","1","4","Purely, data-driven large scale image classification has been achieved using various feature descriptors like SIFT, HOG etc. Major milestone in this regards is Convolutional Neural Networks (CNN) based methods which learn optimal feature descriptors as filters. Little attention has been given to the use of domain knowledge. Ontology plays an important role in learning to categorize images into abstract classes where there may not be a clear visual connect between category and image, for example identifying image mood - happy, sad and neutral. Our algorithm combines CNN and ontology priors to infer abstract patterns in Indian Monument Images. We use a transfer learning based approach in which, knowledge of domain is transferred to CNN while training (top down transfer) and inference is made using CNN prediction and ontology tree/priors (bottom up transfer ). We classify images to categories like Tomb, Fort and Mosque. We demonstrate that our method improves remarkably over logistic classifier and other transfer learning approach. We conclude with a remark on possible applications of the model and note about scaling this to bigger ontology.","","","10.1109/NCVPRIPG.2015.7490037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490037","","Ontologies;Training;Feature extraction;Linear programming;Logistics;Shape;Neural networks","convolution;image classification;learning (artificial intelligence);neural nets;ontologies (artificial intelligence)","deep transfer learning;ontology;data-driven large scale image classification;convolutional neural networks;CNN based methods;optimal feature descriptors;image categorization;Indian monument images;top down transfer;bottom up transfer;tomb;fort;mosque","","2","10","","","","","IEEE","IEEE Conferences"
"Automated Mass Detection in Mammograms Using Cascaded Deep Learning and Random Forests","N. Dhungel; G. Carneiro; A. P. Bradley","NA; NA; NA","2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA)","","2015","","","1","8","Mass detection from mammograms plays a crucial role as a pre- processing stage for mass segmentation and classification. The detection of masses from mammograms is considered to be a challenging problem due to their large variation in shape, size, boundary and texture and also because of their low signal to noise ratio compared to the surrounding breast tissue. In this paper, we present a novel approach for detecting masses in mammograms using a cascade of deep learning and random forest classifiers. The first stage classifier consists of a multi-scale deep belief network that selects suspicious regions to be further processed by a two-level cascade of deep convolutional neural networks. The regions that survive this deep learning analysis are then processed by a two-level cascade of random forest classifiers that use morphological and texture features extracted from regions selected along the cascade. Finally, regions that survive the cascade of random forest classifiers are combined using connected component analysis to produce state-of-the-art results. We also show that the proposed cascade of deep learning and random forest classifiers are effective in the reduction of false positive regions, while maintaining a high true positive detection rate. We tested our mass detection system on two publicly available datasets: DDSM-BCRP and INbreast. The final mass detection produced by our approach achieves the best results on these publicly available datasets with a true positive rate of 0.96 ± 0.03 at 1.2 false positive per image on INbreast and true positive rate of 0.75 at 4.8 false positive per image on DDSM-BCRP.","","","10.1109/DICTA.2015.7371234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371234","","Feature extraction;Mammography;Machine learning;Breast cancer;Support vector machines;Image resolution","belief networks;biological tissues;feature extraction;image classification;image segmentation;image texture;learning (artificial intelligence);mammography;medical image processing;neural nets;object detection;principal component analysis;trees (mathematics)","automated mass detection;mammogram;cascaded deep learning;mass segmentation;mass classification;shape variation;size variation;boundary variation;texture variation;signal to noise ratio;breast tissue;random forest classifier;multiscale deep belief network;deep convolutional neural network;morphological feature extraction;texture feature extraction;connected component analysis;false positive region reduction;DDSM-BCRP;INbreast","","48","37","","","","","IEEE","IEEE Conferences"
"A Deep-Structured Fully Connected Random Field Model for Structured Inference","A. Wong; M. J. Shafiee; P. Siva; X. Y. Wang","Department of Systems Design EngineeringVision and Image Processing Laboratory, University of Waterloo, Waterloo, ON, Canada; Department of Systems Design EngineeringVision and Image Processing Laboratory, University of Waterloo, Waterloo, ON, Canada; Department of Systems Design EngineeringVision and Image Processing Laboratory, University of Waterloo, Waterloo, ON, Canada; Department of Systems Design EngineeringVision and Image Processing Laboratory, University of Waterloo, Waterloo, ON, Canada","IEEE Access","","2015","3","","469","477","There has been significant interest in the use of fully connected graphical models and deep-structured graphical models for the purpose of structured inference. However, fully connected and deep-structured graphical models have been largely explored independently, leaving the unification of these two concepts ripe for exploration. A fundamental challenge with unifying these two types of models is in dealing with computational complexity. In this paper, we investigate the feasibility of unifying fully connected and deep-structured models in a computationally tractable manner for the purpose of structured inference. To accomplish this, we introduce a deep-structured fully connected random field (DFRF) model that integrates a series of intermediate sparse autoencoding layers placed between state layers to significantly reduce the computational complexity. The problem of image segmentation was used to illustrate the feasibility of using the DFRF for structured inference in a computationally tractable manner. Results in this paper show that it is feasible to unify fully connected and deep-structured models in a computationally tractable manner for solving structured inference problems such as image segmentation.","","","10.1109/ACCESS.2015.2425304","Natural Sciences and Engineering Research Council of Canada; Canada Research Chairs Program; Ontario Ministry of Research and Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091871","random fields;structured inference;deep structured;fully connected;learning;image;segmentation;Random fields;structured inference;deep structured;fully connected;learning;image;segmentation","Graph theory;Random processes;Image segmentation;Inference algorithms;Structured inference","image segmentation;learning (artificial intelligence)","image segmentation;intermediate sparse autoencoding layers;DFRF model;computational complexity;deep-structured graphical models;structured inference;deep-structured fully connected random field model","","2","40","","","","","IEEE","IEEE Journals"
"SimNest: Social Media Nested Epidemic Simulation via Online Semi-Supervised Deep Learning","L. Zhao; J. Chen; F. Chen; W. Wang; C. Lu; N. Ramakrishnan","NA; NA; NA; NA; NA; NA","2015 IEEE International Conference on Data Mining","","2015","","","639","648","Infectious disease epidemics such as influenza and Ebola pose a serious threat to global public health. It is crucial to characterize the disease and the evolution of the ongoing epidemic efficiently and accurately. Computational epidemiology can model the disease progress and underlying contact network, but suffers from the lack of real-time and fine-grained surveillance data. Social media, on the other hand, provides timely and detailed disease surveillance, but is insensible to the underlying contact network and disease model. This paper proposes a novel semi-supervised deep learning framework that integrates the strengths of computational epidemiology and social media mining techniques. Specifically, this framework learns the social media users' health states and intervention actions in real time, which are regularized by the underlying disease model and contact network. Conversely, the learned knowledge from social media can be fed into computational epidemic model to improve the efficiency and accuracy of disease diffusion modeling. We propose an online optimization algorithm to substantialize the above interactive learning process iteratively to achieve a consistent stage of the integration. The extensive experimental results demonstrated that our approach can effectively characterize the spatiotemporal disease diffusion, outperforming competing methods by a substantial margin on multiple metrics.","","","10.1109/ICDM.2015.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373368","Twitter;deep learning;epidemic simulation","Diseases;Computational modeling;Media;Surveillance;Data models;Sociology;Statistics","data mining;diseases;learning (artificial intelligence);social networking (online)","SimNest;social media nested epidemic simulation;online semisupervised-deep learning;infectious disease epidemics;influenza;Ebola;global public health;disease progress;contact network;computational epidemiology;social media mining techniques;social media user health states;intervention actions;efficiency improvement;accuracy improvement;disease diffusion modeling;online optimization algorithm;interactive learning process;spatiotemporal disease diffusion","","8","28","","","","","IEEE","IEEE Conferences"
"Audiovisual Fusion: Challenges and New Approaches","A. K. Katsaggelos; S. Bahaadini; R. Molina","Dept. of Electr. Eng. & Comput. Sci., Northwestern Univ., Evanston, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Northwestern Univ., Evanston, IL, USA; Dept. de Cienc. de la Comput. e I.A., Univ. de Granada, Granada, Spain","Proceedings of the IEEE","","2015","103","9","1635","1653","In this paper, we review recent results on audiovisual (AV) fusion. We also discuss some of the challenges and report on approaches to address them. One important issue in AV fusion is how the modalities interact and influence each other. This review will address this question in the context of AV speech processing, and especially speech recognition, where one of the issues is that the modalities both interact but also sometimes appear to desynchronize from each other. An additional issue that sometimes arises is that one of the modalities may be missing at test time, although it is available at training time; for example, it may be possible to collect AV training data while only having access to audio at test time. We will review approaches to address this issue from the area of multiview learning, where the goal is to learn a model or representation for each of the modalities separately while taking advantage of the rich multimodal training data. In addition to multiview learning, we also discuss the recent application of deep learning (DL) toward AV fusion. We finally draw conclusions and offer our assessment of the future in the area of AV fusion.","","","10.1109/JPROC.2015.2459017","U.S. Department of Energy; Spanish Ministry of Economy and Competitiveness; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194741","Audiovisual (AV) fusion;deep learning (DL);machine learning;multimodal analysis;multiview learning;stream asynchrony;Audiovisual (AV) fusion;deep learning (DL);machine learning;multimodal analysis;multiview learning;stream asynchrony","Hidden Markov models;Feature extraction;Speech processing;Visualization;Streaming media;Kalman filters;Speech processing;Multimodal sensors;Data integration","learning (artificial intelligence);speech recognition","audiovisual fusion;AV speech processing;speech recognition;multiview learning;deep learning","","36","171","","","","","IEEE","IEEE Journals"
"Vehicle Speed Prediction Using Deep Learning","J. Lemieux; Y. Ma","NA; NA","2015 IEEE Vehicle Power and Propulsion Conference (VPPC)","","2015","","","1","5","Global optimization of the energy consumption of dual power source vehicles such as hybrid electric vehicles, plug-in hybrid electric vehicles, and plug in fuel cell electric vehicles requires knowledge of the complete route characteristics at the beginning of the trip. One of the main characteristics is the vehicle speed profile across the route. The profile will translate directly into energy requirements for a given vehicle. However, the vehicle speed that a given driver chooses will vary from driver to driver and from time to time, and may be slower, equal to, or faster than the average traffic flow. If the specific driver speed profile can be predicted, the energy usage can be optimized across the route chosen. The purpose of this paper is to research the application of Deep Learning techniques to this problem to identify at the beginning of a drive cycle the driver specific vehicle speed profile for an individual driver repeated drive cycle, which can be used in an optimization algorithm to minimize the amount of fossil fuel energy used during the trip.","","","10.1109/VPPC.2015.7353037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353037","","Vehicles;Standards;Machine learning;Neural networks;Roads;Data mining;Traffic control","energy consumption;fossil fuels;fuel cell vehicles;hybrid electric vehicles","vehicle speed prediction;deep learning;energy consumption;dual power source vehicles;plug-in hybrid electric vehicles;plug in fuel cell electric vehicles;energy usage;fossil fuel energy","","7","6","","","","","IEEE","IEEE Conferences"
"Anatomy-specific classification of medical images using deep convolutional nets","H. R. Roth; C. T. Lee; H. Shin; A. Seff; L. Kim; J. Yao; L. Lu; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892, USA","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","","2015","","","101","104","Automated classification of human anatomy is an important prerequisite for many computer-aided diagnosis systems. The spatial complexity and variability of anatomy throughout the human body makes classification difficult. “Deep learning” methods such as convolutional networks (ConvNets) outperform other state-of-the-art methods in image classification tasks. In this work, we present a method for organ- or body-part-specific anatomical classification of medical images acquired using computed tomography (CT) with ConvNets. We train a ConvNet, using 4,298 separate axial 2D key-images to learn 5 anatomical classes. Key-images were mined from a hospital PACS archive, using a set of 1,675 patients. We show that a data augmentation approach can help to enrich the data set and improve classification performance. Using ConvNets and data augmentation, we achieve anatomy-specific classification error of 5.9 % and area-under-the-curve (AUC) values of an average of 0.998 in testing. We demonstrate that deep learning can be used to train very reliable and accurate classifiers that could initialize further computer-aided diagnosis.","","","10.1109/ISBI.2015.7163826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163826","Image Classification;Computed tomography (CT);Convolutional Networks;Deep Learning","Computed tomography;Medical diagnostic imaging;Convolution;Lungs;Training;Neural networks","biological organs;computerised tomography;image classification;medical image processing;PACS","anatomy-specific classification;medical images;deep convolutional nets;automated classification;human anatomy;computer-aided diagnosis systems;spatial complexity;anatomy variability;deep learning methods;convolutional networks;ConvNets;image classification;organ-specific anatomical classification;body part-specific anatomical classification;computed tomography;axial 2D key-images;hospital PACS archive;data augmentation approach;data augmentation;anatomy-specific classification error;area-under-the-curve","","37","16","","","","","IEEE","IEEE Conferences"
"Deep Learning Framework with Confused Sub-Set Resolution Architecture for Automatic Arabic Diacritization","M. A. A. Rashwan; A. A. Al Sallab; H. M. Raafat; A. Rafea","Engineering Company for the Development of Computer Systems (RDI), Cairo, Egypt; Valeo Interbranch Automotive Sotware, Cairo, Egypt; Computer Science Department, Kuwait University, Safat, Kuwait; Department of Computer Science, American University in Cairo (AUC), Cairo, Egypt","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2015","23","3","505","516","The Arabic language belongs to a group of languages that require diacritization over their characters. Modern Standard Arabic (MSA) transcripts omit the diacritics, which are essential for many machine learning tasks like Text-To-Speech (TTS) systems. In this work Arabic diacritics restoration is tackled under a deep learning framework that includes the Confused Sub-set Resolution (CSR) method to improve the classification accuracy, in addition to an Arabic Part-of-Speech (PoS) tagging framework using deep neural nets. Special focus is given to syntactic diacritization, which still suffers low accuracy as indicated in prior works. Evaluation is done versus state-of-the-art systems reported in literature, with quite challenging datasets collected from different domains. Standard datasets like the LDC Arabic Tree Bank are used in addition to custom ones we have made available online to allow other researchers to replicate these results. Results show significant improvement of the proposed techniques over other approaches, reducing the syntactic classification error to 9.9% and morphological classification error to 3% compared to 12.7% and 3.8% of the best reported results in literature, improving the error by 22% over the best reported systems.","","","10.1109/TASLP.2015.2395255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7050392","Arabic diacritization;classifier design;deep networks;part-of-speech (PoS) tagging","Training;Syntactics;Accuracy;Vectors;Feature extraction;Standards;Context","learning (artificial intelligence);natural language processing;neural nets;speech synthesis","deep learning;confused subset resolution architecture;automatic Arabic diacritization;Arabic language;modern standard Arabic transcript;machine learning;text-to-speech system;TTS system;Arabic diacritics restoration;Arabic part-of-speech tagging;PoS tagging;deep neural nets;syntactic diacritization","","10","12","","","","","IEEE","IEEE Journals"
"Applying deep learning to answer selection: A study and an open task","M. Feng; B. Xiang; M. R. Glass; L. Wang; B. Zhou","IBM Watson, Yorktown Heights, NY, USA, 10598; IBM Watson, Yorktown Heights, NY, USA, 10598; IBM Watson, Yorktown Heights, NY, USA, 10598; IBM Watson, Yorktown Heights, NY, USA, 10598; IBM Watson, Yorktown Heights, NY, USA, 10598","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","813","820","We apply a general deep learning framework to address the non-factoid question answering task. Our approach does not rely on any linguistic tools and can be applied to different languages or domains. Various architectures are presented and compared. We create and release a QA corpus and setup a new QA task in the insurance domain. Experimental results demonstrate superior performance compared to the baseline methods and various technologies give further improvements. For this highly challenging task, the top-1 accuracy can reach up to 65.3% on a test set, which indicates a great potential for practical use.","","","10.1109/ASRU.2015.7404872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404872","Answer Selection;Question Answering;Convolutional Neural Network (CNN);Deep Learning;Spoken Question Answering System","Computer architecture;Machine learning;Knowledge discovery;Training;Convolution;Measurement;Insurance","learning (artificial intelligence);question answering (information retrieval)","deep learning;nonfactoid question answering task;QA corpus;QA task;insurance domain","","85","24","","","","","IEEE","IEEE Conferences"
"Saliency detection by multi-context deep learning","R. Zhao; W. Ouyang; H. Li; X. Wang","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chuangyeyuan Rd, Longgang, Guangdong, China; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chuangyeyuan Rd, Longgang, Guangdong, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","1265","1274","Low-level saliency cues or priors do not produce good enough saliency detection results especially when the salient object presents in a low-contrast background with confusing visual appearance. This issue raises a serious problem for conventional approaches. In this paper, we tackle this problem by proposing a multi-context deep learning framework for salient object detection. We employ deep Convolutional Neural Networks to model saliency of objects in images. Global context and local context are both taken into account, and are jointly modeled in a unified multi-context deep learning framework. To provide a better initialization for training the deep neural networks, we investigate different pre-training strategies, and a task-specific pre-training scheme is designed to make the multi-context modeling suited for saliency detection. Furthermore, recently proposed contemporary deep models in the ImageNet Image Classification Challenge are tested, and their effectiveness in saliency detection are investigated. Our approach is extensively evaluated on five public datasets, and experimental results show significant and consistent improvements over the state-of-the-art methods.","","","10.1109/CVPR.2015.7298731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298731","","Context;Context modeling;Predictive models;Training;Visualization;Object detection;Machine learning","image classification;learning (artificial intelligence);neural nets;object detection","saliency detection;low-level saliency cue;low-contrast background;visual appearance;multicontext deep learning framework;salient object detection;convolutional neural network;deep neural network;pre-training strategy;task-specific pre-training scheme;multicontext modeling;contemporary deep model;ImageNet image classification challenge","","289","64","","","","","IEEE","IEEE Conferences"
"Generation of synthetic structural magnetic resonance images for deep learning pre-training","E. Castro; A. Ulloa; S. M. Plis; J. A. Turner; V. D. Calhoun","The Mind Research Network, Albuquerque, NM; The Mind Research Network, Albuquerque, NM; The Mind Research Network, Albuquerque, NM; The Mind Research Network, Albuquerque, NM; The Mind Research Network, Albuquerque, NM","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","","2015","","","1057","1060","Deep learning methods have significantly improved classification accuracy in different areas such as speech, object and text recognition. However, this field has only began to be explored in the brain imaging field, which differs from other fields in terms of the amount of data available, its data dimensionality and other factors. This paper proposes a methodology to generate an extensive synthetic structural magnetic resonance imaging (sMRI) dataset to be used at the pre-training stage of a shallow network model to address the issue of having a limited amount of data available. Our results show that by extending our dataset using 5,000 synthetic sMRI volumes for pretraining, which accounts to approximately 10 times the size of the original dataset, we can obtain a 5% average improvement on classification results compared to the regular approach on a schizophrenia dataset. While the use of synthetic sMRI data for pre-training has only been tested on a shallow network, this can be readily applied to deeper networks.","","","10.1109/ISBI.2015.7164053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164053","structural MRI;simulation;deep learning;pretraining;schizophrenia","Machine learning;Magnetic resonance imaging;Data models;Training;Neuroimaging;Probability density function;Support vector machines","biomedical MRI;brain;image classification;learning (artificial intelligence);medical disorders;medical image processing","synthetic sMR image;structural MRI;magnetic resonance imaging;deep learning pretraining method;classification accuracy;speech recognition;object recognition;text recognition;brain imaging field;shallow network model;synthetic sMRI volume;schizophrenia dataset;deeper networks","","4","15","","","","","IEEE","IEEE Conferences"
"A deep learning approach for unsupervised domain adaptation in multitemporal remote sensing images","E. Othman; Y. Bazi; H. AlHichri; N. Alajlan","ALISR Laboratory, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; ALISR Laboratory, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; ALISR Laboratory, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; ALISR Laboratory, College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","2401","2404","In this paper, we propose a novel deep convex network method for domain adaptation in multitemporal remote sensing imagery. We fuse the capabilities of the extreme learning machine (ELM) classifier and local feature descriptor techniques to boost the classification accuracy. We use the Affine Scale Invariant Feature Transform (ASIFT) to extract the key points from the image pair, i.e. source and target domain images. The neural network consist of two layers, one layer uses the keypoints extracted by ASIFT to map the training points of the source image to the target image, while layer 2 is used for the purpose of classification. Experimental results obtained on multitemporal VHR images acquired by the IKONOS2 confirm the promising capability of the proposed method.","","","10.1109/IGARSS.2015.7326293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326293","Multitemporal VHR images;Affine Scale Invariant Feature Transform (SIFT);Extreme Learning Machine (ELM);Domain Adaptation (DA)","Kernel;Training;Feature extraction;Artificial neural networks;Remote sensing;Accuracy;Standards","affine transforms;geophysical techniques;neural nets;remote sensing","extreme learning machine approach;unsupervised domain adaptation;multitemporal remote sensing images;deep convex network method;local feature descriptor technique;classifier feature descriptor technique;affine scale invariant feature transform;ASIFT;neural network;target image;multitemporal VHR images;IKONOS2","","3","13","","","","","IEEE","IEEE Conferences"
"Deep data fusion model for risk perception and coordinated control of smart grid","X. Z. Wang; X. L. Bi; Z. Q. Ge; L. Li","East China Electric Power Dispatching and Control Center, East China Grid Company Limited, Shanghai, China; East China Electric Power Dispatching and Control Center, East China Grid Company Limited, Shanghai, China; East China Electric Power Dispatching and Control Center, East China Grid Company Limited, Shanghai, China; Information and Communication Center, East China Grid Company Limited, Shanghai, China","2015 International Conference on Estimation, Detection and Information Fusion (ICEDIF)","","2015","","","110","113","This paper presents a deep data fusion model for risk perception and coordinated control in a regional power system control center. A knowledge learning data fusion approach has been used to find an efficient state representation based on prior knowledge from cross-domain energy management systems. In particular, a kernel principal components analysis technique is presented for nonlinear dimensionality reduction of knowledge learning. The control strategy we study is based on cross-domain global optimization approach, which regards the contingencies and control actions of mutual backup systems as constraints. The objective function is defined as the product of cross-domain assessment and control factors. The method for obtaining optimal solution is given by interior point code. To show the applicability, different machine learning method has been studied. The experimental results show that the proposed knowledge learning approach consistently outperforms the traditional machine learning method. In addition, the proposed coordinated control approach is verified effective on large-scale smart grid decision support system for East China project.","","","10.1109/ICEDIF.2015.7280172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280172","information analysis;machine learning;power system control;smart grid","Power system control;Support vector machines;Principal component analysis;Kernel;Next generation networking","control engineering computing;decision support systems;energy management systems;learning (artificial intelligence);optimisation;power engineering computing;power system control;principal component analysis;risk management;sensor fusion;smart power grids","deep data fusion model;risk perception;coordinated control;large-scale smart grid decision support system;regional power system control center;knowledge learning data fusion approach;state representation;cross-domain energy management systems;kernel principal components analysis technique;nonlinear dimensionality reduction;mutual backup systems;cross-domain global optimization approach;objective function;cross-domain assessment;control factors;machine learning method;East China project","","2","16","","","","","IEEE","IEEE Conferences"
"Coupled Attribute Similarity Learning on Categorical Data","C. Wang; X. Dong; F. Zhou; L. Cao; C. Chi","Commonwealth Scientific and Industrial Research Organisation, Sandy Bay, TAS, Australia; School of Information, Qilu University of Technology, Ji'nan, China; Department of Electronic EngineeringGraduate School at Shenzhen, Tsinghua University, Shenzhen, China; Advanced Analytics Institute, University of Technology at Sydney, Ultimo, NSW, Australia; Commonwealth Scientific and Industrial Research Organisation, Sandy Bay, TAS, Australia","IEEE Transactions on Neural Networks and Learning Systems","","2015","26","4","781","797","Attribute independence has been taken as a major assumption in the limited research that has been conducted on similarity analysis for categorical data, especially unsupervised learning. However, in real-world data sources, attributes are more or less associated with each other in terms of certain coupling relationships. Accordingly, recent works on attribute dependency aggregation have introduced the co-occurrence of attribute values to explore attribute coupling, but they only present a local picture in analyzing categorical data similarity. This is inadequate for deep analysis, and the computational complexity grows exponentially when the data scale increases. This paper proposes an efficient data-driven similarity learning approach that generates a coupled attribute similarity measure for nominal objects with attribute couplings to capture a global picture of attribute similarity. It involves the frequency-based intra-coupled similarity within an attribute and the inter-coupled similarity upon value co-occurrences between attributes, as well as their integration on the object level. In particular, four measures are designed for the inter-coupled similarity to calculate the similarity between two categorical values by considering their relationships with other attributes in terms of power set, universal set, joint set, and intersection set. The theoretical analysis reveals the equivalent accuracy and superior efficiency of the measure based on the intersection set, particularly for large-scale data sets. Intensive experiments of data structure and clustering algorithms incorporating the coupled dissimilarity metric achieve a significant performance improvement on state-of-the-art measures and algorithms on 13 UCI data sets, which is confirmed by the statistical analysis. The experiment results show that the proposed coupled attribute similarity is generic, and can effectively and efficiently capture the intrinsic and global interactions within and between attributes for especially large-scale categorical data sets. In addition, two new coupled categorical clustering algorithms, i.e., CROCK and CLIMBO are proposed, and they both outperform the original ones in terms of clustering quality on UCI data sets and bibliographic data.","","","10.1109/TNNLS.2014.2325872","National Privacy Principles, Tasmania, Australia; Australian Research Council Discovery; Australian Research Council Linkage; National Natural Science Foundation of China; Natural Science Foundation of China; China Post-Doctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6834817","Clustering;coupled attribute similarity;coupled object analysis;similarity analysis;unsupervised learning.;Clustering;coupled attribute similarity;coupled object analysis;similarity analysis;unsupervised learning","Couplings;Clustering algorithms;Frequency measurement;Motion pictures;Unsupervised learning;Algorithm design and analysis","computational complexity;data analysis;data structures;pattern clustering;statistical analysis;unsupervised learning","coupled attribute similarity learning;attribute independence;unsupervised learning;attribute dependency aggregation;attribute coupling;categorical data similarity analysis;computational complexity;data-driven similarity learning approach;frequency-based intracoupled similarity;intercoupled similarity;power set;universal set;joint set;intersection set;data structure;coupled dissimilarity metric;UCI data sets;statistical analysis;coupled categorical clustering algorithms;CROCK;CLIMBO;bibliographic data","","26","31","","","","","IEEE","IEEE Journals"
"Posed and spontaneous facial expression differentiation using deep Boltzmann machines","Q. Gan; C. Wu; S. Wang; Q. Ji","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Department of Electrical, Computer and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180-3590","2015 International Conference on Affective Computing and Intelligent Interaction (ACII)","","2015","","","643","648","Current works on differentiating between posed and spontaneous facial expressions usually use features that are handcrafted for expression category recognition. Till now, no features have been specifically designed for differentiating between posed and spontaneous facial expressions. Recently, deep learning models have been proven to be efficient for many challenging computer vision tasks, and therefore in this paper we propose using the deep Boltzmann machine to learn representations of facial images and to differentiate between posed and spontaneous facial expressions. First, faces are located from images. Then, a two-layer deep Boltzmann machine is trained to distinguish posed and spon-tanous expressions. Experimental results on two benchmark datasets, i.e. the SPOS and USTC-NVIE datasets, demonstrate that the deep Boltzmann machine performs well on posed and spontaneous expression differentiation tasks. Comparison results on both datasets show that our method has an advantage over the other methods.","","","10.1109/ACII.2015.7344637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344637","posed and spontaneous expressions;deep boltzmann machines;expression differentiation;feature learning","Training;Nonhomogeneous media;Neural networks;Face recognition;Machine learning;Standards;Electronic mail","Boltzmann machines;emotion recognition;face recognition;feature extraction;learning (artificial intelligence)","spontaneous facial expression differentiation;posed facial expression differentiation;deep learning model;facial image;two-layer deep Boltzmann machine;SPOS dataset;USTC-NVIE dataset","","5","18","","","","","IEEE","IEEE Conferences"
"Hierarchical Extreme Learning Machine for unsupervised representation learning","W. Zhu; J. Miao; L. Qing; G. Huang","Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China; School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing 100049, China; School of Electrical & Electronic Engineering, Nanyang Technological University, Nanyang Avenue, Singapore 639798","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","Learning representations from massive unlabeled data is a hot topic for high-level tasks in many applications. The recent great improvements on benchmark data sets, which are achieved by increasingly complex unsupervised learning methods and deep learning models with lots of parameters, usually require many tedious tricks and much expertise to tune. However, filters learned by these complex architectures are quite similar to standard hand-crafted features visually, and training the deep models costs quite long time to fine-tune their weights. In this paper, Extreme Learning Machine-Autoencoder (ELM-AE) is employed as the learning unit to learn local receptive fields at each layer, and the lower layer responses are transferred to the last layer (trans-layer) to form a more complete representation to retain more information. In addition, some beneficial methods in deep learning architectures such as local contrast normalization and whitening are added to the proposed hierarchical Extreme Learning Machine networks to further boost the performance. The obtained trans-layer representations are followed by block histograms with binary hashing to learn translation and rotation invariant representations, which are utilized to do high-level tasks such as recognition and detection. Compared to traditional deep learning methods, the proposed trans-layer representation method with ELM-AE based learning of local receptive filters has much faster learning speed and is validated in several typical experiments, such as digit recognition on MNIST and MNIST variations, object recognition on Caltech 101. State-of-the-art performances are achieved on both Caltech 101 15 samples per class task and 4 of 6 MNIST variations data sets, and highly impressive results are obtained on MNIST data set and other tasks.","","","10.1109/IJCNN.2015.7280669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280669","","Niobium;Training","unsupervised learning","unsupervised representation learning;massive unlabeled data;high-level tasks;extreme learning machine-autoencoder;learning unit;deep learning architectures;local contrast normalization;whitening;hierarchical extreme learning machine networks;trans-layer representations;block histograms;binary hashing;translation invariant representations;rotation invariant representations;ELM-AE based learning;local receptive filters;learning speed;digit recognition;object recognition;Caltech 101","","14","36","","","","","IEEE","IEEE Conferences"
"Regularizing deep learning architecture for face recognition with weight variations","S. Nagpal; M. Singh; M. Vatsa; R. Singh","IIIT-Delhi, New Delhi, India; IIIT-Delhi, New Delhi, India; IIIT-Delhi, New Delhi, India; IIIT-Delhi, New Delhi, India","2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems (BTAS)","","2015","","","1","6","Several mathematical models have been proposed for recognizing face images with age variations. However, effect of change in body-weight is also an interesting covariate that has not been much explored. This paper presents a novel approach to incorporate the weight variations during feature learning process. In a deep learning architecture, we propose incorporating the body-weight in terms of a regularization function which helps in learning the latent variables representative of different weight categories. The formulation has been proposed for both Autoencoder and Deep Boltzmann Machine. On extended WIT database of 200 subjects, the comparison with a commercial system and an existing algorithm show that the proposed algorithm outperforms them by more than 9% at rank-10 identification accuracy.","","","10.1109/BTAS.2015.7358791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358791","","Face recognition;Face;Machine learning;Databases;Training;Decoding;Encoding","Boltzmann machines;face recognition;learning (artificial intelligence)","regularizing deep learning architecture;face recognition;weight variations;mathematical models;face image recognition;age variations;feature learning process;regularization function;autoencoder;deep Boltzmann machine","","1","14","","","","","IEEE","IEEE Conferences"
"Semi supervised deep kernel design for image annotation","M. Jiu; H. Sahbi","CNRS LTCI lab, Telecom ParisTech, France; CNRS LTCI lab, Telecom ParisTech, France","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","1156","1160","It is commonly agreed that the success of support vector machines (SVMs), is highly dependent on the choice of particular similarity functions referred to as kernels. The latter are usually handcrafted or designed using appropriate optimization schemes. Multiple kernel learning (MKL) is one possible scheme that designs kernels as sparse or convex linear combinations of existing elementary functions. However, this results into shallow kernels, which are powerless to capture the right similarity between data, especially when content of these data is highly semantic. In this paper, we redefine multiple kernels using a deep architecture. In this new formulation, a global kernel is learned as a multi-layered linear combination of activation functions, each one involves a combination of several elementary or intermediate functions on multiple features. We propose three different settings to learn the weights of these kernel combinations; supervised, unsupervised and semi-supervised. When plugged into SVMs, the resulting deep multiple kernels show a gain, compared to shallow kernels, for the challenging task of image annotation using the ImageCLEF benchmark.","","","10.1109/ICASSP.2015.7178151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178151","Deep kernel learning;multiple kernel learning;support vector machines","Kernel;Support vector machines;Training data;Neural networks;Computer architecture;Training;Standards","convex programming;image processing;learning (artificial intelligence);linear programming;support vector machines","semisupervised deep Kernel design;image annotation;support vector machine;SVM;MKL;convex linear combinations;sparse linear combinations;elementary functions;multiple kernel learning;unsupervised learning;supervised learning;image CLEF benchmark","","6","28","","","","","IEEE","IEEE Conferences"
"Towards structured deep neural network for automatic speech recognition","Y. Liao; H. Lee; L. Lee","National Taiwan University; National Taiwan University; National Taiwan University","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","137","144","In this paper we propose the Structured Deep Neural Network (structured DNN) as a structured and deep learning framework. This approach can learn to find the best structured object (such as a label sequence) given a structured input (such as a vector sequence) by globally considering the mapping relationships between the structures rather than item by item. When automatic speech recognition is viewed as a special case of such a structured learning problem, where we have the acoustic vector sequence as the input and the phoneme label sequence as the output, it becomes possible to comprehensively learn utterance by utterance as a whole, rather than frame by frame. Structured Support Vector Machine (structured SVM) was proposed to perform ASR with structured learning previously, but limited by the linear nature of SVM. Here we propose structured DNN to use nonlinear transformations in multi-layers as a structured and deep learning approach. This approach was shown to beat structured SVM in preliminary experiments on TIMIT.","","","10.1109/ASRU.2015.7404786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404786","structured learning;deep neural network","Support vector machines;Acoustics;Neural networks;Hidden Markov models;Training;Feature extraction;Speech recognition","learning (artificial intelligence);neural nets;speech recognition;support vector machines;transforms","structured deep neural network;automatic speech recognition;structured DNN;deep learning framework;vector sequence;structured learning problem;acoustic vector sequence;phoneme label sequence;utterance;structured support vector machine;structured SVM;ASR;nonlinear transformations;TIMIT","","3","29","","","","","IEEE","IEEE Conferences"
"Rotation-Invariant Object Detection in High-Resolution Satellite Imagery Using Superpixel-Based Deep Hough Forests","Y. Yu; H. Guan; Z. Ji","Fac. of Comput. & Software Eng., Huaiyin Inst. of Technol., Huai'an, China; Coll. of Geogr. & Remote Sensing, Nanjing Univ. of Inf. Sci. & Technol., Nanjing, China; Sch. of Remote Sensing Inf. & Eng., Wuhan Univ., Wuhan, China","IEEE Geoscience and Remote Sensing Letters","","2015","12","11","2183","2187","This letter presents a rotation-invariant method for detecting geospatial objects from high-resolution satellite images. First, a superpixel segmentation strategy is proposed to generate meaningful and nonredundant patches. Second, a multilayer deep feature generation model is developed to generate high-level feature representations of patches using deep learning techniques. Third, a set of multiscale Hough forests with embedded patch orientations is constructed to cast rotation-invariant votes for estimating object centroids. Quantitative evaluations on the images collected from Google Earth service show that an average completeness, correctness, quality, and F1- measure values of 0.958, 0.969, 0.929, and 0.963, respectively, are obtained. Comparative studies with three existing methods demonstrate the superior performance of the proposed method in accurately and correctly detecting objects that are arbitrarily oriented and of varying sizes.","","","10.1109/LGRS.2015.2432135","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7277008","Airplane detection;deep learning;Hough forest;object detection;rotation invariance;ship detection;Airplane detection;deep learning;Hough forest;object detection;rotation invariance;ship detection","Remote sensing;Object detection;Training;Marine vehicles;Satellites;Airplanes;Computational modeling","geophysical image processing;image segmentation;object detection;probability;remote sensing","rotation-invariant object detection;high-resolution satellite imagery;superpixel-based deep Hough forests;rotation-invariant method;geospatial object detection;superpixel segmentation strategy;multilayer deep feature generation model;high-level feature;deep learning technique;multiscale Hough forests;embedded patch orientations;Google Earth service","","15","20","","","","","IEEE","IEEE Journals"
"Automatic localization of the left ventricle in cardiac MRI images using deep learning","O. Emad; I. A. Yassine; A. S. Fahmy","Center for Informatics Science, Nile University, Giza, Egypt; Center for Informatics Science, Nile University, Giza, Egypt; Center for Informatics Science, Nile University, Giza, Egypt","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","683","686","Automatic localization of the left ventricle (LV) in cardiac MRI images is an essential step for automatic segmentation, functional analysis, and content based retrieval of cardiac images. In this paper, we introduce a new approach based on deep Convolutional Neural Network (CNN) to localize the LV in cardiac MRI in short axis views. A six-layer CNN with different kernel sizes was employed for feature extraction, followed by Softmax fully connected layer for classification. The pyramids of scales analysis was introduced in order to take account of the different sizes of the heart. A publically-available database of 33 patients was used for learning and testing. The proposed method was able it localize the LV with 98.66%, 83.91% and 99.07% for accuracy, sensitivity and specificity respectively.","","","10.1109/EMBC.2015.7318454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318454","","Magnetic resonance imaging;Heart;Convolution;Biomedical imaging;Image segmentation;Sensitivity;Feature extraction","biomedical MRI;cardiology;feature extraction;image classification;image segmentation;medical image processing;neural nets","left ventricle automatic localization;cardiac MRI images;deep learning;automatic segmentation;deep convolutional neural network;feature extraction;Softmax;image classification","Heart Ventricles;Humans;Learning;Machine Learning;Magnetic Resonance Imaging","20","21","","","","","IEEE","IEEE Conferences"
"Learning to count with deep object features","S. Seguí; O. Pujol; J. Vitrià","Dept. Matematica Aplicada i Analisis, Universitat de Barcelona, Spain; Dept. Matematica Aplicada i Analisis, Universitat de Barcelona, Spain; Dept. Matematica Aplicada i Analisis, Universitat de Barcelona, Spain","2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","","2015","","","90","96","Learning to count is a learning strategy that has been recently proposed in the literature for dealing with problems where estimating the number of object instances in a scene is the final objective. In this framework, the task of learning to detect and localize individual object instances is seen as a harder task that can be evaded by casting the problem as that of computing a regression value from hand-crafted image features. In this paper we explore the features that are learned when training a counting convolutional neural network in order to understand their underlying representation. To this end we define a counting problem for MNIST data and show that the internal representation of the network is able to classify digits in spite of the fact that no direct supervision was provided for them during training. We also present preliminary results about a deep network that is able to count the number of pedestrians in a scene.","","","10.1109/CVPRW.2015.7301276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301276","","Feature extraction;Training;Supervised learning;Proposals;Accuracy;Visualization;Neural networks","feature extraction;image classification;learning (artificial intelligence);neural nets;object detection;regression analysis","deep object features;count learning;learning strategy;object instances detection;object instances localization;regression value;hand-crafted image features;counting convolutional neural network training;counting problem;MNIST data;network internal representation;digits classification;deep network","","14","21","","","","","IEEE","IEEE Conferences"
"Deep learning & convolutional networks","Y. LeCun","Facebook AI Research & Center for Data Science, NYU","2015 IEEE Hot Chips 27 Symposium (HCS)","","2015","","","1","95","Presents a collection of slides covering the following topics: deep learning; convolutional neural network; image recognition; speech recognition; language translation; reasoning; unsupervised learning; and intelligent machine.","","","10.1109/HOTCHIPS.2015.7477328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477328","","Pose estimation;Supervised learning;Feature extraction;Natural language processing;Pattern recognition;Machine learning;Face detection;Object recognition;Speech recognition","convolution;image recognition;inference mechanisms;language translation;neural nets;speech recognition;unsupervised learning","intelligent machine;unsupervised learning;reasoning;language translation;speech recognition;image recognition;convolutional neural network;deep learning","","1","","","","","","IEEE","IEEE Conferences"
"Deep hashing for compact binary codes learning","V. E. Liong; Jiwen Lu; Gang Wang; P. Moulin; Jie Zhou","Advanced Digital Sciences Center, Singapore; Advanced Digital Sciences Center, Singapore; Advanced Digital Sciences Center, Singapore; Advanced Digital Sciences Center, Singapore; Department of Automation, Tsinghua University, Beijing, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","2475","2483","In this paper, we propose a new deep hashing (DH) approach to learn compact binary codes for large scale visual search. Unlike most existing binary codes learning methods which seek a single linear projection to map each sample into a binary vector, we develop a deep neural network to seek multiple hierarchical non-linear transformations to learn these binary codes, so that the nonlinear relationship of samples can be well exploited. Our model is learned under three constraints at the top layer of the deep network: 1) the loss between the original real-valued feature descriptor and the learned binary vector is minimized, 2) the binary codes distribute evenly on each bit, and 3) different bits are as independent as possible. To further improve the discriminative power of the learned binary codes, we extend DH into supervised DH (SDH) by including one discriminative term into the objective function of DH which simultaneously maximizes the inter-class variations and minimizes the intra-class variations of the learned binary codes. Experimental results show the superiority of the proposed approach over the state-of-the-arts.","","","10.1109/CVPR.2015.7298862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298862","","Binary codes;DH-HEMTs;Synchronous digital hierarchy;Training;Visualization;Machine learning;Optimization","binary codes;computer vision;file organisation;image retrieval;learning (artificial intelligence);neural nets","compact binary codes learning;deep hashing approach;large scale visual search;linear projection;binary vector;deep neural network;multiple hierarchical nonlinear transformation;real-valued feature descriptor;objective function;interclass variation maximization;intraclass variation miminization;computer vision","","115","36","","","","","IEEE","IEEE Conferences"
"3View deep canonical correlation analysis for cross-modal retrieval","J. Shao; Z. Zhao; F. Su; T. Yue","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","2015 Visual Communications and Image Processing (VCIP)","","2015","","","1","4","This paper investigates the problem of modeling Internet images and associated text for cross-modal retrieval tasks such as text-to-image search, and image-to-text search. Canonical correlation analysis (CCA), a classic two view approach for mapping text and image into a common latent space, does not make use of the semantic information of text and image pairs. We use CCA to map text, image and semantic information into a common latent space, in which the correlation of the three views is maximized. To improve the performance of CCA, in this paper, 3view-Deep Canonical Correlation Analysis (3view-DCCA), a nonlinear expansion of CCA is proposed to learn the complex nonlinear transformations between the three views. Like most deep learning methods, DCCA is easy to over-fitting. To overcome over-fitting, we add the reconstruct loss of each view into the loss function, which include the correlation loss of every two views and regularization of parameters. Inspired by PageRank, we propose a search-based similarity method to score relevance. The proposed model (3view-DCCA) is evaluated on three publicly available data sets from real scenes. We demonstrate that our deep model performs significantly better than traditional canonical correlation analysis based models and several other deep learning models on cross-modal retrieval tasks.","","","10.1109/VCIP.2015.7457870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457870","semantic CCA deep reconstruct search-based","Correlation;Image reconstruction;Semantics;Internet;Machine learning;Optimization;Feature extraction","correlation methods;image retrieval;Internet;learning (artificial intelligence)","3View deep canonical correlation analysis;cross-modal retrieval;Internet image modelling;classic two view approach;text mapping;latent space;semantic information;3view-DCCA;complex nonlinear transformation learning;deep learning methods;reconstruct loss;parameter regularization;PageRank;search-based similarity method","","2","12","","","","","IEEE","IEEE Conferences"
"A developemnt model of units of learning for multiple platforms","F. K. de Oliveira; A. S. Gomes","Centro de Informática, Universidade Federal de Pernambuco, Recife-PE, Brasil; Centro de Informática, Universidade Federal de Pernambuco, Recife-PE, Brasil","2015 10th Iberian Conference on Information Systems and Technologies (CISTI)","","2015","","","1","6","The need to ensure flexibility, compatibility, interoperability and reduce costs of production and maintenance of resources can be obtained with the use of internationally recognised specifications as Sharable Content Object Reference Model (SCORM), IEEE Learning Object Metadata (IEEE LOM) or Instructional Management System-Learning Design (IMS-LD). In this way, the research aims to verify whether a development model of Units of learning (UoL) based on open source online tools for authoring and execution of UoLs, that meet the criteria of usability and without deep knowledge of programming language by developers provide effectively increased implementation or the re-use of UAs complete or parts in different Electronic Learning Systems (ELS). For this, we intend to conduct a survey of authoring tools or editors and players to perform an analysis of competitors in order to identify the potential, general operating characteristics. Thus, to perform the surveys will be used as collection instruments: questionnaires and interviews with samples from groups of users and developers of educational content. Already the evaluations of the tool developed will be carried out by groups of users that include samples of all those in potential, in order to ensure the production of reusable content. Therefore, it is expected that the authoring tool and your player can assist in the development of UoLs for different ELS from a lean and simple semantics, which facilitates and ensures interoperability, changes, adjustments, maintenance and reuse of the UoLs.","","","10.1109/CISTI.2015.7170481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7170481","IMS Learning Design;Software reuse;interoperability;Units of learning;Electronic Learning Systems","Unified modeling language;Metadata;Instruments;Interoperability;Education;Production;Maintenance engineering","authoring systems;learning management systems;programming languages;public domain software;software reusability","cost reduction;sharable content object reference model;SCORM;IEEE learning object metadata;IEEE LOM;instructional management system-learning design;IMS-LD;units of learning development model;UoLs execution;UoLs authoring;open source online tools;programming language;electronic learning systems;ELS;educational content;software reuse","","1","19","","","","","IEEE","IEEE Conferences"
"Towards real-time Speech Emotion Recognition using deep neural networks","H. M. Fayek; M. Lech; L. Cavedon","School of Electrical and Computer Engineering, RMIT University, Melbourne, Victoria 3001, Australia; School of Electrical and Computer Engineering, RMIT University, Melbourne, Victoria 3001, Australia; School of Computer Science and IT, RMIT University, Melbourne, Victoria 3001, Australia","2015 9th International Conference on Signal Processing and Communication Systems (ICSPCS)","","2015","","","1","5","Most existing Speech Emotion Recognition (SER) systems rely on turn-wise processing, which aims at recognizing emotions from complete utterances and an overly-complicated pipeline marred by many preprocessing steps and hand-engineered features. To overcome both drawbacks, we propose a real-time SER system based on end-to-end deep learning. Namely, a Deep Neural Network (DNN) that recognizes emotions from a one second frame of raw speech spectrograms is presented and investigated. This is achievable due to a deep hierarchical architecture, data augmentation, and sensible regularization. Promising results are reported on two databases which are the eNTERFACE database and the Surrey Audio-Visual Expressed Emotion (SAVEE) database.","","","10.1109/ICSPCS.2015.7391796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391796","affective computing;deep neural networks;deep learning;emotion recognition;spectrograms;speech processing","Databases;Speech recognition;Emotion recognition;Speech;Training;Neurons;Neural networks","emotion recognition;learning (artificial intelligence);neural nets;speech recognition","real-time speech emotion recognition;deep neural networks;turn-wise processing;end-to-end deep learning;raw speech spectrograms;data augmentation;eNTERFACE database;Surrey Audio-Visual Expressed Emotion database","","12","23","","","","","IEEE","IEEE Conferences"
"Real-time network anomaly detection system using machine learning","S. Zhao; M. Chandrashekar; Y. Lee; D. Medhi","Computer Science & Electrical Engineering Department, University of Missouri-Kansas City, USA; Computer Science & Electrical Engineering Department, University of Missouri-Kansas City, USA; Computer Science & Electrical Engineering Department, University of Missouri-Kansas City, USA; Computer Science & Electrical Engineering Department, University of Missouri-Kansas City, USA","2015 11th International Conference on the Design of Reliable Communication Networks (DRCN)","","2015","","","267","270","The ability to process, analyze, and evaluate realtime data and to identify their anomaly patterns is in response to realized increasing demands in various networking domains, such as corporations or academic networks. The challenge of developing a scalable, fault-tolerant and resilient monitoring system that can handle data in real-time and at a massive scale is nontrivial. We present a novel framework for real time network traffic anomaly detection using machine learning algorithms. The proposed prototype system uses existing big data processing frameworks such as Apache Hadoop, Apache Kafka, and Apache Storm in conjunction with machine learning techniques and tools. Our approach consists of a system for real-time processing and analysis of the real-time network-flow data collected from the campus-wide network at the University of Missouri-Kansas City. Furthermore, the network anomaly patterns were identified and evaluated using machine learning techniques. We present preliminary results on anomaly detection with the campus network data.","","","10.1109/DRCN.2015.7149025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7149025","","Real-time systems;IP networks;Storms;Fasteners;Support vector machines;Ports (Computers);Accuracy","Big Data;fault tolerant computing;learning (artificial intelligence)","real-time network anomaly detection system;scalable fault-tolerant resilient monitoring system;machine learning algorithms;Big data processing frameworks;University of Missouri-Kansas City;campus network data","","19","19","","","","","IEEE","IEEE Conferences"
"A Histopathological Image Feature Representation Method Based on Deep Learning","G. Zhang; L. Zhong; Y. Huang; Y. Zhang","Sch. of Autom., Guangdong Univ. of Technol., Guangzhou, China; Sch. of Autom., Guangdong Univ. of Technol., Guangzhou, China; Sch. of Autom., Guangdong Univ. of Technol., Guangzhou, China; Dept. of Plastic & Reconstructive Surg., First Affiliated Hosp., Sun Yat-Sen Univ., Guangzhou, China","2015 7th International Conference on Information Technology in Medicine and Education (ITME)","","2015","","","13","17","Automated annotation and grading for histopathological image plays an important role in CAD systems. It provides valuable information and support for medical diagnosis. Currently, computer-aid analysis of histopathological images mainly relies on some well-designed digital features, which requires abundant human efforts and experiences in problem domain. Learning a good feature representation from data can have positive effects on constructing the target model. We propose a novel method for histopathological image feature representation based on deep learning. The method extracts high level representation of raw pixels of a local region through a network model with several hidden layers, which can learn potential features automatically. The proposed method is evaluated on a real data set from a large local hospital with comparison to two current state-of-the-art methods. The result is promising indicating that it achieves significant improvement of the model performance. Moreover, our study suggests that features learned through deep models can achieve better performance than human designed features.","","","10.1109/ITME.2015.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429087","histopathological image analysis;feature representation;deep learning;stacked autoencoder","Data models;Training;Feature extraction;Medical services;Solid modeling;Image color analysis;Biomedical imaging","biological tissues;image representation;learning (artificial intelligence);medical image processing","histopathological image feature representation method;deep learning;CAD systems;automated annotation;medical diagnosis;computer-aid analysis;large local hospital","","1","15","","","","","IEEE","IEEE Conferences"
"Deep Learning with MCA-based Instance Selection and Bootstrapping for Imbalanced Data Classification","S. Guan; M. Chen; H. Ha; S. Chen; M. Shyu; C. Zhang","NA; NA; NA; NA; NA; NA","2015 IEEE Conference on Collaboration and Internet Computing (CIC)","","2015","","","288","295","In this paper, we propose an extended deep learning approach that incorporates instance selection and bootstrapping techniques for imbalanced data classification. In supervised learning, classification performance often deteriorates when the training set is imbalanced where at least one of the classes has a substantially fewer number of instances than the others. We propose to use adaptive synthetic sampling approach (ADASYN) to generate synthetic instances for the minority class. A data pruning process based on multiple correspondence analysis (MCA) is then performed to identify a sub-set of synthetic instances that are most suitable to supplement the existing minority instances. This results in a relatively more balanced training dataset which is then bootstrapped and fed into the convolutional neural networks (CNNs) for classification. Furthermore, we propose to use low-level features pre-processed by principal component analysis (PCA), instead of the commonly used raw signal data, as the input to CNNs to reduce the computational time. The experimental results show the effectiveness of our framework in classifying 54 TRECVID concepts with different imbalanced levels by comparing with other state-of-the-art methods.","","","10.1109/CIC.2015.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7423094","Classification;imbalanced data;bootstrapping;Convolutional Neural Network (CNN);supervised learning;Multiple Correspondence Analysis (MCA)","Training;Feature extraction;Machine learning;Neurons;Multimedia communication;Biological neural networks;Principal component analysis","computational complexity;computer bootstrapping;feature extraction;learning (artificial intelligence);neural nets;pattern classification;principal component analysis;sampling methods;statistical analysis","TRECVID concepts;PCA;principal component analysis;low-level features;CNN;convolutional neural networks;balanced training dataset;multiple correspondence analysis;data pruning process;synthetic instances;ADASYN;adaptive synthetic sampling approach;supervised learning;bootstrapping techniques;extended deep learning approach;imbalanced data classification;MCA-based instance selection","","8","51","","","","","IEEE","IEEE Conferences"
"Probabilistic Graphical Models and Deep Belief Networks for Prognosis of Breast Cancer","M. Khademi; N. S. Nedialkov","Dept. of Comput. & Software, McMaster Univ., Hamilton, ON, Canada; Dept. of Comput. & Software, McMaster Univ., Hamilton, ON, Canada","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","727","732","We propose a probabilistic graphical model (PGM) for prognosis and diagnosis of breast cancer. PGMs are suitable for building predictive models in medical applications, as they are powerful tools for making decisions under uncertainty from big data with missing attributes and noisy evidence. Previous work relied mostly on clinical data to create a predictive model. Moreover, practical knowledge of an expert was needed to build the structure of a model, which may not be accurate. In our opinion, since cancer is basically a genetic disease, the integration of microarray and clinical data can improve the accuracy of a predictive model. However, since microarray data is high-dimensional, including genomic variables may lead to poor results for structure and parameter learning due to the curse of dimensionality and small sample size problems. We address these problems by applying manifold learning and a deep belief network (DBN) to microarray data. First, we construct a PGM and a DBN using clinical and microarray data, and extract the structure of the clinical model automatically by applying a structure learning algorithm to the clinical data. Then, we integrate these two models using softmax nodes. Extensive experiments using real-world databases, such as METABRIC and NKI, show promising results in comparison to Support Vector Machines (SVMs) and k-Nearest Neighbors (k-NN) classifiers, for classifying tumors and predicting events like recurrence and metastasis.","","","10.1109/ICMLA.2015.196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424407","probabilistic graphical models;deep belief networks;breast cancer;microarray data","Manifolds;Breast cancer;Training;Prognostics and health management;Approximation algorithms;Probabilistic logic","bioinformatics;cancer;genomics;graph theory;learning (artificial intelligence);pattern classification;probability;tumours","deep-belief networks;breast cancer prognosis;probabilistic graphical model;breast cancer diagnosis;PGM;medical applications;predictive model;genetic disease;microarray data integration;clinical data integration;predictive model accuracy improvement;high-dimensional microarray data;genomic variables;structure learning;parameter learning;curse-of-dimensionality;manifold learning;DBN;structure learning algorithm;softmax nodes;METABRIC database;NKI database;support vector machine classifier;SVM classifier;k-nearest neighbors classifier;k-NN classifier;tumors;recurrence event prediction;metastasis event prediction","","8","20","","","","","IEEE","IEEE Conferences"
"Combining deep learning and unsupervised clustering to improve scene recognition performance","A. Kappeler; R. D. Morris; A. R. Kamat; N. Rasiwasia; G. Aggarval","Northwestern University, 633 Clark Street, Evanston, IL 60208, USA; Yahoo! Inc., 701 1st Ave, Sunnyvale, CA 94089, USA; Yahoo! Inc., 701 1st Ave, Sunnyvale, CA 94089, USA; Yahoo! Inc., 701 1st Ave, Sunnyvale, CA 94089, USA; Yahoo! Inc., 701 1st Ave, Sunnyvale, CA 94089, USA","2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP)","","2015","","","1","6","Deep Neural Networks (DNN) are now the state-of-the-art for many image and object recognition tasks, as illustrated by their performance on standard benchmarks. The success of DNNs is attributed to their ability to learn rich mid-level image representations, as opposed to hand-designed low-level features used in other image analysis methods. Typically a large dataset of unlabeled images is used for unsupervised feature learning, and then standard classifiers are trained on the features extracted from the images in a labeled set. In this paper, we show that clustering the images using the features from the DNN allows more accurate per-cluster classifiers to be learned, which improves the overall classification accuracy. We demonstrate the effectiveness of our approach on a scene recognition task.","","","10.1109/MMSP.2015.7340859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340859","","Training;Feature extraction;Neural networks;Image representation;Support vector machines;Sun;Machine learning","neural nets;object recognition;unsupervised learning","deep learning;unsupervised clustering;scene recognition performance;deep neural networks;DNN;object recognition;mid-level image representations;unsupervised feature learning","","","18","","","","","IEEE","IEEE Conferences"
"Deep convolutional neural networks for dense non-uniform motion deblurring","J. Cronje","Council for Scientific and Industrial Research, Pretoria, South Africa","2015 International Conference on Image and Vision Computing New Zealand (IVCNZ)","","2015","","","1","5","The work in this paper address the problem of removing non-uniform motion blur from a single image. The motion vector for an image patch is estimated by using a convolutional neural network (CNN). All the predicted motion vectors are combined to form a dense non-uniform motion estimation map. Furthermore, a second CNN is trained to perform deblurring given a blurry image patch and the estimated motion vector. Combining the two trained networks result in a deep learning approach that can enhance degraded images. The results show that this approach can accurately determine non-uniform motion blur and restore blurred images.","","","10.1109/IVCNZ.2015.7761567","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7761567","","Kernel;Training;Machine learning;Estimation;Deconvolution;Neural networks;Image restoration","image restoration;learning (artificial intelligence);motion estimation;neural nets;vectors","dense nonuniform motion deblurring;deep convolutional neural networks;motion vector;dense nonuniform motion estimation map;CNN;blurry image patch;deep learning;blurred image restoration","","","16","","","","","IEEE","IEEE Conferences"
"Deep multimodal learning for Audio-Visual Speech Recognition","Y. Mroueh; E. Marcheret; V. Goel","Poggio Lab, CSAIL, MIT, USA; IBM T.J Watson Research Center, USA; IBM T.J Watson Research Center, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","2130","2134","In this paper, we present methods in deep multimodal learning for fusing speech and visual modalities for Audio-Visual Automatic Speech Recognition (AV-ASR). First, we study an approach where uni-modal deep networks are trained separately and their final hidden layers fused to obtain a joint feature space in which another deep network is built. While the audio network alone achieves a phone error rate (PER) of 41% under clean condition on the IBM large vocabulary audio-visual studio dataset, this fusion model achieves a PER of 35.83% demonstrating the tremendous value of the visual channel in phone classification even in audio with high signal to noise ratio. Second, we present a new deep network architecture that uses a bilinear softmax layer to account for class specific correlations between modalities. We show that combining the posteriors from the bilinear networks with those from the fused model mentioned above results in a further significant phone error rate reduction, yielding a final PER of 34.03%.","","","10.1109/ICASSP.2015.7178347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178347","Audio-Visual Automatic Speech Recognition (AV-ASR);Multimodal Learning;Deep Neural Networks","Visualization;Speech;Correlation;Training;Joints;Speech recognition;Error analysis","acoustic noise;acoustic signal processing;speech recognition","deep multimodal learning;audio-visual speech recognition;fusing speech;visual modalities;audio-visual automatic speech recognition;AV-ASR;uni-modal deep networks;hidden layers;audio network;phone error rate;IBM large vocabulary audio-visual studio;IBM large vocabulary audio-visual studio dataset;fusion model;PER;visual channel;phone classification;signal-noise ratio;deep network architecture;bilinear softmax layer;bilinear networks;significant phone error rate reduction","","47","19","","","","","IEEE","IEEE Conferences"
"Automatic Document Summarization via Deep Neural Networks","C. Yao; J. Shen; G. Chen","Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China; Health Inf. Center of Zhejiang, Hangzhou, China; Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China","2015 8th International Symposium on Computational Intelligence and Design (ISCID)","","2015","1","","291","296","Automatic document summarization aim to extracting sentences which might cover the main content of a document or documents. To achieve this, many algorithms have been tried to rank the sentences by using task-specific features in a shallow architecture. The main challenge of these approaches is to keep balance between information coverage and redundancy because of absence of discovering the intrinsic semantic representation. Inspired by the recent successful achievement of Deep Learning, this paper proposes a new framework of document summarization via Deep Neural Networks (DNNs). Specifically, we feed the sentences as the input to the visible layer of DNNs. After pretraining layer by layer and fine-tuning, the lower dimensional semantic space can be revealed. Based on this space, we design sentences extraction algorithm to construct the summary. Experiments on the DUC2006 and DUC2007 dataset show that our framework works better than state-of-the-art methods.","","","10.1109/ISCID.2015.83","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7468953","Document Summarization;Deep Learning;Deep Brief Network;Restricted Boltzmann Machine;Auto-encoder;ROUGE","Semantics;Training;Hidden Markov models;Neural networks;Feeds;Data visualization;Feature extraction","document handling;learning (artificial intelligence);neural nets","automatic document summarization;deep neural networks;task-specific features;information coverage;information redundancy;intrinsic semantic representation;deep learning;DNN;visible layer;sentences extraction algorithm;DUC2006 dataset;DUC2007 dataset","","4","31","","","","","IEEE","IEEE Conferences"
"Continuous Travel Time Prediction for Transit Signal Priority Based on a Deep Network","X. Gang; W. Kang; F. Wang; F. Zhu; Y. Lv; X. Dong; J. Riekki; S. Pirttikangas","NA; State Key Lab. of Manage. & Control for Complex Syst., Inst. of Autom., Beijing, China; State Key Lab. of Manage. & Control for Complex Syst., Inst. of Autom., Beijing, China; Cloud Comput. Center, Dongguan, China; Cloud Comput. Center, Dongguan, China; Cloud Comput. Center, Dongguan, China; Fac. of Inf. Technol. & Electr. Eng., Univ. of Oulu, Oulu, Finland; Fac. of Inf. Technol. & Electr. Eng., Univ. of Oulu, Oulu, Finland","2015 IEEE 18th International Conference on Intelligent Transportation Systems","","2015","","","523","528","It has been recognized by many researchers that accurate bus travel time prediction is critical for successful deployment of traffic signal priority (TSP) systems. Although there exist a lot of studies on travel time prediction for Advanced Traveler Information Systems (ATIS), this problem for TSP purpose is a little different and the amount of literature is limited. This paper proposes a deep learning based approach for continuous travel time prediction problem. Parameters of the deep network are fine-tuned following a layer-by-layer pre-training procedure on a dataset generated by traffic simulations. Variables that may affect continuous travel time are selected carefully. Experiments are conducted to validate the performance of the proposed model. The results indicate that the proposed model produces prediction with mean absolute error less than 4 seconds, which is accurate enough for TSP operations. This paper also reveals that, except for obvious factors like speed, travel distance and traffic density, the signal time when the prediction is made is also an important factor affecting travel time.","","","10.1109/ITSC.2015.92","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313184","travel time prediction;transit signal priority;deep learning;deep network","Vehicles;Predictive models;Object oriented modeling;Training;Artificial neural networks;Data models","digital simulation;driver information systems;graph theory;learning (artificial intelligence);network theory (graphs)","traffic simulation;ATIS;advanced traveler information system;deep learning based approach;deep network;TSP;transit signal priority;continuous travel time prediction","","4","27","","","","","IEEE","IEEE Conferences"
"Deep learning, audio adversaries, and music content analysis","C. Kereliuk; B. L. Sturm; J. Larsen","Technical Univ. of Denmark, DTU Compute; Queen Mary Univ. of London, Centre for Digital Music; Technical Univ. of Denmark, DTU Compute","2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)","","2015","","","1","5","We present the concept of adversarial audio in the context of deep neural networks (DNNs) for music content analysis. An adversary is an algorithm that makes minor perturbations to an input that cause major repercussions to the system response. In particular, we design an adversary for a DNN that takes as input short-time spectral magnitudes of recorded music and outputs a high-level music descriptor. We demonstrate how this adversary can make the DNN behave in any way with only extremely minor changes to the music recording signal. We show that the adversary cannot be neutralised by a simple filtering of the input. Finally, we discuss adversaries in the broader context of the evaluation of music content analysis systems.","","","10.1109/WASPAA.2015.7336950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336950","Deep Learning;Music Content Analysis","Signal to noise ratio;Multiple signal classification;Music;Context;Discrete Fourier transforms;Conferences","audio recording;audio signal processing;filtering theory;learning (artificial intelligence);music;neural nets","deep learning;audio adversaries;deep neural networks;DNN;spectral magnitudes;recorded music;high-level music descriptor;music recording signal;music content analysis systems","","2","26","","","","","IEEE","IEEE Conferences"
"Learning Deep Representation with Large-Scale Attributes","W. Ouyang; H. Li; X. Zeng; X. Wang","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1895","1903","Learning strong feature representations from large scale supervision has achieved remarkable success in computer vision as the emergence of deep learning techniques. It is driven by big visual data with rich annotations. This paper contributes a large-scale object attribute database that contains rich attribute annotations (over 300 attributes) for ~180k samples and 494 object classes. Based on the ImageNet object detection dataset, it annotates the rotation, viewpoint, object part location, part occlusion, part existence, common attributes, and class-specific attributes. Then we use this dataset to train deep representations and extensively evaluate how these attributes are useful on the general object detection task. In order to make better use of the attribute annotations, a deep learning scheme is proposed by modeling the relationship of attributes and hierarchically clustering them into semantically meaningful mixture types. Experimental results show that the attributes are helpful in learning better features and improving the object detection accuracy by 2.6% in mAP on the ILSVRC 2014 object detection dataset and 2.4% in mAP on PASCAL VOC 2007 object detection dataset. Such improvement is well generalized across datasets.","","","10.1109/ICCV.2015.220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410577","","Object detection;Semantics;Machine learning;Databases;Computer vision;Visualization;Feature extraction","computer vision;image representation;learning (artificial intelligence);object detection;object recognition","deep representation;feature representation;large scale supervision;computer vision;deep learning technique;large-scale object attribute database;rich attribute annotation;ImageNet object detection;rotation annotation;viewpoint annotation;object part location;part occlusion;part existence;common attribute annotation;class-specific attribute annotation","","11","49","","","","","IEEE","IEEE Conferences"
"A novel feature extraction method using deep neural network for rolling bearing fault diagnosis","W. Lu; X. Wang; C. Yang; T. Zhang","Department of Automation, School of Information Science and Technology, Tsinghua University, Beijing, 100084, China; Tsinghua National Laboratory for Information Science and Technology, Beijing, 100084, China; Tsinghua National Laboratory for Information Science and Technology, Beijing, 100084, China; Department of Automation, School of Information Science and Technology, Tsinghua University, Beijing, 100084, China","The 27th Chinese Control and Decision Conference (2015 CCDC)","","2015","","","2427","2431","Rolling bearing fault diagnosis has received much attention because of its importance for the rotatory machinery. Feature extraction is the crucial part of rolling bearing fault diagnosis, which determines the diagnosis performance greatly. However, features extracted by many available methods cannot guarantee the sensitiveness to every interested fault category, which leads to incomplete diagnosis results and ability absence of handling with the situation that unknown-category fault appears. To solve this issue, the feature extraction method based on deep neural network (DNN) is proposed to extract a meaningful representation for bearing signal in this article. DNN is a new kind of machine learning tool with strong power of representation, which has been utilized as the feature extractors in lots of practical applications successfully. Afterwards, the effectiveness of this proposed approach is presented by using the actual rolling bearing data.","","","10.1109/CCDC.2015.7162328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7162328","Fault Diagnosis;Feature Extraction;Deep Neural Network","Feature extraction;Fault diagnosis;Data mining;Rolling bearings;Training;Artificial neural networks","feature extraction;learning (artificial intelligence);machinery;mechanical engineering computing;neural nets;rolling bearings;signal representation","feature extraction method;deep neural network;rolling bearing fault diagnosis;rotatory machinery;DNN;bearing signal representation;machine learning tool;rolling bearing data","","11","15","","","","","IEEE","IEEE Conferences"
"Hyper-parameter optimization of deep convolutional networks for object recognition","S. S. Talathi","Qualcomm Research Center, 5775 Morehouse Dr, San Diego CA 92121","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","3982","3986","Recently sequential model based optimization (SMBO) has emerged as a promising hyper-parameter optimization strategy in machine learning. In this work, we investigate SMBO to identify architecture hyper-parameters of deep convolution networks (DCNs) object recognition. We propose a simple SMBO strategy that starts from a set of random initial DCN architectures to generate new architectures, which on training perform well on a given dataset. Using the proposed SMBO strategy we are able to identify a number of DCN architectures that produce results that are comparable to state-of-the-art results on object recognition benchmarks.","","","10.1109/ICIP.2015.7351553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351553","hyper-parameter optimization;deep convolution networks;sequential model based optimization","Optimization;Training;Benchmark testing;Convolution;Object recognition;Neurons;Databases","convolution;learning (artificial intelligence);object recognition;optimisation;random processes","hyperparameter optimization;deep convolutional networks;random initial DCN architectures;object recognition;sequential model based optimization;SMBO strategy;machine learning","","4","15","","","","","IEEE","IEEE Conferences"
"Difference of Gaussian statistical features based blind image quality assessment: A deep learning approach","Y. Lv; G. Jiang; M. Yu; H. Xu; F. Shao; S. Liu","Faculty of Information Science and Engineering, Ningbo University, Ningbo, China, 315211; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China, 315211; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China, 315211; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China, 315211; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China, 315211; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China, 315211","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","2344","2348","Nowadays, natural scene statistics (NSS) based blind image quality assessment (BIQA) models trained by machine learning, tend to achieve excellent performance. However, BIQA is still a very challenging research topic due to the lack of reference images. The key of further improvement lies in feature mining and pooling strategy decision. In this work, a new BIQA model is proposed to utilize local normalized multi-scale difference of Gaussian (DoG) response in distorted images as features which show a high correlation with perceptual quality. Then, a three-step-framework based deep neural network (DNN) is designed and employed as the pooling strategy. Compared with the support vector machine (SVM), the proposed three-step-framework DNN can excavate better feature representation, leading to more accurate predictions and stronger generalization ability. The proposed model achieves state-of-the-art performance on two authoritative databases and excellent generalization ability in cross database experiments.","","","10.1109/ICIP.2015.7351221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351221","Blind image quality assessment;deep neural network;stacked auto-encoder;DoG","Support vector machines;Feature extraction;Image quality;Distortion;Databases;Neural networks;Predictive models","feature extraction;Gaussian processes;generalisation (artificial intelligence);image representation;learning (artificial intelligence);neural nets","cross database;authoritative databases;generalization ability;feature representation;DNN;three-step-framework based deep neural network;perceptual quality;high-correlation features;distorted images;DoG response;local normalized multiscale difference-of-Gaussian response;pooling strategy decision;feature mining;machine learning;BIQA models;NSS-based blind image quality assessment models;natural scene statistics;deep learning approach;difference-of-Gaussian statistical features","","16","29","","","","","IEEE","IEEE Conferences"
"Dyadic Multi-resolution Analysis-Based Deep Learning for Arabic Handwritten Character Classification","A. ElAdel; R. Ejbali; M. Zaied; C. B. Amar","Res. Group in Intell. Machines, Sfax, Tunisia; Res. Group in Intell. Machines, Sfax, Tunisia; Res. Group in Intell. Machines, Sfax, Tunisia; Res. Group in Intell. Machines, Sfax, Tunisia","2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)","","2015","","","807","812","The problem addressed in this paper is the classification and recognition of Arabic handwritten characters. As a solution, we present a Neural Network (NN) architecture based on Fast Wavelet Transform (FWT) and Adaboost algorithm. FWT is used to extract character's features, based on Multi-Resolution Analysis (MRA) at different levels of abstraction. These features are used to calculate inputs of hidden layer. After this first step, the features are filtered, using Adaboost algorithm, to select the best corresponding ones to each shape of input characters. The reported results are tested on Arabic handwritten characters dataset with 6000 characters. The classification rate for the different groups of characters are 93.92%. Additionally, the speed of the classification algorithm is tested and reported.","","","10.1109/ICTAI.2015.119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372215","multiresolution analysis;Adaboost;deep learning;dyadic wavelet;Arabic handwritten character;classification","Shape;Feature extraction;Hidden Markov models;Character recognition;Multiresolution analysis;Machine learning;Biological neural networks","feature extraction;handwritten character recognition;image classification;image resolution;learning (artificial intelligence);natural language processing;neural net architecture;wavelet transforms","dyadic multiresolution analysis-based deep learning;Arabic handwritten character classification;neural network architecture;NN architecture;fast wavelet transform;FWT;adaboost algorithm;feature extraction;MRA;classification rate","","7","28","","","","","IEEE","IEEE Conferences"
"Feature learning with deep scattering for urban sound analysis","J. Salamon; J. P. Bello","Center for Urban Science and Progress, New York University, USA; Music and Audio Research Laboratory, New York University, USA","2015 23rd European Signal Processing Conference (EUSIPCO)","","2015","","","724","728","In this paper we evaluate the scattering transform as an alternative signal representation to the mel-spectrogram in the context of unsupervised feature learning for urban sound classification. We show that we can obtain comparable (or better) performance using the scattering transform whilst reducing both the amount of training data required for feature learning and the size of the learned codebook by an order of magnitude. In both cases the improvement is attributed to the local phase invariance of the representation. We also observe improved classification of sources in the background of the auditory scene, a result that provides further support for the importance of temporal modulation in sound segregation.","","","10.1109/EUSIPCO.2015.7362478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362478","Unsupervised learning;scattering transform;acoustic event classification;urban;machine learning","Scattering;Transforms;Signal processing algorithms;Clustering algorithms;Modulation;Spectrogram;Algorithm design and analysis","audio signal processing;learning (artificial intelligence);signal classification","feature learning;deep scattering;urban sound analysis;scattering transform;signal representation;mel-spectrogram;unsupervised feature learning;urban sound classification;local phase invariance;temporal modulation;sound segregation","","20","29","","","","","IEEE","IEEE Conferences"
"Mixed generative and supervised learning modes in Deep Predictive Coding Networks","E. Santana; J. C. Principe","University of Florida, Department of Electrical and Computer Engineering, Gainesville, 32611 USA; University of Florida, Department of Electrical and Computer Engineering, Gainesville, 32611 USA","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","4","In this paper we propose a modification of the Cognitive Architectures for Sensory Processing proposed by Chalasani and Principe. Here we keep the bottom-up data representation through generative models as before, but propose a top-down flow based on backpropagation of gradients for recognition. By treating the bottom-up procedure involved in the inference step as a recursive neural network, we show that supervised learning can be used in conjunction with other layers commonly used for Deep Learning. Also, this allows us to learn models that incorporate at the same time data classification and statistical modeling of the input. We show that this combination provides classification results that are robust to input noise.","","","10.1109/IJCNN.2015.7280746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280746","","Data models;Computational modeling;Adaptation models;Logic gates;Feedforward neural networks","backpropagation;data structures;pattern classification;statistical analysis","supervised learning mode;deep predictive coding network;cognitive architecture;sensory processing;data representation;backpropagation;recursive neural network;deep learning;time data classification;statistical modeling","","","19","","","","","IEEE","IEEE Conferences"
"Active learning for hyperspectral image classification with a stacked autoencoders based neural network","J. Li","Zhejiang Police College Department of Forensic Science Hangzhou 310053, China","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","","2015","","","1","4","Active learning can effectively reduce labelling effort for remote sensing image classification. In this paper, we propose a new active learning method for hyperspectral image classification. We consider batch mode active learning and relatively large amount of data which can be a problem when using current state of the art algorithm based on kernel machines. The active learning procedure is based on the uncertainty sampling strategy and a deep neural network. Stacked autoencoders trained on redundant spatial and spectral features and a few labeled training samples are used to initialize a deep neural network. Uncertainty for a given sample is measured by the difference between the largest two class outputs of the neural network. The less difference there is, the more uncertainty the sample has. Batch of samples with most uncertainty will be selected after label query and added into the training set. Then the neural network is retrained. And such active batch selection will iterate until the budget (the upper limit of label queries) is reached. Experimental results on Pavia university dataset showed that our method outperforms the current support vector machines (SVMs) based multiclass/level uncertainty (MCLU) method both in classification accuracy and generalization capability.","","","10.1109/WHISPERS.2015.8075429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075429","hyperspectralimage;classification;active learning;stacked autoencoders;neural network","Neural networks;Uncertainty;Training;Hyperspectral imaging;Support vector machines;Measurement uncertainty","image classification;image coding;image sampling;learning (artificial intelligence);neural nets;remote sensing;support vector machines","label query;hyperspectral image classification;stacked autoencoders;remote sensing image classification;active learning method;batch mode active learning;uncertainty sampling strategy;deep neural network;spectral features;labeled training samples;active batch selection;MCLU method;multiclass-level uncertainty method;SVM;Pavia University dataset;kernel machines;spatial features;support vector machines","","1","7","","","","","IEEE","IEEE Conferences"
"Scaling Up the Training of Deep CNNs for Human Action Recognition","M. S. Rajeswar; A. R. Sankar; V. N. Balasubramaniam; C. D. Sudheer","Indian Inst. of Technol. Delhi, Delhi, India; Indian Inst. of Technol. Hyderabad, Hyderabad, India; Indian Inst. of Technol. Hyderabad, Hyderabad, India; IBM Res., New Delhi, India","2015 IEEE International Parallel and Distributed Processing Symposium Workshop","","2015","","","1172","1177","Convolutional deep neural networks (CNNs) has been shown to perform well in difficult learning tasks such as object recognition. They are gaining huge importance in recent times but are computationally intensive. Typically trained on massive datasets, two-dimensional CNNs are used for image classification and recognition purposes and consume huge computational time. For applications like human action recognition involving video inputs, their 3D counterparts termed as 3D convolutional neural networks (3D-CNNs) are employed. Scaling up the computations to support large datasets and accelerating the training on these models for high performance has been the need of the hour especially in 3D deep learning models since the extended connectivity of CNN in the time domain takes huge time for training the model. Also there is a need to look at the model parameters and hyper parameters that determine both the computational performance as well as the accuracy of the deep neural network. Accelerators such as Graphics Processing Units (GPUs) and multi-cores provide a means for speeding up the training of CNNs and achieve higher performance by parallelizing the training of these models by taking advantage of data and model parallelism. In this work we use multi-core CPUs and GPUs to scale-up the training of 3D-CNNs. We achieve a faster implementation as well as report how various network parameters affect the performance of the model thereby providing recommendations on initializing the values of the same. The code scales up well on multi-cores and GPUs, with a speedup of 10x on CPUs and achieves almost 12x on GPUs compared to the serial version. Our work infers that 3D-CNN code scales up best on CPUs when the convolution step is implemented with a highly parallel FFT based approach, thereby achieving the performance comparable to GPUs using OpenMP.","","","10.1109/IPDPSW.2015.93","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284443","OpenMP;3DCNNs;Deep Learning;Action Recognition;Scaling","Convolution;Graphics processing units;Computational modeling;Optimization;Training;Three-dimensional displays;Instruction sets","graphics processing units;image classification;learning (artificial intelligence);multiprocessing systems;neural nets;object recognition;time-domain analysis","deep CNN training;human action recognition;learning tasks;object recognition;two-dimensional CNNs;image classification;video inputs;3D convolutional neural networks;3D-CNNs;3D deep learning models;time domain;model parameters;hyper parameters;graphics processing units;GPUs;accelerators;multicore CPUs;parallel FFT based approach;OpenMP","","1","19","","","","","IEEE","IEEE Conferences"
"Deep Feature Learning with Discrimination Mechanism for Brain Tumor Segmentation and Diagnosis","L. Zhao; K. Jia","Multimedia Inf. Process. Group, Beijing Univ. of Technol., Beijing, China; Multimedia Inf. Process. Group, Beijing Univ. of Technol., Beijing, China","2015 International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP)","","2015","","","306","309","Brain tumor segmentation is one of the main challenging problems in computer vision and its early diagnosis is critical to clinics. Segmentation needs to be accurate, efficient and robust to avoid influences caused by various large and complex biases added to images. This paper proposes a multiple convolutional neural network (CNNs) framework with discrimination mechanism which is effective to achieve these goals. First of all, this paper proposes to construct different triplanar 2D CNNs architecture for 3D voxel classification, greatly reducing segmentation time. Experiment is conducted on images provided by Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized by MICCAI 2013 for both training and testing. As T1, T1-enhanced, T2 and FLAIR MRI images are utilized, multimodal features are combined. As a result, accuracy, sensitivity and specificity are comparable in comparison with manual gold standard images and better than state-of-the-art segmentation methods.","","","10.1109/IIH-MSP.2015.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415818","brain tumor segmentation;CNNs;voting strategy","Tumors;Image segmentation;Computer architecture;Magnetic resonance imaging;Feature extraction;Three-dimensional displays;Cancer","biomedical MRI;brain;computational geometry;computer vision;feedforward neural nets;image classification;image segmentation;learning (artificial intelligence);medical image processing;tumours","FLAIR MRI image utilization;T2 image utilization;T1-enhanced image utilization;T1 image utilization;MICCAI 2013;BRATS;multimodal brain tumor image segmentation benchmark;segmentation time reduction;3D voxel classification;triplanar 2D CNN architecture;multiple convolutional neural network framework;computer vision;brain tumor diagnosis;brain tumor segmentation;discrimination mechanism;deep feature learning","","8","14","","","","","IEEE","IEEE Conferences"
"A Deep Embedding Model for Co-occurrence Learning","Y. Shen; R. Jin; J. Chen; X. He; J. Gao; L. Deng","NA; NA; NA; NA; NA; NA","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","","2015","","","631","638","Co-occurrence Data is a common and important information source in many areas, such as the word co-occurrence in the sentences, friends co-occurrence in social networks and products co-occurrence in commercial transaction data, etc, which contains rich correlation and clustering information about the items. In this paper, we study co-occurrence data using a general energy-based probabilistic model, and we analyze three different categories of energy-based model, namely, the L1, L2 and Lk models, which are able to capture different levels of dependency in the co-occurrence data. We also discuss how several typical existing models are related to these three types of energy models, including the Fully Visible Boltzmann Machine (FVBM) (L2), Matrix Factorization (L2), Log-BiLinear (LBL) models (L2), and the Restricted Boltzmann Machine (RBM) model (Lk). Then, we propose a Deep Embedding Model (DEM) (an Lk model) from the energy model in a principled manner. Furthermore, motivated by the observation that the partition function in the energy model is intractable and the fact that the major objective of modeling the co-occurrence data is to predict using the conditional probability, we apply the maximum pseudo-likelihood method to learn DEM. In consequence, the developed model and its learning method naturally avoid the above difficulties and can be easily used to compute the conditional probability in prediction. Interestingly, our method is equivalent to learning a special structured deep neural network using back-propagation and a special sampling strategy, which makes it scalable on large-scale datasets. Finally, in the experiments, we show that the DEM can achieve comparable or better results than state-of-the-art methods on datasets across several application domains.","","","10.1109/ICDMW.2015.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395726","Deep Embedding;Co-occurrence Data","Data models;Analytical models;Computational modeling;Bayes methods;Predictive models;Electronic mail;Correlation","backpropagation;Boltzmann machines;learning (artificial intelligence);matrix decomposition;maximum likelihood estimation;probability;social networking (online)","L2 model;FVBM model;LBL models;special sampling strategy;back-propagation;special structured deep neural network;conditional probability;maximum pseudo-likelihood method;Lk model;DEM;restricted Boltzmann machine model;log-bilinear models;matrix factorization;energy-based model;general energy-based probabilistic model;social networks;co-occurrence learning;deep embedding model","","3","29","","","","","IEEE","IEEE Conferences"
"Facial expression recognition based on transfer learning from deep convolutional networks","Mao Xu; Wei Cheng; Qian Zhao; Li Ma; Fang Xu","School of Computer Science and Engineering, Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China; School of Science, Southwest Petroleum University, Chengdu, China","2015 11th International Conference on Natural Computation (ICNC)","","2015","","","702","708","It is well-known that deep models could extract robust and abstract features. We propose a efficient facial expression recognition model based on transfer features from deep convolutional networks (ConvNets). We train the deep ConvNets through the task of 1580-class face identification on the MSRA-CFW database and transfer high-level features from the trained deep model to recognize expression. To train and test the facial expression recognition model on a large scope, we built a facial expression database of seven basic emotion states and 2062 imbalanced samples depending on four facial expression databases (CK+, JAFFE, KDEF, Pain expressions form PICS). Compared with 50.65% recognition rate based on Gabor features with the seven-class SVM and 78.84% recognition rate based on distance features with the seven-class SVM, we achieve average 80.49% recognition rate with the seven-class SVM classifier on the self-built facial expression database. Considering occluded face in reality, we test our model in the occluded condition and demonstrate the model could keep its ability of classification in the small occlusion case. To increase the ability further, we improve the facial expression recognition model. The modified model merges high-level features transferred from two trained deep ConvNets of the same structure and the different training sets. The modified model obviously improves its ability of classification in the occluded condition and achieves average 81.50% accuracy on the self-built facial expression database.","","","10.1109/ICNC.2015.7378076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7378076","Facial expression recognition;Deep convolutional networks;Transfer learning","Feature extraction;Face recognition;Face;Databases;Support vector machines;Robustness;Emotion recognition","face recognition;feature extraction;Gabor filters;image classification;learning (artificial intelligence);neural nets;support vector machines;visual databases","facial expression recognition;transfer learning;deep convolutional network;ConvNets;feature extraction;face classification;facial expression database;Gabor feature;support vector machine;SVM","","1","27","","","","","IEEE","IEEE Conferences"
"State of health estimation combining robust deep feature learning with support vector regression","L. Q. Qiao; L. J. Xun","Shanghai Jiao Tong University, Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai 200240, China; Shanghai Jiao Tong University, Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai 200240, China","2015 34th Chinese Control Conference (CCC)","","2015","","","6207","6212","Combining Stacked Contractive Auto-Encoders (SCAE) with Support Vector Regression (SVR) method based on mass of data, a novel state of health estimation method is proposed in this paper. With the development of SCAE-SVR, SCAE could learn features automatically for SVR instead of extracting hand-designed features. SCAE is a deep machine learning method of unsupervised statistical algorithm that makes the learned features more robust and efficient. Then Support Vector Regression machine is used to estimate quantitative values dealing with the new feature representations. The composite structure of network not only remedies not enough features abstracted by a simplex shallow machine learning net, but also effectively avoid over-fitting in data regression. State of health estimation for Fuel cell systems from Prognostics and Health Management (PHM) 2014 Data Challenge demonstrates that the proposed method outperforms than other state of health estimation methods based on data-driven.","","","10.1109/ChiCC.2015.7260613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7260613","State of health estimate;CAE;SVR;Fuel cell systems","Computer aided engineering;Feature extraction;Robustness;Training;Estimation;Support vector machines;Noise","condition monitoring;fuel cells;learning (artificial intelligence);power engineering computing;regression analysis;support vector machines","health state estimation;robust deep feature learning;support vector regression;stacked contractive auto-encoders;SVR;SCAE;hand-designed features;deep machine learning method;unsupervised statistical algorithm;learned features;shallow machine learning net;data regression;fuel cell systems;prognostics and health management 2014 data challenge;PHM","","2","16","","","","","IEEE","IEEE Conferences"
"Deep autoencoders augmented with phone-class feature for reverberant speech recognition","M. Mimura; S. Sakai; T. Kawahara","Academic Center for Computing and Media Studies, Kyoto University, Sakyo-ku, 606-8501, Japan; Academic Center for Computing and Media Studies, Kyoto University, Sakyo-ku, 606-8501, Japan; Academic Center for Computing and Media Studies, Kyoto University, Sakyo-ku, 606-8501, Japan","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4365","4369","This paper addresses reverberant speech recognition based on front-end processing using DAE (Deep AutoEncoder) coupled with DNN (Deep Neural Network) acoustic model. DAE can effectively and flexibly learn mapping from corrupted speech to the original clean speech based on the deep learning scheme. While this mapping is conventionally conducted only with the acoustic information, we presume the mapping is also dependent on the phone information. Therefore, we propose a new scheme (pDAE), which augments a phone-class feature to the standard acoustic features as input. Two types of the phone-class feature are investigated. One is the hard recognition result of monophones, and the other is a soft representation derived from the posterior outputs of monophone DNN. In the evaluation on the Reverb Challenge 2014 task, the augmented feature in either type results in a significant improvement (7-8% relative) from the standard DAE. It is also shown that using the soft representation in the training phase is critical.","","","10.1109/ICASSP.2015.7178795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178795","Reverberant speech recognition;Deep Neural Networks (DNN);Deep Autoencoder (DAE)","Speech;Speech recognition;Hidden Markov models;Training;Acoustics;Speech enhancement;Neural networks","acoustic signal processing;feature extraction;learning (artificial intelligence);neural nets;reverberation;signal representation;smart phones;speech coding;speech recognition","reverberant speech recognition;DAE;deep autoencoder;DNN acoustic model;deep neural network;deep learning scheme;mapping;acoustic information;phone class feature;standard acoustic feature;monophone DNN;soft representation","","7","30","","","","","IEEE","IEEE Conferences"
"Pedestrian detection aided by deep learning semantic tasks","Y. Tian; P. Luo; X. Wang; X. Tang","Department of Information Engineering, The Chinese University of Hong Kong, China; Shenzhen Key Lab of CVPR, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China; Department of Electronic Engineering, The Chinese University of Hong Kong, China; Department of Information Engineering, The Chinese University of Hong Kong, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","5079","5087","Deep learning methods have achieved great successes in pedestrian detection, owing to its ability to learn discriminative features from raw pixels. However, they treat pedestrian detection as a single binary classification task, which may confuse positive with hard negative samples (Fig.1 (a)). To address this ambiguity, this work jointly optimize pedestrian detection with semantic tasks, including pedestrian attributes (e.g. `carrying backpack') and scene attributes (e.g. `vehicle', `tree', and `horizontal'). Rather than expensively annotating scene attributes, we transfer attributes information from existing scene segmentation datasets to the pedestrian dataset, by proposing a novel deep model to learn high-level features from multiple tasks and multiple data sources. Since distinct tasks have distinct convergence rates and data from different datasets have different distributions, a multi-task deep model is carefully designed to coordinate tasks and reduce discrepancies among datasets. Extensive evaluations show that the proposed approach outperforms the state-of-the-art on the challenging Caltech [9] and ETH [10] datasets where it reduces the miss rates of previous deep models by 17 and 5.5 percent, respectively.","","","10.1109/CVPR.2015.7299143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299143","","Feature extraction;Semantics;Barium;Vehicles;Detectors;Machine learning;Data models","feature extraction;image classification;learning (artificial intelligence);optimisation;pedestrians","pedestrian detection optimization;deep learning method;binary classification task;feature learning","","133","37","","","","","IEEE","IEEE Conferences"
"DeepID-Net: Deformable deep convolutional neural networks for object detection","W. Ouyang; X. Wang; X. Zeng; Shi Qiu; P. Luo; Y. Tian; H. Li; Shuo Yang; Zhe Wang; Chen-Change Loy; X. Tang","The Chinese University of Hong Kong, China; The Chinese University of Hong Kong, China; The Chinese University of Hong Kong, China; The Chinese University of Hong Kong, China; The Chinese University of Hong Kong, China; The Chinese University of Hong Kong, China; The Chinese University of Hong Kong, China; The Chinese University of Hong Kong, China; The Chinese University of Hong Kong, China; The Chinese University of Hong Kong, China; The Chinese University of Hong Kong, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","2403","2412","In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep learning object detection framework has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of model averaging. The proposed approach improves the mean averaged precision obtained by RCNN [14], which was the state-of-the-art, from 31% to 50.3% on the ILSVRC2014 detection test set. It also outperforms the winner of ILSVRC2014, GoogLeNet, by 6.1%. Detailed component-wise analysis is also provided through extensive experimental evaluation, which provide a global view for people to understand the deep learning object detection pipeline.","","","10.1109/CVPR.2015.7298854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298854","","Object detection;Visualization;Deformable models;Context modeling;Machine learning;Feature extraction;Training","image representation;learning (artificial intelligence);neural nets;object detection","DeepID-Net;deformable deep convolutional neural network;generic object detection;deep learning object detection framework;deep architecture;deformation constrained pooling layer method;def-pooling layer model;geometric constraint;pre-training strategy;feature representation;object detection task;generalization capability;net structure;mean averaged precision;RCNN;ILSVRC2014 detection test set;GoogLeNet;component-wise analysis;deep learning object detection pipeline","","78","","","","","","IEEE","IEEE Conferences"
"Automated Detection of Urban Road Manhole Covers Using Mobile Laser Scanning Data","Y. Yu; H. Guan; Z. Ji","NA; NA; NA","IEEE Transactions on Intelligent Transportation Systems","","2015","16","6","3258","3269","This paper proposes a novel framework for automated detection of urban road manhole covers using mobile laser scanning (MLS) data. First, to narrow searching regions and reduce the computational complexity, road surface points are segmented from a raw point cloud via a curb-based road surface segmentation approach and rasterized into a georeferenced intensity image through inverse distance weighted interpolation. Then, a supervised deep learning model is developed to construct a multilayer feature generation model for depicting high-order features of local image patches. Next, a random forest model is trained to learn mappings from high-order patch features to the probabilities of the existence of urban road manhole covers centered at specific locations. Finally, urban road manhole covers are detected from georeferenced intensity images based on the multilayer feature generation model and random forest model. Quantitative evaluations show that the proposed algorithm achieves an average completeness, correctness, quality, and F1-measure of 0.955, 0.959, 0.917, and 0.957, respectively, in detecting urban road manhole covers from georeferenced intensity images. Comparative studies demonstrate the advantageous performance of the proposed algorithm over other existing methods for rapid and automated detection of urban road manhole covers using MLS data.","","","10.1109/TITS.2015.2413812","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7084661","Deep learning;manhole cover;mobile laser scanning (MLS);random forest;road distress;road safety;Deep learning;manhole cover;mobile laser scanning (MLS);random forest;road distress;road safety","Road safety;Training;Image segmentation;Feature extraction;Machine learning;Algorithm design and analysis","feature extraction;geophysical image processing;image segmentation;intelligent transportation systems;interpolation;learning (artificial intelligence);object detection;random processes;road safety","automated detection;urban road manhole covers detection;mobile laser scanning data;MLS data;computational complexity;road surface points segmentation;raw point cloud;curb-based road surface segmentation approach;georeferenced intensity image;inverse distance weighted interpolation;supervised deep learning model;multilayer feature generation model;local image patches;random forest model;mappings learning;high-order patch features;F1-measure;road safety;intelligent transportation system","","17","28","","","","","IEEE","IEEE Journals"
"An iterative deep learning framework for unsupervised discovery of speech features and linguistic units with applications on spoken term detection","C. Chung; C. Tsai; H. Lu; C. Liu; H. Lee; L. Lee","Graduate Institute of Electrical Engineering, National Taiwan University; Graduate Institute of Communication Engineering, National Taiwan University; Graduate Institute of Communication Engineering, National Taiwan University; Graduate Institute of Electrical Engineering, National Taiwan University; Graduate Institute of Electrical Engineering, National Taiwan University; Graduate Institute of Communication Engineering, National Taiwan University","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","245","251","In this work we aim to discover high quality speech features and Linguistic units directly from unlabeled speech data in a zero resource scenario. The results are evaluated using the metrics and corpora proposed in the Zero Resource Speech Challenge organized at Interspeech 2015. A Multi-layered Acoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics fof the given corpus and the language behind, thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target Deep Neural Network (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are then used as the feedback input to the MAT and the MDNN itself in the next iteration. We call this iterative deep learning framework the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which generates both high quality speech features for the Track 1 of the Challenge and acoustic tokens for the Track 2 of the Challenge. In addition, we performed extra experiments on the same corpora on the application of query-by-example spoken term detection. The experimental results showed the iterative deep learning framework of MAT-DNN improved the detection performance due to better underlying speech features and acoustic tokens.","","","10.1109/ASRU.2015.7404801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404801","zero resource;unsupervised learning;dnn;hmm","Acoustics;Speech;Hidden Markov models;Feature extraction;Measurement;Training;Neural networks","feature extraction;learning (artificial intelligence);neural nets;speech processing","iterative deep learning framework;unsupervised speech features discovery;linguistic units;Zero Resource Speech Challenge;multilayered acoustic tokenizer;acoustic token set;hyperparameters;multitarget deep neural network;MDNN;low-level acoustic features;feature extraction;MAT-DNN framework;query-by-example spoken term detection","","5","30","","","","","IEEE","IEEE Conferences"
"Deep Representations for Iris, Face, and Fingerprint Spoofing Detection","D. Menotti; G. Chiachia; A. Pinto; W. R. Schwartz; H. Pedrini; A. X. Falcão; A. Rocha","Institute of Computing, University of Campinas, Campinas, Brazil; Institute of Computing, University of Campinas, Campinas, Brazil; Institute of Computing, University of Campinas, Campinas, Brazil; Department of Computer Science, Federal University of Minas Gerais, Belo Horizonte, Brazil; Institute of Computing, University of Campinas, Campinas, Brazil; Institute of Computing, University of Campinas, Campinas, Brazil; Institute of Computing, University of Campinas, Campinas, Brazil","IEEE Transactions on Information Forensics and Security","","2015","10","4","864","879","Biometrics systems have significantly improved person identification and authentication, playing an important role in personal, national, and global security. However, these systems might be deceived (or spoofed) and, despite the recent advances in spoofing detection, current solutions often rely on domain knowledge, specific biometric reading systems, and attack types. We assume a very limited knowledge about biometric spoofing at the sensor to derive outstanding spoofing detection systems for iris, face, and fingerprint modalities based on two deep learning approaches. The first approach consists of learning suitable convolutional network architectures for each domain, whereas the second approach focuses on learning the weights of the network via back propagation. We consider nine biometric spoofing benchmarks - each one containing real and fake samples of a given biometric modality and attack type - and learn deep representations for each benchmark by combining and contrasting the two learning approaches. This strategy not only provides better comprehension of how these approaches interplay, but also creates systems that exceed the best known results in eight out of the nine benchmarks. The results strongly indicate that spoofing detection systems based on convolutional networks can be robust to attacks already known and possibly adapted, with little effort, to image-based attacks that are yet to come.","","","10.1109/TIFS.2015.2398817","São Paulo Research Foundation; Federal University of Ouro Preto through the Brazilian National Research Council; Minas Gerais Research Foundation; Coordination for the Improvement of Higher Education Personnel (CAPES) DeepEyes Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029061","Deep Learning;Convolutional Networks;Hyperparameter Architecture Optimization;Filter Weights Learning;Back-propagation;Spoofing Detection;Deep learning;convolutional networks;hyperparameter architecture optimization;filter weights learning;back-propagation;spoofing detection","Iris recognition;Benchmark testing;Face;Optimization;Fingerprint recognition;Computer architecture;Feature extraction","biometrics (access control);face recognition;fingerprint identification;iris recognition","deep representations;fingerprint spoofing detection;face detection;iris detection;person identification;person authentication;global security;national security;personal security;spoofing detection;domain knowledge;biometric reading systems;convolutional network architectures;biometric spoofing benchmarks;image based attacks","","186","90","","","","","IEEE","IEEE Journals"
"The use of deep learning features in a hierarchical classifier learned with the minimization of a non-greedy loss function that delays gratification","Z. Liao; G. Carneiro","ARC Centre of Excellence for Robotic Vision, University of Adelaide, Australia; ARC Centre of Excellence for Robotic Vision, University of Adelaide, Australia","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","4540","4544","Recently, we have observed the traditional feature representations are being rapidly replaced by the deep learning representations, which produce significantly more accurate classification results when used together with the linear classifiers. However, it is widely known that non-linear classifiers can generally provide more accurate classification but at a higher computational cost involved in their training and testing procedures. In this paper, we propose a new efficient and accurate non-linear hierarchical classification method that uses the aforementioned deep learning representations. In essence, our classifier is based on a binary tree, where each node is represented by a linear classifier trained using a loss function that minimizes the classification error in a non-greedy way, in addition to postponing hard classification problems to further down the tree. In comparison with linear classifiers, our training process increases only marginally the training and testing time complexities, while showing competitive classification accuracy results. In addition, our method is shown to generalize better than shallow non-linear classifiers. Empirical validation shows that the proposed classifier produces more accurate classification results when compared to several linear and non-linear classifiers on Pascal VOC07 database.","","","10.1109/ICIP.2015.7351666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351666","","Training;Support vector machines;Testing;Binary trees;Boosting;Complexity theory","minimisation;pattern classification;trees (mathematics)","Pascal VOC07 database;competitive classification accuracy;training process;hard classification problems;classification error;binary tree;nonlinear hierarchical classification method;nonlinear classifiers;deep learning representations;feature representations;delays gratification;nongreedy loss function;minimization;hierarchical classifier;deep learning features","","3","27","","","","","IEEE","IEEE Conferences"
"An improved deep learning architecture for person re-identification","E. Ahmed; M. Jones; T. K. Marks","University of Maryland, 3364 A.V. Williams, College Park, 20740, United States; Mitsubishi Electric Research Labs, 201 Broadway, Cambridge, MA 02139, United States; Mitsubishi Electric Research Labs, 201 Broadway, Cambridge, MA 02139, United States","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3908","3916","In this work, we propose a method for simultaneously learning features and a corresponding similarity metric for person re-identification. We present a deep convolutional architecture with layers specially designed to address the problem of re-identification. Given a pair of images as input, our network outputs a similarity value indicating whether the two input images depict the same person. Novel elements of our architecture include a layer that computes cross-input neighborhood differences, which capture local relationships between the two input images based on mid-level features from each input image. A high-level summary of the outputs of this layer is computed by a layer of patch summary features, which are then spatially integrated in subsequent layers. Our method significantly outperforms the state of the art on both a large data set (CUHK03) and a medium-sized data set (CUHK01), and is resistant to over-fitting. We also demonstrate that by initially training on an unrelated large data set before fine-tuning on a small target data set, our network can achieve results comparable to the state of the art even on a small data set (VIPeR).","","","10.1109/CVPR.2015.7299016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299016","","Computer architecture;Convolution;Measurement;Machine learning;Training;Feature extraction;Image color analysis","image recognition;learning (artificial intelligence)","deep learning architecture;person reidentification;feature learning;similarity metric;deep convolutional architecture;cross-input neighborhood differences;local relationships","","441","32","","","","","IEEE","IEEE Conferences"
"Leveraging valence and activation information via multi-task learning for categorical emotion recognition","R. Xia; Y. Liu","Computer Science Department, The University of Texas at Dallas, Richardson, 75080, USA; Computer Science Department, The University of Texas at Dallas, Richardson, 75080, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","5301","5305","Deep learning technologies have been successfully applied to acoustic emotion recognition lately. In this work, we propose to apply multi-task learning for acoustic emotion recognition based on the Deep Belief Network (DBN) framework. We treat the categorical emotion recognition task as the major task. For the secondary task, we leverage two continuous labels, valence and activation. Two strategies are employed to achieve multi-task learning. First, we map the continuous labels into three categorical labels: low; medium; high, and use classification for the secondary task. Second, we project the continuous labels into [-1; 1] range, and use regression for the secondary task. The combination of the loss functions from the major and secondary tasks is used in the objective function in multi-task learning. After iterative optimization, the values from the last hidden layer are used as features in the backend SVM classifier for emotion classification. Our experimental results show significant improvement over the baseline results using DBN, suggesting the benefit of utilizing additional information in a multi-task learning setup.","","","10.1109/ICASSP.2015.7178983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178983","Emotion Recognition;Multi-task learning;Deep Belief Network","Computational modeling;Training data;Training;Visualization","acoustic signal processing;belief networks;emotion recognition;iterative methods;learning (artificial intelligence);optimisation;regression analysis;signal classification;support vector machines","multitask learning;categorical emotion recognition;deep learning technology;acoustic emotion recognition;deep belief network;continuous label;categorical label;regression analysis;loss functions;iterative optimization;backend SVM classifier;emotion classification;DBN","","5","28","","","","","IEEE","IEEE Conferences"
"Multistage committees of deep feedforward convolutional sparse denoise autoencoder for object recognition","Shicao Luo; Yongsheng Ding; Kuangrong Hao","College of Information Science and Technology, Donghua University, Shanghai 201620, China; College of Information Science and Technology, Donghua University, Shanghai 201620, China; College of Information Science and Technology, Donghua University, Shanghai 201620, China","2015 Chinese Automation Congress (CAC)","","2015","","","565","570","Deep learning and unsupervised feature learning systems are known to achieve good performance in benchmarks by using extremely large architectures with many features at each layer. However, we found that the number of features' contribution to performance is very small when it is more than the threshold. Meanwhile, the size of pooling layer has an important influence on performance. In this paper, we present an unsupervised method to improve the classification result by going deep and combining multistage classifiers in a committee with a small amount of features at each layer. The network is trained layer-wise via denoise autoencoder (dA) with L-BFGS to optimize convolutional kernels and no backpropagation is used. In addition, we regularize the dA encouraging representations to fit sparse for each coding layer. We apply it on the STL-10 dataset which has very few training examples and a large amount of unlabeled data. Experimental results show that our method presents higher performance than the existing ones on the condition via individual network.","","","10.1109/CAC.2015.7382564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382564","multistage classifiers;sparse denoise autoencoder;object recognition;deep learning","Feature extraction;Noise reduction;Training;Kernel;Encoding;Robustness;Convolution","feedforward neural nets;image classification;image coding;image denoising;object recognition;unsupervised learning","multistage committees;deep feedforward convolutional sparse denoise autoencoder;object recognition;deep learning;unsupervised feature learning systems;pooling layer;multistage classifiers;dA;L-BFGS;convolutional kernel optimization;coding layer;STL-10 dataset","","","30","","","","","IEEE","IEEE Conferences"
"NMF2D-based source separation using extreme learning machine","D. Wu; W. L. Woo; S. S. Dlay","School of Electrical and Electronic Engineering, Newcastle University, England, United Kingdom; School of Electrical and Electronic Engineering, Newcastle University, England, United Kingdom; School of Electrical and Electronic Engineering, Newcastle University, England, United Kingdom","2nd IET International Conference on Intelligent Signal Processing 2015 (ISP)","","2015","","","1","5","In this paper, we study Non-negative Matrix Two-Dimensional Factorization (NMF2D) based Single Channel Source Separation (SCSS) using a newly proposed algorithm named Extreme Learning Machine (ELM). Compared with other machine learning algorithms such as Support Vector Machines and Neural Networks, ELM can provide better generalization performance and a much faster learning speed. Unlike conventional researches that concentrate on generating masks for each source, we use ELM to classify estimated sources separated by NMF2D algorithm. We also explore Deep ELM which means more than one hidden layers to improve the performance. While training Deep ELM, a method named layer by layer pre-training is used, but unlike Deep Belief Networks (DNNs) that need to fine-tune the whole network at the end, Deep ELM can be used without iteration fine-tuning. The experiment results show that the performance of proposed method is improved not only in training and testing speed, but also in the quality of separated signal compared with using DNNs and NMF2D.","","","10.1049/cp.2015.1791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745791","Nonnegative Matrix Two-Dimensional Factorizations;Extreme Learning Machine;Single channel source separation","","feedforward neural nets;learning (artificial intelligence);matrix decomposition;source separation","NMF2D-based source separation;extreme learning machine;nonnegative matrix 2D factorization;single channel source separation;Deep ELM method;layer by layer pretraining;separated signal quality","","","","","","","","IET","IET Conferences"
"Deep neural networks employing Multi-Task Learning and stacked bottleneck features for speech synthesis","Z. Wu; C. Valentini-Botinhao; O. Watts; S. King","Centre for Speech Technology Research, University of Edinburgh, United Kingdom; Centre for Speech Technology Research, University of Edinburgh, United Kingdom; Centre for Speech Technology Research, University of Edinburgh, United Kingdom; Centre for Speech Technology Research, University of Edinburgh, United Kingdom","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4460","4464","Deep neural networks (DNNs) use a cascade of hidden representations to enable the learning of complex mappings from input to output features. They are able to learn the complex mapping from text-based linguistic features to speech acoustic features, and so perform text-to-speech synthesis. Recent results suggest that DNNs can produce more natural synthetic speech than conventional HMM-based statistical parametric systems. In this paper, we show that the hidden representation used within a DNN can be improved through the use of Multi-Task Learning, and that stacking multiple frames of hidden layer activations (stacked bottleneck features) also leads to improvements. Experimental results confirmed the effectiveness of the proposed methods, and in listening tests we find that stacked bottleneck features in particular offer a significant improvement over both a baseline DNN and a benchmark HMM system.","","","10.1109/ICASSP.2015.7178814","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178814","Speech synthesis;acoustic model;multi-task learning;deep neural network;bottleneck feature","Hidden Markov models;Speech;Acoustics;Speech synthesis;Pragmatics;Context;Neural networks","learning (artificial intelligence);neural nets;speech synthesis","deep neural networks;multitask learning;stacked bottleneck features;complex mapping;text based linguistic feature;speech acoustic feature;text-to-speech synthesis;hidden representation","","76","25","","","","","IEEE","IEEE Conferences"
"Multitask Learning of Deep Neural Networks for Low-Resource Speech Recognition","D. Chen; B. K. Mak","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2015","23","7","1172","1183","We propose a multitask learning (MTL) approach to improve low-resource automatic speech recognition using deep neural networks (DNNs) without requiring additional language resources. We first demonstrate that the performance of the phone models of a single low-resource language can be improved by training its grapheme models in parallel under the MTL framework. If multiple low-resource languages are trained together, we investigate learning a set of universal phones (UPS) as an additional task again in the MTL framework to improve the performance of the phone models of all the involved languages. In both cases, the heuristic guideline is to select a task that may exploit extra information from the training data of the primary task(s). In the first method, the extra information is the phone-to-grapheme mappings, whereas in the second method, the UPS helps to implicitly map the phones of the multiple languages among each other. In a series of experiments using three low-resource South African languages in the Lwazi corpus, the proposed MTL methods obtain significant word recognition gains when compared with single-task learning (STL) of the corresponding DNNs or ROVER that combines results from several STL-trained DNNs.","","","10.1109/TASLP.2015.2422573","Research Grants Council of Hong Kong SAR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7084614","Deep neural network (DNN);low-resource speech recognition;multitask learning;universal grapheme set;universal phone set","Acoustics;Hidden Markov models;Training;Uninterruptible power systems;Speech;Data models;Neural networks","learning (artificial intelligence);natural language processing;neural nets;speech recognition","multitask learning approach;deep neural networks;low-resource speech recognition;MTL approach;DNN;single low-resource language;universal phones;phone-to-grapheme mappings;South African languages;Lwazi corpus","","8","58","","","","","IEEE","IEEE Journals"
"A data mining model of knowledge discovery based on the deep learning","Y. Ma; Y. Tan; C. Zhang; Y. Mao","Application Management office of SINOPEC IT management Department, Beijing, 100728, China; Karamay Hongyou Software Co., Xinjiang, 834000, China; Karamay Hongyou Software Co., Xinjiang, 834000, China; Karamay Municipal People's Government Bureau of Information Industry, Xinjiang, 834000, China","2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA)","","2015","","","1212","1216","With the development of the database technology and the spread of the internet, the amount of the data in databases increases at an exponential speed, which yields the difficult problems of “excess data” and “information explosion”, etc. The traditional database technology is restricted in reading and writing, querying and basic statics operations, but can't acquire the deep data attributes or implicit information. Facing with the huge database in all kinds of fields, it is more and more difficult to cope with the big data only by using conventional technology. New technique to deal with these data at a high level is eagerly demanded. Therefore, the KDD (Knowledge Discovery in Database) technology arises at the historic moment. KDD is an integrated process, which includes data input, iterative solving, user interface and many other custom requirements and design decisions, where the data mining (DM) is a key and specific step in KDD. This paper deeply analyzes state of the art technology of DM, and points out the challenge and technological bottleneck of DM. Moreover, a data mining model architecture of knowledge discovery based on deep learning is proposed.","","","10.1109/ICIEA.2015.7334292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334292","KDD;data mining model;deep leaning","Data mining;Machine learning;Data models;Classification algorithms;Databases;Clustering algorithms;Algorithm design and analysis","data mining;database management systems;Internet;iterative methods;learning (artificial intelligence);user interfaces","knowledge discovery;deep learning;database technology development;Internet;KDD technology;integrated process;data input;iterative solving;user interface;custom requirements;design decisions;DM technology;data mining model architecture","","1","26","","","","","IEEE","IEEE Conferences"
"Learning Efficiently- The Deep CNNs-Tree Network","F. Hsu; J. Gubbi; M. Palaniswami","NA; NA; NA","2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA)","","2015","","","1","7","In recent years, deep feature learning has been successfully applied in many fields such as visual recognition, speech recognition, and natural language processing. Based on the recent rapid development in deep learning community, applying Convolutional Neural Network (CNN) has impacted several fields. However, the number of parameters required to develop a sophisticated large CNN model becomes a problem. We aimed at this problem and presented the Deep CNNs-Tree Network model as our solution. By clustering similar channel features in the feature maps, we were able to create a tree of CNNs and replace the original CNN layer with the proposed model. Experiments on popular image datasets, the MNIST and CIFAR-10, has shown that the proposed network achieve similar performance of accuracy when compared to the traditional CNN, and only less than 5% of accuracy loss. A reduction of more than 70% parameters was observed using the proposed method.","","","10.1109/DICTA.2015.7371277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371277","","Convolution;Correlation;Neural networks;Speech recognition;Visualization;Natural language processing;Supervised learning","learning (artificial intelligence);neural nets","deep CNN-tree network model;convolutional neural network;deep feature learning;MNIST dataset;CIFAR-10 dataset","","","30","","","","","IEEE","IEEE Conferences"
"Deep convolutional neural fields for depth estimation from a single image","F. Liu; Chunhua Shen; Guosheng Lin","University of Adelaide, Australia; University of Adelaide, Australia; University of Adelaide, Australia","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","5162","5170","We consider the problem of depth estimation from a single monocular image in this work. It is a challenging task as no reliable depth cues are available, e.g., stereo correspondences, motions etc. Previous efforts have been focusing on exploiting geometric priors or additional sources of information, with all using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) are setting new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated into a continuous conditional random field (CRF) learning problem. Therefore, we in this paper present a deep convolutional neural field model for estimating depths from a single image, aiming to jointly explore the capacity of deep CNN and continuous CRF. Specifically, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. The proposed method can be used for depth estimations of general scenes with no geometric priors nor any extra information injected. In our case, the integral of the partition function can be analytically calculated, thus we can exactly solve the log-likelihood optimization. Moreover, solving the MAP problem for predicting depths of a new image is highly efficient as closed-form solutions exist. We experimentally demonstrate that the proposed method outperforms state-of-the-art depth estimation methods on both indoor and outdoor scene datasets.","","","10.1109/CVPR.2015.7299152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299152","","Estimation;Training;Optimization;Approximation methods;Neural networks;Closed-form solutions;Semantics","computer vision;learning (artificial intelligence);neural nets;optimisation;random processes;stereo image processing","deep convolutional neural fields;single monocular image;stereo correspondences;hand-crafted features;deep convolutional neural networks;vision applications;depth estimations;continuous conditional random field learning problem;CRF learning problem;deep convolutional neural field model;deep CNN;continuous CRF;deep structured learning scheme;log-likelihood optimization;MAP problem;depth prediction;closed-form solutions","","204","24","","","","","IEEE","IEEE Conferences"
"Towards a flipped cyber classroom to facilitate active learning strategies","S. W. Turner; M. Allison; Z. Syed; M. Farmer","Department of Computer Science, Engineering, and Physics, The University of Michigan-Flint, Flint, MI 48502-1950; Department of Computer Science, Engineering, and Physics, The University of Michigan-Flint, Flint, MI 48502-1950; Department of Computer Science, Engineering, and Physics, The University of Michigan-Flint, Flint, MI 48502-1950; Department of Computer Science, Engineering, and Physics, The University of Michigan-Flint, Flint, MI 48502-1950","2015 IEEE Frontiers in Education Conference (FIE)","","2015","","","1","4","Success within the distance learning paradigm typically requires a strong intrinsic motivation on the part of the learner. This motivation needs to be matched with effective delivery of engaging course content. Developing an active engagement environment for deep learning is a nontrivial task as the instructor is not physically present. To address the concern in an online/blended delivery model, we had developed and examined our first cyber classroom, in which lectures are automatically recorded and posted to a website for subsequent online consumption. The observed limitations of this approach were twofold: (1) online student participation in onground classroom activities was unfeasible; and (2) onground student collaborative activities were severely curtailed by the classroom layout. In this work in progress we present our revised concept grounded in active learning scientific principles combined with lessons learned from our prior approach. By leveraging classroom spatial layout best practices combined with the apropos technology, we seek to address the aforementioned two concerns. Specifically we present a description of our revised cyber classroom, along with plans for future application involving integrating online learning with the flipped classroom concept.","","","10.1109/FIE.2015.7344136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344136","","Computer aided instruction;Layout;Collaboration;Streaming media;Conferences;Computer science","distance learning;educational courses;human factors;learning (artificial intelligence)","flipped cyber classroom;active learning strategy;distance learning;intrinsic motivation;deep learning;blended delivery model;online delivery model;Website;onground student collaborative activity;active learning scientific principle","","","25","","","","","IEEE","IEEE Conferences"
"Unsupervised learning of acoustic features via deep canonical correlation analysis","W. Wang; R. Arora; K. Livescu; J. A. Bilmes","TTI-Chicago, USA; Johns Hopkins University, USA; TTI-Chicago, USA; University of Washington, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4590","4594","It has been previously shown that, when both acoustic and articulatory training data are available, it is possible to improve phonetic recognition accuracy by learning acoustic features from this multi-view data with canonical correlation analysis (CCA). In contrast with previous work based on linear or kernel CCA, we use the recently proposed deep CCA, where the functional form of the feature mapping is a deep neural network. We apply the approach on a speaker-independent phonetic recognition task using data from the University of Wisconsin X-ray Microbeam Database. Using a tandem-style recognizer on this task, deep CCA features improve over earlier multi-view approaches as well as over articulatory inversion and typical neural network-based tandem features. We also present a new stochastic training approach for deep CCA, which produces both faster training and better-performing features.","","","10.1109/ICASSP.2015.7178840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178840","multi-view learning;neural networks;deep canonical correlation analysis;XRMB;articulatory measurements","Speech recognition;Principal component analysis;Training;Mel frequency cepstral coefficient;Artificial intelligence;Speech;Kernel","acoustic signal processing;correlation methods;feature extraction;neural nets;speaker recognition;speech processing","unsupervised learning;acoustic features;deep canonical correlation analysis;CCA;articulatory training data;feature mapping;deep neural network;speaker-independent phonetic recognition task;University of Wisconsin X-ray Microbeam Database","","37","28","","","","","IEEE","IEEE Conferences"
"Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer's Disease","S. Liu; S. Liu; W. Cai; H. Che; S. Pujol; R. Kikinis; D. Feng; M. J. Fulham; ADNI","Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, University of Sydney, Sydney, N.S.W., Australia; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, University of Sydney; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, University of Sydney; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, University of Sydney; Surgical Planning Laboratory, Department of Radiology, Brigham and Womens Hospital, Harvard Medical School; Surgical Planning Laboratory, Department of Radiology, Brigham and Womens Hospital, Harvard Medical School; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, University of Sydney; Department of PET and Nuclear Medicine, Royal Prince Alfred Hospital, Sydney Medical School, University of Sydney; NA","IEEE Transactions on Biomedical Engineering","","2015","62","4","1132","1140","The accurate diagnosis of Alzheimer's disease (AD) is essential for patient care and will be increasingly important as disease modifying agents become available, early in the course of the disease. Although studies have applied machine learning methods for the computer-aided diagnosis of AD, a bottleneck in the diagnostic performance was shown in previous methods, due to the lacking of efficient strategies for representing neuroimaging biomarkers. In this study, we designed a novel diagnostic framework with deep learning architecture to aid the diagnosis of AD. This framework uses a zero-masking strategy for data fusion to extract complementary information from multiple data modalities. Compared to the previous state-of-the-art workflows, our method is capable of fusing multimodal neuroimaging features in one setting and has the potential to require less labeled data. A performance gain was achieved in both binary classification and multiclass classification of AD. The advantages and limitations of the proposed framework are discussed.","","","10.1109/TBME.2014.2372011","ARC; AADRF; NA-MIC; NAC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963480","Alzheimer’s Disease;Classification;Neuroimaging;MRI;PET;Deep Learning;Alzheimer's disease (AD);classification;deep Learning;MRI;neuroimaging;positron emission tomography (PET)","Feature extraction;Training;Positron emission tomography;Neurons;Neuroimaging;Diseases;Biomarkers","biomedical MRI;diseases;learning (artificial intelligence);neurophysiology;patient care;patient diagnosis;positron emission tomography;sensor fusion","multimodal neuroimaging feature learning;Alzheimer disease multiclass diagnosis;patient care;machine learning method;Alzheimer disease computer-aided diagnosis;neuroimaging biomarker;zero-masking strategy;data fusion;Alzheimer disease binary classification;Alzheimer disease multiclass classification","Alzheimer Disease;Brain;Humans;Image Interpretation, Computer-Assisted;Multimodal Imaging;Neuroimaging;Support Vector Machine","143","65","","","","","IEEE","IEEE Journals"
"Wishart RBM based DBN for polarimetric synthetic radar data classification","Y. Guo; S. Wang; C. Gao; D. Shi; D. Zhang; B. Hou","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an 710071, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an 710071, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an 710071, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an 710071, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an 710071, P. R. China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an 710071, P. R. China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","1841","1844","Deep Belief Network (DBN) is a classic deep learning model, and it can learn higher feature and do better classification job. We combine DBN's basic component Restricted Boltzmann Machines (RBM) with the statistic distribution of Polarimetric SAR (PolSAR) data. Based on it, we develop a deep learning classification method that is suitable for PolSAR data. To verify the effectiveness of the method, a real PolSAR dataset is tested. Experiment result confirms that the proposed method provides fine improvements both in classification accuracy and visual effect.","","","10.1109/IGARSS.2015.7326150","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326150","polarimetric synthetic radar data;deep learning;feature learning;Wishart distribution","Accuracy;Feature extraction;Data models;Machine learning;Support vector machines;Yttrium;Geoscience and remote sensing","radar polarimetry;synthetic aperture radar","deep belief network;DBN;polarimetric synthetic radar data classification;classic deep learning model;Wishart RBM based DBN;restricted Boltzmann machines;RBM;statistic distribution;PolSAR data;PolSAR dataset;visual effect","","10","12","","","","","IEEE","IEEE Conferences"
"Deep Learning Architectures for Soil Property Prediction","M. Veres; G. Lacey; G. W. Taylor","Sch. of Eng., Univ. of Guelph, Guelph, ON, Canada; Sch. of Eng., Univ. of Guelph, Guelph, ON, Canada; Sch. of Eng., Univ. of Guelph, Guelph, ON, Canada","2015 12th Conference on Computer and Robot Vision","","2015","","","8","15","Advances in diffuse reflectance infra-red spec-cryoscopy measurements have made it possible to estimate number of functional properties of soil inexpensively and accurately. Core to such techniques are machine learning methods that can map high-dimensional spectra to real-valued outputs. While previous works have considered predicting each property individually using simple regression methods, the correlation structure present in the output variables prompts us to consider methods that can leverage this structure to make more accurate predictions. In this paper, we leverage advances in deep learning architectures, specifically convolution neural networks and conditional restricted Boltzmann machines for structured output prediction for soil property prediction. We evaluate our methods on two recent spectral datasets, where output soil properties are shown to have a measurable degree of correlation.","","","10.1109/CRV.2015.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158315","deep learning;convolutional neural network;restricted Boltzmann machines;structured output;reflectance spectroscopy;soil analysis","Soil properties;Artificial neural networks;Training;Convolution;Computer architecture","Boltzmann machines;geophysics computing;learning (artificial intelligence);soil","deep learning architecture;soil property prediction;diffuse reflectance infrared spec-cryoscopy measurements;simple regression methods;correlation structure;convolution neural networks;conditional restricted Boltzmann machines;correlation degree","","3","35","","","","","IEEE","IEEE Conferences"
"Learning Feature Hierarchies: A Layer-Wise Tag-Embedded Approach","Z. Yuan; C. Xu; J. Sang; S. Yan; M. S. Hossain","National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National University of Singapore, Singapore; SWE Dept., College of Computer and Information Sciences, King Saud University, Riyadh, KSA","IEEE Transactions on Multimedia","","2015","17","6","816","827","Feature representation learning is an important and fundamental task in multimedia and pattern recognition research. In this paper, we propose a novel framework to explore the hierarchical structure inside the images from the perspective of feature representation learning, which is applied to hierarchical image annotation. Different from the current trend in multimedia analysis of using pre-defined features or focusing on the end-task “flat” representation, we propose a novel layer-wise tag- embedded deep learning (LTDL) model to learn hierarchical features which correspond to hierarchical semantic structures in the tag hierarchy . Unlike most existing deep learning models, LTDL utilizes both the visual content of the image and the hierarchical information of associated social tags. In the training stage, the two kinds of information are fused in a bottom-up way. Supervised training and multi-modal fusion alternate in a layer-wise way to learn feature hierarchies. To validate the effectiveness of LTDL, we conduct extensive experiments for hierarchical image annotation on a large-scale public dataset. Experimental results show that the proposed LTDL can learn representative features with improved performances.","","","10.1109/TMM.2015.2417777","National Basic Research Program of China; National Natural Science Foundation of China; Beijing Natural Science Foundation; Singapore National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070707","Auto-encoder;deep learning;hierarchical feature learning;social tags","Semantics;Visualization;Learning systems;Multimedia communication;Abstracts;Training;Principal component analysis","feature extraction;image fusion;image representation;learning (artificial intelligence);multimedia communication","learning feature hierarchy;layerwise tag embedded approach;feature representation;hierarchical image annotation;multimedia analysis;end task flat representation;hierarchical semantic structure;tag hierarchy;LTDL;visual content;social tags;information fusion;supervised training;multimodal fusion","","5","60","","","","","IEEE","IEEE Journals"
"Transfer learning for speech and language processing","D. Wang; T. F. Zheng","Center for Speech and Language Technologies (CSLT) Research Institute of Information Technology, Tsinghua University; Tsinghua National Lab for Information Science and Technology Beijing, 100084, P.R. China","2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","","2015","","","1225","1237","Transfer learning is a vital technique that generalizes models trained for one setting or task to other settings or tasks. For example in speech recognition, an acoustic model trained for one language can be used to recognize speech in another language, with little or no re-training data. Transfer learning is closely related to multi-task learning (cross-lingual vs. multilingual), and is traditionally studied in the name of `model adaptation'. Recent advance in deep learning shows that transfer learning becomes much easier and more effective with high-level abstract features learned by deep models, and the `transfer' can be conducted not only between data distributions and data types, but also between model structures (e.g., shallow nets and deep nets) or even model types (e.g., Bayesian models and neural models). This review paper summarizes some recent prominent research towards this direction, particularly for speech and language processing. We also report some results from our group and highlight the potential of this very interesting research field1.","","","10.1109/APSIPA.2015.7415532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415532","","Data models;Speech;Adaptation models;Speech processing;Learning systems;Speech recognition;Conferences","learning (artificial intelligence);linguistics;speech recognition","speech processing;language processing;transfer learning;speech recognition;acoustic model;multitask learning;cross-lingual;multilingual;model adaptation;deep learning;high-level abstract features;deep models;data distributions;data types;model structures;shallow nets;deep nets","","32","150","","","","","IEEE","IEEE Conferences"
"Interleaved text/image Deep Mining on a large-scale radiology database","H. Shin; Le Lu; L. Kim; A. Seff; J. Yao; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Bethesda, MD 20892-1182, United States; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Bethesda, MD 20892-1182, United States; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Bethesda, MD 20892-1182, United States; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Bethesda, MD 20892-1182, United States; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Bethesda, MD 20892-1182, United States; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Bethesda, MD 20892-1182, United States","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","1090","1099","Despite tremendous progress in computer vision, effective learning on very large-scale (> 100K patients) medical image databases has been vastly hindered. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's picture archiving and communication system. Instead of using full 3D medical volumes, we focus on a collection of representative ~216K 2D key images/slices (selected by clinicians for diagnostic reference) with text-driven scalar and vector labels. Our system interleaves between unsupervised learning (e.g., latent Dirichlet allocation, recurrent neural net language models) on document- and sentence-level texts to generate semantic labels and supervised learning via deep convolutional neural networks (CNNs) to map from images to label spaces. Disease-related key words can be predicted for radiology images in a retrieval manner. We have demonstrated promising quantitative and qualitative results. The large-scale datasets of extracted key images and their categorization, embedded vector labels and sentence descriptions can be harnessed to alleviate the deep learning “data-hungry” obstacle in the medical domain.","","","10.1109/CVPR.2015.7298712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298712","","Radiology;Semantics;Machine learning;Medical diagnostic imaging;Visualization","computer vision;data mining;image retrieval;learning (artificial intelligence);medical image processing;PACS;radiology;recurrent neural nets;text analysis","interleaved text/image deep mining;large-scale radiology database;computer vision;very large-scale medical image database;interleaved text/image deep learning system;semantic interaction;radiology image;national research hospital;picture archiving and communication system;3D medical volume;unsupervised learning;latent Dirichlet allocation;recurrent neural net language model;document-level text;sentence-level text;semantic label;deep convolutional neural network;CNN;retrieval manner;extracted key image;embedded vector label;sentence description;data-hungry obstacle","","20","47","","","","","IEEE","IEEE Conferences"
"Deep Neural Networks: A Case Study for Music Genre Classification","A. R. Rajanna; K. Aryafar; A. Shokoufandeh; R. Ptucha","Electr. Eng. Dept., Rochester Inst. of Technol., Rochester, NY, USA; Comput. Sci. Dept., Drexel Univ., Philadelphia, PA, USA; Comput. Sci. Dept., Drexel Univ., Philadelphia, PA, USA; Comput. Eng. Dept., Rochester Inst. of Technol., Rochester, NY, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","655","660","Music classification is a challenging problem with many applications in today's large-scale datasets with Gigabytes of music files and associated metadata and online streaming services. Recent success with deep neural network architectures on large-scale datasets has inspired numerous studies in the machine learning community for various pattern recognition and classification tasks such as automatic speech recognition, natural language processing, audio classification and computer vision. In this paper, we explore a two-layer neural network with manifold learning techniques for music genre classification. We compare the classification accuracy rate of deep neural networks with a set of well-known learning models including support vector machines (SVM and '1-SVM), logistic regression and '1-regression in combination with hand-crafted audio features for a genre classification task on a public dataset. Our experimental results show that neural networks are comparable with classic learning models when the data is represented in a rich feature space.","","","10.1109/ICMLA.2015.160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424393","","Music;Spectrogram;Neural networks;Manifolds;Feature extraction;Support vector machines;Mel frequency cepstral coefficient","learning (artificial intelligence);music;neural nets;pattern classification","deep neural networks;music genre classification;music files;associated metadata;online streaming services;deep neural network architectures;machine learning community;pattern recognition task;two-layer neural network;manifold learning","","8","46","","","","","IEEE","IEEE Conferences"
"Modeling video-based anomaly detection using deep architectures: Challenges and possibilities","Y. S. Chong; Y. H. Tay","Lee Kong Chian Faculty of Engineering and Science, Universiti Tunku Abdul Rahman, KL, Malaysia; Lee Kong Chian Faculty of Engineering and Science, Universiti Tunku Abdul Rahman, KL, Malaysia","2015 10th Asian Control Conference (ASCC)","","2015","","","1","8","We are looking to perform anomaly detection in video streams, within the fastest time possible, and without the need to hand-engineer features to suit for particular scenes. In any scene captured by surveillance camera, there could be single or multiple persons (agents) and activities ongoing concurrently, with or without human-object and/or human-human interactions. These characteristics lead to a very interesting problem, which involves techniques and insights from a number of domains-anomaly detection, activity recognition, sequence modeling, and deep learning. First, we need to know how to represent video frames as a set of features, then model the temporal sequence and the spatio-temporal relations in the sequence, followed by training the system using some machine learning algorithm on the training set of sequences. The trained system would be able to tell when there is an anomaly in the input stream. However, this is very challenging due to large variations in environment and human movement, and also due to the vague definition of anomaly in the domain of video surveillance. In this paper, we would like to give informational insights on how techniques from the four domains above can be applied to perform video-based anomaly detection.","","","10.1109/ASCC.2015.7244871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7244871","Deep learning;anomaly detection;sequence modeling;activity recognition;video surveillance","Hidden Markov models;Feature extraction;Training;Data models;Streaming media;Trajectory;Computer architecture","learning (artificial intelligence);security of data;video signal processing;video streaming","video-based anomaly detection modeling;deep architectures;video streams;surveillance camera;human-object;human-human interactions;activity recognition;sequence modeling;deep learning;video frames;temporal sequence;spatio-temporal relations;machine learning algorithm;training set;environment movement;human movement","","","71","","","","","IEEE","IEEE Conferences"
"Text recognition using deep BLSTM networks","A. Ray; S. Rajeswar; S. Chaudhury","Department of Electrical Engineering, Indian Institute of Technology Delhi, India; Department of Electrical Engineering, Indian Institute of Technology Delhi, India; Department of Electrical Engineering, Indian Institute of Technology Delhi, India","2015 Eighth International Conference on Advances in Pattern Recognition (ICAPR)","","2015","","","1","6","This paper presents a Deep Bidirectional Long Short Term Memory (LSTM) based Recurrent Neural Network architecture for text recognition. This architecture uses Connectionist Temporal Classification (CTC) for training to learn the labels of an unsegmented sequence with unknown alignment. This work is motivated by the results of Deep Neural Networks for isolated numeral recognition and improved speech recognition using Deep BLSTM based approaches. Deep BLSTM architecture is chosen due to its ability to access long range context, learn sequence alignment and work without the need of segmented data. Due to the use of CTC and forward backward algorithms for alignment of output labels, there are no unicode re-ordering issues, thus no need of lexicon or postprocessing schemes. This is a script independent and segmentation free approach. This system has been implemented for the recognition of unsegmented words of printed Oriya text. This system achieves 4.18% character level error and 12.11% word error rate on printed Oriya text.","","","10.1109/ICAPR.2015.7050699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7050699","Recurrent Neural Network;Long Short Term Memory;Deep Neural Networks;Text Recognition","Computer architecture;Text recognition;Logic gates;Training;Recurrent neural networks;Speech recognition","image classification;image segmentation;learning (artificial intelligence);recurrent neural nets;text detection","text recognition;deep BLSTM network;deep bidirectional long short term memory;recurrent neural network architecture;connectionist temporal classification;CTC;label learning;deep neural networks;isolated numeral recognition;speech recognition;long range context;sequence alignment learning;forward backward algorithms;output label alignment;script independent approach;segmentation free approach;unsegmented word recognition;printed Oriya text","","18","32","","","","","IEEE","IEEE Conferences"
"Meta-Parameter Free Unsupervised Sparse Feature Learning","A. Romero; P. Radeva; C. Gatta","Department of MAIA, Universitat de Barcelona, Barcelona, Spain; Department of MAIA, Universitat de Barcelona, Barcelona, Spain; Computer Vision Center, Barcelona, Spain","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2015","37","8","1716","1722","We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm, which exploits a new way of optimizing for sparsity. Experiments on CIFAR-10, STL-10 and UCMerced show that the method achieves the state-of-the-art performance, providing discriminative features that generalize well.","","","10.1109/TPAMI.2014.2366129","APIF-UB; MICINN; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6942193","representation learning;unsupervised feature learning;pre-training of deep networks;sparse visual features;Representation learning;unsupervised feature learning;pre-training of deep networks;sparse visual features","Training;Encoding;Optimization;Sociology;Statistics;Vectors;Niobium","feature selection;unsupervised learning","unsupervised sparse feature learning algorithm;meta-parameter free;CIFAR-10;STL-10;UCMerced;sparse visual feature","","16","33","","","","","IEEE","IEEE Journals"
"Statistical embeddings using a multilayer union of subspaces","R. M. Taylor; B. Necioglu","MITRE Corporation, McLean, VA 22102; MITRE Corporation, McLean, VA 22102","2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)","","2015","","","1","6","Toward the goal of improved representation learning, we propose a novel deep architecture for unsupervised feature learning based on a recursive multilayered union of subspaces (UoS) model. The model is able to accurately generate recursive nested signal segments at increasing fields of view as we progress from one layer to the next. The local subspace dimension (latent space) grows linearly while the observation space grows exponentially at increasing layers. We apply locally linear coordination to our model output at the top layer to create a globally aligned coordinate system. This enables a very low-dimensional statistical embedding useful for tasks like compression and retrieval. Although the architecture is able to model arbitrary sensor modalities, we focus on image modeling in this study. We compare the performance of our model to the deep belief network by measuring the structural similarity index for a fixed dimensionality reduction on sample face images from CalTech-101. We also show samples of content-based retrieval results on image patches using the statistical embedding.","","","10.1109/MLSP.2015.7324341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324341","deep learning;mixture of factor analyzers;dimensionality reduction;union of subspaces;locally linear coordination;statistical embedding","Image reconstruction;Indexes;Data models;Nonhomogeneous media;Face;Bayes methods;Training","belief networks;image segmentation;statistical analysis;unsupervised learning","recursive multilayer union-of-subspaces model;representation learning;deep architecture;unsupervised feature learning;UoS model;recursive nested signal segments;local subspace dimension;latent space;observation space;locally linear coordination;globally aligned coordinate system;low-dimensional statistical embedding;arbitrary sensor modalities;image modeling;deep belief network;structural similarity index;fixed dimensionality reduction;face images;CalTech-101;content-based retrieval;image patches","","1","11","","","","","IEEE","IEEE Conferences"
"Object segmentation with deep regression","J. Li; D. Wang; C. Yan; S. Shan","Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China; Institute of Deep Learning, Baidu, Inc., Beijing, 100085, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","1668","1672","Object segmentation has constantly received much attention due to its fundamental role in scene understanding. Traditional methods formulate it as a structured prediction problem, represented by graphical models (GMs). However, most GMs have difficulties in balancing the effectiveness of context modeling and efficiency of model inference. In this paper, we model the contexts implicitly using the deep convolutional neural network (DCNN). Specifically, we reformulate object segmentation as a regression problem and train a deep network end-to-end to learn the nonlinear mapping from the image to the object mask. The large receptive field of the network incorporates wide contexts to update the network parameters, giving an implicit context model. Moreover, the deep architecture is favorable for modeling nonlinearity. The inference of our method is quite efficient, involving only a simple feed-forward pass. Extensive experiments on public datasets demonstrate the advantages of our method.","","","10.1109/ICIP.2015.7351084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351084","Object segmentation;context modeling;deep convolutional neural networks;regression","Context;Context modeling;Training;Feature extraction;Object segmentation;Image segmentation;Birds","feedforward neural nets;image segmentation;regression analysis","graphical model;context modeling;deep convolutional neural network;DCNN;nonlinear mapping;deep regression problem;object segmentation","","","23","","","","","IEEE","IEEE Conferences"
"Improving speech recognition in reverberation using a room-aware deep neural network and multi-task learning","R. Giri; M. L. Seltzer; J. Droppo; D. Yu","University of California, San Diego, USA; Microsoft Research, Redmond, USA; Microsoft Research, Redmond, USA; Microsoft Research, Redmond, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","5014","5018","In this paper, we propose two approaches to improve deep neural network (DNN) acoustic models for speech recognition in reverberant environments. Both methods utilize auxiliary information in training the DNN but differ in the type of information and the manner in which it is used. The first method uses parallel training data for multi-task learning, in which the network is trained to perform both a primary senone classification task and a secondary feature enhancement task using a shared representation. The second method uses a parameterization of the reverberant environment extracted from the observed signal to train a room-aware DNN. Experiments were performed on the single microphone task of the REVERB Challenge corpus. The proposed approach obtained a word error rate of 7.8% on the SimData test set, which is lower than all reported systems using the same training data and evaluation conditions, and 27.5% on the mismatched RealData test set, which is lower than all but two systems.","","","10.1109/ICASSP.2015.7178925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178925","Multi-task learning;deep neural network;reverberation;room impulse response","Reverberation;Speech;Speech recognition;Training data;Microphones;Training","acoustic noise;learning (artificial intelligence);neural nets;reverberation;signal classification;speech processing;speech recognition","speech recognition;reverberation;room aware deep neural network;multitask learning;DNN acoustic models;reverberant environments;auxiliary information;DNN training;parallel training data;primary senone classification task;secondary feature enhancement task;shared representation;room aware DNN;single microphone task;REVERB Challenge corpus;SimData test set;mismatched RealData test set","","25","17","","","","","IEEE","IEEE Conferences"
"An information fusion approach to recognizing microphone array speech in the CHiME-3 challenge based on a deep learning framework","J. Du; Q. Wang; Y. Tu; X. Bao; L. Dai; C. Lee","University of Science and Technology of China, Hefei, Anhui, P. R. China; University of Science and Technology of China, Hefei, Anhui, P. R. China; University of Science and Technology of China, Hefei, Anhui, P. R. China; University of Science and Technology of China, Hefei, Anhui, P. R. China; University of Science and Technology of China, Hefei, Anhui, P. R. China; Georgia Institute of Technology, Atlanta, Georgia, USA","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","430","435","We present an information fusion approach to robust recognition of microphone array speech for the recently launched 3rd CHiME Challenge. It is based on a deep learning framework with a large neural network consisting of subnets with different architectures. Multiple knowledge sources are integrated via an early fusion of normalized noisy features with different beamforming techniques, speech enhanced features, speaker related features, and other auxiliary features concatenated as the input to each subnet, and a late fusion by combining the outputs of all subnets to produce one single output set. Our experiments demonstrate that all information sources are complementary in our proposed framework. Our best system achieves an average word error rate reduction of 68% from the officially released baseline results on the test set of real data.","","","10.1109/ASRU.2015.7404827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404827","CHiME Challenge;deep learning;information fusion;microphone array;robust speech recognition","Array signal processing;Training;Microphone arrays;Noise measurement;Speech;Speech recognition","array signal processing;feature extraction;learning (artificial intelligence);microphone arrays;neural nets;speaker recognition","information fusion approach;microphone array speech recognition;CHIME-3 challenge;deep learning framework;neural network;speaker related feature;speech enhanced feature;average word error rate reduction;beamforming technique","","3","35","","","","","IEEE","IEEE Conferences"
"Human detection in laser range data using deep learning and 3-D objects","F. Nasiriyan; H. Khotanlou","Department Of Computer Engineering, Bu-Ali Sina University, Hamedan, Iran; Department Of Computer Engineering, Bu-Ali Sina University, Hamedan, Iran","2015 7th Conference on Information and Knowledge Technology (IKT)","","2015","","","1","6","A novel and intelligent method for human detection using laser is presented in this paper. In this method the information is elicited from multiple frames instead of single frames which provides little information. Time has been also added to the data as the third dimension so that the data converted to 3-D data and then were used as short time series to detect 3-D objects. Finally, the data are clustered and then classified using a deep learning network which is called stacked auto encoder. The results show a better and improved performance for this method.","","","10.1109/IKT.2015.7288748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7288748","3-D data;deep learning;human detection;multiple frames;planar laser data;short time series;stacked auto encoder","Feature extraction;Legged locomotion;Tracking;Cost function;Data models;Data mining;Clustering algorithms","laser ranging;learning (artificial intelligence);object detection;pattern classification","human detection;laser range data;3D objects;multiple frame;3D data;data clustering;deep learning network","","1","36","","","","","IEEE","IEEE Conferences"
"Deep Label Distribution Learning for Apparent Age Estimation","X. Yang; B. Gao; C. Xing; Z. Huo; X. Wei; Y. Zhou; J. Wu; X. Geng","Sch. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; Nat. Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; Sch. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; Sch. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; Sch. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; Sch. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China; Nat. Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; Sch. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","344","350","In the age estimation competition organized by ChaLearn, apparent ages of images are provided. Uncertainty of each apparent age is induced because each image is labeled by multiple individuals. Such uncertainty makes this age estimation task different from common chronological age estimation tasks. In this paper, we propose a method using deep CNN (Convolutional Neural Network) with distribution-based loss functions. Using distributions as the training tasks can exploit the uncertainty induced by manual labeling to learn a better model than using ages as the target. To the best of our knowledge, this is one of the first attempts to use the distribution as the target of deep learning. In our method, two kinds of deep CNN models are built with different architectures. After pre-training each deep CNN model with different datasets as one corresponding stream, the competition dataset is then used to fine-tune both deep CNN models. Moreover, we fuse the results of two streams as the final predicted ages. In the final testing dataset provided by competition, the age estimation performance of our method is 0.3057, which is significantly better than the human-level performance (0.34) provided by the competition organizers.","","","10.1109/ICCVW.2015.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406402","","Face;Estimation;Streaming media;Training;Hidden Markov models;Uncertainty;Standards","face recognition;learning (artificial intelligence);neural nets","deep label distribution learning;apparent age estimation;ChaLearn;deep convolutional neural network model;distribution-based loss function;deep CNN model;human-level performance;facial age estimation","","35","20","","","","","IEEE","IEEE Conferences"
"Measuring learning in an open-ended, constructionist-based progamming camp: Developing a set of quantitative measures from qualitative analysis","D. A. Fields; L. C. Quirke; J. Amely","Instructional Tech. & Learning Sci., Utah State University, Logan, USA; Faculty of Information, University of Toronto, Toronto, Canada; Instructional Tech. & Learning Sci, Utah State University, Logan, USA","2015 IEEE Blocks and Beyond Workshop (Blocks and Beyond)","","2015","","","15","17","In this paper we raise the issue of how to assess novice youths' learning of programming in an open-ended, project-based learning environment. One approach could be a way to apply quantitative measures to the analysis of programming education across frequent saves in a variety of open-ended projects. This paper focuses on the first stage of this endeavor: the development of exploratory quantitative measures of youths' learning of computer science concepts through deep qualitative analysis.","","","10.1109/BLOCKS.2015.7368993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7368993","computer science education;big data;Scratch;assessment;novice programmers;constructionism","Programming profession;Parallel processing;Computers;Conferences;Programming environments","computer science education;programming","open-ended-based programming camp;constructionist-based progamming camp;quantitative measures;open-ended learning environment;project-based learning environment;programming education analysis;computer science concepts;deep qualitative analysis","","1","17","","","","","IEEE","IEEE Conferences"
"Work in progress: Blended learning activities development","N. Llobregat-Gómez; F. Minguez; M. -. Roselló; L. M. S. Ruiz","Universitat Politecnica de Valencia, Valencia, Spain; Universitat Politecnica de Valencia, Valencia, Spain; ETSID-Universitat Politecnica de Valencia, Valencia, Spain; ETSID-Universitat Politecnica de Valencia, Valencia, Spain","2015 International Conference on Interactive Collaborative Learning (ICL)","","2015","","","79","81","The upcoming of the European Higher Education Area (EHEA) has brought out a transformation towards more student centered methodologies which go in the line of much more than catching up with the nowadays times and the uprising of the digital new technologies. Encouraging active learning and entrepreneurship are in the skeleton of competencies that the new engineer must achieve for several reasons. Ideas and solutions to new challenges quite often require creative and innovative engineers that have to catch up with a resilient society where environment and conditions are continuously changing. This contrasts with the traditional input-oriented approach to higher education focusing on the transmission of defined curriculum content. Thus there is need to get even further of a learning outcomes orientation, focusing on what the student is expected to know, understand and be able to do. The shift towards a learning outcomes approach is really a major cultural transformation that is taking time to become fully implemented. In order to tackle this issue from the first stages of a just newcomer university student, the authors propose an active methodology in Mathematics theoretical classes for Aerospace Engineering students that provides activities to be developed by them before each theoretical and problem solving exam comes as part of their apprenticeship. In this way the students are faced in front of activities designed to develop their competencies achievement by promoting their deep understanding of the subject and having to confront their engineering skills while learning a basic subject such as Mathematics. Since these activities are not developed in a controlled environment, students must perform an individual test at the time of delivery to check their authorship and understanding of the processes developed at their activities.","","","10.1109/ICL.2015.7318231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318231","active;flipped;partnering;blended;learning;creativity","Decision support systems;Collaborative work;Conferences","engineering education;further education","blended learning activities development;European higher education area;EHEA;student centered methodologies;active learning;entrepreneurship;defined curriculum content;cultural transformation;mathematics theoretical classes;aerospace engineering students;engineering skills","","3","6","","","","","IEEE","IEEE Conferences"
"Sequence searching with deep-learnt depth for condition- and viewpoint-invariant route-based place recognition","M. Milford; S. Lowry; N. Sunderhauf; S. Shirazi; E. Pepperell; B. Upcroft; C. Shen; G. Lin; F. Liu; C. Cadena; I. Reid","Queensland University of Technology Australia, Australian Centre for Robotic Vision, Australia; Queensland University of Technology Australia, Australian Centre for Robotic Vision, Australia; Queensland University of Technology Australia, Australian Centre for Robotic Vision, Australia; Queensland University of Technology Australia, Australian Centre for Robotic Vision, Australia; Queensland University of Technology Australia, Australian Centre for Robotic Vision, Australia; Queensland University of Technology Australia, Australian Centre for Robotic Vision, Australia; The University of Adelaide, Australian Centre for Robotic Vision, Australia; The University of Adelaide, Australian Centre for Robotic Vision, Australia; The University of Adelaide, Australian Centre for Robotic Vision, Australia; The University of Adelaide, Australian Centre for Robotic Vision, Australia; The University of Adelaide, Australian Centre for Robotic Vision, Australia","2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","","2015","","","18","25","Vision-based localization on robots and vehicles remains unsolved when extreme appearance change and viewpoint change are present simultaneously. The current state of the art approaches to this challenge either deal with only one of these two problems; for example FAB-MAP (viewpoint invariance) or SeqSLAM (appearance-invariance), or use extensive training within the test environment, an impractical requirement in many application scenarios. In this paper we significantly improve the viewpoint invariance of the SeqSLAM algorithm by using state-of-the-art deep learning techniques to generate synthetic viewpoints. Our approach is different to other deep learning approaches in that it does not rely on the ability of the CNN network to learn invariant features, but only to produce“good enough” depth images from day-time imagery only. We evaluate the system on a new multi-lane day-night car dataset specifically gathered to simultaneously test both appearance and viewpoint change. Results demonstrate that the use of synthetic viewpoints improves the maximum recall achieved at 100% precision by a factor of 2.2 and maximum recall by a factor of 2.7, enabling correct place recognition across multiple road lanes and significantly reducing the time between correct localizations.","","","10.1109/CVPRW.2015.7301395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301395","","Estimation;Training;Image recognition;Robots;Vehicles;Computational modeling;Trajectory","cellular neural nets;learning (artificial intelligence);object recognition;robot vision","sequence searching;deep-learnt depth;condition-invariant route-based place recognition;viewpoint- invariant route-based place recognition;vision-based localization;robots;vehicles;extreme appearance change;viewpoint change;FAB-MAP;SeqSLAM;state-of-the-art deep learning techniques;CNN network;synthetic viewpoints;day-time imagery;multilane day-night car dataset;place recognition;multiple road lanes","","18","27","","","","","IEEE","IEEE Conferences"
"Self-Organized Mutual Information Maximization Learning for Improved Generalization Performance","R. Kamimura","NA","2015 IEEE International Conference on Systems, Man, and Cybernetics","","2015","","","1613","1618","The paper proposes a new information-theoretic method to improve the generalization performance of multi-layered neural networks, called ""self-organized mutual information maximization learning"". In the method, the self-organizing map (SOM) is successively applied to give the knowledge to the subsequent multi-layered neural networks. In this process, the mutual information between input patterns and competitive neurons is forced to increase by changing the spread parameter. Though several methods to increase information have been proposed in multi-layered neural networks, the present paper is the first to confirm that mutual information play important roles in learning in multi-layered neural networks and how to compute the mutual information. The method was applied to the extended Senate data. In the experiments, it is examined whether mutual information is actually increased by the present method, because mutual information can be seemingly increased by changing the spread parameter. Experimental results shows that even if the parameter responsible for changing mutual information was fixed, mutual information could be increased. This means that neural networks can be organized so as to store information content on input patterns by the present method. In addition, it could be observed that generalization performance was much improved by this increase in mutual information.","","","10.1109/SMC.2015.285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379417","Mutual information;maximum information principle;SOM;deep learning","Mutual information;Conferences;Cybernetics","generalisation (artificial intelligence);information theory;learning (artificial intelligence);self-organising feature maps","self-organized mutual information maximization learning;improved generalization performance;information-theoretic method;multilayered neural networks;self-organizing map;input patterns;competitive neurons;spread parameter","","1","20","","","","","IEEE","IEEE Conferences"
"Driver yawning detection based on deep convolutional neural learning and robust nose tracking","Weiwei Zhang; Y. L. Murphey; Tianyu Wang; Qijie Xu","State Key Laboratory of Advanced Design and Manufacturing for Vehicle Body, Hunan University, Changsha, China; Department of Electrical and Computer Engineering, University of Michigan-Dearborn, USA; Department of Electrical and Computer Engineering, University of Michigan-Dearborn, USA; Department of Electrical and Computer Engineering, University of Michigan-Dearborn, USA","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","Driver yawning detection is one of the key technologies used in driver fatigue monitoring systems. Real-time driver yawning detection is a very challenging problem due to the dynamics in driver's movements and lighting conditions. In this paper, we present a yawning detection system that consists of a face detector, a nose detector, a nose tracker and a yawning detector. Deep learning algorithms are developed for detecting driver face area and nose location. A nose tracking algorithm that combines Kalman filter with a dedicated open-source TLD (Track-Learning-Detection) tracker is developed to generate robust tracking results under dynamic driving conditions. Finally a neural network is developed for yawning detection based on the features including nose tracking confidence value, gradient features around corners of mouth and face motion features. Experiments are conducted on real-world driving data, and results show that the deep convolutional networks can generate a satisfactory classification result for detecting driver's face and nose when compared with other pattern classification methods, and the proposed yawning detection system is effective in real-time detection of driver's yawning states.","","","10.1109/IJCNN.2015.7280566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280566","yawning detection;deep convolutional neural networks;gradient statistics;nose tracking;Kalman filter","Artificial neural networks;Nose;Optical imaging","driver information systems;face recognition;image filtering;Kalman filters;neural nets;tracking","driver yawning detection;deep convolutional neural learning;driver fatigue monitoring systems;face detector;nose detector;nose tracker;yawning detector;driver face area detection;nose location detection;nose tracking algorithm;Kalman filter;open-source TLD tracker;track-learning-detection tracker;nose tracking confidence value;gradient features;deep convolutional networks","","1","23","","","","","IEEE","IEEE Conferences"
"Faster reinforcement learning after pretraining deep networks to predict state dynamics","C. W. Anderson; M. Lee; D. L. Elliott","Department of Computer Science, Colorado State University, Fort Collins, 80523-1873, USA; Department of Computer Science, Colorado State University, Fort Collins, 80523-1873, USA; Department of Computer Science, Colorado State University, Fort Collins, 80523-1873, USA","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","7","Deep learning algorithms have recently appeared that pretrain hidden layers of neural networks in unsupervised ways, leading to state-of-the-art performance on large classification problems. These methods can also pretrain networks used for reinforcement learning. However, this ignores the additional information that exists in a reinforcement learning paradigm via the ongoing sequence of state, action, new state tuples. This paper demonstrates that learning a predictive model of state dynamics can result in a pretrained hidden layer structure that reduces the time needed to solve reinforcement learning problems.","","","10.1109/IJCNN.2015.7280824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280824","","Heuristic algorithms;Dynamics;Classification algorithms;Nickel","learning (artificial intelligence);neural nets;pattern classification","deep networks;state dynamics prediction;deep learning algorithms;neural networks hidden layers;classification problems;reinforcement learning;action sequence;state tuples sequence;predictive model;pretrained hidden layer structure","","13","18","","","","","IEEE","IEEE Conferences"
"Background Prior-Based Salient Object Detection via Deep Reconstruction Residual","J. Han; D. Zhang; X. Hu; L. Guo; J. Ren; F. Wu","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; Department of Electronic and Electrical Engineering, University of Strathclyde, Glasgow, U.K.; School of Information Science, University of Science and Technology of China, Hefei, China","IEEE Transactions on Circuits and Systems for Video Technology","","2015","25","8","1309","1321","Detection of salient objects from images is gaining increasing research interest in recent years as it can substantially facilitate a wide range of content-based multimedia applications. Based on the assumption that foreground salient regions are distinctive within a certain context, most conventional approaches rely on a number of hand-designed features and their distinctiveness is measured using local or global contrast. Although these approaches have been shown to be effective in dealing with simple images, their limited capability may cause difficulties when dealing with more complicated images. This paper proposes a novel framework for saliency detection by first modeling the background and then separating salient objects from the background. We develop stacked denoising autoencoders with deep learning architectures to model the background where latent patterns are explored and more powerful representations of data are learned in an unsupervised and bottom-up manner. Afterward, we formulate the separation of salient objects from the background as a problem of measuring reconstruction residuals of deep autoencoders. Comprehensive evaluations of three benchmark datasets and comparisons with nine state-of-the-art algorithms demonstrate the superiority of this paper.","","","10.1109/TCSVT.2014.2381471","National Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6987333","salient object detection;stacked denoising autoencoder;deep reconstruction residual;Background prior;deep reconstruction residual;salient object detection;stacked denoising autoencoder (SDAE)","Feature extraction;Image reconstruction;Training;Noise reduction;Object detection;Encoding;Robustness","feature extraction;image coding;image denoising;image reconstruction;learning (artificial intelligence);multimedia computing;object detection","background prior-based salient object detection;data representation;deep learning architectures;stacked denoising autoencoders;global contrast;local contrast;foreground salient regions;content-based multimedia applications;deep reconstruction residual","","203","51","","","","","IEEE","IEEE Journals"
"Natural scene recognition based on Convolutional Neural Networks and Deep Boltzmannn Machines","J. Gao; J. Yang; J. Zhang; M. Li","Department of Control & Engineering Beijing University of Technology, NO.100 Chaoyang district, 100124, China; Department of Control & Engineering Beijing University of Technology, NO.100 Chaoyang district, 100124, China; Department of Control & Engineering Beijing University of Technology, NO.100 Chaoyang district, 100124, China; Department of Control & Engineering Beijing University of Technology, NO.100 Chaoyang district, 100124, China","2015 IEEE International Conference on Mechatronics and Automation (ICMA)","","2015","","","2369","2374","Scene recognition is a significant topic in computer vision, and Deep Boltzmann Machines (DBM) is a state-of-the-art deep learning model which has been widely applied in object and hand written digit recognition. However, when the DBM is used in scene recognition, it is difficult to handle large images due to its computational complexity. In this paper, we present a deep learning method based on Convolutional Neural Networks (CNN) and DBM for scene image recognition. First, in order to categorize large images, the CNN is utilized to preprocess images for dimensional reduction. Then, regarding the preprocessed images as the input of the visible layer, the DBM model is trained using Contrastive Divergence (CD) algorithm. Finally, after extracting features by the DBM, the softmax regression is employed to perform scene recognition tasks. Since the CNN can reduce effectively image size, the proposed method can improve the computational efficiency and becomes more suitable for large image recognition. Experimental evaluations using SIFT Flow dataset and fifteen-scene dataset demonstrate that the proposed method can obtain promising results.","","","10.1109/ICMA.2015.7237857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7237857","Deep Boltzmann Machines;Convolutional Neural Networks;Scene Recognition;Deep Learning","Convolution;Feature extraction;Sensitivity;Computational modeling;Training;Mathematical model;Kernel","Boltzmann machines;convolution;feature extraction;image recognition;natural scenes;regression analysis","natural scene recognition;convolutional neural networks;deep Boltzmannn machines;computer vision;DBM;deep learning model;computational complexity;CNN;scene image recognition;large images categorization;dimensional reduction;image preprocessing;visible layer;contrastive divergence;CD algorithm;feature extraction;softmax regression;scene recognition tasks;image size;computational efficiency;SIFT flow dataset;scene dataset","","5","27","","","","","IEEE","IEEE Conferences"
"Stochastic Gradient Variational Bayes for deep learning-based ASR","A. Tjandra; S. Sakti; S. Nakamura; M. Adriani","Faculty of Computer Science, Universitas Indonesia, Indonesia; Graduate School of Information Science, Nara Institute of Science and Technology, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Japan; Faculty of Computer Science, Universitas Indonesia, Indonesia","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","175","180","Many successful methods for training deep neural networks (DNN) rely on an unsupervised pretraining algorithm. It is particularly effective when the number of labeled training samples is not large enough, because pretraining method helps to initialize the parameter values in the appropriate range near a local good minimum, for further discriminative finetuning. However, while the improvement is impressive, training DNN is difficult because the objective function of DNN is highly non-convex function of the parameters. To avoid placing the parameter that generalizes poorly, a robust generative modelling is necessary. This paper explore an alternative of generative modelling for pretraining DNN-based acoustic modelling using Stochastic Gradient Variational Bayes (SGVB) within autoencoder framework called Variational Bayes Autoencoder (VBAE). It performs an efficient approximate inference and learning with directed probabilistic graphical models. During fine-tuning, probabilistic encoder parameters with latent variable components are then used in discriminative training for acoustic model. Here, we investigate the performances of DNN-based acoustic model using the proposed pretrained VBAE in comparison with widely used pretraining algorithms like Restricted Boltzmann Machine (RBM) and Stacked Denoising Autoencoder (SDAE). The results reveal that VBAE pretraining with Gaussian latent variables gave the best performance.","","","10.1109/ASRU.2015.7404791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404791","acoustic model;deep neural network;variational Bayes;autoencoder","Hidden Markov models;Probabilistic logic;Acoustics;Training;Data models;Decoding;Neural networks","directed graphs;gradient methods;inference mechanisms;learning (artificial intelligence);neural nets;speech recognition;stochastic processes","stochastic gradient variational Bayes model;deep learning-based ASR;automatic speech recognition;deep neural networks;DNN training;unsupervised pretraining algorithm;robust generative modelling;SGVB model;autoencoder framework;inference;directed probabilistic graphical model;latent variable components;pretraining algorithm;restricted Boltzmann machine;RBM algorithm;stacked denoising autoencoder algorithm;SDAE algorithm","","4","23","","","","","IEEE","IEEE Conferences"
"Deepshape: Deep learned shape descriptor for 3D shape matching and retrieval","Jin Xie; Yi Fang; Fan Zhu; Edward Wong","Department of Electrical and Computer Engineering, New York University Abu Dhabi, United Arab Emirates; Department of Electrical and Computer Engineering, New York University Abu Dhabi, United Arab Emirates; Department of Electrical and Computer Engineering, New York University Abu Dhabi, United Arab Emirates; Polytechnic School of Engineering, New York University, 11201, United States","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","1275","1283","Complex geometric structural variations of 3D model usually pose great challenges in 3D shape matching and retrieval. In this paper, we propose a high-level shape feature learning scheme to extract features that are insensitive to deformations via a novel discriminative deep auto-encoder. First, a multiscale shape distribution is developed for use as input to the auto-encoder. Then, by imposing the Fisher discrimination criterion on the neurons in the hidden layer, we developed a novel discriminative deep auto-encoder for shape feature learning. Finally, the neurons in the hidden layers from multiple discriminative auto-encoders are concatenated to form a shape descriptor for 3D shape matching and retrieval. The proposed method is evaluated on the representative datasets that contain 3D models with large geometric variations, i.e., Mcgill and SHREC'10 ShapeGoogle datasets. Experimental results on the benchmark datasets demonstrate the effectiveness of the proposed method for 3D shape matching and retrieval.","","","10.1109/CVPR.2015.7298732","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298732","","Shape;Three-dimensional displays;Heating;Solid modeling;Kernel;Feature extraction;Neurons","feature extraction;image matching;image retrieval;learning (artificial intelligence);neural nets;shape recognition","DeepShape;deep learned shape descriptor;3D shape matching;3D shape retrieval;complex geometric structural variations;high-level shape feature learning;feature extraction;discriminative deep autoencoder;multiscale shape distribution;Fisher discrimination criterion;hidden layer neurons;Mcgill dataset;SHREC'10 ShapeGoogle dataset","","8","32","","","","","IEEE","IEEE Conferences"
"Incorporating image degeneration modeling with multitask learning for image super-resolution","Y. Liang; J. Wang; S. Zhang; Y. Gong","Xi'an Jiaotong University, Institute of Artificial Intelligence and Robotics, 28 West Xianning Road, Xi'an, Shaanxi, China, 710049; Xi'an Jiaotong University, Institute of Artificial Intelligence and Robotics, 28 West Xianning Road, Xi'an, Shaanxi, China, 710049; Xi'an Jiaotong University, Institute of Artificial Intelligence and Robotics, 28 West Xianning Road, Xi'an, Shaanxi, China, 710049; Xi'an Jiaotong University, Institute of Artificial Intelligence and Robotics, 28 West Xianning Road, Xi'an, Shaanxi, China, 710049","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","2110","2114","Learning the non-linear image upscaling process has previously been considered as a simple regression process, where various models have been utilized to describe the correlations between high-resolution (HR) and low-resolution (LR) images/patches. In this paper, we present a multitask learning framework based on deep neural network for image super-resolution, where we jointly consider the image super-resolution process and the image degeneration process. By sharing parameters between the two highly relevant tasks, the proposed framework could effectively improve the obtained neural network based mapping model between HR and LR image patches. Experimental results have demonstrated clear visual improvement and high computational efficiency, especially with large magnification factors.","","","10.1109/ICIP.2015.7351173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351173","super-resolution;multitask learning;autoencoder;degeneration modeling","Image resolution;Neural networks;Signal resolution;Image quality;Interpolation;Training;Kernel","image resolution;learning (artificial intelligence);neural nets;regression analysis","image degeneration modeling;multitask learning;image superresolution;nonlinear image upscaling process;regression process;deep neural network based mapping model;high-resolution image patches;low-resolution image patches","","5","23","","","","","IEEE","IEEE Conferences"
"Robust Face Recognition via Multimodal Deep Face Representation","C. Ding; D. Tao","Centre for Quantum Computation and Intelligent Systems, and the Faculty of Engineering and Information Technology, University of Technology, Sydney, Australia; Centre for Quantum Computation and Intelligent Systems, and the Faculty of Engineering and Information Technology, University of Technology, Sydney, Australia","IEEE Transactions on Multimedia","","2015","17","11","2049","2058","Face images appearing in multimedia applications, e.g., social networks and digital entertainment, usually exhibit dramatic pose, illumination, and expression variations, resulting in considerable performance degradation for traditional face recognition algorithms. This paper proposes a comprehensive deep learning framework to jointly learn face representation using multimodal information. The proposed deep learning structure is composed of a set of elaborately designed convolutional neural networks (CNNs) and a three-layer stacked auto-encoder (SAE). The set of CNNs extracts complementary facial features from multimodal data. Then, the extracted features are concatenated to form a high-dimensional feature vector, whose dimension is compressed by SAE. All of the CNNs are trained using a subset of 9,000 subjects from the publicly available CASIA-WebFace database, which ensures the reproducibility of this work. Using the proposed single CNN architecture and limited training data, 98.43% verification rate is achieved on the LFW database. Benefitting from the complementary information contained in multimodal data, our small ensemble system achieves higher than 99.0% recognition rate on LFW using publicly available training set.","","","10.1109/TMM.2015.2477042","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7243358","Convolutional neural networks (CNNs);deep learning;face recognition;multimodal system","Face;Feature extraction;Face recognition;Databases;Training;Multimedia communication;Social network services","face recognition;feature extraction;image representation;learning (artificial intelligence);neural nets","robust face recognition;multimodal deep face representation;face images;comprehensive deep learning framework;face representation;multimodal information;convolutional neural networks;CNN;three-layer stacked auto-encoder;SAE;complementary facial feature extraction;high-dimensional feature vector;CASIA-WebFace database;LFW database","","132","42","","","","","IEEE","IEEE Journals"
"Deep Reconstruction Models for Image Set Classification","M. Hayat; M. Bennamoun; S. An","School of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia; School of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia; School of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2015","37","4","713","727","Image set classification finds its applications in a number of real-life scenarios such as classification from surveillance videos, multi-view camera networks and personal albums. Compared with single image based classification, it offers more promises and has therefore attracted significant research attention in recent years. Unlike many existing methods which assume images of a set to lie on a certain geometric surface, this paper introduces a deep learning framework which makes no such prior assumptions and can automatically discover the underlying geometric structure. Specifically, a Template Deep Reconstruction Model (TDRM) is defined whose parameters are initialized by performing unsupervised pre-training in a layer-wise fashion using Gaussian Restricted Boltzmann Machines (GRBMs). The initialized TDRM is then separately trained for images of each class and class-specific DRMs are learnt. Based on the minimum reconstruction errors from the learnt class-specific models, three different voting strategies are devised for classification. Extensive experiments are performed to demonstrate the efficacy of the proposed framework for the tasks of face and object recognition from image sets. Experimental results show that the proposed method consistently outperforms the existing state of the art methods.","","","10.1109/TPAMI.2014.2353635","SIRF; University of Western Australia; ARC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6888522","Image set classification;deep learning;auto-encoders;video based face recognition;object recognition","Training;Image reconstruction;Manifolds;Decoding;Vectors;Data models;Surface reconstruction","face recognition;image classification;image reconstruction;learning (artificial intelligence);object recognition","image set classification;surveillance video;multiview camera networks;personal albums;single image based classification;geometric surface;deep learning framework;template deep reconstruction model;TDRM;Gaussian restricted Boltzmann machines;GRBM;voting strategy;face recognition;object recognition","","80","45","","","","","IEEE","IEEE Journals"
"Deep Learning for Image Retrieval: What Works and What Doesn't","H. Wang; Y. Cai; Y. Zhang; H. Pan; W. Lv; H. Han","NA; NA; NA; NA; NA; NA","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","","2015","","","1576","1583","To build an industrial content-based image retrieval system (CBIRs), it is highly recommended that feature extraction, feature processing and feature indexing need to be fully considered. Although research that bloomed in the past years suggest that the convolutional neural network (CNN) be in a leading position on feature extraction & representation for CBIRs, there are less instructions on the deep analysis of feature related topics, for example the kind of feature representation that has the best performance among the candidates provided by CNN, the extracted features generalization ability, the relationship between the dimensional reduction and the accuracy loss in CBIRs, the best distance measure technique in CBIRs and the benefit of the coding techniques in improving the efficiency of CBIRs, etc. Therefore, several practicing studies were conducted and a thorough analysis was made in this research attempting to answer the above questions. The results in the study on both ImageNet-2012 and an industrial dataset provided by Sogou demonstrate that fc4096a and fc4096b perform the best on the datasets from unseen categories. Several interesting and practicing conclusions are drawn, for instance, fc4096a and fc4096b are found to have a better generalization ability than other features of CNN and could be considered as the first choice for industrial CBIRs. Furthermore, a novel feature binarization approach is presented in this paper for better efficiency of CBIRs. More specifically, the binarization is capable of reducing 31/32 space usage of original data. To sum up, the conclusions seem to provide practical instructions on real industrial CBIRs.","","","10.1109/ICDMW.2015.121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395863","Deep Learning;Convolutional Neural Network;Content-based Image Retrieval;Feature Representation;Binarization","Feature extraction;Indexing;Training;Image retrieval;Measurement;Convolutional codes;Principal component analysis","content-based retrieval;feature extraction;image retrieval;learning (artificial intelligence);neural nets","deep learning;content-based image retrieval system;CBIR;feature extraction;feature processing;feature indexing;convolutional neural network;CNN;feature representation;generalization ability;distance measure technique;coding technique;ImageNet-2012 dataset;industrial dataset","","10","17","","","","","IEEE","IEEE Conferences"
"Deep Learning Face Attributes in the Wild","Z. Liu; P. Luo; X. Wang; X. Tang","Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, Chile; Shenzhen Key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Dept. of Electron. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, Chile","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","3730","3738","Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.","","","10.1109/ICCV.2015.425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410782","","Face;Feature extraction;Training;Face recognition;Machine learning;Support vector machines;Image recognition","face recognition;feedforward neural nets;image representation;learning (artificial intelligence)","deep learning face attribute prediction;complex face variations;CNN;LNet;ANet;image-level attribute tags;face identities;face representation learning;face localization;pretraining strategies;image-level annotations;attribute recognition;automatic semantic concept discovery","","596","35","","","","","IEEE","IEEE Conferences"
"Achieving ""synergy"" in cognitive behavior of humanoids via deep learning of dynamic visuo-motor-attentional coordination","J. Hwang; M. Jung; N. Madapana; J. Kim; M. Choi; J. Tani","NA; NA; NA; NA; NA; Korea Advanced Institute of Science and Technology","2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)","","2015","","","817","824","The current study examines how adequate coordination among different cognitive processes including visual recognition, attention switching, action preparation and generation can be developed via learning of robots by introducing a novel model, the Visuo-Motor Deep Dynamic Neural Network (VMDNN). The proposed model is built on coupling of a dynamic vision network, a motor generation network, and a higher level network allocated on top of these two. The simulation experiments using the iCub simulator were conducted for cognitive tasks including visual object manipulation responding to human gestures. The results showed that ""synergetic"" coordination can be developed via iterative learning through the whole network when spatio-temporal hierarchy and temporal one can be self-organized in the visual pathway and in the motor pathway, respectively, such that the higher level can manipulate them with abstraction.","","","10.1109/HUMANOIDS.2015.7363448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363448","","Visualization;Hidden Markov models;Robot kinematics;Neural networks;Machine learning;Training","cognitive systems;control engineering computing;humanoid robots;learning (artificial intelligence);neural nets;object recognition;robot programming;robot vision","synergy;cognitive behavior;humanoid robot;visuo-motor deep dynamic neural network;VMDNN;visual recognition;attention switching;action preparation;robot learning;dynamic vision network;motor generation network;iCub simulator;visual object manipulation;iterative learning","","5","29","","","","","IEEE","IEEE Conferences"
"Manifold Regularized Stacked Autoencoder for Feature Learning","S. Lu; H. Liu; C. Li","Sch. of Inf. Sci. & Technol., Tsinghua Univ., Beijing, China; Sch. of Inf. Sci. & Technol., Tsinghua Univ., Beijing, China; Sch. of Inf. Sci. & Technol., Tsinghua Univ., Beijing, China","2015 IEEE International Conference on Systems, Man, and Cybernetics","","2015","","","2950","2955","Stacked auto encoders enjoy their popularization with the prosperity of deep learning in recent years. However, relative studies seldom exploit the intrinsic information buried in the interrelations between the samples with respect to deep networks. Regarding this, the manifold regularization is introduced to analyze the neighborhood of each training sample, which leads to a manifold regularized stacked auto encoder hierarchical framework with deep multilayer substructures. A series of experiments are conducted upon MNIST and Yale Faces using locally linear embedding as the manifold regularization module. The results show that neighborhood analysis should be combined with stacked auto encoders to achieve some notable promotions of their performances.","","","10.1109/SMC.2015.513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379645","manifold regularization;stacked autoencoder;feature learning;locally linear embedding","Manifolds;Training;Artificial neural networks;Machine learning;Optimization;Sparse matrices;Yttrium","learning (artificial intelligence)","manifold regularized stacked autoencoder;feature learning;deep learning;intrinsic information;hierarchical framework;deep multilayer substructures;MNIST database;Yale Faces database;locally linear embedding","","4","14","","","","","IEEE","IEEE Conferences"
"A spectrogram-based voiceprint recognition using deep neural network","P. Li; M. Chen; F. Hu; Y. Xu","Automotive Electronics Engineering Research Center, College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, 400065, China; Automotive Electronics Engineering Research Center, College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, 400065, China; Automotive Electronics Engineering Research Center, College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, 400065, China; Automotive Electronics Engineering Research Center, College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, 400065, China","The 27th Chinese Control and Decision Conference (2015 CCDC)","","2015","","","2923","2927","This paper presents a speaker identification algorithm using the deep neural network (DNN) as the classifier to learn the features of the voiceprints represented by spectrogram. The collected speech signals are pre-emphasized, windowed, divided into some chunks, then calculated to obtain the magnitude of the frequency spectrum, which creates the spectrograms. The local binary patterns (LBP) operator is used to obtain the texture features embedded in spectrograms. These texture features, being represented by LBP vectors, are fed to DNN with four hidden layers to learn the speech features. In the learning progress, both of extraction and reconstruction procedures are reduplicated in each hidden layer. Through these extraction and reconstruction procedures of DNN, the speech features of each individual are given as a recognition figure, which offers the recognition results. The numerical experiments indicate that our approach has an acceptable recognition rate with high accuracy.","","","10.1109/CCDC.2015.7162425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7162425","Voiceprint Recognition;Deep Neural Network;Spectrogram","Spectrogram;Speech;Feature extraction;Speech recognition;Neural networks;Time-frequency analysis;Training","feature extraction;learning (artificial intelligence);neural nets;signal classification;speaker recognition","speech feature extraction;DNN reconstruction procedures;LBP vectors;texture features;local binary pattern operator;frequency spectrum;speech signal collection;DNN classifier;speaker identification algorithm;deep neural network;spectrogram-based voiceprint recognition","","3","12","","","","","IEEE","IEEE Conferences"
"Methodologies for Cross-Domain Data Fusion: An Overview","Y. Zheng","Microsoft Research, Beijing, China","IEEE Transactions on Big Data","","2015","1","1","16","34","Traditional data mining usually deals with data from a single domain. In the big data era, we face a diversity of datasets from different sources in different domains. These datasets consist of multiple modalities, each of which has a different representation, distribution, scale, and density. How to unlock the power of knowledge from multiple disparate (but potentially connected) datasets is paramount in big data research, essentially distinguishing big data from traditional data mining tasks. This calls for advanced techniques that can fuse knowledge from various datasets organically in a machine learning and data mining task. This paper summarizes the data fusion methodologies, classifying them into three categories: stage-based, feature level-based, and semantic meaning-based data fusion methods. The last category of data fusion methods is further divided into four groups: multi-view learning-based, similarity-based, probabilistic dependency-based, and transfer learning-based methods. These methods focus on knowledge fusion rather than schema mapping and data merging, significantly distinguishing between cross-domain data fusion and traditional data fusion studied in the database community. This paper does not only introduce high-level principles of each category of methods, but also give examples in which these techniques are used to handle real big data problems. In addition, this paper positions existing works in a framework, exploring the relationship and difference between different data fusion methods. This paper will help a wide range of communities find a solution for data fusion in big data projects.","","","10.1109/TBDATA.2015.2465959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230259","Big data;cross-domain data mining;data fusion;multi-modality data representation;deep neural networks;multi-view learning;matrix factorization;probabilistic graphical models;transfer learning;urban computing;Big Data;cross-domain data mining;data fusion;multi-modality data representation;deep neural networks;multi-view learning;matrix factorization;probabilistic graphical models;transfer learning;urban computing","Data integration;Big data;Data mining;Feature extraction;Roads;Semantics;Trajectory","Big Data;data mining;database management systems;learning (artificial intelligence);sensor fusion","cross-domain data fusion;data mining;multiple disparate datasets;big data research;knowledge fusion;machine learning;stage-based data fusion methods;feature level-based data fusion methods;semantic meaning-based data fusion methods;multiview learning-based methods;similarity-based methods;probabilistic dependency-based methods;transfer learning-based methods;schema mapping;data merging;database community","","113","93","","","","","IEEE","IEEE Journals"
"Deep kinship verification","M. Wang; Zechao Li; Xiangbo Shu; Jingdong; J. Tang","School of Computer Science and Engineering, Nanjing University of Science and Technology, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China; Microsoft Research, Beijing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China","2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP)","","2015","","","1","6","To improve the performance of kinship verification, we propose a novel deep kinship verification (DKV) model by integrating excellent deep learning architecture into metric learning. Unlike most existing shallow models based on metric learning for kinship verification, we employ a deep learning model followed by a metric learning formulation to select nonlinear features, which can find the appropriate project space to ensure the margin of negative sample pairs (i.e. parent and child without kinship relation) as large as possible and the margin of positive sample pairs (i.e. parent and child with kinship relation) as small as possible. Experimental results show that our method achieves satisfactory performance on two widely-used benchmarks, i.e. KFW-I and KFW-II.","","","10.1109/MMSP.2015.7340820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340820","","Measurement;Machine learning;Face;Feature extraction;Training;Decoding;Image color analysis","computer vision;learning (artificial intelligence)","deep kinship verification;DKV model;deep learning architecture;metric learning;computer vision","","12","31","","","","","IEEE","IEEE Conferences"
"Representation Learning for Single-Channel Source Separation and Bandwidth Extension","M. Zöhrer; R. Peharz; F. Pernkopf","Intelligent Systems Group at the Signal Processing and Speech Communication Laboratory, Graz University of Technology, Graz, Austria; iDN–Institute of Physiology, Medical University of Graz, Graz, Austria; Intelligent Systems Group at the Signal Processing and Speech Communication Laboratory, Graz University of Technology, Graz, Austria","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2015","23","12","2398","2409","In this paper, we use deep representation learning for model-based single-channel source separation (SCSS) and artificial bandwidth extension (ABE). Both tasks are ill-posed and source-specific prior knowledge is required. In addition to well-known generative models such as restricted Boltzmann machines and higher order contractive autoencoders two recently introduced deep models, namely generative stochastic networks (GSNs) and sum-product networks (SPNs), are used for learning spectrogram representations. For SCSS we evaluate the deep architectures on data of the 2 nd CHiME speech separation challenge and provide results for a speaker dependent, a speaker independent, a matched noise condition and an unmatched noise condition task. GSNs obtain the best PESQ and overall perceptual score on average in all four tasks. Similarly, frame-wise GSNs are able to reconstruct the missing frequency bands in ABE best, measured in frequency-domain segmental SNR. They outperform SPNs embedded in hidden Markov models and the other representation models significantly.","","","10.1109/TASLP.2015.2470560","Austrian Science Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7210172","Bandwidth extension;deep neural networks (DNNs);generative stochastic networks;representation learning;single-channel source separation (SCSS);sum-product networks","Hidden Markov models;Spectrogram;Learning systems;Speech processing;Data models;Adaptation models;Bandwidth;Neural networks;Stochastic processes","Boltzmann machines;hidden Markov models;learning (artificial intelligence);signal denoising;source separation;speaker recognition","deep representation learning;model-based single-channel source separation;SCSS;artificial bandwidth extension;ABE;ill-posed prior knowledge;source-specific prior knowledge;generative models;restricted Boltzmann machines;higher order contractive autoencoders;generative stochastic networks;GSN;sumproduct networks;SPN;2nd CHiME speech separation challenge;speaker dependent;speaker independent;matched noise condition;unmatched noise condition task;PESQ;overall perceptual score;frame-wise GSN;missing frequency band reconstruction;frequency-domain segmental SNR;hidden Markov models","","12","78","","","","","IEEE","IEEE Journals"
"Hyperspectral classification via learnt features","Y. Liu; G. Cao; Q. Sun; M. Siegel","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, US","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","2591","2595","This paper presents a new hyperspectral image (HSI) classification method which is capable of automatic feature learning while achieving high classification accuracy. The method contains two major modules: the spectral classification module and the spatial constraint module. Spectral classification module uses a deep network named stacked denoising autoencoders (SdA) to learn feature representation of the data. Through SdA, the data are projected nonlinearly from its original hyperspectral space to some higher dimensional space where more compact distribution is obtained. An interesting aspect of this method is that it does not need a feature design/extraction process guided by human prior. The suitable feature for the classification is learned by the deep network itself. Superpixel is utilized to generate the spatial constraints to refine the spectral classification results. By exploiting the spatial consistency of neighborhood pixels, the accuracy of classification is further improved by a big margin. Experiments on the public datasets reveal the superior performance of the proposed method.","","","10.1109/ICIP.2015.7351271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351271","Deep learning;stacked denoising autoencoders (SdA);hyperspectral image classification;superpixel;remote sensing","Hyperspectral imaging;Noise reduction;Image segmentation;Training;Spatial resolution;Machine learning","feature extraction;hyperspectral imaging;image classification;learning (artificial intelligence)","hyperspectral image classification method;HSI classification method;automatic feature learning;spectral classification module;spatial constraint module;stacked denoising autoencoder;SdA;hyperspectral space;higher dimensional space;deep network;superpixel;neighborhood pixel spatial consistency","","5","21","","","","","IEEE","IEEE Conferences"
"Bottleneck features from SNR-adaptive denoising deep classifier for speaker identification","Z. Tan; M. Mak","Center for Signal Processing, Dept. of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong SAR; Center for Signal Processing, Dept. of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong SAR","2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","","2015","","","1035","1040","In this paper, we explore the potential of using deep learning for extracting speaker-dependent features for noise robust speaker identification. More specifically, an SNR-adaptive denoising classifier is constructed by stacking two layers of restricted Boltzmann machines (RBMs) on top of a denoising deep autoencoder, where the top-RBM layer is connected to a soft-max output layer that outputs the posterior probabilities of speakers and the top-RBM layer outputs speaker-dependent bottleneck features. Both the deep autoencoder and RBMs are trained by contrastive divergence, followed by backpropagation fine-tuning. The autoencoder aims to reconstruct the clean spectra of a noisy test utterance using the spectra of the noisy test utterance and its SNR as input. With this denoising capability, the output from the bottleneck layer of the classifier can be considered as a low-dimension representation of denoised utterances. These frame-based bottleneck features are than used to train an iVector extractor and a PLDA model for speaker identification. Experimental results based on a noisy YOHO corpus show that the bottleneck features slightly outperform the conventional MFCC under low SNR conditions and that fusion of the two features lead to further performance gain, suggesting that the two features are complementary with each other.","","","10.1109/APSIPA.2015.7415429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415429","Deep learning;Bottleneck features;denoising autoencoder;speaker identification;deep belief networks","Feature extraction;Noise reduction;Speech;Noise measurement;Backpropagation;Training;Signal to noise ratio","backpropagation;Boltzmann machines;feature extraction;probability;signal classification;signal denoising;speaker recognition;vectors","speaker-dependent feature extraction;signal to noise ratio;SNR-adaptive denoising classifier;deep learning;deep neural network;DNN;speaker identification;restricted Boltzmann machine;RBM;soft-max output layer;posterior probability;backpropagation fine-tuning;noisy test utterance;iVector extractor;PLDA model","","1","24","","","","","IEEE","IEEE Conferences"
"Deep Learning for Textual Entailment Recognition","C. Lyu; Y. Lu; D. Ji; B. Chen","Comput. Sch., WuHan Univ., Wuhan, China; Comput. Sch., WuHan Univ., Wuhan, China; Comput. Sch., WuHan Univ., Wuhan, China; Comput. Sch., WuHan Univ., Wuhan, China","2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)","","2015","","","154","161","In this paper we propose a novel two-step procedure to recognize textual entailment. Firstly, we build a joint Restricted Boltzmann Machines (RBM) layer to learn the joint representation of the text-hypothesis pairs. Then the reconstruction error is calculated by comparing the original representation with reconstructed representation derived from the joint layer for each pair to recognize textual entailment. The joint RBM training data is automatically generated from a large news corpus. Experiment results show the contribution of the idea to the performance on textual entailment.","","","10.1109/ICTAI.2015.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372131","deep learning;joint Restricted Boltzmann Machines;joint representation;textual entailment","Semantics;Neural networks;Machine learning;Syntactics;Computational modeling;Text recognition;Image reconstruction","Boltzmann machines;text analysis","deep learning;textual entailment recognition;restricted Boltzmann machines;RBM layer;text-hypothesis pairs;reconstructed representation","","4","43","","","","","IEEE","IEEE Conferences"
"Analysis of function of rectified linear unit used in deep learning","K. Hara; D. Saito; H. Shouno","College of Industrial Technology, Nihon University, Narashino, Chiba 275-8575 Japan; Graduate School of Industrial Technology, Nihon University, Japan; Graduate School of Information and Engineering, the University of Electro-communications, Japan","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","Deep Learning is attracting much attention in object recognition and speech processing. A benefit of using the deep learning is that it provides automatic pre-training. Several proposed methods that include auto-encoder are being successfully used in various applications. Moreover, deep learning uses a multilayer network that consists of many layers, a huge number of units, and huge amount of data. Thus, executing deep learning requires heavy computation, so deep learning is usually utilized with parallel computation with many cores or many machines. Deep learning employs the gradient algorithm, however this traps the learning into the saddle point or local minima. To avoid this difficulty, a rectified linear unit (ReLU) is proposed to speed up the learning convergence. However, the reasons the convergence is speeded up are not well understood. In this paper, we analyze the ReLU by a using simpler network called the soft-committee machine and clarify the reason for the speedup. We also train the network in an on-line manner. The soft-committee machine provides a good test bed to analyze deep learning. The results provide some reasons for the speedup of the convergence of the deep learning.","","","10.1109/IJCNN.2015.7280578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280578","","Computers","gradient methods;learning (artificial intelligence);multilayer perceptrons","rectified linear unit;ReLU;deep learning;object recognition;speech processing;multilayer network;parallel computation;gradient algorithm;soft-committee machine","","35","17","","","","","IEEE","IEEE Conferences"
"Social network analysis of TV drama characters via deep concept hierarchies","C. Nan; K. Kim; B. Zhang","School of Computer Science and Engineering, Seoul National University, Seoul 151-744, Korea; School of Computer Science and Engineering, Seoul National University, Seoul 151-744, Korea; School of Computer Science and Engineering, Seoul National University, Seoul 151-744, Korea","2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","","2015","","","831","836","TV drama is a kind of big data, containing enormous knowledge of modern human society. As the character-centered stories unfold, diverse knowledge, such as economics, politics and the culture, is displayed. However, unless we have efficient dynamic multi-modal data processing and picture processing methods, we cannot analyze drama data effectively. Here, we adopt the recently proposed deep concept hierarchies (DCH) and convolutional-recursive neural network (C-RNN) models to analyze the social network between the drama characters. DCH uses multi hierarchies structure to translate the vision-language concepts of drama characters into diversified abstract concepts, and utilizes Markov Chain Monte Carlo algorithm to improve the retrieval efficiency of organizing conceptual spaces. Adopting approximately 4400-minute data of TV drama - Friends, we process face recognition on the characters by using convolutional-recursive deep learning model. Then we establish the social network between the characters by deep concept hierarchies model and analyze their affinity and the change of social network while the stories unfold.","","","10.1145/2808797.2809306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403641","social network analysis;concept learning;deep model;concept drift","Videos;Social network services;Mathematical model;Data mining;Data models;Character recognition;Encoding","humanities;learning (artificial intelligence);Markov processes;Monte Carlo methods;neural nets;social sciences computing","social network analysis;TV drama characters;deep concept hierarchies;Big Data;modern human society;character-centered stories;diverse knowledge;economics;politics;culture;dynamic multimodal data processing;picture processing methods;drama data;DCH;convolutional-recursive neural network;C-RNN;vision-language concepts;Markov chain Monte Carlo algorithm;retrieval efficiency;face recognition;convolutional-recursive deep learning model","","2","21","","","","","IEEE","IEEE Conferences"
"Genetic deep neural networks using different activation functions for financial data mining","L. M. Zhang","Soft Tech Consulting, Inc. Chantilly, USA","2015 IEEE International Conference on Big Data (Big Data)","","2015","","","2849","2851","A Deep Neural Network (DNN) using the same activation function for all hidden neurons has an optimization limitation due to its single mathematical functionality. To solve it, a new DNN with different activation functions is designed to globally optimize both parameters (weights and biases) and function selections. In addition, a novel Genetic Deep Neural Network (GDNN) with different activation functions uses genetic algorithms to optimize the parameters and selects the best activation function combination for different neurons among many activation function combinations through sufficient simulations. Two sample financial data sets (""Dow Jones Industrial Average"" and ""30-Year Treasury Constant Maturity Rate"" were used for performance analysis. Simulation results indicate that a GDNN using different activation functions can perform better than one using a single activation function. Future works include (1) developing more effective DNNs using different activation functions by using both cloud and GPU computing, (2) creating more effective DNNs by using new training optimization methods different from genetic algorithms, (3) using big data to further test the performance of the new GDNN, and (4) expanding its big data mining application areas (i.e. health and biomedical informatics, computer vision, social networks, security, etc.).","","","10.1109/BigData.2015.7364099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364099","Deep learning;machine learning;neural networks;genetic algorithms;big data mining;optimization;activation functions","Neurons;Training;Testing;Biological neural networks;Big data;Genetic algorithms","Big Data;cloud computing;data mining;financial data processing;graphics processing units;neural nets","genetic deep neural networks;activation functions;financial data mining;GDNN;mathematical functionality;financial data sets;Dow Jones industrial average;30-year treasury constant maturity rate;GPU computing;cloud computing;training optimization methods;big data mining application areas","","5","12","","","","","IEEE","IEEE Conferences"
"Feature representation learning on multi-scale receptive fields for objection recognition","Qian Zhao; Li Ma; Fang Xu; Wei Cheng; Mao Xu","School of Computer Science and Engineering, Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China, 611731; School of Computer Science and Engineering, Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China, 611731; School of Science, Southwest Petroleum University, Chengdu, China, 637001; School of Computer Science and Engineering, Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China, 611731; School of Computer Science and Engineering, Center for Robotics, University of Electronic Science and Technology of China, Chengdu, China, 611731","2015 11th International Conference on Natural Computation (ICNC)","","2015","","","19","23","In this paper, we have proposed a novel feature representation on multi-scale receptive fields for objection recognition. The method is based on a modified convolutional neural networks (CNN), named network-in-network (NIN), which has shown a good performance in some computer vision tasks. However, applying NIN to some specific applications may encounter a few problems. First, the NIN removes the fully connected layers, which makes it unsuited to use in large-scale face recognition due to lack of an efficient feature representation, even though it brings a lot of performance benefits. Second, some lowerlayer features, which can make the feature representation more discriminative, is unused. In the pure forward architecture, these features are unseen to the classifier. To solve the two problems, we present a multi-scale receptive fields (MSRF) representation learning scheme. Based on a well trained NIN, we add a pathway to top layer and design a feature vector as final representation. In our experiments, we compare the result of our multi-scale receptive fields with standard NIN architecture. The results show our method can obtain a more explicit feature representation and improvements in performance.","","","10.1109/ICNC.2015.7377959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7377959","deep learning;multi-scale features;receptive field learning;objection recognition","Computer architecture;Machine learning;Convolution;Training;Convolutional codes;Multilayer perceptrons","computer vision;feature extraction;image classification;image representation;learning (artificial intelligence);neural net architecture;object recognition;performance evaluation","feature representation learning;multiscale receptive fields;objection recognition;convolutional neural networks;modified CNN;network-in-network;computer vision;large-scale face recognition;lower-layer features;classifier;MSRF representation learning scheme;feature vector;NIN architecture;performance improvements","","","19","","","","","IEEE","IEEE Conferences"
"Deep hierarchical representation and segmentation of high resolution remote sensing images","J. Wang; Q. Qin; Z. Li; X. Ye; J. Wang; X. Yang; X. Qin","Institute of Remote Sensing and GIS, Peking University, Beijing, China; Institute of Remote Sensing and GIS, Peking University, Beijing, China; College of Resources and Environmental Sciences, China Agricultural University, Beijing, China; Institute of Remote Sensing and GIS, Peking University, Beijing, China; Institute of Remote Sensing and GIS, Peking University, Beijing, China; Institute of Remote Sensing and GIS, Peking University, Beijing, China; Institute of Remote Sensing and GIS, Peking University, Beijing, China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","4320","4323","This paper presents a novel deep hierarchical representation and segmentation approach for high resolution remote sensing image understanding. An information extraction approach using deep hierarchical exploitation for remote sensing image is presented. The key idea is that we adopt a fast scanning image segmentation within a deep hierarchical feature representation framework, using a deep learning technique to split and merge over-segmented regions until they form meaningful objects. The contribution is to develop an effective procedure for multi-scale image representation to address the issue of information uncertainty in practical applications. We test our method on two optical high resolution remote sensing image datasets and produce promising experimental results in the form of multiple layer outputs, which confirm the effectiveness and robustness of the proposed procedure.","","","10.1109/IGARSS.2015.7326782","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326782","Hierarchical representation;Image segmentation;High resolution remote sensing images","Image segmentation;Remote sensing;Machine learning;Merging;Clustering algorithms;Spatial resolution","feature extraction;image representation;image segmentation;remote sensing","high resolution remote sensing image segmentation;feature extraction;deep hierarchical feature representation framework;deep learning technique;image representation;optical high resolution remote sensing image dataset","","4","5","","","","","IEEE","IEEE Conferences"
"A learning model for essentialist concepts","I. Oved; S. Nichols; D. Barner","Department of Psychology, University of California, San Diego La Jolla, 92093; Department of Philosophy, University of Arizona, Tucson, 85721; Departments of Psychology and Linguistics, University of California, San Diego, La Jolla, 92093","2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)","","2015","","","92","97","Many cognitive scientists take it for granted that concepts like CAT (mental terms that are expressed with single nouns) can be learned by observing a co-occurrence in superficial properties, such as having fur, being 4-legged, and tending to purr, and then building a complex category representation from representations for those superficial properties. A less popular account, known as Psychological Essentialism, claims that concepts like CAT pick out deep, hidden properties (essences) that are causal explanations for observable co-occurrences in superficial properties. The trouble is, Psychological Essentialism lacks an account of how such essentialist concepts could be learned, and often adopt the unpalatable conclusion that such concepts are innate. Developmental roboticists have recently started implementing systems that employ learned hidden/latent variables. The present paper spells out a learning theory for essentialist concepts, and presents two psychology experiments that help support the account over the associationist alternative.","","","10.1109/DEVLRN.2015.7346121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346121","Categories;Concept Learning;Developmental Psychology;Grounding","Psychology;Gold;Correlation;Color;Pediatrics;Blood;Animals","learning (artificial intelligence);psychology","learning model;cognitive scientists;CAT;superficial properties;complex category representation;psychological essentialism;hidden properties;observable cooccurrences;latent variable learning;hidden variable learning;learning theory","","","32","","","","","IEEE","IEEE Conferences"
"PhaseFi: Phase Fingerprinting for Indoor Localization with a Deep Learning Approach","X. Wang; L. Gao; S. Mao","Dept. of Electr. & Comput. Eng., Auburn Univ., Auburn, AL, USA; Dept. of Electr. & Comput. Eng., Auburn Univ., Auburn, AL, USA; Dept. of Electr. & Comput. Eng., Auburn Univ., Auburn, AL, USA","2015 IEEE Global Communications Conference (GLOBECOM)","","2015","","","1","6","With the increasing demand of location-based services, indoor localization based on fingerprinting has become an increasingly important technique due to its high accuracy and low hardware requirement. In this paper, we propose PhaseFi, a fingerprinting system for indoor localization with calibrated channel state information (CSI) phase information. In PhaseFi, the raw phase information is first extracted from the multiple antennas and multiple subcarriers of the IEEE 802.11n network interface card (NIC) by accessing the modified driver. Then a linear transform is used to extract the calibrated phase information, which is proven to have a bounded variance. For the offline stage, we design a deep network with three hidden layers to train the calibrated phase data, and employ weights to represent fingerprints. A greedy learning algorithm is incorporated to train the weights layer-by-layer to reduce computational complexity, where a sub-network between two continuous layers forms a Restricted Boltzmann Machine (RBM). In the online stage, we use a probabilistic method based on the radial basis function (RBF) for online location estimation. The proposed PhaseFi scheme is implemented and validated with intensive experiments in two representation indoor environments. It outperforms other three benchmark schemes based on CSI or RSS in both scenarios.","","","10.1109/GLOCOM.2015.7417517","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7417517","","IEEE 802.11 Standard;Training;OFDM;Databases;Machine learning;Data mining;Antennas","Boltzmann machines;indoor navigation;learning (artificial intelligence);mobile radio;telecommunication computing;wireless LAN","restricted Boltzmann machine;computational complexity reduction;greedy learning algorithm;linear transform;modified driver access;IEEE 802.11n network interface card;multiple subcarrier system;multiple antenna system;calibrated channel state information phase information;location based service;deep learning technique;indoor localization;phase fingerprinting;PhaseFi","","14","16","","","","","IEEE","IEEE Conferences"
"BVCNN: A Multi-object Image Recognition Method Based on the Convolutional Neural Networks","H. Shi; X. Mu; S. Wang","Dept. of Inf. Eng., High-Tech Inst. of Xi'an, Xi'an, China; Dept. of Inf. Eng., High-Tech Inst. of Xi'an, Xi'an, China; Dept. of Inf. Eng., High-Tech Inst. of Xi'an, Xi'an, China","2015 International Conference on Virtual Reality and Visualization (ICVRV)","","2015","","","81","84","This article puts forward a kind of huge amounts of multi-object image recognition method -- BVCNN. Firstly, BING method is used to recognize images, which greatly reduces the time of estimating image targets, and makes it possible that quickly identify multiple target images, compared to traditional convolution neural networks only achieving single target image recognition, Secondly, vectorization of deep convolutional neural networks is used for deep learning of characteristics in local image and recognition, which speeds up network training and testing, thirdly, using the context information in multi-object image classification, to a certain extent, helps to distinguish individual of similar characteristics according to environment, improving the multi-object image recognition accuracy. According to experiments, identifying a single image by this model only need less than 1 s, and this model can be used for image information fusion.","","","10.1109/ICVRV.2015.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7467216","Multi-object image recognition;Convolutional neural networks;Information fusion;Deep learning;Vecotorization","Image recognition;Neural networks;Target recognition;Training;Classification algorithms;Visualization;Image classification","image classification;image fusion;learning (artificial intelligence);neural nets","multiple object image recognition metho;BVCNN;BING method;image target estimation;target image recognition;deep convolutional neural network vectorization;deep learning;local image characteristics;network training;testing;context information;image information fusion","","1","17","","","","","IEEE","IEEE Conferences"
"Mobile based big data design patent image retrieval system via Lp norm deep learning approach","J. Su; B. W. K. Ling; Q. Dai; J. Xiao; K. Tsang","School of Information Engineering, Guangdong University of Technology, Guangzhou, 510006, China; School of Information Engineering, Guangdong University of Technology, Guangzhou, 510006, China; School of Information Engineering, Guangdong University of Technology, Guangzhou, 510006, China; School of Information Engineering, Guangdong University of Technology, Guangzhou, 510006, China; Department of Electronic Engineering, City University of H. K., Hong Kong, China","IECON 2015 - 41st Annual Conference of the IEEE Industrial Electronics Society","","2015","","","004886","004889","This paper proposes a mobile based big data design patent image retrieval system via a deep learning approach. The images are represented via sparse vectors by a dictionary. The joint representation and dictionary design problem is formulated as a mixed L2 and Lp optimization problem. An iterative algorithm is employed for finding a locally optimal solution. Experimental results show that the retrieval accuracy is high.","","","10.1109/IECON.2015.7392866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7392866","","","image representation;image retrieval;iterative methods;mobile computing;optimisation;patents","mobile based big data design patent image retrieval system;Lp norm deep learning approach;sparse vectors;dictionary;joint representation;dictionary design problem;optimization problem","","","10","","","","","IEEE","IEEE Conferences"
"Deep neural networks-based vehicle detection in satellite images","Q. Jiang; L. Cao; M. Cheng; C. Wang; J. Li","Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China","2015 International Symposium on Bioelectronics and Bioinformatics (ISBB)","","2015","","","184","187","Vehicle detection in satellite images is an Challenging task, but meaningful at the same time. This paper propose a vehicle detection method in satellite images using Deep Convolutional Neural Network(DNN). DNN is a model of deep learning and it has a high learning capacity when dealing with images. DNN consist of several convolution layers and pooling layers, the last layer is full connection with output(this can be considered as neural network). DNN can automatically learn rich features from trainning dataset, and has achieved excellent performance in many applications such as image classification and object recognition. To benefit from this method, we propose a vehicle detection framework. Firstly we use a graph-based superpixel segmentation to extract a set of image patches, which can help us locate vehicle effectively. And then we train a DNN network to classify these pathes into vehicle and non-vehicle. Experimental results indicate that the proposed method has a good performance, with high detection rates and very few false alarms for all test road segment.","","","10.1109/ISBB.2015.7344954","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344954","","Feature extraction;Vehicles;Vehicle detection;Satellites;Roads;Image segmentation;Training","convolution;image classification;image segmentation;learning (artificial intelligence);neural nets;object detection;remote sensing;traffic engineering computing","convolution layers;DNN;satellite image patch extraction;deep convolutional neural networks-based vehicle detection;graph-based superpixel segmentation;object recognition;image classification;pooling layer;deep learning","","16","17","","","","","IEEE","IEEE Conferences"
"Deep semantic ranking based hashing for multi-label image retrieval","Fang Zhao; Y. Huang; L. Wang; Tieniu Tan","Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, China; Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, China; Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, China; Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","1556","1564","With the rapid growth of web images, hashing has received increasing interests in large scale image retrieval. Research efforts have been devoted to learning compact binary codes that preserve semantic similarity based on labels. However, most of these hashing methods are designed to handle simple binary similarity. The complex multi-level semantic structure of images associated with multiple labels have not yet been well explored. Here we propose a deep semantic ranking based method for learning hash functions that preserve multilevel semantic similarity between multi-label images. In our approach, deep convolutional neural network is incorporated into hash functions to jointly learn feature representations and mappings from them to hash codes, which avoids the limitation of semantic representation power of hand-crafted features. Meanwhile, a ranking list that encodes the multilevel similarity information is employed to guide the learning of such deep hash functions. An effective scheme based on surrogate loss is used to solve the intractable optimization problem of nonsmooth and multivariate ranking measures involved in the learning procedure. Experimental results show the superiority of our proposed approach over several state-of-the-art hashing methods in term of ranking evaluation metrics when tested on multi-label image datasets.","","","10.1109/CVPR.2015.7298763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298763","","Semantics;Measurement;Binary codes;Feature extraction;Convolutional codes;Optimization;Databases","binary codes;convolution;file organisation;image representation;image retrieval;learning (artificial intelligence);neural nets;optimisation","deep semantic ranking based hashing;multilabel image retrieval;Web images;large scale image retrieval;compact binary code;hashing method;binary similarity;multilevel semantic structure;deep semantic ranking based method;learning hash function;multilevel semantic similarity;deep convolutional neural network;feature representation;hash code;semantic representation power;hand-crafted feature;multilevel similarity information;deep hash function;surrogate loss;intractable optimization problem;nonsmooth ranking measure;multivariate ranking measure;learning procedure;ranking evaluation metrics;multilabel image dataset","","26","33","","","","","IEEE","IEEE Conferences"
"Deep feature representation for hyperspectral image classification","J. Li; L. Bruzzone; S. Liu","Zhejiang Police College, Department of Forensic Science, Hangzhou 310053, China; University of Trento, Department of Information Engineering and Computer Science, Trento 38123, Italy; University of Trento, Department of Information Engineering and Computer Science, Trento 38123, Italy","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","4951","4954","Hyperspectral data classification problems have been extensively studied in the past decade. However, well designed features and a robust classifier are still open issues that impact on the performance of an automatic land-cover classification system. In this paper, we propose a deep feature representation method that generates very good features and a classifier for pixel-wise hyperspectral data classification. The proposed method has two main steps: principle components of the hyperspectral image cube is first filtered by three dimensional Gabor wavelets; second, stacked autoencoders are trained on the outputs of the previous step through unsupervised pre-training, finally deep neural network is trained on those stacked autoencoders. Experimental results obtained on real hyperspectral image confirmed the effectiveness of the proposed approach in favors of the high classification accuracy and computation efficiency.","","","10.1109/IGARSS.2015.7326943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326943","stacked autoencoders;deep learning;hyperspectral image;classification;remote sensing","Hyperspectral imaging;Neural networks;Training;Support vector machines;Feature extraction;Three-dimensional displays","Gabor filters;geophysical image processing;hyperspectral imaging;image classification;land cover;neural nets;principal component analysis;terrain mapping","hyperspectral image classification accuracy;robust classifier;automatic land-cover classification system;deep feature representation method;pixel-wise hyperspectral data classification problems;principle components;hyperspectral image cube;3D Gabor wavelets;stacked autoencoders;unsupervised pretraining;deep neural network;computation efficiency","","9","9","","","","","IEEE","IEEE Conferences"
"Multi-manifold deep metric learning for image set classification","J. Lu; G. Wang; W. Deng; P. Moulin; J. Zhou","Advanced Digital Sciences Center, Singapore; Advanced Digital Sciences Center, Singapore; School of ICE, Beijing University of Posts and Telecommunications, China; Advanced Digital Sciences Center, Singapore; Department of Automation, Tsinghua University, Beijing, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","1137","1145","In this paper, we propose a multi-manifold deep metric learning (MMDML) method for image set classification, which aims to recognize an object of interest from a set of image instances captured from varying viewpoints or under varying illuminations. Motivated by the fact that manifold can be effectively used to model the nonlinearity of samples in each image set and deep learning has demonstrated superb capability to model the nonlinearity of samples, we propose a MMDML method to learn multiple sets of nonlinear transformations, one set for each object class, to nonlinearly map multiple sets of image instances into a shared feature subspace, under which the manifold margin of different class is maximized, so that both discriminative and class-specific information can be exploited, simultaneously. Our method achieves the state-of-the-art performance on five widely used datasets.","","","10.1109/CVPR.2015.7298717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298717","","Manifolds;Face;Machine learning;Training;Testing;Computational modeling;Legged locomotion","image classification;learning (artificial intelligence);object recognition","multimanifold deep metric learning;image set classification;MMDML;object recognition;image instances;discriminative information;class-specific information","","57","40","","","","","IEEE","IEEE Conferences"
"Combination of two-dimensional cochleogram and spectrogram features for deep learning-based ASR","A. Tjandra; S. Sakti; G. Neubig; T. Toda; M. Adriani; S. Nakamura","Graduate School of Information Science, Nara Institute of Science and Technology, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Japan; Faculty of Computer Science, Universitas Indonesia, Indonesia; Graduate School of Information Science, Nara Institute of Science and Technology, Japan","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4525","4529","This paper explores the use of auditory features based on cochleograms; two dimensional speech features derived from gammatone filters within the convolutional neural network (CNN) framework. Furthermore, we also propose various possibilities to combine cochleogram features with log-mel filter banks or spectrogram features. In particular, we combine within low and high levels of CNN framework which we refer to as low-level and high-level feature combination. As comparison, we also construct the similar configuration with deep neural network (DNN). Performance was evaluated in the framework of hybrid neural network - hidden Markov model (NN-HMM) system on TIMIT phoneme sequence recognition task. The results reveal that cochleogram-spectrogram feature combination provides significant advantages. The best accuracy was obtained by high-level combination of two dimensional cochleogram-spectrogram features using CNN, achieved up to 8.2% relative phoneme error rate (PER) reduction from CNN single features or 19.7% relative PER reduction from DNN single features.","","","10.1109/ICASSP.2015.7178827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178827","Deep learning;feature combination;cochleogram;DNN and CNN","Spectrogram;Speech;Neural networks;Hidden Markov models;Convolution;Speech recognition;Acoustics","feature extraction;hidden Markov models;neural net architecture;speech recognition","two dimensional cochleogram feature;spectrogram feature;deep learning based ASR;auditory feature;2D speech features;gammatone filters;convolutional neural network;log-mel filter banks;deep neural network;hidden Markov model system;TIMIT phoneme sequence recognition task","","3","33","","","","","IEEE","IEEE Conferences"
"Design of the feedback controller for deep brain stimulation of the parkinsonian state based on the system identification","H. Li; C. Liu; J. Wang","School of Automation and Electrical Engineering, Tianjin University of Technology and Educations, Tianjin, China; School of Automation and Electrical Engineering, Tianjin University of Technology and Educations, Tianjin, China; School of Electrical Engineering and Automation, Tianjin University, Tianjin, China","The 27th Chinese Control and Decision Conference (2015 CCDC)","","2015","","","5573","5578","A novel closed-loop control strategy of deep brain stimulation is explored in this paper. By establishing an input-output model of the basal ganglia, the causality between the external stimuli and neuronal activities can be revealed. One-step ahead prediction constructs the probable future information of the tracking errors, which is used to guide the amplitude of the current pulse train stimuli. By comparing the traditional and iterative learning proportional control algorithms, the latter control strategy not only automatically can optimize the control signals without requirements of any particular knowledge on the details of model, but also can reduce the energy expenditure of the stimuli by accelerating the control process. This work may point to the potential value of model-based design of closed-loop controllers and pave the way towards the optimization of deep brain stimulation parameters and structures for Parkinson's disease.","","","10.1109/CCDC.2015.7161792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7161792","Deep Brain Stimulation;Parkinsonian State;System Identification;Prediction;Iterative Learning Closed-loop Control","Satellite broadcasting;Neurons;Relays;Mathematical model;Proportional control;Predictive models;Biological system modeling","brain;closed loop systems;diseases;feedback;identification;iterative learning control;neuromuscular stimulation;proportional control;signal processing","feedback controller design;deep brain stimulation;system identification;closed-loop control strategy;input-output model;basal ganglia;neuronal activities;tracking errors;iterative learning proportional control algorithms;control signals;energy expenditure;model-based design;closed-loop controllers;deep brain stimulation parameters;Parkinson disease","","","14","","","","","IEEE","IEEE Conferences"
"Normal sparse Deep Belief Network","M. A. Keyvanrad; M. M. Homayounpour","Laboratory for Intelligent Multimedia Processing (LIMP), Computer Engineering and Information Technology, Department Amirkabir University of Technology, Tehran, Iran; Laboratory for Intelligent Multimedia Processing (LIMP), Computer Engineering and Information Technology, Department Amirkabir University of Technology, Tehran, Iran","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","7","Nowadays this is very popular to use deep architectures in machine learning. Deep Belief Networks (DBNs) have deep architectures to create a powerful generative model using training data. Deep Belief Networks can be used in classification and feature learning. A DBN can be learnt unsupervised and then the learnt features are suitable for a simple classifier (like a linear classifier) with a few labeled data. According to researches, training of DBN can be improved to produce features with more interpretability and discrimination ability. One of these improvements is sparsity in learnt features in DBN. By using sparsity we can learn useful low-level feature representations for unlabeled data. In sparse representation we benefit from this property that the learnt features can be interpreted, i.e. they correspond to meaningful aspects of the input, and capture factors of variation in the data. Different methods have been proposed to build sparse RBMs. In this paper we propose a new method namely nsDBN that has different behaviors according to deviation of the activation of the hidden units from a (low) fixed value. Also our proposed method has a variance parameter that can control the force degree of sparseness. According to the results, our new method compared to the state of the art methods including PCA, RBM, qsRBM, and rdsRBM always achieves the best recognition accuracy on the MNIST hand written digit recognition test set even when only 10 to 20 labeled samples per class are used as training data.","","","10.1109/IJCNN.2015.7280688","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280688","Deep Belief Network;Restricted Boltzmann Machine;Normal Sparse RBM;quadratic sparse RBM;rate distortion sparse RBM","Handwriting recognition;Computer architecture;Linear programming","belief networks;classification;data structures;learning (artificial intelligence);principal component analysis","normal sparse deep belief network;deep architectures;machine learning;DBN;training data;classification;feature representations;unlabeled data;PCA;qsRBM;rdsRBM","","9","19","","","","","IEEE","IEEE Conferences"
"Deep learning of binary hash codes for fast image retrieval","K. Lin; H. Yang; J. Hsiao; C. Chen","Academia Sinica, Taiwan; Academia Sinica, Taiwan; Yahoo! Taiwan; Academia Sinica, Taiwan","2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","","2015","","","27","35","Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels. The utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.","","","10.1109/CVPRW.2015.7301269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301269","","Image retrieval;Binary codes;Image representation;Machine learning;Neurons;Semantics;Visualization","binary codes;convolution;image representation;image retrieval;learning (artificial intelligence);neural nets","binary hash codes;approximate nearest neighbor search;convolutional neural networks;CNN;effective deep learning framework;fast image retrieval;data labels;hidden layer;latent concepts;supervised methods;image representations;class labels;pairwised inputs;large-scale datasets;CIFAR-10 datasets;MNIST datasets","","180","32","","","","","IEEE","IEEE Conferences"
"Deep Multi-patch Aggregation Network for Image Style, Aesthetics, and Quality Estimation","X. Lu; Z. Lin; X. Shen; R. Mech; J. Z. Wang","Pennsylvania State Univ., University Park, PA, USA; Adobe Res., San Jose, CA, USA; Adobe Res., San Jose, CA, USA; Adobe Res., San Jose, CA, USA; Pennsylvania State Univ., University Park, PA, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","990","998","This paper investigates problems of image style, aesthetics, and quality estimation, which require fine-grained details from high-resolution images, utilizing deep neural network training approach. Existing deep convolutional neural networks mostly extracted one patch such as a down-sized crop from each image as a training example. However, one patch may not always well represent the entire image, which may cause ambiguity during training. We propose a deep multi-patch aggregation network training approach, which allows us to train models using multiple patches generated from one image. We achieve this by constructing multiple, shared columns in the neural network and feeding multiple patches to each of the columns. More importantly, we propose two novel network layers (statistics and sorting) to support aggregation of those patches. The proposed deep multi-patch aggregation network integrates shared feature learning and aggregation function learning into a unified framework. We demonstrate the effectiveness of the deep multi-patch aggregation network on the three problems, i.e., image style recognition, aesthetic quality categorization, and image quality estimation. Our models trained using the proposed networks significantly outperformed the state of the art in all three applications.","","","10.1109/ICCV.2015.119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410476","","Training;Neural networks;Feature extraction;Estimation;Image resolution;Object detection;Sorting","image recognition;image resolution;learning (artificial intelligence);neural nets","deep multipatch aggregation network;image aesthetics;image quality estimation;high-resolution images;deep neural network training approach;deep convolutional neural networks;down-sized crop;deep multipatch aggregation network training approach;feature learning;aggregation function learning;aesthetic quality categorization;image style recognition","","74","40","","","","","IEEE","IEEE Conferences"
"Synthetic structural magnetic resonance image generator improves deep learning prediction of schizophrenia","A. Ulloa; S. Plis; E. Erhardt; V. Calhoun","Dept. of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, USA; The Mind Research Network, Albuquerque, NM, USA; Dept. of Mathematics and Statistics, University of New Mexico, Albuquerque, NM, USA; Dept. of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, USA","2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)","","2015","","","1","6","Despite the rapidly growing interest, progress in the study of relations between physiological abnormalities and mental disorders is hampered by complexity of the human brain and high costs of data collection. The complexity can be captured by deep learning approaches, but they still may require significant amounts of data. In this paper, we seek to mitigate the latter challenge by developing a generator for synthetic realistic training data. Our method greatly improves generalization in classification of schizophrenia patients and healthy controls from their structural magnetic resonance images. A feed forward neural network trained exclusively on continuously generated synthetic data produces the best area under the curve compared to classifiers trained on real data alone.","","","10.1109/MLSP.2015.7324379","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324379","","Generators;Magnetic resonance imaging;Probability density function;Biological neural networks;Machine learning;Neuroimaging;Training","biomedical MRI;brain;medical disorders","synthetic structural magnetic resonance image generator;mental disorders;physiological abnormalities;data collection;deep learning approaches;synthetic realistic training data;schizophrenia patients;human brain","","4","19","","","","","IEEE","IEEE Conferences"
"Stacked Extreme Learning Machines","H. Zhou; G. Huang; Z. Lin; H. Wang; Y. C. Soh","School of Electrical and Electronic Engineering, Nanyang Technological University, Nanyang, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Nanyang, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Nanyang, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Nanyang, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Nanyang, Singapore","IEEE Transactions on Cybernetics","","2015","45","9","2013","2025","Extreme learning machine (ELM) has recently attracted many researchers' interest due to its very fast learning speed, good generalization ability, and ease of implementation. It provides a unified solution that can be used directly to solve regression, binary, and multiclass classification problems. In this paper, we propose a stacked ELMs (S-ELMs) that is specially designed for solving large and complex data problems. The S-ELMs divides a single large ELM network into multiple stacked small ELMs which are serially connected. The S-ELMs can approximate a very large ELM network with small memory requirement. To further improve the testing accuracy on big data problems, the ELM autoencoder can be implemented during each iteration of the S-ELMs algorithm. The simulation results show that the S-ELMs even with random hidden nodes can achieve similar testing accuracy to support vector machine (SVM) while having low memory requirements. With the help of ELM autoencoder, the S-ELMs can achieve much better testing accuracy than SVM and slightly better accuracy than deep belief network (DBN) with much faster training speed.","","","10.1109/TCYB.2014.2363492","Singapore Academic Research Fund (AcRF) Tier 1; Singapore’s National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6937189","Deep learning;eigenvalue;extreme learning machine (ELM);feature mapping;principal component analysis (PCA);support vector machines (SVMs);Deep learning;eigenvalue;extreme learning machine (ELM);feature mapping;principal component analysis (PCA);support vector machines (SVMs)","Principal component analysis;Training;Accuracy;Testing;Support vector machines;Covariance matrices;Eigenvalues and eigenfunctions","Big Data;feedforward neural nets","extreme learning machine;stacked ELM;ELM network;big data problems;S-ELM;random hidden nodes;ELM autoencoder;generalized single-hidden layer feed-forward networks","","54","43","","","","","IEEE","IEEE Journals"
"A Configurable Deep Network for high-dimensional clinical trial data","J. O' Donoghue; M. Roantree; M. Van Boxtel","Insight Centre for Data Analytics, School of Computing, Dublin City University, Glasnevin, Dublin 9, Ireland; Insight Centre for Data Analytics, School of Computing, Dublin City University, Glasnevin, Dublin 9, Ireland; Department of Psychiatry and Neuropsychology, School for Mental Health and Neuroscience, Maastricht University, Netherlands","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","Clinical studies provide interesting case studies for data mining researchers, given the often high degree of dimensionality and long term nature of these studies. In areas such as dementia, accurate predictions from data scientists provide vital input into the understanding of how certain features (representing lifestyle) can predict outcomes such as dementia. Most research involved has used traditional or shallow data mining approaches which have been shown to offer varying degrees of accuracy in datasets with high dimensionality. In this research, we explore the use of deep learning architectures, as they have been shown to have high predictive capabilities in image and audio datasets. The purpose of our research is to build a framework which allows easy reconfiguration for the performance of experiments across a number of deep learning approaches. In this paper, we present our framework for a configurable deep learning machine and our evaluation and analysis of two shallow approaches: regression and multi-layer perceptron, as a platform to a deep belief network, and using a dataset created over the course of 12 years by researchers in the area of dementia.","","","10.1109/IJCNN.2015.7280841","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280841","","Dementia","belief networks;data mining;learning (artificial intelligence);medical disorders;medical information systems;multilayer perceptrons","configurable deep network;high-dimensional clinical trial data;clinical study;data mining researcher;dementia;data scientist;deep learning architecture;predictive capability;image dataset;audio dataset;easy reconfiguration;configurable deep learning machine;regression perception;multilayer perceptron;deep belief network","","2","27","","","","","IEEE","IEEE Conferences"
"Hybrid approach to crime prediction using deep learning","J. Azeez; D. J. Aravindhar","Department of Computer Science & Engineering, Hindustan Institute of Technology & Science, Padur Chennai, India; Department of Computer Science & Engineering, Hindustan Institute of Technology & Science, Padur, Chennai, India","2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","","2015","","","1701","1710","Prevention is better that Cure. Preventing a crime from occurring is better than investigating what or how the crime had occurred. Just like vaccination is given to a child to prevent disease, in today's world with such higher crime rate and brutal crime happenings, it have become necessary to have a vaccination systems that prevents from crimes happening. By vaccinating society against crime it refers to various methods such as educating peoples, creating awareness, increasing efficiency and proactive policing methods and other deterrent techniques. Inspired by two different existing approach to crime prediction, the first one present a visual analytics approach that provides decision makers with a proactive and predictive environment in order to assist them in making effective resource allocation and deployment decisions. Crime incident prediction has depends mainly on the historical crime record and various geospatial and demographic information [1]. Even though it's promising, they do not take into account the rich and rapidly expanding social & web media context that surrounds incidents of interest. Next approach is based on the semantic analysis and natural language processing of Twitter posts via latent Dirichlet allocation, Topic detection Sentiment analysis[3[4]]. But both the techniques faces inherent limitations. Crime that happens these days are have following key characteristics such as crimes repeating in a periodic fashion, crimes occurring as a result of some other activity and occurrence of crimes pre indicated by some other information.","","","10.1109/ICACCI.2015.7275858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275858","Deep Learning;STL;Sentiment Analysis;LDA;Undirected Probabilistic Graph;RNN","Prediction algorithms;Machine learning;Market research;Computational modeling;Data models;Algorithm design and analysis;Informatics","criminal law;demography;natural language processing;resource allocation;social networking (online)","hybrid approach;deep learning;crime prevention;brutal crime;vaccination systems;visual analytics approach;resource allocation;deployment decisions;crime incident prediction;historical crime record;geospatial information;demographic information;semantic analysis;natural language processing;Twitter posts;latent Dirichlet allocation;topic detection;sentiment analysis","","3","12","","","","","IEEE","IEEE Conferences"
"Semantics in Deep Neural-Network Computing","X. Sun; X. Luo; J. Liu; X. Jiang; J. Zhang","Knowledge Grid Group, Inst. of Comput. Technol., Beijing, China; Shanghai Univ., Shanghai, China; Wuhan Univ., Wuhan, China; Zhejiang Univ. of Technol., Hangzhou, China; Inst. of Sci. & Tech. Inf. of China, Beijing, China","2015 11th International Conference on Semantics, Knowledge and Grids (SKG)","","2015","","","81","88","Artificial Intelligence development is stepping into a new era due to the recent exciting achievements from neural network and statistical machine learning research communities. Statistic neural-computing based machine learning has been deemed as one of promising roads towards realizing the ideal of Artificial Intelligence promoted since last century. Learning is the key in making progress. Statistic machine learning is to obtain a probability distribution or a function from a set of training samples according to a certain optimization target over the training cost based on a predefined model. While there are many significant improvements in image, sound and text recognition and analyzing using neural network based learning strategies, a new open question emerges, that is, what is the next? To ask this question is to mean that the neural network solution is not an ultimate solution, and there will be more challenges to meet in coming future. We discussed these aspects of deep neural network research work in this paper and focused on semantics in deep neural-network computing models. We try to browse how semantics or knowledge are to be involved in deep neural network models and how the semantics and knowledge will be a key factor towards making more intelligent machines. We argue that priori semantics and knowledge in modelling a neural network is important, which could be the key for researchers to design intelligent machine models to perform complex tasks.","","","10.1109/SKG.2015.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429360","semantics;knowledge;artificial intelligence;deep neural network","Artificial intelligence;Semantics;Artificial neural networks;Computational modeling;Biological neural networks;Image classification","learning (artificial intelligence);neural nets;statistical distributions","semantics;deep neural-network computing;artificial intelligence development;statistical machine learning research communities;statistic neural-computing based machine learning;probability distribution;training cost;neural network based learning strategies;intelligent machine model design","","","51","","","","","IEEE","IEEE Conferences"
"Facial smile detection based on deep learning features","K. Zhang; Y. Huang; H. Wu; L. Wang","University of Electronic Science and Technology of China, Chengdu, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Electronic Science and Technology of China, Chengdu, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing, China","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","","2015","","","534","538","Smile detection from facial images is a specialized task in facial expression analysis with many potential applications such as smiling payment, patient monitoring and photo selection. The current methods on this study are to represent face with low-level features, followed by a strong classifier. However, these manual features cannot well discover information implied in facial images for smile detection. In this paper, we propose to extract high-level features by a well-designed deep convolutional networks (CNN). A key contribution of this work is that we use both recognition and verification signals as supervision to learn expression features, which is helpful to reduce same-expression variations and enlarge different-expression differences. Our method is end-to-end, without complex pre-processing often used in traditional methods. High-level features are taken from the last hidden layer neuron activations of deep CNN, and fed into a soft-max classifier to estimate. Experimental results show that our proposed method is very effective, which outperforms the state-of-the-art methods. On the GENKI smile detection dataset, our method reduces the error rate by 21% compared with the previous best method.","","","10.1109/ACPR.2015.7486560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486560","","Feature extraction;Face;Convolution;Databases;Support vector machines;Training;Neurons","face recognition;learning (artificial intelligence)","facial smile detection;deep learning features;facial images;facial expression analysis;patient monitoring;photo selection;smiling payment;deep convolutional networks;deep CNN;hidden layer neuron activations;soft-max classifier;GENKI","","15","14","","","","","IEEE","IEEE Conferences"
"Target classification in oceanographic SAR images with deep neural networks: Architecture and initial results","C. Bentes; D. Velotto; S. Lehner","German Aerospace Center (DLR), Remote Sensing Technology Institute, 28199 Bremen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute, 28199 Bremen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute, 28199 Bremen, Germany","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","3703","3706","Synthetic Aperture Radar (SAR) provides detailed information of Ocean's surface and man-made floating structures. Advances in SAR technology and deployment of new SAR satellites have contributed to an increasing number of remote sensing data available. Handle this large amount of data with human operators is infeasible. Therefore, the use of automated tools to process remote sensing images, identify regions of interest, and select relevant information are needed. The use of neural networks to solve SAR image classification problems is well known. A typical architecture consists of a shallow feed-forward neural network with an input layer, a hidden layer, and an output layer. This type of neural network, combined with back-propagation and a gradient-based training algorithm, is able to solve complex problems in SAR image analysis. However, this architecture is unable to take advantage of unlabeled data during its training process, and in many cases the input features need to be carefully tuned in order to reduce the overall network complexity. This paper proposes the application of Deep Neural Networks (DNN) to perform oceanographic-object classification.","","","10.1109/IGARSS.2015.7326627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326627","SAR Oceanography;Machine Learning;Deep Neural Networks;Automatic Target Identification","Synthetic aperture radar;Biological neural networks;Training;Convolutional codes;Noise reduction;Marine vehicles","image classification;neural nets;object detection;oceanographic techniques;radar imaging;remote sensing by radar;synthetic aperture radar","target classification;oceanographic SAR image analysis;deep neural networks;synthetic aperture radar;man-made floating structure;ocean surface structure;SAR satellite;human operators;remote sensing image;SAR image classification;shallow feed-forward neural network;hidden layer;output layer;back-propagation;gradient-based training algorithm;overall network complexity;oceanographic-object classification;Deep Neural Networks;DNN","","14","14","","","","","IEEE","IEEE Conferences"
"Learning Hierarchical Features for Automated Extraction of Road Markings From 3-D Mobile LiDAR Point Clouds","Y. Yu; J. Li; H. Guan; F. Jia; C. Wang","Fujian Key Laboratory of Sensing and Computing for Smart Cities (SCSC), School of Information Science and Engineering, Xiamen University, Xiamen, FJ, China; Key Laboratory of Underwater Acoustic Communication and Marine Information Technology, Xiamen University, Xiamen, FJ, China; Department of Geography and Environmental Management, University of Waterloo, Waterloo, ON, Canada; Fujian Key Laboratory of Sensing and Computing for Smart Cities (SCSC), School of Information Science and Engineering, Xiamen University, Xiamen, FJ, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities (SCSC), School of Information Science and Engineering, Xiamen University, Xiamen, FJ, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2015","8","2","709","726","This paper presents a novel method for automated extraction of road markings directly from three dimensional (3-D) point clouds acquired by a mobile light detection and ranging (LiDAR) system. First, road surface points are segmented from a raw point cloud using a curb-based approach. Then, road markings are directly extracted from road surface points through multisegment thresholding and spatial density filtering. Finally, seven specific types of road markings are further accurately delineated through a combination of Euclidean distance clustering, voxel-based normalized cut segmentation, large-size marking classification based on trajectory and curb-lines, and small-size marking classification based on deep learning, and principal component analysis (PCA). Quantitative evaluations indicate that the proposed method achieves an average completeness, correctness, and F-measure of 0.93, 0.92, and 0.93, respectively. Comparative studies also demonstrate that the proposed method achieves better performance and accuracy than those of the two existing methods.","","","10.1109/JSTARS.2014.2347276","Natural Sciences and Engineering Research Council of Canada; Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891172","Deep learning;mobile light detection and ranging (LiDAR);point cloud;road marking;three dimensional (3-D) extraction;Deep learning;mobile light detection and ranging (LiDAR);point cloud;road marking;three dimensional (3-D) extraction","Roads;Three-dimensional displays;Feature extraction;Surface treatment;Trajectory;Mobile communication;Laser radar","feature extraction;geophysical image processing;image classification;image filtering;image segmentation;learning (artificial intelligence);optical radar;pattern clustering;principal component analysis;radar imaging;roads;spatial filters","spatial density filtering;Euclidean distance clustering;voxel-based normalized cut segmentation;large-size marking classification;small-size marking classification;curb-line marking classification;principal component analysis;PCA;curb-based approach;multisegment thresholding;road surface point segmentation;light detection and ranging system;three dimensional point cloud;3D mobile LiDAR point cloud;automated road marking extraction;learning hierarchical feature extraction","","55","18","","","","","IEEE","IEEE Journals"
"Feature selection using Deep Neural Networks","D. Roy; K. S. R. Murty; C. K. Mohan","Visual Learning and Intelligence Group (VIGIL), Department of Computer Science and Engineering, Indian Institute of Technology Hyderabad, India; Department of Electrical Engineering, Indian Institute of Technology Hyderabad, India; Visual Learning and Intelligence Group (VIGIL), Department of Computer Science and Engineering, Indian Institute of Technology Hyderabad, India","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","6","Feature descriptors involved in video processing are generally high dimensional in nature. Even though the extracted features are high dimensional, many a times the task at hand depends only on a small subset of these features. For example, if two actions like running and walking have to be identified, extracting features related to the leg movement of the person is enough. Since, this subset is not known apriori, we tend to use all the features, irrespective of the complexity of the task at hand. Selecting task-aware features may not only improve the efficiency but also the accuracy of the system. In this work, we propose a supervised approach for task-aware selection of features using Deep Neural Networks (DNN) in the context of action recognition. The activation potentials contributed by each of the individual input dimensions at the first hidden layer are used for selecting the most appropriate features. The selected features are found to give better classification performance than the original high-dimensional features. It is also shown that the classification performance of the proposed feature selection technique is superior to the low-dimensional representation obtained by principal component analysis (PCA).","","","10.1109/IJCNN.2015.7280626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280626","Supervised Feature Selection;Deep Neural Networks;Action Recognition","Legged locomotion","feature extraction;feature selection;image classification;learning (artificial intelligence);neural nets;video signal processing","task-aware feature selection technique;deep neural networks;feature descriptors;video processing;high-dimensional feature extraction;supervised approach;DNN;action recognition;classification performance;principal component analysis;PCA;low-dimensional representation","","3","28","","","","","IEEE","IEEE Conferences"
"Deep structured learning for mass segmentation from mammograms","N. Dhungel; G. Carneiro; A. P. Bradley","School of Information Technology and Electrical Engineering, The University of Queensland; School of Information Technology and Electrical Engineering, The University of Queensland; ACVT, School of Computer Science, The University of Adelaide","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","2950","2954","In this paper, we present a novel method for the segmentation of breast masses from mammograms exploring structured and deep learning. Specifically, using structured support vector machine (SSVM), we formulate a model that combines different types of potential functions, including one that classifies image regions using deep learning. Our main goal with this work is to show the accuracy and efficiency improvements that these relatively new techniques can provide for the segmentation of breast masses from mammograms. We also propose an easily reproducible quantitative analysis to assess the performance of breast mass segmentation methodologies based on widely accepted accuracy and running time measurements on public datasets, which will facilitate further comparisons for this segmentation problem. In particular, we use two publicly available datasets (DDSM-BCRP and INbreast) and propose the computation of the running time taken for the methodology to produce a mass segmentation given an input image and the use of the Dice index to quantitatively measure the segmentation accuracy. For both databases, we show that our proposed methodology produces competitive results in terms of accuracy and running time.","","","10.1109/ICIP.2015.7351343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351343","Mammograms;mass segmentation;structured learning;structured inference","Image segmentation;Training;Mammography;Indexes;Breast cancer","image segmentation;learning (artificial intelligence);mammography;medical image processing;support vector machines","deep structured learning;mammograms;breast mass segmentation;structured support vector machine;SSVM;image regions;DDSM-BCRP;INbreast;Dice index","","16","19","","","","","IEEE","IEEE Conferences"
"A Map-Reduce Method for Training Autoencoders on Xeon Phi","Q. Yao; X. Liao; H. Jin","NA; NA; Services Comput. Technol. & Syst. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China","2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing","","2015","","","1330","1337","The stacked autoencoder is a deep learning model that consists of multiple autoencoders. This model has been widely applied in numerous machine learning applications. A significant amount of effort has been made to increase the size of the deep learning model with respect to the size of the training dataset and the parameter of the model to improve performance. However, training a large deep learning model is highly time consuming. Recent studies have applied the CPU cluster with thousands of machines as well as the single GPU or the GPU cluster, to train large scale deep learning models. As a high-performance coprocessor like GPU, the Xeon Phi can be an alternative tool for training large scale deep learning models on a single machine. The Xeon Phi can be recognized as a small cluster which features about 60 cores, and each core supports four hardware threads. Massive parallelism offsets the low computing capacity of every core, but challenges an efficient parallel autoencoders design. In this paper, we analyze the training algorithm of autoen-coders based on the matrix operation and point out the thread oversubscription problem, which results in performance degradation. Based on the observation, we propose our map-reduce implementation of autoencoders on the Xeon Phi coprocessor. Our basic idea is to parallelize multiple autoencoder model replicas with bulk synchronous parallel (BSP) communication model where the parameters are updated after the computations of all replicas are completed. Each thread is responsible for one model replica, and all replicas work together on the same mini-batch. This data parallelism method is suitable for training autoencoders on the Xeon Phi, and can extend to asynchronous parallel training method without thread oversubscription. In our experiment the speedup is four times higher than that of sequential implementation. Enlarging the size of the autoencoder model, our method still gets stable speedup.","","","10.1109/CIT/IUCC/DASC/PICOM.2015.197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363241","Deep Learning;Autoencoder;Xeon Phi;Map-reduce;BSP","Computational modeling;Training;Machine learning;Instruction sets;Coprocessors;Graphics processing units;Parallel processing","coprocessors;data handling;learning (artificial intelligence);matrix algebra;parallel processing;pattern clustering","Mapreduce method;autoencoder training;stacked autoencoder;machine learning applications;CPU cluster;GPU cluster;large scale deep learning models;high-performance coprocessor;hardware threads;matrix operation;thread oversubscription problem;autoencoder MapReduce implementation;Xeon Phi coprocessor;bulk synchronous parallel communication model;BSP communication model;data parallelism method;asynchronous parallel training method;thread oversubscription","","1","24","","","","","IEEE","IEEE Conferences"
"Improving the interpretability of deep neural networks with stimulated learning","S. Tan; K. C. Sim; M. Gales","National University of Singapore; National University of Singapore; University of Cambridge","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","617","623","Deep Neural Networks (DNNs) have demonstrated improvements in acoustic modelling for automatic speech recognition. However, they are often used as a black box, and not much is understood about what each of the hidden layers does. We seek to understand how the activations in the hidden layers change with different input, and how we can leverage such knowledge to modify the behaviour of the model. To this end, we propose stimulated deep learning where stimuli are introduced during the DNN training process to influence the behaviour of the hidden units. Specifically, constraints are applied so that the hidden units of each layer will exhibit phone-dependent regional activities when arranged in a 2-dimensional grid. We demonstrate that such constraints are able to yield visible activation regions without compromising the classification of the network and suppressing the activations for a region affects the classification accuracy of the corresponding phone more than the others.","","","10.1109/ASRU.2015.7404853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404853","Deep Neural Networks","Neurons;Biological neural networks;Training;Feature extraction;Visualization;Analytical models","learning (artificial intelligence);neural nets;speech recognition","deep neural network;DNN interpretability;stimulated learning;acoustic modelling;automatic speech recognition;DNN training process;visible activation region;network classification","","13","15","","","","","IEEE","IEEE Conferences"
"Real-time deep learning of robotic manipulator inverse dynamics","A. S. Polydoros; L. Nalpantidis; V. Krüger","Robotics, Vision and Machine Intelligence (RVMI) Lab., Department of Mechanical and Manufacturing Engineering, Aalborg University Copenhagen, Denmark; Robotics, Vision and Machine Intelligence (RVMI) Lab., Department of Mechanical and Manufacturing Engineering, Aalborg University Copenhagen, Denmark; Robotics, Vision and Machine Intelligence (RVMI) Lab., Department of Mechanical and Manufacturing Engineering, Aalborg University Copenhagen, Denmark","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","2015","","","3442","3448","In certain cases analytical derivation of physics-based models of robots is difficult or even impossible. A potential workaround is the approximation of robot models from sensor data-streams employing machine learning approaches. In this paper, the inverse dynamics models are learned by employing a novel real-time deep learning algorithm. The algorithm exploits the methods of self-organized learning, reservoir computing and Bayesian inference. It is evaluated and compared to other state of the art algorithms in terms of generalization ability, convergence and adaptability using five datasets gathered from four robots. Results show that the proposed algorithm can adapt to real-time changes of the inverse dynamics model significantly better than the other state of the art algorithms.","","","10.1109/IROS.2015.7353857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353857","","Adaptation models;Heuristic algorithms;Robot sensing systems;Manipulator dynamics;Computational modeling;Reservoirs","approximation theory;belief networks;inference mechanisms;learning (artificial intelligence);manipulator dynamics;real-time systems","Bayesian inference;reservoir computing;self-organized learning;machine learning;sensor data-streams;approximation;physics-based models;robotic manipulator inverse dynamics;real-time deep learning","","19","23","","","","","IEEE","IEEE Conferences"
"A Generation Method of Immunological Memory in Clonal Selection Algorithm by Using Restricted Boltzmann Machines","S. Kamada; T. Ichimura","Dept. of Intell. Syst., Hiroshima City Univ., Hiroshima, Japan; Fac. of Manage. & Inf. Syst., Prefectural Univ. of Hiroshima, Hiroshima, Japan","2015 IEEE International Conference on Systems, Man, and Cybernetics","","2015","","","2660","2665","Recently, a high technique of image processing is required to extract the image features in real time. In our research, the tourist subject data are collected from the Mobile Phone based Participatory Sensing (MPPS) system. Each record consists of image files with GPS, geographic location name, user's numerical evaluation, and comments written in natural language at sightseeing spots where a user really visits. In our previous research, the famous landmarks in sightseeing spot can be detected by Clonal Selection Algorithm with Immunological Memory Cell (CSAIM). However, some landmarks was not detected correctly by the previous method because they didn't have enough amount of information for the feature extraction. In order to improve the weakness, we propose the generation method of immunological memory by Restricted Boltzmann Machines. To verify the effectiveness of the method, some experiments for classification of the subjective data are executed by using machine learning tools for Deep Learning.","","","10.1109/SMC.2015.465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379597","Image Analysis;Clonal Selection Algorithm;Immunological Memory Cells;Restricted Boltzmann Machines;Deep Learning;Smartphone based Participatory Sensing System;Knowledge Discovery","Training;Feature extraction;Machine learning;Sensors;Machine learning algorithms;Immune system;Data mining","Boltzmann machines;feature extraction;image processing;learning (artificial intelligence)","generation method;clonal selection algorithm;restricted Boltzmann machines;image processing;image feature extraction;tourist subject data;mobile phone based participatory sensing system;MPPS system;image files;GPS;geographic location name;user numerical evaluation;natural language;sightseeing spots;immunological memory cell;CSAIM;machine learning tools;deep learning","","","20","","","","","IEEE","IEEE Conferences"
"Digital CMOS neuromorphic processor design featuring unsupervised online learning","J. Seo; M. Seok","School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, USA; Department of Electrical Engineering, Columbia University, New York, USA","2015 IFIP/IEEE International Conference on Very Large Scale Integration (VLSI-SoC)","","2015","","","49","51","The compute-intensive and power-efficient brain has been a source of inspiration for a broad range of neural networks to solve recognition and classification tasks. Compared to the supervised deep neural networks (DNNs) that have been very successful on well-defined labeled datasets, bio-plausible spiking neural networks (SNNs) with unsupervised learning rules could be well-suited for training and learning representations from the massive amount of unlabeled data. To design dense and low-power hardware for such unsupervised SNNs, we employ digital CMOS circuits for neuromorphic processors, which can exploit transistor scaling and dynamic voltage scaling to the utmost. As exemplary works, we present two neuromorphic processor designs. First, a 45nm neuromorphic chip is designed for a small-scale network of spiking neurons. Through tight integration of memory (64k SRAM synapses) and computation (256 digital neurons), the chip demonstrates on-chip learning on pattern recognition tasks down to 0.53V supply. Secondly, a 65nm neuromorphic processor that performs unsupervised on-line spike-clustering for brain sensing applications is implemented with 1.2k digital neurons and 4.7k latch-based synapses. The processor exhibits a power consumption of 9.3μW/ch at 0.3V supply. Synapse hardware precision, efficient synapse memory array access, overfitting, and voltage scaling will be discussed for dense and power-efficient on-chip learning for CMOS spiking neural networks.","","","10.1109/VLSI-SoC.2015.7314390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314390","neuromorphic computing;CMOS;digital circuits;unsupervised learning;on-chip learning;low-voltage;low-power;spiking neural networks","Neurons;Neuromorphics;Neural networks;CMOS integrated circuits;Hardware;System-on-chip;Training","CMOS digital integrated circuits;logic circuits;logic design;low-power electronics;neural nets;power aware computing;unsupervised learning","digital CMOS neuromorphic processor design;unsupervised online learning;spiking neural networks;digital CMOS circuits;transistor scaling;dynamic voltage scaling;synapse hardware precision;efficient synapse memory array access;voltage 0.53 V;voltage 0.3 V","","7","19","","","","","IEEE","IEEE Conferences"
"Relative attributes with deep Convolutional Neural Network","Dong-Jin Kim; Donggeun Yoo; Sunghoon Im; Namil Kim; T. Sirinukulwattana; In So Kweon","Department of Electrical Engineering, KAIST, Daejeon, Korea; Department of Electrical Engineering, KAIST, Daejeon, Korea; Department of Electrical Engineering, KAIST, Daejeon, Korea; Department of Electrical Engineering, KAIST, Daejeon, Korea; Department of Electrical Engineering, KAIST, Daejeon, Korea; Department of Electrical Engineering, KAIST, Daejeon, Korea","2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","","2015","","","157","158","Our work is based on the idea of relative attributes, aiming to provide more descriptive information to the images. We propose the model that integrates relative-attribute framework with deep Convolutional Neural Networks (CNN) to increase the accuracy of attribute comparison. In addition, we analyzed the role of each network layer in the process. Our model uses features extracted from CNN and is learned by Rank SVM method with these feature vectors. As a result, our model outperforms the original relative attribute model in terms of significant improvement in accuracy.","","","10.1109/URAI.2015.7358851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358851","Deep learning;relative attributes;convolutional neural networks","Support vector machines;Image representation;Feature extraction;Neural networks;Visualization;Computer vision;Image recognition","feature extraction;image processing;neural nets;support vector machines","relative attributes;image descriptive information;deep convolutional neural network;CNN;attribute comparison;network layer;feature extraction;rank SVM method;feature vectors;support vector machine","","","9","","","","","IEEE","IEEE Conferences"
"Semi-Supervised Autoencoder: A Joint Approach of Representation and Classification","W. Haiyan; Y. Haomin; L. Xueming; R. Haijun","Coll. of Comput. Sci., Chongqing Univ., Chongqing, China; Coll. of Comput. Sci., Chongqing Univ., Chongqing, China; Coll. of Comput. Sci., Chongqing Univ., Chongqing, China; Coll. of Comput. Sci., Chongqing Univ., Chongqing, China","2015 International Conference on Computational Intelligence and Communication Networks (CICN)","","2015","","","1424","1430","Recent years have witnessed the significant success of representation learning and deep learning in various prediction and recognition applications. Most of these previous studies adopt the two-phase procedures, namely the first step of representation learning and then the second step of supervised learning. In this process, to fit the training data the initial model weights, which inherits the good properties from the representation learning in the first step, will be changed in the second step. In other words, the second step leans better classification models at the cost of the possible deterioration of the effectiveness of representation learning. Motivated by this observation we propose a joint framework of representation and supervised learning. It aims to learn a model, which not only guarantees the ""semantics"" of the original data from representation learning but also fit the training data well via supervised learning. Along this line we develop the model of semi-supervised Auto encoder under the spirit of the joint learning framework. The experiments on various data sets for classification show the significant effectiveness of the proposed model.","","","10.1109/CICN.2015.275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546333","semi-supervised Autoencoder;deep learning;logistic regression;Sparse Autoencoder","Supervised learning;Logistics;Training data;Training;Neurons;Data models;Semantics","encoding;learning (artificial intelligence);pattern classification","semisupervised autoencoder;representation learning;deep learning;prediction application;recognition application;supervised learning;classification model","","3","17","","","","","IEEE","IEEE Conferences"
"Multi-task joint-learning of deep neural networks for robust speech recognition","Y. Qian; M. Yin; Y. You; K. Yu","Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","310","316","Although deep neural networks (DNNs) have achieved great success in automatic speech recognition (ASR), significant performance degradation still exists in noisy environments. In this paper, a novel multi-task joint-learning framework is proposed to address the noise robustness for speech recognition. The architecture integrates two different DNNs, including the regressive denoising DNN and the discriminative recognition DNN, into a complete multi-task structure and all the parameters can be optimized in a real joint-learning mode just from the beginning in model training. In addition, the basic multi-task structure is further explored and reorganized into a more general framework which can get substantial gains. Furthermore, noise adaptive training can also be easily incorporated within this architecture to achieve further performance improvement. Experiments on the Aurora4 task showed that the proposed approach can achieve a WER below 10% without using adaptation or sequence training, a very large and significant (more than 20% relative) improvement over a strong DNN-HMM baseline.","","","10.1109/ASRU.2015.7404810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404810","Robust speech recognition;Deep neural network;Feature denoising;Multi-task;Noise aware training","Training;Noise reduction;Hidden Markov models;Noise measurement;Speech;Speech recognition;Adaptation models","learning (artificial intelligence);neural nets;speech recognition","multitask joint-learning framework;deep neural networks;robust speech recognition;noise robustness;regressive denoising DNN;discriminative recognition DNN;WER;word error rate;Aurora4 task;DNN-HMM baseline","","9","28","","","","","IEEE","IEEE Conferences"
"Is audio signal processing still useful in the era of machine learning?","E. Vincent","INRIA Nancy, France","2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)","","2015","","","7","7","Summary form only given. Audio signal processing has long been the obvious approach to problems such as microphone array processing, active noise control, or speech enhancement. Yet, it is increasingly being challenged by black-box machine learning approaches based on, e.g., deep neural networks (DNN), which have already achieved superior results on certain tasks. In this talk, I will try to convince that machine learning approaches shouldn't be disregarded, but that black boxes won't solve these problems either. There is hence an opportunity for signal processing researchers to join forces with machine learning researchers and solve these problems together. I will provide examples of this multi-disciplinary approach for audio source separation and robust automatic speech recognition.","","","10.1109/WASPAA.2015.7336882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336882","","Signal processing;Speech recognition;Multiple signal classification;Speech;Conferences;Acoustics;Laboratories","audio signal processing;learning (artificial intelligence);microphone arrays;neural nets;speech recognition","audio signal processing;microphone array processing;active noise control;speech enhancement;black-box machine learning;deep neural networks;DNN;black boxes;machine learning researchers;audio source separation;robust automatic speech recognition","","","","","","","","IEEE","IEEE Conferences"
"Improved Gait recognition based on specialized deep convolutional neural networks","M. Alotaibi; A. Mahmood","Computer Science and Engineering Department, University of Bridgeport, CT 06604, United States; Computer Science and Engineering Department, University of Bridgeport, CT 06604, United States","2015 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)","","2015","","","1","7","Gait recognition is a biometric technique that is used in order to determine the identity of humans based on the style and the manner of their walk. Yet, Gait recognition performance is often degraded by some covariate factors such as a viewing angle variation, clothing and carrying condition changes, and a low-image resolution. In general, current tactics to object recognition highly depend on the use of machine learning techniques. Therefore, a deep convolutional neural network (CNN) is one of the most advanced machine learning techniques that has the ability to approximate complex non-linear functions from high-dimensional input data in a hierarchical process. In this paper, we develop a specialized deep CNN architecture, which consists of multilayers of convolutional and subsampling layers. The proposed technique is less sensitive to several cases of the common variations and occlusions that affect and degrade gait recognition performance. We avoided the use of the typical subspace learning methods, along with its shortcomings, that are widely used in gait recognition. When applied the proposed deep CNN model to CASIA-B large gait database, the experimental results show that the deep CNN model developed in this paper outperforms the other state of art gait recognition techniques in several cases.","","","10.1109/AIPR.2015.7444550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444550","gait recognition;biometrics;convolutional neural networks;deep learning","Gait recognition;Feature extraction;Neural networks;Principal component analysis;Computational modeling;Biological system modeling;Convolution","biometrics (access control);convolution;gait analysis;learning (artificial intelligence);neural nets;nonlinear functions;object recognition","improved gait recognition;specialized deep convolutional neural networks;biometric technique;walking style;object recognition;machine learning techniques;complex nonlinear functions;CASIA-B large gait database","","22","27","","","","","IEEE","IEEE Conferences"
"Fast image search with deep convolutional neural networks and efficient hashing codes","Jun-yi Li; J. Li","Shanghai Jiaotong University Electrical and Electronic, Engineering College, China; Shanghai Jiaotong University Electrical and Electronic, Engineering College, China","2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","","2015","","","1285","1290","Approximate nearest neighbor search is a good method for large-scale image retrieval. We put forward an effective deep learning framework to generate binary hash codes for fast image retrieval after knowing the recent benefits of convolution neural networks (CNN). Our concept is that we can learn binary codes by using a hidden layer to present the latent concepts dominating the class labels when the data labels are usable. CNN also can be used to learn image representations. Other supervised methods require pair-wised inputs for binary code learning. However, our method can be used to learn hash codes and image representations in a point-by-point manner so it is suitable for large-scale datasets. Experimental results show that our method is better than several most advanced hashing algorithms on the CIFAR-10 and MNIST datasets. We will further demonstrate its scalability and efficiency on a largescale dataset with 1 million clothing images.","","","10.1109/FSKD.2015.7382128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382128","convolutional neural networks;nearest neighbor search;hidden layer;LSH;supervised learning","Binary codes;Image retrieval;Image representation;Semantics;Visualization;Image classification;Computational efficiency","binary codes;file organisation;image coding;image representation;image retrieval;learning (artificial intelligence);neural nets;search problems","fast image search;deep convolutional neural networks;hashing codes;approximate nearest neighbor search;large-scale image retrieval;deep learning framework;binary hash codes;image retrieval;convolution neural networks;latent concepts;image representations;supervised methods;binary code learning;point-by-point manner;hashing algorithms;CIFAR-10 datasets;MNIST datasets;clothing images","","1","32","","","","","IEEE","IEEE Conferences"
"Tensor object classification via multilinear discriminant analysis network","Rui Zeng; Jiasong Wu; L. Senhadji; Huazhong Shu","LIST, Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, Nanjing 210096, China; LIST, Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, Nanjing 210096, China; INSERM U 1099, 35000 Rennes, France; LIST, Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, Nanjing 210096, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","1971","1975","This paper proposes an multilinear discriminant analysis network (MLDANet) for the recognition of multidimensional objects, knows as tensor objects. The MLDANet is a variation of linear discriminant analysis network (LDANet) and principal component analysis network (PCANet), both of which are the recently proposed deep learning algorithms. The MLDANet consists of three parts: 1) The encoder learned by MLDA from tensor data. 2) Features maps obtained from decoder. 3) The use of binary hashing and histogram for feature pooling. A learning algorithm for MLDANet is described. Evaluations on UCF11 database indicate that the proposed MLDANet outperforms the PCANet, LDANet, MPCA+LDA, and MLDA in terms of classification for tensor objects.","","","10.1109/ICASSP.2015.7178315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178315","Deep learning;MLDANet;PCANet;LDANet;tensor object classification","Erbium","feature extraction;image classification;image coding;learning (artificial intelligence);object recognition;principal component analysis;tensors","tensor object classification;multilinear discriminant analysis network;MLDANet;multidimensional object recognition;linear discriminant analysis network;LDANet;principal component analysis network;PCANet;deep learning algorithms;features maps;binary hashing;binary histogram;feature pooling;UCF11 database","","5","11","","","","","IEEE","IEEE Conferences"
"Improved deep speaker feature learning for text-dependent speaker recognition","L. Li; Y. Lin; Z. Zhang; D. Wang","Center for Speech and Language Technologies, Division of Technical Innovation and Development, Tsinghua National Laboratory for Information Science and Technology, Center for Speech and Language Technologies, Research Institute of Information Technology, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Center for Speech and Language Technologies, Division of Technical Innovation and Development, Tsinghua National Laboratory for Information Science and Technology, Center for Speech and Language Technologies, Research Institute of Information Technology, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Center for Speech and Language Technologies, Division of Technical Innovation and Development, Tsinghua National Laboratory for Information Science and Technology, Center for Speech and Language Technologies, Research Institute of Information Technology, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Center for Speech and Language Technologies, Division of Technical Innovation and Development, Tsinghua National Laboratory for Information Science and Technology, Center for Speech and Language Technologies, Research Institute of Information Technology, Department of Computer Science and Technology, Tsinghua University, Beijing, China","2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","","2015","","","426","429","A deep learning approach has been proposed recently to derive speaker identifies (d-vector) by a deep neural network (DNN). This approach has been applied to text-dependent speaker recognition tasks and shows reasonable performance gains when combined with the conventional i-vector approach. Although promising, the existing d-vector implementation still can not compete with the i-vector baseline. This paper presents two improvements for the deep learning approach: a phone-dependent DNN structure to normalize phone variation, and a new scoring approach based on dynamic time warping (DTW). Experiments on a text-dependent speaker recognition task demonstrated that the proposed methods can provide considerable performance improvement over the existing d-vector implementation.","","","10.1109/APSIPA.2015.7415306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415306","d-vector;time dynamic warping;speaker recognition","Speaker recognition;Training;Mel frequency cepstral coefficient;Data models;Machine learning;Training data","neural nets;speaker recognition;vectors","deep speaker feature learning;text-dependent speaker recognition;deep neural network;i-vector approach;d-vector;phone-dependent DNN structure;scoring approach;dynamic time warping;DTW","","6","11","","","","","IEEE","IEEE Conferences"
"Detect & Describe: Deep Learning of Bank Stress in the News","S. Rönnqvist; P. Sarlin","NA; NA","2015 IEEE Symposium Series on Computational Intelligence","","2015","","","890","897","News is a pertinent source of information on financial risks and stress factors, which nevertheless is challenging to harness due to the sparse and unstructured nature of natural text. We propose an approach based on distributional semantics and deep learning with neural networks to model and link text to a scarce set of bank distress events. Through unsupervised training, we learn semantic vector representations of news articles as predictors of distress events. The predictive model that we learn can signal coinciding stress with an aggregated index at bank or European level, while crucially allowing for automatic extraction of text descriptions of the events, based on passages with high stress levels. The method offers insight that models based on other types of data cannot provide, while offering a general means for interpreting this type of semantic-predictive model. We model bank distress with data on 243 events and 6.6M news articles for 101 large European banks.","","","10.1109/SSCI.2015.131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7376706","","Semantics;Stress;Predictive models;Machine learning;Training;Neural networks;Indexes","banking;neural nets;text detection;unsupervised learning","detect-and-describe;deep learning;bank distress event;financial risk;stress factor;distributional semantics;neural network;unsupervised training;semantic vector representation;news article;automatic extraction;text description;semantic-predictive model","","3","23","","","","","IEEE","IEEE Conferences"
"Classifier with hierarchical topographical maps as internal representation","T. Trappenberg; P. Hollensen; P. Hartono","Faculty of Computer Science, Dalhousie University, Halifax, Canada; Faculty of Computer Science, Dalhousie University, Halifax, Canada; School of Engineering, Chukyo University, Nagoya, Japan","2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES)","","2015","","","341","345","In this study we want to connect our previously proposed context-relevant topographical maps with the deep learning community. Our architecture is a classifier with hidden layers that are hierarchical two-dimensional topographical maps. These maps differ from the conventional self-organizing maps in that their organizations are influenced by the context of the data labels in a top-down manner. In this way bottom-up and top-down learning are combined in a biologically relevant representational learning setting. Compared to our previous work, we are here specifically elaborating the model in a more challenging setting compared to our previous experiments and to advance more hidden representation layers to bring our discussions into the context of deep representational learning.","","","10.1109/INES.2015.7329752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7329752","","Context;Neurons;Animals;Machine learning;Radial basis function networks;Databases;Organizations","learning (artificial intelligence);pattern classification;self-organising feature maps","hierarchical topographical maps;internal representation;context-relevant topographical maps;deep learning community;hierarchical two-dimensional topographical maps;self-organizing maps;top-down learning;bottom-up learning;deep representational learning","","2","9","","","","","IEEE","IEEE Conferences"
"The Tensor Deep Stacking Network Toolkit","D. Palzer; B. Hutchinson","Computer Science Department, Western Washington University, USA; Computer Science Department, Western Washington University, USA","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","5","In this paper we introduce the Tensor Deep Stacking Network (T-DSN) Toolkit, an implementation of the T-DSN deep learning architecture. The toolkit consists of a Python library and a set of accompanying helper scripts that allow you to train and evaluate T-DSN models. The toolkit is designed to be portable, modular, efficient and parallelized. Our goal for the toolkit is to promote research on this and related deep learning architectures. The T-DSN Toolkit is open source and free for non-commercial use. In this paper, we summarize the core functionality of the toolkit and discuss its design and implementation. We also present a new set of experiments on standard machine learning datasets, demonstrating the model's effectiveness.","","","10.1109/IJCNN.2015.7280297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280297","","Accuracy;Instruction sets;Iris;Computational modeling","learning (artificial intelligence)","tensor deep stacking network toolkit;T-DSN deep learning architecture;Python library;deep learning architectures;machine learning datasets","","2","25","","","","","IEEE","IEEE Conferences"
"Towards 3D object detection with bimodal deep Boltzmann machines over RGBD imagery","Wei Liu; R. Ji; Shaozi Li","Dep. of Cognitive Science, School of Info. Science and Eng., Xiamen University, China; Dep. of Cognitive Science, School of Info. Science and Eng., Xiamen University, China; Dep. of Cognitive Science, School of Info. Science and Eng., Xiamen University, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3013","3021","Nowadays, detecting objects in 3D scenes like point clouds has become an emerging challenge with various applications. However, it retains as an open problem due to the deficiency of labeling 3D training data. To deploy an accurate detection algorithm typically resorts to investigating both RGB and depth modalities, which have distinct statistics while correlated with each other. Previous research mainly focus on detecting objects using only one modality, which ignores exploiting the cross-modality cues. In this work, we propose a cross-modality deep learning framework based on deep Boltzmann Machines for 3D Scenes object detection. In particular, we demonstrate that by learning cross-modality feature from RGBD data, it is possible to capture their joint information to reinforce detector trainings in individual modalities. In particular, we slide a 3D detection window in the 3D point cloud to match the exemplar shape, which the lack of training data in 3D domain is conquered via (1) We collect 3D CAD models and 2D positive samples from Internet. (2) adopt pretrained R-CNNs [2] to extract raw feature from both RGB and Depth domains. Experiments on RMRC dataset demonstrate that the bimodal based deep feature learning framework helps 3D scene object detection.","","","10.1109/CVPR.2015.7298920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298920","","Three-dimensional displays;Solid modeling;Training;Frequency modulation;Joints;Detectors;Feature extraction","Boltzmann machines;feature extraction;image colour analysis;image matching;learning (artificial intelligence);object detection","3D object detection;bimodal deep Boltzmann machines;RGBD imagery;3D scenes;point clouds;3D training data labeling;cross-modality cues;cross-modality deep learning framework;3D scene object detection;cross-modality feature learning;3D detection window;3D point cloud;exemplar shape matching;3D CAD models;2D positive samples;R-CNNs;raw feature extraction;RGB domains;depth domains;RMRC dataset;bimodal based deep feature learning framework","","2","27","","","","","IEEE","IEEE Conferences"
"Deep Head Pose: Gaze-Direction Estimation in Multimodal Video","S. S. Mukherjee; N. M. Robertson","Visionlab at the Edinburgh Research Partnership in Engineering and Mathematics, Heriot-Watt University and the University of Edinburgh, UK; Visionlab at the Edinburgh Research Partnership in Engineering and Mathematics, Heriot-Watt University and the University of Edinburgh, UK","IEEE Transactions on Multimedia","","2015","17","11","2094","2107","In this paper we present a convolutional neural network (CNN)-based model for human head pose estimation in low-resolution multi-modal RGB-D data. We pose the problem as one of classification of human gazing direction. We further fine-tune a regressor based on the learned deep classifier. Next we combine the two models (classification and regression) to estimate approximate regression confidence. We present state-of-the-art results in datasets that span the range of high-resolution human robot interaction (close up faces plus depth information) data to challenging low resolution outdoor surveillance data. We build upon our robust head-pose estimation and further introduce a new visual attention model to recover interaction with the environment . Using this probabilistic model, we show that many higher level scene understanding like human-human/scene interaction detection can be achieved. Our solution runs in real-time on commercial hardware.","","","10.1109/TMM.2015.2482819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279167","Convolutional neural networks (CNNs);deep learning;gaze direction;head-pose;RGB-D","Head;Estimation;Human computer interaction;Surveillance;Magnetic heads;Visualization;Image resolution","approximation theory;estimation theory;feedforward neural nets;human-robot interaction;image classification;image colour analysis;learning (artificial intelligence);pose estimation;probability;regression analysis;robot vision;video surveillance","deep head pose;gaze-direction estimation;multimodal video;convolutional neural network-based model;CNN-based model;human head pose estimation;low-resolution multimodal RGB-D data;human gazing direction classification;learned deep classifier;approximate regression confidence estimation;high-resolution human robot interaction;low resolution outdoor surveillance data;visual attention model;probabilistic model;human-human interaction detection;scene interaction detection","","38","46","","","","","IEEE","IEEE Journals"
"Part-based deep network for pedestrian detection in surveillance videos","Q. Chen; W. Jiang; Y. Zhao; Z. Zhao","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China","2015 Visual Communications and Image Processing (VCIP)","","2015","","","1","4","Accurate pedestrian detection in highly crowded surveillance videos is a challenging task, since the regions of pedestrians in the videos may be largely occluded by other pedestrians. In this paper, we propose an effective part-based deep network cascade (HsNet) to solve this problem. In this model, the part-based scheme effectively restrains the appearance variations of pedestrians caused by heavy occlusion. The deep network captures discriminative information of visible body parts. In addition, the cascade architecture enables very fast detection. We make experiments on one of the largest surveillance video dataset, namely TRECVid SED Pedestrian Dataset (SED-PD). It is shown that in highly crowded surveillance videos, our proposed method achieves very competitive performance compared with state-of-the-art methods. More importantly, our method is significantly faster.","","","10.1109/VCIP.2015.7457855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457855","Pedestrian detection;deep network;cascade;body parts;occlusion handling","Videos;Surveillance;Training;Deformable models;Proposals;Machine learning;Airports","object detection;pedestrians;video signal processing;video surveillance","part-based deep network;pedestrian detection;surveillance video;HsNet;heavy occlusion;TRECVid SED pedestrian dataset;SEDPD","","2","16","","","","","IEEE","IEEE Conferences"
"DEEP-CARVING: Discovering visual attributes by carving deep neural nets","S. Shankar; V. K. Garg; R. Cipolla","Machine Intelligence Lab (MIL), Cambridge University, USA; Computer Science & Artificial Intelligence Lab (CSAIL), MIT, USA; Machine Intelligence Lab (MIL), Cambridge University, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3403","3412","Most of the approaches for discovering visual attributes in images demand significant supervision, which is cumbersome to obtain. In this paper, we aim to discover visual attributes in a weakly supervised setting that is commonly encountered with contemporary image search engines. For instance, given a noun (say forest) and its associated attributes (say dense, sunlit, autumn), search engines can now generate many valid images for any attribute-noun pair (dense forests, autumn forests, etc). However, images for an attributenoun pair do not contain any information about other attributes (like which forests in the autumn are dense too). Thus, a weakly supervised scenario occurs: each of the M attributes corresponds to a class such that a training image in class m ∈ {1, . . . , M} contains a single label that indicates the presence of the mth attribute only. The task is to discover all the attributes present in a test image. Deep Convolutional Neural Networks (CNNs) [20] have enjoyed remarkable success in vision applications recently. However, in a weakly supervised scenario, widely used CNN training procedures do not learn a robust modelfor predicting multiple attribute labels simultaneously. The primary reason is that the attributes highly co-occur within the training data, and unlike objects, do not generally exist as well-defined spatial boundaries within the image. To ameliorate this limitation, we propose Deep-Carving, a novel training procedure with CNNs, that helps the net efficiently carve itselffor the task of multiple attribute prediction. During training, the responses of the feature maps are exploited in an ingenious way to provide the net with multiple pseudo-labels (for training images) for subsequent iterations. The process is repeated periodically after a fixed number of iterations, and enables the net carve itself iteratively for efficiently disentangling features. Additionally, we contribute a noun-adjective pairing inspired Natural Scenes Attributes Dataset to the research community, CAMITNSAD, containing a number of co-occurring attributes within a noun category. We describe, in detail, salient aspects of this dataset. Our experiments on CAMITNSAD and the SUN Attributes Dataset [29], with weak supervision, clearly demonstrate that the Deep-Carved CNNs consistently achieve considerable improvement in the precision of attribute prediction over popular baseline methods.","","","10.1109/CVPR.2015.7298962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298962","","Training;Visualization;Entropy;Computer architecture;Search engines;Predictive models;Robustness","image retrieval;learning (artificial intelligence);neural nets","image visual attribute discovery;deep neural net carving;deep convolutional neural networks;training procedure;multiple attribute prediction;noun-adjective pairing;natural scenes attributes dataset;CAMIT-NSAD;SUN Attributes Dataset;deep-carved CNN","","30","45","","","","","IEEE","IEEE Conferences"
"Prediction of driver's drowsy and alert states from EEG signals with deep learning","M. Hajinoroozi; Z. Mao; Y. Huang","Department of Electrical and Computer Engineering, University of Texas at San Antonio, One UTSA Circle, USA; Department of Electrical and Computer Engineering, University of Texas at San Antonio, One UTSA Circle, USA; Department of Electrical and Computer Engineering, University of Texas at San Antonio, One UTSA Circle, USA","2015 IEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)","","2015","","","493","496","We investigate in this paper deep learning (DL) solutions for prediction of driver's cognitive states (drowsy or alert) using EEG data. We discussed the novel channel-wise convolutional neural network (CCNN) and CCNN-R which is a CCNN variation that uses Restricted Boltzmann Machine in order to replace the convolutional filter. We also consider bagging classifiers based on DL hidden units as an alternative to the conventional DL solutions. To test the performance of the proposed methods, a large EEG dataset from 3 studies of driver's fatigue that includes 70 sessions from 37 subjects is assembled. All proposed methods are tested on both raw EEG and Independent Component Analysis (ICA)-transformed data for cross-session predictions. The results show that CCNN and CCNN-R outperform deep neural networks (DNN) and convolutional neural networks (CNN) as well as other non-DL algorithms and DL with raw EEG inputs achieves better performance than ICA features.","","","10.1109/CAMSAP.2015.7383844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383844","","Electroencephalography;Convolution;Feature extraction;Bagging;Machine learning;Backpropagation;Prediction algorithms","Boltzmann machines;driver information systems;electroencephalography;independent component analysis;learning (artificial intelligence);medical signal processing","driver drowsy state;driver alert state;EEG signal;deep learning;cognitive state;channel-wise convolutional neural network;CCNN-R;restricted Boltzmann machine;bagging classifier;independent component analysis;ICA","","14","18","","","","","IEEE","IEEE Conferences"
"DeepFi: Deep learning for indoor fingerprinting using channel state information","X. Wang; L. Gao; S. Mao; S. Pandey","Department of Electrical and Computer Engineering, Auburn University, Auburn, AL 36849-5201; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL 36849-5201; Department of Electrical and Computer Engineering, Auburn University, Auburn, AL 36849-5201; Cisco Systems, Inc., 170 West Tasman Dr., San Jose, CA 95134","2015 IEEE Wireless Communications and Networking Conference (WCNC)","","2015","","","1666","1671","With the fast growing demand of location-based services in indoor environments, indoor positioning based on fingerprinting has attracted a lot of interest due to its high accuracy. In this paper, we present a novel deep learning based indoor fingerprinting system using Channel State Information (CSI), which is termed DeepFi. Based on three hypotheses on CSI, the DeepFi system architecture includes an off-line training phase and an on-line localization phase. In the off-line training phase, deep learning is utilized to train all the weights as fingerprints. Moreover, a greedy learning algorithm is used to train all the weights layer-by-layer to reduce complexity. In the on-line localization phase, we use a probabilistic method based on the radial basis function to obtain the estimated location. Experimental results are presented to confirm that DeepFi can effectively reduce location error compared with three existing methods in two representative indoor environments.","","","10.1109/WCNC.2015.7127718","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7127718","","Training;Mobile handsets;Antennas;Neurons;IEEE 802.11 Standards;Estimation;Performance evaluation","fingerprint identification;indoor communication;indoor environment;learning (artificial intelligence)","deep learning;indoor fingerprinting;channel state information;location-based services;indoor environments;indoor positioning;indoor fingerprinting system;CSI;DeepFi system architecture;off-line training phase;on-line localization phase;greedy learning algorithm;probabilistic method;radial basis function","","18","14","","","","","IEEE","IEEE Conferences"
"Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation","S. Li; W. Zhang; A. B. Chan","Dept. of Comput. Sci., City Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci., City Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci., City Univ. of Hong Kong, Hong Kong, China","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2848","2856","This paper focuses on structured-output learning using deep neural networks for 3D human pose estimation from monocular images. Our network takes an image and 3D pose as inputs and outputs a score value, which is high when the image-pose pair matches and low otherwise. The network structure consists of a convolutional neural network for image feature extraction, followed by two sub-networks for transforming the image features and pose into a joint embedding. The score function is then the dot-product between the image and pose embeddings. The image-pose embedding and score function are jointly trained using a maximum-margin cost function. Our proposed framework can be interpreted as a special form of structured support vector machines where the joint feature space is discriminatively learned using deep neural networks. We test our framework on the Human3.6m dataset and obtain state-of-the-art results compared to other recent methods. Finally, we present visualizations of the image-pose embedding space, demonstrating the network has learned a high-level embedding of body-orientation and pose-configuration.","","","10.1109/ICCV.2015.326","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410683","","Feature extraction;Three-dimensional displays;Neural networks;Cost function;Support vector machines;Training","feature extraction;image matching;learning (artificial intelligence);neural nets;pose estimation;support vector machines","pose-configuration;body-orientation;image-pose embedding space visualizations;Human3.6m dataset;joint feature space;structured support vector machines;maximum-margin cost function;dot-product;score function;image feature extraction;convolutional neural network;image-pose pair matching;monocular images;deep neural networks;structured-output learning;3D human pose estimation;maximum-margin structured learning","","55","34","","","","","IEEE","IEEE Conferences"
"PCANet for Blind Image Quality Assessment","H. Jia; Q. Sun; T. Wang","Sch. of Comput. Sci. & Eng., Nanjing Univ. of Sci. & Technol., Nanjing, China; Sch. of Comput. Sci. & Eng., Nanjing Univ. of Sci. & Technol., Nanjing, China; Sch. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China","2015 11th International Conference on Computational Intelligence and Security (CIS)","","2015","","","195","198","In this work, we introduce a simple deep learning network, namely, PCANet to general-purpose blind/no-reference image quality assessment (NR-IQA). The goal of no-reference/blind image quality assessment (NR-IQA) is to devise a perceptual model that can accurately predict the quality of a distorted image as human opinions, in which feature extraction is an important issue. However, for most NR-IQA models, their features extraction process were some kind of supervised models and the features are usually natural scene statistics (NSS) based or are perceptually relevant, therefore the performance of these models is limited. In this paper, we present a new NR-IQA metric in which the features are extracted unsupervisely. Once the parameters have been given to the trained deep network, it outputs the final result without any manual mending. Experimental results on the LIVE dataset show that this approach yields state-of-the-art performance.","","","10.1109/CIS.2015.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396285","deep learning;image quality assessment;PCANet;no reference","Feature extraction;Image quality;Measurement;Principal component analysis;Nonlinear distortion;Discrete cosine transforms","feature extraction;image processing;learning (artificial intelligence);natural scenes;principal component analysis","PCANet;deep learning network;general-purpose blind no-reference image quality assessment;perceptual model;human opinions;feature extraction process;natural scene statistics;NSS;NR-IQA metric","","1","18","","","","","IEEE","IEEE Conferences"
"Supervised hashing binary code with deep CNN for image retrieval","J. Li; J. Li","Shanghai Jiaotong University, Electrical and Electronic Engineering College, Shanghai, China; Shanghai Jiaotong University, Electrical and Electronic Engineering College, Shanghai, China","2015 8th International Conference on Biomedical Engineering and Informatics (BMEI)","","2015","","","649","655","Approximate nearest neighbor search is a good method for large-scale image retrieval. We put forward an effective deep learning framework to generate binary hash codes for fast image retrieval after knowing the recent benefits of convolutional neural networks (CNNs). Our concept is that we can learn binary codes by using a hidden layer to present the latent concepts dominating the class labels when the data labels are usable. CNN also can be used to learn image representations. Other supervised methods require pair-wised inputs for binary code learning. However, our method can be used to learn hash codes and image representations in a point-by-point manner so it is suitable for large-scale datasets. Experimental results show that our method is better than several most advanced hashing algorithms on the CIFAR-10 and MNIST datasets. We will further demonstrate its scalability and efficiency on a large-scale dataset with 1 million clothing images.","","","10.1109/BMEI.2015.7401584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401584","convolutional neural networks;nearest neighbor search;hidden layer;LSH;supervised learning","Binary codes;Image retrieval;Image representation;Visualization;Machine learning;Semantics;Computational efficiency","cryptography;image coding;image representation;neural nets","supervised hashing binary code;deep CNN;image retrieval;approximate nearest neighbor search;deep learning framework;convolutional neural networks;CNN;class labels;image representations;binary code learning;CIFAR-10;MNIST","","1","32","","","","","IEEE","IEEE Conferences"
"Deep Semantic Mapping for Cross-Modal Retrieval","C. Wang; H. Yang; C. Meinel","NA; NA; NA","2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)","","2015","","","234","241","Cross-Modal mapping plays an essential role in multimedia information retrieval systems. However, most of existing work paid much attention on learning mapping functions but neglected the exploration of high-level semantic representation of modalities. Inspired by recent success of deep learning, in this paper, deep CNN (convolutional neural networks) features and topic features are utilized as visual and textual semantic representation respectively. To investigate the highly non-linear semantic correlation between image and text, we propose a regularized deep neural network(RE-DNN) for semantic mapping across modalities. By imposing intra-modal regularization as supervised pre-training, we finally learn a joint model which captures both intra-modal and inter-modal relationships. Our approach is superior to previous work in follows: (1) it explores high-level semantic correlations, (2) it requires little prior knowledge for model training, (3) it is able to tackle modality missing problem. Extensive experiments on benchmark Wikipedia dataset show RE-DNN outperforms the state-of-the-art approaches in cross-modal retrieval.","","","10.1109/ICTAI.2015.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372141","deep neural network;semantic representation;cross-modal retrieval;semantic mapping","Semantics;Visualization;Correlation;Training;Neural networks;Feature extraction;Multimedia communication","information retrieval systems;learning (artificial intelligence);multimedia systems;neural nets;text analysis;Web sites","multimedia information retrieval systems;high-level semantic representation;deep CNN features;deep convolutional neural network features;topic features;visual semantic representation;textual semantic representation;nonlinear semantic correlation;regularized deep neural network;intramodal regularization;supervised pretraining;joint model;intermodal relationships;high-level semantic correlations;model training;modality missing problem;benchmark Wikipedia dataset;RE-DNN;deep semantic mapping;cross-modal retrieval;cross-modal mapping","","15","28","","","","","IEEE","IEEE Conferences"
"Zero Shot Deep Learning from Semantic Attributes","P. M. Burlina; A. C. Schmidt; I. Wang","NA; NA; NA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","871","876","We study the problem of classifying images when no training exemplars are available for some image classes, and therefore direct classification is not possible. We use instead semantic attributes: if attributes of yet unseen classes can be determined, then class labels may themselves be decided based on prior knowledge of class to attributes relationships. We present several methods for determining attributes, including (A) an approach based on attribute classifiers, and approaches using (B) MAP and (C) MMSE attribute estimators using image classifiers for known classes. Preliminary tests obtained using a dataset comprised of ImageNet images and Human218 attributes yield encouraging performance.","","","10.1109/ICMLA.2015.140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424431","","Semantics;Training;Estimation;Neural networks;Taxonomy;Visualization;Support vector machines","image classification;learning (artificial intelligence);least mean squares methods","zero shot deep learning;semantic attributes;image classification;image classes;prior knowledge;attribute classifiers;MAP;MMSE;ImageNet images;Human218","","3","15","","","","","IEEE","IEEE Conferences"
"A deep learning approach to structured signal recovery","A. Mousavi; A. B. Patel; R. G. Baraniuk","Department of Electrical and Computer Engineering, Rice University, Houston, TX 77005, United States; Department of Electrical and Computer Engineering, Rice University, Houston, TX 77005, United States; Department of Electrical and Computer Engineering, Rice University, Houston, TX 77005, United States","2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)","","2015","","","1336","1343","In this paper, we develop a new framework for sensing and recovering structured signals. In contrast to compressive sensing (CS) systems that employ linear measurements, sparse representations, and computationally complex convex/greedy algorithms, we introduce a deep learning framework that supports both linear and mildly nonlinear measurements, that learns a structured representation from training data, and that efficiently computes a signal estimate. In particular, we apply a stacked denoising autoencoder (SDA), as an unsupervised feature learner. SDA enables us to capture statistical dependencies between the different elements of certain signals and improve signal recovery performance as compared to the CS approach.","","","10.1109/ALLERTON.2015.7447163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7447163","","Machine learning;Sparse matrices;Neural networks;Training;Atmospheric measurements;Particle measurements;Wavelet domain","compressed sensing;computational complexity;convex programming;greedy algorithms;signal denoising;signal representation;statistical analysis;unsupervised learning","structured signal recovery performance improvement;deep learning approach;compressive sensing system;CS system;linear measurement;sparse representation;computationally complex convex algorithm;computationally complex greedy algorithm;mildly nonlinear measurement;stacked denoising autoencoder;SDA;unsupervised feature learner;statistical dependency capture","","74","36","","","","","IEEE","IEEE Conferences"
"Visualizing extracted feature by deep learning in P300 discrimination task","K. Kawasaki; T. Yoshikawa; T. Furuhashi","Graduate School of Engineering, Nagoya University, Japan; Graduate School of Engineering, Nagoya University, Japan; Graduate School of Engineering, Nagoya University, Japan","2015 7th International Conference of Soft Computing and Pattern Recognition (SoCPaR)","","2015","","","149","154","P300 speller is a system that allows users to input words using electroencephalogram (EEG). A component called P300 is used to interpret the EEG in P300 speller. In order to make a high performance P300 speller, it is essential to discriminate P300 from nonP300 precisely and automatically. In this study, deep learning (DL) is used to discriminate P300. The experimental result shows that DL was possible to discriminate P300 in EEG data, especially in the higher level layer. Furthermore, this study refers to the extracted feature by DL. We can see that DL learns feature from the waveforms correctly to discriminate P300 from others.","","","10.1109/SOCPAR.2015.7492799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7492799","","Feature extraction;Electroencephalography;Data mining;Data visualization;Machine learning;Principal component analysis;Indexes","brain-computer interfaces;data visualisation;electroencephalography;feature extraction;learning (artificial intelligence)","P300 discrimination task;deep learning;P300 speller;electroencephalogram;EEG;feature extraction;DL;brain-computer interfaces","","2","9","","","","","IEEE","IEEE Conferences"
"Active Object Localization with Deep Reinforcement Learning","J. C. Caicedo; S. Lazebnik","Fundacion Univ. Konrad Lorenz, Bogota, Colombia; Univ. of Illinois at Urbana Champaign, Urbana, IL, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2488","2496","We present an active detection model for localizing objects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.","","","10.1109/ICCV.2015.286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410643","","Transforms;Proposals;Search problems;Computational modeling;Prediction algorithms;History;Learning (artificial intelligence)","learning (artificial intelligence);object detection;object recognition","active object localization;deep reinforcement learning;active detection model;Pascal VOC 2007","","97","38","","","","","IEEE","IEEE Conferences"
"When Face Recognition Meets with Deep Learning: An Evaluation of Convolutional Neural Networks for Face Recognition","G. Hu; Y. Yang; D. Yi; J. Kittler; W. Christmas; S. Z. Li; T. Hospedales","NA; NA; NA; NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","384","392","Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a 'good' architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather than investigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluates the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, a traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available.","","","10.1109/ICCVW.2015.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406407","","Face recognition;Face;Databases;Measurement;Training;Object recognition;Convolutional codes","convolution;face recognition;learning (artificial intelligence);neural nets;visual databases","deep learning;convolutional neural network;face recognition systems;public database LFW;labeled faces in the wild;private databases;traditional metric learning method;CNN-FRS performance;source code","","56","33","","","","","IEEE","IEEE Conferences"
"Effective handwritten digit recognition based on multi-feature extraction and deep analysis","Caiyun Ma; Hong Zhang","College of Computer Science and Technology, Wuhan University of Science and Technology, China; Hubei Province Key Laboratory of Intelligent Information, Processing and Real-time Industrial System, Wuhan, China","2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","","2015","","","297","301","Handwritten digit recognition is an important research topic in computer vision and pattern recognition. This paper proposes an effective handwritten digit recognition approach based on specific multi-feature extraction and deep analysis. First, we normalize images of various sizes and stroke thickness in preprocessing to eliminate negative information and keep relevant features. Secondly, considering that handwritten digit image recognition is different from traditional image semantics recognition, we propose specific feature definitions, including structure features, distribution features and projection features. Moreover, we fuse multiple features into the deep neural networks for semantics recognition. Experiments results on benchmark database of MNIST handwritten digit images show that the performance of our algorithm is remarkable and demonstrate its superiority over several existing algorithms.","","","10.1109/FSKD.2015.7381957","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381957","handwritten digit image recognition;multi-feature;deep learning","Feature extraction;Handwriting recognition;Image recognition;Databases;Neural networks;Error analysis;Training","computer vision;feature extraction;handwriting recognition;handwritten character recognition;neural nets","handwritten digit recognition;multifeature extraction;deep analysis;computer vision;pattern recognition;stroke thickness;handwritten digit image recognition;image semantics recognition;distribution features;projection features;neural networks;benchmark database;MNIST handwritten digit images","","","30","","","","","IEEE","IEEE Conferences"
"Simultaneous feature learning and hash coding with deep neural networks","H. Lai; Y. Pan; Ye Liu; S. Yan","Department of Electronic and Computer Engineering, National University of Singapore, Singapore; School of Software, Sun Yan-Sen University, China; School of Information Science and Technology, Sun Yan-Sen University, China; Department of Electronic and Computer Engineering, National University of Singapore, Singapore","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3270","3278","Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. For most existing hashing methods, an image is first encoded as a vector of hand-engineering visual features, followed by another separate projection or quantization step that generates binary codes. However, such visual feature vectors may not be optimally compatible with the coding process, thus producing sub-optimal hashing codes. In this paper, we propose a deep architecture for supervised hashing, in which images are mapped into binary codes via carefully designed deep neural networks. The pipeline of the proposed deep architecture consists of three building blocks: 1) a sub-network with a stack of convolution layers to produce the effective intermediate image features; 2) a divide-and-encode module to divide the intermediate image features into multiple branches, each encoded into one hash bit; and 3) a triplet ranking loss designed to characterize that one image is more similar to the second image than to the third one. Extensive evaluations on several benchmark image datasets show that the proposed simultaneous feature learning and hash coding pipeline brings substantial improvements over other state-of-the-art supervised or unsupervised hashing methods.","","","10.1109/CVPR.2015.7298947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298947","","Convolution;Binary codes;Image representation;Training;Visualization;Quantization (signal);Architecture","binary codes;image coding;image retrieval;learning (artificial intelligence);neural net architecture;quantisation (signal)","simultaneous feature learning;hash coding process;deep neural networks;similarity-preserving hashing method;nearest neighbour search;large-scale image retrieval tasks;hand-engineering visual feature vector;quantization step;binary codes;visual feature vectors;sub-optimal hashing codes;supervised hashing deep architecture;convolution layers;divide-and-encode module;intermediate image features;triplet ranking loss;benchmark image datasets;unsupervised hashing methods","","222","27","","","","","IEEE","IEEE Conferences"
"Detection of seals in remote sensing images using features extracted from deep convolutional neural networks","A. Salberg","Norwegian Computing Center, Gaustadalleen 23a, NO-0373 Oslo, Norway","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","1893","1896","In this paper, we propose an algorithm for automatic detection of seals in aerial remote sensing images using features extracted from a pre-trained deep convolutional neural network (CNN). The method consists of three stages: (i) Detection of potential objects, (ii) feature extraction and (iii) classification of potential objects. The first stage is application dependent, with the aim of detecting all seal pups in the image, with the expense of detecting a large amount of false objects. The second stage extracts generic image features from a local image corresponding to each potential seal detected in the first stage using a CNN trained on the ImageNet database. In the third stage we apply a linear support vector machine to classify the feature vectors extracted in the second stage. The proposed method was demonstrated to an aerial image that contains 84 pups and 128 adult harp seals, and the results show that we are able to detect the seals with high accuracy (2.7% for the adults and 7.3% for the pups). We conclude that deep CNNs trained on the ImageNet database are well suited as a feature extraction module, and using a simple linear SVM, we were able to separate seals from other objects with very high accuracy. We believe that this methodology may be applied to other remote sensing object recognition tasks.","","","10.1109/IGARSS.2015.7326163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326163","Object detection;convolutional neural networks;deep learning;detection of seals","Seals;Feature extraction;Remote sensing;Support vector machines;Databases;Agriculture;Accuracy","feature extraction;geophysical image processing;image classification;neural nets;remote sensing;support vector machines;zoology","feature extraction;deep convolutional neural networks;automatic seal detection;aerial remote sensing image;seal classification;false object detection;ImageNet database;linear SVM;support vector machine;seal pup;adult harp seal;object recognition task","","16","13","","","","","IEEE","IEEE Conferences"
"Efficient representation ranking for transfer learning","S. N. Tran; A. d. Garcez","Department of Computer Science, City University London, Northampton Square, UK, EC1V 0HB; Department of Computer Science, City University London, Northampton Square, UK, EC1V 0HB","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","Representation learning has emerged recently as a useful tool in the extraction of features from data. In a range of applications, features learned from data have been shown superior to their hand-crafted counterpart. Many deep learning approaches have taken advantage of such feature extraction. However, further research is needed on how such features can be evaluated for re-use in related applications, hopefully then improving performance on such applications. In this paper, we present a new method for ranking the representations learned by a Restricted Boltzmann Machine, which has been used regularly as a feature learner by deep networks. We show that high-ranking features, according to our method, should capture more information than low-ranking ones. We then apply representation ranking for pruning the network, and propose a new transfer learning algorithm, which uses such features extracted from a trained network to improve learning performance in another network trained on an analogous domain. We show that by transferring a small number of highest scored representations from source domain our method encourages the learning of new knowledge in target domain while preserving most of the information of the source domain during the transfer. This transfer learning is similar to self-taught learning in that it does not use the source domain data during the transfer process.","","","10.1109/IJCNN.2015.7280454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280454","","Feature extraction;Detectors;Optimization","Boltzmann machines;feature extraction;learning (artificial intelligence)","representation ranking;representation learning;deep learning approaches;feature extraction;restricted Boltzmann machine;feature learner;deep networks;high-ranking features;low-ranking features;network pruning;transfer learning algorithm;learning performance","","","24","","","","","IEEE","IEEE Conferences"
"Learning Complexity-Aware Cascades for Deep Pedestrian Detection","Z. Cai; M. Saberian; N. Vasconcelos","UC San Diego, La Jolla, CA, USA; NA; UC San Diego, La Jolla, CA, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","3361","3369","The design of complexity-aware cascaded detectors, combining features of very different complexities, is considered. A new cascade design procedure is introduced, by formulating cascade learning as the Lagrangian optimization of a risk that accounts for both accuracy and complexity. A boosting algorithm, denoted as complexity aware cascade training (CompACT), is then derived to solve this optimization. CompACT cascades are shown to seek an optimal trade-off between accuracy and complexity by pushing features of higher complexity to the later cascade stages, where only a few difficult candidate patches remain to be classified. This enables the use of features of vastly different complexities in a single detector. In result, the feature pool can be expanded to features previously impractical for cascade design, such as the responses of a deep convolutional neural network (CNN). This is demonstrated through the design of a pedestrian detector with a pool of features whose complexities span orders of magnitude. The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages. This enables state of the art performance on the Caltech and KITTI datasets, at fairly fast speeds.","","","10.1109/ICCV.2015.384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410741","","Complexity theory;Detectors;Feature extraction;Boosting;Proposals;Algorithm design and analysis","computer vision;learning (artificial intelligence);neural nets;pedestrians","learning complexity-aware cascade;deep pedestrian detection;Lagrangian optimization;boosting algorithm;complexity aware cascade training;CompACT cascade;deep convolutional neural network;deep CNN","","126","38","","","","","IEEE","IEEE Conferences"
"High-Speed Security Analytics Powered by In-Memory Machine Learning Engine","A. Sapegin; M. Gawron; D. Jaeger; F. Cheng; C. Meinel","Hasso Plattner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany; Hasso Plattner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany; Hasso Plattner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany; Hasso Plattner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany; Hasso Plattner Inst. (HPI), Univ. of Potsdam, Potsdam, Germany","2015 14th International Symposium on Parallel and Distributed Computing","","2015","","","74","81","Modern Security Information and Event Management systems should be capable to store and process high amount of events or log messages in different formats and from different sources. This requirement often prevents such systems from usage of computational-heavy algorithms for security analysis. To deal with this issue, we built our system based on an in-memory data base with an integrated machine learning library, namely SAP HANA. Three approaches, i.e. (1) deep normalisation of log messages (2) storing data in the main memory and (3) running data analysis directly in the database, allow us to increase processing speed in such a way, that machine learning analysis of security events becomes possible nearly in real-time. To prove our concepts, we measured the processing speed for the developed system on the data generated using Active Directory tested and showed the efficiency of our approach for high-speed analysis of security events.","","","10.1109/ISPDC.2015.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165133","intrusion detection;SAP HANA;in-memory;security;machine learning","Security;Algorithm design and analysis;Machine learning algorithms;Databases;Prediction algorithms;Libraries;Computers","data analysis;learning (artificial intelligence);security of data","high-speed security analytics;in-memory machine learning engine;security information and event management systems;computational-heavy algorithms;in-memory database;integrated machine learning library;SAP HANA;deep log message normalisation;data analysis;machine learning analysis;active directory;high-speed security event analysis","","4","27","","","","","IEEE","IEEE Conferences"
"Recognizing Human Activities from Raw Accelerometer Data Using Deep Neural Networks","L. Zhang; X. Wu; D. Luo","NA; NA; NA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","865","870","Activity recognition from wearable sensor data has been researched for many years. Previous works usually extracted features manually, which were hand-designed by the researchers, and then were fed into the classifiers as the inputs. Due to the blindness of manually extracted features, it was hard to choose suitable features for the specific classification task. Besides, this heuristic method for feature extraction could not generalize across different application domains, because different application domains needed to extract different features for classification. There was also work that used auto-encoders to learn features automatically and then fed the features into the K-nearest neighbor classifier. However, these features were learned in an unsupervised manner without using the information of the labels, thus might not be related to the specific classification task. In this paper, we recommend deep neural networks (DNNs) for activity recognition, which can automatically learn suitable features. DNNs overcome the blindness of hand-designed features and make use of the precious label information to improve activity recognition performance. We did experiments on three publicly available datasets for activity recognition and compared deep neural networks with traditional methods, including those that extracted features manually and auto-encoders followed by a K-nearest neighbor classifier. The results showed that deep neural networks could generalize across different application domains and got higher accuracy than traditional methods.","","","10.1109/ICMLA.2015.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424430","activity recognition;deep neural networks;feature learning;accelerometer data","Feature extraction;Neural networks;Accelerometers;Decision trees;Training;Acceleration;Testing","accelerometers;data analysis;neural nets;pattern classification","accelerometer data;deep neural networks;human activity recognition;auto-encoders;K-nearest neighbor classifier;DNN","","8","22","","","","","IEEE","IEEE Conferences"
"Text Flow: A Unified Text Detection System in Natural Scene Images","S. Tian; Y. Pan; C. Huang; S. Lu; K. Yu; C. L. Tan","Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Baidu Res., Inst. of Deep Learning, China; Baidu Res., Inst. of Deep Learning, China; Visual Comput. Dept., Inst. for Infocomm Res., Singapore, Singapore; Baidu Res., Inst. of Deep Learning, China; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","4651","4659","The prevalent scene text detection approach follows four sequential steps comprising character candidate detection, false character candidate removal, text line extraction, and text line verification. However, errors occur and accumulate throughout each of these sequential steps which often lead to low detection performance. To address these issues, we propose a unified scene text detection system, namely Text Flow, by utilizing the minimum cost (min-cost) flow network model. With character candidates detected by cascade boosting, the min-cost flow network model integrates the last three sequential steps into a single process which solves the error accumulation problem at both character level and text line level effectively. The proposed technique has been tested on three public datasets, i.e, ICDAR2011 dataset, ICDAR2013 dataset and a multilingual dataset and it outperforms the state-of-the-art methods on all three datasets with much higher recall and F-score. The good performance on the multilingual dataset shows that the proposed technique can be used for the detection of texts in different languages.","","","10.1109/ICCV.2015.528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410885","","Boosting;Feature extraction;Image edge detection;Computational efficiency;Joining processes;Layout;Stability analysis","image processing;text analysis","unified text detection system;text flow;natural scene images;scene text detection;false character candidate removal;character candidate detection;text line extraction;text line verification;cascade boosting","","77","32","","","","","IEEE","IEEE Conferences"
"Image classification using RBM to encode local descriptors with group sparse learning","J. Wang; W. Wang; R. Wang; W. Gao","School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University; School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University; School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University; National Engineering Laboratory for Video Technology, Peking University","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","912","916","This paper proposes to employ deep learning model to encode local descriptors for image classification. Previous works using deep architectures to obtain higher representations are often operated from pixel level, which lack the power to be generalized to large-size and complex images due to computational burdens and internal essence capture. Our method slips the leash of this limitation by starting from local descriptors to leverage more semantical inputs. We investigate to use two layers of Restricted Boltzmann Machines (RBMs) to encode different local descriptors with a novel group sparse learning (GSL) inspired by the recent success of sparse coding. Besides, unlike the most existing pure unsupervised feature coding strategies, we use another RBM corresponding to semantic labels to perform supervised fine-tuning which makes our model more suitable for classification task. Experimental results on Caltech-256 and Indoor-67 datasets demonstrate the effectiveness of our method.","","","10.1109/ICIP.2015.7350932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350932","Image Classification;Feature Coding;Restricted Boltzmann Machine (RBM);Group Sparse Learning (GSL)","Computer architecture;Encoding;Training;Machine learning;Training data;Visualization;Computational modeling","Boltzmann machines;image classification;image coding;learning (artificial intelligence)","image classification;local descriptor encoding;group sparse learning;deep learning model;local descriptors;restricted Boltzmann machines;RBMs;GSL;sparse coding;semantic labels;supervised fine-tuning;classification task;Caltech-256 datasets;Indoor-67 datasets","","4","34","","","","","IEEE","IEEE Conferences"
"Multimodal deep learning for robust RGB-D object recognition","A. Eitel; J. T. Springenberg; L. Spinello; M. Riedmiller; W. Burgard","Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany; Department of Computer Science, University of Freiburg, Germany","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","2015","","","681","687","Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset [15] and show recognition in challenging RGB-D real-world noisy settings.","","","10.1109/IROS.2015.7353446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353446","","Object recognition;Training;Image coding;Robot sensing systems;Streaming media;Feature extraction;Robustness","feedforward neural nets;image colour analysis;image fusion;learning (artificial intelligence);object recognition;robot vision","multimodal deep learning;robust RGB-D object recognition;real-world robotics applications;convolutional neural networks;CNN;RGB-D architecture;fusion network;imperfect sensor data;real-world robotics tasks;accurate learning;multistage training methodology;data augmentation scheme;robust learning;realistic noise patterns;RGB-D object dataset;RGB-D real-world noisy settings","","188","25","","","","","IEEE","IEEE Conferences"
"Online marginalized linear stacked denoising autoencoders for learning from big data stream","A. Budiman; M. I. Fanany; C. Basaruddin","Faculty of Computer Science, University of Indonesia, Depok, West Java Indonesia; Faculty of Computer Science, University of Indonesia, Depok, West Java Indonesia; Faculty of Computer Science, University of Indonesia, Depok, West Java Indonesia","2015 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","","2015","","","227","235","Big non-stationary data, which comes in gradual fashion or stream, is one important issue in the application of big data to train deep learning machines. In this paper, we focused on a unique variant of traditional autoencoder, which is called Marginalized Linear Stacked Denoising Autoencoder (MLSDA). MLSDA uses a simple linear model. It is faster and uses less number of parameters than the traditional SDA. It also takes advantages of convex optimization. It has better improvement in the bag of words feature representation. However, the traditional SDA with stochastic gradient descent has been more widely accepted in many applications. The stochastic gradient descent is naturally an online learning. It makes the traditional SDA more scalable for streaming big data. This paper proposes a simple modification of MLSDA. Our modification uses matrix multiplication concept for online learning. The experiment result showed the similar accuracy level compared with a batch version of MLSDA and using lower computation resources. The online MLSDA will improve the scalability of MLSDA for handling streaming big data that representing bag of words features for natural language processing, information retrieval, and computer vision.","","","10.1109/ICACSIS.2015.7415181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415181","Autoencoder;big data;deep learning;denoising;online;sequential","Natural language processing;Graphics processing units;Support vector machines","Big Data;gradient methods;learning (artificial intelligence);matrix multiplication;optimisation","online marginalized linear stacked denoising autoencoders;big data stream;big nonstationary data;deep learning machines;traditional autoencoder;MLSDA;simple linear model;traditional SDA;convex optimization;bag of words feature representation;stochastic gradient descent;matrix multiplication concept;online learning;information retrieval;natural language processing;computer vision","","","21","","","","","IEEE","IEEE Conferences"
"Improving deep neural network ensembles using reconstruction error","Wenhao Huang; Haikun Hong; Kaigui Bian; Xiabing Zhou; Guojie Song; Kunqing Xie","Key Laboratory of Machine Perception, Ministry of Education, Peking University, Beijing, 100871, China; Key Laboratory of Machine Perception, Ministry of Education, Peking University, Beijing, 100871, China; Key Laboratory of Machine Perception, Ministry of Education, Peking University, Beijing, 100871, China; Key Laboratory of Machine Perception, Ministry of Education, Peking University, Beijing, 100871, China; Key Laboratory of Machine Perception, Ministry of Education, Peking University, Beijing, 100871, China; Key Laboratory of Machine Perception, Ministry of Education, Peking University, Beijing, 100871, China","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","7","Ensemble learning of neural network is a learning paradigm where ensembles of several neural networks show improved generalization capabilities that outperform those of single networks. For deep learning of multi-layer neural networks, ensemble learning is still applicable. In addition, characteristics of deep neural networks can provide potential opportunities to improve the performance of traditional neural network ensembles. In this paper, we propose an ensemble criterion of deep neural networks that is based on the reconstruction error and present two strategies to solve the most important issues in ensemble learning of neural networks: component dataset sampling and output averaging. Component training datasets are selected according to the reconstruction error instead of random bootstrap sampling or re-weighting. Moreover, for each testing instance, we can compute the reconstruction error yielded by the sub-model simultaneously with the output. The reconstruction error is used as the weights in output averaging. From the perspectives of prediction interval and confidence interval, we demonstrated that smaller reconstruction error could ensure smaller prediction interval. We also incorporate the famous structure ensemble approach “Dropout” into the proposed approach to achieve the best performance. We conduct experiments on classification and regression datasets to validate the effectiveness of our approach.","","","10.1109/IJCNN.2015.7280524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280524","","Silicon","generalisation (artificial intelligence);learning (artificial intelligence);neural nets;sampling methods","deep neural network ensemble;reconstruction error;ensemble learning;learning paradigm;generalization capability;deep learning;multilayer neural network;ensemble criterion;component dataset sampling;output averaging;component training dataset;random bootstrap sampling;testing instance;prediction interval;confidence interval","","2","24","","","","","IEEE","IEEE Conferences"
"A unified framework for automatic wound segmentation and analysis with deep convolutional neural networks","C. Wang; X. Yan; M. Smith; K. Kochhar; M. Rubin; S. M. Warren; J. Wrobel; H. Lee","Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA; University of Michigan Medical School, Ann Arbor, USA; Department of Plastic Surgery, New York University, USA; Department of Plastic Surgery, New York University, USA; University of Michigan Medical School, Ann Arbor, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","2415","2418","Wound surface area changes over multiple weeks are highly predictive of the wound healing process. Furthermore, the quality and quantity of the tissue in the wound bed also offer important prognostic information. Unfortunately, accurate measurements of wound surface area changes are out of reach in the busy wound practice setting. Currently, clinicians estimate wound size by estimating wound width and length using a scalpel after wound treatment, which is highly inaccurate. To address this problem, we propose an integrated system to automatically segment wound regions and analyze wound conditions in wound images. Different from previous segmentation techniques which rely on handcrafted features or unsupervised approaches, our proposed deep learning method jointly learns task-relevant visual features and performs wound segmentation. Moreover, learned features are applied to further analysis of wounds in two ways: infection detection and healing progress prediction. To the best of our knowledge, this is the first attempt to automate long-term predictions of general wound healing progress. Our method is computationally efficient and takes less than 5 seconds per wound image (480 by 640 pixels) on a typical laptop computer. Our evaluations on a large-scale wound database demonstrate the effectiveness and reliability of the proposed system.","","","10.1109/EMBC.2015.7318881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318881","","Wounds;Image segmentation;Support vector machines;Feature extraction;Neural networks;Visualization;Training","biological tissues;feature extraction;image segmentation;medical image processing;neural nets;unsupervised learning","unified automatic wound segmentation framework;deep convolutional neural networks;wound healing process;tissue quality;tissue quantity;prognostic information;wound surface area measurements;wound treatment;wound images;handcrafted features;unsupervised approach;deep learning method;task-relevant visual features;infection detection;healing progress prediction;long-term predictions;large-scale wound database","Automation;Humans;Image Processing, Computer-Assisted;Machine Learning;Neural Networks (Computer);Reproducibility of Results;Wound Healing","4","24","","","","","IEEE","IEEE Conferences"
"Learning Deep Object Detectors from 3D Models","X. Peng; B. Sun; K. Ali; K. Saenko","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1278","1286","Crowdsourced 3D CAD models are easily accessible online, and can potentially generate an infinite number of training images for almost any object category. We show that augmenting the training data of contemporary Deep Convolutional Neural Net (DCNN) models with such synthetic data can be effective, especially when real training data is limited or not well matched to the target domain. Most freely available CAD models capture 3D shape but are often missing other low level cues, such as realistic object texture, pose, or background. In a detailed analysis, we use synthetic CAD images to probe the ability of DCNN to learn without these cues, with surprising findings. In particular, we show that when the DCNN is fine-tuned on the target detection task, it exhibits a large degree of invariance to missing low-level cues, but, when pretrained on generic ImageNet classification, it learns better when the low-level cues are simulated. We show that our synthetic DCNN training approach significantly outperforms previous methods on the benchmark PASCAL VOC2007 dataset when learning in the few-shot scenario and improves performance in a domain shift scenario on the Office benchmark.","","","10.1109/ICCV.2015.151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410508","","Solid modeling;Three-dimensional displays;Design automation;Image color analysis;Training;Data models;Detectors","CAD;convolution;learning (artificial intelligence);neural nets;object detection;solid modelling","deep object detector learning;3D CAD model;deep convolutional neural net;DCNN model;CAD image;target detection","","82","27","","","","","IEEE","IEEE Conferences"
"Algorithmic content generation for products","C. Khatri; S. Voleti; S. Veeraraghavan; N. Parikh; A. Islam; S. Mahmood; N. Garg; V. Singh","eBay Inc., 2065 Hamilton Av., San Jose, CA; eBay Inc., 2065 Hamilton Av., San Jose, CA; eBay Inc., 2065 Hamilton Av., San Jose, CA; eBay Inc., 2065 Hamilton Av., San Jose, CA; eBay Inc., 2065 Hamilton Av., San Jose, CA; eBay Inc., 2065 Hamilton Av., San Jose, CA; eBay Inc., 2065 Hamilton Av., San Jose, CA; eBay Inc., 2065 Hamilton Av., San Jose, CA","2015 IEEE International Conference on Big Data (Big Data)","","2015","","","2945","2947","Content is one of the most essential parts of products on e-commerce websites such as eBay. It not only drives user-engagement but also traffic from various search engine websites based on the relevance. Generating the content for the products, however comes with a wide set of challenges, due to the complexity of commerce at scale, and requires new applications in text processing and information extraction to address some core issues. Some of the factors which need to be addressed are: scalability (millions of products), dynamism (products change with time), removal of item-specific or seller specific information (maintain generality), size of the content etc. Generally, curators are hired for writing the product descriptions manually, which is not cost-effective and is not scalable. In the current work, an algorithmic framework based on Natural Language Processing and Deep Learning is proposed and used to generate the content for ecommerce products. Seller descriptions for multiple items aggregated at a product level are used for content generation. Furthermore, a combination of behavioral and text signals such as search queries are also used to understand the user intent. Two different approaches are proposed in this work: Extraction (sentence retrieval) and Abstraction (sentence generation). The results of both the methods are analyzed and it is depicted that algorithmic content generation is scalable, fast and has potential to cut down the manualcuration cost dramatically.","","","10.1109/BigData.2015.7364131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364131","e-commerce;content generation;summarization;Natural Language Processing;Deep Learning;Recurrent Neural Networks;data mining;text mining;word vectors","Universal Serial Bus;Context;Recurrent neural networks;Manuals;Search engines;Business;Natural language processing","data mining;electronic commerce;learning (artificial intelligence);natural language processing;text analysis","algorithmic content generation;e-commerce Web sites;electronic commerce;eBay;text processing;information extraction;product description writing;natural language processing;deep learning;extraction approach;sentence retrieval approach;abstraction approach;sentence generation approach","","1","8","","","","","IEEE","IEEE Conferences"
"Heterogeneous Feature Selection With Multi-Modal Deep Neural Networks and Sparse Group LASSO","L. Zhao; Q. Hu; W. Wang","School of Computer Science and Technology, Tianjin University, Tianjin, China; School of Computer Science and Technology, Tianjin University, Tianjin, China; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK","IEEE Transactions on Multimedia","","2015","17","11","1936","1948","Heterogeneous feature representations are widely used in machine learning and pattern recognition, especially for multimedia analysis. The multi-modal, often also high- dimensional , features may contain redundant and irrelevant information that can deteriorate the performance of modeling in classification. It is a challenging problem to select the informative features for a given task from the redundant and heterogeneous feature groups. In this paper, we propose a novel framework to address this problem. This framework is composed of two modules, namely, multi-modal deep neural networks and feature selection with sparse group LASSO. Given diverse groups of discriminative features, the proposed technique first converts the multi-modal data into a unified representation with different branches of the multi-modal deep neural networks. Then, through solving a sparse group LASSO problem, the feature selection component is used to derive a weight vector to indicate the importance of the feature groups. Finally, the feature groups with large weights are considered more relevant and hence are selected. We evaluate our framework on three image classification datasets. Experimental results show that the proposed approach is effective in selecting the relevant feature groups and achieves competitive classification performance as compared with several recent baseline methods.","","","10.1109/TMM.2015.2477058","973 Program; 973 Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7244241","Deep learning;feature selection;heterogeneous data;multi-modal;sparse representation","Feature extraction;Neural networks;Kernel;Multimedia communication;Data mining;Machine learning","feature extraction;feature selection;image classification;image representation;learning (artificial intelligence);neural nets","heterogeneous feature selection;multimodal deep-neural networks;heterogeneous feature representations;high-dimensional features;redundant irrelevant information;informative feature selection;redundant heterogeneous feature groups;discriminative features;unified representation;sparse group LASSO problem;weight vector;feature groups;image classification datasets","","25","55","","","","","IEEE","IEEE Journals"
"Sleep spindle detection using deep learning: A validation study based on crowdsourcing","D. Tan; R. Zhao; J. Sun; W. Qin","Center of sleep and neural image, the school of life science and technology, Xidian University, shannxi, 710126, China; Center of sleep and neural image, the school of life science and technology, Xidian University, shannxi, 710126, China; School of life science and technology, Xidian University, shannxi 710126, China; School of life science and technology, Xidian University, shannxi 710126, China","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","2828","2831","Sleep spindles are significant transient oscillations observed on the electroencephalogram (EEG) in stage 2 of non-rapid eye movement sleep. Deep belief network (DBN) gaining great successes in images and speech is still a novel method to develop sleep spindle detection system. In this paper, crowdsourcing replacing gold standard was applied to generate three different labeled samples and constructed three classes of datasets with a combination of these samples. An F1-score measure was estimated to compare the performance of DBN to other three classifiers on classifying these samples, with the DBN obtaining an result of 92.78%. Then a comparison of two feature extraction methods based on power spectrum density was made on same dataset using DBN. In addition, the DBN trained in dataset was applied to detect sleep spindle from raw EEG recordings and performed a comparable capacity to expert group consensus.","","","10.1109/EMBC.2015.7318980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318980","","Sleep;Electroencephalography;Crowdsourcing;Feature extraction;Sensitivity;Time-frequency analysis;Standards","belief networks;electroencephalography;sleep","sleep spindle detection;deep learning;crowdsourcing;transient oscillations;electroencephalogram;nonrapid eye movement sleep;deep belief network;speech;F1-score measure","Crowdsourcing;Electroencephalography;Humans;Machine Learning;Sleep","5","24","","","","","IEEE","IEEE Conferences"
"Online learning and socratic method in increasing self-motivation: A literature review","S. M. Suhadi; Z. Abdullah; N. M. Zaid; H. Mohamed; B. Aris; M. Sanmugam","Department of Educational Science, Mathematics and Creative Multimedia, Faculty of Education, Universiti Teknologi Malaysia; Department of Educational Science, Mathematics and Creative Multimedia, Faculty of Education, Universiti Teknologi Malaysia; Department of Educational Science, Mathematics and Creative Multimedia, Faculty of Education, Universiti Teknologi Malaysia; Department of Educational Science, Mathematics and Creative Multimedia, Faculty of Education, Universiti Teknologi Malaysia; Department of Educational Science, Mathematics and Creative Multimedia, Faculty of Education, Universiti Teknologi Malaysia; Department of Educational Science, Mathematics and Creative Multimedia, Faculty of Education, Universiti Teknologi Malaysia","2015 3rd International Conference on Information and Communication Technology (ICoICT)","","2015","","","11","16","Independent students who are capable of learning by themselves are hard to find nowadays. Normally, most of the students would require some motivation to learn; either from their peers, teachers or parents. Self-motivation is an ability to do what needs to be done, without influence from other people or situations. Therefore, self-motivation plays a crucial role in the students' knowledge attainment and academic achievement. Hence, it can now be delivered through the use of technology, making learning more fun, addictive and faster knowledge acquirement for these students. We suggest the use of the Socratic Method as a form of providing self-motivation through the usage of an online platform. Meanwhile, through the Socratic Method of questioning, students will have to practice deep thinking which paves the way to the construction of a new knowledge and helps them generate more meaningful ideas and robust answers. The Socratic Method with online approach will also encourage the students to interact with other students; without the limitation of time or location. This concept paper will analyze previous studies that are related to the Socratic Method of learning and online learning; whilst looking at the impact of Socratic Method of learning and online learning in increasing students' self-motivation consequently encouraging independent learning among these students.","","","10.1109/ICoICT.2015.7231389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7231389","Socratic Learning;Motivation in learning;Technology in Socratic Method;Online learning;Self-motivation","Electronic mail;Multimedia communication;Learning systems;Seminars;Analytical models","computer aided instruction","online learning;socratic method;independent student;self-motivation;student knowledge attainment;academic achievement;knowledge acquirement;online platform;deep thinking;Socratic Method","","1","20","","","","","IEEE","IEEE Conferences"
"Essential feature extraction of driving behavior using a deep learning method","H. Liu; T. Taniguchi; Y. Tanaka; K. Takenaka; T. Bando","The Graduate School of Information Science and Engineering, Ritsumeikan University, 1-1-1 Noji Higashi, Kusatsu, Shiga 525-8577, Japan; The College of Information Science and Engineering, Ritsumeikan University, 1-1-1 Noji Higashi, Kusatsu, Shiga 525-8577, Japan; Corporate R&D Div.3, DENSO CORPORATION, Aichi, Japan; Corporate R&D Div.3, DENSO CORPORATION, Aichi, Japan; Corporate R&D Div.3, DENSO CORPORATION, Aichi, Japan","2015 IEEE Intelligent Vehicles Symposium (IV)","","2015","","","1054","1060","Driving behavior can be represented by many different types of measured sensor information obtained through a control area network. We assume that the measured sensor information is generated from several hidden time-series data through multiple nonlinear transformations. These hidden time-series data are statistically independent of each other and capture essential driving behavior. Driving behavior information is usually generated by multiple nonlinear transformations that fuse essential features, e.g., ""Yaw rate"" is generated by fusing the velocity of the vehicle and the change of driving direction. However, driving behavior data is often redundant because such data includes multivariate information and involves duplicated essential features. In this paper, we propose a feature extraction method to extract essential features from redundant driving behavior data using a deep sparse autoencoder (DSAE), which is a deep learning method. Two-dimensional features are extracted from seven-dimensional artificial data using a DSAE and are determined experimentally to be highly correlated with the prepared essential features. DSAEs are also used to extract features from an actual driving behavior data set. To verify a DSAE's ability to extract essential driving behavior features and filter out redundant information, we prepare twelve data sets that include some or all of the driving behavior information. Twelve DSAEs are used to independently extract features from the twelve prepared data sets, and canonical correlation analysis is used to analyze the canonical correlation coefficients between extracted features. Furthermore, we verify DSAEs' ability to extract essential driving behavior features from the redundant driving behavior data sets.","","","10.1109/IVS.2015.7225824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7225824","","Frequency modulation;Correlation;Feature extraction;Principal component analysis;Erbium;Data mining;Intelligent vehicles","behavioural sciences computing;driver information systems;learning (artificial intelligence);time series","deep-learning method;sensor information;control area network;hidden time-series data;multiple nonlinear transformations;statistical analysis;yaw rate;vehicle velocity fusion;driving direction;multivariate information;duplicated essential-feature extraction;deep-sparse autoencoder;DSAE;two-dimensional feature extraction;seven-dimensional artificial data;canonical correlation coefficient analysis;redundant driving behavior data sets","","11","10","","","","","IEEE","IEEE Conferences"
"Deep learning and the information bottleneck principle","N. Tishby; N. Zaslavsky","The Edmond and Lilly Safra Center for Brain Sciences, Hebrew University of Jerusalem, Israel; The Edmond and Lilly Safra Center for Brain Sciences, Hebrew University of Jerusalem, Israel","2015 IEEE Information Theory Workshop (ITW)","","2015","","","1","5","Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.","","","10.1109/ITW.2015.7133169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7133169","","Distortion;Complexity theory;Mutual information;Bifurcation;Computer architecture;Feature extraction;Training","bifurcation;data compression;learning (artificial intelligence);neural nets","information bottleneck principle;deep neural networks;DNN;IB principle;mutual information;output variables;optimal information theoretic limits;finite sample generalization bounds;optimal architecture;layer connections;bifurcation points;input layer compression;output layer;hierarchical layered network representations;structural phase transitions;information curve;optimality bounds;deep learning algorithms;layer features","","70","14","","","","","IEEE","IEEE Conferences"
"A deep architecture for visually analyze Pap cells","O. Chang; P. Constante; A. Gordon; M. Singania; F. Acuna","Universidad de las Fuerzas Armadas. ESPE Extension Latacunga, Ecuador 050150; Universidad de las Fuerzas Armadas. ESPE Extension Latacunga, Ecuador 050150; Universidad de las Fuerzas Armadas. ESPE Extension Latacunga, Ecuador 050150; Universidad de las Fuerzas Armadas. ESPE Extension Latacunga, Ecuador 050150; Universidad de las Fuerzas Armadas. ESPE Extension Latacunga, Ecuador 050150","2015 IEEE 2nd Colombian Conference on Automatic Control (CCAC)","","2015","","","1","6","This work proposes a deep ANN architecture which accomplishes the reliable visual classification of abnormal Pap smear cell. The system is driven by independent agents where the first agent consists of a three layer ANN pretrained to closely track a reticle pattern. This net participates in a local close loop that oscillates and produces unique time-space versions of the visual data. This information is stabilized and sparsed in order to obtain compact data representations, with implicit space time content. The obtained representations are delivered to second level agents, formed by independent three layers ANNs dedicated to learning and recognition activities. To train the system a noise-balanced algorithm is employed, where the training set is composed by pap cells and white noise. This combination operating on finite databases and in a self controlled learning loop, auto develops enough cell recognition knowledge as to classify whole classes of Pap smear cells. The system has been tested in real time utilizing documented data bases.","","","10.1109/CCAC.2015.7345210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345210","deep architectures;deep learning;Pap cell recognition;noise balanced learning","Artificial neural networks;Computer architecture;Training;Databases;Neurons;Tracking;Backpropagation","image classification;medical image processing;neural net architecture","deep ANN architecture;visual classification;abnormal Pap smear cell;reticle pattern;noise-balanced algorithm;finite databases","","","16","","","","","IEEE","IEEE Conferences"
"Localized Deep Extreme Learning Machines for Efficient RGB-D Object Recognition","H. F. M. Zaki; F. Shafait; A. Mian","Comput. Sci. & Software Eng., Univ. of Western Australia, Crawley, WA, Australia; Nat. Univ. of Sci. & Technol., Islamabad, Pakistan; Mechatron. Eng., Int. Islamic Univ. Malaysia, Kuala Lumpur, Malaysia","2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA)","","2015","","","1","8","Existing RGB-D object recognition methods either use channel specific handcrafted features, or learn features with deep networks. The former lack representation ability while the latter require large amounts of training data and learning time. In real-time robotics applications involving RGB-D sensors, we do not have the luxury of both. In this paper, we propose Localized Deep Extreme Learning Machines (LDELM) that efficiently learn features from RGB-D data. By using localized patches, not only is the problem of data sparsity solved, but the learned features are robust to occlusions and viewpoint variations. LDELM learns deep localized features in an unsupervised way from random patches of the training data. Each image is then feed-forwarded, patch-wise, through the LDELM to form a cuboid of features. The cuboid is divided into cells and pooled to get the final compact image representation which is then used to train an ELM classifier. Experiments on the benchmark Washington RGB-D and 2D3D datasets show that the proposed algorithm not only is significantly faster to train but also outperforms state-of-the-art methods in terms of accuracy and classification time.","","","10.1109/DICTA.2015.7371280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371280","","Feature extraction;Training;Neural networks;Object recognition;Training data;Visualization;Three-dimensional displays","image classification;image colour analysis;image representation;object recognition;robot vision","localized deep extreme learning machines;RGB-D object recognition methods;channel specific handcrafted features;deep networks;robotics applications;RGB-D sensors;LDELM;RGB-D data;localized patches;data sparsity;deep localized features;random patches;training data;cuboid;compact image representation;ELM classifier;2D3D datasets;classification time","","6","35","","","","","IEEE","IEEE Conferences"
"Trax solver on Zynq with Deep Q-Network","N. Sugimoto; T. Mitsuishi; T. Kaneda; C. Tsuruta; R. Sakai; H. Shimura; H. Amano","Keio University, Yokohama 223-8522 Japan; Keio University, Yokohama 223-8522 Japan; Keio University, Yokohama 223-8522 Japan; Keio University, Yokohama 223-8522 Japan; Keio University, Yokohama 223-8522 Japan; Keio University, Yokohama 223-8522 Japan; Keio University, Yokohama 223-8522 Japan","2015 International Conference on Field Programmable Technology (FPT)","","2015","","","272","275","A software/hardware co-design system for a Trax solver is proposed. Implementation of Trax AI is challenging due to its complicated rules, so we adopted an embedded system called Zynq (Zynq-7000 AP SoC) and introduced a High Level Synthesis (HLS) design. We also added Deep Q-Network, a machine learning algorithm, to the system for use as an evaluation function. Our solver automatically optimizes its own evaluation function through games with humans or other AIs. The implemented solver works with a 150-MHz clock on the Xilinx XC7Z020-CLG484 of a Digilent ZedBoard. A part of the Deep Q-Network job can be executed on the FPGA of the Zynq board more than 26 times faster than with ARM Coretex-A9 650-MHz software.","","","10.1109/FPT.2015.7393122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7393122","","Games;Kernel;Field programmable gate arrays;Convolution;Monte Carlo methods;Artificial intelligence;Linux","embedded systems;field programmable gate arrays;hardware-software codesign;high level synthesis;learning (artificial intelligence);logic design;system-on-chip","Trax solver;deep Q-network;software-hardware codesign system;Trax AI;embedded system;Zynq-7000 AP SoC;high level synthesis design;HLS design;machine learning algorithm;Xilinx XC7Z020-CLG484;Digilent ZedBoard;ARM Coretex-A9 software;Zynq board;frequency 150 MHz;frequency 650 MHz","","3","6","","","","","IEEE","IEEE Conferences"
"A Decision Making Method Based on Society of Mind Theory in Multi-player Imperfect Information Games","M. Wakatsuki; M. Fujimura; T. Nishino","Grad. Sch. of Inf. & Eng., Univ. of Electro-Commun., Chofu, Japan; Grad. Sch. of Inf. & Eng., Univ. of Electro-Commun., Chofu, Japan; Grad. Sch. of Inf. & Eng., Univ. of Electro-Commun., Chofu, Japan","2015 3rd International Conference on Applied Computing and Information Technology/2nd International Conference on Computational Science and Intelligence","","2015","","","67","72","We are concerned with a card game called Daihinmin, which is a multi-player imperfect information game. Using Marvin Minsky's ""Society of Mind"" theory, we attempt to model the workings of the minds of game players. The UEC Computer Daihinmin Championship is held at The University of Electro-Communications every year, to bring together competitive client programs that correspond to players of Daihinmin, and contest their strengths. In this paper, we extract the behavior of client programs from actual competition records of the computer Daihinmin, and propose a method of building a system that determines the parameters of Daihinmin agencies by machine learning.","","","10.1109/ACIT-CSI.2015.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336035","Daihinmin;Society of Mind;deep learning;multiplayer imperfect information game","Games;Computers;Machine learning;Neural networks;Informatics;Buildings;Tuning","computer games;decision making;learning (artificial intelligence)","decision making method;multiplayer imperfect information game;card game;Society-of-Mind theory;UEC Computer Daihinmin Championship;University of Electro-Communications;Daihinmin agency;machine learning","","","7","","","","","IEEE","IEEE Conferences"
"Stochastic least squares learning for deep architectures","G. Kumar; J. M. Sim; E. Y. Cheu; X. Li","NUS High School of Mathematics and Science, Singapore 129957; Exploit Technologies Pte Ltd, A*STAR (Agency for Science, Technology and Research), Singapore 138671; Rolls Royce Singapore Pte Ltd, Singapore; Institute for Infocomm Research, A*STAR, Singapore 138632","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","7","In this paper, we present a novel way of pre-training deep architectures by using the stochastic least squares autoencoder (SLSA). The SLSA is based on the combination of stochastic least squares estimation and logistic sampling. The usefulness of the stochastic least squares approach coupled with the numerical trick of constraining the logistic sampling process is highlighted in this paper. This approach was tested and benchmarked against other methods including Neural Nets (NN), Deep Belief Nets (DBN), and Stacked Denoising Autoencoder (SDAE) using the MNIST dataset. In addition, the SLSA architecture was also tested against established methods such as the Support Vector Machine (SVM), and the Naive Bayes Classifier (NB) on the Reuters-21578 and MNIST datasets. The experiments show the promise of SLSA as a pre-training step, in which stacked of SLSA yielded the lowest classification error and the highest F-measure scores on the MNIST and Reuters-21578 datasets respectively. Hence, this paper establishes the value of pre-training deep neural network, by using the SLSA.","","","10.1109/IJCNN.2015.7280502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280502","","","learning (artificial intelligence);least squares approximations;neural net architecture;sampling methods;stochastic processes","stochastic least squares learning;pretraining deep-architectures;stochastic least squares autoencoder;stochastic least squares estimation;logistic sampling process;neural nets;deep belief nets;stacked denoising autoencoder;NN;DBN;SDAE;MNIST dataset;support vector machine;SVM;naive Bayes classifier;NB;Reuters-21578 dataset;classification error;F-measure score;SLSA architecture","","","29","","","","","IEEE","IEEE Conferences"
"Pose and category recognition of highly deformable objects using deep learning","I. Mariolis; G. Peleka; A. Kargakos; S. Malassiotis","Information Technologies Institute, Centre for Research & Technology Hellas, 6th km Xarilaou-Thermi, 57001, Thessaloniki, Greece; Information Technologies Institute, Centre for Research & Technology Hellas, 6th km Xarilaou-Thermi, 57001, Thessaloniki, Greece; Information Technologies Institute, Centre for Research & Technology Hellas, 6th km Xarilaou-Thermi, 57001, Thessaloniki, Greece; Information Technologies Institute, Centre for Research & Technology Hellas, 6th km Xarilaou-Thermi, 57001, Thessaloniki, Greece","2015 International Conference on Advanced Robotics (ICAR)","","2015","","","655","662","Category and pose recognition of highly deformable objects is considered a challenging problem in computer vision and robotics. In this study, we investigate recognition and pose estimation of garments hanging from a single point, using a hierarchy of deep convolutional neural networks. The adopted framework contains two layers. The deep convolutional network of the first layer is used for classifying the garment to one of the predefined categories, whereas in the second layer a category specific deep convolutional network performs pose estimation. The method has been evaluated using both synthetic and real datasets of depth images and an actual robotic platform. Experiments demonstrate that the task at hand may be performed with sufficient accuracy, to allow application in several practical scenarios.","","","10.1109/ICAR.2015.7251526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7251526","","Clothing;Robot sensing systems;Estimation;Feature extraction;Grasping;Solid modeling","clothing;learning (artificial intelligence);neural nets;pose estimation;robot vision","category recognition;pose recognition;highly deformable objects;deep learning;computer vision;garment pose estimation;deep convolutional neural networks;depth images;robotic platform","","10","28","","","","","IEEE","IEEE Conferences"
"Discriminative concept learning network: Reveal high-level differential concepts from shallow architecture","Q. Wang; S. Young; A. Harwood; Cheng Soon Ong","Computing and Information Systems, The University of Melbourne, Parkville, VIC 3010, Australia; Computing and Information Systems, The University of Melbourne, Parkville, VIC 3010, Australia; Computing and Information Systems, The University of Melbourne, Parkville, VIC 3010, Australia; NICTA Canberra Research Lab, Tower A, 7 London Circuit, ACT 2601, Australia","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","9","A desired capability of deep learning is to understand the high-level, class-specific features via hierarchical features learning. However the training of deep architectures is costly comparing to simple shallow models. Bringing the high-level feature understanding into a simple shallow architecture remains an open question. We proposed a supervised learning algorithm, enabling binary classification along with an intrinsic ability of learning high-level discriminative concepts via a shallow neural network architecture. The physical architecture of the network has one hidden layer (also serving as the output layer) responsible for the classification and an input layer directly identifies the informative features that constitute the high-level differential concepts between the two classes. Compared to other shallow classifiers, we demonstrate its practicability in real world classification problems. We also illustrate the human-understandable, discriminative concepts learned from the two image recognition exercises. Lastly, we show how it is useful in validating the disease-associated genetic variants in human genome as a real diagnostic genomics application.","","","10.1109/IJCNN.2015.7280525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280525","","Training;Neurons;Ionosphere;Sonar;Single photon emission computed tomography","face recognition;image classification;learning (artificial intelligence);neural net architecture","high-level differential concepts;deep learning;high-level class-specific features;hierarchical feature learning;deep-architecture training;supervised learning algorithm;binary classification;intrinsic ability;high-level discriminative concept learning network;shallow neural network architecture;physical architecture;hidden layer;output layer;input layer;shallow classifiers;image recognition;disease-associated genetic variants;human genome;diagnostic genomics application","","","31","","","","","IEEE","IEEE Conferences"
"Orientation robust object detection in aerial images using deep convolutional neural network","H. Zhu; X. Chen; W. Dai; K. Fu; Q. Ye; J. Jiao","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","3735","3739","Detecting objects in aerial images is challenged by variance of object colors, aspect ratios, cluttered backgrounds, and in particular, undetermined orientations. In this paper, we propose to use Deep Convolutional Neural Network (DCNN) features from combined layers to perform orientation robust aerial object detection. We explore the inherent characteristics of DC-NN as well as relate the extracted features to the principle of disentangling feature learning. An image segmentation based approach is used to localize ROIs of various aspect ratios, and ROIs are further classified into positives or negatives using an SVM classifier trained on DCNN features. With experiments on two datasets collected from Google Earth, we demonstrate that the proposed aerial object detection approach is simple but effective.","","","10.1109/ICIP.2015.7351502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351502","Aerial Object Detection;Orientation Robust;Deep Convolutional Neural Network","Feature extraction;Object detection;Vehicles;Robustness;Pipelines;Image color analysis;Support vector machines","feature extraction;image classification;image segmentation;neural nets;object detection;support vector machines","Google Earth;SVM classifier;ROI;image segmentation;disentangling feature learning;feature extraction;orientation robust aerial object detection;DCNN features;cluttered backgrounds;aspect ratios;object colors;deep convolutional neural network;aerial images","","28","23","","","","","IEEE","IEEE Conferences"
"Deeply-Learned Feature for Age Estimation","X. Wang; R. Guo; C. Kambhamettu","Dept. of Comput. & Inf. Sci., Univ. of Delaware, Newark, DE, USA; Univ. of Tennessee, Knoxville, TN, USA; Dept. of Comput. & Inf. Sci., Univ. of Delaware, Newark, DE, USA","2015 IEEE Winter Conference on Applications of Computer Vision","","2015","","","534","541","Human age provides key demographic information. It is also considered as an important soft biometric trait for human identification or search. Compared to other pattern recognition problems (e.g., object classification, scene categorization), age estimation is much more challenging since the difference between facial images with age variations can be more subtle and the process of aging varies greatly among different individuals. In this work, we investigate deep learning techniques for age estimation based on the convolutional neural network (CNN). A new framework for age feature extraction based on the deep learning model is built. Compared to previous models based on CNN, we use feature maps obtained in different layers for our estimation work instead of using the feature obtained at the top layer. Additionally, a manifold learning algorithm is incorporated in the proposed scheme and this improves the performance significantly. Furthermore, we also evaluate different classification and regression schemes in estimating age using the deep learned aging pattern (DLA). To the best of our knowledge, this is the first time that deep learning technique is introduced and applied to solve the age estimation problem. Experimental results on two datasets show that the proposed approach is significantly better than the state-of-the-art.","","","10.1109/WACV.2015.77","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045931","","Feature extraction;Estimation;Aging;Training;Manifolds;Convolution;Neurons","biometrics (access control);feature extraction;image classification;learning (artificial intelligence);neural nets;regression analysis","age estimation problem;convolutional neural network;age feature extraction;deep learning model;CNN;feature maps;manifold learning algorithm;classification scheme;regression scheme;deep learned aging pattern;DLA;soft biometric trait","","78","51","","","","","IEEE","IEEE Conferences"
"Deep learning based language and orientation recognition in document analysis","L. Chen; S. Wang; W. Fan; J. Sun; N. Satoshi","Fujitsu Research & Development Center, Beijing, China; Fujitsu Research & Development Center, Beijing, China; Fujitsu Research & Development Center, Beijing, China; Fujitsu Research & Development Center, Beijing, China; Fujitsu Research & Development Center, Beijing, China","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","436","440","In practical applications of document understanding, if the documents have multiple languages and orientations, the conventional OCR systems can not be directly applied. This is because those OCR systems are usually designed for texts of single language and normal orientation. To solve this problem, many non-character based recognition approaches were proposed. However, the performance of those methods were not comparable with the mature OCR systems. Consequently, a better idea is to recognize the language type and orientation before the OCR is applied. Besides, the characters of different languages have very ambiguous shape, so it is very difficult to extract stable feature for the recognition. Recently, the convolutional neural networks (CNN) have achieved great success in pattern recognition tasks. Therefore, for such difficult tasks, the CNN is one of the best choice. In this paper, we first applied CNN to the recognition of the document properties. A novel sliding window voting process is proposed to reduce the network scale and fully use the information of the text line. In the experiments, our method had very high recognition rate. The results proved the advantage of the proposed method and which also can be applied to create a document understanding system with OCR systems.","","","10.1109/ICDAR.2015.7333799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333799","","Kernel;Optical character recognition software","document image processing;image recognition;neural nets;text detection","deep learning;orientation recognition;language recognition;document analysis;convolutional neural networks;CNN;document properties recognition;sliding window voting process;text line information;recognition rate;document understanding system","","4","22","","","","","IEEE","IEEE Conferences"
"Gradient-descent-based learning in memristive crossbar arrays","M. V. Nair; P. Dudek","School of Electrical and Electronic Engineering, The University of Manchester, UK; School of Electrical and Electronic Engineering, The University of Manchester, UK","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","7","This paper describes techniques to implement gradient-descent-based machine learning algorithms on crossbar arrays made of memristors or other analog memory devices. We introduce the Unregulated Step Descent (USD) algorithm, which is an approximation of the steepest descent algorithm, and discuss how it addresses various hardware implementation issues. We discuss the effect of device parameters and their variability on performance of the algorithm by using artificially generated and real-world datasets. In addition to providing insights on the effect of device parameters on learning, we illustrate how the USD algorithm partially offsets the effect of device variability. Finally, we discuss how the USD algorithm can be implemented in crossbar arrays using a simple 4-phase training scheme. The method allows parallel update of crossbar memory elements and reduces the hardware cost and complexity of the training architecture significantly.","","","10.1109/IJCNN.2015.7280658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280658","memristor;crossbar arrays;machine learning;gradient descent;back propagation;neural networks;deep learning","Approximation methods;Integrated optics;Optical devices","gradient methods;learning (artificial intelligence);memristors","gradient-descent-based learning;memristive crossbar array;machine learning algorithm;memristors;analog memory device;unregulated step descent algorithm;USD algorithm;real-world dataset;4-phase training scheme;crossbar memory element","","12","26","","","","","IEEE","IEEE Conferences"
"Improving acoustic model for English ASR System using deep neural network","Quoc Bao Nguyen; Tat Thang Vu; Chi Mai Luong","University of Information and Communication Technology, Thai Nguyen University, Vietnam; Institute of Information Technology, Vietnam Academy of Science and Technology, Vietnam; Institute of Information Technology, Vietnam Academy of Science and Technology, Vietnam","The 2015 IEEE RIVF International Conference on Computing & Communication Technologies - Research, Innovation, and Vision for Future (RIVF)","","2015","","","25","29","In this paper, a method based on deep learning is applied to improve acoustic model for English Automatic Speech Recognition (ASR) system using two main approaches of deep neural network (Hybrid and bottleneck feature). Deep neural networks systems are able to achieve significant improvements over a number of last year system. The experiments are carried out on the dataset containing speeches on Technology, Entertainment, and Design (TED) Talks. The results show that applying Deep neural network decrease the relative error rate by 33% compared to the MFCC baseline system.","","","10.1109/RIVF.2015.7049869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7049869","Deep Bottleneck Features;Hybrid HMM/GMM;Automatic Speech Recognition","Hidden Markov models;Acoustics;Neural networks;Speech;Decoding;Training;Speech recognition","Gaussian processes;hidden Markov models;learning (artificial intelligence);mixture models;natural language processing;neural nets;speech recognition","acoustic model;English ASR System;deep neural network;deep learning;English automatic speech recognition system;hybrid feature;bottleneck feature;Technology Entertainment and Design Talks;TED Talks;MFCC baseline system;HMM;hidden Markov models;GMM;Gaussian mixture model","","","17","","","","","IEEE","IEEE Conferences"
"Predicting Students Performance in Educational Data Mining","B. Guo; R. Zhang; G. Xu; C. Shi; L. Yang","Sch. of Comput., Hubei Univ. of Educ., Wuhan, China; Sch. of Comput., Hubei Univ. of Educ., Wuhan, China; Sch. of Comput., Hubei Univ. of Educ., Wuhan, China; Sch. of Comput., Hubei Univ. of Educ., Wuhan, China; Sch. of Comput., Hubei Univ. of Educ., Wuhan, China","2015 International Symposium on Educational Technology (ISET)","","2015","","","125","128","Predicting student academic performance has been an important research topic in Educational Data Mining (EDM) which uses machine learning and data mining techniques to explore data from educational settings. However measuring academic performance of students is challenging since students academic performance hinges on diverse factors. The interrelationship between variables and factors for predicting performance participate in complicated nonlinear ways. Traditional data mining and machine learning techniques may not be applied directly to these types of data and problems. In this study we develop a classification model to predict student performance using Deep Learning which automatically learns multiple levels of representation. We pre-train hidden layers of features layerwisely using an unsupervised learning algorithm sparse auto-encoder from unlabeled data, and then use supervised training for fine-tuning the parameters. We train model on a relatively large real world students dataset, and the experimental results show the effectiveness of the proposed method which can be applied into academic pre-warning mechanism.","","","10.1109/ISET.2015.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439649","","Graphics processing units;Training;Data mining;Neurons;Support vector machines;Machine learning","data mining;educational administrative data processing;pattern classification;unsupervised learning","student academic performance prediction;educational data mining;machine learning techniques;educational settings;classification model;deep learning;automatical learning;feature hidden layer pretraining;unsupervised learning algorithm;sparse autoencoder;unlabeled data;supervised training;parameter fine-tuning;academic prewarning mechanism;EDM","","16","14","","","","","IEEE","IEEE Conferences"
"Weakly-and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation","G. Papandreou; L. Chen; K. P. Murphy; A. L. Yuille","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1742","1750","Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at https://bitbucket.org/deeplab/deeplab-public.","","","10.1109/ICCV.2015.203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410560","","Image segmentation;Training;Semantics;Benchmark testing;Training data;Convolutional codes;Google","expectation-maximisation algorithm;feedforward neural nets;image segmentation;learning (artificial intelligence)","weakly-supervised learning;semisupervised learning;deep convolutional neural network;semantic image segmentation model training;DCNN;pixel-level annotation;weakly annotated training data;strongly labeled image;weakly labeled image;expectation-maximization method;EM method;PASCAL VOC 2012 image segmentation benchmark","","162","42","","","","","IEEE","IEEE Conferences"
"Deep learning of tissue fate features in acute ischemic stroke","N. Stier; N. Vincent; D. Liebeskind; F. Scalzo","Neurovascular Imaging Research Core, Department of Neurology, Univerisity of California, Los Angeles (UCLA), USA; Neurovascular Imaging Research Core, Department of Neurology, Univerisity of California, Los Angeles (UCLA), USA; Neurovascular Imaging Research Core, Department of Neurology, Univerisity of California, Los Angeles (UCLA), USA; Neurovascular Imaging Research Core, Department of Neurology, Univerisity of California, Los Angeles (UCLA), USA","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","1316","1321","In acute ischemic stroke treatment, prediction of tissue survival outcome plays a fundamental role in the clinical decision-making process, as it can be used to assess the balance of risk vs. possible benefit when considering endovascular clot-retrieval intervention. For the first time, we construct a deep learning model of tissue fate based on randomly sampled local patches from the hypoperfusion (Tmax) feature observed in MRI immediately after symptom onset. We evaluate the model with respect to the ground truth established by an expert neurologist four days after intervention. Experiments on 19 acute stroke patients evaluated the accuracy of the model in predicting tissue fate. Results show the superiority of the proposed regional learning framework versus a single-voxel-based regression model.","","","10.1109/BIBM.2015.7359869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359869","","Biomedical imaging;Convolution;Predictive models","biological tissues;biomedical MRI;decision making;image fusion;learning (artificial intelligence);medical image processing;prediction theory","deep learning;tissue fate features;acute ischemic stroke treatment;tissue survival outcome prediction;clinical decision-making process;endovascular clot-retrieval intervention;hypoperfusion feature;MRI;tissue fate prediction;regional learning framework","","16","32","","","","","IEEE","IEEE Conferences"
"Predictive data analytics and machine learning enabling metrology and process control for advanced node IC fabrication","N. Rana; Y. Zhang; D. Wall; B. Dirahoui","IBM Semiconductor Research & Development Center, Hopewell Junction, NY-12533, USA; IBM Semiconductor Research & Development Center, Hopewell Junction, NY-12533, USA; IBM Semiconductor Research & Development Center, Hopewell Junction, NY-12533, USA; IBM Semiconductor Research & Development Center, Hopewell Junction, NY-12533, USA","2015 26th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC)","","2015","","","313","319","Processor technology is going through multiple changes in terms of patterning techniques (multipatterning, EUV and DSA), device architectures (FinFET, nanowire, graphene) and patterning scale (few nanometers). These changes require tighter controls on processes and measurements to achieve the required device performance, and challenge the metrology and process control in terms of capability and quality. Predictive metrology and analytics offer Multivariate data with non-linear trends and complex correlations generally cannot be described well by mathematical models but can be relatively easily learned by computing machines and used to predict or extrapolate. In this paper we present the application of machine learning and analytics to accurately predict the electrical performance of deep trenches and metal lines. Machine learning models can be used in process control where, for example, the electrical test results are predicted early in the processing flow invoking appropriate actionable decisions. It is demonstrated that metal line resistance can be modeled directly by the raw reflectance spectra obtained using scatterometry tool. This obviates the need to make complex geometrical models to measure the CDs and then establishing the correlation of CDs to resistance. It is shown that dimensional parameters such as height and CD can be derived from the predicted electrical measurements. Such information can be used in feedforward or feedback flow to optimize, control or monitor processes in fab. Results show improved correlation of neural network model predicted deep trench capacitance to the measured capacitance compared to the capacitance predicted by multivariate linear regression model that is currently in use. This paper presents the concept of predictive metrology with the use of machine learning and predictive analytics for CD and electrical test predictions. Predictive metrology can be used in conjunction with hybrid metrology to enable APC and novel metrology pathways in gap areas in the advanced semiconductor research, development and manufacturing.","","","10.1109/ASMC.2015.7164502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164502","PM: Predictive Metrology (PM);Hybrid Metrology (HM);Optical Critical Dimension Metrology (OCD);Partial Least Square regression (PLS);Principal Components Analysis (PCA);Critical Dimension Scanning Electron Microscopy (CD-SEM);Critical Dimension Atomic Force Microscopy (CD-AFM);Model Based Infrared Reflectometry (MBIR);Machine Learning (ML);Neural Network (NN);Electrical CD (ECD);Deep Trench Capacitance and Metal Line Resistance","Predictive models;Capacitance;Neural networks;Metrology;Semiconductor device modeling;Resistance;Data models","learning (artificial intelligence);process control;regression analysis","predictive data analytics;machine learning;process control;advanced node IC fabrication;multipatterning;EUV;DSA;FinFET;nanowire;graphene;patterning scale;scatterometry tool;neural network model;deep trench capacitance;multivariate linear regression model","","5","7","","","","","IEEE","IEEE Conferences"
"A novel method with a deep network and directional edges for automatic detection of a fetal head","S. Nie; J. Yu; P. Chen; J. Zhang; Y. Wang","Department of Electronic Engineering, Fudan University, Shanghai, China; Department of Electronic Engineering, Fudan University, Shanghai, China; Ultrasound Department, Shanghai First Maternity and Infant Hospital, Tongji University, Shanghai, China; Department of Electronic Engineering, Fudan University, Shanghai, China; Department of Electronic Engineering, Fudan University, Shanghai, China","2015 23rd European Signal Processing Conference (EUSIPCO)","","2015","","","654","658","In this paper, we propose a novel method for the automatic detection of fetal head in 2D ultrasound images. Fetal head detection has been a challenging task, as the ultrasound images usually have poor quality, the structures contained in the images are complex, and the gray scale distribution is highly variable. Our approach is based on a deep belief network and a modified circle detection method. The whole process can be divided into two steps: first, a deep learning architecture is applied to search the whole image and determine the result patch that contains the entire fetal head; second, a modified circle detection method is used along with Hough transform to detect the position and size of the fetal head. In order to validate our method, experiments are performed on both synthetic data and clinic ultrasound data. A good performance of the proposed method is shown in the paper.","","","10.1109/EUSIPCO.2015.7362464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362464","Fetal head;deep learning;circle detection","Head;Image edge detection;Ultrasonic imaging;Magnetic heads;Training;Europe;Signal processing","belief networks;Hough transforms;medical signal detection;ultrasonic imaging","deep network;directional edges;automatic detection;2D ultrasound images;fetal head detection;gray scale distribution;deep belief network;modified circle detection;deep learning architecture;Hough transform;synthetic data;clinic ultrasound data","","2","19","","","","","IEEE","IEEE Conferences"
"A Simplified Cerebellar Model with Priority-based Delayed Eligibility Trace Learning for Motor Control","V. A. Shim; C. S. N. Ranjit; B. Tian; M. Yuan; H. Tang","Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore","IEEE Transactions on Autonomous Mental Development","","2015","7","1","26","38","The study of cerebellum has resulted in a common agreement that it is implicated in motor learning for movement coordination. Learning governed by error signal through synaptic eligibility traces has been proposed to be a learning mechanism in cerebellum. In this paper, we extend this idea and suggest a simplified and improved cerebellar model with priority-based delayed eligibility trace learning rule (S-CDE) that enables a mobile robot to freely and smoothly navigate in an environment. S-CDE is constructed in a brain-based device which mimics the anatomy, physiology, and dynamics of cerebellum. The input signal in terms of depth information generated from a simulated laser sensor is encoded as neuronal region activity for velocity and turn rate control. A priority-based delayed eligibility trace learning rule is proposed to maximize the usage of input signals for learning in synapses on Purkinje cell and cells in the deep cerebellar nuclei of cerebellum. Error signal generation and input signal conversion algorithms for turn rate and velocity are designed to facilitate training in an environment containing turns of varying curvatures. S-CDE is tested on a simulated mobile robot which had to randomly navigate maps of Singapore and Hong Kong expressways.","","","10.1109/TAMD.2014.2377093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6995999","Brain-based devices;cerebellum;error signals;motor control;motor learning;synaptic eligibility trace","Brain modeling;Robot sensing systems;Delays;Computational modeling;Plastics;Navigation","brain;cellular biophysics;learning (artificial intelligence);medical computing;medical robotics;mobile robots;neurophysiology","simplified cerebellar model;priority-based delayed eligibility trace learning;motor control;cerebellum;motor learning;error signal;synaptic eligibility traces;priority-based delayed eligibility trace learning rule;mobile robot;brain-based device;simulated laser sensor;neuronal region activity;velocity control;synapse learning;Purkinje cell;deep cerebellar nuclei;error signal generation;input signal conversion algorithms;simulated mobile robot;randomly navigate maps","","4","43","","","","","IEEE","IEEE Journals"
"Deep Neural Decision Forests","P. Kontschieder; M. Fiterau; A. Criminisi; S. R. Bulò","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1467","1475","We present Deep Neural Decision Forests - a novel approach that unifies classification trees with the representation learning functionality known from deep convolutional networks, by training them in an end-to-end manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find on-par or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain Top5-Errors of only 7.84%/6.38% on ImageNet validation data when integrating our forests in a single-crop, single/seven model GoogLeNet architecture, respectively. Thus, even without any form of training data set augmentation we are improving on the 6.67% error obtained by the best GoogLeNet architecture (7 models, 144 crops).","","","10.1109/ICCV.2015.172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410529","","Vegetation;Routing;Decision trees;Optimization;Training;Stochastic processes;Computer vision","computer vision;convolution;decision trees;image representation;learning (artificial intelligence);neural nets;optimisation","deep neural decision forest;classification tree;representation learning functionality;deep convolutional network;parameter optimization;machine learning;computer vision","","67","38","","","","","IEEE","IEEE Conferences"
"Deep neural networks for audio scene recognition","Y. Petetin; C. Laroche; A. Mayoue","CEA, LIST, Gif-sur-Yvette, F-91191, France; CEA, LIST, Gif-sur-Yvette, F-91191, France; CEA, LIST, Gif-sur-Yvette, F-91191, France","2015 23rd European Signal Processing Conference (EUSIPCO)","","2015","","","125","129","These last years, artificial neural networks (ANN) have known a renewed interest since efficient training procedures have emerged to learn the so called deep neural networks (DNN), i.e. ANN with at least two hidden layers. In the same time, the computational auditory scene recognition (CASR) problem which consists in estimating the environment around a device from the received audio signal has been investigated. Most of works which deal with the CASR problem have tried to ind well-adapted features for this problem. However, these features are generally combined with a classical classi-ier. In this paper, we introduce DNN in the CASR ield and we show that such networks can provide promising results and perform better than standard classiiers when the same features are used.","","","10.1109/EUSIPCO.2015.7362358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362358","Deep neural networks;deep belief networks;audio scene recognition","Artificial neural networks;Training;Mel frequency cepstral coefficient;Context;Europe;Signal processing","audio signal processing;belief networks;learning (artificial intelligence);neural nets;signal classification","deep neural networks;computational auditory scene recognition problem;artificial neural networks;ANN;training procedures;CASR problem;deep belief networks","","18","19","","","","","IEEE","IEEE Conferences"
"Learning shape priors for object segmentation via neural networks","S. Safar; M. Yang","University of California at Merced; University of California at Merced","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","1835","1839","We present a joint algorithm for object segmentation that integrates both global shape and local edge information in a deep learning framework. The proposed architecture uses convolutional layers to extract image features, followed by a fully connected section to represent shapes specific to a given object class. This preliminary mask is further refined by matching segmentation mask patches to local features. These processing steps facilitate learning the shape priors effectively with a feedforward pass rather than complex inference methods. Furthermore, our novel convolutional refinement stage presents a convincing alternative to Conditional Random Fields, with promising results on multiple datasets.","","","10.1109/ICIP.2015.7351118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351118","Object segmentation;shape priors;convolutional neural networks","Shape;Feature extraction;Training;Image segmentation;Visualization;Object segmentation;Neural networks","convolution;feature extraction;image matching;image segmentation;learning (artificial intelligence);neural nets","shape prior learning;object segmentation;neural network;joint algorithm;global shape;local edge information;deep learning framework;convolutional layers;image feature extraction;segmentation mask patch matching;local features;feedforward pass;convolutional refinement stage;conditional random field","","3","11","","","","","IEEE","IEEE Conferences"
"Action recognition with trajectory-pooled deep-convolutional descriptors","L. Wang; Y. Qiao; X. Tang","Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Shenzhen key lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","4305","4314","Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMD-B51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features [31] and deep-learned features [24]. Our method also achieves superior performance to the state of the art on these datasets.","","","10.1109/CVPR.2015.7299059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299059","","Trajectory;Feature extraction;Videos;Optical imaging;Spatiotemporal phenomena;Visualization;Three-dimensional displays","convolution;learning (artificial intelligence);video signal processing","action recognition;trajectory-pooled deep-convolutional descriptors;TDD;visual features;human action understanding;video representation;hand-crafted features;deep-learned features;deep architectures;discriminative convolutional feature maps;spatiotemporal normalization;channel normalization;HMD-B51 dataset;UCF101 dataset","","433","42","","","","","IEEE","IEEE Conferences"
"Deep Networks for Image Super-Resolution with Sparse Prior","Z. Wang; D. Liu; J. Yang; W. Han; T. Huang","Beckman Inst., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Beckman Inst., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Snapchat, Venice, CA, USA; Beckman Inst., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Beckman Inst., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","370","378","Deep learning techniques have been successfully applied in many areas of computer vision, including low-level image restoration problems. For image super-resolution, several models based on deep neural networks have been recently proposed and attained superior performance that overshadows all previous handcrafted models. The question then arises whether large-capacity and data-driven models have become the dominant solution to the ill-posed super-resolution problem. In this paper, we argue that domain expertise represented by the conventional sparse coding model is still valuable, and it can be combined with the key ingredients of deep learning to achieve further improved results. We show that a sparse coding model particularly designed for super-resolution can be incarnated as a neural network, and trained in a cascaded structure from end to end. The interpretation of the network based on sparse coding leads to much more efficient and effective training, as well as a reduced model size. Our model is evaluated on a wide range of images, and shows clear advantage over existing state-of-the-art methods in terms of both restoration accuracy and human subjective quality.","","","10.1109/ICCV.2015.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410407","","Encoding;Training;Image coding;Machine learning;Neural networks;Dictionaries;Neurons","computer vision;image resolution;learning (artificial intelligence);neural nets","image superresolution;deep learning techniques;computer vision;low-level image restoration problems;deep neural networks;data-driven models;sparse coding model;restoration accuracy;human subjective quality","","225","38","","","","","IEEE","IEEE Conferences"
"Image Classification Using Generative Neuro Evolution for Deep Learning","P. Verbancsics; J. Harguess","Space & Naval Warfare Syst. Center - Pacific, San Diego, CA, USA; Space & Naval Warfare Syst. Center - Pacific, San Diego, CA, USA","2015 IEEE Winter Conference on Applications of Computer Vision","","2015","","","488","493","Research into deep learning has demonstrated performance competitive with humans on some visual tasks, however, these systems have been primarily trained through supervised and unsupervised learning algorithms. Alternatively, research is showing that evolution may have a significant role in the development of visual systems. Thus neuroevolution for deep learning is investigated in this paper. In particular, the Hypercube-based Neuro Evolution of Augmenting Topologies is a NE approach that can effectively learn large neural structures by training an indirect encoding that compresses the artificial neural network (ANN) weight pattern as a function of geometry. The methodologies are tested on a traditional image classification task as well as one tailored to overhead satellite imagery. The results show that Hyper NEAT struggles with performing image classification by itself, but can be effective in training a feature extractor that other ML approaches can learn from. Thus Neuro Evolution combined with other ML methods provides an intriguing area of research that can replicate the processes in nature.","","","10.1109/WACV.2015.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045925","","Artificial neural networks;Training;Feature extraction;Substrates;Accuracy;Biological neural networks;Encoding","feature extraction;image classification;learning (artificial intelligence);neural nets","image classification;generative neuro evolution;deep learning;hypercube-based neuro evolution;augmenting topologies;artificial neural network;ANN weight pattern;overhead satellite imagery;Hyper NEAT;feature extractor;ML methods","","15","30","","","","","IEEE","IEEE Conferences"
"Multi-Layer and Recursive Neural Networks for Metagenomic Classification","G. Ditzler; R. Polikar; G. Rosen","Department of Electrical & Computer Engineering, Drexel University, Philadelphia; Department of Electrical & Computer Engineering, Rowan University, Glassboro; Department of Electrical & Computer Engineering, Drexel University, Philadelphia","IEEE Transactions on NanoBioscience","","2015","14","6","608","616","Recent advances in machine learning, specifically in deep learning with neural networks, has made a profound impact on fields such as natural language processing, image classification, and language modeling; however, feasibility and potential benefits of the approaches to metagenomic data analysis has been largely under-explored. Deep learning exploits many layers of learning nonlinear feature representations, typically in an unsupervised fashion, and recent results have shown outstanding generalization performance on previously unseen data. Furthermore, some deep learning methods can also represent the structure in a data set. Consequently, deep learning and neural networks may prove to be an appropriate approach for metagenomic data. To determine whether such approaches are indeed appropriate for metagenomics, we experiment with two deep learning methods: i) a deep belief network, and ii) a recursive neural network, the latter of which provides a tree representing the structure of the data. We compare these approaches to the standard multi-layer perceptron, which has been well-established in the machine learning community as a powerful prediction algorithm, though its presence is largely missing in metagenomics literature. We find that traditional neural networks can be quite powerful classifiers on metagenomic data compared to baseline methods, such as random forests. On the other hand, while the deep learning approaches did not result in improvements to the classification accuracy, they do provide the ability to learn hierarchical representations of a data set that standard classification methods do not allow. Our goal in this effort is not to determine the best algorithm in terms accuracy-as that depends on the specific application-but rather to highlight the benefits and drawbacks of each of the approach we discuss and provide insight on how they can be improved for predictive metagenomic analysis.","","","10.1109/TNB.2015.2461219","Department of Energy; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219432","Comparative metagenomics;metagenomics;microbiome;neural networks","Neural networks;Machine learning;Feature extraction;Nanobioscience;Training;Vegetation;Organisms","DNA;genomics;image classification;image representation;learning (artificial intelligence);medical image processing;molecular biophysics;multilayer perceptrons;natural language processing;neurophysiology","multilayer-recursive neural networks;metagenomic classification;machine learning;neural networks;natural language processing;image classification;language modeling;metagenomic data analysis;nonlinear feature representations;unsupervised fashion;generalization performance;deep learning methods;deep belief network;data structure;multilayer perceptron;machine learning community;prediction algorithm;metagenomic literature;classification accuracy;hierarchical representations;predictive metagenomic analysis","Algorithms;Metagenomics;Microbiota;Neural Networks (Computer)","34","44","","","","","IEEE","IEEE Journals"
"Vowel duration measurement using deep neural networks","Y. Adi; J. Keshet; M. Goldrick","Dept. of Computer Science, Bar-Ilan University, Ramat-Gan, Israel; Dept. of Computer Science, Bar-Ilan University, Ramat-Gan, Israel; Dept. of Linguistics, Northwestern University, Evanston, IL, USA","2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)","","2015","","","1","6","Vowel durations are most often utilized in studies addressing specific issues in phonetics. Thus far this has been hampered by a reliance on subjective, labor-intensive manual annotation. Our goal is to build an algorithm for automatic accurate measurement of vowel duration, where the input to the algorithm is a speech segment contains one vowel preceded and followed by consonants (CVC). Our algorithm is based on a deep neural network trained at the frame level on manually annotated data from a phonetic study. Specifically, we try two deep-network architectures: convolutional neural network (CNN), and deep belief network (DBN), and compare their accuracy to an HMM-based forced aligner. Results suggest that CNN is better than DBN, and both CNN and HMM-based forced aligner are comparable in their results, but neither of them yielded the same predictions as models fit to manually annotated data.","","","10.1109/MLSP.2015.7324331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324331","vowel duration measurement;convolution neural networks;deep belief networks;hidden Markov models;forced alignment","Hidden Markov models;Speech;Manuals;Context;Data models;Predictive models;Neural networks","hidden Markov models;neural nets;speech processing","vowel duration measurement;deep neural network;phonetics;labor-intensive manual annotation;automatic accurate measurement;speech segment;CVC;manually annotated data;phonetic study;deep-network architecture;convolutional neural network;deep belief network;DBN;HMM-based forced aligner;CNN -based forced aligner","","5","19","","","","","IEEE","IEEE Conferences"
"Deep Learning of Mouth Shapes for Sign Language","O. Koller; H. Ney; R. Bowden","Human Language Technol. & Pattern Recog., RWTH Aachen Univ., Aachen, Germany; Human Language Technol. & Pattern Recog., RWTH Aachen Univ., Aachen, Germany; Centre for Vision Speech & Signal Process., Univ. of Surrey, Guildford, UK","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","477","483","This paper deals with robust modelling of mouth shapes in the context of sign language recognition using deep convolutional neural networks. Sign language mouth shapes are difficult to annotate and thus hardly any publicly available annotations exist. As such, this work exploits related information sources as weak supervision. Humans mainly look at the face during sign language communication, where mouth shapes play an important role and constitute natural patterns with large variability. However, most scientific research on sign language recognition still disregards the face. Hardly any works explicitly focus on mouth shapes. This paper presents our advances in the field of sign language recognition. We contribute in following areas: We present a scheme to learn a convolutional neural network in a weakly supervised fashion without explicit frame labels. We propose a way to incorporate neural network classifier outputs into a HMM approach. Finally, we achieve a significant improvement in classification performance of mouth shapes over the current state of the art.","","","10.1109/ICCVW.2015.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406418","","Mouth;Shape;Hidden Markov models;Assistive technology;Gesture recognition;Training;Neural networks","feedforward neural nets;hidden Markov models;image classification;learning (artificial intelligence);sign language recognition","classification performance;HMM approach;neural network classifier outputs;natural patterns;sign language communication;deep convolutional neural network;sign language recognition;mouth shapes;deep learning","","15","29","","","","","IEEE","IEEE Conferences"
"Glaucoma detection based on deep convolutional neural network","X. Chen; Y. Xu; D. W. Kee Wong; T. Y. Wong; J. Liu","Institute for Infocomm Research, Agency for Science, Technology and Research, 138632, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, 138632, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, 138632, Singapore; Singapore National Eye Centre, 168751, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, 138632, Singapore","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","715","718","Glaucoma is a chronic and irreversible eye disease, which leads to deterioration in vision and quality of life. In this paper, we develop a deep learning (DL) architecture with convolutional neural network for automated glaucoma diagnosis. Deep learning systems, such as convolutional neural networks (CNNs), can infer a hierarchical representation of images to discriminate between glaucoma and non-glaucoma patterns for diagnostic decisions. The proposed DL architecture contains six learned layers: four convolutional layers and two fully-connected layers. Dropout and data augmentation strategies are adopted to further boost the performance of glaucoma diagnosis. Extensive experiments are performed on the ORIGA and SCES datasets. The results show area under curve (AUC) of the receiver operating characteristic curve in glaucoma detection at 0.831 and 0.887 in the two databases, much better than state-of-the-art algorithms. The method could be used for glaucoma detection.","","","10.1109/EMBC.2015.7318462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318462","","Optical imaging;Biomedical optical imaging;Neural networks;Machine learning;Diseases;Training;Prediction algorithms","diseases;image representation;learning (artificial intelligence);medical image processing;neural nets;object detection","glaucoma detection;deep convolutional neural network;irreversible eye disease;deep learning architecture;DL architecture;automated glaucoma diagnosis;CNNs;hierarchical image representation;nonglaucoma patterns;glaucoma patterns;data augmentation strategy;dropout augmentation strategy;SCES datasets;ORIGA datasets;area under curve;AUC;receiver operating characteristic curve","Algorithms;Glaucoma;Humans;Neural Networks (Computer);Quality of Life;ROC Curve","38","18","","","","","IEEE","IEEE Conferences"
"A New Multivariate Approach for Prognostics Based on Extreme Learning Machine and Fuzzy Clustering","K. Javed; R. Gouriveau; N. Zerhouni","Automatic Control and Micro-Mechatronic Systems Department, FEMTO-ST Institute, Besançon, France; Automatic Control and Micro-Mechatronic Systems Department, FEMTO-ST Institute, Besançon, France; Automatic Control and Micro-Mechatronic Systems Department, FEMTO-ST Institute, Besançon, France","IEEE Transactions on Cybernetics","","2015","45","12","2626","2639","Prognostics is a core process of prognostics and health management (PHM) discipline, that estimates the remaining useful life (RUL) of a degrading machinery to optimize its service delivery potential. However, machinery operates in a dynamic environment and the acquired condition monitoring data are usually noisy and subject to a high level of uncertainty/unpredictability, which complicates prognostics. The complexity further increases, when there is absence of prior knowledge about ground truth (or failure definition). For such issues, data-driven prognostics can be a valuable solution without deep understanding of system physics. This paper contributes a new data-driven prognostics approach namely, an “enhanced multivariate degradation modeling,” which enables modeling degrading states of machinery without assuming a homogeneous pattern. In brief, a predictability scheme is introduced to reduce the dimensionality of the data. Following that, the proposed prognostics model is achieved by integrating two new algorithms namely, the summation wavelet-extreme learning machine and subtractive-maximum entropy fuzzy clustering to show evolution of machine degradation by simultaneous predictions and discrete state estimation. The prognostics model is equipped with a dynamic failure threshold assignment procedure to estimate RUL in a realistic manner. To validate the proposition, a case study is performed on turbofan engines data from PHM challenge 2008 (NASA), and results are compared with recent publications.","","","10.1109/TCYB.2014.2378056","Laboratory of Excellence ACTION funded by the French Government through the program ″Investments for the future‴ managed by the National Agency for Research (ANR-11-LABX-01-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7021915","Data-driven;extreme learning machine (ELM);fuzzy clustering;prognostics;remaining useful life (RUL);Data-driven;extreme learning machine (ELM);fuzzy clustering;prognostics;remaining useful life (RUL)","Predictive models;Machinery;Degradation;Prognostics and health management;Clustering algorithms;Monitoring;Data models","condition monitoring;jet engines;learning (artificial intelligence);machinery;mechanical engineering computing;remaining life assessment;statistical analysis;wavelet transforms","multivariate approach;health management;remaining useful life;RUL;degrading machinery;data-driven prognostics;enhanced multivariate degradation modeling;predictability scheme;summation wavelet-extreme learning machine;subtractive-maximum entropy fuzzy clustering;machine degradation;discrete state estimation;dynamic failure threshold assignment;turbofan engine","Algorithms;Cluster Analysis;Engineering;Fuzzy Logic;Machine Learning;Models, Theoretical;Multivariate Analysis","46","44","","","","","IEEE","IEEE Journals"
"Convolutional Neural Network with Corrupted Input","Q. Xu; L. Zhang","Scholl of Mech., Shandong Univ. (Weihai), Weihai, China; Scholl of Mech., Shandong Univ. (Weihai), Weihai, China","2015 7th International Conference on Intelligent Human-Machine Systems and Cybernetics","","2015","2","","77","80","Convolutional neural network is a model of deep neural network, which uses the convolution and sub sampling to realize feature extraction. However, the network is easy to over fitting. In this paper, the denoising method is used to corrupt the sample and force the network to learn the better representations to overcome the over fitting problem. The generalization of the convolutional neural network will be enhanced by this. The simulations exhibit the learning process.","","","10.1109/IHMSC.2015.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334922","Deep learning;CNN;Denoising;Feature learning;Subsampling","Noise reduction;Biological neural networks;Kernel;Mathematical model;Convolution;Visualization;Yttrium","feature extraction;generalisation (artificial intelligence);image denoising;learning (artificial intelligence);neural nets","deep neural network;feature extraction;denoising method;overfitting problem;convolutional neural network generalization;corrupted input","","","7","","","","","IEEE","IEEE Conferences"
"A deep reinforcement learning approach to character segmentation of license plate images","F. Abtahi; Z. Zhu; A. M. Burry","The Graduate Center, CUNY, New York, NY 10016, USA; The City College and Graduate Center, CUNY, New York, NY 10031, USA; Xerox PARC, Webster, NY 14580, USA","2015 14th IAPR International Conference on Machine Vision Applications (MVA)","","2015","","","539","542","Automated license plate recognition (ALPR) has been applied to identify vehicles by their license plates and is critical in several important transportation applications. In order to achieve the recognition accuracy levels typically required in the market, it is necessary to obtain properly segmented characters. A standard method, projection-based segmentation, is challenged by substantial variation across the plate in the regions surrounding the characters. In this paper a reinforcement learning (RL) method is adapted to create a segmentation agent that can find appropriate segmentation paths that avoid characters, traversing from the top to the bottom of a cropped license plate image. Then a hybrid approach is proposed, leveraging the speed and simplicity of the projection-based segmentation technique along with the power of the RL method. The results of our experiments show significant improvement over the histogram projection currently used for character segmentation.","","","10.1109/MVA.2015.7153249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153249","","Image segmentation;Licenses;Training;Character recognition;Learning (artificial intelligence);Object segmentation;Image recognition","character recognition;image segmentation;learning (artificial intelligence)","character segmentation;deep reinforcement learning approach;automated license plate recognition;recognition accuracy levels;properly segmented characters;projection-based segmentation;cropped license plate image;histogram projection","","7","13","","","","","IEEE","IEEE Conferences"
"Learning deep representations for ground-to-aerial geolocalization","T. Lin; Yin Cui; S. Belongie; J. Hays","Cornell Tech, USA; Cornell Tech, USA; Cornell Tech, USA; Brown University, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","5007","5015","The recent availability of geo-tagged images and rich geospatial data has inspired a number of algorithms for image based geolocalization. Most approaches predict the location of a query image by matching to ground-level images with known locations (e.g., street-view data). However, most of the Earth does not have ground-level reference photos available. Fortunately, more complete coverage is provided by oblique aerial or “bird's eye” imagery. In this work, we localize a ground-level query image by matching it to a reference database of aerial imagery. We use publicly available data to build a dataset of 78K aligned crossview image pairs. The primary challenge for this task is that traditional computer vision approaches cannot handle the wide baseline and appearance variation of these cross-view pairs. We use our dataset to learn a feature representation in which matching views are near one another and mismatched views are far apart. Our proposed approach, Where-CNN, is inspired by deep learning success in face verification and achieves significant improvements over traditional hand-crafted features and existing deep features learned from other large-scale databases. We show the effectiveness of Where-CNN in finding matches between street view and aerial view imagery and demonstrate the ability of our learned features to generalize to novel locations.","","","10.1109/CVPR.2015.7299135","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299135","","Geology;Yttrium;Training","computer vision;convolution;geophysical image processing;image matching;image representation;learning (artificial intelligence);neural nets","deep representation learning;ground-to-aerial image based geolocalization;geo-tagged images;geospatial data;ground-level image matching;bird's eye imagery;aerial view imagery;ground-level query image;aligned crossview image pairs;computer vision;feature representation;Where-CNN;convolutional neural network","","82","30","","","","","IEEE","IEEE Conferences"
"Stacked PCA Network (SPCANet): An effective deep learning for face recognition","L. Tian; C. Fan; Y. Ming; Y. Jin","Beijing Key Laboratory of Work Safety Intelligent Monitoring, School of Electronic Engineering, Beijing University of Posts and Telecommunications, 100876, China; Beijing Key Laboratory of Work Safety Intelligent Monitoring, School of Electronic Engineering, Beijing University of Posts and Telecommunications, 100876, China; Beijing Key Laboratory of Work Safety Intelligent Monitoring, School of Electronic Engineering, Beijing University of Posts and Telecommunications, 100876, China; School of Computer & Information Technology, Beijing Jiaotong University, 100044, China","2015 IEEE International Conference on Digital Signal Processing (DSP)","","2015","","","1039","1043","High-level features can represent the semantics of the original data and it is a plausible way to avoid the problem of hand-crafted features for face recognition. This paper proposes an effective deep learning framework by stacking multiple output features that learned through each stage of the Convolutional Neural Network (CNN). Different from the traditional deep learning network, we use Principal Component Analysis (PCA) to get the filter kernels of convolutional layer, which is name as Stacked PCA Network (SPCANet). Our SPCANet model follows the basic architecture of the CNN, which comprises three layers in each stage: convolutional filter layer, nonlinear processing layer and feature pooling layer. Firstly, in the convolutional filter layer of our model, PCA instead of stochastic gradient descent (SGD) is employed to learn filter kernels, and the output of all cascaded convolutional filter layers is used as the input of nonlinear processing layer. Secondly, the following nonlinear processing layer is also simplified. We use hashing method for nonlinear processing. Thirdly, the block based histograms instead of max-pooling technique are employed in the feature pooling layer. In the last output layer, the output of each stage is stacked together as one final feature output of our model. Extensive experiments conducted on many different face recognition scenarios demonstrate the effectiveness of our proposed approach.","","","10.1109/ICDSP.2015.7252036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7252036","Convolutional Neural Network;Stacked PCA Network;Face Recognition;Malicious Occlusion","Robustness;Databases;Histograms","convolution;face recognition;feature extraction;image filtering;learning (artificial intelligence);neural nets;principal component analysis","effective deep learning;face recognition;stacked PCA network;convolutional neural network;CNN;principal component analysis;filter kernel;SPCANet model;nonlinear processing layer;feature pooling layer;all cascaded convolutional filter layer;hashing method;block based histogram","","11","27","","","","","IEEE","IEEE Conferences"
"Improved language identification using deep bottleneck network","Y. Song; R. Cui; X. Hong; I. Mcloughlin; J. Shi; L. Dai","National Engineering Laboratory of Speech and Language Information Processing, USTC, China; National Engineering Laboratory of Speech and Language Information Processing, USTC, China; National Engineering Laboratory of Speech and Language Information Processing, USTC, China; National Engineering Laboratory of Speech and Language Information Processing, USTC, China; Anhui Post and Telecommunication College, China; National Engineering Laboratory of Speech and Language Information Processing, USTC, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4200","4204","Effective representation plays an important role in automatic spoken language identification (LID). Recently, several representations that employ a pre-trained deep neural network (DNN) as the front-end feature extractor, have achieved state-of-the-art performance. However the performance is still far from satisfactory for dialect and short-duration utterance identification tasks, due to the deficiency of existing representations. To address this issue, this paper proposes the improved representations to exploit the information extracted from different layers of the DNN structure. This is conceptually motivated by regarding the DNN as a bridge between low-level acoustic input and high-level phonetic output features. Specifically, we employ deep bottleneck network (DBN), a DNN with an internal bottleneck layer acting as a feature extractor. We extract representations from two layers of this single network, i.e. DBN-TopLayer and DBN-MidLayer. Evaluations on the NIST LRE2009 dataset, as well as the more specific dialect recognition task, show that each representation can achieve an incremental performance gain. Furthermore, a simple fusion of the representations is shown to exceed current state-of-the-art performance.","","","10.1109/ICASSP.2015.7178762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178762","Language Identification;Deep Neural Network;Bottleneck Feature;Representation Learning","Feature extraction;NIST;Speech;Acoustics;Training;Kernel;Speech processing","feature extraction;learning (artificial intelligence);speech processing;speech recognition","incremental performance gain;dialect recognition task;NIST LRE2009 dataset;DBN-MidLayer;DBN-TopLayer;internal bottleneck layer;high-level phonetic output feature;low-level acoustic input feature;DNN structure;short-duration utterance identification task;dialect identification task;front-end feature extractor;pretrained deep neural network;LID;automatic spoken language identification;deep bottleneck network;improved language identification","","4","21","","","","","IEEE","IEEE Conferences"
"Deep Colorization","Z. Cheng; Q. Yang; B. Sheng","NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","415","423","This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.","","","10.1109/ICCV.2015.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410412","","Gray-scale;Image color analysis;Feature extraction;Neural networks;Neurons;Machine learning;Databases","image colour analysis;image filtering;image matching;learning (artificial intelligence)","deep colorization;grayscale image;colorful image;human-labelled color;colorful reference images;high-quality fully-automatic colorization method;patch matching technique;extremely large-scale reference database;patch matching noise;deep learning techniques;joint bilateral filtering based post-processing","","92","27","","","","","IEEE","IEEE Conferences"
"Learning a Deep Convolutional Network for Light-Field Image Super-Resolution","Y. Yoon; H. Jeon; D. Yoo; J. Lee; I. S. Kweon","Robot. & Comput. Vision Lab., KAIST, Daejeon, South Korea; Robot. & Comput. Vision Lab., KAIST, Daejeon, South Korea; Robot. & Comput. Vision Lab., KAIST, Daejeon, South Korea; Robot. & Comput. Vision Lab., KAIST, Daejeon, South Korea; Robot. & Comput. Vision Lab., KAIST, Daejeon, South Korea","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","57","65","Commercial Light-Field cameras provide spatial and angular information, but its limited resolution becomes an important problem in practical use. In this paper, we present a novel method for Light-Field image super-resolution (SR) via a deep convolutional neural network. Rather than the conventional optimization framework, we adopt a datadriven learning method to simultaneously up-sample the angular resolution as well as the spatial resolution of a Light-Field image. We first augment the spatial resolution of each sub-aperture image to enhance details by a spatial SR network. Then, novel views between the sub-aperture images are generated by an angular super-resolution network. These networks are trained independently but finally finetuned via end-to-end training. The proposed method shows the state-of-the-art performance on HCI synthetic dataset, and is further evaluated by challenging real-world applications including refocusing and depth map estimation.","","","10.1109/ICCVW.2015.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406366","","Spatial resolution;Cameras;Lenses;Image restoration;Neural networks","cameras;image enhancement;image resolution;image sampling;learning (artificial intelligence);neural nets","deep-convolutional neural-network learning;light-field image super-resolution;light-field cameras;spatial information;angular information;light-field image SR;data-driven learning method;angular resolution up-sampling;spatial resolution augmentation;subaperture image;spatial SR network;angular super-resolution network;network training;network fine-tuning;HCI synthetic dataset;image refocusing;depth map estimation","","51","33","","","","","IEEE","IEEE Conferences"
"Brain tumor grading based on Neural Networks and Convolutional Neural Networks","Y. Pan; W. Huang; Z. Lin; W. Zhu; J. Zhou; J. Wong; Z. Ding","School of EEE, Nanyang Technological University, Singapore; Institute for Infocomm Research, 1 Fusionopolis Way, #21-01 Connexis, Singapore 138632; School of EEE, Nanyang Technological University, Singapore; School of EEE, Nanyang Technological University, Singapore; Institute for Infocomm Research, 1 Fusionopolis Way, #21-01 Connexis, Singapore 138632; Department of Diagnostic Imaging, National University Hospital, Singapore; Department of Radiology, Zhejiang Provincial People's Hospital, Hangzhou, China","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","699","702","This paper studies brain tumor grading using multiphase MRI images and compares the results with various configurations of deep learning structure and baseline Neural Networks. The MRI images are used directly into the learning machine, with some combination operations between multiphase MRIs. Compared to other researches, which involve additional effort to design and choose feature sets, the approach used in this paper leverages the learning capability of deep learning machine. We present the grading performance on the testing data measured by the sensitivity and specificity. The results show a maximum improvement of 18% on grading performance of Convolutional Neural Networks based on sensitivity and specificity compared to Neural Networks. We also visualize the kernels trained in different layers and display some self-learned features obtained from Convolutional Neural Networks.","","","10.1109/EMBC.2015.7318458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318458","","Tumors;Kernel;Training;Biological neural networks;Artificial neural networks;Sensitivity and specificity;Image segmentation","biomedical MRI;brain;image classification;learning (artificial intelligence);medical disorders;medical image processing;neurophysiology;tumours","self-learned features;testing data;grading performance;deep learning machine;baseline neural networks;deep learning structure;multiphase MRI imaging;convolutional neural networks;brain tumor grading","Brain;Brain Neoplasms;Humans;Machine Learning;Neoplasm Grading;Neural Networks (Computer)","30","8","","","","","IEEE","IEEE Conferences"
"Relaxing from Vocabulary: Robust Weakly-Supervised Deep Learning for Vocabulary-Free Image Tagging","J. Fu; Y. Wu; T. Mei; J. Wang; H. Lu; Y. Rui","Nat. Lab. of Pattern Recognition, Inst. of Autom., Beijing, China; Univ. of Sci. & Technol. of China, Hefei, China; Microsoft Res., Beijing, China; Nat. Lab. of Pattern Recognition, Inst. of Autom., Beijing, China; Nat. Lab. of Pattern Recognition, Inst. of Autom., Beijing, China; Microsoft Res., Beijing, China","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1985","1993","The development of deep learning has empowered machines with comparable capability of recognizing limited image categories to human beings. However, most existing approaches heavily rely on human-curated training data, which hinders the scalability to large and unlabeled vocabularies in image tagging. In this paper, we propose a weakly-supervised deep learning model which can be trained from the readily available Web images to relax the dependence on human labors and scale up to arbitrary tags (categories). Specifically, based on the assumption that features of true samples in a category tend to be similar and noises tend to be variant, we embed the feature map of the last deep layer into a new affinity representation, and further minimize the discrepancy between the affinity representation and its low-rank approximation. The discrepancy is finally transformed into the objective function to give relevance feedback to back propagation. Experiments show that we can achieve a performance gain of 14.0% in terms of a semantic-based relevance metric in image tagging with 63,043 tags from the WordNet, against the typical deep model trained on the ImageNet 1,000 vocabulary set.","","","10.1109/ICCV.2015.230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410587","","Machine learning;Noise measurement;Training data;Vocabulary;Training;Robustness;Tagging","approximation theory;backpropagation;feature extraction;image representation;image retrieval;relevance feedback","vocabulary-free image tagging;image categories;weakly-supervised deep learning model;Web images;human labors;feature map;affinity representation;low-rank approximation;objective function;relevance feedback;back propagation;semantic-based relevance metric;WordNet;ImageNet","","18","27","","","","","IEEE","IEEE Conferences"
"Learning Representative Deep Features for Image Set Analysis","Z. Wu; Y. Huang; L. Wang","Australian Centre for Visual Technologies, University of Adelaide, Adelaide, Australia; National Laboratory of Pattern Recognition, CAS Center for Excellence in Brain Science and Intelligence Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, CAS Center for Excellence in Brain Science and Intelligence Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Multimedia","","2015","17","11","1960","1968","This paper proposes to learn features from sets of labeled raw images. With this method, the problem of over-fitting can be effectively suppressed, so that deep CNNs can be trained from scratch with a small number of training data, i.e., 420 labeled albums with about 30 000 photos. This method can effectively deal with sets of images, no matter if the sets bear temporal structures. A typical approach to sequential image analysis usually leverages motions between adjacent frames, while the proposed method focuses on capturing the co-occurrences and frequencies of features. Nevertheless, our method outperforms previous best performers in terms of album classification, and achieves comparable or even better performances in terms of gait based human identification. These results demonstrate its effectiveness and good adaptivity to different kinds of set data.","","","10.1109/TMM.2015.2477681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7254176","Album classification;deep learning;gait recognition;image set","Feature extraction;Hidden Markov models;Convolution;Training data;Videos;Training;Data models","image classification;learning (artificial intelligence);neural nets","image set analysis;labeled raw images;CNN;temporal structures;sequential image analysis;album classification;gait based human identification;convolutional neural network","","26","43","","","","","IEEE","IEEE Journals"
"A Hardware-Efficient Sigmoid Function With Adjustable Precision for a Neural Network System","C. Tsai; Y. Chih; W. H. Wong; C. Lee","Dept. of Electron. Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan; Dept. of Electron. Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan; Dept. of Electron. Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan; Dept. of Electron. Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan","IEEE Transactions on Circuits and Systems II: Express Briefs","","2015","62","11","1073","1077","A hardware-efficient sigmoid function calculator with adjustable precision for neural network and deep-learning applications is proposed in this brief. By adopting the bit-plane format of the input and output values, the computational latency of the processing time can be dynamically reduced according to the user configuration. To reduce the hardware cost, the coefficients used to calculate the sigmoid value can be shared for multiple calculators without any structural hazard. In addition, the restricted constraint is applied in the coefficients' training stage to further simplify the computation in the calculation stage with a negligible quality loss. A test module is designed for the proposal and operated at 300 MHz to achieve 75 million sigmoid calculations per second. Implemented in 90-nm CMOS technology, the core of the calculator costs 1663 gates, and a 1-kb globally shared memory is used to store the coefficients.","","","10.1109/TCSII.2015.2456531","Ministry of Science and Technology, Taiwan; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7159035","Sigmoid Function;Hardware-Efficient;Adjustable Precision;Neural Network;Deep Learning;Adjustable precision;deep learning;hardware efficient;neural network;sigmoid function","Calculators;Neurons;Biological neural networks;Hardware;Training;Table lookup","CMOS integrated circuits;electronic calculators;learning (artificial intelligence);neural nets","hardware-efficient sigmoid function calculator;neural network;deep-learning applications;bit-plane format;computational latency;processing time;hardware cost;sigmoid value;restricted constraint;training stage;CMOS technology;globally shared memory;frequency 300 MHz;size 90 nm","","15","18","","","","","IEEE","IEEE Journals"
"Unsupervised Joint Feature Learning and Encoding for RGB-D Scene Labeling","A. Wang; J. Lu; J. Cai; G. Wang; T. Cham","School of Computer Engineering, Nanyang Technological University, Singapore; Department of Automation, Tsinghua University, Beijing, China; School of Computer Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Image Processing","","2015","24","11","4459","4473","Most existing approaches for RGB-D indoor scene labeling employ hand-crafted features for each modality independently and combine them in a heuristic manner. There has been some attempt on directly learning features from raw RGB-D data, but the performance is not satisfactory. In this paper, we propose an unsupervised joint feature learning and encoding (JFLE) framework for RGB-D scene labeling. The main novelty of our learning framework lies in the joint optimization of feature learning and feature encoding in a coherent way, which significantly boosts the performance. By stacking basic learning structure, higher level features are derived and combined with lower level features for better representing RGB-D data. Moreover, to explore the nonlinear intrinsic characteristic of data, we further propose a more general joint deep feature learning and encoding (JDFLE) framework that introduces the nonlinear mapping into JFLE. The experimental results on the benchmark NYU depth dataset show that our approaches achieve competitive performance, compared with the state-of-the-art methods, while our methods do not need complex feature handcrafting and feature combination and can be easily applied to other data sets.","","","10.1109/TIP.2015.2465133","Singapore National Research Foundation under its International Research Centre at the Singapore Funding Initiative, and administered by the Interactive Digital Medi Programme Office; Ministry of Education (MOE) Tier 1; Singapore MOE Tier 2; Agency for Science, Technology and Research through the Science and Engineering Research Council, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7185416","RGB-D scene labeling,;unsupervised feature learning;joint feature learning and encoding;multi-modality;RGB-D scene labeling;unsupervised feature learning;joint feature learning and encoding;multi-modality","Labeling;Image coding;Joints;Feature extraction;Optimization;Three-dimensional displays;Encoding","image coding;image colour analysis;unsupervised learning","unsupervised joint feature learning and encoding;RGB-D scene labeling;nonlinear intrinsic characteristic;nonlinear mapping;JFLE framework","","16","43","","","","","IEEE","IEEE Journals"
"Pedestrian detection with a Large-Field-Of-View deep network","A. Angelova; A. Krizhevsky; V. Vanhoucke","Google Research, Mountain view, CA, USA; Google, Mountain view, CA, USA; Google Research, Mountain view, CA, USA","2015 IEEE International Conference on Robotics and Automation (ICRA)","","2015","","","704","711","Pedestrian detection is of crucial importance to autonomous driving applications. Methods based on deep learning have shown significant improvements in accuracy, which makes them particularly suitable for applications, such as pedestrian detection, where reducing the miss rate is very important. Although they are accurate, their runtime has been at best in seconds per image, which makes them not practical for onboard applications. We present a Large-Field-Of-View (LFOV) deep network for pedestrian detection, that can achieve high accuracy and is designed to make deep networks work faster for detection problems. The idea of the proposed Large-Field-of-View deep network is to learn to make classification decisions simultaneously and accurately at multiple locations. The LFOV network processes larger image areas at much faster speeds than typical deep networks have been able to, and can intrinsically reuse computations. Our pedestrian detection solution, which is a combination of a LFOV network and a standard deep network, works at 280 ms per image on GPU and achieves 35.85 average miss rate on the Caltech Pedestrian Detection Benchmark.","","","10.1109/ICRA.2015.7139256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139256","","Computer architecture;Microprocessors;Standards;Context;Training;Graphics processing units;Proposals","learning (artificial intelligence);object detection;pedestrians","large-field-of-view deep network;deep learning;LFOV deep network;Caltech pedestrian detection benchmark","","30","43","","","","","IEEE","IEEE Conferences"
"Deep convolutional activation features for large scale Brain Tumor histopathology image classification and segmentation","Y. Xu; Z. Jia; Y. Ai; F. Zhang; M. Lai; E. I. Chang","Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beihang University, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Department of Pathology, School of Medicine, Zhejiang University, China; Microsoft Research, Beijing, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","947","951","We propose a simple, efficient and effective method using deep convolutional activation features (CNNs) to achieve stat- of-the-art classification and segmentation for the MICCAI 2014 Brain Tumor Digital Pathology Challenge. Common traits of such medical image challenges are characterized by large image dimensions (up to the gigabyte size of an image), a limited amount of training data, and significant clinical feature representations. To tackle these challenges, we transfer the features extracted from CNNs trained with a very large general image database to the medical image challenge. In this paper, we used CNN activations trained by ImageNet to extract features (4096 neurons, 13.3% active). In addition, feature selection, feature pooling, and data augmentation are used in our work. Our system obtained 97.5% accuracy on classification and 84% accuracy on segmentation, demonstrating a significant performance gain over other participating teams.","","","10.1109/ICASSP.2015.7178109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178109","deep convolutional activation features;deep learning;feature learning;segmentation;classification","Image segmentation;Feature extraction;Biomedical imaging;Tumors;Support vector machines;Training data;Training","brain;feature extraction;image classification;image segmentation;medical image processing;tumours","deep convolutional activation features;brain tumor histopathology;image classification;image segmentation;MICCAI 2014 Brain Tumor Digital Pathology Challenge;image dimensions;CNN activations;ImageNet;features extraction","","31","23","","","","","IEEE","IEEE Conferences"
"Robust deep neural network for wind speed prediction","M. Khodayar; M. Teshnehlab","Department of Electrical Engineering, K. N. Toosi University of Technology, Tehran, Iran; Department of Electrical Engineering, K. N. Toosi University of Technology, Tehran, Iran","2015 4th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS)","","2015","","","1","5","Wind speed prediction is a basic requirement of wind energy generation with large generation capacity for large-scale wind power penetration. The intermittency and stochastic quality of wind speed leads to a big challenge for high penetration of wind power in electricity systems due to error-prone wind speed prediction methods. There are many artificial neural network (ANN) approaches proposed in the recent literature in order to tackle this problem. However, hand engineering features and applying shallow architectures can lead to poor prediction performance in these methodologies. Deep neural networks are ANNs with great generalization capability that can automatically extract meaningful features from the data with as less prior knowledge as possible. In this paper we propose a stacked auto-encoder (SAE) neural network for ultra-short-term and short-term wind speed prediction. To the best of our knowledge this is the first paper that applies deep learning on wind speed prediction of a wind site. Moreover, a rough regression layer is applied at the top of this model in order to deal with uncertainty factors existing in the wind speed data. Experimental results show significant improvement compared to other ANN methods that applied shallow architectures in the literature and the traditional SAE.","","","10.1109/CFIS.2015.7391664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391664","Wind speed;Artificial neural network;Deep neural network;Stacked auto-encoder;Uncertainty","Wind speed;Artificial neural networks;Training;Uncertainty;Neurons;Data models","feature extraction;generalisation (artificial intelligence);learning (artificial intelligence);neural nets;power engineering computing;regression analysis;wind power","robust deep neural network;wind energy generation;large-scale wind power penetration;wind speed intermittency;wind speed stochastic quality;electricity;artificial neural network;ANN approach;generalization capability;data feature extraction;stacked autoencoder neural network;SAE neural network;ultrashort-term wind speed prediction;deep learning;wind site;rough regression layer;uncertainty factors;wind speed data","","11","12","","","","","IEEE","IEEE Conferences"
"Robust feature learning by improved auto-encoder from non-Gaussian noised images","D. Zhao; B. Guo; J. Wu; W. Ning; Y. Yan","School of Aerospace Science and Technology, Xidian University, Xi'an, Shaanxi 710071, China; School of Aerospace Science and Technology, Xidian University, Xi'an, Shaanxi 710071, China; School of Aerospace Science and Technology, Xidian University, Xi'an, Shaanxi 710071, China; School of Aerospace Science and Technology, Xidian University, Xi'an, Shaanxi 710071, China; School of Aerospace Science and Technology, Xidian University, Xi'an, Shaanxi 710071, China","2015 IEEE International Conference on Imaging Systems and Techniques (IST)","","2015","","","1","5","Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Net-works(DBN) and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and languages datasets. These learning algorithms aim to find good representations for data, which can be used for classification, reconstruction, visualization and so on. Despite the progress, most existing algorithms would be fragile to non-Gaussian noises and outliers due to the criterion of mean square error(MSE) and cross entropy(CE). In this paper, we propose a robust auto-encoder called correntropy-based contractive auto-encoder(C-CAE) to learn robust features from data with non-Gaussian noises and outliers. The maximum correntropy criterion(MCC) is adopted as reconstruction cost function and a well chosen penalty term is added to the reconstruction cost function. By replacing cross entropy with MCC, the proposed method can learn robust features from the data containing non-Gaussian noises and outliers. The penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. By adding the penalty term, the antinoise ability of the proposed method is improved. The proposed method is evaluated using the MNIST benchmark dataset. Experimental results show that, compared with the traditional auto-encoders, the proposed method learns robust features, improves classification accuracy and reduces the reconstruction error, which demonstrates that the proposed method is capable of learning robust features on noisy data.","","","10.1109/IST.2015.7294537","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7294537","feature learning;stacked autoencoder;correntropy","Noise;Robustness;Image reconstruction;Cost function;Feature extraction;Noise reduction;Entropy","data structures;feature extraction;Gaussian noise;image classification;image denoising;image reconstruction;Jacobian matrices;learning (artificial intelligence);maximum entropy methods;mean square error methods","robust feature learning algorithms;improved autoencoder;nonGaussian noised images;deep belief networks;DBN;language datasets;vision datasets;data representations;mean square error;MSE;cross entropy;correntropy-based contractive auto-encoder;maximum correntropy criterion;MCC;reconstruction cost function;Frobenius norm;Jacobian matrix;antinoise ability;MNIST benchmark dataset;reconstruction error;robust feature learning","","","26","","","","","IEEE","IEEE Conferences"
"Unsupervised Learning of Spatiotemporally Coherent Metrics","R. Goroshin; J. Bruna; J. Tompson; D. Eigen; Y. LeCun","Courant Inst., NYU, New York, NY, USA; Univ. of California, Berkeley, Berkeley, CA, USA; NA; Courant Inst., NYU, New York, NY, USA; Courant Inst., NYU, New York, NY, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","4086","4093","Current state-of-the-art classification and detection algorithms train deep convolutional networks using labeled data. In this work we study unsupervised feature learning with convolutional networks in the context of temporally coherent unlabeled data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity priors. We establish a connection between slow feature learning and metric learning. Using this connection we define ""temporal coherence"" -- a criterion which can be used to set hyper-parameters in a principled and automated manner. In a transfer learning experiment, we show that the resulting encoder can be used to define a more semantically coherent metric without the use of labels.","","","10.1109/ICCV.2015.465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410822","","Feature extraction;Measurement;Dictionaries;Unsupervised learning;Training;Convolution;Video sequences","convolution;unsupervised learning;video signal processing","spatiotemporally coherent metrics;detection algorithms;classification algorithms;deep convolutional networks;labeled data;unsupervised feature learning;video data;video frames;convolutional pooling auto-encoder;metric learning;hyperparameters;transfer learning","","26","22","","","","","IEEE","IEEE Conferences"
"From Facial Parts Responses to Face Detection: A Deep Learning Approach","S. Yang; P. Luo; C. Loy; X. Tang","Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Shenzhen Key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","3676","3684","In this paper, we propose a novel deep convolutional network (DCN) that achieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically, our method achieves a high recall rate of 90.99% on the challenging FDDB benchmark, outperforming the state-of-the-art method [23] by a large margin of 2.91%. Importantly, we consider finding faces from a new perspective through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variation, which are the main difficulty and bottleneck of most existing face detection approaches. We show that despite the use of DCN, our network can achieve practical runtime speed.","","","10.1109/ICCV.2015.419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410776","","Face;Face detection;Proposals;Hair;Mouth;Detectors;Nose","face recognition;learning (artificial intelligence)","facial parts responses;face detection;deep learning approach;novel deep convolutional network;DCN;FDDB;PASCAL Face;AFW;scoring facial parts;spatial structure;spatial arrangement;unconstrained pose variation","","187","41","","","","","IEEE","IEEE Conferences"
"Sound source separation for robot audition using deep learning","K. Noda; N. Hashimoto; K. Nakadai; T. Ogata","Graduate School of Fundamental Science and Engineering, Waseda University, Tokyo 169-8555, Japan; Graduate School of Fundamental Science and Engineering, Waseda University, Tokyo 169-8555, Japan; Honda Research Institute Japan Co., Ltd., Saitama 351-0114, Japan; Graduate School of Fundamental Science and Engineering, Waseda University, Tokyo 169-8555, Japan","2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)","","2015","","","389","394","Noise robust speech recognition is crucial for effective human-machine interaction in real-world environments. Sound source separation (SSS) is one of the most widely used approaches for addressing noise robust speech recognition by extracting a target speaker's speech signal while suppressing simultaneous unintended signals. However, conventional SSS algorithms, such as independent component analysis or nonlinear principal component analysis, are limited in modeling complex projections with scalability. Moreover, conventional systems required designing an independent subsystem for noise reduction (NR) in addition to the SSS. To overcome these issues, we propose a deep neural network (DNN) framework for modeling the separation function (SF) of an SSS system. By training a DNN to predict clean sound features of a target sound from corresponding multichannel deteriorated sound feature inputs, we enable the DNN to model the SF for extracting the target sound without prior knowledge regarding the acoustic properties of the surrounding environment. Moreover, the same DNN is trained to function simultaneously as a NR filter. Our proposed SSS system is evaluated using an isolated word recognition task and a large vocabulary continuous speech recognition task when either nondirectional or directional noise is accumulated in the target speech. Our evaluation results demonstrate that DNN performs noticeably better than the baseline approach, especially when directional noise is accumulated with a low signal-to-noise ratio.","","","10.1109/HUMANOIDS.2015.7363579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363579","","Speech recognition;Feature extraction;Microphones;Speech;Training;Robots;Neural networks","hearing;human-robot interaction;independent component analysis;learning (artificial intelligence);neural nets;principal component analysis;robot programming;source separation;speech recognition","signal-to-noise ratio;NR filter;separation function;DNN;deep neural network;noise reduction;nonlinear principal component analysis;independent component analysis;SSS;human-machine interaction;noise robust speech recognition;deep learning;robot audition;sound source separation","","3","25","","","","","IEEE","IEEE Conferences"
"Harvesting Discriminative Meta Objects with Deep CNN Features for Scene Classification","R. Wu; B. Wang; W. Wang; Y. Yu","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1287","1295","Recent work on scene classification still makes use of generic CNN features in a rudimentary manner. In this paper, we present a novel pipeline built upon deep CNN features to harvest discriminative visual objects and parts for scene classification. We first use a region proposal technique to generate a set of high-quality patches potentially containing objects, and apply a pre-trained CNN to extract generic deep features from these patches. Then we perform both unsupervised and weakly supervised learning to screen these patches and discover discriminative ones representing category-specific objects and parts. We further apply discriminative clustering enhanced with local CNN fine-tuning to aggregate similar objects and parts into groups, called meta objects. A scene image representation is constructed by pooling the feature response maps of all the learned meta objects at multiple spatial scales. We have confirmed that the scene image representation obtained using this new pipeline is capable of delivering state-of-the-art performance on two popular scene benchmark datasets, MIT Indoor 67 [22] and Sun397 [31].","","","10.1109/ICCV.2015.152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410509","","Pipelines;Feature extraction;Proposals;Visualization;Computers;Supervised learning;Aggregates","convolution;feature extraction;image classification;image representation;neural nets;unsupervised learning","feature response map;scene image representation;unsupervised learning;feature extraction;scene classification;deep CNN;convolutional neural network;metaobject learning","","34","34","","","","","IEEE","IEEE Conferences"
"Detector discovery in the wild: Joint multiple instance and representation learning","J. Hoffman; D. Pathak; T. Darrell; K. Saenko","UC Berkeley, USA; UC Berkeley, USA; UC Berkeley, USA; UMass Lowell, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","2883","2891","We develop methods for detector learning which exploit joint training over both weak (image-level) and strong (bounding box) labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. Previous methods for weak-label learning often learn detector models independently using latent variable optimization, but fail to share deep representation knowledge across classes and usually require strong initialization. Other previous methods transfer deep representations from domains with strong labels to those with only weak labels, but do not optimize over individual latent boxes, and thus may miss specific salient structures for a particular category. We propose a model that subsumes these previous approaches, and simultaneously trains a representation and detectors for categories with either weak or strong labels present. We provide a novel formulation of a joint multiple instance learning method that includes examples from classification-style data when available, and also performs domain transfer learning to improve the underlying detector representation. Our model outperforms known methods on ImageNet-200 detection with weak labels.","","","10.1109/CVPR.2015.7298906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298906","","Detectors;Optimization;Training;Feature extraction;Joints;Visualization;Training data","image representation;learning (artificial intelligence);optimisation","detector discovery;joint multiple instance learning;representation learning;detector learning;strongly-labeled auxiliary tasks;weak-label learning;latent variable optimization;deep representation knowledge;domain transfer learning;detector representation;ImageNet-200 detection","","18","40","","","","","IEEE","IEEE Conferences"
"Using Deep Learning for Energy Expenditure Estimation with wearable sensors","J. Zhu; A. Pande; P. Mohapatra; J. J. Han","Department of Computer Science, University of California at Davis, 95616, United States; Department of Computer Science, University of California at Davis, 95616, United States; Department of Computer Science, University of California at Davis, 95616, United States; Dept. Physical Medicine and Rehabilitation, UC Davis School of Medicine, Sacramento, USA","2015 17th International Conference on E-health Networking, Application & Services (HealthCom)","","2015","","","501","506","Energy Expenditure (EE) Estimation is an important step in tracking personal activity and preventing chronic diseases such as obesity, diabetes and cardiovascular diseases. Accurate and online EE estimation using small wearable sensors is a difficult task, primarily because most existing schemes work offline or using heuristics. In this work, we focus on accurate EE estimation for tracking ambulatory activities (walking, standing, climbing upstairs or downstairs) of individuals wearing mobile sensors. We use Convolution Neural Networks (CNNs) to automatically detect important features from data collected from triaxial accelerometer and heart rate sensors. Using CNNs, we find a significant improvement in EE estimation compared to other state-of-the-art models. We compare our results against state-of-the-art Activity-Specific Linear Regression as well as Artificial Neural Networks (ANN) based models. Using a universal CNN model, we obtain an overall low Root Mean Square Error (RMSE) of 1.12 which is 30% and 35% lower than existing models. The results were calibrated against a COSMED K4b2 indirect calorimeter readings.","","","10.1109/HealthCom.2015.7454554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7454554","","Feature extraction;Neural networks;Machine learning;Accelerometers;Sensors;Heart rate;Estimation","accelerometers;biomedical equipment;calibration;cardiology;feature extraction;learning (artificial intelligence);medical computing;neural nets;sensors","calibration;COSMED K4b2 indirect calorimeter readings;root mean square error;heart rate sensors;triaxial accelerometer;feature detection;convolution neural networks;mobile sensors;standing;walking;ambulatory activity tracking;chronic diseases;personal activity tracking;wearable sensors;energy expenditure estimation;deep learning","","10","22","","","","","IEEE","IEEE Conferences"
"Weed seeds classification based on PCANet deep learning baseline","W. Xinshao; C. Cheng","College of Information Engineering, Northwest A&F University, Yangling, 712100; College of Information Engineering, Northwest A&F University, Yangling, 712100","2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","","2015","","","408","415","There are a large number of various kinds of weeds in agriculture. Weeds have a great impact on the development of agricultural production and agricultural economy. The reproduction and spread of weeds are mainly dependent on weed seeds. So we want to find an efficient algorithm with robust and accurate classification of weed seeds, which has an important practical value and economic significance. PCA Network has been applied to image feature extraction and achieved fantastic effects. Here we propose a variant of PCA Network and apply it to the classification of weed seeds in agriculture. The difference between the proposed method and the original PCA Network lies in that we get L1 families of orthogonal filters rather than one family of orthogonal filters in the second stage of PCA. After using the PCA Network variant method to extract image features, we conduct an experiment to test the classification accuracy of weed seeds. The data sets contain 91 types of weed seeds. In the experiment we use a large margin classifier to construct a linear classifier, which is based on affine hulls. Next we use the features extracted from the test samples to examine the recognition accuracy rate. Experiment results show that the PCA Network variant method obtains good classification results and improves the recognition accuracy. In the data sets composed of 91 types of weed seeds, 45 arrives at 100% recognition rate of classification, and 90.96% average recognition rate. At the same time, our algorithm shows relatively higher robustness than the recognition of weed seeds images does. This algorithm improves the classification accuracy rate of weed seeds greatly and thus can be applied to the agricultural production practice.","","","10.1109/APSIPA.2015.7415304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415304","","Feature extraction;Principal component analysis;Machine learning;Filter banks;Training;Histograms;Classification algorithms","agriculture;image classification;learning (artificial intelligence)","weed seeds classification;PCANet deep learning baseline;agricultural production;agricultural economy;PCA network;principal component analysis;orthogonal filters;large margin classifier;linear classifier;affine hulls","","4","24","","","","","IEEE","IEEE Conferences"
"Deep learning classifier for fall detection based on IR distance sensor data","S. Jankowski; Z. Szymański; U. Dziomin; P. Mazurek; J. Wagner","Warsaw University of Technology, Nowowiejska 15/19, 00-665 Warszawa, Poland; Warsaw University of Technology, Nowowiejska 15/19, 00-665 Warszawa, Poland; Warsaw University of Technology, Nowowiejska 15/19, 00-665 Warszawa, Poland; Warsaw University of Technology, Nowowiejska 15/19, 00-665 Warszawa, Poland; Warsaw University of Technology, Nowowiejska 15/19, 00-665 Warszawa, Poland","2015 IEEE 8th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)","","2015","2","","723","727","The goal of research is the fall detection in elderly residents based on infra red depth sensor measurements. Our attention is focused on statistical properties as generalization. The effectiveness of discriminative statistical classifiers (multilayer perceptron) is improved by addition of feature selection block by Gram-Schmidt orthogonalization, which determines the ranking of the features, and NPCA block, which transforms the raw data into a nonlinear manifold and reduces the dimensionality of the data. Performance of our system measured in terms of sensitivity is 92% and precision is 93%, which means it can be used for real life applications.","","","10.1109/IDAACS.2015.7341398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7341398","fall detection;infra red distance sensor;NPCA;feature selection;Gramm-Schmidt orthogonalization","Principal component analysis;Training;Machine learning;Sensitivity;Acceleration;Neural networks;Manifolds","assisted living;feature selection;geriatrics;infrared detectors;learning (artificial intelligence);multilayer perceptrons;object detection;signal classification;spatial variables measurement;statistical analysis","deep learning classifier;fall detection;IR distance sensor data;elderly residents;infrared depth sensor measurements;statistical properties;discriminative statistical classifiers;multilayer perceptron;feature selection block;Gram-Schmidt orthogonalization;feature ranking;NPCA block;nonlinear manifold;data dimensionality reduction","","3","19","","","","","IEEE","IEEE Conferences"
"Fine-grained bird species recognition via hierarchical subset learning","Z. Ge; C. McCool; C. Sanderson; A. Bewley; Z. Chen; P. Corke","Australian Centre for Robotic Vision, Brisbane, Australia; Australian Centre for Robotic Vision, Brisbane, Australia; NICTA, PO Box 10522, Adelaide St, Brisbane, QLD 4001, Australia; Queensland University of Technology, Brisbane, QLD 4000, Australia; Australian Centre for Robotic Vision, Brisbane, Australia; Australian Centre for Robotic Vision, Brisbane, Australia","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","561","565","We propose a novel method to improve fine-grained bird species classification based on hierarchical subset learning. We first form a similarity tree where classes with strong visual correlations are grouped into subsets. An expert local classifier with strong discriminative power to distinguish visually similar classes is then learnt for each subset. On the challenging Caltech200-2011 bird dataset we show that using the hierarchical approach with features derived from a deep convolutional neural network leads to the average accuracy improving from 64.5% to 72.7%, a relative improvement of 12.7%.","","","10.1109/ICIP.2015.7350861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350861","fine-grained classification;subset clustering","Birds;Vegetation;Visualization;Feature extraction;Support vector machines;Neural networks;Training","image classification;learning (artificial intelligence);neural nets;trees (mathematics)","fine-grained bird species recognition;hierarchical subset learning;fine-grained bird species classification;similarity tree;deep convolutional neural network","","10","18","","","","","IEEE","IEEE Conferences"
"Detection of hyperperfusion on arterial spin labeling using deep learning","N. Vincent; N. Stier; S. Yu; D. S. Liebeskind; D. J. Wang; F. Scalzo","Neurovascular Imaging Research Core, Department of Neurology, University of California, Los Angeles (UCLA), USA; Neurovascular Imaging Research Core, Department of Neurology, University of California, Los Angeles (UCLA), USA; Neurovascular Imaging Research Core, Department of Neurology, University of California, Los Angeles (UCLA), USA; Neurovascular Imaging Research Core, Department of Neurology, University of California, Los Angeles (UCLA), USA; Neurovascular Imaging Research Core, Department of Neurology, University of California, Los Angeles (UCLA), USA; Neurovascular Imaging Research Core, Department of Neurology, University of California, Los Angeles (UCLA), USA","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","1322","1327","Hyperperfusion detected on arterial spin labeling (ASL) images acquired after acute stroke onset has been shown to correlate with development of subsequent intracerebral hemorrhage. We present in this study a quantitative hyperperfusion detection model that can provide an objective decision support for the interpretation of ASL cerebral blood flow (CBF) maps and rapidly delineate hyperperfusion regions. The detection problem is solved using Deep Learning such that the model relates ASL image patches to the corresponding label (normal or hyperperfused). Our method takes into account the regional intensity values of contralateral hemisphere during the labeling of a pixel. Each input vector is associated to a label corresponding to the presence of hyperperfusion that was manually established by a clinical researcher in Neurology. When compared to the manually established hyperperfusion, the predicted maps reached an accuracy of 97.45 ± 2.49% after crossvalidation. Pattern recognition based on deep learning can provide an accurate and objective measure of hyperperfusion on ASL CBF images and could therefore improve the detection of hemorrhagic transformation in acute stroke patients.","","","10.1109/BIBM.2015.7359870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359870","","Pattern recognition;Image recognition;Blood;Magnetic resonance imaging;Positron emission tomography;Lesions;Interpolation","biomedical engineering;diseases;haemodynamics;haemorheology;learning (artificial intelligence);pattern recognition","acute stroke patients;hemorrhagic transformation;pattern recognition;neurology;pixel labeling;contralateral hemisphere;hyperperfusion regions;ASL cerebral blood flow maps;hyperperfusion detection model;intracerebral hemorrhage;ASL images;deep learning;arterial spin labeling","","3","28","","","","","IEEE","IEEE Conferences"
"Deep learning using partitioned data vectors","B. Mitchell; H. Tosun; J. Sheppard","Department of Computer Science, Johns Hopkins University, USA; Department of Computer Science, Montana State University, USA; Department of Computer Science, Montana State University, USA","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","Deep learning is a popular field that encompasses a range of multi-layer connectionist techniques. While these techniques have achieved great success on a number of difficult computer vision problems, the representation biases that allow this success have not been thoroughly explored. In this paper, we examine the hypothesis that one strength of many deep learning algorithms is their ability to exploit spatially local statistical information. We present a formal description of how data vectors can be partitioned into sub-vectors that preserve spatially local information. As a test case, we then use statistical models to examine how much of such structure exists in the MNIST dataset. Finally, we present experimental results from training RBMs using partitioned data, and demonstrate the advantages they have over non-partitioned RBMs. Through these results, we show how the performance advantage is reliant on spatially local structure, by demonstrating the performance impact of randomly permuting the input data to destroy local structure. Overall, our results support the hypothesis that a representation bias reliant upon spatially local statistical information can improve performance, so long as this bias is a good match for the data. We also suggest statistical tools for determining a priori whether a dataset is a good match for this bias or not.","","","10.1109/IJCNN.2015.7280484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280484","","","Boltzmann machines;data handling;learning (artificial intelligence)","deep learning algorithms;partitioned data vectors;sub-vectors;statistical models;MNIST dataset;RBM training;spatially local structure;spatially local statistical information;representation bias;restricted Boltzman machine","","2","31","","","","","IEEE","IEEE Conferences"
"Deep learning based FACS Action Unit occurrence and intensity estimation","A. Gudi; H. E. Tasli; T. M. den Uyl; A. Maroulis","Vicarious Perception Technologies, Amsterdam, The Netherlands; Vicarious Perception Technologies, Amsterdam, The Netherlands; Vicarious Perception Technologies, Amsterdam, The Netherlands; Vicarious Perception Technologies, Amsterdam, The Netherlands","2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)","","2015","06","","1","5","Ground truth annotation of the occurrence and intensity of FACS Action Unit (AU) activation requires great amount of attention. The efforts towards achieving a common platform for AU evaluation have been addressed in the FG 2015 Facial Expression Recognition and Analysis challenge (FERA 2015). Participants are invited to estimate AU occurrence and intensity on a common benchmark dataset. Conventional approaches towards achieving automated methods are to train multiclass classifiers or to use regression models. In this paper, we propose a novel application of a deep convolutional neural network (CNN) to recognize AUs as part of FERA 2015 challenge. The 7 layer network is composed of 3 convolutional layers and a max-pooling layer. The final fully connected layers provide the classification output. For the selected tasks of the challenge, we have trained two different networks for the two different datasets, where one focuses on the AU occurrences and the other on both occurrences and intensities of the AUs. The occurrence and intensity of AU activation are estimated using specific neuron activations of the output layer. This way, we are able to create a single network architecture that could simultaneously be trained to produce binary and continuous classification output.","","","10.1109/FG.2015.7284873","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284873","","Face;Gold;Training;Conferences;Machine learning;Estimation;Face recognition","emotion recognition;face recognition;image classification;neural nets;regression analysis","deep learning;FACS action unit occurrence;intensity estimation;ground truth annotation;FACS AU activation;AU occurrence estimation;multiclass classifiers;regression models;deep convolutional neural network;CNN;convolutional layers;max-pooling layer;neuron activations;single network architecture;binary classification output;continuous classification output;FG 2015 Facial Expression Recognition and Analysis challenge","","49","21","","","","","IEEE","IEEE Conferences"
"Subset feature learning for fine-grained category classification","Z. Ge; C. McCool; C. Sanderson; P. Corke","Australian Centre for Robotic Vision, Brisbane, Australia; Queensland University of Technology (QUT), Brisbane, Australia; University of Queensland, Brisbane, Australia; Australian Centre for Robotic Vision, Brisbane, Australia","2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","","2015","","","46","52","Fine-grained categorisation has been a challenging problem due to small inter-class variation, large intra-class variation and low number of training images. We propose a learning system which first clusters visually similar classes and then learns deep convolutional neural network features specific to each subset. Experiments on the popular fine-grained Caltech-UCSD bird dataset show that the proposed method outperforms recent fine-grained categorisation methods under the most difficult setting: no bounding boxes are presented at test time. It achieves a mean accuracy of 77.5%, compared to the previous best performance of 73.2%. We also show that progressive transfer learning allows us to first learn domain-generic features (for bird classification) which can then be adapted to specific set of bird classes, yielding improvements in accuracy.","","","10.1109/CVPRW.2015.7301271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301271","","Birds;Accuracy;Training;Feature extraction;Australia;Learning systems;Neural networks","feature extraction;feedforward neural nets;image classification;learning (artificial intelligence)","subset feature learning;fine-grained category classification;fine-grained categorisation;small interclass variation;large intraclass variation;low training image number;deep convolutional neural network feature learning;Caltech-UCSD bird dataset;mean accuracy;domain-generic feature learning","","18","27","","","","","IEEE","IEEE Conferences"
"Supervised Image Classification Using Deep Convolutional Wavelets Network","S. Hassairi; R. Ejbali; M. Zaied","NA; NA; NA","2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)","","2015","","","265","271","This paper gives a review of the deep learning history and proposes a new approach to supervised image classification by the combination of two techniques of learning: the wavelet network and the deep learning. This new approach consists of performing the classification of one class versus all the other classes of the dataset by the reconstruction of a convolutional deep neural wavelet network. This network is obtained using a series of stacked auto-encoders and a linear classifier. The experimental test of our approach performed on ""COIL-100"" dataset demonstrates that our model is remarkably efficient for image classification compared to a known classifier.","","","10.1109/ICTAI.2015.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372145","Wavelet Network;Deep learning;Stacked AutoEncoders;Convolutional Neural Network","Biological neural networks;Machine learning;Training;Convolutional codes;Neurons;Approximation methods","image classification;wavelet neural nets","supervised image classification;deep convolutional wavelet network;one class classification;stacked auto-encoders;linear classifier;COIL-100 dataset","","8","40","","","","","IEEE","IEEE Conferences"
"Multi-layer feature extractions for image classification — Knowledge from deep CNNs","K. Ueki; T. Kobayashi","Faculty of Science and Engineering, Waseda University, Room 40-701, Waseda-machi 27, Shinjuku-ku, Tokyo, 162-0042 Japan; Faculty of Science and Engineering, Waseda University, Room 40-701, Waseda-machi 27, Shinjuku-ku, Tokyo, 162-0042 Japan","2015 International Conference on Systems, Signals and Image Processing (IWSSIP)","","2015","","","9","12","Recently, there has been considerable research into the application of deep learning to image recognition. Notably, deep convolutional neural networks (CNNs) have achieved excellent performance in a number of image classification tasks, compared with conventional methods based on techniques such as Bag-of-Features (BoF) using local descriptors. In this paper, to cultivate a better understanding of the structure of CNN, we focus on the characteristics of deep CNNs, and adapt them to SIFT+BoF-based methods to improve the classification accuracy. We introduce the multi-layer structure of CNNs into the classification pipeline of the BoF framework, and conduct experiments to confirm the effectiveness of this approach using a fine-grained visual categorization dataset. The results show that the average classification rate is improved from 52.4% to 69.8%.","","","10.1109/IWSSIP.2015.7313925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313925","Deep learning;Feature extraction;Bag-of-Features;Generic object recognition;Fine-grained visual categorization","Principal component analysis;Feature extraction;Training;Computer vision;Neural networks;Visualization;Conferences","","","","","24","","","","","IEEE","IEEE Conferences"
"Multi-attribute learning for pedestrian attribute recognition in surveillance scenarios","D. Li; X. Chen; K. Huang","CRIPAC & NLPR, CASIA; CRIPAC & NLPR, CASIA; CRIPAC & NLPR, CASIA","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","","2015","","","111","115","In real video surveillance scenarios, visual pedestrian attributes, such as gender, backpack, clothes types, are very important for pedestrian retrieval and person reidentification. Existing methods for attributes recognition have two drawbacks: (a) handcrafted features (e.g. color histograms, local binary patterns) cannot cope well with the difficulty of real video surveillance scenarios; (b) the relationship among pedestrian attributes is ignored. To address the two drawbacks, we propose two deep learning based models to recognize pedestrian attributes. On the one hand, each attribute is treated as an independent component and the deep learning based single attribute recognition model (DeepSAR) is proposed to recognize each attribute one by one. On the other hand, to exploit the relationship among attributes, the deep learning framework which recognizes multiple attributes jointly (DeepMAR) is proposed. In the DeepMAR, one attribute can contribute to the representation of other attributes. For example, the gender of woman can contribute to the representation oflong hair and wearing skirt. Experiments on recent popular pedestrian attribute datasets illustrate that our proposed models achieve the state-of-the-art results.","","","10.1109/ACPR.2015.7486476","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486476","","Surveillance;Hair;Training;Computational modeling;Pattern recognition;Visualization;Machine learning","independent component analysis;learning (artificial intelligence);pedestrians;video surveillance","multi attribute learning;pedestrian attribute recognition;real video surveillance scenarios;visual pedestrian attributes;pedestrian retrieval;person reidentification;independent component;deep learning based single attribute recognition model","","35","21","","","","","IEEE","IEEE Conferences"
"Deep and Shallow Architecture of Multilayer Neural Networks","C. Chang","Department of Applied Mathematics, National University of Kaohsiung, Kaohsiung, Taiwan","IEEE Transactions on Neural Networks and Learning Systems","","2015","26","10","2477","2486","This paper focuses on the deep and shallow architecture of multilayer neural networks (MNNs). The demonstration of whether or not an MNN can be replaced by another MNN with fewer layers is equivalent to studying the topological conjugacy of its hidden layers. This paper provides a systematic methodology to indicate when two hidden spaces are topologically conjugated. Furthermore, some criteria are presented for some specific cases.","","","10.1109/TNNLS.2014.2387439","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010967","Deep architecture;factor-like matrix;multilayer neural networks (MNNs);sofic shift;topological entropy.;Deep architecture;factor-like matrix;multilayer neural networks (MNNs);sofic shift;topological entropy","Artificial neural networks;Multi-layer neural network;Entropy;Nonhomogeneous media;Matrix decomposition;Biological neural networks;Mathematical model","multilayer perceptrons;neural net architecture;topology","deep-shallow architecture;multilayer neural networks;MNN;hidden layers;topologically conjugated hidden spaces","Algorithms;Humans;Models, Neurological;Nerve Net;Neural Networks (Computer);Neurons","24","41","","","","","IEEE","IEEE Journals"
"Multi-class learning using data driven ECOC with deep search and re-balancing","N. Japkowicz; V. Barnabe-Lortie; S. Horvatic; J. Zhou","School of Information Technology and Engineering, University of Ottawa, Ottawa, ON Canada; School of Information Technology and Engineering, University of Ottawa, Ottawa, ON Canada; Department of Computer Science, Northern Illinois University, DeKalb, IL USA; Department of Computer Science, Northern Illinois University, DeKalb, IL USA","2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)","","2015","","","1","10","Multi-class learning is an important task in Data Science. One of the ways to achieve good performance on this task is to use Error Correcting Output Codes (ECOC), which is a powerful ensemble learning method that transforms a multi-class problem into a series of binary classifiers which it uses indirectly to learn the original multi-class problem. A crucial component of ECOC is the design of the coding matrix, which determines which binary problems should be combined to achieve multi-class classification. There are two general ways of designing the coding matrix. One is rooted in information theory while the other is data driven. In this work, we investigate the data-driven approach which was previously shown to bear greater promise and propose a better search through the coding-matrix space, keeping in mind the tradeoff between efficiency and effectiveness, as well as considerations about class-imbalance issues in the underlying binary problems. After consolidating our hy! potheses with a study on artificial domains, we propose the Unsupervised Deep Search Algorithm (UDS) coupled with re-sampling, to address both concerns. Our results on real world domains show that our method outperforms traditional multi-class learning methods.","","","10.1109/DSAA.2015.7344788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344788","","Encoding;Search problems;Complexity theory;Decoding;Testing;Algorithm design and analysis;Biological system modeling","error correction codes;learning (artificial intelligence);matrix algebra;pattern classification","multiclass learning;data driven ECOC;rebalancing;data science;error correcting output codes;binary classifiers;coding matrix;unsupervised deep search algorithm;UDS","","2","29","","","","","IEEE","IEEE Conferences"
"Investigation of ensemble models for sequence learning","A. Celikyilmaz; D. Hakkani-Tur","Microsoft, USA; Microsoft, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","5381","5385","While ensemble models have proven useful for sequence learning tasks there is relatively fewer work that provide insights into what makes them powerful. In this paper, we investigate the empirical behavior of the ensemble approaches on sequence modeling, specifically for the semantic tagging task. We explore this by comparing the performance of commonly used and easy to implement ensemble methods such as majority voting, linear combination and stacking to a learning based and rather complex ensemble method. Next, we ask the question: when models of different learning methods such as predictive and representation learning (e.g., deep learning) are aggregated, do we get performance gains over the individual baseline models. We explore these questions on a range of datasets on syntactic and semantic tagging tasks such as slot filling. Our findings show that a ranking based ensemble model outperforms all other well-known ensemble models.","","","10.1109/ICASSP.2015.7178999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178999","ensemble learning;conditional random fields;slot tagging;spoken language understanding","Tagging;Predictive models;Stacking;Semantics;Learning systems;Syntactics;Training","speech processing","ensemble models investigation;sequence learning;sequence modeling;majority voting;linear combination;representation learning;deep learning;syntactic tagging tasks;semantic tagging tasks;slot filling","","1","26","","","","","IEEE","IEEE Conferences"
"Reliability over time of EEG-based mental workload evaluation during Air Traffic Management (ATM) tasks","P. Aricò; G. Borghini; G. Di Flumeri; A. Colosimo; I. Graziani; J. Imbert; G. Granger; R. Benhacene; M. Terenzi; S. Pozzi; F. Babiloni","Dept. Physiology and Pharmacology, University “Sapienza” of Rome, Italy; Dept. Physiology and Pharmacology, University “Sapienza” of Rome, Italy; Dept. Anatomical, Histological, Forensic & Orthopedic Sciences, University “Sapienza” of Rome, Italy; Dept. Anatomical, Histological, Forensic & Orthopedic Sciences, University “Sapienza” of Rome, Italy; Dept. Physiology and Pharmacology, University “Sapienza” of Rome, Italy; Ecole Nationale de l'Aviation Civile Toulouse, France; Ecole Nationale de l'Aviation Civile Toulouse, France; Ecole Nationale de l'Aviation Civile Toulouse, France; Deep Blue Research and Consulting Rome, Italy; Deep Blue Research and Consulting Rome, Italy; Dept. Molecular Medicine, University “Sapienza” of Rome, Italy","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","7242","7245","Machine-learning approaches for mental workload (MW) estimation by using the user brain activity went through a rapid expansion in the last decades. In fact, these techniques allow now to measure the MW with a high time resolution (e.g. few seconds). Despite such advancements, one of the outstanding problems of these techniques regards their ability to maintain a high reliability over time (e.g. high accuracy of classification even across consecutive days) without performing any recalibration procedure. Such characteristic will be highly desirable in real world applications, in which human operators could use such approach without undergo a daily training of the device. In this work, we reported that if a simple classifier is calibrated by using a low number of brain spectral features, between those ones strictly related to the MW (i.e. Frontal and Occipital Theta and Parietal Alpha rhythms), those features will make the classifier performance stable over time. In other words, the discrimination accuracy achieved by the classifier will not degrade significantly across different days (i.e. until one week). The methodology has been tested on twelve Air Traffic Controls (ATCOs) trainees while performing different Air Traffic Management (ATM) scenarios under three different difficulty levels.","","","10.1109/EMBC.2015.7320063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320063","","Electroencephalography;Reliability;Training;Brain modeling;Testing;Feature extraction;Air traffic control","air traffic control;electroencephalography;learning (artificial intelligence);medical signal processing;pattern classification","EEG-based mental workload evaluation;air traffic management task;machine-learning approach;mental workload estimation;user brain activity;recalibration procedure;brain spectral feature;classifier performance;air traffic control trainees","Adult;Aviation;Brain;Electroencephalography;Humans;Machine Learning;Reproducibility of Results;Task Performance and Analysis;Workload;Young Adult","8","13","","","","","IEEE","IEEE Conferences"
"Deep Belief Networks Ensemble with Multi-objective Optimization for Failure Diagnosis","C. Zhang; J. H. Sun; K. C. Tan","Dept. of Electr. & Comput. Eng., Nat. Univ. of Singapore, Singapore, Singapore; Dept. of Electr. & Comput. Eng., Nat. Univ. of Singapore, Singapore, Singapore; Dept. of Electr. & Comput. Eng., Nat. Univ. of Singapore, Singapore, Singapore","2015 IEEE International Conference on Systems, Man, and Cybernetics","","2015","","","32","37","Early diagnosis that can detect faults from some symptoms accurately is critical, because it provides the potential benefits such as reducing maintenance costs, improving productivity and avoiding serious damages. Degradation pattern classification for early diagnosis has not been explored in many researches yet. This paper will use hybrid ensemble model for degradation pattern classification. Supervised training of deep models (e.g. Many-layered Neural Nets) is difficult for optimization problem with unlabeled datasets or insufficient data sample. Shallow models (SVMs, Neural Networks, etc...) are unlikely candidates for learning high-level abstractions, since they are affected by the curse of dimensionality. Therefore, deep learning network (DBN), an unsupervised learning model, in diagnosis problem has been investigated to do classification. Few researches have been done for exploring the effects of DBN in diagnosis. In this paper, an ensemble of DBNs with MOEA/D has been applied for diagnosis to handle failure degradation with multivariate sensory data. Turbofan engine degradation dataset is employed to demonstrate the efficacy of the proposed model. We believe that deep learning with multi-objective ensemble for degradation pattern classification can shed new light on failure diagnosis, and our work presented the applicability of this method to diagnosis as well as prognostics.","","","10.1109/SMC.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379151","Degradation Pattern Classification;Deep Belief Networks;Multi-objective Ensemble;Failure Diagnosis","Degradation;Training;Data models;Engines;Sensors;Optimization;Neurons","belief networks;fault diagnosis;jet engines;mechanical engineering computing;optimisation;pattern classification;unsupervised learning","turbofan engine degradation dataset;multivariate sensory data;failure degradation;unsupervised learning model;DBN;shallow model;hybrid ensemble model;degradation pattern classification;failure diagnosis;multiobjective optimization;deep belief network","","14","21","","","","","IEEE","IEEE Conferences"
"Learning joint features for color and depth images with Convolutional Neural Networks for object classification","E. Santana; K. Dockendorf; J. C. Principe","University of Florida, United States; Paracosm, United States; University of Florida, United States","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","1320","1323","In this paper we investigate the advantages of learning representations of color plus depth images (Red-Blue-Green-Depth, RGB-D) over color only images (RGB) for computer vision. Specifically, we investigate the advantages on the task of object recognition. For this purpose, we applied the state-of-art deep convolutional neural networks (CNN) for classification of images on the RGB-D dataset published by (Bo et al., 2011). We show that this approach provides better results than those that use separate features for color and depth. Also, we probe the resulting CNN to gain intuition about how filters for depth and color channels iterate to generate useful features.","","","10.1109/ICASSP.2015.7178184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178184","","Image color analysis;Neural networks;Training;Object recognition;Feature extraction;Computer vision;Computer architecture","image classification;image colour analysis;learning (artificial intelligence);neural nets;object recognition","color images;depth images;object classification;learning representations;computer vision;object recognition;deep convolutional neural networks;image classification;RGB-D dataset;depth channels;color channels","","","18","","","","","IEEE","IEEE Conferences"
"Learning Spectral Mapping for Speech Dereverberation and Denoising","K. Han; Y. Wang; D. Wang; W. S. Woods; I. Merks; T. Zhang","Facebook, Menlo Park, United States; Department of Computer Science and Engineering, The Ohio State University, Columbus, United States; Department of Computer Science and Engineering and the Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, United States; Starkey Hearing Technologies, Eden Prairie, United States; Starkey Hearing Technologies, Eden Prairie, United States; Starkey Hearing Technologies, Eden Prairie, United States","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2015","23","6","982","992","In real-world environments, human speech is usually distorted by both reverberation and background noise, which have negative effects on speech intelligibility and speech quality. They also cause performance degradation in many speech technology applications, such as automatic speech recognition. Therefore, the dereverberation and denoising problems must be dealt with in daily listening environments. In this paper, we propose to perform speech dereverberation using supervised learning, and the supervised approach is then extended to address both dereverberation and denoising. Deep neural networks are trained to directly learn a spectral mapping from the magnitude spectrogram of corrupted speech to that of clean speech. The proposed approach substantially attenuates the distortion caused by reverberation, as well as background noise, and is conceptually simple. Systematic experiments show that the proposed approach leads to significant improvements of predicted speech intelligibility and quality, as well as automatic speech recognition in reverberant noisy conditions. Comparisons show that our approach substantially outperforms related methods.","","","10.1109/TASLP.2015.2416653","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7067387","Deep neural networks (DNNs);denoising;dereverberation;spectral mapping;supervised learning","Speech;Reverberation;Spectrogram;Speech processing;Time-domain analysis;Noise reduction;Training","learning (artificial intelligence);neural nets;reverberation;speech intelligibility;speech recognition","spectral mapping;speech dereverberation;speech denoising;real-world environments;human speech;reverberation noise;background noise;speech intelligibility;speech quality;automatic speech recognition;supervised learning;deep neural networks;magnitude spectrogram;corrupted speech;reverberant noisy conditions","","75","44","","","","","IEEE","IEEE Journals"
"Data integration in machine learning","Y. Li; A. Ngom","Information and Communications Technologies, National Research Council of Canada, Ottawa, Ontario, Canada; School of Computer Science, University of Windsor, Ontario, Canada","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","1665","1671","Modern data generated in many fields are in a strong need of integrative machine learning models in order to better make use of heterogeneous information in decision making and knowledge discovery. How data from multiple sources are incorporated in a learning system is key step for a successful analysis. In this paper, we provide a comprehensive review on data integration techniques from a machine learning perspective.","","","10.1109/BIBM.2015.7359925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359925","data integration;Bayesian network;decision tree;random forest;multiple kernel learning;feature extraction;deep learning","Genomics;Bioinformatics;Yttrium;Lead;Loading","bioinformatics;learning (artificial intelligence)","integrative machine learning model;heterogeneous information;decision making;knowledge discovery;data integration technique","","5","53","","","","","IEEE","IEEE Conferences"
"Multi-label vs. combined single-label sound event detection with deep neural networks","E. Cakir; T. Heittola; H. Huttunen; T. Virtanen","Department of Signal Processing, Tampere University of Technology, Finland; Department of Signal Processing, Tampere University of Technology, Finland; Department of Signal Processing, Tampere University of Technology, Finland; Department of Signal Processing, Tampere University of Technology, Finland","2015 23rd European Signal Processing Conference (EUSIPCO)","","2015","","","2551","2555","In real-life audio scenes, many sound events from different sources are simultaneously active, which makes the automatic sound event detection challenging. In this paper, we compare two different deep learning methods for the detection of environmental sound events: combined single-label classification and multi-label classification. We investigate the accuracy of both methods on the audio with different levels of polyphony. Multi-label classification achieves an overall 62.8% accuracy, whereas combined single-label classification achieves a very close 61.9% accuracy. The latter approach offers more flexibility on real-world applications by gathering the relevant group of sound events in a single classifier with various combinations.","","","10.1109/EUSIPCO.2015.7362845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362845","Sound event detection;deep neural networks;multi-label classification;binary classification;audio analysis","Training;Feature extraction;Signal processing;Europe;Event detection;Databases;Cost function","audio signal processing;learning (artificial intelligence);neural nets;signal classification;signal detection","multilabel sound event detection;combined single-label sound event detection;deep neural network;real-life audio scenes;deep learning method;combined single-label classification;multilabel classification","","9","14","","","","","IEEE","IEEE Conferences"
"Optimized deep belief networks on CUDA GPUs","Teng Li; Yong Dou; Jingfei Jiang; Yueqing Wang; Qi Lv","National Laboratory for Parallel & Distributed Processing, National University of Defense and Technology, Changsha, China; National Laboratory for Parallel & Distributed Processing, National University of Defense and Technology, Changsha, China; National Laboratory for Parallel & Distributed Processing, National University of Defense and Technology, Changsha, China; National Laboratory for Parallel & Distributed Processing, National University of Defense and Technology, Changsha, China; National Laboratory for Parallel & Distributed Processing, National University of Defense and Technology, Changsha, China","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","A deep belief network (DBN) is an important branch of deep learning models and has been successfully applied in many machine learning and pattern recognition fields such as computer vision and speech recognition. However, the training of billions of parameters in DBN is computationally challenging for modern central processing units (CPUs). Many studies have reported the efficient implementations of the pre-training process of DBNs for graphics processing units (GPUs), but few studies have mentioned the fine-tuning process of DBNs. In this paper, we describe an efficient DBN implementation on the GPU, including the pre-training and fine-tuning processes. Experimental results show that our proposed method on the GPU (NVIDIA Tesla K40c) achieves up to 22 speedups on the pre-training process and 33 speedups on the fine-tuning processes compared with conventional CPU (Intel Core i7-4790K) implementations. Moreover, the performance of our algorithm is superior to that of the OpenBLAS library on the CPU and the CUBLAS library on the GPU.","","","10.1109/IJCNN.2015.7280511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280511","","Libraries;Graphics processing units","belief networks;graphics processing units;learning (artificial intelligence);parallel architectures;pattern recognition","optimized deep belief network;CUDA GPU;DBN;deep learning model;machine learning;pattern recognition;central processing unit;graphics processing unit;CPU;fine-tuning process;NVIDIA Tesla K40c;OpenBLAS library;CUBLAS library","","","18","","","","","IEEE","IEEE Conferences"
"Hybrid multi-layer deep CNN/aggregator feature for image classification","P. Kulkarni; J. Zepeda; F. Jurie; P. Perez; L. Chevallier","Technicolor 975 avenue des Champs Blancs, CS 17616, 35576 Cesson Sévigné, France; Technicolor 975 avenue des Champs Blancs, CS 17616, 35576 Cesson Sévigné, France; University of Caen Basse-Normandie, CNRS UMR 6072, ENSICAEN, France; Technicolor 975 avenue des Champs Blancs, CS 17616, 35576 Cesson Sévigné, France; Technicolor 975 avenue des Champs Blancs, CS 17616, 35576 Cesson Sévigné, France","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","1379","1383","Deep Convolutional Neural Networks (DCNN) have established a remarkable performance benchmark in the field of image classification, displacing classical approaches based on hand-tailored aggregations of local descriptors. Yet DCNNs impose high computational burdens both at training and at testing time, and training them requires collecting and annotating large amounts of training data. Supervised adaptation methods have been proposed in the literature that partially re-learn a transferred DCNN structure from a new target dataset. Yet these require expensive bounding-box annotations and are still computationally expensive to learn. In this paper, we address these shortcomings of DCNN adaptation schemes by proposing a hybrid approach that combines conventional, unsupervised aggregators such as Bag-of-Words (BoW), with the DCNN pipeline by treating the output of intermediate layers as densely extracted local descriptors. We test a variant of our approach that uses only intermediate DCNN layers on the standard PASCAL VOC 2007 dataset and show performance significantly higher than the standard BoW model and comparable to Fisher vector aggregation but with a feature that is 150 times smaller. A second variant of our approach that includes the fully connected DCNN layers significantly outperforms Fisher vector schemes and performs comparably to DCNN approaches adapted to Pascal VOC 2007, yet at only a small fraction of the training and testing cost.","","","10.1109/ICASSP.2015.7178196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178196","Deep Convolutional Neural Networks;Bag-of-Words;Fisher Vector aggregator","Training;Pipelines;Feature extraction;Kernel;Testing;Convolutional codes;Standards","feature extraction;image classification;multilayer perceptrons;unsupervised learning","hybrid multilayer deep CNN feature;hybrid multilayer deep aggregator feature;image classification;deep convolutional neural networks;performance benchmark;supervised adaptation methods;DCNN adaptation schemes;bag-of-words;BoW model;unsupervised aggregators;PASCAL VOC 2007 dataset","","10","23","","","","","IEEE","IEEE Conferences"
"Mining relationships in learning-oriented social networks","M. E. Sousa-Vieira; J. C. López-Ardao; M. Fernández-Veiga; M. Rodríguez-Pérez; C. López-García","Department of Telematics Engineering, University of Vigo, Spain; Department of Telematics Engineering, University of Vigo, Spain; Department of Telematics Engineering, University of Vigo, Spain; Department of Telematics Engineering, University of Vigo, Spain; Department of Telematics Engineering, University of Vigo, Spain","2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)","","2015","","","1","9","The widespread use of computing and communications technologies has enabled the popularity of social networks oriented to learn. In this work, we study the nature and strength of associations between students using an online social network embedded in a learning management system. With datasets from two offerings of the same course, we mined the sequences of questions and answers posted by the students to identify structural properties of the social graph, patterns of collaboration among students and factors influencing the final achievements. The results show some hints to pursue and investigate deep user analytics in online social learning systems, e.g., to build accurate prediction models for the success of effectiveness of the learning tasks based on the patterns of students' participation in the platform.","","","10.1109/DSAA.2015.7344877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344877","Online Social Networks;Informal Learning;Social Networks Analysis","Social network services;Collaboration;Software;Data mining;Collaborative work;Learning management systems","data mining;graph theory;groupware;learning management systems;social networking (online)","mining relationships;learning-oriented social networks;online social network;learning management system;social graph;collaboration","","","16","","","","","IEEE","IEEE Conferences"
"Outdoor scene labelling with learned features and region consistency activation","Y. Li; F. Sohel; M. Bennamoun; H. Lei","University of Electronic Science and Technology of China, Chengdu, China; The University of Western Australia, Perth, Australia; The University of Western Australia, Perth, Australia; University of Electronic Science and Technology of China, Chengdu, China","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","1374","1378","This paper presents a learned feature based method for scene labelling. This method is combined with a novel strategy to improve global label consistency. We first follow a traditional way to investigate trained features from convolutional neural networks (ConvNets) for scene labelling. Then, motivated by the recent successful use of general features extracted from ConvNets for various applications, we extend the use of the general features to scene labelling (for the first time). We further propose an algorithm called Region Consistency Activation (RCA) to improve the global label consistency. RCA is based on a novel transformation between Ultrametric Contour Map (UCM) and the Probability of Regions Consistency (PRC). Our algorithms were rigorously tested on the popular Stanford Background and SIFT Flow datasets. We achieved superior performances compared with the state-of-the-art methods on both of these datasets.","","","10.1109/ICIP.2015.7351025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351025","Scene labelling;semantic segmentation;convolutional neural networks;deep learning","Labeling;Feature extraction;Yttrium;Probability distribution;Transforms;Mathematical model;Image segmentation","feature extraction;neural nets;probability","outdoor scene labelling;learned feature based method;global label consistency;convolutional neural networks;ConvNets;features extraction;region consistency activation;RCA;ultrametric contour map;UCM;probability of regions consistency;PRC;Stanford background;SIFT flow datasets","","","22","","","","","IEEE","IEEE Conferences"
"Cognitive Sensing in Smart Cities Using Optical Sensors","B. S. Leelar; E. S. Shivaleela; T. Srinivas","Dept. of ECE, Indian Inst. of Sci., Bangalore, India; Dept. of ECE, Indian Inst. of Sci., Bangalore, India; Dept. of ECE, Indian Inst. of Sci., Bangalore, India","2015 International Conference on Advanced Computing and Communications (ADCOM)","","2015","","","13","15","The concept of Smart Cities becomes paramount importance both to Government and Private Sectors. The increasing pressure of urbanization creates tremendous pressure to civic infrastructures like drainage, water system, electrical networks (Smart Grid), transportation etc [1] -- [3]. The use of Big Data Analytics is becoming a necessary tool to design and monitor Smart Cities. With the cost of sensors decreasing everyday, the flood of data is ubiquitous. We have designed a neural network based on cognitive algorithm for monitoring Smart Cities. Our algorithm is developed on the highly scalable distributed machine learning GraphLab Framework, embeds the deep learning features in the GraphLab Architecture to gain insight of the unstructured data through hierarchical abstraction. This is a scalable software architecture using Apache STORM. An Update Function (UF) is defined, which updates the scope of the data in the GraphLab. The Graph Rules, used by Graph Transformation, are defined to construct hidden layers in GraphLab to get more control over information flow. The ""Update Function"" and Graph Transformation trigger each other, which forms Hybrid GraphLab. The Hybrid GraphLab allows more flexible, fault tolerant, computational capabilities and possibilities to add more hidden multi-layers in tune with deep learning. Our algorithm can be extended to other applications like old age homes, hospitals etc for improved monitoring and management, with various sensors such as Gas Sensors, Light Sensors, Directionable Sensor Probes and Noise Sensors.","","","10.1109/ADCOM.2015.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529815","Smart City;Sensor Networks;GraphLab;Deep Learning;Cognitive Sensing;Neural Network;Apache STORM","Smart cities;Intelligent sensors;Gas detectors;Storms;Monitoring","Big Data;graph grammars;graph theory;learning (artificial intelligence);neural nets;optical sensors;smart cities;software architecture","cognitive sensing;smart cities;optical sensors;Big Data analytics;neural network;cognitive algorithm;distributed machine learning;GraphLab framework;GraphLab architecture;Apache STORM;update function;graph transformation trigger;hybrid GraphLab;scalable software architecture","","","16","","","","","IEEE","IEEE Conferences"
"Two Parallel Deep Convolutional Neural Networks for pedestrian detection","Bo-Yao Lin; C. Chen","Institute of Information Science, Academia Sinica, Taipei, Taiwan; Institute of Information Science & Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","2015 International Conference on Image and Vision Computing New Zealand (IVCNZ)","","2015","","","1","6","Pedestrian detection attracts lots of attentions in the field of computer vision in recent years. It is difficult to handle data imbalance between positive and negative examples and easy-to-confused negative samples for pedestrian detection when training a single deep convolutional neural network (CNN) model. In this paper, we present a deep learning approach that combines two parallel deep CNN models for pedestrian detection. We propose using two deep CNNs, and each of which is capable of solving a particular mission-oriented task to form parallel classification models. Then, the models are integrated to build a more robust pedestrian detector. Experimental results on the Caltech dataset demonstrate the effectiveness of our approach for pedestrian detection compared to other state-of-the-art deep CNN methods.","","","10.1109/IVCNZ.2015.7761510","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7761510","","Feature extraction;Training;Neural networks;Detectors;Training data;Computational modeling;Object detection","computer vision;convolution;image classification;learning (artificial intelligence);neural nets;object detection;pedestrians","parallel deep convolutional neural networks;pedestrian detection;computer vision;data imbalance;deep learning approach;parallel deep CNN models;mission-oriented task;parallel classification models;Caltech dataset","","","37","","","","","IEEE","IEEE Conferences"
"Accelerating Machine Learning Kernel in Hadoop Using FPGAs","K. Neshatpour; M. Malik; H. Homayoun","NA; NA; NA","2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing","","2015","","","1151","1154","Big data applications share inherent characteristics that are fundamentally different from traditional desktop CPU, parallel and web service applications. They rely on deep machine learning and data mining applications. A recent trend for big data analytics is to provide heterogeneous architectures to allow support for hardware specialization to construct the right processing engine for analytics applications. However, these specialized heterogeneous architectures require extensive exploration of design aspects to find the optimal architecture in terms of performance and cost. % Considering the time dedicated to create such specialized architectures, a model that estimates the potential speedup achievable through offloading various parts of the algorithm to specialized hardware would be necessary. This paper analyzes how offloading computational intensive kernels of machine learning algorithms to a heterogeneous CPU+FPGA platform enhances the performance. We use the latest Xilinx Signboards for implementation and result analysis. Furthermore, we perform a comprehensive analysis of communication and computation overheads such as data I/O movements, and calling several standard libraries that can not be offloaded to the accelerator to understand how the speedup of each application will contribute to its overall execution in an end-to-end Hadoop MapReduce environment.","","","10.1109/CCGrid.2015.165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7152609","Big Data;Acceleration;FPGA","Acceleration;Kernel;Computer architecture;Hardware;Field programmable gate arrays;Big data;Machine learning algorithms","Big Data;data analysis;data mining;field programmable gate arrays;learning (artificial intelligence);parallel processing","machine-learning kernels;Hadoop;FPGA;Big data applications;desktop CPU;Web service applications;deep machine learning application;data mining applications;processing engine;heterogeneous architectures;optimal architecture;machine learning algorithms;heterogeneous CPU+FPGA platform;Xilinx Zynq boards;data I/O movements;end-to-end Hadoop MapReduce environment","","14","10","","","","","IEEE","IEEE Conferences"
"Discriminative Learning of Deep Convolutional Feature Point Descriptors","E. Simo-Serra; E. Trulls; L. Ferraz; I. Kokkinos; P. Fua; F. Moreno-Noguer","NA; NA; NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","118","126","Deep learning has revolutionalized image-level tasks such as classification, but patch-level tasks, such as correspondence, still rely on hand-crafted features, e.g. SIFT. In this paper we use Convolutional Neural Networks (CNNs) to learn discriminant patch representations and in particular train a Siamese network with pairs of (non-)corresponding patches. We deal with the large number of potential pairs with the combination of a stochastic sampling of the training set and an aggressive mining strategy biased towards patches that are hard to classify. By using the L2 distance during both training and testing we develop 128-D descriptors whose euclidean distances reflect patch similarity, and which can be used as a drop-in replacement for any task involving SIFT. We demonstrate consistent performance gains over the state of the art, and generalize well against scaling and rotation, perspective transformation, non-rigid deformation, and illumination changes. Our descriptors are efficient to compute and amenable to modern GPUs, and are publicly available.","","","10.1109/ICCV.2015.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410379","","Training;Three-dimensional displays;Measurement;Computer architecture;Computer vision;Computational modeling;Semantics","data mining;image classification;image sampling;learning (artificial intelligence);neural nets;stochastic processes;transforms","discriminative learning;deep convolutional feature point descriptors;convolutional neural networks;CNN;discriminant patch representations;Siamese network training;stochastic sampling;mining strategy;Euclidean distance;SIFT;L<sub>2</sub> distance;image classification","","182","34","","","","","IEEE","IEEE Conferences"
"Regularization of context-dependent deep neural networks with context-independent multi-task training","P. Bell; S. Renals","Centre for Speech Technology Research, University of Edinburgh, EH8 9AB, UK; Centre for Speech Technology Research, University of Edinburgh, EH8 9AB, UK","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4290","4294","The use of context-dependent targets has become standard in hybrid DNN systems for automatic speech recognition. However, we argue that despite the use of state-tying, optimising to context-dependent targets can lead to over-fitting, and that discriminating between arbitrary tied context-dependent targets may not be optimal. We propose a multitask learning method where the network jointly predicts context-dependent and monophone targets. We evaluate the method on a large-vocabulary lecture recognition task and show that it yields relative improvements of 3-10% over baseline systems.","","","10.1109/ICASSP.2015.7178780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178780","deep neural networks;multitask learning;regularization","Hidden Markov models;Context;Context modeling;Data models;Training data;Standards;Transforms","learning (artificial intelligence);speech recognition","context-dependent deep neural network regularization;context-independent multitask training;context-dependent target optimization;hybrid DNN systems;automatic speech recognition;multitask learning method;monophone target prediction;large-vocabulary lecture recognition task","","18","30","","","","","IEEE","IEEE Conferences"
"Maxout based deep neural networks for Arabic phonemes recognition","A. AbdAlmisreb; A. F. Abidin; N. M. Tahir","Faculty of Electrical Engineering, UniversitiTechnologi Mara, 40450 Shah Alam, Selangor, Malaysia; Faculty of Electrical Engineering, UniversitiTechnologi Mara, 40450 Shah Alam, Selangor, Malaysia; Faculty of Electrical Engineering, UniversitiTechnologi Mara, 40450 Shah Alam, Selangor, Malaysia","2015 IEEE 11th International Colloquium on Signal Processing & Its Applications (CSPA)","","2015","","","192","197","Arabic is widely articulated by Malay race due to several factors such as; performing worship and reciting the Holy book of Muslims. Newly, Maxout deep neural networks have conveyed substantial perfections to speech recognition systems. Hence, in this paper, a fully connected feed-forward neural network with Maxout units is introduced. The proposed deep neural network involves three hidden layers, 500 Maxout units and 2 neurons for each unit along with Mel-Frequency Cepstral Coefficients (MFCC) as feature extraction of the phonemes waveforms. Further, the deep neural network is trained and tested over a corpus comprised of consonant Arabic phonemes recorded from 20 Malay speakers. Each person is required to pronounce the twenty eight consonant phonemes within the three chances given to each subjects articulate all the letters. Conversely, continuous recording has been established to record all the letters in each chance. The recording process is accomplished using SAMSON C03U USB multi-pattern condenser microphone. Here, the data are divided into five waveforms for training the proposed Maxout network and fifteen waveforms for testing. Experimentally, the proposed Dropout function for training has shown considerable performance over Sigmoid and Rectified Linear Unit (ReLU) functions. Eventually, testing Maxout network has shown considerable outcome compare to Restricted Boltzmann Machine (RBM), Deep Belief Network (DBN), Convolutional Neural Network (CNN), the conventional feedforward neural network (NN) and Convolutional Auto-Encoder (CAE).","","","10.1109/CSPA.2015.7225644","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7225644","Maxout Networks;Deep learning;Arabic;Deep Belief Network;Convolutional Neural Network","Biological neural networks;Speech recognition;Training;Error analysis;Hidden Markov models;Acoustics","cepstral analysis;feature extraction;feedforward neural nets;natural language processing;speech recognition","maxout based deep neural networks;Arabic phonemes recognition;Malay race;speech recognition systems;fully connected feed-forward neural network;maxout units;mel-frequency cepstral coefficients;MFCC;feature extraction;phonemes waveforms;consonant Arabic phonemes;Malay speakers;recording process;SAMSON C03U USB multipattern condenser microphone;dropout function;rectified linear unit;ReLU functions","","2","20","","","","","IEEE","IEEE Conferences"
"Localization based stereo speech separation using deep networks","Y. Yu; W. Wang; J. Luo; P. Feng","School of Marine Science and Technology, Northwestern Polytechnical University, Xi'an, China, 710072; Centre for Vision Speech and Signal Processing, University of Surrey, UK, GU2 7XH; School of Marine Science and Technology, Northwestern Polytechnical University, Xi'an, China, 710072; Centre for Vision Speech and Signal Processing, University of Surrey, UK, GU2 7XH","2015 IEEE International Conference on Digital Signal Processing (DSP)","","2015","","","153","157","Time-frequency (T-F) masking is an effective method for stereo speech source separation. However, reliable estimation of the T-F mask from sound mixtures is a challenging task, especially when room reverberations are present in the mixtures. In this paper, we proposed a new stereo speech separation system where deep networks are used to generate soft T-F mask for separation. More specifically, the deep network, which is composed of two sparse autoencoders and a softmax classifier, is used to estimate the orientations of the target and interferers at each T-F unit, based on low-level features, such as mixing vector (MV), interaural level and phase difference (IPD/ILD). The deep network is trained by a greedy layer-wise method using a dataset that was generated by convolving room impulse responses (RIRs) with clean speech signals positioned in different angles with respect to the sensors. With the trained deep networks, the probability that each T-F unit belongs to the target or interferer can be estimated based on the localization cues for generating the soft mask. Experiments based on real binaural RIRs and TIMIT dataset are provided to show the performance of the proposed system for reverberant speech mixtures, as compared with a model based T-F masking technique proposed recently.","","","10.1109/ICDSP.2015.7251849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7251849","Deep learning;Deep networks;Source separation;Soft mask","Speech;Training;Feature extraction;Speech processing;Source separation;Neural networks;Reverberation","reverberation;signal classification;source separation;speech coding;speech intelligibility;speech processing;time-frequency analysis","localization based stereo speech source separation;deep network;time-frequency masking;T-F masking;room reverberation;sparse autoencoder;softmax classifier;mixing vector;MV;interaural level;phase difference;greedy layer-wise method;room impulse response;RIR","","","25","","","","","IEEE","IEEE Conferences"
"Arabic handwritten characters recognition using Deep Belief Neural Networks","M. Elleuch; N. Tagougui; M. Kherallah","National School of Computer Science (ENSI), University of Manouba, Tunisia; The Higher Institute of Management of Gabes, University of Gabes, Tunisia; Advanced Technologies for Medicine and Signals (ATMS), University of Sfax, Tunisia","2015 IEEE 12th International Multi-Conference on Systems, Signals & Devices (SSD15)","","2015","","","1","5","In the handwriting recognition field, the deep learning is becoming the new trend thanks to their ability to deal with unlabeled raw data especially with the huge size of raw data available nowadays. In this paper, we investigate Deep Belief Neural Network (DBNN) for Arabic handwritten character/word recognition. The proposed system takes the raw data as input and proceeds with a grasping layer-wise unsupervised learning algorithm. The approach was tested on two different databases. For the character level one, the results were promising with an error classification rate of 2.1% on the HACDB database. Unlike, the character level, the evaluation on the ADAB database to deal with word level shows an error rate which exceeds the 40%. Hence, the proposed DBNN structure is not already able to deal with high-level dimensional data and thus has to be improved.","","","10.1109/SSD.2015.7348121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348121","DBNN;unsupervised training;Arabic handwritten;recognition","Databases;Error analysis;Training;Handwriting recognition;Character recognition;Shape;Neurons","error statistics;handwritten character recognition;neural nets;word processing","Arabic handwritten characters recognition;deep belief neural networks;handwriting recognition field;deep learning;unlabeled raw data;Arabic handwritten character/word recognition;grasping layer-wise unsupervised learning algorithm;error classification rate;HACDB database;character level;ADAB database;word level;error rate;DBNN structure;high-level dimensional data","","8","17","","","","","IEEE","IEEE Conferences"
"Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition","X. Li; X. Wu","Speech and Hearing Research Center, Key Laboratory of Machine Perception (Ministry of Education), Peking University, Beijing, 100871, China; Speech and Hearing Research Center, Key Laboratory of Machine Perception (Ministry of Education), Peking University, Beijing, 100871, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4520","4524","Long short-term memory (LSTM) based acoustic modeling methods have recently been shown to give state-of-the-art performance on some speech recognition tasks. To achieve a further performance improvement, in this research, deep extensions on LSTM are investigated considering that deep hierarchical model has turned out to be more efficient than a shallow one. Motivated by previous research on constructing deep recurrent neural networks (RNNs), alternative deep LSTM architectures are proposed and empirically evaluated on a large vocabulary conversational telephone speech recognition task. Meanwhile, regarding to multi-GPU devices, the training process for LSTM networks is introduced and discussed. Experimental results demonstrate that the deep LSTM networks benefit from the depth and yield the state-of-the-art performance on this task.","","","10.1109/ICASSP.2015.7178826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178826","long short-term memory;recurrent neural networks;deep neural networks;acoustic modeling;large vocabulary speech recognition","Speech recognition;Hidden Markov models;Training;Speech;Computer architecture;Recurrent neural networks;Acoustics","graphics processing units;learning (artificial intelligence);recurrent neural nets;speech recognition","training process;multiGPU device;telephone speech recognition task;RNN;hierarchical model;acoustic modeling method;deep LSTM network;large vocabulary speech recognition;deep recurrent neural network;long short-term memory","","47","36","","","","","IEEE","IEEE Conferences"
"Road segmentation via iterative deep analysis","X. Chen; Y. Qiao","Shenzhen Key Laboratory of Computer Vision and Pattern Recognition, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, 1068 Xueyuan Avenue, Shenzhen University Town, Shenzhen, P.R. China; Shenzhen Key Laboratory of Computer Vision and Pattern Recognition, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, 1068 Xueyuan Avenue, Shenzhen University Town, Shenzhen, P.R. China","2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)","","2015","","","2640","2645","Nowadays, people are increasingly concerned about the safety of traffic systems. Road segmentation and recognition is a fundamental problem in perceiving traffic environments and serve as the basis for self-driving cars. In this paper, inspired by an iterative deep analysis thinking, we propose a novel method which is able to learning powerful features step by step, and solve the optimal precision by balancing local and global information to conduct pixel-level classification for road segmentation. Firstly, we introduce an iterative deep analysis thinking which shows that how to design a strong and robustness deep model from failure experience. Secondly, we choose a powerful global features learning network as basis to create a novel framework for our task. Meanwhile, we employ the patch and multi-scale pyramid as input to enhance local features learning. We conduct experiments on three datasets from KITTI Vision Benchmark, namely UU, UM, UMM. The experimental results demonstrate that our proposed method obtains comparable performance with state-of-the-art methods on these datasets.","","","10.1109/ROBIO.2015.7419738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7419738","","Roads;Feature extraction;Machine learning;Neural networks;Image segmentation;Training;Semantics","feature extraction;image recognition;image segmentation;intelligent transportation systems;learning (artificial intelligence);mobile robots;road safety;road vehicles;traffic information systems","road segmentation;road recognition;iterative deep analysis thinking;traffic system safety;feature learning network;autonomous vehicle;intelligent transportation system;ITS","","2","23","","","","","IEEE","IEEE Conferences"
"Temporal Difference Learning for the Game Tic-Tac-Toe 3D: Applying Structure to Neural Networks","M. V. D. Steeg; M. M. Drugan; M. Wiering","NA; Dept. of Comput. Sci., Tech. Univ. of Eindhoven, Eindhoven, Netherlands; Inst. of Artificial Intell. & Cognitive Eng., Univ. of Groningen, Groningen, Netherlands","2015 IEEE Symposium Series on Computational Intelligence","","2015","","","564","570","When reinforcement learning is applied to large state spaces, such as those occurring in playing board games, the use of a good function approximator to learn to approximate the value function is very important. In previous research, multi-layer perceptrons have often been quite successfully used as function approximator for learning to play particular games with temporal difference learning. With the recent developments in deep learning, it is important to study if using multiple hidden layers or particular network structures can help to improve learning the value function. In this paper, we compare five different structures of multilayer perceptrons for learning to play the game Tic-Tac-Toe 3D, both when training through self-play and when training against the same fixed opponent they are tested against. We compare three fully connected multilayer perceptrons with a different number of hidden layers and/or hidden units, as well as two structured ones. These structured multilayer perceptrons have a first hidden layer that is only sparsely connected to the input layer, and has units that correspond to the rows in Tic-Tac-Toe 3D. This allows them to more easily learn the contribution of specific patterns on the corresponding rows. One of the two structured multilayer perceptrons has a second hidden layer that is fully connected to the first one, which allows the neural network to learn to non-linearly integrate the information in these detected patterns. The results on Tic-Tac-Toe 3D show that the deep structured neural network with integrated pattern detectors has the strongest performance out of the compared multilayer perceptrons against a fixed opponent, both through self-training and through training against this fixed opponent.","","","10.1109/SSCI.2015.89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7376662","","Games;Three-dimensional displays;Learning (artificial intelligence);Multilayer perceptrons;Training;Benchmark testing","computer games;learning (artificial intelligence);multilayer perceptrons","temporal difference learning;Tic-Tac-Toe 3D game;neural networks;deep structured neural network;integrated pattern detectors","","6","19","","","","","IEEE","IEEE Conferences"
"Self Learning Network Traffic Classification","Vandana M; S. Manmadhan","Computer Science & Engineering, NSS College of Engineering, Palakkad Kerala, India; Computer Science & Engineering, NSS College of Engineering, Palakkad, India","2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)","","2015","","","1","5","Network management is part of traffic engineering and security. The current solutions - Deep Packet Inspection (DPI) and statistical classification, rely on the availability of a training set. In case of these there is a cumbersome need to regularly update the signatures. Further their visibility is limited to classes the classifier has been trained for. Unsupervised algorithms have been envisioned as a an alternative to automatically identify classes of traffic. To address these issues Self Learning Network Traffic Classification is proposed. It uses unsupervised algorithms along with an adaptive seeding approach to automatically lets classes of traffic to emerge, making them identified and labelled. Unlike traditional classifiers, there is no need of a-priori knowledge of signatures nor a training set to extract the signatures. Instead, Self Learning Network Traffic Classification automatically groups flows into pure (or homogeneous) clusters using simple statistical features. This label assignment (which is still based on some manual intervention) ensures that class labels can be easily discovered. Furthermore, Self Learning Network Traffic Classification uses an iterative seeding approach which will boost its ability to cope with new protocols and applications. Unlike state-of-art classifiers, the biggest advantage of Self Learning Network Traffic Classification is its ability to discover new protocols and applications in an almost automated fashion.","","","10.1109/ICIIECS.2015.7193038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7193038","Traffic classification;clustering;self-seeding;unsupervised machine learning","Ports (Computers);Classification algorithms;Telecommunication traffic;IP networks;Protocols;Clustering algorithms;Filtering","pattern classification;statistical analysis;traffic engineering computing;unsupervised learning","network management;traffic engineering;deep packet inspection;DPI;statistical classification;self learning network traffic classification;adaptive seeding approach;protocols;unsupervised machine learning","","3","9","","","","","IEEE","IEEE Conferences"
"Sentiment Analysis Using Convolutional Neural Network","X. Ouyang; P. Zhou; C. H. Li; L. Liu","NA; NA; NA; NA","2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing","","2015","","","2359","2364","Sentiment analysis of text content is important for many natural language processing tasks. Especially, as the development of the social media, there is a big need in dig meaningful information from the big data on Internet through the sentiment analysis. Inspired by the successes of deep learning, we are interested in handling the sentiment analysis task using deep learning models. In this paper, we propose a framework called Word2vec + Convolutional Neural Network (CNN). Firstly, we use the word2vec proposed by Google to compute vector representations of words, which will be the input for the CNN. The purpose of using word2vec is to gain the vector representation of word and reflect the distance of words. That will lead to initialize the parameters at a good point of CNN, which can efficiently improve the performance of the nets in this problem. Secondly, we design a suitable CNN architecture for the sentiment analysis task. We use 3 pairs of convolutional layers and pooling layers in this architecture. To the best of our knowledge, this is the first time that a 7-layers architecture model is applied using word2vec and CNN to analyze sentences' sentiment. We also use the Parametric Rectified Linear Unit (PReLU), Normalization and Dropout technology to improve the accuracy and generalizability of our model. We test our framework in a public dataset which is the corpus of movie review excerpts that includes fives labels: negative, somewhat negative, neural, somewhat positive and positive. Our network achieves test accuracy of 45.4% in this dataset, which is a better performance than some other neural network model like Recursive Neural Network (RNN) and Matrix-Vector Recursive Neural Network (MV-RNN).","","","10.1109/CIT/IUCC/DASC/PICOM.2015.349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363395","sentiment analysis;deep learning;word2vec","Neural networks;Sentiment analysis;Machine learning;Google;Analytical models;Media;Computational modeling","learning (artificial intelligence);neural net architecture;sentiment analysis","text content sentiment analysis;convolutional neural network;natural language processing tasks;social media;Big Data;Internet;deep learning models;Word2vec framework;word vector representation;CNN architecture;pooling layers;convolutional layers;7-layers architecture model;sentence sentiment analysis;parametric rectified linear unit;PReLU;normalization and dropout technology;recursive neural network;RNN;matrix-vector recursive neural network;MV-RNN","","22","21","","","","","IEEE","IEEE Conferences"
"Cross-Domain Feature Learning in Multimedia","X. Yang; T. Zhang; C. Xu","National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Multimedia","","2015","17","1","64","78","In the Web 2.0 era, a huge number of media data, such as text, image/video, and social interaction information, have been generated on the social media sites (e.g., Facebook, Google, Flickr, and YouTube). These media data can be effectively adopted for many applications (e.g., image/video annotation, image/video retrieval, and event classification) in multimedia. However, it is difficult to design an effective feature representation to describe these data because they have multi-modal property (e.g., text, image, video, and audio) and multi-domain property (e.g., Flickr, Google, and YouTube). To deal with these issues, we propose a novel cross-domain feature learning (CDFL) algorithm based on stacked denoising auto-encoders. By introducing the modal correlation constraint and the cross-domain constraint in conventional auto-encoder, our CDFL can maximize the correlations among different modalities and extract domain invariant semantic features simultaneously. To evaluate our CDFL algorithm , we apply it to three important applications: sentiment classification, spam filtering, and event classification. Comprehensive evaluations demonstrate the encouraging performance of the proposed approach.","","","10.1109/TMM.2014.2375793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6971165","Cross-domain;deep learning;feature learning;multi-modal","Media;Multimedia communication;Streaming media;Google;Correlation;Semantics;Noise reduction","learning (artificial intelligence);multimedia systems;social networking (online)","cross-domain feature learning;multimedia;Web 2.0;social media sites;Facebook;Google;Flickr;YouTube;feature representation;multimodal property;multidomain property;stacked denoising auto-encoder;modal correlation constraint;cross-domain constraint;sentiment classification;spam filtering;event classification","","57","59","","","","","IEEE","IEEE Journals"
"Multi task sequence learning for depression scale prediction from video","L. Chao; J. Tao; M. Yang; Y. Li","National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing, China","2015 International Conference on Affective Computing and Intelligent Interaction (ACII)","","2015","","","526","531","Depression is a typical mood disorder, which affects people in mental and even physical problems. People who suffer depression always behave abnormal in visual behavior and the voice. In this paper, an audio visual based multimodal depression scale prediction system is proposed. Firstly, features are extracted from video and audio are fused in feature level to represent the audio visual behavior. Secondly, long short memory recurrent neural network (LSTM-RNN) is utilized to encode the dynamic temporal information of the abnormal audio visual behavior. Thirdly, emotion information is utilized by multi-task learning to boost the performance further. The proposed approach is evaluated on the Audio-Visual Emotion Challenge (AVEC2014) dataset. Experiments results show the dimensional emotion recognition helps to depression scale prediction.","","","10.1109/ACII.2015.7344620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344620","depression recognition;affective computing;multi-task learning;deep learning","Feature extraction;Visualization;Face;Shape;Training;Emotion recognition;Context","audio signal processing;audio-visual systems;feature extraction;learning (artificial intelligence);medical disorders;psychology;recurrent neural nets;sensor fusion;video signal processing","video features;LSTM-RNN;depression scale prediction;dimensional emotion recognition;AVEC2014 dataset;Audio-Visual Emotion Challenge dataset;emotion information;abnormal audio visual behavior;dynamic temporal information encoding;long short memory recurrent neural network;feature level fusion;feature extraction;multimodal depression scale prediction system;mood disorder;multitask sequence learning","","7","35","","","","","IEEE","IEEE Conferences"
"A new unsupervised model of action recognition","D. Wang; Q. Shao; X. Li","School of Computer Engineer and Science, Shanghai University, Shanghai, China; School of Computer Engineer and Science, Shanghai University, Shanghai, China; School of Computer Engineer and Science, Shanghai University, Shanghai, China","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","1160","1164","The hand-craft feature description such as blob detection (SIFT), the gradient of edges (HOG) are widely used in the action recognition, while they are not suitable to all kinds of videos and increase the manual intervention. And the deep learning based on unsupervised method have a lot of parameter tuning and iterations. Our paper proposes a new model of action recognition combine the hand-craft spatiotemporal interest points with unsupervised descriptors. This model uses STIP as the spatiotemporal interest points extractor and improved K-means as the unsupervised learning method to build the descriptor. This K-Means based unsupervised descriptor provides higher accuracy than hand-craft descriptors and lower training time than multi-layer unsupervised learning methods. Moreover, we update the BoF model in recognition framework, which constructs local vocabularies to each category. Experimental results indicate that this proposed framework works well.","","","10.1109/ICIP.2015.7350982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350982","Action recognition;K-Means;unsupervised feature;updated BoF model","Feature extraction;Videos;Dictionaries;Computational modeling;Learning systems;Training;Vocabulary","feature extraction;image recognition;unsupervised learning","unsupervised model;action recognition;hand-craft feature description;blob detection;SIFT;gradient of edges;HOG;deep learning;hand-craft spatiotemporal interest points;STIP;spatiotemporal interest points extractor;k-means based unsupervised descriptor;hand-craft descriptors;multilayer unsupervised learning methods;BoF model;local vocabularies","","2","22","","","","","IEEE","IEEE Conferences"
"Deep learning for automatic cell detection in wide-field microscopy zebrafish images","B. Dong; L. Shao; M. Da Costa; O. Bandmann; A. F. Frangi","Centre of Computational Imaging & Simulation Technologies in Biomedicine (CISTIB); Department of Computer Science and Digital Technologies, Northumbria University; Department of Neuroscience, The University of Sheffield; Department of Neuroscience, The University of Sheffield; Centre of Computational Imaging & Simulation Technologies in Biomedicine (CISTIB)","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","","2015","","","772","776","The zebrafish has become a popular experimental model organism for biomedical research. In this paper, a unique framework is proposed for automatically detecting Tyrosine Hydroxylase-containing (TH-labeled) cells in larval zebrafish brain z-stack images recorded through the wide-field microscope. In this framework, a supervised max-pooling Convolutional Neural Network (CNN) is trained to detect cell pixels in regions that are preselected by a Support Vector Machine (SVM) classifier. The results show that the proposed deep-learned method outperforms hand-crafted techniques and demonstrate its potential for automatic cell detection in wide-field microscopy z-stack zebrafish images.","","","10.1109/ISBI.2015.7163986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163986","","Computer architecture;Microprocessors;Training;Microscopy;Three-dimensional displays;Neurons;Histograms","biomedical optical imaging;brain;cellular biophysics;convolution;enzymes;feature extraction;image classification;learning (artificial intelligence);medical image processing;molecular biophysics;neural nets;neurophysiology;optical microscopy;support vector machines","deep learning;wide-field microscopy;experimental model organism;biomedical research;automatic tyrosine hydroxylase-containing cell detection;automatic TH-labeled cell detection;larval zebrafish brain z-stack image recording;supervised max-pooling CNN training;convolutional neural network;cell pixel detection;region preselection;support vector machine;SVM classifier;hand-crafted technique","","28","26","","","","","IEEE","IEEE Conferences"
"DEX: Deep EXpectation of Apparent Age from a Single Image","R. Rothe; R. Timofte; L. V. Gool","Comput. Vision Lab., ETH Zurich, Zurich, Switzerland; Comput. Vision Lab., ETH Zurich, Zurich, Switzerland; NA","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","252","257","In this paper we tackle the estimation of apparent age in still face images with deep learning. Our convolutional neural networks (CNNs) use the VGG-16 architecture [13] and are pretrained on ImageNet for image classification. In addition, due to the limited number of apparent age annotated images, we explore the benefit of finetuning over crawled Internet face images with available age. We crawled 0.5 million images of celebrities from IMDB and Wikipedia that we make public. This is the largest public dataset for age prediction to date. We pose the age regression problem as a deep classification problem followed by a softmax expected value refinement and show improvements over direct regression training of CNNs. Our proposed method, Deep EXpectation (DEX) of apparent age, first detects the face in the test image and then extracts the CNN predictions from an ensemble of 20 networks on the cropped face. The CNNs of DEX were finetuned on the crawled images and then on the provided images with apparent age annotations. DEX does not use explicit facial landmarks. Our DEX is the winner (1st place) of the ChaLearn LAP 2015 challenge on apparent age estimation with 115 registered teams, significantly outperforming the human reference.","","","10.1109/ICCVW.2015.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406390","","Face;Estimation;Training;Encyclopedias;Internet;Electronic publishing","age issues;convolution;estimation theory;face recognition;image classification;learning (artificial intelligence);neural nets;regression analysis","DEX;deep expectation;apparent age estimation;still face images;deep learning;convolutional neural networks;VGG-16 architecture;ImageNet;image classification;apparent age annotated images;crawled Internet face images;age prediction;age regression problem;deep classification problem;softmax expected value refinement;ChaLearn LAP 2015 challenge","","116","13","","","","","IEEE","IEEE Conferences"
"Application of deep neural network in estimation of the weld bead parameters","S. Keshmiri; Xin Zheng; Lu Wen Feng; Chee Khiang Pang; Chee Meng Chew","Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore; Department of Mechanical Engineering, National University of Singapore, Singapore","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","2015","","","3518","3523","We present a deep learning approach to estimation of the bead parameters in welding tasks. Our model is based on a four-hidden-layer neural network architecture. More specifically, the first three hidden layers of this architecture utilize Sigmoid function to produce their respective intermediate outputs. On the other hand, the last hidden layer uses a linear transformation to generate the final output of this architecture. This transforms our deep network architecture from a classifier to a non-linear regression model. We compare the performance of our deep network with a selected number of results in the literature to show a considerable improvement in reducing the errors in estimation of these values. Furthermore, we show its scalability on estimating the weld bead parameters with same level of accuracy on combination of datasets that pertain to different welding techniques. This is a nontrivial result that is counter-intuitive to the general belief in this field of research.","","","10.1109/IROS.2015.7353868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353868","","Welding;Estimation;Computer architecture;Training data;Neural networks;Yttrium;Training","learning (artificial intelligence);neural net architecture;neurocontrollers;parameter estimation;regression analysis;transforms;welding;welds","welding techniques;nonlinear regression model;classifier;deep network architecture;linear transformation;Sigmoid function;four-hidden-layer neural network architecture;welding tasks;weld bead parameters estimation;deep learning approach;deep neural network","","3","30","","","","","IEEE","IEEE Conferences"
"Deep learning based super-resolution for improved action recognition","K. Nasrollahi; S. Escalera; P. Rasti; G. Anbarjafari; X. Baro; H. J. Escalante; T. B. Moeslund","Visual Analysis of People laboratory, Aalborg University, Denmark; Human Pose Recovery and Behavior Analysis Group, University of Barcelona, Computer Vision Center, Spain; iCv Group, Inst. of Technology, University of Tartu, Estonia; iCv Group, Inst. of Technology, University of Tartu, Estonia; Universitat Oberta de Catalunya, Computer Vision Center, Spain; INAOE, Puebla, Mexico; Visual Analysis of People laboratory, Aalborg University, Denmark","2015 International Conference on Image Processing Theory, Tools and Applications (IPTA)","","2015","","","67","72","Action recognition systems mostly work with videos of proper quality and resolution. Even most challenging benchmark databases for action recognition, hardly include videos of low-resolution from, e.g., surveillance cameras. In videos recorded by such cameras, due to the distance between people and cameras, people are pictured very small and hence challenge action recognition algorithms. Simple upsampling methods, like bicubic interpolation, cannot retrieve all the detailed information that can help the recognition. To deal with this problem, in this paper we combine results of bicubic interpolation with results of a state-of-the-art deep learning-based super-resolution algorithm, through an alpha-blending approach. The experimental results obtained on down-sampled version of a large subset of Hoolywood2 benchmark database show the importance of the proposed system in increasing the recognition rate of a state-of-the-art action recognition system for handling low-resolution videos.","","","10.1109/IPTA.2015.7367098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367098","","Videos;Image recognition;Spatial resolution;Cameras;Interpolation;Trajectory","image recognition;image resolution;interpolation;learning (artificial intelligence);set theory;video retrieval;visual databases","improved action recognition system;information retrieval;bicubic interpolation;deep learning-based super-resolution algorithm;alpha-blending approach;down-sampled version;Hoolywood2 benchmark database;low-resolution video handling","","14","25","","","","","IEEE","IEEE Conferences"
"Classification with Extreme Learning Machine on GPU","T. Jeowicz; P. Gajdo; V. Uher; V. Snáel","NA; NA; Dept. of Comput. Sci., VSB-Tech. Univ. of Ostrava, Ostrava - Poruba, Czech Republic; NA","2015 International Conference on Intelligent Networking and Collaborative Systems","","2015","","","116","122","The general classification is a machine learning task that tries to assign the best class to a given unknown input vector based on past observations (training data). Most of developed algorithms are very time consuming for large datasets (Support Vector Machine, Deep Neural Networks, etc.). Extreme Learning Machine (ELM) is a high quality classification algorithm that gains much popularity in recent years. This paper shows that the speed of learning of this algorithm may be improved by using GPU platform. Experimental results showed that proposed approach is much faster and provides the same accuracy as the original ELM algorithm. The proposed approach runs completely on GPU platform and thus it may be effectively incorporated within other applications.","","","10.1109/INCoS.2015.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312059","Extreme learning machine;CUDA;GPU;Fast;Parallel;Classification","Graphics processing units;Neurons;Training;Support vector machines;Matrix decomposition;Neural networks;Algorithm design and analysis","learning (artificial intelligence);neural nets;pattern classification;support vector machines","extreme learning machine;GPU;general classification;machine learning task;support vector machine;deep neural network;ELM;classification algorithm","","4","28","","","","","IEEE","IEEE Conferences"
"Face occlusion detection based on multi-task convolution neural network","Yizhang Xia; Bailing Zhang; F. Coenen","Department of Computer Science & Software Eng., Xi'an Jiaotong-Liverpool University, Suzhou, 215123, China; Department of Computer Science, The University of Liverpool, UK; Department of Computer Science, The University of Liverpool, UK","2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","","2015","","","375","379","With the rise of crimes associated with ATM, security reinforcement by surveillance techniques has been in high agenda for both academia and industries. Though cameras are generally installed in ATMs to capture the facial images of users, the function is only limited to recording for follow-up criminal investigations, which could become useless when a criminal's face is occluded. Therefore, face occlusion detection has become very important to prevent crimes connected with ATMs. Traditional approaches to solve the problem typically consist of a succession of steps such as localization, segmentation, feature extraction and recognition. This paper proposes robust and effective facial occlusion detection based on convolutional neural networks (ConvNets) with multi-task learning. Covering of different facial parts, namely, left eye, right eye, nose and mouth, can be predicted by the multi-task CNN. In comparison with previous approaches, CNN is optimal from the system point of view as the design is based on end-to-end principle and the model operates directly on the image pixels. We created a large scale face occlusion database, consisting of over fifty thousand images, with annotated facial parts. Experimental results revealed that the proposed method is extremely effective.","","","10.1109/FSKD.2015.7381971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381971","component;face occlusion detection;ATM;deep learning;convolusional neurol network;multi-task learning","Face;Feature extraction;Mouth;Convolution;Nose;Yttrium;Neural networks","automatic teller machines;criminal law;face recognition;image segmentation;learning (artificial intelligence);neural nets","face occlusion detection;multitask convolution neural network;ATM;security reinforcement;surveillance techniques;facial images;face localization;face segmentation;feature extraction;feature recognition;facial occlusion detection;convolutional neural networks;multitask learning;large scale face occlusion database","","3","14","","","","","IEEE","IEEE Conferences"
"Is noise always harmful? Visual learning from weakly-related data","S. Zhong; Y. Liu; K. A. Hua; S. Wu","College of Computer Science and Software Engineering, Shenzhen University, China; Department of Computing, The Hong Kong Polytechnic University, China; Department of Computer Science, University of Central Florida, USA; Department of Computing, The Hong Kong Polytechnic, University Hong Kong, China","2015 International Conference on Orange Technologies (ICOT)","","2015","","","181","184","Noise exists universally in multimedia data, especially in Internet era. For example, tags from web users are often incomplete, arbitrary, and low relevant with the visual information. Intuitively, noise in the dataset is harmful to learning tasks, which implies that huge volumes of image tags from social media can't be utilized directly. To collect the reliable training dataset, labor-intensive manual labeling and various learning based outlier detection techniques are widely used. This paper intends to discuss whether such kind of preprocessing is always needed. We focus on a very normal case in image classification that the available dataset includes a large amount of images weakly related to any target classes. We use deep models as the platform and design a series of experiments to compare the semi-supervised learning performance with/without weakly related unlabeled data. Fortunately, we validate that weakly related data is not always harmful, which is an encouraging finding for research on web image learning.","","","10.1109/ICOT.2015.7498518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498518","Weakly-related data;deep learning;semi-supervised learning","Training;Standards;Machine learning;Erbium;Data models;Multimedia communication;Training data","","","","","18","","","","","IEEE","IEEE Conferences"
"A new unsupervised convolutional neural network model for Chinese scene text detection","X. Ren; K. Chen; X. Yang; Y. Zhou; J. He; J. Sun","Institute of Image Communication and Information Processing, Shanghai Jiao Tong University; Institute of Image Communication and Information Processing, Shanghai Jiao Tong University; Institute of Image Communication and Information Processing, Shanghai Jiao Tong University; Institute of Image Communication and Information Processing, Shanghai Jiao Tong University; School of Engineering and Applied Science, Aston University; Institute of Image Communication and Information Processing, Shanghai Jiao Tong University","2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)","","2015","","","428","432","As one of the most popular deep learning models, convolution neural network (CNN) has achieved huge success in image information extraction. Traditionally CNN is trained by supervised learning method with labeled data and used as a classifier by adding a classification layer in the end. Its capability of extracting image features is largely limited due to the difficulty of setting up a large training dataset. In this paper, we propose a new unsupervised learning CNN model, which uses a so-called convolutional sparse auto-encoder (CSAE) algorithm pre-train the CNN. Instead of using labeled natural images for CNN training, the CSAE algorithm can be used to train the CNN with unlabeled artificial images, which enables easy expansion of training data and unsupervised learning. The CSAE algorithm is especially designed for extracting complex features from specific objects such as Chinese characters. After the features of articficial images are extracted by the CSAE algorithm, the learned parameters are used to initialize the first CNN convolutional layer, and then the CNN model is fine-trained by scene image patches with a linear classifier. The new CNN model is applied to Chinese scene text detection and is evaluated with a multilingual image dataset, which labels Chinese, English and numerals texts separately. More than 10% detection precision gain is observed over two CNN models.","","","10.1109/ChinaSIP.2015.7230438","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230438","","Feature extraction;Training;Detection algorithms;Convolutional codes;Unsupervised learning;Machine learning;Neural networks","feature extraction;image classification;neural nets;object detection;unsupervised learning","unsupervised convolutional neural network model;Chinese scene text detection;deep learning models;image information extraction;supervised learning method;labeled data classification;classification layer;image features extraction;unsupervised learning CNN model;convolutional sparse auto-encoder algorithm;CSAE algorithm;scene image patches;linear classifier;multilingual image dataset","","4","21","","","","","IEEE","IEEE Conferences"
"Variational Bayesian PHD Filter with Deep Learning Network Updating for Multiple Human Tracking","P. Feng; W. Wang; S. M. Naqvi; J. Chambers","Commun., Sensors, Signal & Inf. Process. Group, Newcastle Univ., Newcastle upon Tyne, UK; Center for Vision Speech & Signal Process., Univ. of Surrey, Guildford, UK; Commun., Sensors, Signal & Inf. Process. Group, Newcastle Univ., Newcastle upon Tyne, UK; Commun., Sensors, Signal & Inf. Process. Group, Newcastle Univ., Newcastle upon Tyne, UK","2015 Sensor Signal Processing for Defence (SSPD)","","2015","","","1","5","We propose a robust particle probability hypothesis density (PHD) filter where the variational Bayesian method is applied in joint recursive prediction of the state and the time varying measurement noise parameters. The proposed particle PHD filter is based on forming variational approximation to the joint distribution of states and noise parameters at each frame separately; the state is estimated with a particle PHD filter and the measurement noise variances used in the update step are estimated with a fixed point iteration approach. A deep belief network (DBN) is used in the update step to mitigate the effect of measurement noise on the calculation of particle weights in each frame. The deep learning network is trained based on both colour and oriented gradient histogram (HOG) features and then used to mitigate the measurement noise from the particle selection step, thereby improving the tracking performance. Simulation results using sequences from the CAVIAR dataset show the improvements of the proposed DBN aided variational Bayesian particle PHD filter over the traditional particle PHD filter.","","","10.1109/SSPD.2015.7288526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7288526","","Noise;Target tracking;Noise measurement;Atmospheric measurements;Particle measurements;Bayes methods;Approximation methods","Bayes methods;belief networks;image colour analysis;object tracking;particle filtering (numerical methods);prediction theory;recursive estimation","variational Bayesian PHD filter;deep learning network updating;multiple human tracking;particle probability hypothesis density filter;joint recursive prediction;time-varying measurement noise parameter;variational approximation;state joint distribution;deep belief network;colour feature;oriented gradient histogram feature;CAVIAR dataset","","1","18","","","","","IEEE","IEEE Conferences"
"Classifying digestive organs in wireless capsule endoscopy images based on deep convolutional neural network","Y. Zou; L. Li; Y. Wang; J. Yu; Y. Li; W. J. Deng","ADSPLAB/ELIP, School of ECE, Peking University, Shenzhen 518055, China; ADSPLAB/ELIP, School of ECE, Peking University, Shenzhen 518055, China; ADSPLAB/ELIP, School of ECE, Peking University, Shenzhen 518055, China; ADSPLAB/ELIP, School of ECE, Peking University, Shenzhen 518055, China; Shenzhen JiFu Technology Ltd., China; Shenzhen JiFu Technology Ltd., China","2015 IEEE International Conference on Digital Signal Processing (DSP)","","2015","","","1274","1278","This paper studies the classification problem of the digestive organs in wireless capsule endoscopy (WCE) images based on deep convolutional neural network (DCNN) framework. Essentially, DCNN proves having powerful ability to learn layer-wise hierarchy models with huge training data, which works similar to human biological visual systems. Classifying digestive organs in WCE images intuitively means to recognize higher semantic image features. To achieve this, an effective deep CNN-based WCE classification system has been constructed (DCNN-WCE-CS). With about 1 million real WCE images, intensive experiments are conducted to evaluate its performance by setting different network parameters. Results illustrate its superior performance compared to traditional classification methods, where about 95% classification accuracy can be achieved in average. Moreover, it is observed that the DCNN-WCE-CS is robust to the large variations of the WCE images due to the individuals and complex digestive tract circumstance, including the rotation, the luminance change of the WCE images.","","","10.1109/ICDSP.2015.7252086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7252086","wireless capsule endoscopy;digestive organs classification;deep convolutional neural network;parameter selection","Feature extraction;Endoscopes;Intestines;Training;Accuracy;Convolution;Wireless communication","biological organs;biomedical optical imaging;brightness;endoscopes;feature extraction;feedforward neural nets;image classification;learning (artificial intelligence);medical image processing;object recognition","digestive organ classification problem;wireless capsule endoscopy images;deep convolutional neural network;layer-wise hierarchy models;training data;human biological visual systems;semantic image feature recognition;deep CNN-based WCE classification system;DCNN-WCE-CS;complex digestive tract circumstance;luminance change","","14","14","","","","","IEEE","IEEE Conferences"
"Image Recognition Method Using Modular Systems","S. Je; H. Nguyen; J. Lee","NA; NA; NA","2015 International Conference on Computational Science and Computational Intelligence (CSCI)","","2015","","","504","508","In this paper, we implemented an open source based IDE (Integrated Development Environment) that modularizes various deep-learning techniques in different environments. This modularization system can schematize a complex structure deep-learning analysis into a simple form that help user to understand and use very difficult deep learning techniques conveniently.","","","10.1109/CSCI.2015.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424144","deepLearning;Moduler;Caffe;IDE(integrated development environment)","Machine learning;Training;Neural networks;Graphics processing units;Visualization;Mathematical model;Databases","image recognition;learning (artificial intelligence);programming environments;public domain software","image recognition;modular systems;open source based IDE;integrated development environment;deep learning techniques;modularization system","","","13","","","","","IEEE","IEEE Conferences"
"Multi-lingual speech recognition with low-rank multi-task deep neural networks","A. Mohan; R. Rose","Department of Electrical and Computer Engineering, McGill University, Montreal, Canada; Department of Electrical and Computer Engineering, McGill University, Montreal, Canada","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4994","4998","Multi-task learning (MTL) for deep neural network (DNN) multilingual acoustic models has been shown to be effective for learning parameters that are common or shared between multiple languages[1, 2]. In the MTL paradigm, the number of parameters in the output layer is large and scales with the number of languages used in training. This output layer becomes a computational bottleneck. For mono-lingual DNNs, low-rank matrix factorization (LRMF) of weight matrices have yielded large computational savings[3, 4]. The LRMF proposed in this work for MTL, is for the original languagespecific block matrices to “share” a common matrix, with resulting low-rank language specific block matrices. The impact of LRMF is presented in two scenarios, namely : (a) improving performance in a target language when auxiliary languages are included during multi-lingual training; and (b) cross-language transfer to an unseen language with only 1 hour of transcribed training data. A 44% parameter reduction in the final layer, manifests itself in providing a lower memory footprint and faster training times. An experimental study shows that the LRMF multi-lingual DNN provides competitive performance compared to a full-rank multi-lingual DNN in both scenarios.","","","10.1109/ICASSP.2015.7178921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178921","Low-resource speech recognition;Multi-lingual speech recognition;Neural Networks for speech recognition;Multitask Learning","Training;Hidden Markov models;Acoustics;Speech recognition;Speech;Neural networks;Adaptation models","neural nets;speech recognition","multi-lingual speech recognition;low-rank multi-task deep neural networks;multi-task learning;deep neural network multi-lingual acoustic models;DNN multi-lingual acoustic models;learning parameters;MTL paradigm;output layer;computational bottleneck;mono-lingual DNN;low-rank matrix factorization;weight matrices;original language-specific block matrices;low-rank language specific block matrices;LRMF impact;auxiliary languages;cross-language transfer;unseen language;transcribed training data;parameter reduction;lower memory footprint;training times;LRMF multi-lingual DNN;full-rank multi-lingual DNN","","15","26","","","","","IEEE","IEEE Conferences"
"Study of soft sensor modeling based on deep learning","Yujun Lin; W. Yan","Department of automation of Shanghai Jiaotong University, 200240, China; Department of automation of Shanghai Jiaotong University, 200240, China","2015 American Control Conference (ACC)","","2015","","","5830","5835","Soft sensor are widely used to estimate process variables which are difficult to measure online in industrial process control. This paper proposes a new soft sensor modeling method based on a deep learning method, which integrates denoising auto-encoders (DAE) with support vector regression (SVR) method. The denoising auto-encoders are designed to capture robust high-level feature representation of import data and the SVR model is employed to precisely estimate output data based on the feature representation obtained from DAE. In case study, the method combining denoising auto-encoders with support vector regression (DAE-SVR) is applied to the estimation of oxygen-content in flue gasses in ultra-supercritical units. The results show DAE-SVR is a promising modeling method for soft sensors.","","","10.1109/ACC.2015.7172253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7172253","","Support vector machines;Noise reduction;Training;Machine learning;Testing;Computational modeling;Data models","flue gases;process control;regression analysis;sensors;support vector machines","soft sensor modeling;deep learning;industrial process control;denoising auto-encoders;support vector regression;robust high-level feature representation;oxygen-content;flue gasses","","1","25","","","","","IEEE","IEEE Conferences"
"Generating multi-fingered robotic grasps via deep learning","J. Varley; J. Weisz; J. Weiss; P. Allen","NA; NA; NA; NA","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","2015","","","4415","4420","This paper presents a deep learning architecture for detecting the palm and fingertip positions of stable grasps directly from partial object views. The architecture is trained using RGBD image patches of fingertip and palm positions from grasps computed on complete object models using a grasping simulator. At runtime, the architecture is able to estimate grasp quality metrics without the need to explicitly calculate the given metric. This ability is useful as the exact calculation of these quality functions is impossible from an incomplete view of a novel object without any tactile feedback. This architecture for grasp quality prediction provides a framework for generalizing grasp experience from known to novel objects.","","","10.1109/IROS.2015.7354004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7354004","","Training;Machine learning;Heating;Grasping;Image segmentation;Training data;Computer architecture","grippers;haptic interfaces;image colour analysis","multifingered robotic grasps;deep learning architecture;fingertip positions;RGBD image patches;palm positions;object models;grasping simulator;grasp quality metrics;quality functions;tactile feedback;grasp quality prediction","","23","19","","","","","IEEE","IEEE Conferences"
"Stochastic optimization for deep CCA via nonlinear orthogonal iterations","W. Wang; R. Arora; K. Livescu; N. Srebro","Toyota Technological Institute at Chicago 6045 S. Kenwood Ave., IL 60637, United States; Johns Hopkins University 3400 N. Charles St., Baltimore, MD 21218, United States; Toyota Technological Institute at Chicago 6045 S. Kenwood Ave., IL 60637, United States; Toyota Technological Institute at Chicago 6045 S. Kenwood Ave., IL 60637, United States","2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)","","2015","","","688","695","Deep CCA is a recently proposed deep neural network extension to the traditional canonical correlation analysis (CCA), and has been successful for multi-view representation learning in several domains. However, stochastic optimization of the deep CCA objective is not straightforward, because it does not decouple over training examples. Previous optimizers for deep CCA are either batch-based algorithms or stochastic optimization using large minibatches, which can have high memory consumption. In this paper, we tackle the problem of stochastic optimization for deep CCA with small minibatches, based on an iterative solution to the CCA objective, and show that we can achieve as good performance as previous optimizers and thus alleviate the memory requirement.","","","10.1109/ALLERTON.2015.7447071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7447071","","Training;Optimization;Correlation;Feature extraction;Stochastic processes;Convergence;Algorithm design and analysis","iterative methods;learning (artificial intelligence);neural nets;stochastic programming","stochastic optimization;deep CCA;canonical correlation analysis;deep neural network extension;nonlinear orthogonal iterations;multiview representation learning;batch-based algorithm;iterative solution;memory requirement","","12","53","","","","","IEEE","IEEE Conferences"
"The effective diagnosis of schizophrenia by using multi-layer RBMs deep networks","Chen Qiao; D. Lin; Shao-Long Cao; Yu-Ping Wang","School of Mathematics and Statistics, Xi'an Jiaotong University, 710049, China; Department of Biomedical Engineering, Tulane University, New Orleans, LA, 70118, USA; Department of Biomedical Engineering, Tulane University, New Orleans, LA, 70118, USA; Department of Biomedical Engineering, Tulane University, New Orleans, LA, 70118, USA","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","603","606","Schizophrenia is one of the most prevalent mental diseases, and is considered to be caused by the interplay of a number of genetic factors. In this paper, by constructing a multilayer restricted Boltzmann machines (RBMs) deep network, we use the genomic data (i.e., SNP data) for unsupervised feature learning and disease diagnosis of schizophrenia. In order to obtain some more accurate diagnosis results by RBMs, firstly, we transform the SNP data into binary sequences, and then by training the multi-layer RBMs deep network on unlabeled data, the multi-level abstract features of the genomic data are obtained and stored in the network. Finally, by adding a linear classifier to the top of the multi-layer RBMs deep network, the classification results on the testing data are gained. The results show that the average performance of this method is better than that of other methods, e.g., SVM (including linear SVM as well as SVM with multilayer perceptron kernel), sparse representations based classifier and k-nearest neighbors method. It is indicated that the multi-layer RBMs deep network can extract deep hierarchical representations of the genomic data, and then promises a more comprehensive approach for the mental disease diagnosis.","","","10.1109/BIBM.2015.7359751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359751","","Support vector machines;Genomics;Bioinformatics;Quality control","binary sequences;Boltzmann machines;diseases;genomics;patient diagnosis;support vector machines;unsupervised learning","multilayer RBM deep network;genetic factor;multi-layer restricted Boltzmann machines deep network;genomic data;unsupervised feature learning;schizophrenia disease diagnosis;SNP data;binary sequence;linear classifier;SVM;sparse representation-based classifier;k-nearest neighbor method;mental disease diagnosis","","2","16","","","","","IEEE","IEEE Conferences"
"Learning Efficient Sparse and Low Rank Models","P. Sprechmann; A. M. Bronstein; G. Sapiro","Department of Electrical and Computer Engineering, Duke University, Durham, NC; School of Electrical Engineering, Tel Aviv University, Tel Aviv, Israel; Department of Electrical and Computer Engineering, Duke University, Durham, NC","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2015","37","9","1821","1833","Parsimony, including sparsity and low rank, has been shown to successfully model data in numerous machine learning and signal processing tasks. Traditionally, such modeling approaches rely on an iterative algorithm that minimizes an objective function with parsimony-promoting terms. The inherently sequential structure and data-dependent complexity and latency of iterative optimization constitute a major limitation in many applications requiring real-time performance or involving large-scale data. Another limitation encountered by these modeling techniques is the difficulty of their inclusion in discriminative learning scenarios. In this work, we propose to move the emphasis from the model to the pursuit algorithm, and develop a process-centric view of parsimonious modeling, in which a learned deterministic fixed-complexity pursuit process is used in lieu of iterative optimization. We show a principled way to construct learnable pursuit process architectures for structured sparse and robust low rank models, derived from the iteration of proximal descent algorithms. These architectures learn to approximate the exact parsimonious representation at a fraction of the complexity of the standard optimization methods. We also show that appropriate training regimes allow to naturally extend parsimonious models to discriminative settings. State-of-the-art results are demonstrated on several challenging problems in image and audio processing with several orders of magnitude speed-up compared to the exact optimization algorithms.","","","10.1109/TPAMI.2015.2392779","National Science Foundation; ONR; NGA; DARPA; AFOSR; ARO; BSF; ERC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010964","Parsimonious modeling;sparse and low-rank models;NMF;deep learning;real-time implementations;big data;proximal methods;Parsimonious modeling;sparse and low-rank models;NMF;deep learning;real-time implementations;big data;proximal methods","Data models;Dictionaries;Vectors;Robustness;Computational modeling;Encoding;Optimization","","","","49","48","","","","","IEEE","IEEE Journals"
"Self-tuned deep super resolution","Z. Wang; Y. Yang; Z. Wang; S. Chang; W. Han; J. Yang; T. Huang","Beckman Institute, University of Illinois at Urbana-Champaign, 61801, USA; Beckman Institute, University of Illinois at Urbana-Champaign, 61801, USA; Adobe Systems Inc, San Jose, CA 95110, USA; Beckman Institute, University of Illinois at Urbana-Champaign, 61801, USA; Beckman Institute, University of Illinois at Urbana-Champaign, 61801, USA; Snapchat Inc, Venice, CA 90291, USA; Beckman Institute, University of Illinois at Urbana-Champaign, 61801, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","","2015","","","1","8","Deep learning has been successfully applied to image super resolution (SR). In this paper, we propose a deep joint super resolution (DJSR) model to exploit both external and self similarities for SR. A Stacked Denoising Convolutional Auto Encoder (SDCAE) is first pre-trained on external examples with proper data augmentations. It is then fine-tuned with multi-scale self examples from each input, where the reliability of self examples is explicitly taken into account. We also enhance the model performance by sub-model training and selection. The DJSR model is extensively evaluated and compared with state-of-the-arts, and show noticeable performance improvements both quantitatively and perceptually on a wide range of images.","","","10.1109/CVPRW.2015.7301266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301266","","Yttrium;Adaptation models;Training;Image resolution;Joints;Convolutional codes;Pediatrics","image coding;image denoising;image resolution;learning (artificial intelligence)","self-tuned deep super resolution;deep learning;deep joint super resolution model;DJSR model;stacked denoising convolutional auto encoder;SDCAE","","34","28","","","","","IEEE","IEEE Conferences"
"Recognize complex events from static images by fusing deep channels","Yuanjun Xiong; Kai Zhu; Dahua Lin; X. Tang","Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","1600","1609","A considerable portion of web images capture events that occur in our personal lives or social activities. In this paper, we aim to develop an effective method for recognizing events from such images. Despite the sheer amount of study on event recognition, most existing methods rely on videos and are not directly applicable to this task. Generally, events are complex phenomena that involve interactions among people and objects, and therefore analysis of event photos requires techniques that can go beyond recognizing individual objects and carry out joint reasoning based on evidences of multiple aspects. Inspired by the recent success of deep learning, we formulate a multi-layer framework to tackle this problem, which takes into account both visual appearance and the interactions among humans and objects, and combines them via semantic fusion. An important issue arising here is that humans and objects discovered by detectors are in the form of bounding boxes, and there is no straightforward way to represent their interactions and incorporate them with a deep network. We address this using a novel strategy that projects the detected instances onto multi-scale spatial maps. On a large dataset with 60, 000 images, the proposed method achieved substantial improvement over the state-of-the-art, raising the accuracy of event recognition by over 10%.","","","10.1109/CVPR.2015.7298768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298768","","Detectors;Visualization;Image recognition;Semantics;Face;Videos;Feature extraction","image fusion;inference mechanisms;Internet;learning (artificial intelligence);object recognition","complex event recognition;static images;deep channel fusion;Web images;social activities;personal lives;event photos;joint reasoning;deep learning;multilayer framework;semantic fusion;bounding boxes;deep network;multiscale spatial maps","","5","41","","","","","IEEE","IEEE Conferences"
"DeepContour: A deep convolutional feature learned by positive-sharing loss for contour detection","Wei Shen; Xinggang Wang; Yan Wang; Xiang Bai; Z. Zhang","Key Lab of Specialty Fiber Optics and Optical Access Networks, Shanghai University, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Hongshan, Wuhan, Hubei, China; Rapid-Rich Object Search Lab, Nanyang Technological University, Singapore 639798; School of Electronic Information and Communications, Huazhong University of Science and Technology, Hongshan, Wuhan, Hubei, China; Key Lab of Specialty Fiber Optics and Optical Access Networks, Shanghai University, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3982","3991","Contour detection serves as the basis of a variety of computer vision tasks such as image segmentation and object recognition. The mainstream works to address this problem focus on designing engineered gradient features. In this work, we show that contour detection accuracy can be improved by instead making the use of the deep features learned from convolutional neural networks (CNNs). While rather than using the networks as a blackbox feature extractor, we customize the training strategy by partitioning contour (positive) data into subclasses and fitting each subclass by different model parameters. A new loss function, named positive-sharing loss, in which each subclass shares the loss for the whole positive class, is proposed to learn the parameters. Compared to the sofmax loss function, the proposed one, introduces an extra regularizer to emphasizes the losses for the positive and negative classes, which facilitates to explore more discriminative features. Our experimental results demonstrate that learned deep features can achieve top performance on Berkeley Segmentation Dataset and Benchmark (BSDS500) and obtain competitive cross dataset generalization result on the NYUD dataset.","","","10.1109/CVPR.2015.7299024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299024","","Shape;Feature extraction;Training;Standards;Machine learning;Neural networks;Data models","computer vision;convolution;feature extraction;image segmentation;neural nets;visual databases","deep convolutional feature;positive-sharing loss;DeepContour;image segmentation;object recognition;computer vision tasks;contour detection accuracy;engineered gradient features;convolutional neural networks;CNN;blackbox feature extractor;training strategy;sofmax loss function;negative classes;positive classes;discriminative features;Berkeley segmentation dataset and benchmark;BSDS500;NYUD dataset;cross dataset generalization","","26","48","","","","","IEEE","IEEE Conferences"
"The 3-dimensional medical image recognition of right and left kidneys by deep GMDH-type neural network","T. Kondo; S. Takao; J. Ueno","Graduate School of Health Sciences, Tokushima University, 3-18-15 Kuramoto-cho Tokushima, 770-8509 Japan; Graduate School of Health Sciences, Tokushima University, 3-18-15 Kuramoto-cho Tokushima, 770-8509 Japan; Graduate School of Health Sciences, Tokushima University, 3-18-15 Kuramoto-cho Tokushima, 770-8509 Japan","2015 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)","","2015","","","313","320","In this study, the deep multi-layered Group Method of Data Handling (GMDH)-type neural network algorithm using principal component-regression analysis is applied to recognition problems of the right and left kidney regions. The deep multi-layered GMDH-type neural network algorithm can automatically organize the deep neural network architectures which have many hidden layers and these deep neural networks can identify the characteristics of very complex nonlinear systems. The architecture of the deep neural network with many hidden layers is automatically organized using the heuristic self-organization method, so as to minimize the prediction error criterion defined as Akaike's information criterion (AIC) or Prediction Sum of Squares (PSS). The heuristic self-organization method is a type of the evolutional computation. In this deep GMDH-type neural network, principal component-regression analysis is used as the learning algorithm of the weights in the deep GMDH-type neural network, and multi-colinearity does not occur and stable and accurate prediction values are obtained. This new algorithm is applied to the medical image recognitions of the right and left kidney regions. The optimum neural network architectures, which fit the complexity of the right and left kidney regions, are automatically organized and the right and left kidney regions are automatically recognized and extracted by the organized deep GMDH-type neural networks. The recognition results are compared with the conventional sigmoid function neural network trained using back propagation method and it is shown that this deep GMDH-type neural networks are useful for the medical image recognition problems of the right and left kidney regions.","","","10.1109/ICIIBMS.2015.7439548","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439548","Neural network;GMDH;Medical image recognition;Evolutional computation;Deep neural network","Biological neural networks;Neurons;Computer architecture;Input variables;Kidney;Biomedical imaging;Algorithm design and analysis","data handling;feature extraction;kidney;learning (artificial intelligence);medical image processing;neural nets;principal component analysis;regression analysis","medical image recognition;kidney region extraction;group method of data handling;GMDH-type neural network;principal component analysis;regression analysis;deep neural network","","4","10","","","","","IEEE","IEEE Conferences"
"Image annotation via deep neural network","S. Chengjian; S. Zhu; Z. Shi","School of Automation, Nanjing University of Posts and Telecommunications, Nanjing 210046, China; School of Automation, Nanjing University of Posts and Telecommunications, Nanjing 210046, China; School of Automation, Nanjing University of Posts and Telecommunications, Nanjing 210046, China","2015 14th IAPR International Conference on Machine Vision Applications (MVA)","","2015","","","518","521","Multilabel image annotation is one of the most important open problems in computer vision field. Unlike existing works that usually use conventional visual features to annotate images, features based on deep learning have shown potential to achieve outstanding performance. In this work, we propose a multimodal deep learning framework, which aims to optimally integrate multiple deep neural networks pretrained with convolutional neural networks. In particular, the proposed framework explores a unified two-stage learning scheme that consists of (i) learning to fune-tune the parameters of deep neural network with respect to each individual modality, and (ii) learning to find the optimal combination of diverse modalities simultaneously in a coherent process. Experiments conducted on the NUS-WIDE dataset evaluate the performance of the proposed framework for multilabel image annotation, in which the encouraging results validate the effectiveness of the proposed algorithms.","","","10.1109/MVA.2015.7153244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153244","","Neural networks;Feature extraction;Pattern recognition;Image classification;Computer vision;Visualization;Computer architecture","computer vision;feature extraction;neural nets","multilabel image annotation;computer vision;conventional visual features;multimodal deep learning framework;multiple deep neural networks;convolutional neural networks;unified two-stage learning scheme;optimal combination;diverse modality;coherent process;NUS-WIDE dataset","","2","20","","","","","IEEE","IEEE Conferences"
"Deep neural networks for recognizing online handwritten mathematical symbols","H. Dai Nguyen; A. D. Le; M. Nakagawa","Tokyo University of Agriculture and Technology, 2-24-16 Nakacho, Koganei-shi, Tokyo; Tokyo University of Agriculture and Technology, 2-24-16 Nakacho, Koganei-shi, Tokyo; Tokyo University of Agriculture and Technology, 2-24-16 Nakacho, Koganei-shi, Tokyo","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","","2015","","","121","125","This paper presents application of deep learning to recognize online handwritten mathematical symbols. Recently various deep learning architectures such as Convolution neural network (CNN), Deep neural network (DNN) and Long short term memory (LSTM) RNN have been applied to fields such as computer vision, speech recognition and natural language processing where they have been shown to produce state-of-the-art results on various tasks. In this paper, we apply max-out-based CNN and BLSTM to image patterns created from online patterns and to the original online patterns, respectively and combine them. We also compare them with traditional recognition methods which are MRF and MQDF by carrying out some experiments on CROHME database.","","","10.1109/ACPR.2015.7486478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486478","","Feature extraction;Context;Hidden Markov models;Character recognition;Handwriting recognition;Convolutional codes","feedforward neural nets;handwritten character recognition;learning (artificial intelligence);mathematics computing;optical character recognition;recurrent neural nets","deep neural networks;online handwritten mathematical symbols recognition;deep learning architectures;convolution neural network;DNN;long short term memory RNN;LSTM RNN;computer vision;speech recognition;natural language processing;max-out-based CNN;max-out-based BLSTM;CROHME database","","8","14","","","","","IEEE","IEEE Conferences"
"RGB-D Object Recognition via Incorporating Latent Data Structure and Prior Knowledge","J. Tang; L. Jin; Z. Li; S. Gao","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, P. R. China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, P. R. China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, P. R. China; School of Electronic and Information Engineering, ShanghaiTech University, Shanghai, P. R. China","IEEE Transactions on Multimedia","","2015","17","11","1899","1908","For the task of RGB-D object recognition, it is important to identify suitable representations of images, which can boost the performance of object recognition. In this work, we propose a novel representation learning method for RGB-D images by jointly incorporating the underlying data structure and the prior knowledge of the data. Specifically, the convolutional neural networks (CNN) are employed to learn image representation by exploiting the underlying data structure. To handle the problem of the limited RGB and depth images for object recognition, the multi-level hierarchies of features trained on ImageNet from the CNN are transferred to learn rich generic feature representation for RGB and depth images while the labeled images are leveraged. On the other hand, we propose a novel deep auto-encoders (DAE) to exploit the prior knowledge, which can overcome the expensive computational cost of optimization in feature encoding. The expected representations of images are obtained by integrating the two types of image representations. To verify the effectiveness of the proposed method, we thoroughly conduct extensive experiments on two publicly available RGB-D datasets. The encouraging experimental results compared with the state-of-the-art approaches demonstrate the advantages of the proposed method.","","","10.1109/TMM.2015.2476660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7239585","Deep learning;RGB-D object recognition;transfer learning","Object recognition;Image representation;Feature extraction;Encoding;Image coding;Visualization;Data structures","data structures;image representation;learning (artificial intelligence);object recognition","RGB-D object recognition;latent data structure;learning method;convolutional neural networks;CNN;image representation;depth images;multilevel hierarchies;ImageNet;deep auto-encoders;DAE","","23","46","","","","","IEEE","IEEE Journals"
"Effective learning through meaning construction in digital role playing games","K. P. Jantke; T. Hume","Fraunhofer Institute for Digital Media Technology, Erich-K&#x00E4;stner-Stra&#x00DF;e 1a, 99094 Erfurt, Germany; Universal Learning Games, Framtidsv&#x00E4;gen 12A, 352 57 V&#x00E4;xj&#x00F6;, Sweden","2015 IEEE International Conference on Consumer Electronics (ICCE)","","2015","","","653","656","Game-based learning (GBL) is promising, almost as promising as learning when sleeping. But why does it work? And how to implement it? For which purpose is it appropriate? And when is GBL doomed to fail? Serious games design is an art. How can game designers and developers utilize deep and well-established results of the humanities to make the design and implementation of digital games for learning a routine process? The understanding of digital role-playing based on theories of social psychology such as symbolic interactionism leads to the authors' original generic approach to game-based learning design. A serious game has already been designed, implemented, and evaluated for language learning that runs on conventional computers, as well as on varying mobile devices such as tablets. The stage is set for a series of similar role-playing games which allow for learning in largely varying domains.","","","10.1109/ICCE.2015.7066566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7066566","Game-based learning;role playing;game design","Games;Computers;Media;Conferences;Consumer electronics;Art;Psychology","computer aided instruction;linguistics;psychology;serious games (computing)","meaning construction;digital role playing games;GBL;serious game design;social psychology;symbolic interactionism;game-based learning design;language learning;mobile devices;tablets","","2","33","","","","","IEEE","IEEE Conferences"
"Deep BLSTM neural networks for unconstrained continuous handwritten text recognition","V. Frinken; S. Uchida","Faculty of Information Science and Electrical Engineering, Kyushu University, Japan; Faculty of Information Science and Electrical Engineering, Kyushu University, Japan","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","911","915","Recently, two different trends in neural network-based machine learning could be observed. The first one are the introduction of Bidirectional Long Short-Term Memory (BLSTM) neural networks (NN) which made sequences with long-distant dependencies amenable for neural network-based processing. The second one are deep learning techniques, which greatly increased the performance of neural networks, by making use of many hidden layers. In this paper, we propose to combine these two ideas for the task of unconstrained handwriting recognition. Extensive experimental evaluation on the IAM database demonstrate an increase of the recognition performance when using deep learning approaches over commonly used BLSTM neural networks, as well as insight into how different types of hidden layers affect the recognition accuracy.","","","10.1109/ICDAR.2015.7333894","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333894","","Computers;Error analysis;Information science;Logic gates;Chlorine;Training;Pattern recognition","handwritten character recognition;image sequences;learning (artificial intelligence);neural nets;text detection","deep BLSTM neural networks;unconstrained continuous handwritten text recognition;bidirectional long short-term memory NN;sequences;deep learning techniques;lAM database","","6","26","","","","","IEEE","IEEE Conferences"
"How to measure qualitative data","S. L. Vadim","Institute for Information Transmission Problems RAS, Moscow, Russian Federation","2015 Annual Conference of the North American Fuzzy Information Processing Society (NAFIPS) held jointly with 2015 5th World Conference on Soft Computing (WConSC)","","2015","","","1","4","To overcome some problems with deep understanding of fuzzy values, certain learning finite automaton was put into a fuzzy environment. Previously such a device has been studied in the probabilistic environment, where the classic technique of standard Markov chains was applicable. The new study became possible due to several previous results by the present author, namely the axiomatic of fuzzy evidences accumulation and the theory of generalized Markov chains. The mathematical results, obtained in the paper, prove that the learning automaton has the property of asymptotic optimality. We propose to use this property for measuring membership functions in case of values analogous to singletons or point functions. It is claimed that the obtained results might lead to a fuzzy value measurement procedure resembling the statistics in probability area.","","","10.1109/NAFIPS-WConSC.2015.7284202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284202","fuzzy environment;probabilistic environment;finite automata with learning;asymptotic optimality;generalized Markov chain;fuzzy sungletons","Learning automata;Markov processes;Automata;Probabilistic logic;Man machine systems;Probability;Fuzzy sets","fuzzy set theory;learning automata;Markov processes","qualitative data measure;learning finite automaton;fuzzy environment;fuzzy evidences accumulation;generalized Markov chains;asymptotic optimality;membership functions;singletons;point functions;fuzzy value measurement","","1","16","","","","","IEEE","IEEE Conferences"
"Detecting actionable items in meetings by convolutional deep structured semantic models","Y. Chen; D. Hakkani-Tür; X. He","Carnegie Mellon University, Pittsburgh, PA; Microsoft Research, Redmond, WA; Microsoft Research, Redmond, WA","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","375","382","The recent success of voice interaction with smart devices (human-machine genre) and improvements in speech recognition for conversational speech show the possibility of conversation-related applications. This paper investigates the task of actionable item detection in meetings (human-human genre), where the intelligent assistant dynamically provides the participants access to information (e.g. scheduling a meeting, taking notes) without interrupting the meetings. A convolutional deep structured semantic model (CDSSM) is applied to learn the latent semantics for human actions and utterances from human-machine (source genre) and human-human (target) interactions. Furthermore, considering the mismatch between source and target genre and scarcity of annotated data sets for the target genre, we develop adaptation techniques that adjust the learned embeddings to better fit the target genre. Experiments show that CDSSM performs better for actionable item detection compared to baselines using lexical features (27.5% relative) and other semantic features (15.9% relative) when the source genre and target genre match with each other. When the target genre mismatches with the source genre, our proposed adaptation techniques further improve the performance. The discussion and analysis of the experiments provide a reasonable direction for such an actionable item detection task1.","","","10.1109/ASRU.2015.7404819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404819","Actionable item;Convolotional Deep Structured Semantic Model (CDSSM);embeddings;adaptation","Semantics;Feature extraction;Adaptation models;Man machine systems;Electronic mail;Training;Data models","learning (artificial intelligence);speech recognition","actionable item detection;convolutional deep structured semantic models;voice interaction;speech recognition;conversational speech;CDSSM;latent semantics learning;human-machine interaction;human-human interaction","","2","36","","","","","IEEE","IEEE Conferences"
"Reinforcement learning approach to learning human experience in tuning cavity filters","Z. Wang; J. Yang; J. Hu; W. Feng; Y. Ou","Guangdong Provincial Key Laboratory of Robotics and Intelligent System, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, 518055 Shenzhen, P.R. China; Shenzhen College of Advanced Technology, University of Chinese Academy of Sciences, 518055 Shenzhen, P.R. China; Shenzhen College of Advanced Technology, University of Chinese Academy of Sciences, 518055 Shenzhen, P.R. China; Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, 518055 Shenzhen, P.R. China; Shenzhen College of Advanced Technology, University of Chinese Academy of Sciences, 518055 Shenzhen, P.R. China","2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)","","2015","","","2145","2150","Owing to the rapid development of the communication industry, various kinds of radio frequency components are in great demand and put into mass production. Among them, passive devices such as microwave cavity filters, duplexers and combiners have experienced fast and unexpected upgrades. However, the tuning process of these products, which is always manually operated, still seems hard to be automatically replaced or improved because of the difficulties in extracting human experience. In this study, we make deep investigations into some previous automatic cavity filter tuning solutions, especially the ones using intelligent algorithms. In addition, we propose the method of intelligent tuning based on the reinforcement learning algorithm which dynamically extracts the human strategies during the tuning process. The experimental results prove the powerful performance of reinforcement learning in mastering human skills.","","","10.1109/ROBIO.2015.7419091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7419091","","Tuning;Scattering parameters;Fasteners;Cavity resonators;Games;Robots;Industries","electronic engineering computing;learning (artificial intelligence);microwave filters;radiofrequency filters","human experience learning;communication industry;radio frequency components;mass production;passive devices;microwave cavity filters;duplexers;combiners;tuning process;automatic cavity filter tuning solutions;intelligent tuning;reinforcement learning algorithm;human strategies;human skills","","4","17","","","","","IEEE","IEEE Conferences"
"A novel single channel speech enhancement based on joint Deep Neural Network and Wiener Filter","Wei Han; Xiongwei Zhang; Gang Min; Xingyu Zhou","PLA University of Science and Technology, Nanjing 210007, China; PLA University of Science and Technology, Nanjing 210007, China; PLA University of Science and Technology, Nanjing 210007, China; PLA University of Science and Technology, Nanjing 210007, China","2015 IEEE International Conference on Progress in Informatics and Computing (PIC)","","2015","","","163","167","In this paper, we present a novel single channel speech enhancement method based on joint Deep Neural Network (DNN) and Wiener Filter as a whole network named Wiener Deep Neural Network (WDNN). The proposed method contains two stages: the training stage and the enhancement stage. In the training stage, WDNN predicts the clean speech magnitude spectra and the noise magnitude spectra from noisy speech features simultaneously. Then, the Wiener filter is placed on top of the two output of the neural network as an extra layer to generate the enhanced speech magnitude spectra. Finally, we use the phase of noisy speech to reconstruct clean speech. In the enhancement stage, the well-trained WDNN is fed with the features of noisy speech in order to obtain the enhanced speech. Extensive experimental results show that the proposed method outperforms state-of-the-art methods such as the non-negative matrix factorization (NMF) and the tradition DNN methods.","","","10.1109/PIC.2015.7489830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489830","speech enhancement;Deep Neural Network;Wiener Filter","Wiener filters;Noise measurement;Speech;Signal to noise ratio;Public transportation;Speech enhancement;Mathematical model","learning (artificial intelligence);neural nets;speech enhancement;Wiener filters","novel single channel speech enhancement method;joint deep neural network;Wiener filter;Wiener deep neural network;WDNN;training stage;enhancement stage;clean speech magnitude spectra prediction;noise magnitude spectra prediction;noisy speech features","","2","14","","","","","IEEE","IEEE Conferences"
"Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?","P. Khorrami; T. L. Paine; T. S. Huang","NA; NA; NA","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","19","27","Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN's predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject's facial movements.","","","10.1109/ICCVW.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406361","","Face;Face recognition;Training;Emotion recognition;Databases;Biological neural networks;Benchmark testing","data visualisation;emotion recognition;face recognition;feedforward neural nets;filters;image classification;learning (artificial intelligence)","filter visualization;FAU;convolutional layers;spatial pattern visualization;TFD;Toronto face dataset;CK+ dataset;extended Cohn-Kanade dataset;zero-bias CNN training;convolutional neural networks;appearance-based classifier;expression recognition;facial action unit learning;deep neural network","","66","34","","","","","IEEE","IEEE Conferences"
"Deeply learned attributes for crowded scene understanding","J. Shao; K. Kang; C. C. Loy; X. Wang","Department of Electronic Engineering, The Chinese University of Hong Kong, China; Department of Electronic Engineering, The Chinese University of Hong Kong, China; Department of Information Engineering, The Chinese University of Hong Kong, China; Department of Electronic Engineering, The Chinese University of Hong Kong, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","4657","4666","Crowded scene understanding is a fundamental problem in computer vision. In this study, we develop a multi-task deep model to jointly learn and combine appearance and motion features for crowd understanding. We propose crowd motion channels as the input of the deep model and the channel design is inspired by generic properties of crowd systems. To well demonstrate our deep model, we construct a new large-scale WWW Crowd dataset with 10, 000 videos from 8, 257 crowded scenes, and build an attribute set with 94 attributes on WWW. We further measure user study performance on WWW and compare this with the proposed deep models. Extensive experiments show that our deep models display significant performance improvements in cross-scene attribute recognition compared to strong crowd-related feature-based baselines, and the deeply learned features behave a superior performance in multi-task learning.","","","10.1109/CVPR.2015.7299097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299097","","Videos;World Wide Web;Time factors;Accuracy;Tracking;Computational modeling;Stability analysis","computer vision;image motion analysis;image recognition;learning (artificial intelligence)","deeply learned attributes;crowded scene understanding;computer vision;multitask deep model;motion features;crowd motion channels;large-scale WWW Crowd dataset;cross-scene attribute recognition;multitask learning","","84","49","","","","","IEEE","IEEE Conferences"
"Learning Concept Embeddings with Combined Human-Machine Expertise","M. J. Wilber; I. S. Kwak; D. Kriegman; S. Belongie","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","981","989","This paper presents our work on ""SNaCK,"" a low-dimensional concept embedding algorithm that combines human expertise with automatic machine similarity kernels. Both parts are complimentary: human insight can capture relationships that are not apparent from the object's visual similarity and the machine can help relieve the human from having to exhaustively specify many constraints. We show that our SNaCK embeddings are useful in several tasks: distinguishing prime and nonprime numbers on MNIST, discovering labeling mistakes in the Caltech UCSD Birds (CUB) dataset with the help of deep-learned features, creating training datasets for bird classifiers, capturing subjective human taste on a new dataset of 10,000 foods, and qualitatively exploring an unstructured set of pictographic characters. Comparisons with the state-of-the-art in these tasks show that SNaCK produces better concept embeddings that require less human supervision than the leading methods.","","","10.1109/ICCV.2015.118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410475","","Kernel;Visualization;Labeling;Birds;Crowdsourcing;Machine learning;Computer vision","feature extraction;image classification;learning (artificial intelligence)","human-machine expertise;low-dimensional concept embedding algorithm;automatic machine similarity kernels;human insight;object visual similarity;SNaCK embeddings;nonprime numbers;MNIST;labeling mistakes discovery;Caltech UCSD Birds;CUB dataset;deep-learned features;bird classifiers;pictographic characters;supervised learning","","7","42","","","","","IEEE","IEEE Conferences"
"A Combination of Multi-state Activation Functions, Mean-normalisation and Singular Value Decomposition for learning Deep Neural Networks","Chenghao Cai; Dengfeng Ke; Yanyan Xu; Kaile Su","School of Technology, Beijing Forestry University, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Information, Science and Technology, Beijing Forestry University, China; Institute for Integrated and Intelligent Systems, Griffith University, Brisbane, QLD, Australia","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","In this paper, we propose Multi-state Activation Functions (MSAFs) for Deep Neural Networks (DNNs). These multi-state functions do extra classification based on the 2-state Logistic function. Discussions on the MSAFs reveal that these activation functions have potentials for altering the parameter distribution of the DNN models, improving model performances and reducing model sizes. Meanwhile, an extension of the XOR problem indicates how neural networks with the multistate functions facilitate classifying patterns. Furthermore, basing on running average mean-normalisation rules, we actualise a combination of mean-normalised optimisation with the MSAFs as well as Singular Value Decomposition (SVD). Experimental results on TIMIT reveal that acoustic models based on DNNs can be improved by applying the MSAFs. The models obtain better phone error rates when the Logistic function is replaced with the multi-state functions. Further experiments on large vocabulary continuous speech recognition tasks reveal that the MSAFs and mean-normalised Stochastic Gradient Descent (MN-SGD) bring better recognition performances for DNNs in comparison with the conventional Logistic function and SGD learning method. Beyond this, the combination of the MSAFs, the SVD method and MN-SGD shrinks the parameter scales of DNNs to 44% approximately, leading to considerable increasing on decoding speed and decreasing on model sizes without any loss of recognition performances.","","","10.1109/IJCNN.2015.7280321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280321","","Optimization;Computational modeling;Manganese;Logistics","neural nets;pattern classification;singular value decomposition","multistate activation functions;singular value decomposition;deep neural network learning;MSAFs;deep neural networks;DNNs;2-state logistic function;XOR problem;multistate functions;pattern classification;average mean-normalisation rules;mean-normalised optimisation;SVD;TIMIT;acoustic models;continuous speech recognition tasks;mean-normalised stochastic gradient descent;MN-SGD","","","25","","","","","IEEE","IEEE Conferences"
"Deep learning and recurrent connectionist-based approaches for Arabic text recognition in videos","S. Yousfi; S. Berrani; C. Garcia","Orange Labs - France Telecom, 35510 Cesson-Sévigné, France; Orange Labs - France Telecom, 35510 Cesson-Sévigné, France; University of Lyon, INSA-Lyon, LIRIS, UMR5205 CNRS, 69621 Villeurbanne, France","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","1026","1030","This paper focuses on recognizing Arabic embedded text in videos. The proposed methods proceed without applying any prior pre-processing operations or character segmentation. Difficulties related to the video or text properties are faced using a learned robust representation of the input text image. This is performed using Convolutional Neural Networks and Deep Auto-Encoders. Features are computed using a multi-scale sliding window scheme. A connectionist recurrent approach is then used. It is trained to predict correct transcriptions of the input image from the associated sequence of features. Proposed methods are extensively evaluated on a large video database recorded from several Arabic TV channels.","","","10.1109/ICDAR.2015.7333917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333917","","Hidden Markov models","feature extraction;image recognition;image representation;image sequences;natural language processing;text analysis;video signal processing;visual databases","recurrent connectionist;deep learning;video Arabic text recognition;Arabic embedded text;character segmentation;input text image;convolutional neural networks;deep autoencoders;multiscale sliding window scheme;associated sequence;video database;Arabic TV channels","","12","20","","","","","IEEE","IEEE Conferences"
"AgeNet: Deeply Learned Regressor and Classifier for Robust Apparent Age Estimation","X. Liu; S. Li; M. Kan; J. Zhang; S. Wu; W. Liu; H. Han; S. Shan; X. Chen","Key Lab. of Intell. Inf. Process., Inst. of Comput. Technol., Beijing, China; Tencent BestImage Team, Shanghai, China; Key Lab. of Intell. Inf. Process., Inst. of Comput. Technol., Beijing, China; Key Lab. of Intell. Inf. Process., Inst. of Comput. Technol., Beijing, China; Key Lab. of Intell. Inf. Process., Inst. of Comput. Technol., Beijing, China; Key Lab. of Intell. Inf. Process., Inst. of Comput. Technol., Beijing, China; Key Lab. of Intell. Inf. Process., Inst. of Comput. Technol., Beijing, China; Key Lab. of Intell. Inf. Process., Inst. of Comput. Technol., Beijing, China; Key Lab. of Intell. Inf. Process., Inst. of Comput. Technol., Beijing, China","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","258","266","Apparent age estimation from face image has attracted more and more attentions as it is favorable in some real-world applications. In this work, we propose an end-to-end learning approach for robust apparent age estimation, named by us AgeNet. Specifically, we address the apparent age estimation problem by fusing two kinds of models, i.e., real-value based regression models and Gaussian label distribution based classification models. For both kind of models, large-scale deep convolutional neural network is adopted to learn informative age representations. Another key feature of the proposed AgeNet is that, to avoid the problem of over-fitting on small apparent age training set, we exploit a general-to-specific transfer learning scheme. Technically, the AgeNet is first pre-trained on a large-scale web-collected face dataset with identity label, and then it is fine-tuned on a large-scale real age dataset with noisy age label. Finally, it is fine-tuned on a small training set with apparent age label. The experimental results on the ChaLearn 2015 Apparent Age Competition demonstrate that our AgeNet achieves the state-of-the-art performance in apparent age estimation.","","","10.1109/ICCVW.2015.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406391","","Estimation;Encoding;Robustness;Neural networks;Face;Aging;Standards","age issues;convolution;face recognition;Gaussian distribution;image classification;learning (artificial intelligence);neural nets;regression analysis","ChaLearn 2015 Apparent Age Competition;Web-collected face dataset;general-to-specific transfer learning scheme;apparent age training set;informative age representation learning;large-scale deep convolutional neural network;Gaussian label distribution based classification model;real-value based regression models;end-to-end learning approach;face image;robust apparent age estimation;classifier;deeply learned regressor;AgeNet","","42","32","","","","","IEEE","IEEE Conferences"
"Exemplar-based speech enhancement for deep neural network based automatic speech recognition","D. Baby; J. F. Gemmeke; T. Virtanen; H. Van hamme","Department ESAT, KU Leuven, Belgium; Department ESAT, KU Leuven, Belgium; Department of Signal Processing, Tampere University of Technology, Finland; Department ESAT, KU Leuven, Belgium","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4485","4489","Deep neural network (DNN) based acoustic modelling has been successfully used for a variety of automatic speech recognition (ASR) tasks, thanks to its ability to learn higher-level information using multiple hidden layers. This paper investigates the recently proposed exemplar-based speech enhancement technique using coupled dictionaries as a pre-processing stage for DNN-based systems. In this setting, the noisy speech is decomposed as a weighted sum of atoms in an input dictionary containing exemplars sampled from a domain of choice, and the resulting weights are applied to a coupled output dictionary containing exemplars sampled in the short-time Fourier transform (STFT) domain to directly obtain the speech and noise estimates for speech enhancement. In this work, settings using input dictionary of exemplars sampled from the STFT, Mel-integrated magnitude STFT and modulation envelope spectra are evaluated. Experiments performed on the AURORA-4 database revealed that these pre-processing stages can improve the performance of the DNN-HMM-based ASR systems with both clean and multi-condition training.","","","10.1109/ICASSP.2015.7178819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178819","deep neural networks;non-negative matrix factorisation;coupled dictionaries;speech enhancement;modulation envelope","Training;Speech recognition;Neural networks;Testing;Computational modeling;Speech","Fourier transforms;hidden Markov models;learning (artificial intelligence);signal denoising;speech enhancement;speech recognition","exemplar-based speech enhancement technique;deep neural network based automatic speech recognition;deep neural network based acoustic modelling;DNN-based systems;multiple hidden layers;preprocessing stage;weighted sum-of-atoms;noisy speech decomposition;coupled output dictionary;short-time Fourier transform domain;mel-integrated magnitude STFT;modulation envelope spectra;AURORA-4 database;DNN-HMM-based ASR systems;multicondition training","","6","21","","","","","IEEE","IEEE Conferences"
"Learning Image and User Features for Recommendation in Social Networks","X. Geng; H. Zhang; J. Bian; T. Chua","Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","4274","4282","Good representations of data do help in many machine learning tasks such as recommendation. It is often a great challenge for traditional recommender systems to learn representative features of both users and images in large social networks, in particular, social curation networks, which are characterized as the extremely sparse links between users and images, and the extremely diverse visual contents of images. To address the challenges, we propose a novel deep model which learns the unified feature representations for both users and images. This is done by transforming the heterogeneous user-image networks into homogeneous low-dimensional representations, which facilitate a recommender to trivially recommend images to users by feature similarity. We also develop a fast online algorithm that can be easily scaled up to large networks in an asynchronously parallel way. We conduct extensive experiments on a representative subset of Pinterest, containing 1,456,540 images and 1,000,000 users. Results of image recommendation experiments demonstrate that our feature learning approach significantly outperforms other state-of-the-art recommendation methods.","","","10.1109/ICCV.2015.486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410843","","Social network services;Sparse matrices;Collaboration;Computer vision;Recommender systems;Machine learning;Matrix decomposition","image representation;learning (artificial intelligence);recommender systems;social networking (online)","recommendation method;feature learning approach;image recommendation experiment;online algorithm;feature similarity;homogeneous low-dimensional representation;heterogeneous user-image network;feature representation;visual content;sparse link;social curation network;representative feature;recommender system;machine learning task;social network recommendation;user feature;learning image","","29","33","","","","","IEEE","IEEE Conferences"
"Learning the Structure of Deep Convolutional Networks","J. Feng; T. Darrell","NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2749","2757","In this work, we develop a novel method for automatically learning aspects of the structure of a deep model, in order to improve its performance, especially when labeled training data are scarce. We propose a new convolutional neural network model with the Indian Buffet Process (IBP) prior, termed ibpCNN. The ibpCNN automatically adapts its structure to provided training data, achieves an optimal balance among model complexity, data fidelity and training loss, and thus offers better generalization performance. The proposed ibpCNN captures complicated data distribution in an unsupervised generative way. Therefore, ibpCNN can exploit unlabeled data -- which can be collected at low cost -- to learn its structure. After determining the structure, ibpCNN further learns its parameters according to specified tasks, in an end-to-end fashion, and produces discriminative yet compact representations. We evaluate the performance of ibpCNN, on fully-and semi-supervised image classification tasks, ibpCNN surpasses standard CNN models on benchmark datasets, with much smaller size and higher efficiency.","","","10.1109/ICCV.2015.315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410672","","Data models;Adaptation models;Training data;Complexity theory;Training;Neural networks;Convolutional codes","computational complexity;image classification;learning (artificial intelligence);neural nets","deep convolutional networks;convolutional neural network model;Indian Buffet Process;IBP prior;ibpCNN;model complexity;data fidelity;training loss;data distribution;semisupervised image classification;CNN models","","16","29","","","","","IEEE","IEEE Conferences"
"SpokenWord identification for Malayalam using Artificial Neural Network","M. Moneykumar; S. Elizabeth","Indian Institute of Information Technology and Management- Kerala, Indian; Indian Institute of Information Technology and Management- Kerala, Indian","2015 International Conference on Computing and Network Communications (CoCoNet)","","2015","","","230","233","This paper focuses on developing a syllable based speaker independent speech recognition for Malayalam language. An ANN model is proposed for an automatic syllabification to understand the isolated word utterances as syllables. The learning was performed with isolated word utterances of multiple speakers after pre-processing. Pre-processing involves noise removal, framing, segmentation, filtering and feature extraction. It is found that ANN shows satisfactory result for neutral and gender independent speech identification with an accuracy of 75% in an average for 4 different experiments. The work has also been extended by proposing a deep learning architecture for Automatic Speech Recognition(ASR) for better accuracy.","","","10.1109/CoCoNet.2015.7411191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7411191","Artificial Neural Network(ANN);Automatic Speech Recognition (ASR);Deep Neural Network(DNN);Segmentation","Speech recognition;Speech;Artificial neural networks;Training;Machine learning;Neurons","feature extraction;filtering theory;neural nets;signal denoising;speech recognition","spoken word identification;Malayalam;artificial neural network;syllable based speaker independent speech recognition;ANN model;automatic syllabification;noise removal;framing;segmentation;filtering;feature extraction;speech identification;deep learning architecture;automatic speech recognition;ASR","","","16","","","","","IEEE","IEEE Conferences"
"An advanced method for pedestrian dead reckoning using BLSTM-RNNs","M. Edel; E. Köppe","Freie Universität Berlin/Mathematics and Computer Science, Germany; BAM Federal Institute for Material Research and Testing, Berlin, Germany","2015 International Conference on Indoor Positioning and Indoor Navigation (IPIN)","","2015","","","1","6","Location estimation and navigation, especially on smartphones has shown great progress in the past decade due to its low cost and ability to work without additional infrastructure. However, a challenge is the positioning, both in terms of step detection, step length approximation as well as heading estimation, which must be accurate and robust, even when the use of the device is varied in terms of placement or orientation. In this paper, we propose a scheme for retrieving relevant information to detect steps and to estimate the correct step length from raw inertial measurement unit (IMU) data. This approach uses Bidirectional Long Short-Term Memory Recurrent Neural Networks (BLSTM-RNNs). Designed to take contextual information into account, the network can process data gathered from different positions, resulting in a system, which is invariant with respect to transformation and distortions of the input patterns. An experimental evaluation on a dataset produced from 10 individuals demonstrates that this new approach achieves significant improvements over previous attempts and increase the current state-of-the-art results even in the presence of variations and degradations. We achieved a mean classification rate of 98.5% and a standard deviation of 0.70 for 10000 different test sequences and an average error of 1.45% regarding the step length. Thus is the best result on the task gathered in the experiments compared with competing techniques.","","","10.1109/IPIN.2015.7346954","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346954","Indoor positioning;inertial tracking;dead reckoning;deep learning;machine learning","Estimation;Legged locomotion;Logic gates;Sensor fusion;Detectors;Yttrium","indoor navigation;information retrieval;learning (artificial intelligence);pedestrians;recurrent neural nets;smart phones","pedestrian dead reckoning;BLSTM-RNN;bidirectional long short-term memory recurrent neural networks;location estimation;location navigation;smartphones;step detection;step length approximation;heading estimation;relevant information retrieval;correct step length estimation;inertial measurement unit data;IMU;indoor positioning;deep learning;machine learning","","6","24","","","","","IEEE","IEEE Conferences"
"A Unified Gradient Regularization Family for Adversarial Examples","C. Lyu; K. Huang; H. Liang","NA; NA; NA","2015 IEEE International Conference on Data Mining","","2015","","","301","309","Adversarial examples are augmented data points generated by imperceptible perturbation of input samples. They have recently drawn much attention with the machine learning and data mining community. Being difficult to distinguish from real examples, such adversarial examples could change the prediction of many of the best learning models including the state-of-the-art deep learning models. Recent attempts have been made to build robust models that take into account adversarial examples. However, these methods can either lead to performance drops or lack mathematical motivations. In this paper, we propose a unified framework to build robust machine learning models against adversarial examples. More specifically, using the unified framework, we develop a family of gradient regularization methods that effectively penalize the gradient of loss function w.r.t. inputs. Our proposed framework is appealing in that it offers a unified view to deal with adversarial examples. It incorporates another recently-proposed perturbation based approach as a special case. In addition, we present some visual effects that reveals semantic meaning in those perturbations, and thus support our regularization method and provide another explanation for generalizability of adversarial examples. By applying this technique to Maxout networks, we conduct a series of experiments and achieve encouraging results on two benchmark datasets. In particular, we attain the best accuracy on MNIST data (without data augmentation) and competitive performance on CIFAR-10 data.","","","10.1109/ICDM.2015.84","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373334","Adversarial examples;Deep learning;Regularization;Robust classification","Robustness;Mathematical model;Predictive models;Approximation methods;Training;Data mining;Optimization","data mining;learning (artificial intelligence)","unified gradient regularization family;augmented data points;imperceptible perturbation;data mining community;deep learning models;mathematical motivations;machine learning models;unified framework;gradient regularization methods;loss function;perturbation based approach;Maxout networks;MNIST data;data augmentation;CIFAR-10 data","","7","23","","","","","IEEE","IEEE Conferences"
"Multi-Task CNN Model for Attribute Prediction","A. H. Abdulnabi; G. Wang; J. Lu; K. Jia","Rapid-Rich Object Search Lab, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Electrical and Electronic Engineering, Room S1-B1c-88, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Automation, Advanced Digital Sciences Center, Tsinghua University, Beijing, China, Singapore; Department of Computer and Information Science, Faculty of Science and Technology, University of Macau, Macau SAR, China","IEEE Transactions on Multimedia","","2015","17","11","1949","1959","This paper proposes a joint multi-task learning algorithm to better predict attributes in images using deep convolutional neural networks (CNN). We consider learning binary semantic attributes through a multi-task CNN model, where each CNN will predict one binary attribute. The multi-task learning allows CNN models to simultaneously share visual knowledge among different attribute categories. Each CNN will generate attribute-specific feature representations, and then we apply multi-task learning on the features to predict their attributes. In our multi-task framework, we propose a method to decompose the overall model's parameters into a latent task matrix and combination matrix. Furthermore, under-sampled classifiers can leverage shared statistics from other classifiers to improve their performance. Natural grouping of attributes is applied such that attributes in the same group are encouraged to share more knowledge. Meanwhile, attributes in different groups will generally compete with each other, and consequently share less knowledge. We show the effectiveness of our method on two popular attribute datasets.","","","10.1109/TMM.2015.2477680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7254184","Deep CNN;latent tasks matrix;multi-task learning;semantic attributes","Matrix decomposition;Visualization;Training;Semantics;Predictive models","image classification;learning (artificial intelligence);matrix algebra;neural nets","multitask CNN model;joint multitask learning algorithm;deep convolutional neural networks;learning binary semantic attributes;attribute-specific feature representations;latent task matrix;combination matrix;under-sampled classifiers","","81","62","","","","","IEEE","IEEE Journals"
"Where to Buy It: Matching Street Clothing Photos in Online Shops","M. H. Kiapour; X. Han; S. Lazebnik; A. C. Berg; T. L. Berg","Univ. of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Univ. of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Univ. of North Carolina at Chapel Hill, Chapel Hill, NC, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","3343","3351","In this paper, we define a new task, Exact Street to Shop, where our goal is to match a real-world example of a garment item to the same item in an online shop. This is an extremely challenging task due to visual differences between street photos (pictures of people wearing clothing in everyday uncontrolled settings) and online shop photos (pictures of clothing items on people, mannequins, or in isolation, captured by professionals in more controlled settings). We collect a new dataset for this application containing 404,683 shop photos collected from 25 different online retailers and 20,357 street photos, providing a total of 39,479 clothing item matches between street and shop photos. We develop three different methods for Exact Street to Shop retrieval, including two deep learning baseline methods, and a method to learn a similarity measure between the street and shop domains. Experiments demonstrate that our learned similarity significantly outperforms our baselines that use existing deep learning based representations.","","","10.1109/ICCV.2015.382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410739","","Clothing;Machine learning;Computer vision;Lighting;Image retrieval;Visualization;Image color analysis","clothing;image matching;image representation;image retrieval;Internet;learning (artificial intelligence);retail data processing","street clothing photo matching;visual differences;online shop photos;online retailers;exact street to shop retrieval;deep learning baseline method;deep learning based representations","","102","41","","","","","IEEE","IEEE Conferences"
"A pairwise algorithm for pitch estimation and speech separation using deep stacking network","H. Zhang; X. Zhang; S. Nie; G. Gao; W. Liu","Computer Science Department, Inner Mongolia University, Hohhot, China, 010021; Computer Science Department, Inner Mongolia University, Hohhot, China, 010021; National Laboratory of Patten Recognition (NLPR), Institute of Automation, University of Chinese Academy of Sciences, Beijing, China, 100190; Computer Science Department, Inner Mongolia University, Hohhot, China, 010021; National Laboratory of Patten Recognition (NLPR), Institute of Automation, University of Chinese Academy of Sciences, Beijing, China, 100190","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","246","250","Pitch information is an important cue for speech separation. However, pitch estimation in noisy condition is also a task as challenging as speech separation. In this paper, we propose a supervised learning architecture which combines these two problems concisely. The proposed algorithm is based on deep stacking network (DSN) which provides a method of stacking simple processing modules in building deep architecture. In the training stage, an ideal binary mask is used as target. The input vector includes the outputs of lower module and frame-level features which consist of spectral and pitch-based features. In the testing stage, each module provides an estimated binary mask which is employed to re-estimate pitch. Then we update the pitch-based features to the next module. This procedure is embedded iteratively in DSN, and we obtain the final separation results from the last module of DSN. Systematic evaluations show that the proposed approach produces high quality estimated binary mask and outperforms recent systems in generalization.","","","10.1109/ICASSP.2015.7177969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177969","Speech separation;Pitch estimation;Computational auditory scene analysis;Supervised learning","Noise;Testing;Training;Speech","learning (artificial intelligence);speech processing","pairwise algorithm;pitch estimation;speech separation;deep stacking network;supervised learning architecture;binary mask","","3","18","","","","","IEEE","IEEE Conferences"
"A New Pan-Sharpening Method With Deep Neural Networks","W. Huang; L. Xiao; Z. Wei; H. Liu; S. Tang","Sch. of Comput. Sci. & Eng., Nanjing Univ. of Sci. & Technol., Nanjing, China; Sch. of Comput. Sci. & Eng., Nanjing Univ. of Sci. & Technol., Nanjing, China; Sch. of Comput. Sci. & Eng., Nanjing Univ. of Sci. & Technol., Nanjing, China; Nanjing Univ. of Sci. & Technol., Nanjing, China; Sch. of Comput. Sci. & Eng., Nanjing Univ. of Sci. & Technol., Nanjing, China","IEEE Geoscience and Remote Sensing Letters","","2015","12","5","1037","1041","A deep neural network (DNN)-based new pansharpening method for the remote sensing image fusion problem is proposed in this letter. Research on representation learning suggests that the DNN can effectively model complex relationships between variables via the composition of several levels of nonlinearity. Inspired by this observation, a modified sparse denoising autoencoder (MSDA) algorithm is proposed to train the relationship between high-resolution (HR) and low-resolution (LR) image patches, which can be represented by the DNN. The HR/LR image patches only sample from the HR/LR panchromatic (PAN) images at hand, respectively, without requiring other training images. By connecting a series of MSDAs, we obtain a stacked MSDA (S-MSDA), which can effectively pretrain the DNN. Moreover, in order to better train the DNN, the entire DNN is again trained by a back-propagation algorithm after pretraining. Finally, assuming that the relationship between HR/LR multispectral (MS) image patches is the same as that between HR/LR PAN image patches, the HR MS image will be reconstructed from the observed LR MS image using the trained DNN. Comparative experimental results with several quality assessment indexes show that the proposed method outperforms other pan-sharpening methods in terms of visual perception and numerical measures.","","","10.1109/LGRS.2014.2376034","National Natural Science Foundation of China; National Scientific Equipment Developing Project of China; Jiangsu Provincial Postdoctoral Research Funding plan of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018004","Deep neural networks (DNNs);multispectral (MS) image;panchromatic (PAN) image;pan-sharpening;Deep neural networks (DNNs);multispectral (MS) image;panchromatic (PAN) image;pan-sharpening","Image reconstruction;Remote sensing;Training;Spatial resolution;Image fusion;Neural networks","geophysical image processing;geophysical techniques;image coding;image denoising;image fusion;image reconstruction;image representation;image resolution;neural nets;numerical analysis;remote sensing","pan-sharpening method;deep neural network;remote sensing image fusion problem;DNN;representation learning;model complex relationships;nonlinearity;modified sparse denoising autoencoder algorithm;high-resolution image patches;low-resolution image patches;low-resolution panchromatic images;high-resolution panchromatic images;MSDA series;high-resolution multispectral image patches;low-resolution multispectral image patches;HR MS image reconstruction;quality assessment indexes;visual perception;numerical measures;back-propagation algorithm","","87","17","","","","","IEEE","IEEE Journals"
"Channel-level acceleration of deep face representations","A. Polyak; L. Wolf","Blavatnik School of Computer Science, Tel Aviv University, Tel Aviv, Israel; Blavatnik School of Computer Science, Tel Aviv University, Tel Aviv, Israel","IEEE Access","","2015","3","","2163","2175","A major challenge in biometrics is performing the test at the client side, where hardware resources are often limited. Deep learning approaches pose a unique challenge: while such architectures dominate the field of face recognition with regard to accuracy, they require elaborate, multi-stage computations. Recently, there has been some work on compressing networks for the purpose of reducing run time and network size. However, it is not clear that these compression methods would work in deep face nets, which are, generally speaking, less redundant than the object recognition networks, i.e., they are already relatively lean. We propose two novel methods for compression: one based on eliminating lowly active channels and the other on coupling pruning with repeated use of already computed elements. Pruning of entire channels is an appealing idea, since it leads to direct saving in run time in almost every reasonable architecture.","","","10.1109/ACCESS.2015.2494536","Broadcom Foundation and Tel Aviv University Authentication Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7303876","Face recognition;neural network compression","Biometrics;Resource management;Face recognition;Image compression","biometrics (access control);data compression;face recognition;image coding;image representation;learning (artificial intelligence);object recognition","object recognition networks;compression methods;face recognition;deep learning approach;biometrics;deep face representations;channel-level acceleration","","35","38","","","","","IEEE","IEEE Journals"
"Modeling temporal dependencies in data using a DBN-LSTM","R. Vohra; K. Goel; J. K. Sahoo","Department of Mathematics, BITS Pilani Goa, Goa, India; Department of Computer Science, BITS Pilani Goa, Goa, India; Department of Mathematics, BITS Pilani Goa, Goa, India","2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)","","2015","","","1","4","Since the advent of deep learning, it has been used to solve various problems using many different architectures. The application of such deep architectures to auditory data is also not uncommon. However, these architectures do not always adequately consider the temporal dependencies in data. We thus propose a new generic architecture called the Deep Belief Network - Long Short-Term Memory (DBN-LSTM) network that models sequences by keeping track of the temporal information while enabling deep representations in the data. We demonstrate this new architecture by applying it to the task of music generation and obtain state-of-the-art results.","","","10.1109/DSAA.2015.7344820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344820","","Computer architecture;Hidden Markov models;Recurrent neural networks;Data models;Microprocessors;Yttrium;Computational modeling","belief networks;data handling;learning (artificial intelligence)","temporal dependencies modeling;deep learning;deep architectures;auditory data;generic architecture;deep belief network;long short-term memory;DBN-LSTM network;data deep representations;music generation","","4","13","","","","","IEEE","IEEE Conferences"
"Deep neural network with RBF and sparse auto-encoders for numeral recognition","D. Mellouli; T. M. Hamdani; A. M. Alimi","REGIM-Lab: REsearch Groups in Intelligent Machines, University of Sfax, National Engineering School of Sfax, BP 1173, 3038, Tunisia; REGIM-Lab: REsearch Groups in Intelligent Machines, University of Sfax, National Engineering School of Sfax, BP 1173, 3038, Tunisia; REGIM-Lab: REsearch Groups in Intelligent Machines, University of Sfax, National Engineering School of Sfax, BP 1173, 3038, Tunisia","2015 15th International Conference on Intelligent Systems Design and Applications (ISDA)","","2015","","","468","472","In this paper we proposed a new deep neural network architecture which is composed from a radial basis function neural network (RBF NN) followed by two auto-encoders and softmax classifier and we presented some comparison between this architecture and other architecture on numeral recognition applications. We gave also a review about RBF and sparse auto-encoder neural networks in the literature. First we defined neural networks and their different type's especially radial basis function neural networks (RBF NN) due to their specificity. Second we focused on auto-encoders and sparse coding then we moved to sparse auto-encoders and finally we demonstrated the effectiveness of our deep architecture by showing our experimental results and some comparisons.","","","10.1109/ISDA.2015.7489160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489160","neural networks NN;radial basis function neural networks RBF NN;sparse coding;auto-encoder;sparse auto-encoder","Artificial neural networks;Unsupervised learning;Computer architecture;Neurons;Databases;Robustness","encoding;neural net architecture;pattern classification;radial basis function networks","deep neural network architecture;radial basis function neural network;RBF NN;softmax classifier;numeral recognition applications;sparse autoencoder neural networks;sparse coding","","","15","","","","","IEEE","IEEE Conferences"
"Face de-identification using facial identity preserving features","H. Chi; Y. H. Hu","State Key Laboratory of Software Engineering, Computer School, Wuhan University, China; Electrical and Computer Engineering, University of Wisconsin-Madison, USA","2015 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","","2015","","","586","590","Automated human facial image de-identification is a much needed technology for privacy-preserving social media and intelligent surveillance applications. Other than the usual face blurring techniques, in this work, we propose to achieve facial anonymity by slightly modifying existing facial images into ""averaged faces"" so that the corresponding identities are difficult to uncover. This approach preserves the aesthesis of the facial images while achieving the goal of privacy protection. In particular, we explore a deep learning-based facial identity-preserving (FIP) features. Unlike conventional face descriptors, the FIP features can significantly reduce intra-identity variances, while maintaining inter-identity distinctions. By suppressing and tinkering FIP features, we achieve the goal of k-anonymity facial image de-identification while preserving desired utilities. Using a face database, we successfully demonstrate that the resulting ""averaged faces"" will still preserve the aesthesis of the original images while defying facial image identity recognition.","","","10.1109/GlobalSIP.2015.7418263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7418263","face representation;face de-identification;privacy protection;deep learning;k-anonymity","Face;Face recognition;Active appearance model;Privacy;Databases;Image recognition;Machine learning","data privacy;face recognition;learning (artificial intelligence);social networking (online);surveillance","facial image identity recognition;k-anonymity facial image de-identification;intra-identity variances;deep learning-based facial identity-preserving features;privacy protection;facial images;face blurring techniques;intelligent surveillance applications;privacy-preserving social media;automated human facial image de-identification;facial identity preserving features;face de-identification","","5","23","","","","","IEEE","IEEE Conferences"
"Feature classification by means of deep belief networks for speaker recognition","P. Safari; O. Ghahabi; J. Hernando","TALP Research Center, Department of Signal Theory and Communications, Universitat Politecnica de Catalunya - BarcelonaTech, Spain; TALP Research Center, Department of Signal Theory and Communications, Universitat Politecnica de Catalunya - BarcelonaTech, Spain; TALP Research Center, Department of Signal Theory and Communications, Universitat Politecnica de Catalunya - BarcelonaTech, Spain","2015 23rd European Signal Processing Conference (EUSIPCO)","","2015","","","2117","2121","In this paper, we propose to discriminatively model target and impostor spectral features using Deep Belief Networks (DBNs) for speaker recognition. In the feature level, the number of impostor samples is considerably large compared to previous works based on i-vectors. Therefore, those i-vector based impostor selection algorithms are not computationally practical. On the other hand, the number of samples for each target speaker is different from one speaker to another which makes the training process more difficult. In this work, we take advantage of DBN unsupervised learning to train a global model, which will be referred to as Universal DBN (UDBN). Then we adapt this UDBN to the data of each target speaker. The evaluation is performed on the core test condition of the NIST SRE 2006 database and it is shown that the proposed architecture achieves more than 8% relative improvement in comparison to the conventional Multilayer Perceptron (MLP).","","","10.1109/EUSIPCO.2015.7362758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362758","Speaker Recognition;Deep Belief Network;Restricted Boltzmann Machine;Feature Classification","Adaptation models;Training;Feature extraction;Data models;Speaker recognition;Europe;Signal processing","belief networks;feature extraction;speaker recognition;unsupervised learning","feature clasification;UDBN;universal DBN;DBN unsupervised learning;i-vector based impostor selection algorithms;impostor samples;speaker recognition;deep belief networks;spectral features","","1","27","","","","","IEEE","IEEE Conferences"
"Knowledge discovery in databases based on deep neural networks","Y. Tan; C. Zhang; Y. Ma; Y. Mao","Karamay Hongyou Software Co., Xinjiang, 834000, China; Karamay Hongyou Software Co., Xinjiang, 834000, China; Application Management Office of SINOPEC IT Management Department, Beijing, 100728, China; Karamay Municipal People's Government Bureau of Information Industry, Xinjiang, 834000, China","2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA)","","2015","","","1217","1222","Knowledge discovery in databases (KDD) has received great progress in recent years for the need of mining useful knowledge in the ever growing information. The advances in machine learning technologies effectively promote KDD in the procedures of feature extraction and data categorization. This paper introduces a framework that combines feature extraction and categorization of the collected data in order to recognize useful structured patterns that underlies the raw data. This frame work consists of three modules: data pre-processing module, feature extraction module, and feature classification module. We propose a four-layered deep neural network as the feature extraction architecture. Each layer is trained in an unsupervised way as one auto-encoder with sparsity constraint. We employ a softmax classifier to assign a label to the extracted feature. The supervised and unsupervised training strategies are discussed at the end of this paper to disambiguate the training procedure of the entire model.","","","10.1109/ICIEA.2015.7334293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334293","Knowledge discovery;deep neural network;sparse auto-encoder;softmax classification","Feature extraction;Neurons;Data mining;Encoding;Neural networks;Computer architecture;Transforms","data mining;database management systems;neural nets;unsupervised learning","deep neural networks;knowledge discovery in databases;KDD;machine learning technology;feature extraction module;data categorization;feature classification module;autoencoder;sparsity constraint;softmax classifier;unsupervised training","","","29","","","","","IEEE","IEEE Conferences"
"Material classification and semantic segmentation of railway track images with deep convolutional neural networks","X. Giben; V. M. Patel; R. Chellappa","Center for Automation Research, UMIACS, University of Maryland, College Park, MD 20742-3275, USA; Center for Automation Research, UMIACS, University of Maryland, College Park, MD 20742-3275, USA; Center for Automation Research, UMIACS, University of Maryland, College Park, MD 20742-3275, USA","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","621","625","The condition of railway tracks needs to be periodically monitored to ensure passenger safety. Cameras mounted on a moving vehicle such as a hi-rail vehicle or a geometry inspection car can generate large volumes of high resolution images. Extracting accurate information from those images has been challenging due to background clutter in railroad environments. In this paper, we describe a novel approach to visual track inspection using material classification and semantic segmentation with Deep Convolutional Neural Networks (DCNN). We show that DCNNs trained end-to-end for material classification are more accurate than shallow learning machines with hand-engineered features and are more robust to noise. Our approach results in a material classification accuracy of 93.35% using 10 classes of materials. This allows for the detection of crumbling and chipped tie conditions at detection rates of 86.06% and 92.11%, respectively, at a false positive rate of 10 FP/mile on the 85-mile Northeast Corridor (NEC) 2012-2013 concrete tie dataset.","","","10.1109/ICIP.2015.7350873","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350873","Deep Convolutional Neural Networks;Railway Track Inspection;Material Classification","Concrete;Inspection;Fasteners;Rails;Electronic ballasts;Convolution;Rail transportation","cameras;feature extraction;image classification;image segmentation;learning (artificial intelligence);neural nets;railway engineering;railway safety","material classification;semantic segmentation;railway track image;deep convolutional neural network;passenger safety;camera;image resolution;visual track inspection;DCNN;learning machine;hand-engineered feature;crumbling condition;chipped tie condition;Northeast Corridor;NEC","","13","27","","","","","IEEE","IEEE Conferences"
"Context Dependent Encoding Using Convolutional Dynamic Networks","R. Chalasani; J. C. Principe","AnalytXbook inc., Boston, MA, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, Fl, USA","IEEE Transactions on Neural Networks and Learning Systems","","2015","26","9","1992","2004","Perception of sensory signals is strongly influenced by their context, both in space and time. In this paper, we propose a novel hierarchical model, called convolutional dynamic networks, that effectively utilizes this contextual information, while inferring the representations of the visual inputs. We build this model based on a predictive coding framework and use the idea of empirical priors to incorporate recurrent and top-down connections. These connections endow the model with contextual information coming from temporal as well as abstract knowledge from higher layers. To perform inference efficiently in this hierarchical model, we rely on a novel scheme based on a smoothing proximal gradient method. When trained on unlabeled video sequences, the model learns a hierarchy of stable attractors, representing low-level to high-level parts of the objects. We demonstrate that the model effectively utilizes contextual information to produce robust and stable representations for object recognition in video sequences, even in case of highly corrupted inputs.","","","10.1109/TNNLS.2014.2360060","Office of Naval Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6948269","Context;deep learning;dynamic models;empirical priors;object recognition.;Context;deep learning;dynamic models;empirical priors;object recognition","Predictive models;Mathematical model;Video sequences;Predictive coding;Context;Context modeling;State-space methods","gradient methods;image sequences;inference mechanisms;object recognition;smoothing methods;video signal processing","context dependent encoding;hierarchical model;convolutional dynamic networks;predictive coding framework;smoothing proximal gradient method;video sequences;object recognition;inference","","4","61","","","","","IEEE","IEEE Journals"
"Compact deep neural networks for device based image classification","Zejia Zheng; Zhu Li; A. Nagar; Kyungmo Park","Michigan State University, USA; Samsung Research America, Korea; Samsung Electronics America, Korea; Samsung Electronics, Korea","2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","","2015","","","1","6","Convolutional Neural Network (CNN) is efficient in learning hierarchical features from large image datasets, but its model complexity and large memory foot prints are preventing it from being deployed to devices without a server back-end support. Modern CNNs are always trained on GPUs or even GPU clusters with high speed computation capability due to the immense size of the network. A device based deep learning CNN engine for image classification can be very useful for situations where server back-end is either not available, or its communication link is weak and unreliable. Methods on regulating the size of the network, on the other hand, are rarely studied. In this paper we present a novel compact architecture that minimizes the number and complexity of lower level kernels in a CNN by separating the color information from the original image. A 9-patch histogram extractor is built to exploit the unused color information. A high level classifier is then used to learn the combined features obtained from the compact CNN that was trained only on grayscale image with limited number of kernels, and the histogram extractor. We apply our compact architecture to Samsung Mobile Image Dataset for image classification. The proposed solution has a recognition accuracy on par with the state of the art CNNs, while achieving significant reduction in model memory foot print. With this advantage, our model is being deployed to the mobile devices.","","","10.1109/ICMEW.2015.7169768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169768","","Kernel;Mobile communication;Image color analysis;Computer architecture;Image recognition;Accuracy;Lead","feature extraction;image classification;image colour analysis;learning (artificial intelligence);mobile radio;neural nets;visual databases","compact deep neural networks;device based image classification;convolutional neural network;hierarchical feature extraction;large image datasets;model complexity;memory foot prints;server back-end support;GPU clusters;high speed computation capability;device based deep learning CNN engine;compact architecture;lower level kernels;image color;9-patch histogram extractor;high level classifier;grayscale image;Samsung mobile image dataset;recognition accuracy;mobile devices","","","15","","","","","IEEE","IEEE Conferences"
"Recognizing objectionable images using convolutional neural nets","R. Moradi; R. Yousefzadeh","Iran University of Science and Technology, Tehran, Iran; Shahrood University of technology, Shahrood, Iran","2015 Signal Processing and Intelligent Systems Conference (SPIS)","","2015","","","133","137","In recent years different methods for detecting objectionable images have proposed. All of the previous systems are based on extracting pre-defined and certain features from the images. In this paper a method is proposed in order to detect objectionable images using convolutional neural networks. In this method first features are learned through a sparse auto-encoder and then training is done by a convolutional neural network. The architecture of the network consists of convolution and sub-sampling layers followed by a fully connected output layer which feeds a softmax classifier with cross entropy cost function. The proposed method is able to effectively detect 90.5% of images correctly employing a rather small training dataset.","","","10.1109/SPIS.2015.7422327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422327","deep learning;convolutional neural network;objectionable image recognition;auto-encoder","Feature extraction;Skin;Training;Image color analysis;Neural networks;Entropy;Visualization","convolution;entropy;feature extraction;image classification;image coding;image sampling;learning (artificial intelligence);neural nets","objectionable image recognition;convolutional neural nets;objectionable image detection;predefined feature extraction;learning;sparse autoencoder;subsampling layers;softmax classifier;cross entropy cost function","","1","17","","","","","IEEE","IEEE Conferences"
"Maximum likelihood nonlinear transformations based on deep neural networks","X. Cui; V. Goel","IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4320","4324","This paper investigates modeling nonlinear transformations based on deep neural networks (DNNs). Specifically, a DNN is used as a nonlinear mapping function for feature space transformation for HMM acoustic models. The nonlinear transformations are estimated under the sequence-based maximum likelihood criterion. The likelihood partition function is evaluated using the Monte Carlo method based on importance sampling. The DNN is first pre-trained approximately to a linear transformation then followed by fine-tuning using the gradient descent algorithm. In addition, a deep stacked architecture is proposed that builds a DNN as a series of sub-networks hierarchically with each representing a nonlinear transformation. A block-wise learning strategy is introduced. LVCSR speaker adaptation experiments on the proposed maximum likelihood nonlinear transformation have shown superior results than the widely-used CMLLR transformation.","","","10.1109/ICASSP.2015.7178786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178786","deep neural networks;nonlinear transformation;maximum likelihood;Monte Carlo method;importance sampling","Hidden Markov models;Artificial neural networks;Speech","gradient methods;hidden Markov models;Monte Carlo methods;neural nets;speech recognition","maximum likelihood nonlinear transformations;deep neural networks;DNN;nonlinear mapping function;feature space transformation;HMM acoustic models;sequence based maximum likelihood criterion;Monte Carlo method;linear transformation;gradient descent algorithm;deep stacked architecture;blockwise learning strategy;LVCSR speaker adaptation;CMLLR transformation","","3","17","","","","","IEEE","IEEE Conferences"
"Robust chinese traffic sign detection and recognition with deep convolutional neural network","Rongqiang Qian; Bailing Zhang; Yong Yue; Zhao Wang; F. Coenen","Department of Computer Science, Xi'an Jiaotong-Liverpool University, Suzhou, China; Department of Computer Science, Xi'an Jiaotong-Liverpool University, Suzhou, China; Department of Computer Science, Xi'an Jiaotong-Liverpool University, Suzhou, China; Department of Computer Science, Bournemouth University, UK; Department of Computer Science, The University of Liverpool, UK","2015 11th International Conference on Natural Computation (ICNC)","","2015","","","791","796","Detection and recognition of traffic sign, including various road signs and text, play an important role in autonomous driving, mapping/navigation and traffic safety. In this paper, we proposed a traffic sign detection and recognition system by applying deep convolutional neural network (CNN), which demonstrates high performance with regard to detection rate and recognition accuracy. Compared with other published methods which are usually limited to a predefined set of traffic signs, our proposed system is more comprehensive as our target includes traffic signs, digits, English letters and Chinese characters. The system is based on a multi-task CNN trained to acquire effective features for the localization and classification of different traffic signs and texts. In addition to the public benchmarking datasets, the proposed approach has also been successfully evaluated on a field-captured Chinese traffic sign dataset, with performance confirming its robustness and suitability to real-world applications.","","","10.1109/ICNC.2015.7378092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7378092","component;traffic sign detection;traffic sign recognition;deep learning;convolutional neural networks;multi task CNN","Neurons;Proposals;Neural networks;Machine learning;Computer vision;Object detection;Feature extraction","feedforward neural nets;image classification;natural language processing;object detection;object recognition;text analysis;traffic engineering computing","robust Chinese traffic sign detection system;robust Chinese traffic sign recognition system;deep convolutional neural network;autonomous driving;traffic safety;multi task CNN;English letters;Chinese characters;traffic sign localization;traffic sign classification;public benchmarking datasets","","10","35","","","","","IEEE","IEEE Conferences"
"FingerNet: Deep learning-based robust finger joint detection from radiographs","S. Lee; M. Choi; H. Choi; M. S. Park; S. Yoon","EECS, Seoul National University, Seoul, 151-744, Korea; EECS, Seoul National University, Seoul, 151-744, Korea; EECS, Seoul National University, Seoul, 151-744, Korea; Orthopedic Surgery, Seoul National University Bundang Hospital, Seongnam, 463-707, Korea; EECS, Seoul National University, Seoul, 151-744, Korea","2015 IEEE Biomedical Circuits and Systems Conference (BioCAS)","","2015","","","1","4","Radiographic image assessment is the most common method used to measure physical maturity and diagnose growth disorders, hereditary diseases and rheumatoid arthritis, with hand radiography being one of the most frequently used techniques due to its simplicity and minimal exposure to radiation. Finger joints are considered as especially important factors in hand skeleton examination. Although several automation methods for finger joint detection have been proposed, low accuracy and reliability are hindering full-scale adoption into clinical fields. In this paper, we propose FingerNet, a novel approach for the detection of all finger joints from hand radiograph images based on convolutional neural networks, which requires little user intervention. The system achieved 98.02% average detection accuracy for 130 test data sets containing over 1,950 joints. Further analysis was performed to verify the system robustness against factors such as epiphysis and metaphysis in different age groups.","","","10.1109/BioCAS.2015.7348440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348440","","Joints;Thumb;Convolution;Radiography;Bones;Merging","diagnostic radiography;learning (artificial intelligence);medical image processing;neural nets","FingerNet;deep learning-based robust finger joint detection;radiograph images;convolutional neural networks;epiphysis;metaphysis","","9","18","","","","","IEEE","IEEE Conferences"
"EMEURO: A framework for generating multi-purpose accelerators via deep learning","L. McAfee; K. Olukotun","Stanford University; Stanford University","2015 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)","","2015","","","125","135","Approximate computing is a very promising design paradigm for crossing the CPU power wall, primarily driven by the potential to sacrifice output quality for significant gains in performance, energy, and fault tolerance. Unfortunately, existing solutions have primarily either focused on new programming models, or new hardware designs, leaving significant room between these two ends for software-based optimizations. To fill this void, additional efforts should target the compilation and runtime stages, which have a critical impact on controlling the interactions of the many approximate subcomputations to form a well-optimized application. This paper presents EMEURO, a neural-network (NN) based emulation and acceleration platform. By restructuring algorithms to have the same data flow as a NN, EMEURO is able to achieve significant speedup across several domains with minimal design effort. EMEURO uses novel NN-based approximate computing techniques, including methods for efficiently searching the high-dimension subroutine space, and fine-grain control of error during runtime. EMEURO is able to achieve 7×-109× maximum speedup over the original algorithm with 0.1%-10% approximation error.","","","10.1109/CGO.2015.7054193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7054193","","Runtime;Computational modeling;Artificial neural networks;Approximation methods;Data models;Emulation;Programming","fault tolerant computing;learning (artificial intelligence);neural nets;power aware computing;program compilers","EMEURO;multipurpose accelerator generation;deep learning;approximate computing;design paradigm;CPU power wall;fault tolerance;software-based optimizations;compilation stages;runtime stages;neural-network based emulation;NN-based approximate computing techniques;fine-grain control","","5","31","","","","","IEEE","IEEE Conferences"
"Integrated Optic Disc and Cup Segmentation with Deep Learning","G. Lim; Y. Cheng; W. Hsu; M. L. Lee","NA; NA; NA; NA","2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)","","2015","","","162","169","Glaucoma is a widespread ocular disorder leading to irreversible loss of vision. Therefore, there is a pressing need for cost-effective screening, such that preventive measures can be taken. This can be achieved with an accurate segmentation of the optic disc and cup from retinal images to obtain the cup-to-disc ratio. We describe a comprehensive solution based on applying convolutional neural networks to feature exaggerated inputs emphasizing disc pallor without blood vessel obstruction, as well as the degree of vessel kinking. The produced raw probability maps then undergo a robust refinement procedure that takes into account prior knowledge about retinal structures. Analysis of these probability maps further allows us to obtain a confidence estimate on the correctness of the segmentation, which can be used to direct the most challenging cases for manual inspection. Tests on two large real-world databases, including the publicly-available MESSIDOR collection, demonstrate the effectiveness of our proposed system.","","","10.1109/ICTAI.2015.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372132","Glaucoma screening;optic disc segmentation;optic cup segmentation","Optical imaging;Adaptive optics;Image segmentation;Retina;Optical computing;Training;Neural networks","biomedical optical imaging;eye;feature extraction;image segmentation;learning (artificial intelligence);medical disorders;medical image processing;probability","integrated optic disc;cup segmentation;deep learning;glaucoma;ocular disorder;vision loss;cost-effective screening;retinal images;cup-to-disc ratio;convolutional neural networks;feature-exaggerated inputs;disc pallor;raw probability maps;robust refinement procedure;retinal structures;real-world databases;publicly-available MESSIDOR collection","","17","33","","","","","IEEE","IEEE Conferences"
"Joint Object and Part Segmentation Using Deep Learned Potentials","P. Wang; X. Shen; Z. Lin; S. Cohen; B. Price; A. Yuille","Univ. of California, Los Angeles, Los Angeles, CA, USA; NA; NA; NA; NA; Univ. of California, Los Angeles, Los Angeles, CA, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1573","1581","Segmenting semantic objects from images and parsing them into their respective semantic parts are fundamental steps towards detailed object understanding in computer vision. In this paper, we propose a joint solution that tackles semantic object and part segmentation simultaneously, in which higher object-level context is provided to guide part segmentation, and more detailed part-level localization is utilized to refine object segmentation. Specifically, we first introduce the concept of semantic compositional parts (SCP) in which similar semantic parts are grouped and shared among different objects. A two-stream fully convolutional network (FCN) is then trained to provide the SCP and object potentials at each pixel. At the same time, a compact set of segments can also be obtained from the SCP predictions of the network. Given the potentials and the generated segments, in order to explore long-range context, we finally construct an efficient fully connected conditional random field (FCRF) to jointly predict the final object and part labels. Extensive evaluation on three different datasets shows that our approach can mutually enhance the performance of object and part segmentation, and outperforms the current state-of-the-art on both tasks.","","","10.1109/ICCV.2015.184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410541","","Semantics;Image segmentation;Training;Context;Object segmentation;Legged locomotion;Proposals","computer vision;image segmentation;learning (artificial intelligence);prediction theory;random processes","deep learned potentials;semantic object segmentation;parsing;object understanding;computer vision;part segmentation;object-level context;part-level localization;semantic compositional parts;two-stream fully convolutional network;FCN;object potentials;SCP predictions;fully connected conditional random field;FCRF;object labels;part labels","","30","47","","","","","IEEE","IEEE Conferences"
"Mine the fine: Fine-grained fragment discovery","M. H. Kiapour; W. Di; V. Jagadeesh; R. Piramuthu","University of North Carolina at Chapel Hill Chapel Hill, NC, USA; eBay Research Labs, San Jose, CA, USA; eBay Research Labs, San Jose, CA, USA; eBay Research Labs, San Jose, CA, USA","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","3555","3559","While discriminative visual element mining has been introduced before, in this paper we present an approach that requires minimal annotation in both training and test time. Given only a bounding box localization of the foreground objects, our approach automatically transforms the input images into a roughly-aligned pose space and discovers the most discriminative visual fragments for each category. These fragments are then used to learn robust classifiers that discriminate between very similar categories under challenging conditions such as large variations in pose or habitats. The minimal required input, is a critical characteristic that enables our approach to generalize over visual domains where expert knowledge is not readily available. Moreover, our approach takes advantage of deep networks that are targeted towards fine-grained classification. It learns mid-level representations that are specific to a category and generalize well across the category instances at the same time. Our evaluations demonstrate that the automatically learned representation based on discriminative fragments, significantly outperforms globally extracted deep features in classification accuracy.","","","10.1109/ICIP.2015.7351466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351466","Fine-grained;mid-level representation;deep learning;classification","Feature extraction;Yttrium;Training;Birds;Visualization;Robustness;Machine learning","data mining;feature extraction;image classification;image representation;learning (artificial intelligence);neural nets","fine-grained fragment discovery;discriminative visual element mining;bounding box localization;foreground objects;roughly-aligned pose space;discriminative visual fragments;robust classifiers;visual domains;deep networks;fine-grained classification;mid-level representations;feature extraction;classification accuracy","","1","25","","","","","IEEE","IEEE Conferences"
"Video Super-Resolution via Deep Draft-Ensemble Learning","R. Liao; X. Tao; R. Li; Z. Ma; J. Jia","Chinese Univ. of Hong Kong, Hong Kong, China; Chinese Univ. of Hong Kong, Hong Kong, China; Chinese Univ. of Hong Kong, Hong Kong, China; Univ. of Chinese Acad. of Sci., Beijing, China; Chinese Univ. of Hong Kong, Hong Kong, China","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","531","539","We propose a new direction for fast video super-resolution (VideoSR) via a SR draft ensemble, which is defined as the set of high-resolution patch candidates before final image deconvolution. Our method contains two main components -- i.e., SR draft ensemble generation and its optimal reconstruction. The first component is to renovate traditional feedforward reconstruction pipeline and greatly enhance its ability to compute different super resolution results considering large motion variation and possible errors arising in this process. Then we combine SR drafts through the nonlinear process in a deep convolutional neural network (CNN). We analyze why this framework is proposed and explain its unique advantages compared to previous iterative methods to update different modules in passes. Promising experimental results are shown on natural video sequences.","","","10.1109/ICCV.2015.68","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410425","","Image reconstruction;Image resolution;Deconvolution;Feedforward neural networks;Motion estimation;Optical imaging;Kernel","deconvolution;image motion analysis;image reconstruction;image resolution;image sequences;neural nets;video signal processing","natural video sequences;VideoSR;CNN;deep convolutional neural network;motion variation;feedforward reconstruction pipeline;SR draft ensemble generation;image deconvolution;high-resolution patch candidates;deep draft-ensemble learning;video super-resolution","","56","21","","","","","IEEE","IEEE Conferences"
"Audio-visual speech recognition using deep bottleneck features and high-performance lipreading","S. Tamura; H. Ninomiya; N. Kitaoka; S. Osuga; Y. Iribe; K. Takeda; S. Hayamizu","Gifu University, Japan; Nagoya University, Japan; Tokushima University, Japan; Aisin Seiki Co., Ltd., Japan; Aichi Prefectural University, Japan; Nagoya University, Japan; Gifu University, Japan","2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","","2015","","","575","582","This paper develops an Audio-Visual Speech Recognition (AVSR) method, by (1) exploring high-performance visual features, (2) applying audio and visual deep bottleneck features to improve AVSR performance, and (3) investigating effectiveness of voice activity detection in a visual modality. In our approach, many kinds of visual features are incorporated, subsequently converted into bottleneck features by deep learning technology. By using proposed features, we successfully achieved 73.66% lipreading accuracy in speaker-independent open condition, and about 90% AVSR accuracy on average in noisy environments. In addition, we extracted speech segments from visual features, resulting 77.80% lipreading accuracy. It is found VAD is useful in both audio and visual modalities, for better lipreading and AVSR.","","","10.1109/APSIPA.2015.7415335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415335","","Visualization;Feature extraction;Speech recognition;Hidden Markov models;Discrete cosine transforms;Principal component analysis;Mouth","audio-visual systems;learning (artificial intelligence);speech recognition","audio-visual speech recognition;deep bottleneck features;high-performance lipreading;AVSR method;deep learning technology;speech segment extraction","","13","32","","","","","IEEE","IEEE Conferences"
"DASA: Domain adaptation in stacked autoencoders using systematic dropout","A. G. Roy; D. Sheet","Department of Electrical Engineering, Indian Institute of Technology, Kharagpur, India; Department of Electrical Engineering, Indian Institute of Technology, Kharagpur, India","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","","2015","","","735","739","Domain adaptation deals with adapting behaviour of machine learning based systems trained using samples in source domain to their deployment in target domain where the statistics of samples in both domains are dissimilar The task of directly training or adapting a learner in the target domain is challenged by lack of abundant labeled samples. In this paper we propose a technique for domain adaptation in stacked autoencoder (SAE) based deep neural networks (DNN) performed in two stages: (i) unsupervised weight adaptation using systematic dropouts in mini-batch training, (ii) supervised fine-tuning with limited number of labeled samples in target domain. We experimentally evaluate performance in the problem of retinal vessel segmentation where the SAE-DNN is trained using large number of labeled samples in the source domain (DRIVE dataset) and adapted using less number of labeled samples in target domain (STARE dataset). The performance of SAE-DNN measured using logloss in source domain is 0.19, without and with adaptation are 0.40 and 0.18, and 0.39 when trained exclusively with limited samples in target domain. The area under ROC curve is observed respectively as 0.90, 0.86, 0.92 and 0.87. The high efficiency of vessel segmentation with DASA strongly substantiates our claim.","","","10.1109/ACPR.2015.7486600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486600","","Training;Systematics;Image color analysis;Learning systems;Neural networks;Retinal vessels","image segmentation;learning (artificial intelligence);neural nets;retinal recognition","DASA;domain adaptation;stacked autoencoders;systematic dropout;machine learning based systems;stacked autoencoder;deep neural networks;systematic dropouts;minibatch training;retinal vessel segmentation;SAE-DNN;source domain;DRIVE dataset;STARE dataset;ROC curve","","2","10","","","","","IEEE","IEEE Conferences"
"Denoising hybrid noises in image with stacked autoencoder","X. Ye; L. Wang; H. Xing; L. Huang","College of Automation, Harbin Engineering University, Heilongjiang Province, China; College of Automation, Harbin Engineering University, Heilongjiang Province, China; College of Automation, Harbin Engineering University, Heilongjiang Province, China; College of Automation, Harbin Engineering University, Heilongjiang Province, China","2015 IEEE International Conference on Information and Automation","","2015","","","2720","2724","A method based on sparse denoising autoencoder for denoising hybrid noises in image is proposed in this paper. The method is experimented on natural images and the performance is evaluated in terms of peak signal to noise ratio (PSNR). By specifically designing the training process of sparse denoising autoencoder, our model not only achieves good performance on single kind of noises, but also is relatively robust to mixed noises, which are more widely existed in practical situation. Autoencoder is a major branch of deep learning. It has been used in many applications as the method to exact features for its ability to represent the input data. Applying autoencoder to image denoising has been achieved good performance. Further research was deployed to find that autoencoder method is relatively robust compared with BM3D. And a sparse denoising autoencoder model is employed to train the network and it works well for the hybrid noise situation.","","","10.1109/ICInfA.2015.7279746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279746","image denoising;stacked sparse denoising autoencoder;deep learning","Noise;Noise reduction;Training;Image denoising;Testing;Speckle;Neural networks","feature extraction;image denoising;natural scenes;performance evaluation","image denoising;hybrid noise;stacked autoencoder;natural image;performance evaluation;peak signal to noise ratio;PSNR;training process;deep learning;feature extraction;sparse denoising autoencoder model","","4","13","","","","","IEEE","IEEE Conferences"
"DLDR: Deep Linear Discriminative Retrieval for Cultural Event Classification from a Single Image","R. Rothe; R. Timofte; L. V. Gool","Comput. Vision Lab., ETH Zurich, Zurich, Switzerland; Comput. Vision Lab., ETH Zurich, Zurich, Switzerland; NA","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","295","302","In this paper we tackle the classification of cultural events from a single image with a deep learning based method. We use convolutional neural networks (CNNs) with VGG-16 architecture [17], pretrained on ImageNet or the Places205 dataset for image classification, and fine-tuned on cultural events data. CNN features are robustly extracted at 4 different layers in each image. At each layer Linear Discriminant Analysis (LDA) is employed for discriminative dimensionality reduction. An image is represented by the concatenated LDA-projected features from all layers or by the concatenation of CNN pooled features at each layer. The classification is then performed through the Iterative Nearest Neighbors-based Classifier (INNC) [20]. Classification scores are obtained for different image representation setups at train and test. The average of the scores is the output of our deep linear discriminative retrieval (DLDR) system. With 0.80 mean average precision (mAP) DLDR is a top entry for the ChaLearn LAP 2015 cultural event recognition challenge.","","","10.1109/ICCVW.2015.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406396","","Cultural differences;Feature extraction;Training;Computer architecture;Agriculture;Robustness;Linear discriminant analysis","feature extraction;feedforward neural nets;history;image classification;image representation;image retrieval;iterative methods;learning (artificial intelligence);statistical analysis","DLDR;cultural event classification;ChaLearn LAP 2015 cultural event recognition challenge;mean average precision;DLDR system;deep linear discriminative retrieval system;INNC;iterative nearest neighbors-based classifier;concatenated LDA-projected features;image representation setups;discriminative dimensionality reduction;linear discriminant analysis;CNN feature extraction;image classification;Places205 dataset;ImageNet dataset;VGG-16 architecture;convolutional neural networks;deep learning based method","","3","25","","","","","IEEE","IEEE Conferences"
"Pedestrian recognition method based on depth hierarchical feature representation","Rui Sun; Guang-Hai Zhang; Jun Gao","School of Computer and Information, Hefei University of Technology, 230009, China; School of Computer and Information, Hefei University of Technology, 230009, China; School of Computer and Information, Hefei University of Technology, 230009, China","2015 12th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)","","2015","","","173","178","For feature representation of pedestrian recognition, a hybrid hierarchical feature representation method which combines representation ability of bag of words model and depth layered with learning adaptability is presented. This method first uses HOG local descriptor for local features extraction, and then encoding the feature by a depth of layered coding method, the layered coding method by spatial aggregating restricted Boltzmann machine (RBM). For each coding layer, we steer the unsupervised RBM learning and apply supervised fine-tuning to enhance the visual features representation in classification task. Finally, we learn high-level image feature representation by the positive and negative max pooling, and then classify with the linear support vector machine, feature extraction of depth architectures effectively improve the accuracy of subsequent recognition. Experimental results show that the proposed method has a high recognition rate.","","","10.1109/ICCWAMTIP.2015.7493969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493969","Hybrid structure;deep learning;depth hierarchical coding;positive and negative max pooling;restricted Boltzmann machine (RBM)","Encoding;Feature extraction;Visualization;Dictionaries;Support vector machines;Machine learning;Training","Boltzmann machines;feature extraction;image classification;image representation;pedestrians;support vector machines;unsupervised learning","pedestrian recognition method;depth hierarchical feature representation;hybrid hierarchical feature representation;bag of words model;learning adaptability;HOG local descriptor;local features extraction;feature encoding;layered coding method;spatial aggregating restricted Boltzmann machine;coding layer;unsupervised RBM learning;supervised fine-tuning;visual features representation;classification task;high-level image feature representation;positive max pooling;negative max pooling;linear support vector machine;depth architectures","","","13","","","","","IEEE","IEEE Conferences"
"Listening with Your Eyes: Towards a Practical Visual Speech Recognition System Using Deep Boltzmann Machines","C. Sui; M. Bennamoun; R. Togneri","Sch. of Comput. Sci. & Software Eng., Univ. of Western Australia, Perth, WA, Australia; Sch. of Comput. Sci. & Software Eng., Univ. of Western Australia, Perth, WA, Australia; Sch. of Electr., Electron. & Comput. Eng., Univ. of Western Australia, Perth, WA, Australia","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","154","162","This paper presents a novel feature learning method for visual speech recognition using Deep Boltzmann Machines (DBM). Unlike all existing visual feature extraction techniques which solely extracts features from video sequences, our method is able to explore both acoustic information and visual information to learn a better visual feature representation in the training stage. During the test stage, instead of using both audio and visual signals, only the videos are used for generating the missing audio feature, and both the given visual and given audio features are used to obtain a joint representation. We carried out our experiments on a large scale audio-visual data corpus, and experimental results show that our proposed techniques outperforms the performance of the hadncrafted features and features learned by other commonly used deep learning techniques.","","","10.1109/ICCV.2015.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410383","","Visualization;Speech recognition;Feature extraction;Hidden Markov models;Acoustics;Noise measurement;Speech","Boltzmann machines;feature extraction;image representation;image sequences;speech recognition","practical visual speech recognition system;deep Boltzmann machines;feature learning method;DBM;visual feature extraction techniques;video sequences;acoustic information;visual information;visual feature representation;audio signals;visual signals;joint representation;audio visual data corpus;deep learning techniques","","9","32","","","","","IEEE","IEEE Conferences"
"Learning to learn: Creating engineering classrooms for deep understanding","S. Hug; E. Q. Villa; P. Golding; G. Gandara","ATLAS Institute, University of Colorado, Boulder, Boulder, CO 80309; College of Education, The University of Texas at El Paso, El Paso, Texas 79902 USA; College of Engineering, The University of Texas at El Paso, El Paso, Texas 79902 USA; College of Engineering, The University of Texas at El Paso, El Paso, Texas 79902 USA","2015 IEEE Frontiers in Education Conference (FIE)","","2015","","","1","5","Metacognition involves an ability to reflect upon a learning episode, understand what strategies provoked learning, and gauge one's current level of understanding. This paper details preliminary evidence regarding the University of Texas at El Paso (UTEP) Metacognitive Learners course, a pre-engineering course structured to develop metacognitive strategies and habits through scaffolded collaborative group work, small and large group discussions about learning, and guided writing exercises designed to support learner reflection. Course curriculum addressed metacognition and metacognitive skill development in three explicit ways: a) team problem solving, b) student goal setting, and c) reflective writing about group process, project progress, and individual learning. Following this course, student survey data across all 4 sections suggests initial success in creating a learning environment that supports developing metacognition - students considered how they learned best, described their knowledge to others, and were asked to check their own understanding and progress throughout the course via dialogue and reflective writing.","","","10.1109/FIE.2015.7344277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344277","metacognition;introductory engineering courses;community of learners","Writing;Reflection;Robots;Collaboration;Mathematics;Monitoring;Interviews","cognition;educational courses;engineering education","University of Texas at El Paso;UTEP Metacognitive Learners course;preengineering course;scaffolded collaborative group work;metacognition;metacognitive skill development;team problem solving;student goal setting;reflective writing","","","13","","","","","IEEE","IEEE Conferences"
"Interactive Multimodal Learning for Venue Recommendation","J. Zahálka; S. Rudinac; M. Worring","Intelligent Sensory Information Systems, University of Amsterdam, Amsterdam, Netherlands; Intelligent Sensory Information Systems, University of Amsterdam, Amsterdam, Netherlands; Intelligent Sensory Information Systems, University of Amsterdam, Amsterdam, Netherlands","IEEE Transactions on Multimedia","","2015","17","12","2235","2244","In this paper, we propose City Melange, an interactive and multimodal content-based venue explorer. Our framework matches the interacting user to the users of social media platforms exhibiting similar taste. The data collection integrates location-based social networks such as Foursquare with general multimedia sharing platforms such as Flickr or Picasa. In City Melange, the user interacts with a set of images and thus implicitly with the underlying semantics. The semantic information is captured through convolutional deep net features in the visual domain and latent topics extracted using Latent Dirichlet allocation in the text domain. These are further clustered to provide representative user and venue topics. A linear SVM model learns the interacting user's preferences and determines similar users. The experiments show that our content-based approach outperforms the user-activity-based and popular vote baselines even from the early phases of interaction, while also being able to recommend mainstream venues to mainstream users and off-the-beaten-track venues to afficionados. City Melange is shown to be a well-performing venue exploration approach.","","","10.1109/TMM.2015.2480007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272105","Deep nets;interactive city exploration;location-based social networks;semantic concept detectors;topic models;user-centered design","Interactive systems;Support vector machines;Machine learning;Semantics;Recommender systems;Social network services","feature extraction;interactive systems;learning (artificial intelligence);pattern clustering;recommender systems;semantic networks;social networking (online);support vector machines;text analysis","interactive multimodal learning;venue recommendation;City Melange;social media platform;semantic information;latent topic extraction;latent Dirichlet allocation;text clustering;linear SVM model","","9","35","","","","","IEEE","IEEE Journals"
"My camera can see through fences: A deep learning approach for image de-fencing","S. Jonna; K. K. Nakka; R. R. Sahay","School of Information Technology, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Electrical Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; School of Information Technology, Indian Institute of Technology Kharagpur, Kharagpur, India","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","","2015","","","261","265","In recent times, the availability of inexpensive image capturing devices such as smartphones/tablets has led to an exponential increase in the number of images/videos captured. However, sometimes the amateur photographer is hindered by fences in the scene which have to be removed after the image has been captured. Conventional approaches to image de-fencing suffer from inaccurate and non-robust fence detection apart from being limited to processing images of only static occluded scenes. In this paper, we propose a semi-automated de-fencing algorithm using a video of the dynamic scene. We use convolutional neural networks for detecting fence pixels. We provide qualitative as well as quantitative comparison results with existing lattice detection algorithms on the existing PSU NRT data set [1] and a proposed challenging fenced image dataset. The inverse problem offence removal is solved using split Bregman technique assuming total variation of the de-fenced image as the regularization constraint.","","","10.1109/ACPR.2015.7486506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486506","","Heuristic algorithms;Cameras;Machine learning;Dynamics;Training;Databases;Robustness","cameras;feature extraction;image capture;neural nets","camera;deep learning approach;image de-fencing;inexpensive image capturing device availability;image capture;video capture;amateur photographer;fence detection;static occluded scene;semiautomated de-fencing algorithm;convolutional neural network;fence pixel detection;PSU NRT data set;fenced image dataset;inverse problem;fence removal;split Bregman technique;regularization constraint","","2","21","","","","","IEEE","IEEE Conferences"
"Learning feature mapping using deep neural network bottleneck features for distant large vocabulary speech recognition","I. Himawan; P. Motlicek; D. Imseng; B. Potard; N. Kim; J. Lee","Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Samsung Electronics Co. Ltd, Suwon, South Korea; Samsung Electronics Co. Ltd, Suwon, South Korea","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4540","4544","Automatic speech recognition from distant microphones is a difficult task because recordings are affected by reverberation and background noise. First, the application of the deep neural network (DNN)/hidden Markov model (HMM) hybrid acoustic models for distant speech recognition task using AMI meeting corpus is investigated. This paper then proposes a feature transformation for removing reverberation and background noise artefacts from bottleneck features using DNN trained to learn the mapping between distant-talking speech features and close-talking speech bottleneck features. Experimental results on AMI meeting corpus reveal that the mismatch between close-talking and distant-talking conditions is largely reduced, with about 16% relative improvement over conventional bottleneck system (trained on close-talking speech). If the feature mapping is applied to close-talking speech, a minor degradation of 4% relative is observed.","","","10.1109/ICASSP.2015.7178830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178830","Deep neural network;bottleneck features;distant speech recognition;meetings;AMI corpus","Hidden Markov models;Speech;Speech recognition;Adaptation models;Training;Feature extraction;Acoustics","hidden Markov models;microphones;neural nets;reverberation;speech recognition","distant large vocabulary speech recognition;feature mapping;deep neural network bottleneck features;automatic speech recognition;distant microphones;reverberation noise;background noise;hidden Markov model;hybrid acoustic models;distant speech recognition task;AMI meeting corpus;feature transformation;distant-talking speech features;close-talking speech bottleneck features","","8","19","","","","","IEEE","IEEE Conferences"
"Speech emotion recognition using Deep Dropout Autoencoders","A. Pal; S. Baskar","Dept. of CST, Goa University, Goa - India; Dept, Dept of CST, Goa University, Goa - India","2015 IEEE International Conference on Engineering and Technology (ICETECH)","","2015","","","1","6","This work describes speech emotion recognition in Konkani with Deep Dropout Autoencoder using Multilayer Perceptron trained through backpropagation algorithm (DDA). To learn robust representation and to reduce the chance of co-adaption, hidden units along with their connections are randomly dropped out in Dropout Autoencoders while input layer remain untouched at training time. Dropout Autoencoders are pre-trained to bring the initial weights of the network to some good solution and thereafter can be stacked to form a DDA that then converted to a Deep Classifier by adding a classification layer. A final fine-tune training was applied to the whole classifier. Several configurations have been tested to find a good classifier to predict seven emotion states. To validate the experiment DDA has been compared with other state-of-art systems like Deep Autoencoder using Multilayer Perceptron trained through backpropagation algorithm (DA), Hidden Markov Model (HMM) to evaluate the improvement. It has been found that the overall recognition accuracy of DDA gives better performance than DA and HMM which are 82%, 80% and DDA gives a performance 87% that have been studied by using four fold leave-one-out cross validation.","","","10.1109/ICETECH.2015.7275003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275003","Dropout Autoencoder;MLP;Deep Network","Hidden Markov models;Feature extraction;Training;Emotion recognition;Speech;Backpropagation;Speech recognition","backpropagation;emotion recognition;hidden Markov models;multilayer perceptrons;speech processing","speech emotion recognition;deep dropout autoencoders;Konkani;deep dropout autoencoder;multilayer perceptron;backpropagation algorithm;DDA;deep classifier;emotion states;Hidden Markov Model;HMM","","1","13","","","","","IEEE","IEEE Conferences"
"Unsupervised Learning of Visual Representations Using Videos","X. Wang; A. Gupta","Robot. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA; Robot. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2794","2802","Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation.","","","10.1109/ICCV.2015.320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410677","","Videos;Visualization;Training;Unsupervised learning;Semantics;Clustering algorithms;Tracking","image representation;neural nets;unsupervised learning;video signal processing","unsupervised learning;visual representation;convolutional neural network;CNN;semantically-labeled images;deep feature space;Siamese-triplet network;ImageNet;VOC 2012 dataset","","202","56","","","","","IEEE","IEEE Conferences"
"Deep classification of vehicle makers and models: The effectiveness of pre-training and data enhancement","F. Zhang; X. Xu; Y. Qiao","Shenzhen Institutes of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Science, Shenzhen, China","2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)","","2015","","","231","236","Vision-based vehicle detection and classification is an important problem in machine vision and attracted widespread attention due to its wide range of applications in intelligent traffic system. Most of the current vehicle type classification methods require to precisely locate car positions and use the cropped car regions as input. In this paper, we propose deep convolutional neural networks for vehicle makers and models classification, which can take whole image as input without detecting car regions. Our main contributions are two-fold. Firstly, we find pre-train the deep CNN in the task to identify whether vehicle exists in input image can boost the performance of vehicle type classification. Secondly, we show data enhancement can further improve the classification accuracy. Our methods achieve the accuracy of 79% on a large scale Cars dataset, which is comparable with the recent state of the art which requires cropped car image as input.","","","10.1109/ROBIO.2015.7418772","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7418772","","Automobiles;Training;Computational modeling;Mirrors;Data models;Computer architecture","automobiles;feedforward neural nets;image classification;image enhancement;learning (artificial intelligence);traffic engineering computing","deep vehicle maker classification;deep vehicle model classification;data enhancement;vision-based vehicle detection;machine vision;intelligent traffic system;car positions;cropped car regions;deep convolutional neural networks;deep CNN pretraining;Cars dataset;cropped car image","","4","20","","","","","IEEE","IEEE Conferences"
"Deep feature synthesis: Towards automating data science endeavors","J. M. Kanter; K. Veeramachaneni","CSAIL, MIT, Cambridge, MA - 02139; CSAIL, MIT, Cambridge, MA- 02139","2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)","","2015","","","1","10","In this paper, we develop the Data Science Machine, which is able to derive predictive models from raw data automatically. To achieve this automation, we first propose and develop the Deep Feature Synthesis algorithm for automatically generating features for relational datasets. The algorithm follows relationships in the data to a base field, and then sequentially applies mathematical functions along that path to create the final feature. Second, we implement a generalizable machine learning pipeline and tune it using a novel Gaussian Copula process based approach. We entered the Data Science Machine in 3 data science competitions that featured 906 other data science teams. Our approach beats 615 teams in these data science competitions. In 2 of the 3 competitions we beat a majority of competitors, and in the third, we achieved 94% of the best competitor's score. In the best case, with an ongoing competition, we beat 85.6% of the teams and achieved 95.7% of the top submissions score.","","","10.1109/DSAA.2015.7344858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344858","","Feature extraction;Predictive models;Machine learning algorithms;Prediction algorithms;Data models;Algorithm design and analysis;Data mining","data analysis;Gaussian processes;learning (artificial intelligence)","deep feature synthesis;data science endeavor automation;data science machine;feature generation;relational datasets;mathematical functions;generalizable machine learning pipeline;Gaussian copula process","","39","17","","","","","IEEE","IEEE Conferences"
"Sparsity analysis of learned factors in Multilayer NMF","I. Redko; Y. Bennani","Laboratoire d'Informatique de Paris-Nord, CNRS (UMR 7030), Université Paris 13, Sorbonne Paris Cité, F-93430, Villetaneuse, France; Laboratoire d'Informatique de Paris-Nord, CNRS (UMR 7030), Université Paris 13, Sorbonne Paris Cité, F-93430, Villetaneuse, France","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","7","The concept of nonnegative matrix factorization is a recent machine learning technique that is used to decompose large data matrices imposing the non-negativity constraints on the factors. This technique is now used in many data mining applications and thus remains a topic of ongoing interest. In this paper we are particularly interested in the Multilayer NMF - a model that can be seen as a pretraining step of Deep NMF model for learning hidden representations. We analyze the factors obtained using Multilayer NMF and show that the process of building layers can be seen as a repeated application of the Hoyer's projection operator applied sequentially to the factor of the second layer. We also provide the sparsity analysis for matrices obtained during the optimization procedure at each layer. We conclude that the overall sparsity decreases with the increasing number of layers despite the general assumption that Multilayer NMF is efficient due to the fact that it increases the sparsity of learned factors.","","","10.1109/IJCNN.2015.7280551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280551","","Nonhomogeneous media;Matrix decomposition","data mining;learning (artificial intelligence);matrix decomposition;optimisation","learned factor sparsity analysis;multilayer NMF;nonnegative matrix factorization;machine learning technique;large data matrices;data mining applications;deep NMF model;hidden representation learning;Hoyer projection operator;optimization","","1","26","","","","","IEEE","IEEE Conferences"
"Comparative Analysis of Existing Architectures for General Game Agents","I. Hosu; A. Urzica","NA; NA","2015 17th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)","","2015","","","257","260","This paper addresses the development of general purpose game agents able to learn a vast number of games using the same architecture. The article analyzes the main existing approaches to general game playing, reviews their performance and proposes future research directions. Methods such as deep learning, reinforcement learning and evolutionary algorithms are considered for this problem. The testing platform is the popular video game console Atari 2600. Research into developing general purpose agents for games is closely related to achieving artificial general intelligence (AGI).","","","10.1109/SYNASC.2015.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426092","convolutional neural networks;Q-learning;model-free algorithms;neuroevolution;games","Games;Neural networks;Network topology;Topology;Computer architecture;Learning (artificial intelligence);Computer science","computer games;evolutionary computation;learning (artificial intelligence);multi-agent systems","general game agents;general game playing;deep learning;reinforcement learning;evolutionary algorithms;video game console;Atari 2600;artificial general intelligence;AGI","","","15","","","","","IEEE","IEEE Conferences"
"Supervised Machine Learning Model for High Dimensional Gene Data in Colon Cancer Detection","H. Chen; H. Zhao; J. Shen; R. Zhou; Q. Zhou","Sch. of Inf. Sci. & Eng., Lanzhou Univ., Lanzhou, China; Dept. of Phys., Xiamen Univ., Xiamen, China; Sch. of Inf. Syst. & Technol., Univ. of Wollongong, Wollongong, NSW, Australia; Sch. of Inf. Sci. & Eng., Lanzhou Univ., Lanzhou, China; Sch. of Inf. Sci. & Eng., Lanzhou Univ., Lanzhou, China","2015 IEEE International Congress on Big Data","","2015","","","134","141","With well-developed methods in gene level data extraction, there are huge amount of gene expression data, including normal composition and abnormal ones. Therefore, mining gene expression data is currently an urgent research question, for detecting a corresponding pattern, such as cancer species, quickly and accurately. Since gene expression data classification problem has been widely studied accompanying with the development of gene technology, by far numerous methods, mainly neural network related, have been deployed in medical data analysis, which is mainly dealing with the high dimension and small quantity. A lot of research has been conducted on clustering approaches, extreme learning machine and so on. They are usually applied in a shallow neural network model. Recently deep learning has shown its power and good performance on high dimensional datasets. Unlike current popular deep neural network, we will continue to apply shallow neural network but develop an innovative algorithm for shallow neural network. In the supervised model, we demonstrate a shallow neural network model with a batch of parameters, and narrow its computational process into several positive parts, which process smoothly for a better result and finally achieve an optimal goal. It shows a stable and excellent result comparable to deep neural network. An analysis of the algorithm is also presented in this paper.","","","10.1109/BigDataCongress.2015.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207212","Neural Network;Monte Carlo;high dimensional data","Gene expression;Cost function;Monte Carlo methods;Cancer;Support vector machines;Neural networks;Colon","cancer;data analysis;data mining;genetics;learning (artificial intelligence);medical information systems;neural nets;pattern classification;pattern clustering","supervised machine learning model;high dimensional gene data;colon cancer detection;gene level data extraction;gene expression data mining;cancer species;gene expression data classification problem;medical data analysis;clustering approaches;extreme learning machine;neural network model;deep neural network;shallow neural network","","11","24","","","","","IEEE","IEEE Conferences"
"Benchmarking classification of earth-observation data: From learning explicit features to convolutional networks","A. Lagrange; B. Le Saux; A. Beaupère; A. Boulch; A. Chan-Hon-Tong; S. Herbin; H. Randrianarivo; M. Ferecatu","Onera - The French Aerospace Lab, F-91761 Palaiseau, France; Onera - The French Aerospace Lab, F-91761 Palaiseau, France; Onera - The French Aerospace Lab, F-91761 Palaiseau, France; Onera - The French Aerospace Lab, F-91761 Palaiseau, France; Onera - The French Aerospace Lab, F-91761 Palaiseau, France; Onera - The French Aerospace Lab, F-91761 Palaiseau, France; Onera - The French Aerospace Lab, F-91761 Palaiseau, France; CNAM - Cedric, 292 rue St-Martin, 75141 Paris, France","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","","2015","","","4173","4176","In this paper, we address the task of semantic labeling of multisource earth-observation (EO) data. Precisely, we benchmark several concurrent methods of the last 15 years, from expert classifiers, spectral support-vector classification and high-level features to deep neural networks. We establish that (1) combining multisensor features is essential for retrieving some specific classes, (2) in the image domain, deep convolutional networks obtain significantly better overall performances and (3) transfer of learning from large generic-purpose image sets is highly effective to build EO data classifiers.","","","10.1109/IGARSS.2015.7326745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326745","Remote sensing;Image classification;Pattern analysis;Neural networks","Support vector machines;Semantics;Laser radar;Neural networks;Remote sensing;Buildings;Feature extraction","geophysical image processing;image classification;neural nets;support vector machines;terrain mapping","semantic labeling;multisource earth-observation data benchmarking classification;expert classifiers;spectral support-vector classification;high-level features;deep neural networks;multisensor features;image domain;deep convolutional networks;generic-purpose image sets;learning explicit features","","39","14","","","","","IEEE","IEEE Conferences"
"Towards Computational Baby Learning: A Weakly-Supervised Approach for Object Detection","X. Liang; S. Liu; Y. Wei; L. Liu; L. Lin; S. Yan","Nat. Univ. of Singapore, Singapore, Singapore; Nat. Univ. of Singapore, Singapore, Singapore; Nat. Univ. of Singapore, Singapore, Singapore; Nat. Univ. of Singapore, Singapore, Singapore; NA; Nat. Univ. of Singapore, Singapore, Singapore","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","999","1007","Intuitive observations show that a baby may inherently possess the capability of recognizing a new visual concept (e.g., chair, dog) by learning from only very few positive instances taught by parent(s) or others, and this recognition capability can be gradually further improved by exploring and/or interacting with the real instances in the physical world. Inspired by these observations, we propose a computational model for weakly-supervised object detection, based on prior knowledge modelling, exemplar learning and learning with video contexts. The prior knowledge is modeled with a pre-trained Convolutional Neural Network (CNN). When very few instances of a new concept are given, an initial concept detector is built by exemplar learning over the deep features the pre-trained CNN. The well-designed tracking solution is then used to discover more diverse instances from the massive online weakly labeled videos. Once a positive instance is detected/identified with high score in each video, more instances possibly from different view-angles and/or different distances are tracked and accumulated. Then the concept detector can be fine-tuned based on these new instances. This process can be repeated again and again till we obtain a very mature concept detector. Extensive experiments on Pascal VOC-07/10/12 object detection datasets [9] well demonstrate the effectiveness of our framework. It can beat the state-of-the-art full-training based performances by learning from very few samples for each object category, along with about 20,000 weakly labeled videos.","","","10.1109/ICCV.2015.120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410477","","Detectors;Pediatrics;Computational modeling;Context;Object detection;Training;Feature extraction","feature extraction;image recognition;learning (artificial intelligence);neural nets;object detection;video signal processing","computational baby learning;visual concept recognition;recognition capability improvement;weakly-supervised object detection;prior knowledge modelling;exemplar learning;video contexts;pre-trained convolutional neural network;initial concept detector;deep features;pre-trained CNN;online weakly labeled videos;Pascal VOC-07/10/12 object detection datasets;object category","","43","41","","","","","IEEE","IEEE Conferences"
"Self-supervised learning model for skin cancer diagnosis","A. Masood; A. Al- Jumaily; K. Anam","University of Technology Sydney, P.O. Box 123 Broadway, NSW 2007 Australia; School of Electrical, Mechanical and Mechatronic Systems, University of Technology, Sydney, Australia; University of Technology Sydney, P.O. Box 123 Broadway, NSW 2007 Australia","2015 7th International IEEE/EMBS Conference on Neural Engineering (NER)","","2015","","","1012","1015","Automated diagnosis of skin cancer is an active area of research with different classification methods proposed so far. However, classification models based on insufficient labeled training data can badly influence the diagnosis process if there is no self-advising and semi supervising capability in the model. This paper presents a semi supervised, self-advised learning model for automated recognition of melanoma using dermoscopic images. Deep belief architecture is constructed using labeled data together with unlabeled data, and fine tuning done by an exponential loss function in order to maximize separation of labeled data. In parallel a self-advised SVM algorithm is used to enhance classification results by counteracting the effect of misclassified data. To increase generalization capability and redundancy of the model, polynomial and radial basis function based SA-SVMs and Deep network are trained using training samples randomly chosen via a bootstrap technique. Then the results are aggregated using least square estimation weighting. The proposed model is tested on a collection of 100 dermoscopic images. The variation in classification error is analyzed with respect to the ratio of labeled and unlabeled data used in the training phase. The classification performance is compared with some popular classification methods and the proposed model using the deep neural processing outperforms most of the popular techniques including KNN, ANN, SVM and semi supervised algorithms like Expectation maximization and transductive SVM.","","","10.1109/NER.2015.7146798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7146798","","Support vector machines;Training;Skin cancer;Malignant tumors;Training data;Lesions","cancer;image classification;image segmentation;learning (artificial intelligence);least squares approximations;medical image processing;skin;support vector machines","self-supervised learning model;skin cancer diagnosis;classification models;training data;automated recognition;melanoma;dermoscopic imaging;deep belief architecture;exponential loss function;self-advised SVM algorithm;misclassified data effect;polynomial basis function;radial basis function;SA-SVM;bootstrap technique;least square estimation weighting;classification error;classification performance;deep neural processing outperforms;KNN;ANN;semisupervised algorithms;expectation maximization;transductive SVM","","11","29","","","","","IEEE","IEEE Conferences"
"Cross-scene crowd counting via deep convolutional neural networks","Cong Zhang; Hongsheng Li; X. Wang; Xiaokang Yang","Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, China; Department of Electronic Engineering, The Chinese University of Hong Kong, China; Department of Electronic Engineering, The Chinese University of Hong Kong, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","833","841","Cross-scene crowd counting is a challenging task where no laborious data annotation is required for counting people in new target surveillance crowd scenes unseen in the training set. The performance of most existing crowd counting methods drops significantly when they are applied to an unseen scene. To address this problem, we propose a deep convolutional neural network (CNN) for crowd counting, and it is trained alternatively with two related learning objectives, crowd density and crowd count. This proposed switchable learning approach is able to obtain better local optimum for both objectives. To handle an unseen target crowd scene, we present a data-driven method to fine-tune the trained CNN model for the target scene. A new dataset including 108 crowd scenes with nearly 200,000 head annotations is introduced to better evaluate the accuracy of cross-scene crowd counting methods. Extensive experiments on the proposed and another two existing datasets demonstrate the effectiveness and reliability of our approach.","","","10.1109/CVPR.2015.7298684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298684","","Training;Switches;Adaptation models;Head;Videos;Surveillance;Estimation","learning (artificial intelligence);natural scenes;neural nets;video retrieval;video surveillance","cross-scene crowd counting;deep-convolutional neural networks;target surveillance crowd scenes;learning objectives;crowd density;crowd count;switchable learning approach;local optimum;data-driven method;trained CNN model;target crowd scene;head annotations","","226","31","","","","","IEEE","IEEE Conferences"
"Visual attention with deep neural networks","A. Canziani; E. Culurciello","Weldon School of Biomedical Engineering, Purdue University, USA; Weldon School of Biomedical Engineering, Purdue University, USA","2015 49th Annual Conference on Information Sciences and Systems (CISS)","","2015","","","1","3","Animals use attentional mechanisms for being able to process enormous amount of sensory input in real time. Analogously, computerised systems could take advantage of similar techniques for achieving better timing performance. Visual attentional control uses bottom-up and top-down saliency maps for establishing the most relevant locations to observe. This article presents a novel fully-learnt unbiassed biologically plausible algorithm for computing both feature based and proto-object saliency maps, using a deep convolutional neural network simply trained on a single-class classification task, by unveiling its internal attentional apparatus. We are able to process 2 megapixels (MPs) colour images in real-time, i.e. at more than 10 frames per second, producing a 2MP map of interest.","","","10.1109/CISS.2015.7086900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7086900","","Visualization;Computational modeling;Feature extraction;Computer vision;Biological neural networks;Real-time systems","image classification;neural nets","visual attentional control;top-down saliency maps;bottom-up saliency maps;fully-learnt unbiassed biologically plausible algorithm;proto-object saliency maps;feature based saliency maps;deep convolutional neural network;single-class classification task;internal attentional apparatus","","3","23","","","","","IEEE","IEEE Conferences"
"Learning factorized feature transforms for speaker normalization","L. Samarakoon; K. C. Sim","School of Computing, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","145","152","This paper proposes an approach to improve automatic speech recognition (ASR) by normalizing the speaker variability of a well trained Deep Neural Network (DNN) acoustic model using i-vectors. Our approach learns a speaker dependent transformation of the acoustic features combined with the standard speaker dependent bias, to minimize the mismatch due to the inter-speaker variability. Speaker normalization experiments on the Aurora 4 task show 10.9% relative improvement over the baseline. Moreover, the proposed approach reported 4.5% relative improvement over the standard i-vector based method where only a speaker dependent bias is used. Furthermore, we report an analysis to compare our approach with the Constrained Maximum Likelihood Linear Regression (CMLLR) method.","","","10.1109/ASRU.2015.7404787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404787","Automatic speech recognition;deep neural networks;speaker normalization","Adaptation models;Training;Hidden Markov models;Estimation;Mathematical model;Acoustics;Transforms","learning (artificial intelligence);maximum likelihood estimation;neural nets;regression analysis;speaker recognition","factorized feature transform learning;speaker normalization;automatic speech recognition;ASR;DNN acoustic model;deep neural network;speaker variability;inter-speaker variability;Aurora 4 task;CMLLR method;constrained maximum likelihood linear regression method","","9","31","","","","","IEEE","IEEE Conferences"
"High-Resolution SAR Image Classification via Deep Convolutional Autoencoders","J. Geng; J. Fan; H. Wang; X. Ma; B. Li; F. Chen","Sch. of Inf. & Commun. Eng., Dalian Univ. of Technol., Dalian, China; Sch. of Control Sci. & Eng., Dalian Univ. of Technol., Dalian, China; Sch. of Inf. & Commun. Eng., Dalian Univ. of Technol., Dalian, China; Sch. of Inf. & Commun. Eng., Dalian Univ. of Technol., Dalian, China; Space Star Technol. Co., Ltd., Beijing, China; Space Star Technol. Co., Ltd., Beijing, China","IEEE Geoscience and Remote Sensing Letters","","2015","12","11","2351","2355","Synthetic aperture radar (SAR) image classification is a hot topic in the interpretation of SAR images. However, the absence of effective feature representation and the presence of speckle noise in SAR images make classification difficult to handle. In order to overcome these problems, a deep convolutional autoencoder (DCAE) is proposed to extract features and conduct classification automatically. The deep network is composed of eight layers: a convolutional layer to extract texture features, a scale transformation layer to aggregate neighbor information, four layers based on sparse autoencoders to optimize features and classify, and last two layers for postprocessing. Compared with hand-crafted features, the DCAE network provides an automatic method to learn discriminative features from the image. A series of filters is designed as convolutional units to comprise the gray-level cooccurrence matrix and Gabor features together. Scale transformation is conducted to reduce the influence of the noise, which integrates the correlated neighbor pixels. Sparse autoencoders seek better representation of features to match the classifier, since training labels are added to fine-tune the parameters of the networks. Morphological smoothing removes the isolated points of the classification map. The whole network is designed ingeniously, and each part has a contribution to the classification accuracy. The experiments of TerraSAR-X image demonstrate that the DCAE network can extract efficient features and perform better classification result compared with some related algorithms.","","","10.1109/LGRS.2015.2478256","National Natural Science Foundation of China; Research of Dynamic Monitoring of High Resolution Sea Area Application; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7286736","Deep convolutional autoencoders (DCAEs);supervised classification;synthetic aperture radar (SAR) image;Deep convolutional autoencoders (DCAEs);supervised classification;synthetic aperture radar (SAR) image","Feature extraction;Synthetic aperture radar;Noise;Accuracy;Speckle;Training;Transforms","convolution;feature extraction;image classification;image texture;matrix algebra;radar imaging;radar resolution;speckle;synthetic aperture radar","high-resolution SAR image classification;deep convolutional autoencoders;synthetic aperture radar;feature representation;speckle noise;feature extraction;convolutional layer;texture features;scale transformation layer;sparse autoencoders;convolutional units;gray-level cooccurrence matrix;Gabor features;training labels;morphological smoothing;TerraSAR-X image","","90","18","","","","","IEEE","IEEE Journals"
"Extreme learning machines in the field of text classification","R. K. Roul; A. Nanda; V. Patel; S. K. Sahay","Department of Computer Science, BITS Pilani-K.K. Birla Goa Campus; Department of Computer Science, BITS Pilani-K.K. Birla Goa Campus; Department of Computer Science, BITS Pilani-K.K. Birla Goa Campus; Department of Computer Science, BITS Pilani-K.K. Birla Goa Campus","2015 IEEE/ACIS 16th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","","2015","","","1","7","The World Wide Web serves as a huge repository of information that is highly dynamic, diverse and growing at an exponential rate in a lightening speed. In order to speed-up and further improve tasks like information search and retrieval, personalization etc; it is highly important to develop techniques to classify text documents more accurately and efficiently than before. This paper is an effort in that direction, where the effectiveness of Extreme Learning Machines(ELM) in the domain of text classification is studied and compared with many of the existing relevant techniques like Support Vector Machines(SVM), which are currently one of the most popular and effective techniques for classifying text documents. Ours is one of the few works that highlight the high performance of ELM in the field of text classification, by implementing classifiers based on different interpretations of ELM, analyzing their performance, and studying which feature selection techniques are most suited to improve their accuracy. In our multi-class classification problem, we studied a single ELM classifier based on the one-against-all scheme, and a multi-layer ELM classifier inspired from deep networks, and then perform extensive experiments on different datasets to demonstrate the applicability and effectiveness of our approach. Results show that ELM based classifiers can outperform many of the traditional classification techniques including the most powerful state-of-the-art technique such as SVM.","","","10.1109/SNPD.2015.7176204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176204","Extreme Learning Machine;Feature Selection;Multi-layer ELM;Support Vector Machine;Text Classification","Support vector machines;Training;Machine learning algorithms;Tuning;Training data;Computer science;Electronic mail","feature selection;learning (artificial intelligence);parallel processing;pattern classification;support vector machines;text analysis","World Wide Web;text document classification;extreme learning machine;support vector machine;SVM;feature selection technique;multiclass classification problem;multilayer ELM classifier","","7","15","","","","","IEEE","IEEE Conferences"
"Rotating your face using multi-task deep neural network","Junho Yim; Heechul Jung; ByungIn Yoo; Changkyu Choi; Dusik Park; Junmo Kim","School of Electrical Engineering, KAIST, South Korea; School of Electrical Engineering, KAIST, South Korea; School of Electrical Engineering, KAIST, South Korea; Samsung Advanced Institute of Technology, Korea; Samsung Advanced Institute of Technology, Korea; School of Electrical Engineering, KAIST, South Korea","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","676","684","Face recognition under viewpoint and illumination changes is a difficult problem, so many researchers have tried to solve this problem by producing the pose- and illumination- invariant feature. Zhu et al. [26] changed all arbitrary pose and illumination images to the frontal view image to use for the invariant feature. In this scheme, preserving identity while rotating pose image is a crucial issue. This paper proposes a new deep architecture based on a novel type of multitask learning, which can achieve superior performance in rotating to a target-pose face image from an arbitrary pose and illumination image while preserving identity. The target pose can be controlled by the user's intention. This novel type of multi-task model significantly improves identity preservation over the single task model. By using all the synthesized controlled pose images, called Controlled Pose Image (CPI), for the pose-illumination-invariant feature and voting among the multiple face recognition results, we clearly outperform the state-of-the-art algorithms by more than 4~6% on the MultiPIE dataset.","","","10.1109/CVPR.2015.7298667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298667","","Face;Lighting;Face recognition;Feature extraction;Image reconstruction;Training;Three-dimensional displays","face recognition;learning (artificial intelligence);lighting;neural nets;pose estimation","deep architecture;multitask learning;target-pose face image rotation;illumination image;identity preservation;controlled pose image;pose-illumination-invariant feature;face recognition;multitask deep neural network;CPI","","2","27","","","","","IEEE","IEEE Conferences"
"Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs","Bo Li; Chunhua Shen; Yuchao Dai; A. van den Hengel; Mingyi He","Northwestern Polytechnical University, China; University of Adelaide, Australia; Australian National University, Canberra ACT 0200, Australia; University of Adelaide, Australia; Northwestern Polytechnical University, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","1119","1127","Predicting the depth (or surface normal) of a scene from single monocular color images is a challenging task. This paper tackles this challenging and essentially underdetermined problem by regression on deep convolutional neural network (DCNN) features, combined with a post-processing refining step using conditional random fields (CRF). Our framework works at two levels, super-pixel level and pixel level. First, we design a DCNN model to learn the mapping from multi-scale image patches to depth or surface normal values at the super-pixel level. Second, the estimated super-pixel depth or surface normal is refined to the pixel level by exploiting various potentials on the depth or surface normal map, which includes a data term, a smoothness term among super-pixels and an auto-regression term characterizing the local structure of the estimation map. The inference problem can be efficiently solved because it admits a closed-form solution. Experiments on the Make3D and NYU Depth V2 datasets show competitive results compared with recent state-of-the-art methods.","","","10.1109/CVPR.2015.7298715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298715","","Estimation;Feature extraction;Training;Context;Color;Three-dimensional displays;Training data","feature extraction;image colour analysis;inference mechanisms;learning (artificial intelligence);neural nets;regression analysis;smoothing methods","depth normal estimation;surface normal estimation;monocular images;deep features;hierarchical CRF;depth Predicting;deep-convolutional neural network features;postprocessing refining step;conditional random fields;CRF;super-pixel level;DCNN model;mapping learning;multiscale image patches;surface normal map;depth normal map;smoothness term;data term;auto-regression term;local structure;estimation map;inference problem;closed-form solution;Make3D dataset;NYU Depth V2 dataset","","9","30","","","","","IEEE","IEEE Conferences"
"Deep Convolutional Neural Networks for efficient vision based tunnel inspection","K. Makantasis; E. Protopapadakis; A. Doulamis; N. Doulamis; C. Loupos","Technical University of Crete, Chania, Greece; Technical University of Crete, Chania, Greece; National Technical University of Athens, Greece; National Technical University of Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece","2015 IEEE International Conference on Intelligent Computer Communication and Processing (ICCP)","","2015","","","335","342","The inspection, assessment, maintenance and safe operation of the existing civil infrastructure consists one of the major challenges facing engineers today. Such work requires either manual approaches, which are slow and yield subjective results, or automated approaches, which depend upon complex handcrafted features. Yet, for the latter case, it is rarely known in advance which features are important for the problem at hand. In this paper, we propose a fully automated tunnel assessment approach; using the raw input from a single monocular camera we hierarchically construct complex features, exploiting the advantages of deep learning architectures. Obtained features are used to train an appropriate defect detector. In particular, we exploit a Convolutional Neural Network to construct high-level features and as a detector we choose to use a Multi-Layer Perceptron due to its global function approximation properties. Such an approach achieves very fast predictions due to the feedforward nature of Convolutional Neural Networks and Multi-Layer Perceptrons.","","","10.1109/ICCP.2015.7312681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312681","","Feature extraction;Visualization;Image edge detection;Concrete;Inspection;Entropy;Kernel","civil engineering computing;computer vision;function approximation;geotechnical engineering;inspection;learning (artificial intelligence);maintenance engineering;multilayer perceptrons;tunnels","deep convolutional neural networks;vision based tunnel inspection;civil infrastructure inspection;civil infrastructure assessment;civil infrastructure maintenance;operation safety;automated tunnel assessment approach;monocular camera;deep learning architectures;defect detector training;high-level features;global function approximation properties;feedforward nature;multilayer perceptrons","","33","28","","","","","IEEE","IEEE Conferences"
"Current measurements from a deep real-time metocean mooring: lessons learned on real-time data QA/QC","B. A. Magnell; L. I. Ivanov; A. T. Morrison; E. G. Hasbrouck","Oceanography and Measurement Systems Woods Hole Group Inc. East Falmouth, USA; Oceanography and Measurement Systems Woods Hole Group Inc. East Falmouth, USA; Oceanography and Measurement Systems Woods Hole Group Inc. East Falmouth, USA; Oceanography and Measurement Systems Woods Hole Group Inc. East Falmouth, USA","2015 IEEE/OES Eleveth Current, Waves and Turbulence Measurement (CWTM)","","2015","","","1","6","An automated Quality Assurance and Quality Control (QA/QC) procedure implemented by Woods Hole Group, Inc. (WHG) to improve the quality of ADCP data collected on a deep Real-Time Metocean Mooring (RTMM) is described. The RTMM is equipped with three TRDI ADCPs and one Nortek single-point current meter (SPCM). A full water column current profile is collected every 20 minutes and the data are sent by satellite to a shore-based computer and web server, which creates graphical displays in near-real-time and serves them over the Internet to user's web browsers. WHG's automated QA/QC procedure was developed based on analysis of a first batch of ADCP data collected by the RTMM and is system-specific. The analysis of the data and of data quality indicators revealed major causes that lead to the occurrence of spurious and/or noisy current data. It also suggested approaches to improving the quality of the data provided to the client in near-real-time. The analysis revealed certain limitations of the QA/QC algorithm implemented by NDBC and recommended by QARTOD. From our experience with the RTMM data, the QA/QC of ADCP data should not be limited to automated review of quality parameters measured by the ADCP on individual ensemble-averaged profiles. A better result is achieved by adjusting ADCP setup and customizing data screening thresholds for a specific deployment. The thresholds should be customized based on the analysis of the system-specific data.","","","10.1109/CWTM.2015.7098133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7098133","deep metocean mooring;current measurements;real-time data QA/QC;data screening","Acoustic beams;Sea measurements;Real-time systems;Current measurement;Sea surface;Noise measurement;Acoustic measurements","ocean waves;oceanographic techniques","real-time data;automated quality assurance procedure;automated quality control procedure;Woods Hole Group;ADCP data quality;deep RTMM;Real-Time Metocean mooring;Nortek single-point current meter;shore-based computer;web server;QA-QC algorithm;RTMM data","","","3","","","","","IEEE","IEEE Conferences"
"Multi-speaker modeling and speaker adaptation for DNN-based TTS synthesis","Y. Fan; Y. Qian; F. K. Soong; L. He","Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4475","4479","In DNN-based TTS synthesis, DNNs hidden layers can be viewed as deep transformation for linguistic features and the output layers as representation of acoustic space to regress the transformed linguistic features to acoustic parameters. The deep-layered architectures of DNN can not only represent highly-complex transformation compactly, but also take advantage of huge amount of training data. In this paper, we propose an approach to model multiple speakers TTS with a general DNN, where the same hidden layers are shared among different speakers while the output layers are composed of speaker-dependent nodes explaining the target of each speaker. The experimental results show that our approach can significantly improve the quality of synthesized speech objectively and subjectively, comparing with speech synthesized from the individual, speaker-dependent DNN-based TTS. We further transfer the hidden layers for a new speaker with limited training data and the resultant synthesized speech of the new speaker can also achieve a good quality in term of naturalness and speaker similarity.","","","10.1109/ICASSP.2015.7178817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178817","statistical parametric speech synthesis;deep neural networks;multi-task learning;transfer learning","Speech;Training;Pragmatics;Hidden Markov models;Acoustics;Adaptation models;Training data","neural nets;speech synthesis","multispeaker modeling;speaker adaptation;DNN-based TTS synthesis;linguistic features;deep-layered architectures;speaker-dependent nodes;deep neural networks;text-to-speech synthesis","","21","10","","","","","IEEE","IEEE Conferences"
"Forecasting the weather of Nevada: A deep learning approach","M. Hossain; B. Rekabdar; S. J. Louis; S. Dascalu","Dept of Computer Science and Engineering, University of Nevada, Reno, USA; Dept of Computer Science and Engineering, University of Nevada, Reno, USA; Dept of Computer Science and Engineering, University of Nevada, Reno, USA; Dept of Computer Science and Engineering, University of Nevada, Reno, USA","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","6","This paper compares two approaches for predicting air temperature from historical pressure, humidity, and temperature data gathered from meteorological sensors in Northwestern Nevada. We describe our data and our representation and compare a standard neural network against a deep learning network. Our empirical results indicate that a deep neural network with Stacked Denoising Auto-Encoders (SDAE) outperforms a standard multilayer feed forward network on this noisy time series prediction task. In addition, predicting air temperature from historical air temperature data alone can be improved by employing related weather variables like barometric pressure, humidity and wind speed data in the training process.","","","10.1109/IJCNN.2015.7280812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280812","","Meteorology;Artificial neural networks;Noise reduction;History","atmospheric humidity;atmospheric temperature;feedforward neural nets;time series;weather forecasting;wind","Nevada weather forecasting;air temperature predictiion;pressure data;humidity data;meteorological sensor;Northwestern Nevada;neural network;deep learning network;stacked denoising autoencoder;multilayer feed forward network;time series prediction task;air temperature data;barometric pressure;wind speed data","","14","19","","","","","IEEE","IEEE Conferences"
"Structure discovery of deep neural network based on evolutionary algorithms","T. Shinozaki; S. Watanabe","Tokyo Institute of Technology, 4259 Nagatsuta-cho, Midori-ku, Yokohama, Kanagawa, 226-8502 JAPAN; Mitsubishi Electric Research Laboratories (MERL), 201 Broadway, Cambridge, MA, 02139 USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4979","4983","Deep neural networks (DNNs) are constructed by considering highly complicated configurations including network structure and several tuning parameters (number of hidden states and learning rate in each layer), which greatly affect the performance of speech processing applications. To reach optimal performance in such systems, deep understanding and expertise in DNNs is necessary, which limits the development of DNN systems to skilled experts. To overcome the problem, this paper proposes an efficient optimization strategy for DNN structure and parameters using evolutionary algorithms. The proposed approach parametrizes the DNN structure by a directed acyclic graph, and the DNN structure is represented by a simple binary vector. Genetic algorithm and covariance matrix adaptation evolution strategy efficiently optimize the performance jointly with respect to the above binary vector and the other tuning parameters. Experiments on phoneme recognition and spoken digit detection tasks show the effectiveness of the proposed approach by discovering the appropriate DNN structure automatically.","","","10.1109/ICASSP.2015.7178918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178918","Deep neural network;structure optimization;evolutionary algorithm;genetic algorithm;CMA-ES","Optimization;Evolutionary computation;Genetic algorithms;Training;Tuning;Feature extraction;Speech processing","covariance matrices;directed graphs;genetic algorithms;learning (artificial intelligence);speech processing;speech recognition;vectors","structure discovery;deep neural networks;evolutionary algorithms;network structure;tuning parameters;speech processing applications;optimization strategy;DNN structure;directed acyclic graph;binary vector;genetic algorithm;covariance matrix adaptation evolution strategy;phoneme recognition task;spoken digit detection task","","21","23","","","","","IEEE","IEEE Conferences"
"Classification of Persian handwritten digits using spiking neural networks","K. Kiani; E. M. Korayem","Faculty of Electrical and Computer Engineering, Semnan University, Semnan, Iran; Faculty of Electrical and Computer Engineering, Semnan University, Semnan, Iran","2015 2nd International Conference on Knowledge-Based Engineering and Innovation (KBEI)","","2015","","","1113","1116","In recent years Spiking Neural Networks (SNNs) have gained in popularity due to their low complexity. They have been used in many processes like learning and classification of data such as images. In this paper we have used the SNN Model, in order to have robust learning and classification of handwritten digits, i.e., to have a learning process which is persistent against changes and high noise levels. Due to the similarities among handwritten digits, the classifications have been erratic but the Deep Belief Network we have used in this paper solves this problem to a great extent. Our model consists of three layers. The first layer, composed of 225 neurons (15*15 pixels for each image), works as the receptor of input images. The middle layer is used for processes, encoding and network learning, while the last layer, which is composed of 10 neurons (as we have 10 distinct classes), does the job of prediction and classification of images. The model was implemented using MATLAB and we have used Hoda Persian handwritten digits dataset as our input images. The obtained results show that the implemented model can carry out, with good accuracy (95%), the learning and classification of images of handwritten digits with high levels of noise.","","","10.1109/KBEI.2015.7436202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436202","Spiking neural networks;SNN;Image classification;deep belief networks;STDP;Artificial neuron","Decision support systems;Biological neural networks;Image classification;Neurons","belief networks;handwritten character recognition;image classification;natural language processing;neural nets","Persian handwritten digit classification;spiking neural networks;data classification;data learning;SNN model;robust handwritten digit learning;handwritten digit classification;deep belief network;network learning;image classification;MATLAB;Hoda Persian handwritten digit dataset","","3","10","","","","","IEEE","IEEE Conferences"
"Real-Time Activity Recognition on Smartphones Using Deep Neural Networks","L. Zhang; X. Wu; D. Luo","Key Lab. of Machine Perception (Minist. of Educ.), Peking Univ. Beijing, Beijing, China; Key Lab. of Machine Perception (Minist. of Educ.), Peking Univ. Beijing, Beijing, China; Key Lab. of Machine Perception (Minist. of Educ.), Peking Univ. Beijing, Beijing, China","2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)","","2015","","","1236","1242","Real-time activity recognition is an important research problem, which has drawn the attention of many researchers for many years. All previous works of real-time activity recognition needed to manually extract features and then fed these features into the classifiers as the inputs. However, due to the recognition time limit, they could only extract a small amount of features, which might not achieve good accuracy and could be improved. Besides, these features were manually extracted according to the experience of the researchers. Because researchers did not know which features worked better for classification, it was hard to choose suitable features, thus these manually extracted features might not be related to the specific classification task and have poor discrimination ability and could be improved. In this paper, we recommend deep neural networks for real-time activity recognition, which automatically learn suitable features and then perform classification. We collected accelerometer data of seven activities of interest on an android smartphone, including walking, running, standing, sitting, lying, walking upstairs and walking downstairs, and conducted experiments on our collected dataset to compare our method with traditional methods. Besides, we implemented a deep neural network on a smartphone and tested the recognition time of the model. The results showed that deep neural networks could achieve real-time performance and got higher accuracy than traditional methods.","","","10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518403","deep neural networks;real-time activity recognition;smartphone;accelerometer;feature learning","Feature extraction;Smart phones;Legged locomotion;Real-time systems;Accelerometers;Neural networks","gesture recognition;image classification;neural nets","real-time activity recognition;deep neural networks;feature extraction;Android smartphone","","4","29","","","","","IEEE","IEEE Conferences"
"Speaker Adaptation Using Speaker Similarity Score on DNN Features","M. Rizwan; D. V. Anderson","NA; NA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","877","882","This paper proposes a novel speaker adaptation algorithm for classifying speech based on deep neural networks (DNNs). The adaptation algorithm consists of two steps. In the first step a deep neural network is trained using raw Mel-frequency cepstral coefficient (MFCC) features to discover hidden structures in the data and employing the activations of the last hidden layers of the DNN as acoustic features. In the second step using nearest neighbor, an adaptation algorithm learns speaker similarity scores based on a small amount of adaptation data from each target speaker using the DNN-based acoustic features. Based on the speaker similarity score, classification is done using a k-nearest neighbor (k-NN) classifier. The novelty of this work is that instead of modifying and re-training the DNN for speaker adaptation, which comprises a large number of parameters and is computationally expensive, activations of the learned DNN are used to project features from MFCC to a sparse DNN space, then speaker adaptation is performed based on similarity (i.e. nearest neighbor) using k-NN algorithm. With only a small amount of adaptation data, it reduces the number of phoneme classification error in the TIMIT dataset by 23%. This work also analyzes impact of deep neural networks architecture on speaker adaptation performance.","","","10.1109/ICMLA.2015.168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424432","Deep neural networks;k-nearest neighbor;phoneme classification;feature learning;speaker adaptation","Neural networks;Training;Feature extraction;Hidden Markov models;Speech;Mel frequency cepstral coefficient;Testing","cepstral analysis;neural nets;pattern classification;speaker recognition","speaker adaptation;speaker similarity score;DNN features;speech classification;deep neural networks;Mel-frequency cepstral coefficient;MFCC features;k-nearest neighbor classifier;k-NN classifier","","2","32","","","","","IEEE","IEEE Conferences"
"3D ShapeNets: A deep representation for volumetric shapes","Zhirong Wu; S. Song; A. Khosla; Fisher Yu; Linguang Zhang; Xiaoou Tang; J. Xiao","Princeton University, USA; Princeton University, USA; Massachusetts Institute of Technology, USA; Princeton University, USA; Princeton University, USA; Chinese University of Hong Kong, China; Princeton University, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","1912","1920","3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.","","","10.1109/CVPR.2015.7298801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298801","","Three-dimensional displays;Shape;Solid modeling;Object recognition;Planning;Computational modeling;Convolution","belief networks;CAD;computer vision;image recognition;image representation;object recognition;solid modelling;statistical distributions","3D ShapeNets;volumetric shapes;computer vision systems;2.5D depth sensors;3D shape representation;category recognition;full 3D shape recovery;visual understanding;geometric 3D shape;probability distribution;3D voxel grid;convolutional deep belief network;hierarchical compositional part representation;joint object recognition;shape completion;active object recognition;view planning;3D deep learning model;ModelNet;3D CAD model dataset;3D deep representation","","69","33","","","","","IEEE","IEEE Conferences"
"Deep roto-translation scattering for object classification","E. Oyallon; S. Mallat","Département Informatique, Ecole Normale Supérieure, 45 rue d'Ulm, 75005 Paris, France; Département Informatique, Ecole Normale Supérieure, 45 rue d'Ulm, 75005 Paris, France","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","2865","2873","Dictionary learning algorithms or supervised deep convolution networks have considerably improved the efficiency of predefined feature representations such as SIFT. We introduce a deep scattering convolution network, with complex wavelet filters over spatial and angular variables. This representation brings an important improvement to results previously obtained with predefined features over object image databases such as Caltech and CIFAR. The resulting accuracy is comparable to results obtained with unsupervised deep learning and dictionary based representations. This shows that refining image representations by using geometric priors is a promising direction to improve image classification and its understanding.","","","10.1109/CVPR.2015.7298904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298904","","Wavelet transforms;Scattering;Convolution;Computer architecture;Support vector machines;Three-dimensional displays","convolution;image classification;image filtering;image representation;wavelet transforms","deep roto-translation scattering;object classification;dictionary learning algorithms;supervised deep convolution networks;predefined feature representations;SIFT;deep scattering convolution network;complex wavelet filters;Caltech object image database;CIFAR object image database;image representations;image classification","","27","30","","","","","IEEE","IEEE Conferences"
"Speaker Adaptive Training of Deep Neural Network Acoustic Models Using I-Vectors","Y. Miao; H. Zhang; F. Metze","Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pttsburgh, PA, USA; Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pttsburgh, PA, USA; Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pttsburgh, PA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2015","23","11","1938","1949","In acoustic modeling, speaker adaptive training (SAT) has been a long-standing technique for the traditional Gaussian mixture models (GMMs). Acoustic models trained with SAT become independent of training speakers and generalize better to unseen testing speakers. This paper ports the idea of SAT to deep neural networks (DNNs), and proposes a framework to perform feature-space SAT for DNNs. Using i-vectors as speaker representations, our framework learns an adaptation neural network to derive speaker-normalized features. Speaker adaptive models are obtained by fine-tuning DNNs in such a feature space. This framework can be applied to various feature types and network structures, posing a very general SAT solution. In this paper, we fully investigate how to build SAT-DNN models effectively and efficiently. First, we study the optimal configurations of SAT-DNNs for large-scale acoustic modeling tasks. Then, after presenting detailed comparisons between SAT-DNNs and the existing DNN adaptation methods, we propose to combine SAT-DNNs and model-space DNN adaptation during decoding. Finally, to accelerate learning of SAT-DNNs, a simple yet effective strategy, frame skipping, is employed to reduce the size of training data. Our experiments show that compared with a strong DNN baseline, the SAT-DNN model achieves 13.5% and 17.5% relative improvement on word error rates (WERs), without and with model-space adaptation applied respectively. Data reduction based on frame skipping results in 2 × speed-up for SAT-DNN training, while causing negligible WER loss on the testing data.","","","10.1109/TASLP.2015.2457612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160703","Acoustic modeling;deep neural networks (DNNs);speaker adaptive training (SAT)","Adaptation models;Training;Speech;Acoustics;Testing;IEEE transactions;Speech processing","acoustic signal processing;Gaussian processes;learning (artificial intelligence);mixture models;neural nets;signal representation;speaker recognition;vectors","speaker adaptive training;feature-space SAT;deep neural network acoustic models;i-vectors;Gaussian mixture models;GMM;speaker representations;speaker-normalized features;feature types;network structures;SAT-DNN models;large-scale acoustic modeling tasks;model-space DNN adaptation;frame skipping;training data size reduction;word error rates;negligible WER loss","","34","56","","","","","IEEE","IEEE Journals"
"Prediction of Users' Response Time in Q&A Communities","N. Burlutskiy; A. Fish; N. Ali; M. Petridis","NA; NA; NA; NA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","618","623","Social media and online Question and Answer (Q&A) communities in particular have become a successful solution for finding answers on diverse topics. However, not all questions are answered by these communities. Also, many questions are not answered quickly enough. In this paper, we propose a framework for predicting users' response time. The framework uses a diverse set of features including information on users, the content they generate while communicating, question tags, spatial and temporal features. Then these features are used as input for training predictive models by various machine learning algorithms. As a case study, three diverse Q&A communities from Stack Exchange are selected to test the framework. We demonstrate that Deep Belief Networks outperform Logistic Regression (LR), k-nearest neighbors (k-NN), and Decision Trees (DT) in the accuracy of the prediction across the three diverse Q&A communities.","","","10.1109/ICMLA.2015.190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424386","Social media;temporal user behavior;machine learning","Time factors;Predictive models;Twitter;Prediction algorithms;Algorithm design and analysis;Context;Electronic mail","belief networks;decision trees;learning (artificial intelligence);question answering (information retrieval);regression analysis;social networking (online)","user response time prediction;Q&A communities;social media;online question and answer communities;predictive modeltraining;machine learning algorithms;stack exchange;deep belief networks;logistic regression;LR;k-nearest neighbors;k-NN;decision trees;DT","","3","21","","","","","IEEE","IEEE Conferences"
"Designing a socially assistive robot for personalized number concepts learning in preschool children","C. Clabaugh; G. Ragusa; F. Sha; M. Matarić","Computer Science Department, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 30332-0250; Division of Engineering Education, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 30332-0250; Computer Science Department, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 30332-0250; Computer Science Department, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 30332-0250","2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)","","2015","","","314","319","Designing technological systems for personalized education is an iterative and interdisciplinary process that demands a deep understanding of the application domain, the limitations of current methods and technologies, and the computational methods and complexities behind user modeling and adaptation. We present our design process and the Socially Assistive Robot (SAR) tutoring system to support the efforts of educators in teaching number concepts to preschool children. We focus on the computational considerations of designing a SAR system for young children that may later be personalized along multiple dimensions. We conducted an initial data collection to validate that the system is at the proper challenge level for our target population, and discovered promising patterns in participants' learning styles, nonverbal behavior, and performance. We discuss our plans to leverage the data collected to learn and validate a computational, multidimensional model of number concepts learning.","","","10.1109/DEVLRN.2015.7346164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346164","","Robots;Education;Computational modeling;Data collection;Sociology;Statistics;Visualization","computational complexity;control engineering computing;educational robots;intelligent tutoring systems;number theory;teaching","socially assistive robot design;personalized number concept learning;preschool children;technological system design;personalized education;computational method;computational complexity;user modeling;user adaptation;SAR tutoring system;number concept teaching;participant learning styles;nonverbal behavior;number concept learning multidimensional model;intelligent tutoring systems","","11","22","","","","","IEEE","IEEE Conferences"
"Differential Evolution with an Evolution Path: A DEEP Evolutionary Algorithm","Y. Li; Z. Zhan; Y. Gong; W. Chen; J. Zhang; Y. Li","Sun Yat-sen University, Guangzhou, China; Sun Yat-sen University, Guangzhou, China; Sun Yat-sen University, Guangzhou, China; Sun Yat-sen University, Guangzhou, China; Sun Yat-sen University, Guangzhou, China; School of Engineering, University of Glasgow, Glasgow, U.K.","IEEE Transactions on Cybernetics","","2015","45","9","1798","1810","Utilizing cumulative correlation information already existing in an evolutionary process, this paper proposes a predictive approach to the reproduction mechanism of new individuals for differential evolution (DE) algorithms. DE uses a distributed model (DM) to generate new individuals, which is relatively explorative, whilst evolution strategy (ES) uses a centralized model (CM) to generate offspring, which through adaptation retains a convergence momentum. This paper adopts a key feature in the CM of a covariance matrix adaptation ES, the cumulatively learned evolution path (EP), to formulate a new evolutionary algorithm (EA) framework, termed DEEP, standing for DE with an EP. Without mechanistically combining two CM and DM based algorithms together, the DEEP framework offers advantages of both a DM and a CM and hence substantially enhances performance. Under this architecture, a self-adaptation mechanism can be built inherently in a DEEP algorithm, easing the task of predetermining algorithm control parameters. Two DEEP variants are developed and illustrated in the paper. Experiments on the CEC'13 test suites and two practical problems demonstrate that the DEEP algorithms offer promising results, compared with the original DEs and other relevant state-of-the-art EAs.","","","10.1109/TCYB.2014.2360752","National High-Technology Research and Development Program (863 Program) of China; National Natural Science Foundation of China (NSFC); NSFC Key Program; NSFC for Distinguished Young Scholars; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6919306","Cumulative learning;differential evolution (DE);evolution path (EP);evolutionary computation;Cumulative learning;differential evolution (DE);evolution path (EP);evolutionary computation","Sociology;Vectors;Adaptation models;Algorithm design and analysis;Covariance matrices;Optimization","covariance matrices;evolutionary computation","differential evolution with evolution path algorithm;DEEP evolutionary algorithm;cumulative correlation information;predictive approach;reproduction mechanism;distributed model;evolution strategy;centralized model;convergence momentum;covariance matrix adaptation ES;EA framework;DM based algorithms;CM based algorithms","","71","54","","","","","IEEE","IEEE Journals"
"Discriminatively-learned global image representation using CNN as a local feature extractor for image retrieval","W. Ku; H. Chou; W. Peng","Department of Computer Science, National Chiao Tung University, Taiwan; Department of Computer Science, National Chiao Tung University, Taiwan; Department of Computer Science, National Chiao Tung University, Taiwan","2015 Visual Communications and Image Processing (VCIP)","","2015","","","1","4","This work introduces an image retrieval framework based on using deep convolutional neural networks (CNN) as a local feature extractor. Motivated by the great success of CNN in recognition tasks, one may be tempted to simply adopt the output of CNN as a global image representation for retrieval. This straightforward approach, however, has proved deficient, because it can be vulnerable to various image transformation attacks. To address this issue, we propose to treat CNN as a local feature extractor, and a local image patch selection mechanism is developed to extract discriminative patches by observing their objectness responses, aspect ratios, relative scales, and locations in the image. The criterion is given by a learned posterior probability indicating how likely the image patch in question will find a correspondence in another similar image. In addition, the CNN's weight parameters are specifically adapted by a contrastive loss function to suit retrieval tasks. Extensive experiments on typical retrieval datasets confirm the superiority of the proposed scheme over the state-of-the-art methods.","","","10.1109/VCIP.2015.7457829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457829","image retrieval;image representation;feature learning;deep convolutional neural network;object detection","Feature extraction;Image representation;Data mining;Image retrieval;Pipelines;Training;Neural networks","feature extraction;image recognition;image representation;image retrieval;probability","discriminatively-learned global image representation;CNN;local feature extractor;image retrieval;deep convolutional neural networks;image transformation attacks;local image patch selection mechanism;discriminative patch extraction;objectness responses;aspect ratios;relative scales","","8","17","","","","","IEEE","IEEE Conferences"
"DCTNet: A simple learning-free approach for face recognition","C. J. Ng; A. Beng Jin Teoh","Yonsei University, Seoul, South Korea; Yonsei University, Seoul, South Korea","2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","","2015","","","761","768","PCANet was proposed as a lightweight deep learning network that mainly leverages Principal Component Analysis (PCA) to learn multistage filter banks followed by binarization and block-wise histograming. PCANet was shown worked surprisingly well in various image classification tasks. However, PCANet is data-dependence hence inflexible. In this paper, we proposed a data-independence network, dubbed DCTNet for face recognition in which we adopt Discrete Cosine Transform (DCT) as filter banks in place of PCA. This is motivated by the fact that 2D DCT basis is indeed a good approximation for high ranked eigenvectors of PCA. Both 2D DCT and PCA resemble a kind of modulated sine-wave patterns, which can be perceived as a bandpass filter bank. DCTNet is free from learning as 2D DCT bases can be computed in advance. Besides that, we also proposed an effective method to regulate the block-wise histogram feature vector of DCTNet for robustness. It is shown to provide surprising performance boost when the probe image is considerably different in appearance from the gallery image. We evaluate the performance of DCTNet extensively on a number of benchmark face databases and being able to achieve on par with or often better accuracy performance than PCANet.","","","10.1109/APSIPA.2015.7415375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415375","","Discrete cosine transforms;Principal component analysis;Filter banks;Histograms;Convolution;Correlation;Filtering theory","band-pass filters;channel bank filters;discrete cosine transforms;face recognition;image classification;learning (artificial intelligence)","DCTNet approach;learning-free approach;face recognition;PCANet;principal component analysis;deep learning network;multistage filter banks;binarization;blockwise histogram;image classification tasks;data-independence network;discrete cosine transform;DCT;modulated sine-wave patterns;bandpass filter bank","","16","26","","","","","IEEE","IEEE Conferences"
"Collaborative learning from Mobile Crowd Sensing: A case study in electromagnetic monitoring","A. Longo; M. Zappatore; M. A. Bochicchio","Dept. of Innovation Engineering, University of Salento, via Monteroni sn, 73100 - Lecce (Italy); Dept. of Innovation Engineering, University of Salento, via Monteroni sn, 73100 - Lecce (Italy); Dept. of Innovation Engineering, University of Salento, via Monteroni sn, 73100 - Lecce (Italy)","2015 IEEE Global Engineering Education Conference (EDUCON)","","2015","","","742","750","Personal mobile devices are nowadays so pervasive that a broad range of novel learning practices and paradigms can profitably exploits them. Mobile Crowd Sensing (MCS) is one of them. In MCS, mobiles act as data sources for monitoring tasks (e.g., traffic planning, air pollution monitoring, emergency management), thanks to their computational capability and their embedded sensors. From a pedagogical perspective, MCS offers continuous learning experiences that increases students' skills and expertise by engaging them directly into practical activities and on-the-field analyses. However, the wide diffusion of mobiles requires a reliable wireless coverage, to guarantee proper Quality of Service levels, thus potentially increasing the electromagnetic field levels in a given geographical area. Therefore, we propose a complete data warehouse solution that exploits MCS paradigm to pursue three main research purposes. Firstly, motivating students from engineering courses to acquire a better knowledge in wireless communication topics by offering them experiential and collaborative learning approaches. Secondly, performing a preliminary screening of the signals received by mobiles for 3G/4G standards (e.g., UMTS, LTE), since this domain did not benefit from MCS solutions so far. Thirdly, identifying prospective areas where more detailed measuring campaigns must be addressed. A deep analysis of the achievable pedagogical benefits as well as the thorough description of both design and implementation phases is provided. Evaluation results and preliminary users' feedback complete this research work.","","","10.1109/EDUCON.2015.7096052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7096052","Collaborative Learning;Seamless Learning;Mobile Crowd Sensing;Multidimensional Data Analysis;Data Warehouse;Electromagnetic Monitoring","Sensors;Monitoring;Mobile communication;Antenna measurements;Wireless communication;Wireless sensor networks;Servers","3G mobile communication;4G mobile communication;computer aided instruction;data warehouses;educational courses;electromagnetic waves;groupware;mobile computing;telecommunication computing;telecommunication engineering education","collaborative learning;mobile crowd sensing;electromagnetic monitoring;MCS;data warehouse;engineering courses;wireless communication topics;experiential learning;3G/4G standards","","7","41","","","","","IEEE","IEEE Conferences"
"Deep networks for saliency detection via local estimation and global search","L. Wang; H. Lu; X. Ruan; M. Yang","Dalian University of Technology, China; Dalian University of Technology, China; OMRON Corporation, Japan; University of California at Merced, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3183","3192","This paper presents a saliency detection algorithm by integrating both local estimation and global search. In the local estimation stage, we detect local saliency by using a deep neural network (DNN-L) which learns local patch features to determine the saliency value of each pixel. The estimated local saliency maps are further refined by exploring the high level object concepts. In the global search stage, the local saliency map together with global contrast and geometric information are used as global features to describe a set of object candidate regions. Another deep neural network (DNN-G) is trained to predict the saliency score of each object region based on the global features. The final saliency map is generated by a weighted sum of salient object regions. Our method presents two interesting insights. First, local features learned by a supervised scheme can effectively capture local contrast, texture and shape information for saliency detection. Second, the complex relationship between different global saliency cues can be captured by deep networks and exploited principally rather than heuristically. Quantitative and qualitative experiments on several benchmark data sets demonstrate that our algorithm performs favorably against the state-of-the-art methods.","","","10.1109/CVPR.2015.7298938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298938","","Estimation;Image color analysis;Training;Feature extraction;Neural networks;Search problems;Accuracy","learning (artificial intelligence);neural nets;object detection","deep neural network;supervised learning scheme;salient object regions;saliency score;DNN-G;geometric information;contrast information;local saliency maps;pixel saliency value;local estimation;global search;saliency detection algorithm;DNN-L","","188","42","","","","","IEEE","IEEE Conferences"
"Shadow optimization from structured deep edge detection","Li Shen; Teck Wee Chua; K. Leman","Institute for Infocomm Research, Singapore; Institute for Infocomm Research, Singapore; Institute for Infocomm Research, Singapore","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","2067","2074","Local structures of shadow boundaries as well as complex interactions of image regions remain largely unexploited by previous shadow detection approaches. In this paper, we present a novel learning-based framework for shadow region recovery from a single image. We exploit local structures of shadow edges by using a structured CNN learning framework. We show that using structured label information in classification can improve local consistency over pixel labels and avoid spurious labelling. We further propose and formulate shadow/bright measure to model complex interactions among image regions. The shadow and bright measures of each patch are computed from the shadow edges detected by the proposed CNN. Using the global interaction constraints on patches, we formulate a least-square optimization problem for shadow recovery that can be solved efficiently. Our shadow recovery method achieves state-of-the-art results on major shadow benchmark databases collected under various conditions.","","","10.1109/CVPR.2015.7298818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298818","","Image edge detection;Optimization;Labeling;Training;Yttrium;Computational modeling;Image color analysis","edge detection;learning (artificial intelligence);least squares approximations;neural nets;optimisation","shadow optimization;structured deep edge detection;shadow boundaries;image regions interaction;shadow detection approach;learning-based framework;shadow region recovery;structured CNN learning framework;convolutional neural networks;structured label information;shadow measure;bright measure;least-square optimization problem","","2","26","","","","","IEEE","IEEE Conferences"
"Deep hierarchical parsing for semantic segmentation","A. Sharma; O. Tuzel; D. W. Jacobs","Computer Science Department, University of Maryland, USA; MERL, Cambridge, USA; Computer Science Department, University of Maryland, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","530","538","This paper proposes a learning-based approach to scene parsing inspired by the deep Recursive Context Propagation Network (RCPN). RCPN is a deep feed-forward neural network that utilizes the contextual information from the entire image, through bottom-up followed by top-down context propagation via random binary parse trees. This improves the feature representation of every super-pixel in the image for better classification into semantic categories. We analyze RCPN and propose two novel contributions to further improve the model. We first analyze the learning of RCPN parameters and discover the presence of bypass error paths in the computation graph of RCPN that can hinder contextual propagation. We propose to tackle this problem by including the classification loss of the internal nodes of the random parse trees in the original RCPN loss function. Secondly, we use an MRF on the parse tree nodes to model the hierarchical dependency present in the output. Both modifications provide performance boosts over the original RCPN and the new system achieves state-of-the-art performance on Stanford Background, SIFT-Flow and Daimler urban datasets.","","","10.1109/CVPR.2015.7298651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298651","","Semantics;Training;Visualization;Neural networks;Image segmentation;Context;Accuracy","feedforward neural nets;image representation;image segmentation;trees (mathematics)","Daimler urban dataset;SIFT-Flow dataset;Stanford Background dataset;hierarchical dependency;parse tree nodes;RCPN loss function;random parse trees;classification loss;contextual propagation;computation graph;bypass error paths;semantic categories;feature representation;random binary parse trees;top-down context propagation;bottom-up context propagation;deep feed-forward neural network;deep recursive context propagation network;scene parsing;learning-based approach;semantic segmentation;deep hierarchical parsing","","44","25","","","","","IEEE","IEEE Conferences"
"Regionlets for Generic Object Detection","X. Wang; M. Yang; S. Zhu; Y. Lin","Department of Media Analytics, NEC Laboratories America, Cupertino, CA, 95014; AI Research, Facebook Inc., Menlo Park, CA, 94025; Alibaba Group; Department of Media Analytics, NEC Laboratories America, Cupertino, CA, 95014","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2015","37","10","2071","2084","Generic object detection is confronted by dealing with different degrees of variations, caused by viewpoints or deformations in distinct object classes, with tractable computations. This demands for descriptive and flexible object representations which can be efficiently evaluated in many locations. We propose to model an object class with a cascaded boosting classifier which integrates various types of features from competing local regions, each of which may consist of a group of subregions, named as regionlets. A regionlet is a base feature extraction region defined proportionally to a detection window at an arbitrary resolution (i.e., size and aspect ratio). These regionlets are organized in small groups with stable relative positions to be descriptive to delineate fine-grained spatial layouts inside objects. Their features are aggregated into a one-dimensional feature within one group so as to be flexible to tolerate deformations. The most discriminative regionlets for each object class are selected through a boosting learning procedure. Our regionlet approach achieves very competitive performance on popular multi-class detection benchmark datasets with a single method, without any context. It achieves a detection mean average precision of 41.7 percent on the PASCAL VOC 2007 dataset, and 39.7 percent on the VOC 2010 for 20 object categories. We further develop support pixel integral images to efficiently augment regionlet features with the responses learned by deep convolutional neural networks. Our regionlet based method won second place in the ImageNet Large Scale Visual Object Recognition Challenge (ILSVRC 2013).","","","10.1109/TPAMI.2015.2389830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005538","Object Detection;Regionlet;Boosting;Object Proposals;Selective Search;Deep Convolutional Neural Network;Object detection;regionlet;boosting;object proposals;selective search;deep convolutional neural network","Feature extraction;Boosting;Object detection;Detectors;Search problems;Proposals;Deformable models","deformation;feature extraction;image classification;image resolution;learning (artificial intelligence);neural nets;object detection","ILSVRC;ImageNet Large Scale Visual Object Recognition Challenge;deep convolutional neural networks;pixel integral image;PASCAL VOC 2007 dataset;regionlet approach;boosting learning procedure;spatial layout;arbitrary resolution;base feature extraction region;cascaded boosting classifier;tractable computation;distinct object class deformation;generic object detection","","46","46","","","","","IEEE","IEEE Journals"
"Parallel convolutional-linear neural network for motor imagery classification","S. Sakhavi; C. Guan; S. Yan","A∗STAR Institute for Infocomm Research (IR) Brain-Computer Interface Lab Singapore; A∗STAR Institute for Infocomm Research (IR) Brain-Computer Interface Lab Singapore; National University of Singapore Department of Electrical and Computer Engineering, Learning and Vision Lab Singapore","2015 23rd European Signal Processing Conference (EUSIPCO)","","2015","","","2736","2740","Deep learning, recently, has been successfully applied to image classification, object recognition and speech recognition. However, the benefits of deep learning and accompanying architectures have been largely unknown for BCI applications. In motor imagery-based BCI, an energy-based feature, typically after spatial filtering, is commonly used for classification. Although this feature corresponds to the estimate of event-related synchronization/desynchronization in the brain, it neglects energy dynamics which may contain valuable discriminative information. Because traditional classiication methods, such as SVM, cannot handle this dynamical property, we proposed an architecture that inputs a dynamic energy representation of EEG data and utilizes convolutional neural networks for classification. By combining this network with a static energy network, we saw a significant increase in performance. We evaluated the proposed method and compared with SVM on a multi-class motor imagery dataset (BCI competition dataset IV-2a). Our method outperforms SVM with static energy features significantly (p <; 0.01).","","","10.1109/EUSIPCO.2015.7362882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362882","Convolutional Neural Network;Deep Learning;Motor Imagery;Brain-Computer Interface;EEG","Computer architecture;Electroencephalography;Convolution;Support vector machines;Feature extraction;Europe","brain-computer interfaces;electroencephalography;learning (artificial intelligence);medical signal processing;neural nets;signal classification;synchronisation","parallel convolutional-linear neural network;motor imagery classification;motor imagery-based BCI;energy-based feature;event-related synchronization-desynchronization;energy dynamics;support vector machines;dynamic energy representation;EEG data;static energy network;multiclass motor imagery dataset;BCI competition dataset IV-2a","","17","20","","","","","IEEE","IEEE Conferences"
"Big/little deep neural network for ultra low power inference","E. Park; D. Kim; S. Kim; Y. Kim; G. Kim; S. Yoon; S. Yoo","Computing Memory Architecture Lab, Seoul National University; Computing Memory Architecture Lab, Seoul National University; Computing Memory Architecture Lab, Seoul National University; Software R&D Center, Device Solutions, Samsung Electronics; Vision and Learning Lab, Seoul National University; Advanced Computing Lab, Seoul National University; Computing Memory Architecture Lab, Seoul National University","2015 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","","2015","","","124","132","Deep neural networks (DNNs) have recently proved their effectiveness in complex data analyses such as object/speech recognition. As their applications are being expanded to mobile devices, their energy efficiencies are becoming critical. In this paper, we propose a novel concept called big/LITTLE DNN (BL-DNN) which significantly reduces energy consumption required for DNN execution at a negligible loss of inference accuracy. The BL-DNN consists of a little DNN (consuming low energy) and a full-fledged big DNN. In order to reduce energy consumption, the BL-DNN aims at avoiding the execution of the big DNN whenever possible. The key idea for this goal is to execute the little DNN first for inference (without big DNN execution) and simply use its result as the final inference result as long as the result is estimated to be accurate. On the other hand, if the result from the little DNN is not considered to be accurate, the big DNN is executed to give the final inference result. This approach reduces the total energy consumption by obtaining the inference result only with the little, energy-efficient DNN in most cases, while maintaining the similar level of inference accuracy through selectively utilizing the big DNN execution. We present design-time and runtime methods to control the execution of big DNN under a trade-off between energy consumption and inference accuracy. Experiments with state-of-the-art DNNs for ImageNet and MNIST show that our proposed BL-DNN can offer up to 53.7% (ImageNet) and 94.1% (MNIST) reductions in energy consumption at a loss of 0.90% (ImageNet) and 0.12% (MNIST) in inference accuracy, respectively.","","","10.1109/CODESISSS.2015.7331375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7331375","Deep neural network;low power","Energy consumption;Accuracy;Biological neural networks;Hardware;Energy efficiency;Neurons;Memory management","data analysis;energy consumption;inference mechanisms;neural nets","big/little deep neural network;ultra low power inference;complex data analyses;BL-DNN;energy consumption;ImageNet;MNIST","","13","37","","","","","IEEE","IEEE Conferences"
"Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation","P. Huang; M. Kim; M. Hasegawa-Johnson; P. Smaragdis","Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Computer Science and Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2015","23","12","2136","2147","Monaural source separation is important for many real world applications. It is challenging because, with only a single channel of information available, without any constraints, an infinite number of solutions are possible. In this paper, we explore joint optimization of masking functions and deep recurrent neural networks for monaural source separation tasks, including speech separation, singing voice separation, and speech denoising. The joint optimization of the deep recurrent neural networks with an extra masking layer enforces a reconstruction constraint. Moreover, we explore a discriminative criterion for training neural networks to further enhance the separation performance. We evaluate the proposed system on the TSP, MIR-1K, and TIMIT datasets for speech separation, singing voice separation, and speech denoising tasks, respectively. Our approaches achieve 2.30-4.98 dB SDR gain compared to NMF models in the speech separation task, 2.30-2.48 dB GNSDR gain and 4.32-5.42 dB GSIR gain compared to existing models in the singing voice separation task, and outperform NMF and DNN baselines in the speech denoising task.","","","10.1109/TASLP.2015.2468583","Army Research Office; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194774","Deep recurrent neural network (DRNN);discriminative training;monaural source separation;time–frequency masking","Neural networks;Recurrent neural networks;Speech processing;Blind source separation","learning (artificial intelligence);optimisation;recurrent neural nets;signal denoising;source separation;speech enhancement","joint mask optimization;joint deep recurrent neural network optimization;monaural source separation tasks;speech separation;singing voice separation;speech denoising;reconstruction constraint;discriminative criterion;neural network training;separation performance enhancement;SDR gain;GNSDR gain","","143","40","","","","","IEEE","IEEE Journals"
"Shared representation learning for heterogenous face recognition","D. Yi; Z. Lei; S. Z. Li","Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CASIA); Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CASIA); Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CASIA)","2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)","","2015","1","","1","7","After intensive research, heterogenous face recognition is still a challenging problem. The main difficulties are owing to the complex relationship between heterogenous face image spaces. The heterogeneity is always tightly coupled with other variations, which makes the relationship of heterogenous face images highly nonlinear. Many excellent methods have been proposed to model the nonlinear relationship, but they apt to overfit to the training set, due to limited samples. Inspired by the unsupervised algorithms in deep learning, this paper proposes a novel framework for heterogeneous face recognition. We first extract Gabor features at some localized facial points, and then use Restricted Boltzmann Machines (RBMs) to learn a shared representation locally to remove the heterogeneity around each facial point. Finally, the shared representations of local RBMs are connected together and processed by PCA. Near infrared (NIR) to visible (VIS) face recognition problem and two databases are selected to evaluate the performance of the proposed method. On CASIA HFB database, we obtain comparable results to state-of-the-art methods. On a more difficult database, CASIA NIR-VIS 2.0, we outperform other methods significantly.","","","10.1109/FG.2015.7163093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163093","","Face;Face recognition;Training;Feature extraction;Principal component analysis;Databases;Standards","Boltzmann machines;face recognition;infrared imaging;learning (artificial intelligence);principal component analysis","shared representation learning;heterogenous face recognition;heterogenous face image spaces;nonlinear relationship;training set;unsupervised algorithms;deep learning;restricted Boltzmann machines;RBM;PCA;near infrared;CASIA HFB database;CASIA NIR-VIS 2.0","","","38","","","","","IEEE","IEEE Conferences"
"MatchNet: Unifying feature and metric learning for patch-based matching","Xufeng Han; T. Leung; Y. Jia; R. Sukthankar; A. C. Berg","University of North Carolina at Chapel Hill, USA; Google Research, USA; Google Research, USA; Google Research, USA; University of North Carolina at Chapel Hill, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3279","3286","Motivated by recent successes on learning feature representations and on learning feature comparison functions, we propose a unified approach to combining both for training a patch matching system. Our system, dubbed Match-Net, consists of a deep convolutional network that extracts features from patches and a network of three fully connected layers that computes a similarity between the extracted features. To ensure experimental repeatability, we train MatchNet on standard datasets and employ an input sampler to augment the training set with synthetic exemplar pairs that reduce overfitting. Once trained, we achieve better computational efficiency during matching by disassembling MatchNet and separately applying the feature computation and similarity networks in two sequential stages. We perform a comprehensive set of experiments on standard datasets to carefully study the contributions of each aspect of MatchNet, with direct comparisons to established methods. Our results confirm that our unified approach improves accuracy over previous state-of-the-art results on patch matching datasets, while reducing the storage requirement for descriptors. We make pre-trained MatchNet publicly available.","","","10.1109/CVPR.2015.7298948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298948","","Measurement;Training;Poles and towers;Feature extraction;Reservoirs;Standards;Robustness","feature extraction;image matching;image representation;learning (artificial intelligence)","MatchNet;feature learning;metric learning;patch-based matching;feature representations;patch matching system;deep convolutional network;feature extraction","","14","37","","","","","IEEE","IEEE Conferences"
"Fusion of learned multi-modal representations and dense trajectories for emotional analysis in videos","E. Acar; F. Hopfgartner; S. Albayrak","DAI Laboratory, Technische Universität Berlin, Berlin, Germany; Humanities Advanced Technology and Information Institute, University of Glasgow, Glasgow, UK; DAI Laboratory, Technische Universität Berlin, Berlin, Germany","2015 13th International Workshop on Content-Based Multimedia Indexing (CBMI)","","2015","","","1","6","When designing a video affective content analysis algorithm, one of the most important steps is the selection of discriminative features for the effective representation of video segments. The majority of existing affective content analysis methods either use low-level audio-visual features or generate handcrafted higher level representations based on these low-level features. We propose in this work to use deep learning methods, in particular convolutional neural networks (CNNs), in order to automatically learn and extract mid-level representations from raw data. To this end, we exploit the audio and visual modality of videos by employing Mel-Frequency Cepstral Coefficients (MFCC) and color values in the HSV color space. We also incorporate dense trajectory based motion features in order to further enhance the performance of the analysis. By means of multi-class support vector machines (SVMs) and fusion mechanisms, music video clips are classified into one of four affective categories representing the four quadrants of the Valence-Arousal (VA) space. Results obtained on a subset of the DEAP dataset show (1) that higher level representations perform better than low-level features, and (2) that incorporating motion information leads to a notable performance gain, independently from the chosen representation.","","","10.1109/CBMI.2015.7153603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153603","","Videos;Visualization;Feature extraction;Trajectory;Support vector machines;Mel frequency cepstral coefficient;Color","audio-visual systems;cepstral analysis;feature selection;learning (artificial intelligence);neural nets;support vector machines;video signal processing","learned multimodal representations;emotional analysis;video affective content analysis algorithm;discriminative feature selection;video segment representation;low-level audio-visual features;handcrafted higher level representations;deep learning methods;convolutional neural networks;CNNs;midlevel representations;mel-frequency cepstral coefficients;MFCC;HSV color space;dense trajectory based motion features;multiclass support vector machines;multiclass SVMs;fusion mechanisms;music video clips;valence-arousal space;VA space;DEAP dataset","","3","23","","","","","IEEE","IEEE Conferences"
"A-Wristocracy: Deep learning on wrist-worn sensing for recognition of user complex activities","P. Vepakomma; D. De; S. K. Das; S. Bhansali","Department of Electrical and Computer Engineering, Florida International University; Department of Computer Science, Missouri University of Science & Technology; Department of Computer Science, Missouri University of Science & Technology; Department of Electrical and Computer Engineering, Florida International University","2015 IEEE 12th International Conference on Wearable and Implantable Body Sensor Networks (BSN)","","2015","","","1","6","In this work we present A-Wristocracy, a novel framework for recognizing very fine-grained and complex inhome activities of human users (particularly elderly people) with wrist-worn device sensing. Our designed A-Wristocracy system improves upon the state-of-the-art works on in-home activity recognition using wearables. These works are mostly able to detect coarse-grained ADLs (Activities of Daily Living) but not large number of fine-grained and complex IADLs (Instrumental Activities of Daily Living). These are also not able to distinguish similar activities but with different context (such as sit on floor vs. sit on bed vs. sit on sofa). Our solution helps accurate detection of in-home ADLs/ IADLs and contextual activities, which are all critically important for remote elderly care in tracking their physical and cognitive capabilities. A-Wristocracy makes it feasible to classify large number of fine-grained and complex activities, through Deep Learning based data analytics and exploiting multi-modal sensing on wrist-worn device. It exploits minimal functionality from very light additional infrastructure (through only few Bluetooth beacons), for coarse level location context. A-Wristocracy preserves direct user privacy by excluding camera/ video imaging on wearable or infrastructure. The classification procedure consists of practical feature set extraction from multi-modal wearable sensor suites, followed by Deep Learning based supervised fine-level classification algorithm. We have collected exhaustive home-based ADLs and IADLs data from multiple users. Our designed classifier is validated to be able to recognize very fine-grained complex 22 daily activities (much larger number than 6-12 activities detected by state-of-the-art works using wearable and no camera/ video) with high average test accuracies of 90% or more for two users in two different home environments.","","","10.1109/BSN.2015.7299406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299406","","Sensors;Accuracy;Context;Accelerometers;Machine learning;Bluetooth;Biomedical monitoring","biomechanics;body sensor networks;geriatrics;patient care;telemedicine","user complex activity recognition;inhome user activity;wrist-worn device sensing;A-Wristocracy system;inhome activity recognition;inhome ADL- IADL detection;remote elderly care;physical capability;cognitive capability;data analytics;Bluetooth;user privacy;camera- video imaging;multimodal wearable sensor suite;supervised fine-level classification algorithm;home-based IADL data;home environment","","12","17","","","","","IEEE","IEEE Conferences"
"Fast DNN training based on auxiliary function technique","D. T. Tran; N. Ono; E. Vincent","Inria, Villers-lès-Nancy, F-54600, France; National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, 101-8430, Japan; Inria, Villers-lès-Nancy, F-54600, France","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","2160","2164","Deep neural networks (DNN) are typically optimized with stochastic gradient descent (SGD) using a fixed learning rate or an adaptive learning rate approach (ADAGRAD). In this paper, we introduce a new learning rule for neural networks that is based on an auxiliary function technique without parameter tuning. Instead of minimizing the objective function, a quadratic auxiliary function is recursively introduced layer by layer which has a closed-form optimum. We prove the monotonic decrease of the new learning rule. Our experiments show that the proposed algorithm converges faster and to a better local minimum than SGD. In addition, we propose a combination of the proposed learning rule and ADAGRAD which further accelerates convergence. Experimental evaluation on the MNIST database shows the benefit of the proposed approach in terms of digit recognition accuracy.","","","10.1109/ICASSP.2015.7178353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178353","DNN;back-propagation;auxiliary function technique;gradient descent;adaptive learning rate","Approximation methods;Training;Optimization;Artificial neural networks;Switches;Robustness;Approximation algorithms","gradient methods;image sampling;learning (artificial intelligence);neural nets;stochastic processes","fast DNN training;deep neural network;stochastic gradient descent;SGD;fixed learning rate;adaptive learning rate approach;quadratic auxiliary function technique;learning rule monotonic decrease;ADAGRAD;convergence method;MNIST database;digit recognition accuracy;image sampling","","","30","","","","","IEEE","IEEE Conferences"
"Deep learning helicopter dynamics models","A. Punjani; P. Abbeel","Department of Electrical Engineering and Computer Science, University of California at Berkeley, USA; Department of Electrical Engineering and Computer Science, University of California at Berkeley, USA","2015 IEEE International Conference on Robotics and Automation (ICRA)","","2015","","","3223","3230","We consider the problem of system identification of helicopter dynamics. Helicopters are complex systems, coupling rigid body dynamics with aerodynamics, engine dynamics, vibration, and other phenomena. Resultantly, they pose a challenging system identification problem, especially when considering non-stationary flight regimes. We pose the dynamics modeling problem as direct high-dimensional regression, and take inspiration from recent results in Deep Learning to represent the helicopter dynamics with a Rectified Linear Unit (ReLU) Network Model, a hierarchical neural network model. We provide a simple method for initializing the parameters of the model, and optimization details for training. We describe three baseline models and show that they are significantly outperformed by the ReLU Network Model in experiments on real data, indicating the power of the model to capture useful structure in system dynamics across a rich array of aerobatic maneuvers. Specifically, the ReLU Network Model improves 58% overall in RMS acceleration prediction over state-of-the-art methods. Predicting acceleration along the helicopter's up-down axis is empirically found to be the most difficult, and the ReLU Network Model improves by 60% over the prior state-of-the-art. We discuss explanations of these performance gains, and also investigate the impact of hyperparameters in the novel model.","","","10.1109/ICRA.2015.7139643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139643","","Helicopters;Mathematical model;Trajectory;Aerodynamics;Acceleration;Data models;Training","aircraft control;helicopters;mechanical engineering computing;neural nets;regression analysis;vehicle dynamics","deep learning helicopter dynamics models;system identification;direct high-dimensional regression;rectified linear unit network model;ReLU network model;hierarchical neural network model;aerobatic maneuvers","","43","31","","","","","IEEE","IEEE Conferences"
"Greek folk music classification into two genres using lyrics and audio via canonical correlation analysis","N. Bassiou; C. Kotropoulos; A. Papazoglou-Chalikias","Department of Informatics, Aristotle University of Thessaloniki, 54124, GREECE; Department of Informatics, Aristotle University of Thessaloniki, 54124, GREECE; Department of Informatics, Aristotle University of Thessaloniki, 54124, GREECE","2015 9th International Symposium on Image and Signal Processing and Analysis (ISPA)","","2015","","","238","243","We are interested in Greek folk music genre classification by resorting to canonical correlation analysis (CCA). Here, the genre is related to the place of origin of the song. The CCA learns a linear transformation of the song lyrics descriptors that is highly correlated with their genre labels as well as another linear transformation of the audio features extracted from music recordings, which is maximally correlated with their genre labels. In the latter task, thanks to the deep CCA (DCCA), deep nonlinear transformations of the audio features are learnt, which are maximally correlated with the genre labels. Experimental findings are disclosed for a two-class genre recognition problem, employing folk songs originated from Pontus and Asia Minor. It is demonstrated that the CCA achieves an average accuracy of 97.02% across the 5 folds, when the term frequency-inverse document frequency features model the song lyrics. By modeling the music signal of each song with 28 mel-frequency cepstral coefficients (MFCCs) extracted from each frame and averaged over all frames, the average accuracy of the CCA drops to 72.9% across the 5 folds. The DCCA yields an accuracy of 69% for audio-based genre recognition.","","","10.1109/ISPA.2015.7306065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306065","Canonical Correlation Analysis;Least-Squares Regression;Deep Canonical Correlation Analysis;Greek Folk Music Classification","Correlation;Yttrium;Accuracy;Asia;Signal processing;Multiple signal classification;Feature extraction","audio signal processing;cepstral analysis;feature extraction;learning (artificial intelligence);music;signal classification","canonical correlation analysis;Greek folk music genre classification;linear transformation;song lyrics descriptors;genre labels;music recordings;deep-CCA;deep-nonlinear transformations;audio feature learning;two-class genre recognition problem;folk songs;Pontus;Asia Minor;term frequency-inverse document frequency feature model;music signal modeling;mel-frequency cepstral coefficients;MFCC extraction;DCCA;audio-based genre recognition","","6","29","","","","","IEEE","IEEE Conferences"
"Instantaneous real-time head pose at a distance","S. S. Mukherjee; R. H. Baxter; N. M. Robertson","Visionlab, Heriot-Watt University, Edinburgh, UK; Visionlab, Heriot-Watt University, Edinburgh, UK; Visionlab, Heriot-Watt University, Edinburgh, UK","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","3471","3475","In this paper we focus on robust, real-time human head pose estimation in low resolution RGB data without any smoothing motion priors e.g. direction of motion. Our main contributions lie in three major areas. First, we show that a generative Deep Belief Network model can be learned on human head data from multiple types of data sources. These sources have similar underlying data that are not necessarily labelled or have the same kind of ground truth. Second, we perform discriminative training using multiple disparate supervisory labels to fine tune the model for head pose estimation. Third, we present state-of-the-art results on two publicly available datasets using this new approach. Our implementation computes head pose for a head image in 0.8 milliseconds, making it real-time and highly scalable.","","","10.1109/ICIP.2015.7351449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351449","Head Pose;Gaze;Surveillance;Deep Belief Network;Deep Learning;Unsupervised Learning","Head;Magnetic heads;Training;Surveillance;Image reconstruction;Cities and towns;Hair","belief networks;image colour analysis;image resolution;pose estimation","instantaneous real-time head pose;real-time human head pose estimation;low resolution RGB data;generative deep belief network model;discriminative training;disparate supervisory labels","","","22","","","","","IEEE","IEEE Conferences"
"Multiscale collaborative speech denoising based on deep stacking network","Wei Jiang; Hao Zheng; Shuai Nie; W. Liu","NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","5","A growing number of noise reduction algorithms based on supervised learning have begun to emerge in recent years and show great promise. In this study, we focus on the problem of speech denoising at very low signal-to-noise ratio (SNR) conditions using artificial neural networks. The overall objective is to increase speech intelligibility in the presence of noise. Inspired by multitask learning (MTL), a novel framework based on deep stacking network (DSN) is proposed to do speech denoising at three different time-frequency scales simultaneously and collaboratively. Experiment results show that our algorithm outperforms a state-of-the-art method that is based on traditional deep neural network (DNN).","","","10.1109/IJCNN.2015.7280604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280604","","Noise reduction;Speech","learning (artificial intelligence);neural nets;signal denoising;speech enhancement;speech intelligibility","multiscale collaborative speech denoising;deep stacking network;noise reduction algorithms;signal-to-noise ratio;artificial neural networks;speech intelligibility;multitask learning;time-frequency scales;SNR;MTL;DSN","","","24","","","","","IEEE","IEEE Conferences"
"Deep neural network based malware detection using two dimensional binary program features","J. Saxe; K. Berlin","Invincea Labs, LLC; Invincea Labs, LLC","2015 10th International Conference on Malicious and Unwanted Software (MALWARE)","","2015","","","11","20","In this paper we introduce a deep neural network based malware detection system that Invincea has developed, which achieves a usable detection rate at an extremely low false positive rate and scales to real world training example volumes on commodity hardware. We show that our system achieves a 95% detection rate at 0.1% false positive rate (FPR), based on more than 400,000 software binaries sourced directly from our customers and internal malware databases. In addition, we describe a non-parametric method for adjusting the classifier's scores to better represent expected precision in the deployment environment. Our results demonstrate that it is now feasible to quickly train and deploy a low resource, highly accurate machine learning classification model, with false positive rates that approach traditional labor intensive expert rule based malware detection, while also detecting previously unseen malware missed by these traditional approaches. Since machine learning models tend to improve with larger datasizes, we foresee deep neural network classification models gaining in importance as part of a layered network defense strategy in coming years.","","","10.1109/MALWARE.2015.7413680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7413680","","Feature extraction;Malware;Neural networks;Histograms;Computational modeling;Training;Data models","invasive software;learning (artificial intelligence);pattern classification","two dimensional binary program features;deep neural network based malware detection system;Invincea;usable detection rate;commodity hardware;false positive rate;FPR;nonparametric method;deployment environment;machine learning classification model;false positive rates;labor intensive expert rule based malware detection;layered network defense strategy","","104","36","","","","","IEEE","IEEE Conferences"
"Improving EEG feature learning via synchronized facial video","X. Li; X. Jia; G. Xun; A. Zhang","Dept. of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, U.S.A.; Dept. of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, U.S.A.; Dept. of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, U.S.A.; Dept. of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, U.S.A.","2015 IEEE International Conference on Big Data (Big Data)","","2015","","","843","848","Morden physiological analysis begins to involve more and more types of information. Electroencephalogram (EEG) signals as a typical example is starting to be analyzed with facial expressions videos to detect emotions. Emotions play an important role in the daily life of human beings, the need and importance of automatic emotion recognition has grown with increasing role of human computer interface applications. In this paper, we concentrate on recognition of the emotions jointly from ""inner"" and ""outer"" reactions, which are electroencephalogram (EEG) signals and facial expression video. Due to the streaming nature of this problem, the data volume and velocity is very challenging. We address these challenges from the theoretic perspective and propose a real time algorithm based on EEG signals and synchronized facial video to learn feature vector jointly. Our algorithm consists of an unsupervisedly EEG dictionary component based on deep learning theorem, and a probability pooling component transforms a continuous sequential signal into an EEG ""sentence"" which consists of a sequence of EEG words. The EEG sentence is then jointly learned with video features into a new fixed length feature representation for emotion classification. We overcome several computational challenges on the data based on the idea of convolution and pooling, and we conduct extensive evaluation for each component of our model. We also demonstrate the state-of-the-art classification result on real-world dataset. The superior performances on the emotion recognition task indicates that 1) the natural language scenario can be applied in EEG sequences and 2) borrowing video modality can increase the overall performance.","","","10.1109/BigData.2015.7363831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363831","EEG;Data stream;Feature Learning;Classification","Electroencephalography;Dictionaries;Streaming media;Physiology;Synchronization;Emotion recognition;Transforms","electroencephalography;emotion recognition;face recognition;medical image processing;probability;video signal processing","EEG feature learning;synchronized facial video;physiological analysis;electroencephalogram signal;facial expression video;emotion detection;automatic emotion recognition;human computer interface;feature vector;EEG dictionary component;deep learning theorem;probability pooling component;emotion classification;natural language;video modality","","4","10","","","","","IEEE","IEEE Conferences"
"Botnet Domain Name Detection based on machine learning","J. Jin; Z. Yan; G. Geng; B. Yan","China Internet Network Information Center, Beijing 100190, China; China Internet Network Information Center, Beijing 100190, China; China Internet Network Information Center, Beijing 100190, China; University of Chinese Academy of Science, Beijing 100190, China","6th International Conference on Wireless, Mobile and Multi-Media (ICWMMN 2015)","","2015","","","273","276","Domain Name System (DNS) is a fundamental component of today's Internet: it provides mappings between domain names used by people and the corresponding IP addresses required by network protocols. However, the open and fundamental characteristics of DNS are recently used by the botnet for the communication between bots and C&C. In this paper, we select six kinds of special features of botnet domain querying traffic based on the deep studies of the DNS log. Then three popular classifiers are adopted in order to pick the malicious domains out from the DNS traffic using those features.","","","10.1049/cp.2015.0953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453917","DNS;FFSN;Botnet;Classifier","","Internet;invasive software;IP networks;learning (artificial intelligence);pattern classification;protocols;telecommunication traffic","botnet domain name detection;fundamental component;Internet;IP addresses;network protocols;open fundamental characteristics;DNS;C and C;botnet domain querying traffic;DNS log;classifiers;DNS traffic;machine learning","","1","","","","","","IET","IET Conferences"
"Fuzzy Rule Reduction using Sparse Auto-Encoders","R. K. Sevakula; N. K. Verma","Department of Electrical Engineering, Indian Institute of Technology Kanpur, India; Department of Electrical Engineering, Indian Institute of Technology Kanpur, India","2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)","","2015","","","1","7","Fuzzy Rule based regression, classification and control have found great use in modern applications due to its simplicity, flexibility and capability. A key issue in all such methods is the computation time. Computational complexity of training and testing is linearly dependent on the size of fuzzy rule base and the respective fuzzy rule space is exponentially dependent on data dimensionality. Sparse Auto-Encoders (SAs) have become popular in giving compact feature representations for image, audio and speech data and have helped in giving state of the art pattern recognition performances in most of the domains. These feature representation are learnt in an unsupervised fashion and are found to give higher order building blocks with which the data is seemingly made of. This paper proposes a method where SAs are used for getting compact feature representation of input data and if needed with reduced dimensionality. The regular fuzzy rule based models are then learnt from data in the new feature space. The method was tested for Regression and Classification problems, giving impressive results in both. The method with Regression problem gave comparable performance with almost half the number of rules and with Classification problem it gave improvement in classification accuracy by 2.67% while reducing the size of fuzzy rule base by 11.25 times and 7.5 times by number.","","","10.1109/FUZZ-IEEE.2015.7338118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7338118","sparse autoencoder;fuzzy rule reduction;deep learning;deep belief networks;dimensionality reduction","Training;Artificial neural networks;Cost function;Data models;Computational complexity;Speech;Knowledge based systems","computational complexity;data reduction;data structures;encoding;knowledge based systems;pattern classification;regression analysis;unsupervised learning","fuzzy rule reduction;sparse autoencoder;SA;computational complexity;data dimensionality;feature representation;unsupervised learning;regression problem;classification problem","","","19","","","","","IEEE","IEEE Conferences"
"Predicting Deep Zero-Shot Convolutional Neural Networks Using Textual Descriptions","J. L. Ba; K. Swersky; S. Fidler; R. Salakhutdinov","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","4247","4255","One of the main challenges in Zero-Shot Learning of visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids the problem of having to explicitly define these attributes. We present a new model that can classify unseen categories from their textual description. Specifically, we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neural network (CNN). We take advantage of the architecture of CNNs and learn features at different layers, rather than just learning an embedding space for both modalities, as is common with existing approaches. The proposed model also allows us to automatically generate a list of pseudo-attributes for each visual category consisting of words from Wikipedia articles. We train our models end-to-end using the Caltech-UCSD bird and flower datasets and evaluate both ROC and Precision-Recall curves. Our empirical results show that the proposed model significantly outperforms previous methods.","","","10.1109/ICCV.2015.483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410840","","Encyclopedias;Training;Neural networks;Visualization;Electronic publishing;Internet","feature extraction;image texture;learning (artificial intelligence);neural nets","deep zero-shot convolutional neural networks;textual description;zero-shot learning;visual category;semantic attribute;Wikipedia articles;text features;CNN;Caltech-UCSD bird dataset;flower dataset;ROC curve;receiver operating characteristic curve;precision-recall curve","","56","32","","","","","IEEE","IEEE Conferences"
"Privacy Preserving Back-Propagation Based on BGV on Cloud","F. Bu; Y. Ma; Z. Chen; H. Xu","Sch. of Software Technol., Dalian Univ. of Technol., Dalian, China; Sch. of Software Technol., Dalian Univ. of Technol., Dalian, China; Sch. of Software Technol., Dalian Univ. of Technol., Dalian, China; Sch. of Software Technol., Dalian Univ. of Technol., Dalian, China","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","1791","1795","Back-propagation is the most effective algorithm for training deep learning models that are proved to have a great ability for big data feature learning. However, back-propagation is of high time complexity, leading to a low efficiency of big data learning. Aiming at this problem, the paper proposes a privacy preserving back-propagation algorithm based on the BGV encryption scheme on cloud. The proposed algorithm improved the efficiency of back-propagation learning by offloading the expensive operations on the cloud. Furthermore, the BGV encryption scheme is used to protect the private data during the learning process using the power of the cloud computing. Experiments show that our proposed scheme is secure and efficient.","","","10.1109/HPCC-CSS-ICESS.2015.323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336431","Back-propagation; big data learning; cloud computing; the BGV encryption scheme","Encryption;Big data;Algorithm design and analysis;Cloud computing;Privacy;Training;Computational modeling","backpropagation;Big Data;cloud computing;computational complexity;cryptography;data privacy","privacy preserving back-propagation algorithm;training deep learning models;Big Data feature learning;time complexity;BGV encryption scheme;cloud computing;backpropagation learning efficiency","","5","24","","","","","IEEE","IEEE Conferences"
"Deep convolutional architecture for natural image denoising","X. Wang; Q. Tao; L. Wang; D. Li; M. Zhang","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China","2015 International Conference on Wireless Communications & Signal Processing (WCSP)","","2015","","","1","4","Natural image is an important source of human access to information, however observed image signals are often corrupted in the process of acquisition or transmission. As an important link of image preprocessing, image denoising has significant influence on the follow-up procedures. Unlike traditional methods that use related features of spatial or transform domain in a single image, we propose a deep learning method for natural image denoising. Our method directly learns an end-to-end mapping from a noisy image to a corresponding de-noised image. It's based on a deep convolutional architecture with rectified linear units and local response normalization. The experiment results show that the proposed deep convolutional architecture learns various features from noisy images, and achieves denoising results of high quality within short time for practical usage.","","","10.1109/WCSP.2015.7341021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7341021","image denoising;convolutional neural networks;rectified linear units","Noise reduction;Convolution;Image denoising;Noise measurement;Training;Computer architecture;Standards","convolutional codes;image denoising","deep convolutional architecture;natural image denoising;image preprocessing;deep learning method;rectified linear units;local response normalization","","9","15","","","","","IEEE","IEEE Conferences"
"On Heuristic Randomization and Reuse as an Enabler of Domain Transference","S. H. Rubin; T. bouabana-Tebibel; Y. Hoadjli; K. Habib; B. Y. Belamiri","Space & Naval Warfare Syst. Center Pacific, San Diego, CA, USA; LCSI Lab., Ecole Nat. Super. d'Inf., Algiers, Algeria; LCSI Lab., Ecole Nat. Super. d'Inf., Algiers, Algeria; LCSI Lab., Ecole Nat. Super. d'Inf., Algiers, Algeria; LCSI Lab., Ecole Nat. Super. d'Inf., Algiers, Algeria","2015 IEEE International Conference on Information Reuse and Integration","","2015","","","411","418","The solution of NP-hard problems requires the use of one or more explicit or implicit heuristics as a practical measure. Quantum computers promise to make this practical for O (2n) problems or less, but have yet to deliver a solution to a single NP-hard problem. The question addressed by this paper is whether domain transference and reuse of problem-solving knowledge can be mediated through the reuse of heuristics, and, if so, the extent to which such transference may occur in the solution of NP-hard problems. Neural networks have zero domain transference on account of their inability to represent modus ponens. Similarly, CBR, deep learning, EP, GAs, SVMs, the predicate calculus, learning via conventional expert systems, and all other machine learning technologies are unable to theoretically or practically mediate domain transference because they don't respect randomization as the core underpinning technology. The paper offers a constructive proof of the unbounded density of knowledge in support of the Semantic Randomization Theorem (SRT). It details this result and its potential impact on the machine learning community.","","","10.1109/IRI.2015.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301006","domain transference;heuristics;machine learning;n-puzzle;randomization;reuse;SRT","Games;Neural networks;NP-hard problem;Semantics;Heuristic algorithms;Indexes;Calculus","computational complexity;learning (artificial intelligence)","heuristic randomization;domain transference enabler;single NP-hard problems;implicit heuristics;explicit heuristics;quantum computers;problem-solving knowledge reuse;neural networks;modus ponens;CBR;deep learning;EP;GAs;SVMs;predicate calculus;learning via conventional expert systems;machine learning technology;semantic randomization theorem;SRT","","3","14","","","","","IEEE","IEEE Conferences"
"Deep correlation for matching images and text","F. Yan; K. Mikolajczyk","Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, United Kingdom, GU2 7XH; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, United Kingdom, GU2 7XH","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3441","3450","This paper addresses the problem of matching images and captions in a joint latent space learnt with deep canonical correlation analysis (DCCA). The image and caption data are represented by the outputs of the vision and text based deep neural networks. The high dimensionality of the features presents a great challenge in terms of memory and speed complexity when used in DCCA framework. We address these problems by a GPU implementation and propose methods to deal with overfitting. This makes it possible to evaluate DCCA approach on popular caption-image matching benchmarks. We compare our approach to other recently proposed techniques and present state of the art results on three datasets.","","","10.1109/CVPR.2015.7298966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298966","","Correlation;Yttrium;Graphics processing units;Protocols;Training;Libraries;Visualization","computational complexity;graphics processing units;image matching;learning (artificial intelligence)","deep correlation;text matching;captions matching;joint latent space;deep canonical correlation analysis;DCCA framework;caption data;speed complexity;GPU implementation;caption-image matching benchmarks","","97","53","","","","","IEEE","IEEE Conferences"
"Extracting deep bottleneck features for visual speech recognition","C. Sui; R. Togneri; M. Bennamoun","School of Computer Science and Software Engineering, University of Western Australia, Australia; School of Electrical, Electronic and Computer Engineering, University of Western Australia, Australia; School of Computer Science and Software Engineering, University of Western Australia, Australia","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","1518","1522","Motivated by the recent progresses in the use of deep learning techniques for acoustic speech recognition, we present in this paper a visual deep bottleneck feature (DBNF) learning scheme using a stacked auto-encoder combined with other techniques. Experimental results show that our proposed deep feature learning scheme yields approximately 24% relative improvement for visual speech accuracy. To the best of our knowledge, this is the first study which uses deep bottleneck feature on visual speech recognition. Our work firstly shows that the deep bottleneck visual feature is able to achieve a significant accuracy improvement on visual speech recognition.","","","10.1109/ICASSP.2015.7178224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178224","Visual speech recognition;stacked denoising auto-encoder;deep bottleneck feature","Visualization;Speech recognition;Speech;Feature extraction;Hidden Markov models;Discrete cosine transforms;Accuracy","speech recognition","deep bottleneck features;visual speech recognition;stacked auto-encoder;visual speech accuracy","","4","26","","","","","IEEE","IEEE Conferences"
"Using Deep Belief Network to Capture Temporal Information for Audio Event Classification","F. Guo; D. Yang; X. Chen","Inst. of Comput. Sci. & Technol., Peking Univ., Beijing, China; Inst. of Comput. Sci. & Technol., Peking Univ., Beijing, China; Inst. of Comput. Sci. & Technol., Peking Univ., Beijing, China","2015 International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP)","","2015","","","421","424","Audio event classification plays an important role in surveillance systems. Due to the constrain of short-time Fourier transform (STFT), the extraction of the audio frequency domain features, as the essential work among the audio event classification, still have some difficulty when conducted on a big audio frame. The traditional concatenation method of feature vector for the successive audio windows in one big audio frame is not perfect for the information redundancy in the low level audio representations. However the temporal information is very important in the audio event classification. In this paper, we try to capture the underlying temporal information in the audio event using the Deep Belief Network (DBN). Here the feature is extracted on a long time span. For the clear description, we call this kind of audio block in our method as audio unit rather than audio frame. There are mainly two contributions in this paper. First we segment the audio into units with different sizes and conduct an evaluation about the classification performance of different features on different unit sizes, including the traditional features and the DBN features. Second we present a method to merge audio features learned from multi-scale audio units to train a support vector machine (SVM) classifier. The classifier based on the merged DBN features outperforms other classifiers which are only based on the DBN features before merging or the traditional features respectively.","","","10.1109/IIH-MSP.2015.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415846","audio temporal information;multi-scale representations;audio event classification;Deep Belief Network","Feature extraction;Machine learning;Frequency-domain analysis;Neurons;Multimedia communication;Speech recognition;Mel frequency cepstral coefficient","audio signal processing;belief networks;feature extraction;Fourier transforms;signal classification;signal representation;support vector machines;surveillance","SVM classifier;support vector machine classifier;DBN features;deep belief network;audio representations;big audio frame;audio frequency domain features extraction;STFT;short-time Fourier transform;surveillance systems;audio event classification;capture temporal information","","2","14","","","","","IEEE","IEEE Conferences"
"Loop closure detection for visual SLAM systems using deep neural networks","X. Gao; T. Zhang","Department of Automation, Tsinghua University, Beijing, 100084, China; Department of Automation, Tsinghua University, Beijing, 100084, China","2015 34th Chinese Control Conference (CCC)","","2015","","","5851","5856","The detection of loop closure is of essential importance in visual simultaneous localization and mapping systems. It can reduce the accumulating drift of localization algorithms if the loops are checked correctly. Traditional loop closure detection approaches take advantage of Bag-of-Words model, which clusters the feature descriptors as words and measures the similarity between the observations in the word space. However, the features are usually designed artificially and may not be suitable for data from new-coming sensors. In this paper a novel loop closure detection approach is proposed that learns features from raw data using deep neural networks instead of common visual features. We discuss the details of the method of training neural networks. Experiments on an open dataset are also demonstrated to evaluate the performance of the proposed method. It can be seen that the neural network is feasible to solve this problem.","","","10.1109/ChiCC.2015.7260555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7260555","Simultaneous Localization and Mapping;Loop Closure Detection;Deep Neural Networks;Denoising Autoencoder","Training;Simultaneous localization and mapping;Neural networks;Visualization;Feature extraction;Machine learning;Sparse matrices","neurocontrollers;robot vision;SLAM (robots)","loop closure detection;visual SLAM systems;deep neural networks;visual simultaneous localization and mapping systems;bag-of-words","","14","20","","","","","IEEE","IEEE Conferences"
"Fixed point optimization of deep convolutional neural networks for object recognition","S. Anwar; K. Hwang; W. Sung","Department of Electrical and Computer Engineering, Seoul National University, 151-744 South Korea; Department of Electrical and Computer Engineering, Seoul National University, 151-744 South Korea; Department of Electrical and Computer Engineering, Seoul National University, 151-744 South Korea","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","1131","1135","Deep convolutional neural networks have shown promising results in image and speech recognition applications. The learning capability of the network improves with increasing depth and size of each layer. However this capability comes at the cost of increased computational complexity. Thus reduction in hardware complexity and faster classification are highly desired. This work proposes an optimization method for fixed point deep convolutional neural networks. The parameters of a pre-trained high precision network are first directly quantized using L2 error minimization. We quantize each layer one by one, while other layers keep computation with high precision, to know the layer-wise sensitivity on word-length reduction. Then the network is retrained with quantized weights. Two examples on object recognition, MNIST and CIFAR-10, are presented. Our results indicate that quantization induces sparsity in the network which reduces the effective number of network parameters and improves generalization. This work reduces the required memory storage by a factor of 1/10 and achieves better classification results than the high precision networks.","","","10.1109/ICASSP.2015.7178146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178146","convolutional neural network;quantization;word length optimization;sparsity","Quantization (signal);Convolution;Training;Biological neural networks;Minimization;Sensitivity analysis","neural nets;object recognition;optimisation;speech recognition","fixed point optimization;deep convolutional neural networks;object recognition;speech recognition applications;image recognition applications;computational complexity;hardware complexity;faster classification;optimization method;fixed point deep convolutional neural networks;L2 error minimization;network parameters","","56","15","","","","","IEEE","IEEE Conferences"
"A Distributed Anomaly Detection Method of Operation Energy Consumption Using Smart Meter Data","Y. Yuan; K. Jia","Coll. of Electron. Inf. & Control Eng., Beijing Univ. of Technol. Beijing, Beijing, China; Coll. of Electron. Inf. & Control Eng., Beijing Univ. of Technol. Beijing, Beijing, China","2015 International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP)","","2015","","","310","313","Along with the rapid development of communication network construction, the operation energy consumption grows significantly in recent years, and the expensive electricity cost is hard to be ignored. Therefore, it is necessary to develop an operation energy anomaly detection mechanism to enhance the control ability of electricity cost. According to the practical distribution and data characteristic of smart meters, this paper presents a distributed anomaly detection method of operation energy consumption based on deep learning methods. An IOT-based distributed structure is implemented to execute data interaction. Stacked sparse autoencoder is used to extract the high-level representation from massive monitoring data acquired automatically from actual smart meter network. Then softmax is used for classification to detect anomaly and send alarm messages using web technologies. The experimental results show that the proposed method with good prospect for intelligent applications achieves better accuracy and meanwhile decreases computing delay caused by central arithmetic method.","","","10.1109/IIH-MSP.2015.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415819","operation consumption;anomaly detection;stacked sparse autoencoder;smart meter;deep learning;IOT","Multimedia communication;Signal processing;Smart meters;Machine learning;Organizations;Data preprocessing;Feature extraction","energy consumption;Internet of Things;power engineering computing;smart meters","central arithmetic method;Web technologies;softmax;smart meter network;massive monitoring data;stacked sparse autoencoder;IOT-based distributed structure;smart meter data;operation energy consumption;distributed anomaly detection method","","8","8","","","","","IEEE","IEEE Conferences"
"Multitask learning and system combination for automatic speech recognition","O. Siohan; D. Rybach","Google Inc., New York; Google Inc., New York","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","589","595","In this paper we investigate the performance of an ensemble of convolutional, long short-term memory deep neural networks (CLDNN) on a large vocabulary speech recognition task. To reduce the computational complexity of running multiple recognizers in parallel, we propose instead an early system combination approach which requires the construction of a static decoding network encoding the multiple context-dependent state inventories from the distinct acoustic models. To further reduce the computational load, the hidden units of those models can be shared while keeping the output layers distinct, leading to a multitask training formulation. However in contrast to the traditional multitask training, our formulation uses all predicted outputs leading to a multitask system combination strategy. Results are presented on a Voice Search task designed for children and outperform our current production system.","","","10.1109/ASRU.2015.7404849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404849","system combination;multitask learning;children's speech;ROVER","Hidden Markov models;Acoustics;Training;Transducers;Speech recognition;Decoding;Speech","learning (artificial intelligence);neural nets;speech recognition","multitask learning;automatic speech recognition;long short-term memory deep neural networks;CLDNN;large vocabulary speech recognition task;system combination approach;context-dependent state inventory;multitask training formulation;voice search task","","5","22","","","","","IEEE","IEEE Conferences"
"Deep Learning Strong Parts for Pedestrian Detection","Y. Tian; P. Luo; X. Wang; X. Tang","Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Shenzhen Key Lab. of Comp. Vis. & Pat. Rec., Shenzhen Inst. of Adv. Technol., Shenzhen, China; Dept. of Electron. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1904","1912","Recent advances in pedestrian detection are attained by transferring the learned features of Convolutional Neural Network (ConvNet) to pedestrians. This ConvNet is typically pre-trained with massive general object categories (e.g. ImageNet). Although these features are able to handle variations such as poses, viewpoints, and lightings, they may fail when pedestrian images with complex occlusions are present. Occlusion handling is one of the most important problem in pedestrian detection. Unlike previous deep models that directly learned a single detector for pedestrian detection, we propose DeepParts, which consists of extensive part detectors. DeepParts has several appealing properties. First, DeepParts can be trained on weakly labeled data, i.e. only pedestrian bounding boxes without part annotations are provided. Second, DeepParts is able to handle low IoU positive proposals that shift away from ground truth. Third, each part detector in DeepParts is a strong detector that can detect pedestrian by observing only a part of a proposal. Extensive experiments in Caltech dataset demonstrate the effectiveness of DeepParts, which yields a new state-of-the-art miss rate of 11:89%, outperforming the second best method by 10%.","","","10.1109/ICCV.2015.221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410578","","Detectors;Training;Proposals;Feature extraction;Prototypes;Training data;Semantics","convolution;learning (artificial intelligence);neural nets;object detection;pedestrians","pedestrian detection;convolutional neural network;ConvNet;massive general object categories;ImageNet;pedestrian images;occlusion handling;DeepParts;extensive part detectors;pedestrian bounding boxes;IoU positive proposals","","156","43","","","","","IEEE","IEEE Conferences"
"Deep recurrent regularization neural network for speech recognition","J. Chien; T. Lu","Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan 30010, ROC; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan 30010, ROC","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4560","4564","This paper presents a deep recurrent regularization neural network (DRRNN) for speech recognition. Our idea is to build a regularization neural network acoustic model by conducting the hybrid Tikhonov and weight-decay regularization which compensates the variations due to the input speech as well as the model parameters in the restricted Boltzmann machine as a pre-training stage for feature learning and structural modeling. In addition, a new backpropagation through time (BPTT) algorithm is developed by extending the truncated minibatch training for recurrent neural network where the minibatch BPTT is not only performed in recurrent layer but also in feedforward layer. The DRRNN acoustic model is accordingly established to capture the temporal correlation in a regularization neural network. Experimental results on the tasks of RM and Aurora4 show the effectiveness and robustness of using DRRNN for speech recognition.","","","10.1109/ICASSP.2015.7178834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178834","Recurrent neural network;model regularization;deep learning;acoustic model","Training;Speech;Hidden Markov models;Acoustics;Neurons;Recurrent neural networks","Boltzmann machines;speech recognition","deep recurrent regularization neural network acoustic model;speech recognition;hybrid Tikhonov and weight-decay regularization;restricted Boltzmann machine;backpropagation through time algorithm;BPTT algorithm;truncated minibatch training;DRRNN acoustic model","","4","17","","","","","IEEE","IEEE Conferences"
"NMF-based Target Source Separation Using Deep Neural Network","T. G. Kang; K. Kwon; J. W. Shin; N. S. Kim","Department of Electrical and Computer Engineering and the Institute of New Media and Communications, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering and the Institute of New Media and Communications, Seoul National University, Seoul, Korea; School of Information and Communications, Gwangju Institute of Science and Technology, Gwangju, Korea; Department of Electrical and Computer Engineering and the Institute of New Media and Communications, Seoul National University, Seoul, Korea","IEEE Signal Processing Letters","","2015","22","2","229","233","Non-negative matrix factorization (NMF) is one of the most well-known techniques that are applied to separate a desired source from mixture data. In the NMF framework, a collection of data is factorized into a basis matrix and an encoding matrix. The basis matrix for mixture data is usually constructed by augmenting the basis matrices for independent sources. However, target source separation with the concatenated basis matrix turns out to be problematic if there exists some overlap between the subspaces that the bases for the individual sources span. In this letter, we propose a novel approach to improve encoding vector estimation for target signal extraction. Estimating encoding vectors from the mixture data is viewed as a regression problem and a deep neural network (DNN) is used to learn the mapping between the mixture data and the corresponding encoding vectors. To demonstrate the performance of the proposed algorithm, experiments were conducted in the speech enhancement task. The experimental results show that the proposed algorithm outperforms the conventional encoding vector estimation scheme.","","","10.1109/LSP.2014.2354456","Ministry of Science ICT and Future Planning; National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6892992","Deep neural network;dictionary learning;non-negative matrix factorization;speech enhancement;target source separation","Vectors;Speech;Encoding;Noise;Speech enhancement;Source separation;Signal processing algorithms","encoding;estimation theory;matrix decomposition;mixture models;neural nets;source separation;speech enhancement;vectors","NMF-based target source separation;deep neural network;nonnegative matrix factorization;mixture data;basis matrix;encoding matrix;data factorization;target source separation;concatenated basis matrix;encoding vector estimation;target signal extraction;encoding vectors estimation;DNN;speech enhancement task","","29","26","","","","","IEEE","IEEE Journals"
"A Nonparametric Bayesian Approach toward Stacked Convolutional Independent Component Analysis","S. P. Chatzis; D. Kosmopoulos","Cyprus Univ. of Technol., Limassol, Cyprus; Univ. of Patras, Agrinion, Greece","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2803","2811","Unsupervised feature learning algorithms based on convolutional formulations of independent components analysis (ICA) have been demonstrated to yield state-of-the-art results in several action recognition benchmarks. However, existing approaches do not allow for the number of latent components (features) to be automatically inferred from the data in an unsupervised manner. This is a significant disadvantage of the state-of-the-art, as it results in considerable burden imposed on researchers and practitioners, who must resort to tedious cross-validation procedures to obtain the optimal number of latent features. To resolve these issues, in this paper we introduce a convolutional nonparametric Bayesian sparse ICA architecture for overcomplete feature learning from high-dimensional data. Our method utilizes an Indian buffet process prior to facilitate inference of the appropriate number of latent features under a hybrid variational inference algorithm, scalable to massive datasets. As we show, our model can be naturally used to obtain deep unsupervised hierarchical feature extractors, by greedily stacking successive model layers, similar to existing approaches. In addition, inference for this model is completely heuristics-free, thus, it obviates the need of tedious parameter tuning, which is a major challenge most deep learning approaches are faced with. We evaluate our method on several action recognition benchmarks, and exhibit its advantages over the state-of-the-art.","","","10.1109/ICCV.2015.321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410678","","Feature extraction;Inference algorithms;Training;Data models;Adaptation models;Bayes methods;Machine learning","Bayes methods;convolution;feature extraction;image recognition;independent component analysis","nonparametric Bayesian approach;stacked convolutional independent component analysis;unsupervised feature learning algorithms;convolutional formulations;action recognition benchmarks;cross-validation procedures;convolutional nonparametric Bayesian sparse ICA architecture;overcomplete feature learning;high-dimensional data;Indian buffet process;hybrid variational inference algorithm;massive datasets;deep unsupervised hierarchical feature extractors;successive model layers;heuristics-free;parameter tuning;deep learning","","3","37","","","","","IEEE","IEEE Conferences"
"Multi-objective convolutional learning for face labeling","Sifei Liu; J. Yang; Chang Huang; M. Yang","UC Merced, USA; UC Merced, USA; Baidu Research, USA; UC Merced, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3451","3459","This paper formulates face labeling as a conditional random field with unary and pairwise classifiers. We develop a novel multi-objective learning method that optimizes a single unified deep convolutional network with two distinct non-structured loss functions: one encoding the unary label likelihoods and the other encoding the pairwise label dependencies. Moreover, we regularize the network by using a nonparametric prior as new input channels in addition to the RGB image, and show that significant performance improvements can be achieved with a much smaller network size. Experiments on both the LFW and Helen datasets demonstrate state-of-the-art results of the proposed algorithm, and accurate labeling results on challenging images can be obtained by the proposed algorithm for real-world applications.","","","10.1109/CVPR.2015.7298967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298967","","Labeling;Face;Training;Testing;Hair;Image edge detection;Semantics","face recognition;image classification;image colour analysis;learning (artificial intelligence);neural nets;nonparametric statistics;random processes","multiobjective convolutional learning;face labeling;conditional random field;unary classifier;pairwise classifier;multiobjective learning method;single unified deep convolutional network optimization;distinct nonstructured loss function;unary label likelihood encoding;pairwise label dependency encoding;nonparametric prior;RGB image;LFW dataset;Helen dataset","","5","26","","","","","IEEE","IEEE Conferences"
"Holistically-Nested Edge Detection","S. Xie; Z. Tu","Dept. of CSE, Univ. of California, San Diego, La Jolla, CA, USA; Dept. of CSE, Univ. of California, San Diego, La Jolla, CA, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1395","1403","We develop a new edge detection algorithm that addresses two critical issues in this long-standing vision problem: (1) holistic image training, and (2) multi-scale feature learning. Our proposed method, holistically-nested edge detection (HED), turns pixel-wise edge classification into image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets. HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are crucially important in order to approach the human ability to resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSD500 dataset (ODS F-score of 0.782) and the NYU Depth dataset (ODS F-score of 0.746), and do so with an improved speed (0.4 second per image) that is orders of magnitude faster than recent CNN-based edge detection algorithms.","","","10.1109/ICCV.2015.164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410521","","Image edge detection;Training;Neural networks;Detectors;Feature extraction;Machine learning;Predictive models","edge detection;feature extraction;feedforward neural nets;image classification;image representation;image resolution;learning (artificial intelligence)","holistically-nested edge detection algorithm;holistic image training;multiscale feature learning;pixel-wise edge classification;image-to-image prediction;deep learning model;convolutional neural networks;deeply-supervised nets;hierarchical representations;object boundary detection;BSD500 dataset;NYU Depth dataset","","483","41","","","","","IEEE","IEEE Conferences"
"SCUT-FBP: A Benchmark Dataset for Facial Beauty Perception","D. Xie; L. Liang; L. Jin; J. Xu; M. Li","Sch. of Electron. & Inf. Eng., South China Univ. of Technol., Guangzhou, China; Sch. of Electron. & Inf. Eng., South China Univ. of Technol., Guangzhou, China; Sch. of Electron. & Inf. Eng., South China Univ. of Technol., Guangzhou, China; Sch. of Electron. & Inf. Eng., South China Univ. of Technol., Guangzhou, China; Sch. of Electron. & Inf. Eng., South China Univ. of Technol., Guangzhou, China","2015 IEEE International Conference on Systems, Man, and Cybernetics","","2015","","","1821","1826","In this paper, a novel face dataset with attractiveness ratings, namely the SCUT-FBP dataset, is developed for automatic facial beauty perception. This dataset provides a benchmark to evaluate the performance of different methods for facial attractiveness prediction, including the state-of-the-art deep learning method. The SCUT-FBP dataset contains face portraits of 500 Asian female subjects with attractiveness ratings, all of which have been verified in terms of rating distribution, standard deviation, consistency, and self-consistency. Benchmark evaluations for facial attractiveness prediction were performed with different combinations of facial geometrical features and texture features using classical statistical learning methods and the deep learning method. The best Pearson correlation 0.8187 was achieved by the CNN model. The results of the experiments indicate that the SCUT-FBP dataset provides a reliable benchmark for facial beauty perception.","","","10.1109/SMC.2015.319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379451","Face dataset;facial attractiveness prediction;facial beauty assessment;facial beautification","Face;Standards;Correlation;Feature extraction;Databases;Benchmark testing;Machine learning","human computer interaction;learning (artificial intelligence);statistical analysis","facial beauty perception;SCUT-FBP dataset;automatic facial beauty perception;deep learning method;facial attractiveness prediction;facial geometrical feature;texture feature;statistical learning method;CNN model","","18","34","","","","","IEEE","IEEE Conferences"
"Geodesic Invariant Feature: A Local Descriptor in Depth","Y. Liu; P. Lasang; M. Siegel; Q. Sun","Department of Computer Science and Engineering, Nanjing Institute of Science and Technology, Nanjing, China; Panasonic Research and Development Center Singapore, Singapore; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Computer Science and Engineering, Nanjing Institute of Science and Technology, Nanjing, China","IEEE Transactions on Image Processing","","2015","24","1","236","248","Different from the photometric images, depth images resolve the distance ambiguity of the scene, while the properties, such as weak texture, high noise, and low resolution, may limit the representation ability of the well-developed descriptors, which are elaborately designed for the photometric images. In this paper, a novel depth descriptor, geodesic invariant feature (GIF), is presented for representing the parts of the articulate objects in depth images. GIF is a multilevel feature representation framework, which is proposed based on the nature of depth images. Low-level, geodesic gradient is introduced to obtain the invariance to the articulate motion, such as scale and rotation variation. Midlevel, superpixel clustering is applied to reduce depth image redundancy, resulting in faster processing speed and better robustness to noise. High-level, deep network is used to exploit the nonlinearity of the data, which further improves the classification accuracy. The proposed descriptor is capable of encoding the local structures in the depth data effectively and efficiently. Comparisons with the state-of-the-art methods reveal the superiority of the proposed method.","","","10.1109/TIP.2014.2378019","Program of Introducing Talents of Discipline to Universities; Doctoral Fund through the Ministry of Education, China; Open Project Program through the Jiangsu Key Laboratory of Image and Video Understanding for Social Safety; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975216","Body parts recognition;pose recognition;depth image;deep learning;superpixel;Body parts recognition;pose recognition;depth image;deep learning;superpixel","Feature extraction;Noise;Image resolution;Robustness;Image recognition;Image color analysis;Sensors","differential geometry;feature extraction;gradient methods;image classification;image coding;image denoising;image motion analysis;image representation;image resolution;invariance;pattern clustering;redundancy","scene distance ambiguity resolving;photometric image classification;geodesic invariant feature;GIF depth descriptor;multilevel feature representation framework;low-level geodesic gradient;motion articulation;superpixel clustering;depth image redundancy reduction;high-level deep network;data nonlinearity;image local structure encoding","","7","65","","","","","IEEE","IEEE Journals"
"Temporal alignment for deep neural networks","P. Lin; D. Lyu; Y. Chang; Y. Tsao","Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; ASUS Headquarters, Advanced Technology Division, Kauhsiung, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","2015 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","","2015","","","108","112","Alternative features were derived from extracted temporal envelope bank (TBANK). These simplified temporal representations were investigated in alignment procedures to generate frame-level training labels for deep neural networks (DNNs). TBANK features improved temporal alignments both for supervised training and for context dependent tree building.","","","10.1109/GlobalSIP.2015.7418166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7418166","alignment;temporal features;deep neural networks","Hidden Markov models;Training;Speech;Frequency modulation;Neural networks;Speech recognition;Context","learning (artificial intelligence);neural nets","deep neural networks;temporal envelope bank;TBANK extraction;temporal alignment procedure;supervised training;context dependent tree building","","1","30","","","","","IEEE","IEEE Conferences"
"Unsupervised speaker adaptation of deep neural network based on the combination of speaker codes and singular value decomposition for speech recognition","S. Xue; H. Jiang; L. Dai; Q. Liu","National Engineering Laboratory of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; Department of Electrical Engineering and Computer Science, York University, Toronto, Canada; National Engineering Laboratory of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4555","4559","Recently, we have proposed a general adaptation scheme for deep neural network based on discriminant condition codes and applied it to supervised speaker adaptation in speech recognition based on either frame-level cross-entropy or sequence-level maximum mutual information training criterion [1, 2, 3, 4]. In this case, each condition code is associated with one speaker in data, which is thus called speaker code for convenience. Our previous work has shown that speaker code based methods are quite effective in adapting DNNs even when only a very small amount of adaptation data is available. However, we have to use a large speaker code size and complex processes to obtain the best ASR performance since good initializations of speaker codes and connection weights are very important. In this paper, we propose a method using singular value decomposition (SVD) as in [5] to initialize speaker codes and connection weights to obtain a comparable ASR performance as before but with a smaller speaker code size and much less computation complexity. Meanwhile, we have evaluated unsupervised speaker adaptation with the proposed method in large vocabulary speech recognition in the Switchboard task. Experimental results have shown that it is effective for providing well initializations and suitable in adapting large DNN models.","","","10.1109/ICASSP.2015.7178833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178833","Deep Neural Network (DNN);Speaker Code;Speaker Adaptation;singular value decomposition (SVD)","Neural networks;Hidden Markov models;Adaptation models;Speech recognition;Training;Speech;Matrix decomposition","entropy;neural nets;singular value decomposition;speech recognition;unsupervised learning","deep neural network;discriminant condition codes;frame-level cross-entropy;sequence-level maximum mutual information training criterion;speaker code;DNN;singular value decomposition;SVD;connection weights;unsupervised speaker adaptation;large vocabulary speech recognition","","3","24","","","","","IEEE","IEEE Conferences"
"Investigation of mixture splitting concept for training linear bottlenecks of deep neural network acoustic models","M. A. Tahir; S. Wiesler; R. Schlüter; H. Ney","Human Language Technology and Pattern Recognition, Computer Science Department, RWTH Aachen University, Germany; Human Language Technology and Pattern Recognition, Computer Science Department, RWTH Aachen University, Germany; Human Language Technology and Pattern Recognition, Computer Science Department, RWTH Aachen University, Germany; Human Language Technology and Pattern Recognition, Computer Science Department, RWTH Aachen University, Germany","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4614","4618","A Gaussian or log-linear mixture model trained by maximum likelihood may be trained further using discriminative training. It is desirable that the mixture splitting is also done during the discriminative training, to achieve better mixture density distribution. In previous work such a discriminative splitting approach was presented. Similarly, the resolution of a deep neural network may also be increased by splitting. In this paper, discriminative splitting is applied as a way of initializing a linear bottleneck between two layers of a DNN. Experiments for a single hidden layer and six hidden layer cases show the potential of this approach as an alternative method of pre-training for linear bottlenecks for MLP hidden layers.","","","10.1109/ICASSP.2015.7178845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178845","mixture splitting;deep neural network;linear bottleneck","Hidden Markov models;Training;Neural networks;Acoustics;Speech recognition;Matrix converters;Adaptation models","acoustic signal processing;Gaussian processes;learning (artificial intelligence);maximum likelihood estimation;mixture models;multilayer perceptrons;neural nets","deep neural network acoustic model;mixture splitting concept;log-linear mixture model;Gaussian mixture model;maximum likelihood;discriminative training;mixture density distribution;discriminative splitting approach;DNN;MLP hidden layers;linear bottleneck","","","11","","","","","IEEE","IEEE Conferences"
"Spectral conversion using deep neural networks trained with multi-source speakers","L. Liu; L. Chen; Z. Ling; L. Dai","National Engineering Laboratory of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4849","4853","This paper presents a method for voice conversion using deep neural networks (DNNs) trained with multiple source speakers. The proposed DNNs can be used in two ways for different scenarios: 1) in the absence of training data for source speaker, the DNNs can be treated as source-speaker-independent models and perform conversions directly from arbitrary source speakers to certain target speaker; 2) the DNNs can also be used as initial models for further fine-tuning of source-speaker-dependent DNNs when parallel training data for both source and target speakers are available. Experimental results show that, as source-speaker-independent models, the proposed DNNs can achieve comparable performance to conventional source-speaker-dependent models. On the other hand, the proposed method outperforms the conventional initialization method with restricted Boltzmann machines (RBMs).","","","10.1109/ICASSP.2015.7178892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178892","voice conversion;deep neural networks;source-speaker-independent mapping","Training;Speech;Training data;Data models;Speech processing;Artificial neural networks","learning (artificial intelligence);neural nets;speaker recognition;speech processing","voice conversion;deep neural network;source speaker independent model;arbitrary source speaker;source speaker dependent DNN;parallel training data;spectral conversion","","3","16","","","","","IEEE","IEEE Conferences"
"Semantic Pose Using Deep Networks Trained on Synthetic RGB-D","J. Papon; M. Schoeler","NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","774","782","In this work we address the problem of indoor scene understanding from RGB-D images. Specifically, we propose to find instances of common furniture classes, their spatial extent, and their pose with respect to generalized class models. To accomplish this, we use a deep, wide, multi-output convolutional neural network (CNN) that predicts class, pose, and location of possible objects simultaneously. To overcome the lack of large annotated RGB-D training sets (especially those with pose), we use an on-the-fly rendering pipeline that generates realistic cluttered room scenes in parallel to training. We then perform transfer learning on the relatively small amount of publicly available annotated RGB-D data, and find that our model is able to successfully annotate even highly challenging real scenes. Importantly, our trained network is able to understand noisy and sparse observations of highly cluttered scenes with a remarkable degree of accuracy, inferring class and pose from a very limited set of cues. Additionally, our neural network is only moderately deep and computes class, pose and position in tandem, so the overall run-time is significantly faster than existing methods, estimating all output parameters simultaneously in parallel.","","","10.1109/ICCV.2015.95","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410452","","Solid modeling;Training;Three-dimensional displays;Rendering (computer graphics);Adaptation models;Semantics;Proposals","image colour analysis;learning (artificial intelligence);neural nets;pose estimation;realistic images;rendering (computer graphics)","semantic pose;deep network;cluttered scene;sparse observation;noisy observation;real scene;transfer learning;realistic cluttered room scene;on-the-fly rendering pipeline;RGB-D training set;CNN;multioutput convolutional neural network;generalized class model;furniture class;RGB-D image;indoor scene understanding;synthetic RGB-D","","19","17","","","","","IEEE","IEEE Conferences"
"Multi-task deep visual-semantic embedding for video thumbnail selection","W. Liu; T. Mei; Y. Zhang; C. Che; J. Luo","Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Microsoft Research, Beijing 100080, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Microsoft Research, Beijing 100080, China; University of Rochester, NY 14627, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3707","3715","Given the tremendous growth of online videos, video thumbnail, as the common visualization form of video content, is becoming increasingly important to influence user's browsing and searching experience. However, conventional methods for video thumbnail selection often fail to produce satisfying results as they ignore the side semantic information (e.g., title, description, and query) associated with the video. As a result, the selected thumbnail cannot always represent video semantics and the click-through rate is adversely affected even when the retrieved videos are relevant. In this paper, we have developed a multi-task deep visual-semantic embedding model, which can automatically select query-dependent video thumbnails according to both visual and side information. Different from most existing methods, the proposed approach employs the deep visual-semantic embedding model to directly compute the similarity between the query and video thumbnails by mapping them into a common latent semantic space, where even unseen query-thumbnail pairs can be correctly matched. In particular, we train the embedding model by exploring the large-scale and freely accessible click-through video and image data, as well as employing a multi-task learning strategy to holistically exploit the query-thumbnail relevance from these two highly related datasets. Finally, a thumbnail is selected by fusing both the representative and query relevance scores. The evaluations on 1,000 query-thumbnail dataset labeled by 191 workers in Amazon Mechanical Turk have demonstrated the effectiveness of our proposed method.","","","10.1109/CVPR.2015.7298994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298994","","Semantics;Visualization;Training;Computational modeling;Mathematical model;Data models;Nails","content-based retrieval;data visualisation;learning (artificial intelligence);relevance feedback;video retrieval","video thumbnail selection;online video;video content visualization;user browsing experience;user searching experience;side semantic information;video semantics;click-through rate;video retrieval;multitask deep visual-semantic embedding model;query-dependent video thumbnails;visual information;side information;query-video thumbnail similarity;latent semantic space;query-thumbnail pair matching;click-through video data;multitask learning strategy;query-thumbnail relevance;Amazon Mechanical Turk","","72","24","","","","","IEEE","IEEE Conferences"
"Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing","P. U. Diehl; D. Neil; J. Binas; M. Cook; S. Liu; M. Pfeiffer","Institute of Neuroinformatics, University of Zurich and ETH Zurich, Winterthurerstrasse 190, CH-8057, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Winterthurerstrasse 190, CH-8057, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Winterthurerstrasse 190, CH-8057, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Winterthurerstrasse 190, CH-8057, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Winterthurerstrasse 190, CH-8057, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Winterthurerstrasse 190, CH-8057, Switzerland","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","Deep neural networks such as Convolutional Networks (ConvNets) and Deep Belief Networks (DBNs) represent the state-of-the-art for many machine learning and computer vision classification problems. To overcome the large computational cost of deep networks, spiking deep networks have recently been proposed, given the specialized hardware now available for spiking neural networks (SNNs). However, this has come at the cost of performance losses due to the conversion from analog neural networks (ANNs) without a notion of time, to sparsely firing, event-driven SNNs. Here we analyze the effects of converting deep ANNs into SNNs with respect to the choice of parameters for spiking neurons such as firing rates and thresholds. We present a set of optimization techniques to minimize performance loss in the conversion process for ConvNets and fully connected deep networks. These techniques yield networks that outperform all previous SNNs on the MNIST database to date, and many networks here are close to maximum performance after only 20 ms of simulated time. The techniques include using rectified linear units (ReLUs) with zero bias during training, and using a new weight normalization method to help regulate firing rates. Our method for converting an ANN into an SNN enables low-latency classification with high accuracies already after the first output spike, and compared with previous SNN approaches it yields improved performance without increased training time. The presented analysis and optimization techniques boost the value of spiking deep networks as an attractive framework for neuromorphic computing platforms aimed at fast and efficient pattern recognition.","","","10.1109/IJCNN.2015.7280696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280696","","Neuromorphics;Handheld computers;Neurons;Robustness;Accuracy","neural nets;pattern classification","threshold balancing;weight balancing;spiking neural networks;deep ANN;spiking neurons;optimization techniques;ConvNets;fully connected deep networks;MNIST database;rectified linear units;ReLU;weight normalization method;firing rates;SNN;low-latency classification;spiking deep networks;convolutional neural networks","","145","35","","","","","IEEE","IEEE Conferences"
"Multi-attributes gait identification by convolutional neural networks","C. Yan; B. Zhang; F. Coenen","Department of Computer Science & Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, 215123, China; Department of Computer Science & Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, 215123, China; Department of Computer Science, The University of Liverpool, Liverpool, UK","2015 8th International Congress on Image and Signal Processing (CISP)","","2015","","","642","647","Gait as a biometric feature that can be measured remotely without physical contact and proximal sensing has attract significant attention. This paper proposes to use con-volutional neural networks (ConvNets) and multi-task learning model(MLT) to identify human gait and to predict multiple human attributes simultaneously. In comparison to previous approaches, two novelty in our convolutional approach can be summarised as (i)using ConvNets to learn rich features from the training set is more generic and requires minimal domain knowledge of the problem compared to hand-craft feature, (ii) to identify human gait and to predict other human attributes simultaneously can achieve improved performance for all task than standalone gait identification. Specifically, we first extract Gait Energy Image(GEI) from each walking period as the low level input for the ConvNets. Secondly, we train the ConvNets through back-propagation using a joint loss of each task. Finally, high-level feature is hierarchically extracted in ConvNets, which is shared by each task and used to identify human gait and to predict attribute. The approach was verified on CASIA gait database B, achieving over 95.88% accuracy for each task. To the authors' best knowledge, this is the first time multi-attributes gait identification being proposed.","","","10.1109/CISP.2015.7407957","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407957","Gait recognition;Human gait identification;Multi-task learning;Deep learning;Convolutional neural network;Gait energy image","Convolution;Feature extraction;Testing;Training;Biological neural networks;Convergence","biometrics (access control);convolution;feature extraction;gait analysis;image motion analysis;learning (artificial intelligence);neural nets","CASIA gait database;GEI;gait energy image;handcraft feature;human gait;MLT;multitask learning model;ConvNets;convolutional neural networks;proximal sensing;biometric feature;multiattributes gait identification","","7","27","","","","","IEEE","IEEE Conferences"
"Deep convolutional neural networks as generic feature extractors","L. Hertel; E. Barth; T. Käster; T. Martinetz","Institute for Signal Processing, University of Luebeck, Germany; Institute for Neuro- and Bioinformatics, University of Luebeck, Germany; Institute for Neuro- and Bioinformatics, University of Luebeck, Germany; Institute for Neuro- and Bioinformatics, University of Luebeck, Germany","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","4","Recognizing objects in natural images is an intricate problem involving multiple conflicting objectives. Deep convolutional neural networks, trained on large datasets, achieve convincing results and are currently the state-of-the-art approach for this task. However, the long time needed to train such deep networks is a major drawback. We tackled this problem by reusing a previously trained network. For this purpose, we first trained a deep convolutional network on the ILSVRC-12 dataset. We then maintained the learned convolution kernels and only retrained the classification part on different datasets. Using this approach, we achieved an accuracy of 67.68% on CIFAR-100, compared to the previous state-of-the-art result of 65.43%. Furthermore, our findings indicate that convolutional networks are able to learn generic feature extractors that can be used for different tasks.","","","10.1109/IJCNN.2015.7280683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280683","","Convolution;Kernel;Handwriting recognition;Art","convolution;feature extraction;image classification;neural nets;object recognition","deep convolutional neural network;feature extractor;object recognition;natural image classification","","29","23","","","","","IEEE","IEEE Conferences"
"Improvement in k-Means Clustering Algorithm Using Data Clustering","K. Rajeswari; O. Acharya; M. Sharma; M. Kopnar; K. Karandikar","Pimpri Chinchwad Coll. of Eng., Pune, India; Chinchwad Coll. of Eng., Pune, India; Chinchwad Coll. of Eng., Pune, India; Chinchwad Coll. of Eng., Pune, India; Chinchwad Coll. of Eng., Pune, India","2015 International Conference on Computing Communication Control and Automation","","2015","","","367","369","The set of objects having same characteristics are organized in groups and clusters of these objects reformed known as Data Clustering. It is an unsupervised learning technique for classification of data. K-means algorithm is widely used and famous algorithm for analysis of clusters. In this algorithm, n number of data points are divided into k clusters based on some similarity measurement criterion. K-Means Algorithm has fast speed and thus is used commonly clustering algorithm. Vector quantization, cluster analysis, feature learning are some of the application of K-Means. However results generated using this algorithm are mainly dependant on choosing initial cluster centroids. The main short come of this algorithm is to provide appropriate number of clusters. Provision of number of clusters before applying the algorithm is highly impractical and requires deep knowledge of clustering field. In this project, we are going to propose an algorithm for improvement in the initializing the centroids for K-Means algorithm. We are going to work on numerical data sets along with the categorical datasets with the n dimensions. For similarity measurement we are going to consider the Manhattan distance,Dice distance and cosine distance. The result of this proposed algorithm will be compared with the original K-Means. Also the quality and complexity of the proposed algorithm will be checked with the existing algorithm.","","","10.1109/ICCUBEA.2015.205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155869","Data Clustering;K-Means;unsupervised learning;centroid","Clustering algorithms;Algorithm design and analysis;Computers;Electronic mail;Communities;Linear programming;Complexity theory","data analysis;pattern clustering;unsupervised learning;vectors","k-means clustering algorithm;data clustering;unsupervised learning technique;data points;similarity measurement criterion;vector quantization;feature learning;cluster centroids;numerical data sets;manhattan distance;cosine distance;dice distance","","5","7","","","","","IEEE","IEEE Conferences"
"A novel method based on data visual autoencoding for time series similarity matching","C. Qian; Y. Wang; G. Hu; L. Guo","School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, China","The 27th Chinese Control and Decision Conference (2015 CCDC)","","2015","","","2551","2555","A variety of techniques are currently presented for querying and mining time series data based on different kinds of representations and similarity measures. These techniques mainly focus on the numerical characteristics of data and are sensitive to the changes of time series. However, we find that time series data generally contain curves sharing some set of visual characteristics and features. These characteristics offer a deeper understanding of time series data, and open up a potential new technique for time series analysis. Particularly beneficial from recent advances in deep neural networks (DNNs), representations and features can be automatically learnt by deep learning architectures such as autoencoders. In this paper, we propose a novel method, named Time Series Visualization (TSV), to efficiently match similar time series data. Architecture and algorithms of TSV based on stacked autoencoders are introduced in this paper. Further, important factors affecting the performance of TSV are discussed based on the empirical results. Through extensive empirical evaluation, it is demonstrated that TSV has a significant superiority in efficiency and accuracy compared to existing methods for time series similarity matching.","","","10.1109/CCDC.2015.7162351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7162351","time series;autoencoder;similarity matching;dimensionality reduction;input dropout;TSV","Time series analysis;Training;Time measurement;Neural networks;Data mining;Algorithm design and analysis;Accuracy","data mining;data structures;data visualisation;encoding;learning (artificial intelligence);mathematics computing;neural nets;pattern matching;query processing;time series","data visual autoencoding;time series similarity matching;time series data querying;time series data mining;data representation;deep neural network;DNN;deep learning architecture;time series visualization;TSV","","","15","","","","","IEEE","IEEE Conferences"
"Learning a non-linear knowledge transfer model for cross-view action recognition","H. Rahmani; A. Mian","Computer Science and Software Engineering, The University of Western Australia, Australia; Computer Science and Software Engineering, The University of Western Australia, Australia","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","2458","2466","This paper concerns action recognition from unseen and unknown views. We propose unsupervised learning of a non-linear model that transfers knowledge from multiple views to a canonical view. The proposed Non-linear Knowledge Transfer Model (NKTM) is a deep network, with weight decay and sparsity constraints, which finds a shared high-level virtual path from videos captured from different unknown viewpoints to the same canonical view. The strength of our technique is that we learn a single NKTM for all actions and all camera viewing directions. Thus, NKTM does not require action labels during learning and knowledge of the camera viewpoints during training or testing. NKTM is learned once only from dense trajectories of synthetic points fitted to mocap data and then applied to real video data. Trajectories are coded with a general codebook learned from the same mocap data. NKTM is scalable to new action classes and training data as it does not require re-learning. Experiments on the IXMAS and N-UCLA datasets show that NKTM outperforms existing state-of-the-art methods for cross-view action recognition.","","","10.1109/CVPR.2015.7298860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298860","","Computational modeling;Training","gesture recognition;image motion analysis;unsupervised learning;video cameras;video coding","nonlinear knowledge transfer model;cross-view action recognition;unseen views;unknown views;unsupervised learning;nonlinear model;canonical view;NKTM;deep network;weight decay;sparsity constraints;high-level virtual path;camera viewing directions;camera viewpoints;dense trajectories;mocap data;real video data;codebook;IXMAS datasets;N-UCLA datasets","","51","41","","","","","IEEE","IEEE Conferences"
"Improving Spatial Feature Representation from Aerial Scenes by Using Convolutional Networks","K. Nogueira; W. O. Miranda; J. A. D. Santos","Dept. of Comput. Sci., Univ. Fed. de Minas Gerais, Belo Horizonte, Brazil; Dept. of Comput. Sci., Univ. Fed. de Minas Gerais, Belo Horizonte, Brazil; NA","2015 28th SIBGRAPI Conference on Graphics, Patterns and Images","","2015","","","289","296","The performance of image classification is highly dependent on the quality of extracted features. Concerning high resolution remote image images, encoding the spatial features in an efficient and robust fashion is the key to generating discriminatory models to classify them. Even though many visual descriptors have been proposed or successfully used to encode spatial features of remote sensing images, some applications, using this sort of images, demand more specific description techniques. Deep Learning, an emergent machine learning approach based on neural networks, is capable of learning specific features and classifiers at the same time and adjust at each step, in real time, to better fit the need of each problem. For several task, such image classification, it has achieved very good results, mainly boosted by the feature learning performed which allows the method to extract specific and adaptable visual features depending on the data. In this paper, we propose a novel network capable of learning specific spatial features from remote sensing images, with any pre-processing step or descriptor evaluation, and classify them. Specifically, automatic feature learning task aims at discovering hierarchical structures from the raw data, leading to a more representative information. This task not only poses interesting challenges for existing vision and recognition algorithms, but also brings huge opportunities for urban planning, crop and forest management and climate modelling. The propose convolutional neural network has six layers: three convolutional, two fully-connected and one classifier layer. So, the five first layers are responsible to extract visual features while the last one is responsible to classify the images. We conducted a systematic evaluation of the proposed method using two datasets: (i) the popular aerial image dataset UCMerced Land-use and, (ii) a multispectral high-resolution scenes of the Brazilian Coffee Scenes. The experiments show that the proposed method outperforms state-of-the-art algorithms in terms of overall accuracy.","","","10.1109/SIBGRAPI.2015.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314576","Deep Learning;Remote Sensing;Feature Learning;Image Classification;Machine Learning;High-resolution Images","Feature extraction;Neurons;Remote sensing;Mathematical model;Biological neural networks;Visualization;Robustness","feature extraction;geophysical image processing;image classification;neural nets;remote sensing","spatial feature representation;aerial scenes;remote sensing images;automatic feature learning task;convolutional neural network;convolutional layer;fully-connected layer;classifier layer;visual feature extraction;image classification;UCMerced Land-use;multispectral high-resolution scenes;Brazilian Coffee Scenes","","16","35","","","","","IEEE","IEEE Conferences"
"Clinical deep brain stimulation region prediction using regression forests from high-field MRI","J. Kim; Y. Duchin; G. Sapiro; J. Vitek; N. Harel","Department of Electrical and Computer Engineering, Duke University, Durham, USA; Center for Magnetic Resonance Research, University of Minnesota, Minneapolis, USA; Department of Electrical and Computer Engineering, Duke University, Durham, USA; Department of Neurology, University of Minnesota, Minneapolis, USA; Center for Magnetic Resonance Research, University of Minnesota, Minneapolis, USA","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","2480","2484","This paper presents a prediction framework of brain subcortical structures which are invisible on clinical low-field MRI, learning detailed information from ultrahigh-field MR training data. Volumetric segmentation of Deep Brain Stimulation (DBS) structures within the Basal ganglia is a prerequisite process for reliable DBS surgery. While ultrahigh-field MR imaging (7 Tesla) allows direct visualization of DBS targeting structures, such ultrahigh-fields are not always clinically available, and therefore the relevant structures need to be predicted from the clinical data. We address the shape prediction problem with a regression forest, non-linearly mapping predictors to target structures with high confidence, exploiting ultrahigh-field MR training data. We consider an application for the subthalamic nucleus (STN) prediction as a crucial DBS target. Experimental results on Parkinson's patients validate that the proposed approach enables reliable estimation of the STN from clinical 1.5T MRI.","","","10.1109/ICIP.2015.7351248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351248","Deep brain stimulation;ultrahigh-field MRI;statistical shape models;regression forests","Shape;Training;Magnetic resonance imaging;Satellite broadcasting;Vegetation;Predictive models;Training data","biomedical MRI;brain;image segmentation;medical image processing;neuromuscular stimulation;regression analysis;surgery","Parkinson patients;STN prediction;subthalamic nucleus prediction;nonlinearly mapping predictors;shape prediction problem;ultrahighfield MR imaging;DBS surgery;Basal ganglia;direct DBS targeting structure visualization;deep brain stimulation structures;volumetric segmentation;ultrahigh-field MR training data;clinical low-field MRI;brain subcortical structure prediction framework;high-field MRI;regression forests;clinical deep brain stimulation region prediction","","1","21","","","","","IEEE","IEEE Conferences"
"State-Clustering Based Multiple Deep Neural Networks Modeling Approach for Speech Recognition","P. Zhou; H. Jiang; L. Dai; Y. Hu; Q. Liu","National Engineering Laboratory of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; Department of Electrical Engineering and Computer Science Lassonde School of Engineering, York University, Toronto, Canada; National Engineering Laboratory of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2015","23","4","631","642","The hybrid deep neural network (DNN) and hidden Markov model (HMM) has recently achieved dramatic performance gains in automatic speech recognition (ASR). The DNN-based acoustic model is very powerful but its learning process is extremely time-consuming. In this paper, we propose a novel DNN-based acoustic modeling framework for speech recognition, where the posterior probabilities of HMM states are computed from multiple DNNs (mDNN), instead of a single large DNN, for the purpose of parallel training towards faster turnaround. In the proposed mDNN method all tied HMM states are first grouped into several disjoint clusters based on data-driven methods. Next, several hierarchically structured DNNs are trained separately in parallel for these clusters using multiple computing units (e.g. GPUs). In decoding, the posterior probabilities of HMM states can be calculated by combining outputs from multiple DNNs. In this work, we have shown that the training procedure of the mDNN under popular criteria, including both frame-level cross-entropy and sequence-level discriminative training, can be parallelized efficiently to yield significant speedup. The training speedup is mainly attributed to the fact that multiple DNNs are parallelized over multiple GPUs and each DNN is smaller in size and trained by only a subset of training data. We have evaluated the proposed mDNN method on a 64-hour Mandarin transcription task and the 320-hour Switchboard task. Compared to the conventional DNN, a 4-cluster mDNN model with similar size can yield comparable recognition performance in Switchboard (only about 2% performance degradation) with a greater than 7 times speed improvement in CE training and a 2.9 times improvement in sequence training, when 4 GPUs are used.","","","10.1109/TASLP.2015.2392944","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010902","Cross entropy training;data partition;deep neural networks (DNN);model parallelism;multiple DNNs (mDNN);parallel training;sequence training;speech recognition;state clustering","Hidden Markov models;Training;Computational modeling;Speech recognition;Training data;Speech;Acoustics","graphics processing units;hidden Markov models;neural nets;speech recognition;speech recognition equipment","switchboard task;Mandarin transcription task;sequence-level discriminative training;frame-level cross-entropy;GPU;ASR;automatic speech recognition;HMM;hidden Markov model;DNN-based acoustic model;hybrid deep neural network;mDNN method;multiple deep neural networks modeling;state-clustering;time 64 hour;time 320 hour","","7","44","","","","","IEEE","IEEE Journals"
"Stacked Face De-Noising Auto Encoders for Expression-Robust Face Recognition","C. S. N. Pathirage; L. Li; W. Liu; M. Zhang","Dept. of Comput., Curtin Univ., Perth, WA, Australia; Dept. of Comput., Curtin Univ., Perth, WA, Australia; Dept. of Comput., Curtin Univ., Perth, WA, Australia; Dept. of Math., Tianjin Univ., Tianjin, China","2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA)","","2015","","","1","8","Recent advancement in unsupervised and transfer learning methods of deep learning networks has seen a complete paradigm shift in machine learning. Inspired by the recent evolution of deep learning (DL) networks that demonstrates a proven pathway of addressing challenging dilemmas in various problem domains, we propose a novel DL framework for expression-robust feature acquisition. The framework exploits the contributions of different colour components in different local face regions by recovering the neutral expression from various expressions. Furthermore, the framework rigorously de-noises a face with dynamic expressions in a progressive way thus it is termed as stacked face de-noising auto-encoders (SFDAE). The high-level expression-robust representations that are learnt via this framework will not only yield better reconstruction of neutral expression faces but also boost the performance of the subsequent LDA[1] classifier. The experimental results reveal the superiority of the proposed method to the existing works in terms of its generalization ability and the high recognition accuracy.","","","10.1109/DICTA.2015.7371310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371310","","Face;Noise reduction;Face recognition;Training;Image reconstruction;Dictionaries;Noise measurement","face recognition;feature extraction;image classification;image colour analysis;image denoising;image reconstruction;image representation;unsupervised learning","expression-robust face recognition;unsupervised learning methods;transfer learning methods;deep learning networks;machine learning;DL networks;expression-robust feature acquisition;colour components;local face regions;dynamic expressions;stacked face denoising auto-encoders;SFDAE;high-level expression-robust representations;neutral expression faces reconstruction;LDA classifier","","3","21","","","","","IEEE","IEEE Conferences"
"Traffic lights recognition based on PCANet","Zhaojing Wang; Zhuo Bi; Cheng Wang; Lan Lin; Hui Wang","Department of Computer Science and Technology, Tongji University, Shanghai, 201804, China; Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, 201804, China; Department of Computer Science and Technology, Tongji University, Shanghai, 201804, China; Department of Electronic Science and Technology, Tongji University, Shanghai, 201804, China; Department of Electronic Science and Technology, Tongji University, Shanghai, 201804, China","2015 Chinese Automation Congress (CAC)","","2015","","","559","564","Traffic lights recognition is important to make intelligent vehicles safe. Most of existing means to detect and recognize traffic lights focus on color, size and shape of traffic lights, which are great affected by weather and illumination conditions. In this work, we utilize deep learning and SVM classifiers to recognize traffic lights for varying illumination conditions. More specifically, a PCA Network (PCANet) is used to extract features from a set of traffic light images to train SVM to classify the traffic lights. We have analyzed and compared the features extracted by PCANet with HoG's. The features have excellent characteristic to maintain features of traffic lights. In our expect, experiment results present us a satisfied recognition rate, and the algorithm can provide robust and efficient traffic lights information to support the intelligent vehicle.","","","10.1109/CAC.2015.7382563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382563","Deep Learning;Neural Network;Traffic Lights Recognition;Principal Component Analysis(PCA);Intelligent Vehicle","Feature extraction;Principal component analysis;Support vector machines;Histograms;Covariance matrices;Training;Green products","learning (artificial intelligence);object recognition;principal component analysis;support vector machines;traffic information systems","traffic lights recognition;PCANet;intelligent vehicles;illumination conditions;deep learning;SVM classifiers;PCA network;satisfied recognition rate","","1","14","","","","","IEEE","IEEE Conferences"
"Using distance estimation and deep learning to simplify calibration in food calorie measurement","P. Kuhad; A. Yassine; S. Shimohammadi","Distributed and Collaborative Virtual Environments Research Laboratory, University of Ottawa, Canada; Distributed and Collaborative Virtual Environments Research Laboratory, University of Ottawa, Canada; Distributed and Collaborative Virtual Environments Research Laboratory, University of Ottawa, Canada","2015 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA)","","2015","","","1","6","High calorie intake in the human body on the one hand, has proved harmful in numerous occasions leading to several diseases and on the other hand, a standard amount of calorie intake has been deemed essential by dietitians to maintain the right balance of calorie content in human body. As such, researchers have proposed a variety of automatic tools and systems to assist users measure their calorie in-take. In this paper, we consider the category of those tools that use image processing to recognize the food, and we propose a method for fully automatic and user-friendly calibration of the dimension of the food portion sizes, which is needed in order to measure food portion weight and its ensuing amount of calories. Experimental results show that our method, which uses deep learning, mobile cloud computing, distance estimation and size calibration inside a mobile device, leads to an accuracy improvement to 95 percent on average compared to previous work.","","","10.1109/CIVEMSA.2015.7158594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158594","","Sensors;Training;Mobile communication;Calibration;Smart phones;Biological neural networks","calibration;cloud computing;computerised instrumentation;distance measurement;image recognition;learning (artificial intelligence);size measurement;weighing","distance estimation;calibration;food calorie measurement;image processing;image recognition;food portion size dimension;food portion weight measurement;mobile cloud computing","","11","20","","","","","IEEE","IEEE Conferences"
"Reduced reference image quality assessment via Boltzmann Machines","D. C. Mocanu; G. Exarchakos; H. B. Ammar; A. Liotta","Department of Electrical Engineering, Eindhoven University of Technology, The Netherlands; Department of Electrical Engineering, Eindhoven University of Technology, The Netherlands; Department of computer and information science, University of Pennsylvania, USA; Department of Electrical Engineering, Eindhoven University of Technology, The Netherlands","2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)","","2015","","","1278","1281","Monitoring and controlling the user's perceived quality, in modern video services is a challenging proposition, mainly due to the limitations of current Image Quality Assessment (IQA) algorithms. Subjective Quality of Experience (QoE) is widely used to get a right impression, but unfortunately this can not be used in real world scenarios. In general, objective QoE algorithms represent a good substitution for the subjective ones, and they are split in three main directions: Full Reference (FR), Reduced Reference (RR), and No Reference (NR). From these three, the RR IQA approach offers a practical solution to assess the quality of an impaired image due to the fact that just a small amount of information is needed from the original image. At the same time, keeping in mind that we need automated QoE algorithms which are context independent, in this paper we introduce a novel stochastic RR IQA metric to assess the quality of an image based on Deep Learning, namely Restricted Boltzmann Machine Similarity Measure (RBMSim). RBMSim was evaluated on two benchmarked image databases with subjective studies, against objective IQA algorithms. The results show that its performance is comparable, or even better in some cases, with widely known FR IQA methods.","","","10.1109/INM.2015.7140481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7140481","Quality of Experience;Reduced Reference Image Quality Assessment;Restricted Boltzmann Machine;Deep Learning;Similarity Measure","Neurons;Streaming media;Image quality;Measurement;Databases;Quality assessment;Conferences","Boltzmann machines;learning (artificial intelligence);stochastic processes;video signal processing;visual databases","reduced reference image quality assessment;user perceived quality control;user perceived quality monitoring;video services;subjective quality-of-experience;objective QoE algorithms;full-reference;reduced reference;no-reference;FR;NR;impaired image quality assessment;automated QoE algorithms;stochastic RR IQA metric;deep-learning;restricted Boltzmann machine similarity measure;RBMSim;benchmarked image database evaluation","","11","26","","","","","IEEE","IEEE Conferences"
"Combining local appearance and holistic view: Dual-Source Deep Neural Networks for human pose estimation","Xiaochuan Fan; Kang Zheng; Yuewei Lin; S. Wang","Department of Computer Science & Engineering, University of South Carolina, Columbia, 29208, USA; Department of Computer Science & Engineering, University of South Carolina, Columbia, 29208, USA; Department of Computer Science & Engineering, University of South Carolina, Columbia, 29208, USA; Department of Computer Science & Engineering, University of South Carolina, Columbia, 29208, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","1347","1355","We propose a new learning-based method for estimating 2D human pose from a single image, using Dual-Source Deep Convolutional Neural Networks (DS-CNN). Recently, many methods have been developed to estimate human pose by using pose priors that are estimated from physiologically inspired graphical models or learned from a holistic perspective. In this paper, we propose to integrate both the local (body) part appearance and the holistic view of each local part for more accurate human pose estimation. Specifically, the proposed DS-CNN takes a set of image patches (category-independent object proposals for training and multi-scale sliding windows for testing) as the input and then learns the appearance of each local part by considering their holistic views in the full body. Using DS-CNN, we achieve both joint detection, which determines whether an image patch contains a body joint, and joint localization, which finds the exact location of the joint in the image patch. Finally, we develop an algorithm to combine these joint detection/localization results from all the image patches for estimating the human pose. The experimental results show the effectiveness of the proposed method by comparing to the state-of-the-art human-pose estimation methods based on pose priors that are estimated from physiologically inspired graphical models or learned from a holistic perspective.","","","10.1109/CVPR.2015.7298740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298740","","Joints;Proposals;Training;Testing;Graphical models;Neural networks","neural nets;pose estimation","local appearance;holistic view;dual-source deep neural networks;learning-based method;2D human pose estimation;dual-source deep convolutional neural networks;DS-CNN;graphical models;category-independent object proposals;joint detection;joint localization","","3","40","","","","","IEEE","IEEE Conferences"
"Predicting Eye Fixations on Webpage With an Ensemble of Early Features and High-Level Representations from Deep Network","C. Shen; X. Huang; Q. Zhao","Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore","IEEE Transactions on Multimedia","","2015","17","11","2084","2093","In recent decades, webpages are becoming an increasingly important visual information source. Compared with natural images, webpages are different in many ways. For example, webpages are usually rich in semantically meaningful visual media (text, pictures, logos, and animations), which make the direct application of some traditional low-level saliency models ineffective. Besides, distinct web-viewing patterns such as top-left bias and banner blindness suggest different ways for predicting attention deployment on a webpage. In this study, we utilize a new scheme of low-level feature extraction pipeline and combine it with high-level representations from deep neural networks. The proposed model is evaluated on a newly published webpage saliency dataset with three popular evaluation metrics. Results show that our model outperforms other existing saliency models by a large margin and both low- and high-level features play an important role in predicting fixations on webpage.","","","10.1109/TMM.2015.2483370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7294708","Deep learning;visual attention;web viewing;webpage saliency","Visualization;Computational modeling;Feature extraction;Predictive models;Media;Internet;Image color analysis","feature extraction;Internet;neural nets","Web page;high-level representation;visual information source;visual media;Web-viewing pattern;low-level feature extraction pipeline;deep neural network","","11","50","","","","","IEEE","IEEE Journals"
"An Approach to Iris Contact Lens Detection Based on Deep Image Representations","P. Silva; E. Luz; R. Baeta; H. Pedrini; A. X. Falcao; D. Menotti","Comput. Dept., Fed. Univ. of Ouro Preto, Ouro Preto, Brazil; Comput. Dept., Fed. Univ. of Ouro Preto, Ouro Preto, Brazil; Comput. Dept., Fed. Univ. of Ouro Preto, Ouro Preto, Brazil; Inst. of Comput., Univ. of Campinas, Campinas, Brazil; Inst. of Comput., Univ. of Campinas, Campinas, Brazil; Comput. Dept., Fed. Univ. of Ouro Preto, Ouro Preto, Brazil","2015 28th SIBGRAPI Conference on Graphics, Patterns and Images","","2015","","","157","164","Spoofing detection is a challenging task in biometric systems, when differentiating illegitimate users from genuine ones. Although iris scans are far more inclusive than fingerprints, and also more precise for person authentication, iris recognition systems are vulnerable to spoofing via textured cosmetic contact lenses. Iris spoofing detection is also referred to as liveness detection (binary classification of fake and real images). In this work, we focus on a three-class detection problem: images with textured (colored) contact lenses, soft contact lenses, and no lenses. Our approach uses a convolutional network to build a deep image representation and an additional fully-connected single layer with soft max regression for classification. Experiments are conducted in comparison with a state-of-the-art approach (SOTA) on two public iris image databases for contact lens detection: 2013 Notre Dame and IIIT-Delhi. Our approach can achieve a 30% performance gain over SOTA on the former database (from 80% to 86%) and comparable results on the latter. Since IIIT-Delhi does not provide segmented iris images and, differently from SOTA, our approach does not segment the iris yet, we conclude that these are very promising results.","","","10.1109/SIBGRAPI.2015.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314559","Iris Biometrics;Contact Lens Detection;Deep Learning;Convolutional Networks","Lenses;Iris recognition;Databases;Iris;Training;Network topology;Image representation","authorisation;image representation;image segmentation;image texture;iris recognition;regression analysis","iris contact lens detection;deep image representations;biometric systems;illegitimate users;iris scans;person authentication;iris recognition systems;textured cosmetic contact lenses;iris spoofing detection;liveness detection;soft contact lenses;softmax regression;fully-connected single layer;state-of-the-art approach;SOTA;Notre Dame;IIIT-Delhi;segmented iris images","","20","45","","","","","IEEE","IEEE Conferences"
"Recognizing complex mental states with deep hierarchical features for Human-Robot Interaction","P. Barros; S. Wermter","University of Hamburg, Department of Informatics, Vogt-Kolln-Strae 30, 22527, Germany; University of Hamburg, Department of Informatics, Vogt-Kolln-Strae 30, 22527, Germany","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","2015","","","4065","4070","The use of emotional states for Human-Robot Interaction (HRI) has attracted considerable attention in recent years. One of the most challenging tasks is to recognize the spontaneous expression of emotions, especially in an HRI scenario. Every person has a different way to express emotions, and this is aggravated by the complexity of interaction with different subjects, multimodal information and different environments. We propose a deep neural model which is able to deal with these characteristics and which is applied in recognition of complex mental states. Our system is able to learn and extract deep spatial and temporal features and to use them to classify emotions in sequences. To evaluate the system, the CAM3D corpus is used. This corpus is composed of videos recorded from different subjects and in different indoor environments. Each video contains the recording of the upper-body part of the subject expressing one of twelve complex mental states. Our system is able to recognize spontaneous complex mental states from different subjects and can be used in such an HRI scenario.","","","10.1109/IROS.2015.7353951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353951","","Feature extraction;Visualization;Training;Emotion recognition;Computer architecture;Robots;Human-robot interaction","emotion recognition;feature extraction;human-robot interaction;neural nets;robot vision","CAM3D corpus;emotion classification;deep temporal feature extraction;deep spatial feature extraction;deep neural model;emotion recognition;human-robot interaction;deep hierarchical features;complex mental state recognition","","","27","","","","","IEEE","IEEE Conferences"
"MMSS: Multi-modal Sharable and Specific Feature Learning for RGB-D Object Recognition","A. Wang; J. Cai; J. Lu; T. Cham","Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore; Dept. of Autom., Tsinghua Univ., Beijing, China; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1125","1133","Most of the feature-learning methods for RGB-D object recognition either learn features from color and depth modalities separately, or simply treat RGB-D as undifferentiated four-channel data, which cannot adequately exploit the relationship between different modalities. Motivated by the intuition that different modalities should contain not only some modal-specific patterns but also some shared common patterns, we propose a multi-modal feature learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, and then connect them with our carefully designed multi-modal layers, which fuse color and depth information by enforcing a common part to be shared by features of different modalities. In this way, we obtain features reflecting shared properties as well as modal-specific properties in different modalities. The information of the multi-modal learning frameworks is back-propagated to the early CNN layers. Experimental results show that our proposed multi-modal feature learning method outperforms state-of-the-art approaches on two widely used RGB-D object benchmark datasets.","","","10.1109/ICCV.2015.134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410491","","Image color analysis;Object recognition;Feature extraction;Labeling;Sparse matrices;Computer vision;Learning systems","backpropagation;image colour analysis;object recognition","MMSS;multimodal sharable and specific feature learning;RGB-D object recognition;color modalities;depth modalities;modal-specific patterns;deep CNN layers;modal-specific properties;backpropagation;convolutional neural network","","26","34","","","","","IEEE","IEEE Conferences"
"Saliency-guided deep framework for image quality assessment","W. Hou; X. Gao","Xidian University, Xi'an; Xidian University, Xi'an","IEEE MultiMedia","","2015","PP","99","1","1","Image quality assessment (IQA) has thrived for decades, and still remains its significance in the fields of image processing and computer vision. However, instead of pure engineering applications, researchers become keener to explore how human brain perceives the visual stimuli. Massive psychological evidences show that human beings prefer qualitative description to evaluate image quality, however most IQA researches still concentrate on the numerical one. Furthermore, the hand-crafting features are widely used in this community, which constrains the models' flexibility. Therefore, a novel model is proposed with two major advantages: 1) the saliency-guided feature learning is able to learn features unsupervisedly; 2) the deep framework recasts IQA as a classification problem, analogous to human qualitative evaluation. The experiments are conducted on popular databases to validate the effectiveness of the proposed model.","","","10.1109/MMUL.2015.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155457","","Image quality;Feature extraction;Adaptation models;Visualization;Numerical models;Distortion;Image coding","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Modelling a Quadrotor Vehicle Using a Modular Deep Recurrent Neural Network","N. Mohajerin; S. L. Waslander","Dept. of Mech. & Mechatron. Eng., Univ. of Waterloo, Waterloo, ON, Canada; Dept. of Mech. & Mechatron. Eng., Univ. of Waterloo, Waterloo, ON, Canada","2015 IEEE International Conference on Systems, Man, and Cybernetics","","2015","","","376","381","In this paper, the Modular Deep Recurrent Neural Network (MODERNN) framework is studied for learning a Multi-Input-Multi-Output (MIMO) model of a quad rotor. Comparing a Single-Input-Single-Output (SISO) system, a MIMO system is much harder to model because of the intercoupling of the system variables as well as the multi-dimensionality of the input and output spaces. In this paper it is shown that the MODERNN framework is capable of modelling complex MIMO dynamical mappings, such as a simulated MIMO model (4-by-4) of a quad rotor vehicle in the presence of noise and ground effect.","","","10.1109/SMC.2015.77","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379209","Deep Recurrent Neural Networks;Quadrotor Modelling","Mathematical model;Jacobian matrices;MIMO;Vehicles;Recurrent neural networks;Numerical models","autonomous aerial vehicles;helicopters;MIMO systems;neurocontrollers","modular deep recurrent neural network framework;MODERNN framework;learning;MIMO model;single-input-single-output system;SISO system;MIMO dynamical mapping;multiinput-multioutput model;quadrotor vehicle","","5","21","","","","","IEEE","IEEE Conferences"
"The NAIST ASR system for the 2015 Multi-Genre Broadcast challenge: On combination of deep learning systems using a rank-score function","Q. T. Do; M. Heck; S. Sakti; G. Neubig; T. Toda; S. Nakamura","Augmented Human Communication Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Augmented Human Communication Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Augmented Human Communication Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Augmented Human Communication Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Augmented Human Communication Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Augmented Human Communication Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","654","659","The Multi-Genre Broadcast challenge is an official challenge of the IEEE Automatic Speech Recognition and Understanding Workshop. This paper presents NAISTs contribution to the premiere of this challenge. The presented speech-to-text system for English makes use of various front-ends (e.g., MFCC, i-vector and FBANK), DNN acoustic models and several language models for decoding and rescoring (N-gram, RNNLM). Subsets of the training data with varying sizes were evaluated with respect to the overall training quality. Two speech segmentation systems were developed for the challenge, based on DNNs and GMM-HMMs. Recognition was performed in three stages: Decoding, lattice rescoring and system combination. This paper focuses on the system combination experiments and presents a rank-score based system weighting approach, which gave better performance compared to a normal system combination strategy. The DNN based ASR system trained on MFCC + i-vector features with the sMBR training criterion gives the best performance of 27.8% WER, and thus significantly outperforms the baseline DNN-HMM sMBR yielding 33.7% WER.","","","10.1109/ASRU.2015.7404858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404858","speech recognition;ASRU MGB;broadcast;evaluation system;system development","Training;Decoding;Speech;Mel frequency cepstral coefficient;Standards;Data models","Gaussian processes;hidden Markov models;mixture models;neural nets;speech recognition;speech synthesis","rank-score based system weighting approach;system combination;lattice rescoring;GMM-HMM;speech segmentation systems;overall training quality;RNNLM;N-gram;language models;DNN acoustic models;speech-to-text system;IEEE Automatic Speech Recognition and Understanding Workshop;rank-score function;deep learning systems;multigenre broadcast challenge;NAIST ASR system","","1","20","","","","","IEEE","IEEE Conferences"
"Auditory features for the close talk speech enhancement with parameter masks","Y. Jiang; Y. Zu; R. Liu","The Quartermaster Equipment Research Institute, CPLA Beijing, P.R. China; The Quartermaster Equipment Research Institute, CPLA Beijing, P.R. China; The Department of Electronic Engineering, Tsinghua University, Beijing, P. R. China","2015 8th International Congress on Image and Signal Processing (CISP)","","2015","","","1194","1198","The speech segregation and enhancement is a hard task in speech communication. In order to get the clean target speech, a close talk system is used to collect the speech with a nearby microphone. A deep neural networks (DNN) estimator is used in a frequency channel for speech energy calculation with parameter masks. The adjusted binaural auditory features are used as the main input for DNN speech energy estimation. The energy difference between the two microphones is used as the main binaural auditory feature. The time difference is also used as the comparison feature. Experiments show the energy difference feature can get the similar performance to the combination two microphones monaural and binaural auditory features with limited calculation complexity. The two microphones energy difference feature is one of the key features in close talk speech enhancement.","","","10.1109/CISP.2015.7408062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7408062","Auditory feature;close talk;speech enhancement;parameter masks;deep neural networks (DNN)","Speech;Filter banks;Microphones;Feature extraction;Ear;Speech enhancement","estimation theory;feature extraction;learning (artificial intelligence);neural nets;speech enhancement","auditory feature;close talk system;speech enhancement;parameter mask;speech segregation;deep neural network;DNN speech energy estimation;frequency channel","","","10","","","","","IEEE","IEEE Conferences"
"Deep evolution of image representations for handwritten digit recognition","A. Agapitos; M. O'Neill; M. Nicolau; D. Fagan; A. Kattan; A. Brabazon; K. Curran","Complex and Adaptive Systems Laboratory, University College Dublin, Ireland; Complex and Adaptive Systems Laboratory, University College Dublin, Ireland; Complex and Adaptive Systems Laboratory, University College Dublin, Ireland; Complex and Adaptive Systems Laboratory, University College Dublin, Ireland; Computer Science Department, Um Al-Qura University, Saudi Arabia; Complex and Adaptive Systems Laboratory, University College Dublin, Ireland; Complex and Adaptive Systems Laboratory, University College Dublin, Ireland","2015 IEEE Congress on Evolutionary Computation (CEC)","","2015","","","2452","2459","A training protocol for learning deep neural networks, called greedy layer-wise training, is applied to the evolution of a hierarchical, feed-forward Genetic Programming based system for feature construction and object recognition. Results on a popular handwritten digit recognition benchmark clearly demonstrate that two layers of feature transformations improves generalisation compared to a single layer. In addition, we show that the proposed system outperforms several standard Genetic Programming systems, which are based on hand-designed features, and use different program representations and fitness functions.","","","10.1109/CEC.2015.7257189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7257189","","Training;Feature extraction;Object recognition;Logistics;Convolution;Error analysis;Image representation","feature extraction;genetic algorithms;greedy algorithms;handwritten character recognition;image representation;learning (artificial intelligence);neural nets;object recognition","image representations;training protocol;deep neural network learning;greedy layerwise training;hierarchical feedforward genetic programming based system;feature construction;object recognition;handwritten digit recognition benchmark;feature transformation;program representation;fitness functions","","6","16","","","","","IEEE","IEEE Conferences"
"Reconstruction combined training for convolutional neural networks on character recognition","L. Chen; S. Wang; W. Fan; J. Sun; N. Satoshi","Fujitsu Research & Development Center, Beijing, China; Fujitsu Research & Development Center, Beijing, China; Fujitsu Research & Development Center, Beijing, China; Fujitsu Research & Development Center, Beijing, China; Fujitsu Research & Development Center, Beijing, China","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","431","435","Recently, the deep learning methods have achieved great success in pattern recognition tasks. Especially for character recognition, most of the state-of-the-art results belong to the deep learning models. Among those models, the convolutional neural network (CNN) becomes the most popular due to its outstanding performance. Therefore, many trials were made in order to make improvements on CNN. However, most of the trials only focused on the network structure or training skills, the inter-class information is usually ignored. In this paper, we have proposed a novel CNN model with two training feedbacks: the reconstruction feedback and the classification feedback. By using the reconstruction feedback, the inter-class information (for example, shape similarity) of the characters is taken into account. Consequently, without enlarging the network structure, our model can outperform those state-of-the-art improved CNN models, which is proved by the experimental results.","","","10.1109/ICDAR.2015.7333798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333798","","","character recognition;image classification;image reconstruction;learning (artificial intelligence);neural nets","convolutional neural networks;character recognition;reconstruction combined training;deep learning methods;pattern recognition tasks;deep learning models;training skills;interclass information;reconstruction feedback;classification feedback;network structure;improved CNN models","","2","26","","","","","IEEE","IEEE Conferences"
"Improving object detection with deep convolutional networks via Bayesian optimization and structured prediction","Y. Zhang; K. Sohn; R. Villegas; G. Pan; H. Lee","Department of Computer Science, Zhejiang University, Hangzhou, China; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA; Department of Computer Science, Zhejiang University, Hangzhou, China; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","249","258","Object detection systems based on the deep convolutional neural network (CNN) have recently made ground-breaking advances on several object detection benchmarks. While the features learned by these high-capacity neural networks are discriminative for categorization, inaccurate localization is still a major source of error for detection. Building upon high-capacity CNN architectures, we address the localization problem by 1) using a search algorithm based on Bayesian optimization that sequentially proposes candidate regions for an object bounding box, and 2) training the CNN with a structured loss that explicitly penalizes the localization inaccuracy. In experiments, we demonstrate that each of the proposed methods improves the detection performance over the baseline method on PASCAL VOC 2007 and 2012 datasets. Furthermore, two methods are complementary and significantly outperform the previous state-of-the-art when combined.","","","10.1109/CVPR.2015.7298621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298621","","Bayes methods;Optimization;Yttrium;Object detection;Proposals;Search problems;Training","Bayes methods;convolution;learning (artificial intelligence);neural nets;object detection;optimisation","object detection;deep convolutional neural network;CNN;Bayesian optimization;feature learning","","74","47","","","","","IEEE","IEEE Conferences"
"Modeling long temporal contexts in convolutional neural network-based phone recognition","L. Tóth","MTA-SZTE Research Group on Artificial Intelligence, Hungarian Academy of Sciences and University of Szeged, Hungary","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4575","4579","The deep neural network component of current hybrid speech recognizers is trained on a context of consecutive feature vectors. Here, we investigate whether the time span of this input can be extended by splitting it up and modeling it in smaller chunks. One method for this is to train a hierarchy of two networks, while the less well-known split temporal context (STC) method models the left and right contexts of a frame separately. Here, we evaluate these techniques within a convolutional neural network framework, and find that the two approaches can be nicely combined. With the combined model we can expand the time-span of our network to 69 frames, and we achieve a 7.5% relative error rate reduction compared to modeling this large context as one block. We report a phone error rate of 17.1% on the TIMIT core test set, which is one of the best scores published.","","","10.1109/ICASSP.2015.7178837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178837","Deep neural network;convolutional neural network;maxout;split temporal context;TIMIT","Context;Hidden Markov models;Context modeling;Neural networks;Speech recognition;Error analysis;Convolution","convolution;learning (artificial intelligence);neural nets;speech recognition;vectors","long temporal context modeling;convolutional neural network-based phone recognition;deep neural network component;current hybrid speech recognizers;consecutive feature vectors;split temporal context method;STC method;relative error rate reduction;TIMIT core test set;phone error rate","","","26","","","","","IEEE","IEEE Conferences"
"SALICON: Reducing the Semantic Gap in Saliency Prediction by Adapting Deep Neural Networks","X. Huang; C. Shen; X. Boix; Q. Zhao","Dept. of Electr. & Comput. Eng., Nat. Univ. of Singapore, Singapore, Singapore; Dept. of Electr. & Comput. Eng., Nat. Univ. of Singapore, Singapore, Singapore; Dept. of Electr. & Comput. Eng., Nat. Univ. of Singapore, Singapore, Singapore; Dept. of Electr. & Comput. Eng., Nat. Univ. of Singapore, Singapore, Singapore","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","262","270","Saliency in Context (SALICON) is an ongoing effort that aims at understanding and predicting visual attention. Conventional saliency models typically rely on low-level image statistics to predict human fixations. While these models perform significantly better than chance, there is still a large gap between model prediction and human behavior. This gap is largely due to the limited capability of models in predicting eye fixations with strong semantic content, the so-called semantic gap. This paper presents a focused study to narrow the semantic gap with an architecture based on Deep Neural Network (DNN). It leverages the representational power of high-level semantics encoded in DNNs pretrained for object recognition. Two key components are fine-tuning the DNNs fully convolutionally with an objective function based on the saliency evaluation metrics, and integrating information at different image scales. We compare our method with 14 saliency models on 6 public eye tracking benchmark datasets. Results demonstrate that our DNNs can automatically learn features particularly for saliency prediction that surpass by a big margin the state-of-the-art. In addition, our model ranks top to date under all seven metrics on the MIT300 challenge set.","","","10.1109/ICCV.2015.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410395","","Semantics;Computational modeling;Measurement;Predictive models;Object recognition;Neurons;Spatial resolution","feature extraction;gaze tracking;learning (artificial intelligence);neural nets;object recognition","object recognition;saliency evaluation metrics;image scales;public eye tracking benchmark datasets;feature learning;MIT300 challenge set;high-level semantics representational power;DNN;semantic content;eye fixation prediction;human fixation prediction;low-level image statistics;visual attention prediction;saliency in context;deep neural networks;saliency prediction;semantic gap reduction;SALICON","","131","44","","","","","IEEE","IEEE Conferences"
"Learning Spatiotemporal Features with 3D Convolutional Networks","D. Tran; L. Bourdev; R. Fergus; L. Torresani; M. Paluri","NA; NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","4489","4497","We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.","","","10.1109/ICCV.2015.510","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410867","","Three-dimensional displays;Convolution;Kernel;Feature extraction;Solid modeling;Streaming media;Training","feature extraction;image classification;learning (artificial intelligence);neural nets;spatiotemporal phenomena;video signal processing","spatiotemporal feature learning;3D convolutional networks;deep 3-dimensional convolutional networks;3D ConvNets;large scale supervised video dataset;homogeneous architecture;convolution kernels;C3D features;convolutional 3D;linear classifier;UCF101 dataset","","1164","48","","","","","IEEE","IEEE Conferences"
"Deeply learned face representations are sparse, selective, and robust","Y. Sun; X. Wang; X. Tang","Department of Information Engineering, The Chinese University of Hong Kong, China; Department of Electronic Engineering, The Chinese University of Hong Kong, China; Department of Information Engineering, The Chinese University of Hong Kong, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","2892","2900","This paper designs a high-performance deep convolutional network (DeepID2+) for face recognition. It is learned with the identification-verification supervisory signal. By increasing the dimension of hidden representations and adding supervision to early convolutional layers, DeepID2+ achieves new state-of-the-art on LFW and YouTube Faces benchmarks. Through empirical studies, we have discovered three properties of its deep neural activations critical for the high performance: sparsity, selectiveness and robustness. (1) It is observed that neural activations are moderately sparse. Moderate sparsity maximizes the discriminative power of the deep net as well as the distance between images. It is surprising that DeepID2+ still can achieve high recognition accuracy even after the neural responses are binarized. (2) Its neurons in higher layers are highly selective to identities and identity-related attributes. We can identify different subsets of neurons which are either constantly excited or inhibited when different identities or attributes are present. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such high-level concepts. (3) It is much more robust to occlusions, although occlusion patterns are not included in the training set.","","","10.1109/CVPR.2015.7298907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298907","","Face;Neurons;Accuracy;Training;Face recognition;Robustness;Convolution","face recognition;neural nets","deeply learned face representations;deep convolutional neural network;face recognition;identification-verification supervisory signal;DeepID2+;deep neural activations;neural responses;occlusion","","271","38","","","","","IEEE","IEEE Conferences"
"Aggregating Local Deep Features for Image Retrieval","A. B. Yandex; V. Lempitsky","Moscow Inst. of Phys. & Technol., Dolgoprudny, Russia; Skolkovo Inst. of Sci. & Technol., Russia","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1269","1277","Several recent works have shown that image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems. It also has been shown that the activations from the convolutional layers can be interpreted as local features describing particular image regions. These local features can be aggregated using aggregating methods developed for local features (e.g. Fisher vectors), thus providing new powerful global descriptor. In this paper we investigate possible ways to aggregate local deep features to produce compact descriptors for image retrieval. First, we show that deep features and traditional hand-engineered features have quite different distributions of pairwise similarities, hence existing aggregation methods have to be carefully re-evaluated. Such re-evaluation reveals that in contrast to shallow features, the simple aggregation method based on sum pooling provides the best performance for deep convolutional features. This method is efficient, has few parameters, and bears little risk of overfitting when e.g. learning the PCA matrix. In addition, we suggest a simple yet efficient query expansion scheme suitable for the proposed aggregation method. Overall, the new compact global descriptor improves the state-of-the-art on four common benchmarks considerably.","","","10.1109/ICCV.2015.150","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410507","","Image retrieval;Principal component analysis;Feature extraction;Reliability;Aggregates;Buildings;Neural networks","image classification;image retrieval;matrix algebra;neural nets;principal component analysis;vectors","compact global descriptor;query expansion scheme;PCA matrix;deep convolutional feature;sum pooling;shallow feature;aggregation method;pairwise similarity;hand-engineered feature;compact descriptor;Fisher vector;aggregating method;image region;local feature;convolutional layer;image retrieval problem;image classification;deep convolutional neural network;image descriptor;local deep feature","","30","26","","","","","IEEE","IEEE Conferences"
"Multi-modal sensor registration for vehicle perception via deep neural networks","M. Giering; V. Venugopalan; K. Reddy","United Technologies Research Center, E. Hartford, CT 06018, USA; United Technologies Research Center, E. Hartford, CT 06018, USA; United Technologies Research Center, E. Hartford, CT 06018, USA","2015 IEEE High Performance Extreme Computing Conference (HPEC)","","2015","","","1","6","The ability to simultaneously leverage multiple modes of sensor information is critical for perception of an automated vehicle's physical surroundings. Spatio-temporal alignment of registration of the incoming information is often a prerequisite to analyzing the fused data. The persistence and reliability of multi-modal registration is therefore the key to the stability of decision support systems ingesting the fused information. LiDAR-video systems like on those many driverless cars are a common example of where keeping the LiDAR and video channels registered to common physical features is important. We develop a deep learning method that takes multiple channels of heterogeneous data, to detect the misalignment of the LiDAR-video inputs. A number of variations were tested on the Ford LiDAR-video driving test data set and will be discussed. To the best of our knowledge the use of multi-modal deep convolutional neural networks for dynamic real-time LiDAR-video registration has not been presented.","","","10.1109/HPEC.2015.7322485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322485","","Laser radar;Optical imaging;Optical sensors;Accuracy;Testing;Three-dimensional displays;Training","image fusion;image registration;neural nets;optical radar;radar imaging;video signal processing","multimodal sensor registration;vehicle perception;multiple sensor information modes;automated vehicle physical surroundings;spatio-temporal alignment;fused data analysis;multimodal registration reliability;multimodal registration persistence;decision support systems;LiDAR-video systems;driverless cars;physical features;deep-learning method;heterogeneous data;LiDAR-video input misalignment detection;Ford LiDAR-video driving test data set;multimodal deep-convolutional neural networks;dynamic real-time LiDAR-video registration","","10","30","","","","","IEEE","IEEE Conferences"
"Acoustic modeling with neural graph embeddings","Y. Liu; K. Kirchhoff","Department of Electrical Engineering, University of Washington, Seattle, WA 98195; Department of Electrical Engineering, University of Washington, Seattle, WA 98195","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","581","588","Graph-based learning (GBL) is a form of semi-supervised learning that has been successfully exploited in acoustic modeling in the past. It utilizes manifold information in speech data that is represented as a joint similarity graph over training and test samples. Typically, GBL is used at the output level of an acoustic classifier; however, this setup is difficult to scale to large data sets, and the graph-based learner is not optimized jointly with other components of the speech recognition system. In this paper we explore a different approach where the similarity graph is first embedded into continuous space using a neural autoencoder. Features derived from this encoding are then used at the input level to a standard DNN-based speech recognizer. We demonstrate improved scalability and performance compared to the standard GBL approach as well as significant improvements in word error rate on a medium-vocabulary Switchboard task.","","","10.1109/ASRU.2015.7404848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404848","Acoustic modeling;deep neural networks;graph-based learning","Acoustics;Hidden Markov models;Training;Standards;Data models;Error analysis;Encoding","graph theory;learning (artificial intelligence);neural nets;speech recognition","acoustic modeling;neural graph embeddings;graph-based learning;GBL;semisupervised learning;manifold information;speech data;similarity graph;neural autoencoder;DNN-based speech recognizer;medium-vocabulary Switchboard task","","3","28","","","","","IEEE","IEEE Conferences"
"Deep visual-semantic alignments for generating image descriptions","A. Karpathy; L. Fei-Fei","Department of Computer Science, Stanford University, USA; Department of Computer Science, Stanford University, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3128","3137","We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.","","","10.1109/CVPR.2015.7298932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298932","","","image retrieval;image segmentation;learning (artificial intelligence);natural language processing;recurrent neural nets","deep visual-semantic alignment;image description generation;natural language image description;sentence description;learning;intermodal correspondence;visual data;convolutional neural network;image region;bidirectional recurrent neural network;multimodal embedding;multimodal recurrent neural network architecture;Flickr8K dataset;Flickr30K dataset;MSCOCO dataset;image retrieval;region-level annotation","","838","60","","","","","IEEE","IEEE Conferences"
"Effective CAD research in the sea of papers","J. Liu; D. Juan; Y. Shi","Department of Computer, Science and Engineering, University of Notre Dame, IN 46556, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Department of Computer, Science and Engineering, University of Notre Dame, IN 46556, USA","2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","","2015","","","781","785","In the past decade, there has been a rapid growth in the number of journal, conference and workshop publications from academic research. The growth seems to be accelerated as time goes by. Accordingly, it has become increasingly difficult for researchers to efficiently identify papers related to a given topic, leading to missing important references or even repetitive work. Moreover, even when these papers are found, it is very time-consuming to find their inherent relations. In this paper, using CAD research as a vehicle, we will demonstrate a novel deep learning based framework that can automatically search for papers related to a given abstract of research, and suggest how they are correlated. We also provide the analysis and comparison among several classic machine-learning approaches. Experimental results show that the proposed approach always outperforms the conventional keyword-based rankings, in both accuracy and F1 scores.","","","10.1109/ICCAD.2015.7372650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372650","","Support vector machines;Kernel;Feature extraction;Neurons;Training;Data models;Machine learning algorithms","information retrieval;learning (artificial intelligence);publishing","CAD research;papers;journal;conference;workshop publications;academic research;deep learning based framework;paper searching;abstract;machine-learning approaches","","1","10","","","","","IEEE","IEEE Conferences"
"The effect of different hidden unit number of sparse autoencoder","Q. Xu; L. Zhang","School of Mechanical, Electrical & Information Engineering, Shandong University, Weihai 264209; School of Mechanical, Electrical & Information Engineering, Shandong University, Weihai 264209","The 27th Chinese Control and Decision Conference (2015 CCDC)","","2015","","","2464","2467","Sparse autoencoder is the fundamental part in some deep architecture. The hidden layer output is the compression of the input data which gives a better representation of the input than the original raw input. However, the determination of hidden unit number is always experiential. In this paper, the different hidden unit number is discussed. The weight of sparse autoencoder will learn the digital number outline of the handwriting instead of pen strokes when the hidden unit number is smaller. The weight can learn the pen strokes of the handwriting when the hidden unit number is larger.","","","10.1109/CCDC.2015.7162335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7162335","Sparse autoencoder;Different hidden unit number;Backpropagation;MNIST database","Computer architecture;Visualization;Accuracy;Backpropagation;Unsupervised learning;Neural networks;Databases","backpropagation;data compression;handwriting recognition;image coding;image representation","sparse autoencoder;deep architecture;hidden layer output;input data compression;input representation;hidden unit number;digital number outline;handwriting;pen strokes;deep learning;backpropagation","","5","6","","","","","IEEE","IEEE Conferences"
"Joint Fine-Tuning in Deep Neural Networks for Facial Expression Recognition","H. Jung; S. Lee; J. Yim; S. Park; J. Kim","NA; NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2983","2991","Temporal information has useful features for recognizing facial expressions. However, to manually design useful features requires a lot of effort. In this paper, to reduce this effort, a deep learning technique, which is regarded as a tool to automatically extract useful features from raw data, is adopted. Our deep network is based on two different models. The first deep network extracts temporal appearance features from image sequences, while the other deep network extracts temporal geometry features from temporal facial landmark points. These two models are combined using a new integration method in order to boost the performance of the facial expression recognition. Through several experiments, we show that the two models cooperate with each other. As a result, we achieve superior performance to other state-of-the-art methods in the CK+ and Oulu-CASIA databases. Furthermore, we show that our new integration method gives more accurate results than traditional methods, such as a weighted summation and a feature concatenation method.","","","10.1109/ICCV.2015.341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410698","","Image sequences;Three-dimensional displays;Face recognition;Feature extraction;Databases;Image recognition;Training","computational geometry;face recognition;feature extraction;image sequences;learning (artificial intelligence);neural nets","joint fine-tuning;deep neural networks;facial expression recognition;temporal information;image sequences;weighted summation;feature concatenation method","","156","26","","","","","IEEE","IEEE Conferences"
"Revealing critical channels and frequency bands for emotion recognition from EEG with deep belief network","W. Zheng; H. Guo; B. Lu","Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University, 800 Dong Chuan Road, 200240, China; Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University, 800 Dong Chuan Road, 200240, China; Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University, 800 Dong Chuan Road, 200240, China","2015 7th International IEEE/EMBS Conference on Neural Engineering (NER)","","2015","","","154","157","For EEG-based emotion recognition tasks, there are many irrelevant channel signals contained in multichannel EEG data, which may cause noise and degrade the performance of emotion recognition systems. In order to tackle this problem, we propose a novel deep belief network (DBN) based method for examining critical channels and frequency bands in this paper. First, we design an emotion experiment and collect EEG data while subjects are watching emotional film clips. Then we train DBN for recognizing three emotions (positive, neutral, and negative) with extracted differential entropy features as input and compare DBN with other shallow models such as KNN, LR, and SVM. The experiment results show that DBN achieves the best average accuracy of 86.08%. We further explore critical channels and frequency bands by examining the weight distribution learned by DBN, which is different from the existing work. We identify four profiles with 4, 6, 9 and 12 channels, which achieve recognition accuracies of 82.88%, 85.03%, 84.02%, 86.65%, respectively, using SVM.","","","10.1109/NER.2015.7146583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7146583","","Emotion recognition;Electroencephalography;Accuracy;Electrodes;Support vector machines;Feature extraction;Standards","belief networks;electroencephalography;emotion recognition;feature extraction;medical signal processing;support vector machines","EEG-based emotion recognition tasks;deep belief network;critical channels;frequency bands;positive emotion;neutral emotion;negative emotion;differential entropy feature extraction;weight distribution;SVM","","10","13","","","","","IEEE","IEEE Conferences"
"Deep bi-directional recurrent networks over spectral windows","A. Mohamed; F. Seide; D. Yu; J. Droppo; A. Stoicke; G. Zweig; G. Penn","Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; University of Toronto","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","78","83","Long short-term memory (LSTM) acoustic models have recently achieved state-of-the-art results on speech recognition tasks. As a type of recurrent neural network, LSTMs potentially have the ability to model long-span phenomena relating the spectral input to linguistic units. However, it has not been clear whether their observed performance is actually due to this capability, or instead if it is due to a better modeling of short term dynamics through the recurrence. In this paper. we answer this question by applying a windowed (truncated) LSTM to conversational speech transcription, and find that a limited context is adequate, and that it is not necessaary to scan the entire utterance. The sliding window approach allows not only incremental (online) recognition with a bidirectional model, but also frame-wise randomization (as opposed to utterance randomization), which results in faster convergence. On the SWBD/Fisher corpus, applying bidirectional LSTM RNNs to spectral windows of about 0.5s improves WER on the Hub5'00 benchmark set by 16% relative compared to our best sequence-trained DNN. On an extended 3850h training set that that also includes lectures, the relative gain becomes 28% (Hub5'00 WER 9.2%). In-house conversational data improves by 12 to 17% relative.","","","10.1109/ASRU.2015.7404777","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404777","Recurrent networks;LSTM;Deep learning;acoustic modeling","Hidden Markov models;Acoustics;Training;Bidirectional control;Speech;Recurrent neural networks;Context","recurrent neural nets;speech recognition","deep bidirectional recurrent networks;spectral windows;LSTM acoustic models;long short-term memory acoustic models;recurrent neural network;speech recognition;windowed LSTM;speech transcription;frame-wise randomization;SWBD/Fisher corpus;sequence-trained DNN","","10","22","","","","","IEEE","IEEE Conferences"
"Singing voice detection with deep recurrent neural networks","S. Leglaive; R. Hennequin; R. Badeau","Audionamix, 171 quai de Valmy, 75010 Paris, France; Audionamix, 171 quai de Valmy, 75010 Paris, France; Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, 37-39 rue Dareau, 75014, France","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","121","125","In this paper, we propose a new method for singing voice detection based on a Bidirectional Long Short-Term Memory (BLSTM) Recurrent Neural Network (RNN). This classifier is able to take a past and future temporal context into account to decide on the presence/absence of singing voice, thus using the inherent sequential aspect of a short-term feature extraction in a piece of music. The BLSTM-RNN contains several hidden layers, so it is able to extract a simple representation fitted to our task from low-level features. The results we obtain significantly outperform state-of-the-art methods on a common database.","","","10.1109/ICASSP.2015.7177944","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177944","Singing Voice Detection;Deep Learning;Recurrent Neural Networks;Long Short-Term Memory","Feature extraction;Training;Context;Recurrent neural networks;Harmonic analysis;Databases;Computer architecture","recurrent neural nets;speech","singing voice detection;deep recurrent neural networks;bidirectional long short-term memory;BLSTM;recurrent neural network;RNN;short-term feature extraction;state-of-the-art methods","","21","22","","","","","IEEE","IEEE Conferences"
"Understanding Deep Networks with Gradients","H. Z. Lo; W. Ding","NA; NA","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","","2015","","","1548","1555","Existing methods for understanding the inner workings of convolutional neural networks have relied on visualizations, which do not describe the connections between the layers and units of the network. We introduce the prediction gradient as a measure of a neuron's relevance to prediction. Using this quantity, we study a relatively small convolutional neural network and make three observations. First, there exists a small number of high prediction-gradient units, which upon removal, severely impact the ability of the network to classify correctly. Second, this performance loss generalizes spans multiple classes, and is not mirrored by removing low-gradient units. Third, the distributed representation of the neural network prevents performance from being impacted until a critical number of units are destroyed, the number depending highly on the prediction gradient of the units removed. These three observations validate the utility of the prediction gradient in identifying important units in a neural network. We finally use the prediction gradient in order to generate and study adversarial examples.","","","10.1109/ICDMW.2015.227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395858","Deep Learning","Biological neural networks;Visualization;Training;Conferences;Computational modeling;Robustness","feature extraction;image classification;neural nets","deep-neural networks;convolutional neural networks;neuron relevance measure;high-prediction-gradient units;classification task;performance loss;low-gradient units;distributed representation","","","17","","","","","IEEE","IEEE Conferences"
"On the use of convolutional neural networks and augmented CSP features for multi-class motor imagery of EEG signals classification","H. Yang; S. Sakhavi; K. K. Ang; C. Guan","Institute for Infocomm Research, Agency for Science, Technology and Research (A*STAR), Singapore 138632; Institute for Infocomm Research, Agency for Science, Technology and Research (A*STAR), Singapore 138632; Institute for Infocomm Research, Agency for Science, Technology and Research (A*STAR), Singapore 138632; Institute for Infocomm Research, Agency for Science, Technology and Research (A*STAR), Singapore 138632","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","2620","2623","Learning the deep structures and unknown correlations is important for the detection of motor imagery of EEG signals (MI-EEG). This study investigates the use of convolutional neural networks (CNNs) for the classification of multi-class MI-EEG signals. Augmented common spatial pattern (ACSP) features are generated based on pair-wise projection matrices, which covers various frequency ranges. We propose a frequency complementary feature map selection (FCMS) scheme by constraining the dependency among frequency bands. Experiments are conducted on BCI competition IV dataset IIa with 9 subjects. Averaged cross-validation accuracy of 68.45% and 69.27% is achieved for FCMS and all feature maps, respectively, which is significantly higher (4.53% and 5.34%) than random map selection and higher (1.44% and 2.26%) than filter-bank CSP (FBCSP). The results demonstrate that the CNNs are capable of learning discriminant, deep structure features for EEG classification without relying on the handcrafted features.","","","10.1109/EMBC.2015.7318929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318929","multi-class motor imagery of EEG;deep learning;convolutional neural network;augmented CSP","Electroencephalography;Convolution;Feature extraction;Neural networks;Accuracy;Brain models","bioelectric potentials;channel bank filters;convolution;electroencephalography;feature selection;learning (artificial intelligence);medical signal detection;medical signal processing;neural nets;neurophysiology;random processes;signal classification","convolutional neural networks;augmented CSP features;multiclass motor imagery;multiclass MI-EEG signal classification;augmented common spatial pattern features;pair-wise projection matrices;frequency complementary feature map selection scheme;BCI competition IV dataset IIa;averaged cross-validation accuracy;random map selection;filter-bank CSP;learning discriminant","Algorithms;Electrodes;Electroencephalography;Humans;Movement;Neural Networks (Computer);Signal Processing, Computer-Assisted","14","13","","","","","IEEE","IEEE Conferences"
"Deep Human Parsing with Active Template Regression","X. Liang; S. Liu; X. Shen; J. Yang; L. Liu; J. Dong; L. Lin; S. Yan","School of Information Science and Technology, Sun Yat-sen University; Institute of Information Engineering, Chinese Academy of Sciences; Adobe Research, San Jose, California; Adobe Research, San Jose, California; Department of Electrical and Computer Engineering, National University of Singapore; Department of Electrical and Computer Engineering, National University of Singapore; School of Advanced Computing, Sun Yat-Sen Unviersity; Department of Electrical and Computer Engineering, National University of Singapore","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2015","37","12","2402","2414","In this work, the human parsing task, namely decomposing a human image into semantic fashion/body regions, is formulated as an active template regression (ATR) problem, where the normalized mask of each fashion/body item is expressed as the linear combination of the learned mask templates, and then morphed to a more precise mask with the active shape parameters, including position, scale and visibility of each semantic region. The mask template coefficients and the active shape parameters together can generate the human parsing results, and are thus called the structure outputs for human parsing. The deep Convolutional Neural Network (CNN) is utilized to build the end-to-end relation between the input human image and the structure outputs for human parsing. More specifically, the structure outputs are predicted by two separate networks. The first CNN network is with max-pooling, and designed to predict the template coefficients for each label mask, while the second CNN network is without max-pooling to preserve sensitivity to label mask position and accurately predict the active shape parameters. For a new image, the structure outputs of the two networks are fused to generate the probability of each label for each pixel, and super-pixel smoothing is finally used to refine the human parsing result. Comprehensive evaluations on a large dataset well demonstrate the significant superiority of the ATR framework over other state-of-the-arts for human parsing. In particular, the F1-score reaches 64.38 percent by our ATR framework, significantly higher than 44.76 percent based on the state-of-the-art algorithm [28].","","","10.1109/TPAMI.2015.2408360","National Natural Science Foundation of China; Microsoft Research Asia collaboration projects, Guangdong Natural Science Foundation; Program of Guangzhou Zhujiang Star of Science and Technology; Special Project on Integration of Industry, Education and Research of Guangdong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7053923","Active Template Regression;CNN;Human Parsing;Active Template Network;Active Shape Network;Active template regression;CNN;human parsing;active template network;active shape network","Shape analysis;Semantics;Feature extraction;Neural networks","image processing;neural nets;regression analysis","deep human parsing;active template regression;human image;semantic fashion-body regions;ATR problem;linear combination;active shape parameters;semantic region;mask template coefficients;convolutional neural network;CNN;end-to-end relation","Algorithms;Computer Simulation;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Machine Learning;Models, Biological;Models, Statistical;Pattern Recognition, Automated;Photography;Regression Analysis;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique;Whole Body Imaging","83","31","","","","","IEEE","IEEE Journals"
"Deep Features for Person Re-identification","S. Wang; Y. Shang; J. Wang; L. Mei; C. Hu","Third Res. Inst. of Minist. of Public Security, Shanghai, China; Third Res. Inst. of Minist. of Public Security, Shanghai, China; Third Res. Inst. of Minist. of Public Security, Shanghai, China; Third Res. Inst. of Minist. of Public Security, Shanghai, China; Third Res. Inst. of Minist. of Public Security, Shanghai, China","2015 11th International Conference on Semantics, Knowledge and Grids (SKG)","","2015","","","244","247","Matching observations captured by pedestrian detectors across the cameras with non-overlapping views, known as person re-identification, is challenging due to the appearance changes caused by pose, viewpoint and illumination variations, occlusions and cluttered background. Different from various hand-crafted features, this paper extract the features through the fine-tuned deep convolutional neural network to measure the similarities between the observations. Our approach significantly outperforms the state-of-the-art methods on the publicly available CUHK03 dataset.","","","10.1109/SKG.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429387","","Feature extraction;Measurement;Training;Cameras;Neural networks;Image color analysis;Machine learning","feature extraction;image matching;neural nets;object detection;pedestrians","deep features;pedestrian detectors;cameras;nonoverlapping views;person re-identification;appearance changes;pose variations;viewpoint variations;illumination variations;occlusions;cluttered background;feature exraction;deep convolutional neural network;pedestrians matching","","3","32","","","","","IEEE","IEEE Conferences"
"Cross-Device Consumer Identification","G. Kejela; C. Rong","NA; NA","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","","2015","","","1687","1689","Nowadays, a typical household owns multiple digital devices that can be connected to the Internet. Advertising companies always want to seamlessly reach consumers behind devices instead of the device itself. However, the identity of consumers becomes fragmented as they switch from one device to another. A naive attempt is to use deterministic features such as user name, telephone number and email address. However consumers might refrain from giving away their personal information because of privacy and security reasons. The challenge in ICDM2015 contest is to develop an accurate probabilistic model for predicting cross-device consumer identity without using the deterministic user information. In this paper we present an accurate and scalable cross-device solution using an ensemble of Gradient Boosting Decision Trees (GBDT) and Random Forest. Our final solution ranks 9<sup>th</sup> both on the public and private LB with F0.5 score of 0.855.","","","10.1109/ICDMW.2015.241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395888","Ensemble;Xgboost;Deep Learning;GBM;Random Forest;ICDM2015 contest","Predictive models;Training;Computers;Computational modeling;Data models;IP networks;Performance evaluation","advertising data processing;consumer behaviour;decision trees;learning (artificial intelligence)","random forest algorithm;GBDT;gradient boosting decision trees;personal information;advertising companies;consumer identity;Internet;cross-device consumer identification","","","4","","","","","IEEE","IEEE Conferences"
"Mixed handwritten and printed digit recognition in Sudoku with Convolutional Deep Belief Network","B. Wicht; J. Henneberty","University of Fribourg, Switzerland, HES-SO, University of Applied Science of Western Switzerland; University of Fribourg, Switzerland, HES-SO, University of Applied Science of Western Switzerland","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","861","865","In this paper, we propose a method to recognize Sudoku puzzles containing both handwritten and printed digits from images taken with a mobile camera. The grid and the digits are detected using various image processing techniques including Hough Transform and Contour Detection. A Convolutional Deep Belief Network is then used to extract high-level features from raw pixels. The features are finally classified using a Support Vector Machine. One of the scientific question addressed here is about the capability of the Deep Belief Network to learn extracting features on mixed inputs, printed and handwritten. The system is thoroughly tested on a set of 200 Sudoku images captured with smartphone cameras under varying conditions, e.g. distortion and shadows. The system shows promising results with 92% of the cells correctly classified. When cell detection errors are not taken into account, the cell recognition accuracy increases to 97.7%. Interestingly, the Deep Belief Network is able to handle the complex conditions often present on images taken with phone cameras and the complexity of mixed printed and handwritten digits.","","","10.1109/ICDAR.2015.7333884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333884","Convolutional Deep Belief Network;Convolution;Text Detection;Text Recognition;Camera-based OCR","","belief networks;handwriting recognition;Hough transforms;image sensors;mobile computing;support vector machines","printed digit recognition;Sudoku;convolutional deep belief network;mixed handwritten recognition;Sudoku puzzles;printed digits;handwritten digits;image processing techniques;Hough Transform;contour detection;support vector machine;Sudoku images;smartphone cameras","","3","18","","","","","IEEE","IEEE Conferences"
"Learning architectures with enhanced capabilities and easier training","B. M. Wilamowski; J. Korniak","Auburn University, AL, USA; Auburn University, AL, USA","2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES)","","2015","","","21","29","Although discovery of the Error Back Propagation (EBP) learning algorithm was a real breakthrough, this is not only a very slow algorithm, but it also is not capable of training networks with super compact architecture. The most noticeable progress was done with an adaptation of the LM algorithm to neural network training. The LM algorithm is capable of training networks with 100 to 1000 fewer iterations, but the size of the problems are significantly limited. Also, the LM algorithm was adopted primarily for traditional MLP architectures. More recently two new revolutionary concepts were developed: Support Vector Machine and Extreme Learning Machines. They are very fast, but they train only shallow networks with one hidden layer. It was shown that these shallow networks have very limited capabilities. It has already demonstrated much higher capabilities of super compact architectures having 10 to 100 times more processing power than commonly used learning architectures For example, such a shallow MLP architecture with 10 neurons can solve only a Parity-9 problem, but a special deep FCC (Fully Connected Cascade) architecture with the same 10 neurons can solve as large a problem as a Parity-1023. Unfortunately, with the vanishing gradient problem), deep architectures are very difficult to train. By introducing additional connections across layers it was possible to efficiently train deep networks using the powerful NBN algorithm. Our early results show that there is a solution for this difficult problem.","","","10.1109/INES.2015.7329714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7329714","","Neurons;Training;Biological neural networks;Computer architecture;Support vector machines;Artificial neural networks;FCC","backpropagation;neural nets;support vector machines","NBN algorithm;fully connected cascade architecture;FCC architecture;extreme learning machine;support vector machine;MLP architecture;neural network training;LM algorithm;EBP learning algorithm;error back propagation learning algorithm;learning architecture","","4","35","","","","","IEEE","IEEE Conferences"
"Asymmetric self-learning for tackling Twitter Spam Drift","C. Chen; J. Zhang; Y. Xiang; W. Zhou","School of Information Technology, Deakin University, Victoria 3125, Australia; School of Information Technology, Deakin University, Victoria 3125, Australia; School of Information Technology, Deakin University, Victoria 3125, Australia; School of Information Technology, Deakin University, Victoria 3125, Australia","2015 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)","","2015","","","208","213","Spam has become a critical problem on Twitter. In order to stop spammers, security companies apply blacklisting services to filter spam links. However, over 90% victims will visit a new malicious link before it is blocked by blacklists. To eliminate the limitation of blacklists, researchers have proposed a number of statistical features based mechanisms, and applied machine learning techniques to detect Twitter spam. In our labelled large dataset, we observe that the statistical properties of spam tweets vary over time, and thus the performance of existing ML based classifiers are poor. This phenomenon is referred as “Twitter Spam Drift”. In order to tackle this problem, we carry out deep analysis of 1 million spam tweets and 1 million non-spam tweets, and propose an asymmetric self-learning (ASL) approach. The proposed ASL can discover new information of changed tweeter spam and incorporate it into classifier training process. A number of experiments are performed to evaluate the ASL approach. The results show that the ASL approach can be used to significantly improve the spam detection accuracy of using traditional ML algorithms.","","","10.1109/INFCOMW.2015.7179386","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7179386","","Twitter;Feature extraction;Unsolicited electronic mail;Training;Market research;Security;Uniform resource locators","learning (artificial intelligence);pattern classification;social networking (online);unsolicited e-mail","asymmetric self-learning;Twitter spam drift;blacklisting services;filter spam links;malicious link;statistical features based mechanisms;machine learning techniques;ML based classifiers;ASL approach;classifier training process;ML algorithms","","11","16","","","","","IEEE","IEEE Conferences"
"Designing deep networks for surface normal estimation","X. Wang; D. F. Fouhey; A. Gupta","Robotics Institute, Carnegie Mellon University, USA; Robotics Institute, Carnegie Mellon University, USA; Robotics Institute, Carnegie Mellon University, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","539","547","In the past few years, convolutional neural nets (CNN) have shown incredible promise for learning visual representations. In this paper, we use CNNs for the task of predicting surface normals from a single image. But what is the right architecture? We propose to build upon the decades of hard work in 3D scene understanding to design a new CNN architecture for the task of surface normal estimation. We show that incorporating several constraints (man-made, Manhattan world) and meaningful intermediate representations (room layout, edge labels) in the architecture leads to state of the art performance on surface normal estimation. We also show that our network is quite robust and show state of the art results on other datasets as well without any fine-tuning.","","","10.1109/CVPR.2015.7298652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298652","","Layout;Estimation;Image edge detection;Convolutional codes;Three-dimensional displays;Neurons;Surface treatment","computer vision;estimation theory;learning (artificial intelligence);neural net architecture","deep networks;surface normal estimation;convolutional neural nets;learning;visual representations;CNN architecture","","84","40","","","","","IEEE","IEEE Conferences"
"A blended learning model to achieve academic excellence in preparing post graduate engineering students to become University teachers","A. Mantri","Chitkara University Research and Innovation Network (CUPJN), Chitkara University, Chandigarh, India","2015 IEEE 3rd International Conference on MOOCs, Innovation and Technology in Education (MITE)","","2015","","","9","14","Blended learning approach has proved to have great potential to support deep and meaningful learning. This paper presents a case study of the transformative potential of blended learning in the context of the challenge facing higher education across the globe - designing learning instructions for inculcating research oriented teaching in future technical teachers. The need to inculcate research at all levels of University education has been long felt and should be addressed by designing the learning instruction in such a way that research becomes an integral part of the culture of teaching. This paper describes the approach to do so in the case study presented here. Some important leadership, administrative, implementation and scalability issues are addressed. Outlines of action plan using cloud based technologies to support blended learning models are also presented. It was observed that by using the described approach, the usual advantages of traditional model were encashed while the learning experience of students was enhanced using on-line approach.","","","10.1109/MITE.2015.7375278","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375278","","Weaving;Seminars","cloud computing;computer aided instruction;educational institutions;engineering computing;engineering education;further education","blended learning model;post graduate engineering students;university teachers;higher education;learning instructions;research oriented teaching;learning instruction;teaching culture;cloud based technologies;student learning experience","","","9","","","","","IEEE","IEEE Conferences"
"Hierarchical Aggregation Based Deep Aging Feature for Age Prediction","J. Qiu; Y. Dai; Y. Zhang; J. M. Alvarez","Res. Sch. of Eng., Australian Nat. Univ., Canberra, ACT, Australia; Res. Sch. of Eng., Australian Nat. Univ., Canberra, ACT, Australia; Chalmers Univ. of Technol., Gothenburg, Sweden; Nat. ICT Australia Ltd., Australia","2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA)","","2015","","","1","5","We propose a new, hierarchical, aggregation-based deep neural network to learn aging features from facial images. Our deep-aging feature vector is designed to capture both local and global aging cues from facial images. A Convolutional Neural Network (CNN) is employed to extract region- specific features at the lowest level of our hierarchy. These features are then hierarchically aggregated to consecutive higher levels and the resultant aging feature vector, of dimensionality 110, achieves both good discriminative ability and efficiency. Experimental results of age prediction on the MORPH-II databases show that our method outperforms state-of-the-art aging features by a clear margin. Experimental trails of our method across race and gender provide further confidence in its performance and robustness.","","","10.1109/DICTA.2015.7371264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371264","","Aging;Feature extraction;Databases;Estimation;Labeling;Training;Australia","face recognition;gender issues","hierarchical aggregation based deep aging feature;age prediction;hierarchical aggregation-based deep neural network;facial images;deep-aging feature vector;local aging cues;global aging cues;convolutional neural network;CNN;resultant aging feature vector;discriminative ability;MORPH-II databases;gender","","1","20","","","","","IEEE","IEEE Conferences"
"Multimodal fusion for sensor data using stacked autoencoders","P. Zhang; X. Ma; W. Zhang; S. Lin; H. Chen; A. L. Yirun; G. Xiao","“Sense and Sense-abilities” Programme, Institute for Infocomm Research, Singapore; “Sense and Sense-abilities” Programme, Institute for Infocomm Research, Singapore; Department of Statistics, Cornell University, New York, United States; “Sense and Sense-abilities” Programme, Institute for Infocomm Research, Singapore; “Sense and Sense-abilities” Programme, Institute for Infocomm Research, Singapore; “Sense and Sense-abilities” Programme, Institute for Infocomm Research, Singapore; School of EEE, Nanyang Technological University, Singapore","2015 IEEE Tenth International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP)","","2015","","","1","2","As sensor networks collect different kinds of data to make better-informed decisions, we need multimodal fusion for basic analytical tasks such as event prediction, error reduction and data compression. Inspired by unsupervised feature discovery methods in deep learning, we propose an approach based on the stacked autoencoder: a multi-layer feed-forward neural network. After extracting key features from multimodal sensor data, the algorithm computes compact representations which can also be used directly in analytical tasks. Using simulated and real-world environmental data, we evaluate the performance of our approach for data fusion and regression. We demonstrate improvements over situations where only single modality data is available and where multimodal data are fused together without learning intermodality correlations. For regression, we attained more than 45% improvement in root-mean-square errors (RMSE) over linear approaches, and up to 10% improvement over shallow neural network methods.","","","10.1109/ISSNIP.2015.7106972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7106972","","Multimodal sensors","data compression;data reduction;feature extraction;feedforward neural nets;regression analysis;sensor fusion;unsupervised learning","multimodal fusion;multimodal sensor data;stacked autoencoders;event prediction;error reduction;data compression;unsupervised feature discovery methods;deep learning;multilayer feedforward neural network;key feature extraction;data fusion;data regression;root-mean-square errors;shallow neural network methods;sensor networks","","2","2","","","","","IEEE","IEEE Conferences"
"Learning from massive noisy labeled data for image classification","Tong Xiao; Tian Xia; Yi Yang; Chang Huang; Xiaogang Wang","The Chinese University of Hong Kong, China; Baidu Research, USA; Baidu Research, USA; Baidu Research, USA; The Chinese University of Hong Kong, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","2691","2699","Large-scale supervised datasets are crucial to train convolutional neural networks (CNNs) for various computer vision problems. However, obtaining a massive amount of well-labeled data is usually very expensive and time consuming. In this paper, we introduce a general framework to train CNNs with only a limited number of clean labels and millions of easily obtained noisy labels. We model the relationships between images, class labels and label noises with a probabilistic graphical model and further integrate it into an end-to-end deep learning system. To demonstrate the effectiveness of our approach, we collect a large-scale real-world clothing classification dataset with both noisy and clean labels. Experiments on this dataset indicate that our approach can better correct the noisy labels and improves the performance of trained CNNs.","","","10.1109/CVPR.2015.7298885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298885","","Noise","clothing;computer vision;convolution;image classification;learning (artificial intelligence);neural nets;probability","image classification;large-scale supervised dataset;convolutional neural network;CNN;computer vision problem;class label;label noise;probabilistic graphical model;end-to-end deep learning system;large-scale real-world clothing classification dataset","","10","31","","","","","IEEE","IEEE Conferences"
"Learning a convolutional neural network for non-uniform motion blur removal","J. Sun; Wenfei Cao; Zongben Xu; J. Ponce","Xi'an Jiaotong University, China; Xi'an Jiaotong University, China; Xi'an Jiaotong University, China; École Normale Supérieure / PSL Research University, France","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","769","777","In this paper, we address the problem of estimating and removing non-uniform motion blur from a single blurry image. We propose a deep learning approach to predicting the probabilistic distribution of motion blur at the patch level using a convolutional neural network (CNN). We further extend the candidate set of motion kernels predicted by the CNN using carefully designed image rotations. A Markov random field model is then used to infer a dense non-uniform motion blur field enforcing motion smoothness. Finally, motion blur is removed by a non-uniform deblurring model using patch-level image prior. Experimental evaluations show that our approach can effectively estimate and remove complex non-uniform motion blur that is not handled well by previous approaches.","","","10.1109/CVPR.2015.7298677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298677","","Kernel;Estimation;Neural networks;Cameras;Markov processes;Predictive models;Neurons","convolution;image motion analysis;image restoration;learning (artificial intelligence);Markov processes;neural nets;probability","convolutional neural network;CNN;nonuniform motion blur removal;deep learning approach;motion blur probabilistic distribution;motion kernels;image rotations;Markov random field model;nonuniform deblurring model;patch-level image prior","","120","32","","","","","IEEE","IEEE Conferences"
"A hybrid convolutional neural networks with extreme learning machine for WCE image classification","J. Yu; J. Chen; Z. Q. Xiang; Y. Zou","ADSPLAB, School of ECE, Peking University, Shenzhen 518055, China; ADSPLAB, School of ECE, Peking University, Shenzhen 518055, China; ADSPLAB, School of ECE, Peking University, Shenzhen 518055, China; ADSPLAB/ELIP, School of ECE, Peking University, Shenzhen 518055, China","2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)","","2015","","","1822","1827","Wireless Capsule Endoscopy (WCE) is considered as a promising technology for non-invasive gastrointestinal disease examination. This paper studies the classification problem of the digestive organs for wireless capsule endoscopy (WCE) images aiming at saving the review time of doctors. Our previous study has proved the Convolutional Neural Networks (CNN)-based WCE classification system is able to achieve 95% classification accuracy in average, but it is difficult to further improve the classification accuracy owing to the variations of individuals and the complex digestive tract circumstance. Research shows that there are two possible approaches to improve classification accuracy: to extract more discriminative image features and to employ a more powerful classifier. In this paper, we propose to design a WCE classification system by a hybrid CNN with Extreme Learning Machine (ELM). In our approach, we construct the CNN as a data-driven feature extractor and the cascaded ELM as a strong classifier instead of the conventional used full-connection classifier in deep CNN classification system. Moreover, to improve the convergence and classification capability of ELM under supervision manner, a new initialization is employed. Our developed WCE image classification system is named as HCNN-NELM. With about 1 million real WCE images (25 examinations), intensive experiments are conducted to evaluate its performance. Results illustrate its superior performance compared to traditional classification methods and conventional CNN-based method, where about 97.25% classification accuracy can be achieved in average.","","","10.1109/ROBIO.2015.7419037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7419037","","Feature extraction;Training;Iron;Image classification;Esophagus;Testing;Endoscopes","diseases;endoscopes;feature extraction;image classification;learning (artificial intelligence);medical image processing;neural nets","hybrid convolutional neural networks;extreme learning machine;WCE image classification system;wireless capsule endoscopy;noninvasive gastrointestinal disease examination;digestive organs;CNN-based WCE classification system;complex digestive tract;discriminative image features extraction;hybrid CNN;data-driven feature extractor;cascaded ELM;full-connection classifier;deep CNN classification system;HCNN-NELM","","14","14","","","","","IEEE","IEEE Conferences"
"Indoor localization by denoising autoencoders and semi-supervised learning in 3D simulated environment","A. Shantia; R. Timmers; L. Schomaker; M. Wiering","Institute of Artificial Intelligence and Cognitive Engineering, University of Groningen, The Netherlands; Institute of Artificial Intelligence and Cognitive Engineering, University of Groningen, The Netherlands; Institute of Artificial Intelligence and Cognitive Engineering, University of Groningen, The Netherlands; Institute of Artificial Intelligence and Cognitive Engineering, University of Groningen, The Netherlands","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","7","Robotic mapping and localization methods are mostly dominated by using a combination of spatial alignment of sensory inputs, loop closure detection, and a global fine-tuning step. This requires either expensive depth sensing systems, or fast computational hardware at run-time to produce a 2D or 3D map of the environment. In a similar context, deep neural networks are used extensively in scene recognition applications, but are not yet applied to localization and mapping problems. In this paper, we adopt a novel approach by using denoising autoencoders and image information for tackling robot localization problems. We use semi-supervised learning with location values that are provided by traditional mapping methods. After training, our method requires much less run-time computations, and therefore can perform real-time localization on normal processing units. We compare the effects of different feature vectors such as plain images, the scale invariant feature transform and histograms of oriented gradients on the localization precision. The best system can localize with an average positional error of ten centimeters and an angular error of four degrees in 3D simulation.","","","10.1109/IJCNN.2015.7280715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280715","","Training;Gray-scale;Image storage;Image resolution;Neural networks;Weaving","gradient methods;image denoising;indoor navigation;learning (artificial intelligence);neural nets;robots;transforms","indoor localization;denoising autoencoder;semisupervised learning;3D simulated environment;robotic mapping;localization method;spatial alignment;sensory input;loop closure detection;global fine-tuning step;expensive depth sensing system;computational hardware;deep neural network;scene recognition application;image information;robot localization problem;traditional mapping method;run-time computation;real-time localization;normal processing unit;feature vector;scale invariant feature transform;histograms of oriented gradient;localization precision;positional error;ten centimeter;3D simulation","","2","30","","","","","IEEE","IEEE Conferences"
"Efficient O(1) edge-aware filter","M. Zhou; Z. Liu; T. Hong; X. Wang; H. Wang; X. Sun","Samsung Research Center-Beijing, SAIT China Lab Beijing, China; Samsung Research Center-Beijing, SAIT China Lab Beijing, China; Samsung Research Center-Beijing, SAIT China Lab Beijing, China; Samsung Research Center-Beijing, SAIT China Lab Beijing, China; Samsung Research Center-Beijing, SAIT China Lab Beijing, China; Baidu Institute of Deep Learning, Beijing, China","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","1295","1299","In this paper, we propose an exact and efficient O(1) edge-aware filter. Starting from the generic 1D weighted average filter, by enforcing two assumptions on the filter kernel, we derive the fast O(1) algorithm to exactly implement the filter by two pass recursions. Performing the 1D filter horizontally and vertically can easily extend it to 2D case. A typical real valued geodesic kernel and a novel complex valued bilateral-like kernel are investigated in our proposed filtering framework. Experiments show that both kernels work well for the edge-aware smoothing task, and the bilateral-like kernel based filter shows the similar ability as the guided filter in the image feathering application.","","","10.1109/ICIP.2015.7351009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351009","Geodesic distance;bilateral filter;edge-aware smoothing;image feathering","Kernel;Image edge detection;Smoothing methods;Transforms;Filtering;Computer vision;Image color analysis","computational complexity;image filtering","O(1) edge-aware filter;generic 1D weighted average filter;O(1) algorithm;image feathering application","","1","16","","","","","IEEE","IEEE Conferences"
"Human intention understanding based on object affordance and action classification","Z. Yu; S. Kim; R. Mallipeddi; M. Lee","School of Electronics Engineering, Kyungpook National University, Taegu, Korea; School of Electronics Engineering, Kyungpook National University, Taegu, Korea; School of Electronics Engineering, Kyungpook National University, Taegu, Korea; School of Electronics Engineering, Kyungpook National University, Taegu, Korea","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","6","Intention understanding is a basic requirement for human-machine interaction. Action classification and object affordance recognition are two possible ways to understand human intention. In this study, Multiple Timescale Recurrent Neural Network (MTRNN) is adapted to analyze human action. Supervised MTRNN, which is an extension of Continuous Timescale Recurrent Neural Network (CTRNN), is used for action and intention classification. On the other hand, deep learning algorithms proved to be efficient in understanding complex concepts in complex real world environment. Stacked denoising auto-encoder (SDA) is used to extract human implicit intention related information from the observed objects. A feature based object detection method namely Speeded Up Robust Features (SURF) is also used to find the object information. Object affordance describes the interactions between agent and the environment. In this paper, we propose an intention recognition system using `action classification' and `object affordance information'. Experimental result shows that supervised MTRNN is able to use different information in different time period and improve the intention recognition rate by cooperating with the SDA.","","","10.1109/IJCNN.2015.7280587","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280587","Supervised learning;Object affordance;Action classification;Intention understanding","Man machine systems;Robustness;Robots;Biological system modeling;Artificial neural networks;Nickel","feature extraction;image classification;image denoising;learning (artificial intelligence);object detection;object recognition;recurrent neural nets","action classification;human intention understanding;human-machine interaction;object affordance recognition;multiple timescale recurrent neural network;supervised MTRNN;continuous timescale recurrent neural network;CTRNN;intention classification;deep learning algorithms;complex real world environment;stacked denoising auto-encoder;SDA;human implicit intention related information extraction;feature based object detection method;speeded up robust features;SURF;intention recognition system;object affordance information;supervised learning","","2","19","","","","","IEEE","IEEE Conferences"
"Recognizing Actions Through Action-Specific Person Detection","F. S. Khan; J. Xu; J. van de Weijer; A. D. Bagdanov; R. M. Anwer; A. M. Lopez","Department of Electrical EngineeringComputer Vision Laboratory, Linköping University, Linköping, Sweden; Computer Vision Centre Barcelona, Barcelona, Spain; Computer Vision Centre Barcelona, Barcelona, Spain; Computer Vision Centre Barcelona, Barcelona, Spain; Department of Information and Computer Science, Aalto University School of Science, Aalto, Finland; Computer Vision Centre Barcelona, Barcelona, Spain","IEEE Transactions on Image Processing","","2015","24","11","4422","4432","Action recognition in still images is a challenging problem in computer vision. To facilitate comparative evaluation independently of person detection, the standard evaluation protocol for action recognition uses an oracle person detector to obtain perfect bounding box information at both training and test time. The assumption is that, in practice, a general person detector will provide candidate bounding boxes for action recognition. In this paper, we argue that this paradigm is suboptimal and that action class labels should already be considered during the detection stage. Motivated by the observation that body pose is strongly conditioned on action class, we show that: 1) the existing state-of-the-art generic person detectors are not adequate for proposing candidate bounding boxes for action classification; 2) due to limited training examples, the direct training of action-specific person detectors is also inadequate; and 3) using only a small number of labeled action examples, the transfer learning is able to adapt an existing detector to propose higher quality bounding boxes for subsequent action classification. To the best of our knowledge, we are the first to investigate transfer learning for the task of action-specific person detection in still images. We perform extensive experiments on two benchmark data sets: 1) Stanford-40 and 2) PASCAL VOC 2012. For the action detection task (i.e., both person localization and classification of the action performed), our approach outperforms methods based on general person detection by 5.7% mean average precision (MAP) on Stanford-40 and 2.1% MAP on PASCAL VOC 2012. Our approach also significantly outperforms the state of the art with a MAP of 45.4% on Stanford-40 and 31.4% on PASCAL VOC 2012. We also evaluate our action detection approach for the task of action classification (i.e., recognizing actions without localizing them). For this task, our approach, without using any ground-truth person localization at test time, outperforms on both data sets state-of-the-art methods, which do use person locations.","","","10.1109/TIP.2015.2465147","Svalbard Science Forum through the Collaborative Unmanned Aircraft Systems Project; VR through the ETT Project and through the Strategic Area for ICT Research ELLIIT, CADICS; Academy of Finland; Data to Intelligence DIGILE SHOK Project; Spanish Morocco Economic Competitiveness Project; Spanish Ministry of Science through the Spanish DGT Project; Generalitat de Catalunya Project; MICINN through Ramon y Cajal Fellowship; Chinese Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180357","Action Recognition;Transfer Learning;Deep features;Action recognition;transfer learning;deep features","Detectors;Training;Proposals;Adaptation models;Image recognition;Standards;Feature extraction","computer vision;image recognition;protocols","action recognition;action-specific person detection;computer vision;standard evaluation protocol;action class labels;action classification;limited training examples;bounding boxes;Stanford-40;PASCAL VOC 2012;person localization;person classification;mean average precision","","17","43","","","","","IEEE","IEEE Journals"
"Evaluation of deep convolutional nets for document image classification and retrieval","A. W. Harley; A. Ufkes; K. G. Derpanis","Department of Computer Science, Ryerson University, Toronto, Ontario, Canada; Department of Computer Science, Ryerson University, Toronto, Ontario, Canada; Department of Computer Science, Ryerson University, Toronto, Ontario, Canada","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","991","995","This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular handcrafted alternatives. Extensive experiments show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories.","","","10.1109/ICDAR.2015.7333910","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333910","","Image coding;Radio frequency;Principal component analysis;Yttrium;Libraries","document image processing;image classification;image retrieval;neural nets","deep convolutional nets evaluation;document image classification;document image retrieval;deep convolutional neural networks;CNN;pixel inputs;descriptive representations;representation strategy;nondocument images;document analysis tasks","","29","23","","","","","IEEE","IEEE Conferences"
"Second-order constrained parametric proposals and sequential search-based structured prediction for semantic segmentation in RGB-D images","D. Banica; C. Sminchisescu","Institute of Mathematics of the Romanian Academy, Romania; Lund University, Sweden","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3517","3526","We focus on the problem of semantic segmentation based on RGB-D data, with emphasis on analyzing cluttered indoor scenes containing many visual categories and instances. Our approach is based on a parametric figureground intensity and depth-constrained proposal process that generates spatial layout hypotheses at multiple locations and scales in the image followed by a sequential inference algorithm that produces a complete scene estimate. Our contributions can be summarized as follows: (1) a generalization of parametric max flow figure-ground proposal methodology to take advantage of intensity and depth information, in order to systematically and efficiently generate the breakpoints of an underlying spatial model in polynomial time, (2) new region description methods based on second-order pooling over multiple features constructed using both intensity and depth channels, (3) a principled search-based structured prediction inference and learning process that resolves conflicts in overlapping spatial partitions and selects regions sequentially towards complete scene estimates, and (4) extensive evaluation of the impact of depth, as well as the effectiveness of a large number of descriptors, both pre-designed and automatically obtained using deep learning, in a difficult RGB-D semantic segmentation problem with 92 classes. We report state of the art results in the challenging NYU Depth Dataset V2 [44], extended for the RMRC 2013 and RMRC 2014 Indoor Segmentation Challenges, where currently the proposed model ranks first. Moreover, we show that by combining second-order and deep learning features, over 15% relative accuracy improvements can be additionally achieved. In a scene classification benchmark, our methodology further improves the state of the art by 24%.","","","10.1109/CVPR.2015.7298974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298974","","Image segmentation;Proposals;Feature extraction;Three-dimensional displays;Semantics;Predictive models;Training","image classification;image colour analysis;image segmentation;inference mechanisms;learning (artificial intelligence)","second-order constrained parametric proposals;sequential search-based structured prediction;semantic segmentation;RGB-D image segmentation;red-green-blue-depth image;visual category;visual instance;parametric figure-ground intensity;image location;image scales;sequential inference algorithm;parametric max flow figure-ground proposal methodology;intensity information;depth information;region description methods;principled search-based structured prediction inference process;learning process;NYU Depth Dataset;scene classification benchmark;second-order learning;deep learning","","22","49","","","","","IEEE","IEEE Conferences"
"Speaker diarization through speaker embeddings","M. Rouvier; P. Bousquet; B. Favre","Aix-Marseille Université, CNRS, LIF UMR 7279, 13000, Marseille, France; Avignon Universite, LIA, 84000, Avignon, France; Aix-Marseille Université, CNRS, LIF UMR 7279, 13000, Marseille, France","2015 23rd European Signal Processing Conference (EUSIPCO)","","2015","","","2082","2086","This paper proposes to learn a set of high-level feature representations through deep learning, referred to as Speaker Embeddings, for speaker diarization. Speaker Embedding features are taken from the hidden layer neuron activations of Deep Neural Networks (DNN), when learned as classifiers to recognize a thousand speaker identities in a training set. Although learned through identification, speaker embeddings are shown to be effective for speaker verification in particular to recognize speakers unseen in the training set. In particular, this approach is applied to speaker diarization. Experiments, conducted on the corpus of French broadcast news ETAPE, show that this new speaker modeling technique decreases DER by 1.67 points (a relative improvement of about 8% DER).","","","10.1109/EUSIPCO.2015.7362751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362751","Speaker Diarization;Deep Neural Network;Speaker Embeddings;Speaker Clustering;i-vector","Training;Density estimation robust algorithm;Speech;Neurons;Feature extraction;Europe;Signal processing","neural nets;signal representation;speaker recognition","speaker verification;speaker identities;DNN;deep neural networks;hidden layer neuron activations;speaker diarization;speaker embeddings;deep learning;high-level feature representations","","9","20","","","","","IEEE","IEEE Conferences"
"Spatial-Aware Object-Level Saliency Prediction by Learning Graphlet Hierarchies","L. Zhang; Y. Xia; R. Ji; X. Li","Dept. of Comput. Sci., Zhejiang Univ., Hangzhou, China; Dept. of Comput. Sci., Zhejiang Univ., Hangzhou, China; Dept. of Cognitive Sci., Xiamen Univ., Xiamen, China; Center for Opt. IMagery Anal. & Learning, Xi'an Inst. of Opt. & Precision Mech., Xi'an, China","IEEE Transactions on Industrial Electronics","","2015","62","2","1301","1308","To fill the semantic gap between the predictive power of computational saliency models and human behavior, this paper proposes to predict where people look at using spatial-aware object-level cues. While object-level saliency has been recently suggested by psychophysics experiments and shown effective with a few computational models, the spatial relationship between the objects has not yet been explored in this context. We in this work for the first time explicitly model such spatial relationship, as well as leveraging semantic information of an image to enhance object-level saliency modeling. The core computational module is a graphlet-based (i.e., graphlets are moderate-sized connected subgraphs) deep architecture, which hierarchically learns a saliency map from raw image pixels to object-level graphlets (oGLs) and further to spatial-level graphlets (sGLs). Eye tracking data are also used to leverage human experience in saliency prediction. Experimental results demonstrate that the proposed oGLs and sGLs well capture object-level and spatial-level cues relating to saliency, and the resulting saliency model performs competitively compared with the state-of-the-art.","","","10.1109/TIE.2014.2336602","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Key Science and Technology Program of Zhejiang Province of China; 985 Project of Xiamen University; Zhejiang Provincial Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6850010","Eye tracking;graphlet;object-level;saliency;spatial","Semantics;Visualization;Computational modeling;Layout;Probabilistic logic;Computer architecture;Vectors","gaze tracking;image enhancement;learning (artificial intelligence)","spatial-aware object-level saliency prediction;graphlet hierarchies;semantic gap;computational saliency models;spatial-aware object-level cues;semantic information;object-level saliency modeling;object-level graphlets;oGL;spatial-level graphlets;sGL;eye tracking data","","73","55","","","","","IEEE","IEEE Journals"
"Stacked Multilayer Self-Organizing Map for Background Modeling","Z. Zhao; X. Zhang; Y. Fang","Institute of Robotics and Automatic Information System, Nankai University, Tianjin, China; Institute of Robotics and Automatic Information System, Nankai University, Tianjin, China; Institute of Robotics and Automatic Information System, Nankai University, Tianjin, China","IEEE Transactions on Image Processing","","2015","24","9","2841","2850","In this paper, a new background modeling method called stacked multilayer self-organizing map background model (SMSOM-BM) is proposed, which presents several merits such as strong representative ability for complex scenarios, easy to use, and so on. In order to enhance the representative ability of the background model and make the parameters learned automatically, the recently developed idea of representative learning (or deep learning) is elegantly employed to extend the existing single-layer self-organizing map background model to a multilayer one (namely, the proposed SMSOM-BM). As a consequence, the SMSOM-BM gains several merits including strong representative ability to learn background model of challenging scenarios, and automatic determination for most network parameters. More specifically, every pixel is modeled by a SMSOM, and spatial consistency is considered at each layer. By introducing a novel over-layer filtering process, we can train the background model layer by layer in an efficient manner. Furthermore, for real-time performance consideration, we have implemented the proposed method using NVIDIA CUDA platform. Comparative experimental results show superior performance of the proposed approach.","","","10.1109/TIP.2015.2427519","Natural Science Foundation of Tianjin City; Specialized Research Fund for the Doctoral Program of Higher Education of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7097028","Background modeling;representative learning;self-organizing map;Background modeling;representative learning;self-organizing map","Training;Computational modeling;Adaptation models;Data models;Training data;Maintenance engineering;Biological neural networks","image filtering;learning (artificial intelligence);motion estimation;parallel architectures;self-organising feature maps","stacked multilayer self-organizing map;background modeling method;representative learning;SMSOM-BM;automatic network parameter determination;spatial consistency;overlayer filtering process;NVIDIA CUDA;motion detection","","16","46","","","","","IEEE","IEEE Journals"
"A hypothesize-and-verify framework for text recognition using deep recurrent neural networks","A. Ray; S. Rajeswar; S. Chaudhury","Department of Electrical Engineering, Indian Institute of Technology Delhi, India; Department of Electrical Engineering, Indian Institute of Technology Delhi, India; Department of Electrical Engineering, Indian Institute of Technology Delhi, India","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","936","940","Deep LSTM is an ideal candidate for text recognition. However text recognition involves some initial image processing steps like segmentation of lines and words which can induce error to the recognition system. Without segmentation, learning very long range context is difficult and becomes computationally intractable. Therefore, alternative soft decisions are needed at the pre-processing level. This paper proposes a hybrid text recognizer using a deep recurrent neural network with multiple layers of abstraction and long range context along with a language model to verify the performance of the deep neural network. In this paper we construct a multi-hypotheses tree architecture with candidate segments of line sequences from different segmentation algorithms at its different branches. The deep neural network is trained on perfectly segmented data and tests each of the candidate segments, generating unicode sequences. In the verification step, these unicode sequences are validated using a sub-string match with the language model and best first search is used to find the best possible combination of alternative hypothesis from the tree structure. Thus the verification framework using language models eliminates wrong segmentation outputs and filters recognition errors.","","","10.1109/ICDAR.2015.7333899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333899","","Transforms;Classification algorithms;Training","image matching;image segmentation;image sequences;recurrent neural nets;text detection;tree searching","hypothesize-and-verify framework;hybrid text recognizer;deep recurrent neural network;abstraction;language model;multihypotheses tree architecture;line sequences;segmentation algorithms;unicode sequences;sub-string match;best first search;recognition errors;deep BLSTM;bidirectional long short term memory","","4","17","","","","","IEEE","IEEE Conferences"
"Sparse Autoencoder for Facial Expression Recognition","B. Huang; Z. Ying","Sch. of Inf. Eng., Wuyi Univ., Jiangmen, China; Sch. of Inf. Eng., Wuyi Univ., Jiangmen, China","2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)","","2015","","","1529","1532","Facial expression recognition has become one of the most interesting topics in human computer interaction. A lot of methods have been proposed and studied for facial expression recognition. Among some of these methods, feature extraction is very important. However, feature extraction in these methods involves human intervention more or less which may make the recognition task unstable and not suitable for practical application. To overcome these problems, a deep learning method for facial expression recognition is proposed. First, it divides the training images to 7 groups correspond to 7 expressions to train 7 sparse auto encoder networks. Then, it lets each training group gets thought the network that have been trained by this group, and each training image gets 150 outputs as the training features. In testing step, a testing image is input into the 7 trained networks and gets 150 outputs as the testing features of each network. Comparing the square of error between testing features with the training features in each network, and classifying the testing image into the class with the minimum error. Experiments on JAFFE database has proved the effectiveness of the proposed method.","","","10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518455","Facial expression;sparse autoencoder","Feature extraction;Training;Testing;Face recognition;Face;Databases","face recognition;feature extraction;image classification;image coding;learning (artificial intelligence)","facial expression recognition;human computer interaction;feature extraction;deep learning method;training images;sparse autoencoder networks;JAFFE database;testing image classification","","5","13","","","","","IEEE","IEEE Conferences"
"Denoising Convolutional Neural Network","Q. Xu; C. Zhang; L. Zhang","Scholl of mechanical, electrical & engineering, Shandong University(Weihai), 264209, China; Scholl of mechanical, electrical & engineering, Shandong University(Weihai), 264209, China; Scholl of mechanical, electrical & engineering, Shandong University(Weihai), 264209, China","2015 IEEE International Conference on Information and Automation","","2015","","","1184","1187","Convolutional Neural Network (CNN) is a kind of deep artificial neural network. CNN has kinds of merits, such as multidimensional data input, and fewer parameters. However, the network always has the problem of overfitting due to lots of connection in the full connection layer. In order to overcome the overfitting problem, the denoising method is used to corrupt input data and hidden unit output which will enforce the network learning a better feature representations of the sample data. In the simulation, some situations are considered, such as input data corruption and hidden unit output corruption, and a comparison is exhibited.","","","10.1109/ICInfA.2015.7279466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279466","Convolutional Neural Network;Denoising;Stochastic selection","Noise reduction;Kernel;Feature extraction;Artificial neural networks;Data models;Image recognition","convolution;image denoising;image representation;learning (artificial intelligence);neural nets","convolutional neural network denoising;CNN denoising;deep artificial neural network;network learning;feature representation","","2","6","","","","","IEEE","IEEE Conferences"
"Development of a jellyfish reconnaissance and removal robot system using unmanned aerial and surface vehicles","D. Kim; H. Kim; Sungwook Jung; Jungmo Koo; Jongheon Kim; J. Shin; H. Myung","URL (Urban Robotics Lab.), KAIST (Korea Advanced Institute of Science and Technology), 291 Daehak-ro (373-1 Guseong-dong), Yuseong-gu, Daejeon 34141, Korea; Rastech Inc., 964 Tamnip-dong, Yuseong-gu, Daejeon, 305-510, Korea; URL (Urban Robotics Lab.), KAIST (Korea Advanced Institute of Science and Technology), 291 Daehak-ro (373-1 Guseong-dong), Yuseong-gu, Daejeon 34141, Korea; URL (Urban Robotics Lab.), KAIST (Korea Advanced Institute of Science and Technology), 291 Daehak-ro (373-1 Guseong-dong), Yuseong-gu, Daejeon 34141, Korea; URL (Urban Robotics Lab.), KAIST (Korea Advanced Institute of Science and Technology), 291 Daehak-ro (373-1 Guseong-dong), Yuseong-gu, Daejeon 34141, Korea; Rastech Inc., 964 Tamnip-dong, Yuseong-gu, Daejeon, 305-510, Korea; URL (Urban Robotics Lab.), KAIST (Korea Advanced Institute of Science and Technology), 291 Daehak-ro (373-1 Guseong-dong), Yuseong-gu, Daejeon 34141, Korea","2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","","2015","","","101","101","Summary form only given. In this paper, a novel robot system for a jellyfish reconnaissance and removal using unmanned aerial vehicle (UAV) and unmanned surface vehicle (USV) is introduced. In recent years, overpopulated jellyfish are threatening ocean ecosystem as well as damaging to ocean-related industries such as fishery, seaside power plants, and casualties. The damage is estimated about 300M USD per year in South Korea. To resolve this problem, a jellyfish removal net towed by trawl boats and a few of devices to prevent the influx of jellyfish have been developed. However, most of these devices have the limited usage due to environmental constraints. To solve this problem using a robotic system, the jellyfish removal robot system, named JEROS (Jellyfish Elimination RObotic Swarm), has been proposed [1]. The JEROS consists of a USV and a jellyfish remover device. The USV can perform a jellyfish removal task using an autonomous navigation system. The jellyfish remover device is originally designed to shred jellyfish. It is additionally designed using a conveyor device to remove venomous jellyfish without shredding. JEROS is extended to a swarm robot system to enhance the performance of jellyfish removal [2]. The swarm robot system performs the jellyfish removal task while maintaining a formation. The UAV-type jellyfish reconnaissance system is proposed. The UAV flies over the sea along a coverage path to search a swarm of jellyfish. A camera mounted on the UAV takes images of the sea surface and a jellyfish recognition algorithm based on a deep learning finds distribution of jellyfish using the taken images. The jellyfish removal task is performed by JEROS in combination with the reconnaissance system. The reconnaissance system recognizes the distribution of jellyfish and transmits the position of the distribution to the JEROS system. The JEROS system generates a path from a starting position to the received region and follows the path. On arriving, the JEROS system remove jellyfish using the jellyfish remover device while following a coverage path.","","","10.1109/URAI.2015.7358971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358971","Jellyfish removal;jellyfish reconnaissance;unmanned surface vehicle;unmanned aerial vehicle;autonomous navigation;object recognition","Reconnaissance;Sea surface;Vehicles;Service robots;Performance evaluation;Electronic mail","autonomous aerial vehicles;environmental factors;learning (artificial intelligence);marine control;multi-robot systems;object recognition;path planning;robot vision;swarm intelligence","jellyfish reconnaissance and removal robot system;unmanned aerial vehicles;unmanned surface vehicles;JEROS;jellyfish elimination robotic swarm;autonomous navigation system;conveyor device;venomous jellyfish removal;swarm robot system;UAV-type jellyfish reconnaissance system;jellyfish recognition algorithm;deep learning","","","2","","","","","IEEE","IEEE Conferences"
"Semantic description of a video using representative frames","I. Jindal; S. Raman","Electrical Engineering, Indian Institute of Technology Gandhinagar, India; Electrical Engineering, Indian Institute of Technology Gandhinagar, India","2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)","","2015","","","1","4","Analysis of a very long video and semantically describe the contents is a challenging task in computer vision. The present approaches such as video shot detection and summarization address this problem partially while maintaining the temporal coherency. To reduce the user efforts for seeing the whole video we have introduced a new technique which combines similar content irrespective of their presence at different time instants. In this approach, we automatically identify only the representative frames corresponding to similar scenes which were captured at different instants of time. We also provide the labels of the objects that are present in the representative frames along with the compact representation for the video. We achieve the task of semantic labelling of frames in a unified framework using a deep learning framework involving pre-trained features through a convolutional neural network. We show that the proposed approach is able to address the semantic labelling effectively as justified by the results obtained for videos of different scenes captured through different modalities.","","","10.1109/NCVPRIPG.2015.7490054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490054","","Semantics;Histograms;Cameras;YouTube;Surveillance;Feature extraction;Labeling","feedforward neural nets;learning (artificial intelligence);object detection;video signal processing","representative frames;semantic video description;computer vision;video shot detection;video shot summarization;temporal coherency;time instants;semantic frame labelling;deep learning framework;pretrained features;convolutional neural network","","","33","","","","","IEEE","IEEE Conferences"
"Optimization Approach to Depot Location in Car Sharing Systems with Big Data","X. Zhu; J. Li; Z. Liu; F. Yang","State Key Lab. of Networking & Switching Technol., Beijing Univ. of Posts & Telecommun., Beijing, China; State Key Lab. of Networking & Switching Technol., Beijing Univ. of Posts & Telecommun., Beijing, China; State Key Lab. of Networking & Switching Technol., Beijing Univ. of Posts & Telecommun., Beijing, China; State Key Lab. of Networking & Switching Technol., Beijing Univ. of Posts & Telecommun., Beijing, China","2015 IEEE International Congress on Big Data","","2015","","","335","342","Determining the location of depots of car sharing systems is a fundamental problem in car sharing systems. Existing methods to determine the location of depots mainly use qualitative method and do not take real demand into account. This paper proposes a novel optimization approach to determine the depot location in car sharing systems scientifically. To predict the car sharing demand accurately, we propose a deep learning approach which has been implemented as a stacked auto-encoder (SAE) model at the bottom with a logistic regression layer at the top. The SAE model is employed for unsupervised feature learning, which has been proved to be effective. Meanwhile the spatial and temporal correlations is considered inherently in the prediction model. The results allow us to determine the location of depots scientifically. Experiments on the datasets illustrate that the proposed model for car sharing demand prediction has superior performance.","","","10.1109/BigDataCongress.2015.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207241","car sharing;depots location;deep learning;stacked auto-encoders;car sharing demand prediction;optimization","Vehicles;Global Positioning System;Trajectory;Semantics;Predictive models;Correlation","automobiles;Big Data;learning (artificial intelligence);regression analysis;traffic information systems","depot location;car sharing systems;big data;optimization approach;stacked auto-encoder model;SAE;logistic regression layer;unsupervised feature learning;car sharing demand prediction","","2","17","","","","","IEEE","IEEE Conferences"
"An exploratory study on the use of convolutional neural networks for object grasp classification","G. Ghazaei; A. Alameer; P. Degenaar; G. Morgan; K. Nazarpour","School of Electrical and Electronic Engineering, Newcastle University, Newcastle NE1 7RU, UK; School of Electrical and Electronic Engineering, Newcastle University, Newcastle NE1 7RU, UK; School of Electrical and Electronic Engineering, Newcastle University, Newcastle NE1 7RU, UK; School of Computing Science, Newcastle University, Newcastle NE1 7RU, UK; School of Electrical and Electronic Engineering, Newcastle University, Newcastle NE1 7RU, UK","2nd IET International Conference on Intelligent Signal Processing 2015 (ISP)","","2015","","","1","5","The loss of hand profoundly affects an individual's quality of life. Prosthetic hands can provide a route to functional rehabilitation by allowing the amputees to undertake their daily activities. However, the performance of current artificial hands falls well short of the dexterity that natural hands offer. The aim of this study is to test whether an intelligent vision system could be used to enhance the grip functionality of prosthetic hands. To this end, a convolutional neural network (CNN) deep learning architecture was implemented to classify the objects in the COIL100 database in four basic grasp groups: tripod, pinch, palmar and palmar with wrist rotation. Our preliminary, yet promising, results suggest that the additional machine vision system can provide prosthetic hands with the ability to detect object and propose the user an appropriate grasp.","","","10.1049/cp.2015.1760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745760","Artificial vision;Convolutional neural network (CNN);Hand prosthesis;Object recognition","","computer vision;image classification;learning (artificial intelligence);neural net architecture;object recognition;prosthetics","convolutional neural networks;object grasp classification;prosthetic hands;functional rehabilitation;natural hand dexterity;intelligent vision system;grip functionality enhancement;CNN-deep learning architecture;COIL100 database;tripod group;pinch group;palmar group;wrist rotation;machine vision system;object detection","","2","","","","","","IET","IET Conferences"
"Towards reduction of the training and search running time complexities for non-rigid object segmentation","J. C. Nascimento; G. Carneiro","Instituto de Sistemas e Robótica, Instituto Superior Técnico, 1049-001 Lisboa, Portugal; Australian Centre for Visual Technologies, The University of Adelaide, Australia","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","4713","4717","The problem of non-rigid object segmentation is formulated in a two-stage approach in Machine Learning based methodologies. In the first stage, the automatic initialization problem is solved by the estimation of a rigid shape of the object. In the second stage, the non-rigid segmentation is performed. The rational behind this strategy, is that the rigid detection can be performed at lower dimensional space than the original contour space. In this paper, we explore this idea and propose the use of manifolds to reduce even more the dimensionality of the rigid transformation space (first stage) of current state-of-the-art top-down segmentation methodologies. Also, we propose the use of deep belief networks to allow for a training process capable to produce robust appearance models. Experiments in lips segmentation from frontal face images are conducted to testify the performance of the proposed algorithm.","","","10.1109/ICIP.2015.7351701","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351701","","Training;Manifolds;Image segmentation;Training data;Complexity theory;Visualization;Robustness","belief networks;face recognition;image segmentation;learning (artificial intelligence);object detection","frontal face images;lips segmentation;robust appearance models;training process;deep belief networks;rigid transformation space;contour space;rigid detection;automatic initialization problem;machine learning based methodologies;nonrigid object segmentation;search running time complexities","","","10","","","","","IEEE","IEEE Conferences"
"What can We Learn from the Last 100 SMPTE Years? What will Tell Us about the Next 10?","G. Hosier","EyeExplore Films, Eastgate Av, Killara, NSW, 2071, Australia","SMPTE15: Persistence of Vision - Defining the Future","","2015","","","1","7","SMPTE celebrates its 100th Anniversary and a lot has changed in those years. B&W film, colour film, television with its evolving quality, computing, Internet, handheld computing, 3D, Smart TV's and the emerging technologies of data-mining with Deep Data Analysis. Does the history of these ideas give us a clue as to what the next big thrust is. Perhaps we should not only look through the front door but also the back door. Be surprised.","","","10.5594/M001601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398362","History;Big Data;DDN;plenoptic photography;Near Field photography;Artificial intelligence;NextVR;360 cameras;robot;picture recognition;facial recognition;voice recognition;smart TV;Lytro;ambient intelligence;Google;B&W Film;colour film;television;computing;Internet;handheld computing;3D;Smart TV;data-mining;Deep Data Analysis;future;robots;cloud;history;Futurama","","","","","","10","","","","","SMPTE","SMPTE Conferences"
"Learning Social Relation Traits from Face Images","Z. Zhang; P. Luo; C. Loy; X. Tang","Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","3631","3639","Social relation defines the association, e.g., warm, friendliness, and dominance, between two or more people. Motivated by psychological studies, we investigate if such fine grained and high-level relation traits can be characterised and quantified from face images in the wild. To address this challenging problem we propose a deep model that learns a rich face representation to capture gender, expression, head pose, and age-related attributes, and then performs pairwise-face reasoning for relation prediction. To learn from heterogeneous attribute sources, we formulate a new network architecture with a bridging layer to leverage the inherent correspondences among these datasets. It can also cope with missing target attribute labels. Extensive experiments show that our approach is effective for fine-grained social relation learning in images and videos.","","","10.1109/ICCV.2015.414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410771","","Face;Psychology;Computer vision;Face recognition;Cognition;Videos","face recognition;image representation;psychology;social sciences computing","social relation trait learning;face images;psychological studies;high-level relation traits;deep model;face representation;pairwise-face reasoning;relation prediction;heterogeneous attribute sources;network architecture;fine-grained social relation learning","","29","43","","","","","IEEE","IEEE Conferences"
"BubbLeNet: Foveated Imaging for Visual Discovery","K. Matzen; N. Snavely","Cornell Univ., Ithaca, NY, USA; Cornell Univ., Ithaca, NY, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1931","1939","We propose a new method for turning an Internet-scale corpus of categorized images into a small set of human-interpretable discriminative visual elements using powerful tools based on deep learning. A key challenge with deep learning methods is generating human-interpretable models. To address this, we propose a new technique that uses bubble images -- images where most of the content has been obscured -- to identify spatially localized, discriminative content in each image. By modifying the model training procedure to use both the source imagery and these bubble images, we can arrive at final models which retain much of the original classification performance, but are much more amenable to identifying interpretable visual elements. We apply our algorithm to a wide variety of datasets, including two new Internet-scale datasets of people and places, and show applications to visual mining and discovery. Our method is simple, scalable, and produces visual elements that are highly representative compared to prior work.","","","10.1109/ICCV.2015.224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410581","","Visualization;Training;Neurons;Machine learning;Market research;Training data;Computer vision","computer vision;Internet;learning (artificial intelligence)","BubbLeNet;foveated imaging;visual discovery;Internet scale corpus;human interpretable discriminative visual elements;deep learning methods;bubble images;identify spatially localized;training procedure;visual elements;Internet scale datasets;visual mining;computer vision","","4","30","","","","","IEEE","IEEE Conferences"
"Video event recognition with deep hierarchical context model","X. Wang; Q. Ji","Dept. of ECSE, Rensselaer Polytechnic Institute, USA; Dept. of ECSE, Rensselaer Polytechnic Institute, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","4418","4427","Video event recognition still faces great challenges due to large intra-class variation and low image resolution, in particular for surveillance videos. To mitigate these challenges and to improve the event recognition performance, various context information from the feature level, the semantic level, as well as the prior level is utilized. Different from most existing context approaches that utilize context in one of the three levels through shallow models like support vector machines, or probabilistic models like BN and MRF, we propose a deep hierarchical context model that simultaneously learns and integrates context at all three levels, and holistically utilizes the integrated contexts for event recognition. We first introduce two types of context features describing the event neighborhood, and then utilize the proposed deep model to learn the middle level representations and combine the bottom feature level, middle semantic level and top prior level contexts together for event recognition. The experiments on state of art surveillance video event benchmarks including VIRAT 1.0 Ground Dataset, VIRAT 2.0 Ground Dataset, and the UT-Interaction Dataset demonstrate that the proposed model is quite effective in utilizing the context information for event recognition. It outperforms the existing context approaches that also utilize multiple level contexts on these event benchmarks.","","","10.1109/CVPR.2015.7299071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299071","","Benchmark testing;Yttrium","feature extraction;image representation;image resolution;video signal processing;video surveillance","video event recognition;intra-class variation;low image resolution;surveillance videos;event recognition performance;context information;deep hierarchical context model;context features;event neighborhood;deep model;middle level representations;bottom feature level;middle semantic level;top prior level;VIRAT 1.0 ground dataset;VIRAT 2.0 ground dataset;UT-Interaction dataset","","18","40","","","","","IEEE","IEEE Conferences"
"Robust stock value prediction using support vector machines with particle swarm optimization","T. M. Sands; D. Tayal; M. E. Morris; S. T. Monteiro","Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY 14623 USA; Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY 14623 USA; Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY 14623 USA; Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY 14623 USA","2015 IEEE Congress on Evolutionary Computation (CEC)","","2015","","","3327","3331","Attempting to understand and characterize trends in the stock market has been the goal of numerous market analysts, but these patterns are often difficult to detect until after they have been firmly established. Recently, attempts have been made by both large companies and individual investors to utilize intelligent analysis and trading algorithms to identify potential trends before they occur in the market environment, effectively predicting future stock values and outlooks. In this paper, three different classification algorithms will be compared for the purposes of maximizing capital while minimizing risk to the investor. The main contribution of this work is a demonstrated improvement over other prediction methods using machine learning; the results show that tuning support vector machine parameters with particle swarm optimization leads to highly accurate (approximately 95%) and robust stock forecasting for historical datasets.","","","10.1109/CEC.2015.7257306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7257306","","Support vector machines;Particle swarm optimization;Market research;Accuracy;Mathematical model;Kernel;Prediction algorithms","economic forecasting;learning (artificial intelligence);minimisation;particle swarm optimisation;stock markets;support vector machines","robust stock value prediction;support vector machines;particle swarm optimization;intelligent analysis;trading algorithms;market environment;classification algorithms;capital maximization;risk minimization;machine learning;robust stock forecasting","","10","17","","","","","IEEE","IEEE Conferences"
"Neural network based model for visual-motor integration learning of robot's drawing behavior: Association of a drawing motion from a drawn image","K. Sasaki; H. Tjandra; K. Noda; K. Takahashi; T. Ogata","Graduate School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan; Graduate School of Creative Science and Engineering, Waseda University, Tokyo, Japan; Graduate School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan; Graduate School of Creative Science and Engineering, Waseda University, Tokyo, Japan; Graduate School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","2015","","","2736","2741","In this study, we propose a neural network based model for learning a robot's drawing sequences in an unsupervised manner. We focus on the ability to learn visual-motor relationships, which can work as a reusable memory in association of drawing motion from a picture image. Assuming that a humanoid robot can draw a shape on a pen tablet, the proposed model learns drawing sequences, which comprises drawing motion and drawn picture image frames. To learn raw pixel data without any given specific features, we utilized a deep neural network for compressing large dimensional picture images and a continuous time recurrent neural network for integration of motion and picture images. To confirm the ability of the proposed model, we performed an experiment for learning 15 sequences comprising three types of shapes. The model successfully learns all the sequences and can associate a drawing motion from a not trained picture image and a trained picture with similar success. We also show that the proposed model self-organizes its behavior according to types shapes.","","","10.1109/IROS.2015.7353752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353752","","Context;Robots;Training;Recurrent neural networks;Neurons;Shape","control engineering computing;humanoid robots;learning (artificial intelligence);recurrent neural nets;robot vision","neural network based model;visual-motor integration learning;robot drawing behavior;drawing motion;robot drawing sequence;visual-motor relationship;reusable memory;trained picture image;continuous time recurrent neural network;raw pixel data;drawn picture image frame;pen tablet;humanoid robot","","4","17","","","","","IEEE","IEEE Conferences"
"Structural Kernel Learning for Large Scale Multiclass Object Co-detection","Z. Hayder; X. He; M. Salzmann","Australian Nat. Univ., Canberra, ACT, Australia; NA; CVLab, EPFL, Lausanne, Switzerland","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2632","2640","Exploiting contextual relationships across images has recently proven key to improve object detection. The resulting object co-detection algorithms, however, fail to exploit the correlations between multiple classes and, for scalability reasons are limited to modeling object instance similarity with relatively low-dimensional hand-crafted features. Here, we address the problem of multiclass object co-detection for large scale datasets. To this end, we formulate co-detection as the joint multiclass labeling of object candidates obtained in a class-independent manner. To exploit the correlations between objects, we build a fully-connected CRF on the candidates, which explicitly incorporates both geometric layout relations across object classes and similarity relations across multiple images. We then introduce a structural boosting algorithm that lets us exploits rich, high-dimensional deep network features to learn object similarity within our fully-connected CRF. Our experiments on PASCAL VOC 2007 and 2012 evidences the benefits of our approach over object detection with RCNN, single-image CRF methods and state-of-the-art co-detection algorithms.","","","10.1109/ICCV.2015.302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410659","","Conferences;Computer vision","learning (artificial intelligence);object detection","structural kernel learning;multiclass object co-detection;joint multiclass labeling;fully-connected CRF;structural boosting algorithm;object similarity learning;RCNN;single-image CRF methods;conditional random fields","","4","30","","","","","IEEE","IEEE Conferences"
"A Deep Visual Correspondence Embedding Model for Stereo Matching Costs","Z. Chen; X. Sun; L. Wang; Y. Yu; C. Huang","NA; NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","972","980","This paper presents a data-driven matching cost for stereo matching. A novel deep visual correspondence embedding model is trained via Convolutional Neural Network on a large set of stereo images with ground truth disparities. This deep embedding model leverages appearance data to learn visual similarity relationships between corresponding image patches, and explicitly maps intensity values into an embedding feature space to measure pixel dissimilarities. Experimental results on KITTI and Middlebury data sets demonstrate the effectiveness of our model. First, we prove that the new measure of pixel dissimilarity outperforms traditional matching costs. Furthermore, when integrated with a global stereo framework, our method ranks top 3 among all two-frame algorithms on the KITTI benchmark. Finally, cross-validation results show that our model is able to make correct predictions for unseen data which are outside of its labeled training set.","","","10.1109/ICCV.2015.117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410474","","Feature extraction;Computational modeling;Visualization;Data models;Training;Neural networks;Machine learning","image matching;neural nets;stereo image processing","deep visual correspondence embedding model;stereo matching costs;data-driven matching cost;convolutional neural network;stereo images;ground truth disparities;visual similarity relationships;image patches;embedding feature space;pixel dissimilarity measurement;Middlebury data sets;global stereo framework;KITTI benchmark;cross-validation results;labeled training set","","53","42","","","","","IEEE","IEEE Conferences"
"Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks","K. Cho; A. Courville; Y. Bengio","Information and Operational Research, Université de Montréal, Montréal, Canada; Information and Operational Research, Université de Montréal, Montréal, Canada; Information and Operational Research, Université de Montréal, Montréal, Canada","IEEE Transactions on Multimedia","","2015","17","11","1875","1886","Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. In this paper we focus on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description, and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.","","","10.1109/TMM.2015.2477044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7243334","Attention mechanism;deep learning;recurrent neural networks","Context;Decoding;Context modeling;Computational modeling;Recurrent neural networks;Mathematical model","learning (artificial intelligence);multimedia systems;recurrent neural nets","multimedia content;attention-based encoder-decoder networks;deep neural networks;machine translation;image caption generation;video clip description;speech recognition;gated recurrent neural networks;convolutional neural networks","","92","75","","","","","IEEE","IEEE Journals"
"Hybrid penetration depth computation using local projection and machine learning","Y. Kim; D. Manocha; Y. J. Kim","Department of Computer Science and Engineering at Ewha Womans University in Seoul, Korea; Department of Computer Science at the University of North Carolina at Chapel Hill, United States; Department of Computer Science and Engineering at Ewha Womans University in Seoul, Korea","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","2015","","","4804","4809","We present a new hybrid approach to computing penetration depth (PD) for general polygonal models. Our approach exploits both local and global approaches to PD computation and can compute error-bounded PD approximations for both deep and shallow penetrations. We use a two-step formulation: the first step corresponds to a global approximation approach that samples the configuration space with bounded error using support vector machines; the second step corresponds to a local optimization that performs a projection operation refining the penetration depth. We have implemented this hybrid algorithm on a standard PC platform and tested its performance with various benchmarks. The experimental results show that our algorithm offers significant benefits over previously developed local-only and global-only methods used to compute the PD.","","","10.1109/IROS.2015.7354052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7354052","","Approximation algorithms;Handheld computers;Support vector machines;Approximation methods;Benchmark testing;Computational modeling;Robots","approximation theory;control engineering computing;learning (artificial intelligence);robots;solid modelling;spatial variables measurement;support vector machines","hybrid penetration depth computation;local projection;machine learning;polygonal models;error-bounded PD approximations;deep penetrations;shallow penetrations;global approximation approach;support vector machines;configuration space;PC platform;local-only methods;global-only methods","","2","19","","","","","IEEE","IEEE Conferences"
"Long-term recurrent convolutional networks for visual recognition and description","J. Donahue; L. A. Hendricks; S. Guadarrama; M. Rohrbach; S. Venugopalan; T. Darrell; K. Saenko","UC Berkeley, USA; UC Berkeley, USA; UC Berkeley, USA; UC Berkeley, USA; UT Austin, TX, USA; UC Berkeley, USA; UMass Lowell, MA, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","2625","2634","Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or “temporally deep”, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are “doubly deep” in that they can be compositional in spatial and temporal “layers”. Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.","","","10.1109/CVPR.2015.7298878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298878","","Visualization;Computer architecture;Computational modeling;Data models;Logic gates;Image recognition;Microprocessors","image recognition;image retrieval;learning (artificial intelligence);neural net architecture;recurrent neural nets;video signal processing","recurrent convolutional networks;visual recognition;visual description;deep convolutional networks;image interpretation tasks;recurrent convolutional architecture;visual learning;benchmark video recognition tasks;image description;retrieval problems;video narration challenges;long-term dependency learning;RNN models;backpropagation;temporal dynamics;convolutional perceptual representations","","1019","48","","","","","IEEE","IEEE Conferences"
"A deep recurrent approach for acoustic-to-articulatory inversion","P. Liu; Q. Yu; Z. Wu; S. Kang; H. Meng; L. Cai","Tsinghua-CUHK Joint Research Center for Media Sciences, Technologies and Systems, Shenzhen Key Laboratory of Information Science and Technology, Graduate School at Shenzhen, Tsinghua University, 518055, China; Tsinghua-CUHK Joint Research Center for Media Sciences, Technologies and Systems, Shenzhen Key Laboratory of Information Science and Technology, Graduate School at Shenzhen, Tsinghua University, 518055, China; Tsinghua-CUHK Joint Research Center for Media Sciences, Technologies and Systems, Shenzhen Key Laboratory of Information Science and Technology, Graduate School at Shenzhen, Tsinghua University, 518055, China; Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N.T., China; Tsinghua-CUHK Joint Research Center for Media Sciences, Technologies and Systems, Shenzhen Key Laboratory of Information Science and Technology, Graduate School at Shenzhen, Tsinghua University, 518055, China; Tsinghua-CUHK Joint Research Center for Media Sciences, Technologies and Systems, Shenzhen Key Laboratory of Information Science and Technology, Graduate School at Shenzhen, Tsinghua University, 518055, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4450","4454","To solve the acoustic-to-articulatory inversion problem, this paper proposes a deep bidirectional long short term memory recurrent neural network and a deep recurrent mixture density network. The articulatory parameters of the current frame may have correlations with the acoustic features many frames before or after. The traditional pre-designed fixed-length context window may be either insufficient or redundant to cover such correlation information. The advantage of recurrent neural network is that it can learn proper context information on its own without the requirement of externally specifying a context window. Experimental results indicate that recurrent model can produce more accurate predictions for acoustic-to-articulatory inversion than deep neural network having fixed-length context window. Furthermore, the predicted articulatory trajectory curve of recurrent neural network is smooth. Average root mean square error of 0.816 mm on the MNGU0 test set is achieved without any post-filtering, which is state-of-the-art inversion accuracy.","","","10.1109/ICASSP.2015.7178812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178812","long short term memory (LSTM);recurrent nueral network (RNN);mixture density network (MDN);layer-wise pre-training","Context;Speech;Trajectory;Recurrent neural networks;Acoustics;Hidden Markov models;Correlation","recurrent neural nets;speech synthesis","acoustic-to-articulatory inversion problem;deep bidirectional long short term memory recurrent neural network;deep recurrent mixture density network;pre-designed fixed-length context window;root mean square error;MNGU0 test set;speech synthesis","","13","25","","","","","IEEE","IEEE Conferences"
"Face retriever: Pre-filtering the gallery via deep neural net","D. Wang; A. K. Jain","Department of Computer Science and Engineering, Michigan State University, East Lansing, 48824, U.S.A.; Department of Computer Science and Engineering, Michigan State University, East Lansing, 48824, U.S.A.","2015 International Conference on Biometrics (ICB)","","2015","","","473","480","Face retrieval is an enabling technology for many applications, including automatic face annotation, deduplication, and surveillance. In this paper, we propose a face retrieval system which combines a k-NN search procedure with a COTS matcher (PittPatt1) in a cascaded manner. In particular, given a query face, we first pre-filter the gallery set and find the top-k most similar faces for the query image by using deep facial features that are learned with a deep convolutional neural network. The top-k most similar faces are then re-ranked based on score-level fusion of the similarities between deep features and the COTS matcher. To further boost the retrieval performance, we develop a manifold ranking algorithm. The proposed face retrieval system is evaluated on two large-scale face image databases: (i) a web face image database, which consists of over 3, 880 query images of 1, 507 subjects and a gallery of 5, 000, 000 faces, and (ii) a mugshot database, which consists of 1, 000 query images of 1, 000 subjects and a gallery of 1, 000, 000 faces. Experimental results demonstrate that the proposed face retrieval system can simultaneously improve the retrieval performance (CMC and precision-recall) and scalability for large-scale face retrieval problems.","","","10.1109/ICB.2015.7139112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139112","","Face;Databases;Manifolds;Training;Face recognition;Facial features;Scalability","face recognition;image fusion;image matching;image retrieval;neural nets","face retriever;pre-filtering;gallery;deep neural net;automatic face annotation;deduplication;surveillance;face retrieval system;k-NN search procedure;COTS matcher;PittPatt;query face;query image;facial feature;deep convolutional neural network;score-level fusion;retrieval performance;manifold ranking algorithm;large-scale face image database;Web face image database;mugshot database;CMC;precision-recall;large-scale face retrieval problem","","4","16","","","","","IEEE","IEEE Conferences"
"Comparing SVD and SDAE for Analysis of Islamist Forum Postings","N. Alsadhan; D. B. Skillicorn","NA; NA","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","","2015","","","948","953","We analyze postings in the Turn to Islam forum using techniques based on singular value decomposition (SVD) and the deep learning technique of stacked denoising autoencoders (SDAE). Models based on frequent words and jihadist language intensity are used, and the results compared. Our main conclusion is that SDAE approaches, while clearly discovering structure in document-word matrices, do not yet provide a natural interpretation strategy, limiting their practical usefulness. In contrast, SVD approaches provide interpretable models, primarily because of the coupling between document and word variation patterns.","","","10.1109/ICDMW.2015.108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395769","s22211","Noise reduction;Training;Singular value decomposition;Conferences;Machine learning;Matrix decomposition;Encoding","document handling;humanities;natural language processing;singular value decomposition;Web sites","SVD;SDAE;Islamist forum postings analysis;singular value decomposition;deep learning technique;stacked denoising autoencoders;frequent words;jihadist language intensity;document-word matrices;natural interpretation strategy;interpretable models;word variation patterns;document patterns","","","13","","","","","IEEE","IEEE Conferences"
"On the importance of modeling and robustness for deep neural network feature","S. Chang; S. Wegmann","EECS Department, University of California-Berkeley, USA; International Computer Science Institute, Berkeley, CA, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4530","4534","A large body of research has shown that acoustic features for speech recognition can be learned from data using neural networks with multiple hidden layers (DNNs) and that these learned features are superior to standard features (e.g., MFCCs). However, this superiority is usually demonstrated when the data used to learn the features is very similar in character to the data used to test recognition performance. An open question is how well these learned features generalize to realistic data that is different in character to their training data. The ability of a feature representation to generalize to unfamiliar data is a highly desirable form of robustness. In this paper we investigate the robustness of two DNN-based feature sets to training/test mismatch using the ICSI meeting corpus. The experiments were performed under 3 training/test scenarios: (1) matched near-field (2) matched far-field and (3) the mismatched condition near-field training with far-field testing. The experiments leverage simulation and a novel sampling process that we have developed for diagnostic analysis within the HMM-based speech recognition framework. First, diagnostic analysis shows that a DNN-based feature representation that uses MFCC inputs (MFCC-DNN) is indeed superior to the corresponding MFCC baselines in the two matched scenarios where the source of recognition errors are from incorrect model, but the DNN-based features and MFCCs have nearly identical and poor performance in the mismatched scenario. Second, we show that a DNN-based feature representation that uses a more robust input, namely power normalized spectrum (PNS) and Gabor filters, performs nearly as well as the MFCC-DNN features in the matched scenarios and much better than MFCCs and MFCC-DNNs in the mismatched scenario.","","","10.1109/ICASSP.2015.7178828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178828","deep neural network;acoustic feature;robust speech recognition","Hidden Markov models;Mel frequency cepstral coefficient;Data models;Neural networks;Training;Speech recognition;Robustness","feature extraction;neural nets;signal representation;speech recognition","acoustic features;neural networks;recognition performance;learned features;realistic data;training data;DNN-based feature sets;ICSI meeting corpus;matched near-field;matched far-field;mismatched condition near-field training;far-field testing;diagnostic analysis;HMM-based speech recognition framework;DNN-based feature representation;MFCC inputs;MFCC-DNN;recognition errors;power normalized spectrum;PNS;Gabor filters","","9","21","","","","","IEEE","IEEE Conferences"
"Malware classification with recurrent networks","R. Pascanu; J. W. Stokes; H. Sanossian; M. Marinescu; A. Thomas","University of Montreal, Montréal QC H3C 3J7 Canada; Microsoft Research, One Microsoft Way, Redmond, WA 98052 USA; Microsoft Pty Ltd, Level 5, 4 Freshwater Place, Southbank, VIC 3006 Australia; Microsoft Corp., One Microsoft Way, Redmond, WA 98052 USA; NA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","1916","1920","Attackers often create systems that automatically rewrite and reorder their malware to avoid detection. Typical machine learning approaches, which learn a classifier based on a handcrafted feature vector, are not sufficiently robust to such reorderings. We propose a different approach, which, similar to natural language modeling, learns the language of malware spoken through the executed instructions and extracts robust, time domain features. Echo state networks (ESNs) and recurrent neural networks (RNNs) are used for the projection stage that extracts the features. These models are trained in an unsupervised fashion. A standard classifier uses these features to detect malicious files. We explore a few variants of ESNs and RNNs for the projection stage, including Max-Pooling and Half-Frame models which we propose. The best performing hybrid model uses an ESN for the recurrent model, Max-Pooling for non-linear sampling, and logistic regression for the final classification. Compared to the standard trigram of events model, it improves the true positive rate by 98.3% at a false positive rate of 0.1%.","","","10.1109/ICASSP.2015.7178304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178304","Malware Classification;Recurrent Neural Network;Deep Learning","Logistics;Spyware;Computational modeling","invasive software;learning (artificial intelligence);natural languages;recurrent neural nets;regression analysis;sampling methods;time-domain analysis","malware classification;recurrent neural network;machine learning approach;handcrafted feature vector;natural language modeling;time domain feature;echo state network;ESN;malicious file;Max-Pooling model;half-frame model;nonlinear sampling;logistic regression;trigram of events model","","62","26","","","","","IEEE","IEEE Conferences"
"Improved deep convolutional neural network for online handwritten Chinese character recognition using domain-specific knowledge","W. Yang; L. Jin; Z. Xie; Z. Feng","College of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; College of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; College of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; College of Electronic and Information Engineering, South China University of Technology, Guangzhou, China","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","551","555","Deep convolutional neural networks (DCNNs) have achieved great success in various computer vision and pattern recognition applications, including those for handwritten Chinese character recognition (HCCR). However, most current DCNN-based HCCR approaches treat the handwritten sample simply as an image bitmap, ignoring some vital domain-specific information that may be useful but that cannot be learnt by traditional networks. In this paper, we propose an enhancement of the DCNN approach to online HCCR by incorporating a variety of domain-specific knowledge, including deformation, non-linear normalization, imaginary strokes, path signature, and 8-directional features. Our contribution is twofold. First, these domain-specific technologies are investigated and integrated with a DCNN to form a composite network to achieve improved performance. Second, the resulting DCNNs with diversity in their domain knowledge are combined using a hybrid serial-parallel (HSP) strategy. Consequently, we achieve a promising accuracy of 97.20% and 96.87% on CASIA-OLHWDB1.0 and CASIA-OLHWDB1.1, respectively, outperforming the best results previously reported in the literature.","","","10.1109/ICDAR.2015.7333822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333822","Handwritten Chinese character recognition;deep convolutional neural network;domain-specific knowledge;hybrid serial-parallel ensemble strategy","Image recognition;Handwriting recognition;Databases;Testing","computer vision;handwritten character recognition;image enhancement;natural language processing;neural nets","improved deep convolutional neural network;online handwritten Chinese character recognition;domain-specific knowledge;pattern recognition applications;computer vision applications;DCNN-based HCCR approach;image bitmap;online HCCR;nonlinear normalization;imaginary strokes;path signature;8-directional features;domain-specific technologies;composite network;hybrid serial-parallel strategy;HSP strategy;CASIA-OLHWDB1.0;CASIA-OLHWDB1.1","","28","25","","","","","IEEE","IEEE Conferences"
"Exploring visual literacy as a global competency: An international study of the teaching and learning of communication","C. White; L. Breslow; D. Hastings","Engineering Education, SMART, Singapore; Sloan School of Management, Founding Director Emeritus, TLL, MIT, Cambridge, MA; Aeronautics and Astronautics, MIT, CEO & Director, SMART, Singapore","2015 International Conference on Interactive Collaborative Learning (ICL)","","2015","","","771","778","Engineering education is called upon to foster the development of 21st century skills in addition to teaching deep technical expertise. Both of these kinds of knowledge are needed to lead companies and strengthen communities. Indeed as society and the workplace become more global, it is imperative that we develop effective communication skills in undergraduate engineering students. Our larger, comprehensive research study explores the teaching and learning of four communication capabilities: writing, developing and delivering presentations, visual literacy, and participating in teams. This paper focuses specifically on opportunities for undergraduate engineering students to develop visual literacy skills at three universities located in two different countries.","","","10.1109/ICL.2015.7318126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318126","communication;visual literacy;global competency","Visualization;Media;Engineering students;Cultural differences;Collaborative work;Law","educational institutions;engineering education;further education;teaching","global competency;communication teaching;communication learning;undergraduate engineering students;visual literacy skills;universities","","1","46","","","","","IEEE","IEEE Conferences"
"Spectral mask estimation using deep neural networks for inter-sensor data ratio model based robust DOA estimation","W. Q. Zheng; Y. X. Zou; C. Ritz","ADSPLAB/ELIP, School of ECE, Peking University, Shenzhen, 518055, China; ADSPLAB/ELIP, School of ECE, Peking University, Shenzhen, 518055, China; School of ECTE, University of Wollongong, 2522, Australia","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","325","329","Accurate DOA estimation based on clustering the inter-sensor data ratios (ISDRs) of a single acoustic vector sensor (AVS), referred as AVS-ISDR, relies on reliable extraction of time-frequency points with high local signal-to-noise ratio (HLSNR-TFPs) and its performance degrades in noisy environments. This paper investigates deep neural networks (DNNs) trained with noisy-clean speech pairs under different SNR levels and noise types to improve the performance of AVS-ISDR in noise conditions. The DNNs is trained to learn characteristics reflecting the level of speech information at different TFPs, which helps to generate a reliable spectral mask for obtaining a noise-reduced spectral. Correspondingly, a robust DOA estimation algorithm named as AVS-DNN-ISDR has been developed. Experimental results verify the proposed DNN-based spectral mask improves the reliable HLSNR-TFPs extraction at different SNR levels. Results from simulations and real AVS recordings further validate AVS-DNN-ISDR achieving high DOA estimation accuracy even when the SNR is lower than 0dB.","","","10.1109/ICASSP.2015.7177984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177984","Direction of arrival estimation;acoustic vector sensor;deep neural networks;spectral mask estimation;inter-sensor data ratios","Estimation;Direction-of-arrival estimation;Speech;Signal to noise ratio;Acoustics;Noise measurement","direction-of-arrival estimation;neural nets;speech processing;time-frequency analysis","spectral mask estimation;deep neural networks;inter-sensor data ratio model based robust DOA estimation;acoustic vector sensor;time-frequency points;high local signal-to-noise ratio;noisy environments;noisy-clean speech pairs;noise conditions;speech information;AVS-DNN-ISDR","","5","24","","","","","IEEE","IEEE Conferences"
"An experimental study of speech emotion recognition based on deep convolutional neural networks","W. Q. Zheng; J. S. Yu; Y. X. Zou","ADSPLAB/ELIP, School of Electronic Computer Engineering, Peking University, Shenzhen, China; ADSPLAB/ELIP, School of Electronic Computer Engineering, Peking University, Shenzhen, China; ADSPLAB/ELIP, School of Electronic Computer Engineering, Peking University, Shenzhen, China","2015 International Conference on Affective Computing and Intelligent Interaction (ACII)","","2015","","","827","831","Speech emotion recognition (SER) is a challenging task since it is unclear what kind of features are able to reflect the characteristics of human emotion from speech. However, traditional feature extractions perform inconsistently for different emotion recognition tasks. Obviously, different spectrogram provides information reflecting difference emotion. This paper proposes a systematical approach to implement an effectively emotion recognition system based on deep convolution neural networks (DCNNs) using labeled training audio data. Specifically, the log-spectrogram is computed and the principle component analysis (PCA) technique is used to reduce the dimensionality and suppress the interferences. Then the PCA whitened spectrogram is split into non-overlapping segments. The DCNN is constructed to learn the representation of the emotion from the segments with labeled training speech data. Our preliminary experiments show the proposed emotion recognition system based on DCNNs (containing 2 convolution and 2 pooling layers) achieves about 40% classification accuracy. Moreover, it also outperforms the SVM based classification using the hand-crafted acoustic features.","","","10.1109/ACII.2015.7344669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344669","speech emotion recognition;deep convolutional neural networks;principle component analysis whitening;speech spectrogram","Speech;Speech recognition;Emotion recognition;Spectrogram;Feature extraction;Principal component analysis;Convolution","emotion recognition;feature extraction;neural nets;principal component analysis;speech recognition;support vector machines","speech emotion recognition;deep convolutional neural network;SER;human emotion characteristics;feature extraction;spectrogram;systematical approach;DCNN;principle component analysis technique;PCA technique;SVM","","28","27","","","","","IEEE","IEEE Conferences"
"Downbeat tracking with multiple features and deep neural networks","S. Durand; J. P. Bello; B. David; G. Richard","Institut Mines-Telecom, Telecom ParisTech, CNRS-LTCI, 37/39, rue Dareau, 75014 - France; Music and Audio Research Laboratory (MARL), New York University - USA; Institut Mines-Telecom, Telecom ParisTech, CNRS-LTCI, 37/39, rue Dareau, 75014 - France; Institut Mines-Telecom, Telecom ParisTech, CNRS-LTCI, 37/39, rue Dareau, 75014 - France","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","409","413","In this paper, we introduce a novel method for the automatic estimation of downbeat positions from music signals. Our system relies on the computation of musically inspired features capturing important aspects of music such as timbre, harmony, rhythmic patterns, or local similarities in both timbre and harmony. It then uses several independent deep neural networks to learn higher-level representations. The downbeat sequences are finally obtained thanks to a temporal decoding step based on the Viterbi algorithm. The comparative evaluation conducted on varied datasets demonstrates the efficiency and robustness across different music styles of our approach.","","","10.1109/ICASSP.2015.7178001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178001","Downbeat Tracking;Music Information Retrieval;Music Signal Processing;Deep Networks","Multiple signal classification;Feature extraction;Robustness;Estimation;Timbre;Viterbi algorithm","acoustic signal processing;music;neural nets;Viterbi decoding","downbeat tracking;deep neural networks;automatic estimation;downbeat positions;music signals;rhythmic patterns;downbeat sequences;temporal decoding step;Viterbi algorithm;music styles","","6","33","","","","","IEEE","IEEE Conferences"
"Neural networks with dynamic structure using a GA-based learning method","E. Fall; H. Chiang","Department of Electrical Engineering, Fu Jen Catholic University, New Taipei City, Taiwan, ROC; Department of Electrical Engineering, Fu Jen Catholic University, New Taipei City, Taiwan, ROC","2015 IEEE 12th International Conference on Networking, Sensing and Control","","2015","","","7","12","Artificial neural networks (NNs) are traditionally designed with distinctly defined layers (input layer, hidden layers, output layer) and accordingly network design techniques and training algorithms are based on this concept of strictly defined layers. In this paper, a new approach to designing neural networks is presented. The structure of the proposed NN is not strictly defined (each neuron may receive input from any other neuron). Instead, the initial network structure can be randomly generated, and traditional methods of training, such as back-propagation, are replaced or augmented by a genetic algorithm (GA). The weighting of each neuron input is encoded genetically to serve as the genes for the GA. By means of the training data provided to the supervised network, the contribution of each neuron in creating a desired output serves as a selection function. Each of the neurons is then modified to store and recall past weightings for possible future use. A simple network is trained to recognize vertical and horizontal lines as a proof of concept.","","","10.1109/ICNSC.2015.7116001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7116001","Neural network;genetic algorithm;biologically inspired;weight retention;deep learning;complex neuron","Neurons;Training;Genetic algorithms;Artificial neural networks;Biological neural networks;Algorithm design and analysis","backpropagation;genetic algorithms;neural nets","dynamic structure;GA-based learning method;artificial neural networks;ANN;backpropagation;genetic algorithm;supervised network;selection function","","2","7","","","","","IEEE","IEEE Conferences"
"Learning Deconvolution Network for Semantic Segmentation","H. Noh; S. Hong; B. Han","NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1520","1528","We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction, our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.","","","10.1109/ICCV.2015.178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410535","","Deconvolution;Semantics;Image segmentation;Visualization;Feature extraction;Shape;Image reconstruction","convolution;deconvolution;image segmentation;learning (artificial intelligence);neural nets;prediction theory;semantic networks","deconvolution network learning;semantic segmentation algorithm;convolutional neural network;CNN;proposal-wise prediction","","886","30","","","","","IEEE","IEEE Conferences"
"Data Stream Classification Using Random Feature Functions and Novel Method Combinations","J. Read; A. Bifet","NA; NA","2015 IEEE Trustcom/BigDataSE/ISPA","","2015","2","","211","216","Data streams are being generated in a faster, bigger, and more commonplace manner. In this scenario, Hoeffding Trees are an established method for classification. Several extensions exist, including high-performing ensemble setups such as online and leveraging bagging. Also, k-nearest neighbours is a popular choice, with most extensions dealing with the inherent performance limitations over a potentially-infinite stream. At the same time, gradient descent methods are becoming increasingly popular, owing to the proliferation of interest and successes in deep learning. Although deep neural networks can learn incrementally, they have so far proved too sensitive to hyperparameter options and initial conditions to be considered an effective 'off-the-shelf' data streams solution. In this work, we look at combinations of Hoeffding trees, nearest neighbour, and gradient descent methods with a streaming preprocessing approach in the form of a random feature functions filter for additional predictive power. Our empirical evaluation yields positive results for the novel approaches that we experiment with, and also highlight important issues, and shed light on promising future directions in approaches to data stream classification.","","","10.1109/Trustcom.2015.585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345498","","Bagging;Training;Adaptation models;Electronic mail;Random access memory;Neural networks;Benchmark testing","gradient methods;learning (artificial intelligence);neural nets;pattern classification;trees (mathematics)","data stream classification;method combinations;Hoeffding trees;high-performing ensemble setups;online bagging;leveraging bagging;k-nearest neighbours;performance limitations;gradient descent methods;deep learning;deep neural networks;off-the-shelf data streams;streaming preprocessing approach;random feature functions filter;predictive power","","1","20","","","","","IEEE","IEEE Conferences"
"A useful feature-engineering approach for a LVCSR system based on CD-DNN-HMM algorithm","S. J. Lee; B. O. Kang; H. Chung; J. G. Park","Speech Processing Lab., Electronics and Telecommunication Research Institute (ETRI), Address: 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350 South Korea; Speech Processing Lab., Electronics and Telecommunication Research Institute (ETRI), Address: 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350 South Korea; Speech Processing Lab., Electronics and Telecommunication Research Institute (ETRI), Address: 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350 South Korea; Speech Processing Lab., Electronics and Telecommunication Research Institute (ETRI), Address: 161 Gajeong-dong, Yuseong-gu, Daejeon, 305-350 South Korea","2015 23rd European Signal Processing Conference (EUSIPCO)","","2015","","","1421","1425","In this paper, we propose a useful feature-engineering approach for Context-Dependent Deep-Neural-Network Hidden-Markov-Model (CD-DNN-HMM) based Large-Vocabulary-Continuous-Speech-Recognition (LVCSR) systems. The speech recognition performance of a LVCSR system is improved from two feature-engineering perspectives. The first performance improvement is achieved by adopting the intra/inter-frame feature subsets when the Gaussian-Mixture-Model (GMM) HMMs for the HMM state-level alignment are built. And the second performance gain is then followed with the additional features augmenting the front-end of the DNN. We evaluate the effectiveness of our feature-engineering approach under a series of Korean speech recognition tasks (isolated single-syllable recognition with a medium-sized speech corpus and conversational speech recognition with a large-sized database) using the Kaldi speech recognition toolkit. The results show that the proposed feature-engineering approach outperforms the traditional Mel Frequency Cepstral Coefficient (MFCCs) GMM + Mel-frequency filter-bank output DNN method.","","","10.1109/EUSIPCO.2015.7362618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362618","Feature extraction;feature engineering;speech recognition;deep learning;deep neural network","Speech recognition;Speech;Feature extraction;Hidden Markov models;Entropy;Harmonic analysis;Acoustics","feature extraction;hidden Markov models;neural nets;speech recognition","feature-engineering approach;context-dependent deep-neural-network hidden-Markov-model;large-vocabulary-continuous-speech-recognition systems;CD-DNN-HMM LVCSR systems;intra-inter-frame feature subsets;Gaussian-mixture-model HMM;GMM HMM;HMM state-level alignment;Korean speech recognition tasks;Kaldi speech recognition toolkit","","1","23","","","","","IEEE","IEEE Conferences"
"Enhancing RGB CNNs with depth","A. Sharma; K. P. Sankar","Xerox Research Centre India; Xerox Research Centre India","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","","2015","","","031","035","Most current approaches for recognition in RGB-D images fall in either the late fusion or the early fusion category. A drawback of the early fusion scheme is its inapplicability when one of the modalities is absent at test time. On the other hand, a late fusion of features does not allow the correlated nature of modalities to be exploited effectively. Recent approaches using Deep Learning are not immune to these problems either. In this work, we propose a simple, yet elegant method towards combining early and late fusion of colour and depth information when training deep Convolutional Neural Networks (CNNs). We show that when fine-tuning CNNs, an intermediate depth pre-training step provides a significant jump in colour recognition accuracy. The trends are observed consistently over several benchmark RGB-D datasets.","","","10.1109/ACPR.2015.7486460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486460","","Feature extraction;Image color analysis;Training;Testing;Object recognition;Three-dimensional displays;Machine learning","image colour analysis;learning (artificial intelligence);neural nets;object recognition","RGB CNN;RGB-D images;early fusion scheme;deep learning;convolutional neural networks;colour recognition accuracy;object recognition","","","18","","","","","IEEE","IEEE Conferences"
"Deep neural support vector machines for speech recognition","S. Zhang; C. Liu; K. Yao; Y. Gong","Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4275","4279","A new type of deep neural networks (DNNs) is presented in this paper. Traditional DNNs use the multinomial logistic regression (softmax activation) at the top layer for classification. The new DNN instead uses a support vector machine (SVM) at the top layer. Two training algorithms are proposed at the frame and sequence-level to learn parameters of SVM and DNN in the maximum-margin criteria. In the frame-level training, the new model is shown to be related to the multiclass SVM with DNN features; In the sequence-level training, it is related to the structured SVM with DNN features and HMM state transition features. Its decoding process is similar to the DNN-HMM hybrid system but with frame-level posterior probabilities replaced by scores from the SVM. We term the new model deep neural support vector machine (DNSVM). We have verified its effectiveness on the TIMIT task for continuous speech recognition.","","","10.1109/ICASSP.2015.7178777","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178777","DNN;multiclass SVM;structured SVM;maximum margin;sequence training","Support vector machines;Training;Mathematical model;Speech recognition;Hidden Markov models;Neural networks;Speech","hidden Markov models;neural nets;speech recognition;support vector machines","continuous speech recognition;deep neural support vector machines;deep neural networks;DNN;multinomial logistic regression;softmax activation;training algorithms;maximum-margin criteria;frame-level training;sequence-level training;HMM state transition features;TIMIT task;frame-level posterior probability","","10","26","","","","","IEEE","IEEE Conferences"
"Motor task event detection using Subthalamic Nucleus Local Field Potentials","S. Niketeghad; A. O. Hebb; J. Nedrud; S. J. Hanrahan; M. H. Mahoor","Electrical and computer engineering dept., University of Denver, CO, USA; Colorado brain and spine institute, Englewood, USA; Investigator Scientist at Colorado Neurological Institute, Englewood, USA; Investigator Scientist at Colorado Neurological Institute, Englewood, USA; Department of electrical and computer engineering, University of Denver, CO, USA","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","5553","5556","Deep Brain Stimulation (DBS) provides significant therapeutic benefit for movement disorders such as Parkinson's disease. Current DBS devices lack real-time feedback (thus are open loop) and stimulation parameters are adjusted during scheduled visits with a clinician. A closed-loop DBS system may reduce power consumption and DBS side effects. In such systems, DBS parameters are adjusted based on patient's behavior, which means that behavior detection is a major step in designing such systems. Various physiological signals can be used to recognize the behaviors. Subthalamic Nucleus (STN) Local Field Potential (LFP) is a great candidate signal for the neural feedback, because it can be recorded from the stimulation lead and does not require additional sensors. A practical behavior detection method should be able to detect behaviors asynchronously meaning that it should not use any prior knowledge of behavior onsets. In this paper, we introduce a behavior detection method that is able to asynchronously detect the finger movements of Parkinson patients. As a result of this study, we learned that there is a motor-modulated inter-hemispheric connectivity between LFP signals recorded bilaterally from STN. We used non-linear regression method to measure this connectivity and use it to detect the finger movements. Performance of this method is evaluated using Receiver Operating Characteristic (ROC).","","","10.1109/EMBC.2015.7319650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319650","Deep Brain Stimulation;Parkinson's Disease;Local Field Potential (LFP);Nonlinear Regression;Receiver Operating Characteristic","Satellite broadcasting;Correlation;Pressing;Real-time systems;Lead;Time series analysis;Detectors","bioelectric potentials;biomedical measurement;brain;diseases;neuromuscular stimulation","motor task event detection;subthalamic nucleus local field potentials;deep brain stimulation;movement disorders;Parkinson disease;DBS devices;closed-loop DBS system;power consumption;DBS side effects;DBS parameters;neural feedback;Parkinson patients;finger movements;motor-modulated interhemispheric connectivity;nonlinear regression method;receiver operating characteristic","Deep Brain Stimulation;Fingers;Humans;Movement;Parkinson Disease;Subthalamic Nucleus","3","19","","","","","IEEE","IEEE Conferences"
"Ensemble of deep long short term memory networks for labelling origin of replication sequences","U. Singh; S. Chauhan; A. Krishnamachari; L. Vig","School of Computational and Integrative Sciences, Jawaharlal Nehru University, New Delhi, India; School of Computational and Integrative Sciences, Jawaharlal Nehru University, New Delhi, India; School of Computational and Integrative Sciences, Jawaharlal Nehru University, New Delhi, India; School of Computational and Integrative Sciences, Jawaharlal Nehru University, New Delhi, India","2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)","","2015","","","1","7","Advancement in sequence data generation technologies are churning out voluminous omics data and posing a massive challenge to annotate the biological functional features. Sequence data from the well studied model organism Saccharomyces cerevisiae has been commonly used to test and validate in silico prediction methods. DNA replication is a critical step in the cellular process and the sequence location where this process originates in the genomic landscape is generally referred as origin of replication. In this paper we investigate the application bidirectional Long Short Term (LSTM) Networks to predict origin of replication sequences. Long Short Term Memory (LSTM) networks have recently been shown to yield state of the art performance in speech recognition, and music generation. These networks are capable of learning long term patterns via the use of multiplication gates. This paper utilizes Deep bidirectional LSTM for prediction of origin of replication sequences belonging to the organism Saccharomyces cerevisiae. Results demonstrate that LSTMs outperform the commonly used machine learning classifiers such as Support Vector Machine (SVM), Random Forest (RF), Artificial Neural Network (ANN), and Hidden Markov Model (HMM). An important additional advantage of LSTMs is that they work directly on the sequences and obviate the need for hand coded features.","","","10.1109/DSAA.2015.7344871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344871","","Logic gates;Genomics;Bioinformatics;DNA;Biological cells;Computer architecture;Hidden Markov models","biology computing;DNA;genomics;recurrent neural nets","deep long short term memory network;replication sequences;sequence data generation technology;voluminous omics data;biological functional feature;Saccharomyces cerevisiae;DNA replication;cellular process;genomic landscape;bidirectional long short term memory;multiplication gates;deep bidirectional LSTM","","1","24","","","","","IEEE","IEEE Conferences"
"Visual saliency based on multiscale deep features","Guanbin Li; Y. Yu","Department of Computer Science, The University of Hong Kong, China; Department of Computer Science, The University of Hong Kong, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","5455","5463","Visual saliency is a fundamental problem in both cognitive and computational sciences, including computer vision. In this paper, we discover that a high-quality visual saliency model can be learned from multiscale features extracted using deep convolutional neural networks (CNNs), which have had many successes in visual recognition tasks. For learning such saliency models, we introduce a neural network architecture, which has fully connected layers on top of CNNs responsible for feature extraction at three different scales. We then propose a refinement method to enhance the spatial coherence of our saliency results. Finally, aggregating multiple saliency maps computed for different levels of image segmentation can further boost the performance, yielding saliency maps better than those generated from a single segmentation. To promote further research and evaluation of visual saliency models, we also construct a new large database of 4447 challenging images and their pixelwise saliency annotations. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks, improving the F-Measure by 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset (HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively on these two datasets.","","","10.1109/CVPR.2015.7299184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299184","","","feature extraction;image segmentation;neural net architecture","multiscale deep features;cognitive sciences;computational sciences;computer vision;high-quality visual saliency model;multiscale features extraction;deep convolutional neural networks;CNN;visual recognition tasks;neural network architecture;refinement method;spatial coherence;saliency maps;image segmentation","","15","40","","","","","IEEE","IEEE Conferences"
"Towards utterance-based neural network adaptation in acoustic modeling","I. Himawan; P. Motlicek; M. F. Font; S. Madikeri","Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","289","295","Despite the superior classification ability of deep neural networks (DNN), the performance of DNN suffers when there is a mismatch between training and testing conditions. Many speaker adaptation techniques have been proposed for DNN acoustic modeling but in case of environmental robustness the progress is still limited. It is also possible to use techniques developed for adapting speakers to handle the impact of environments at the same time, or to combine both approaches. Directly adapting the large number of DNN parameters is challenging when the adaptation set is small. The learning hidden unit contributions (LHUC) technique for unsupervised speaker adaptation of DNN introduces speaker dependent parameters to the existing speaker independent network to increase the automatic speech recognition (ASR) performance of the target speaker using small amounts of adaptation data. This paper investigates the LHUC to adapt the speech recognizer to target speakers and environments where the impacts of speakers and noise differences are quantified separately. Our finding shows that the LHUC is capable of adapting to both speaker and noise conditions at the same time. Compared to the speaker independent model, about 9% to 13% relative word error rate (WER) improvement are observed for all test conditions using AMI meeting corpus.","","","10.1109/ASRU.2015.7404807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404807","Deep neural networks;acoustic model adaptation;environmental robustness;AMI corpus;LHUC","Adaptation models;Hidden Markov models;Acoustics;Training;Speech;Data models;Signal to noise ratio","learning (artificial intelligence);neural nets;speaker recognition","utterance-based neural network adaptation;deep neural networks;speaker adaptation techniques;DNN acoustic modeling;LHUC technique;learning hidden unit contributions technique;automatic speech recognition;speaker dependent parameter;word error rate;speaker condition;noise condition;AMI meeting corpus","","1","32","","","","","IEEE","IEEE Conferences"
"Two-stage ASGD framework for parallel training of DNN acoustic models using Ethernet","Z. Wang; X. Na; X. Li; J. Pan; Y. Yan","The Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences; The Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences; The Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences; The Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences; The Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","59","64","Deep neural networks have shown significant improvements on acoustic modelling, pushing state-of-the-art performance in large vocabulary continuous speech recognition (LVCSR) tasks. However, training DNNs is very time-consuming on scaled data. In this paper, a data-parallel method, namely two-stage ASGD, is proposed. Two-stage ASGD is based on asynchronous stochastic gradient descent (ASGD) paradigm and is tuned for GPU-equipped computing cluster connected by 10Gbit/s Ethernet other than Infiniband. Several techniques, such as hierarchical learning rate control, double-buffering and order-locking are applied to optimise the communication-to-transmission ratio. The proposed framework is evaluated by training a DNN with 29.5M parameters using a 500-hours Chinese continuous telephone speech data set. By using 4 computer nodes and 8 GPU devices (2 devices used in each node), a 5.9 times acceleration is obtained over a single GPU with acceptable loss of accuracy (0.5% in average). A comparative experiment is done to compare the proposed two-stage ASGD with the parallel DNN training systems reported in prior work.","","","10.1109/ASRU.2015.7404774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404774","Speech recognition;deep neural network;asynchronous stochastic gradient descent;parallel training","Graphics processing units;Training;Computational modeling;Speech recognition;Bandwidth;Acoustics;Parallel processing","acoustic signal processing;gradient methods;graphics processing units;learning (artificial intelligence);local area networks;neural nets;speech recognition","two-stage ASGD framework;DNN acoustic models;Ethernet;deep neural networks;acoustic modelling;large vocabulary continuous speech recognition task;LVCSR task;DNN training;data-parallel method;asynchronous stochastic gradient descent;GPU-equipped computing cluster;graphics processing unit;Infiniband;hierarchical learning rate control;double-buffering technique;order-locking technique;Chinese continuous telephone speech data set","","","17","","","","","IEEE","IEEE Conferences"
"Vehicle Logo Recognition System Based on Convolutional Neural Networks With a Pretraining Strategy","Y. Huang; R. Wu; Y. Sun; W. Wang; X. Ding","Dept. of Commun. Eng., Xiamen Univ., Xiamen, China; Dept. of Commun. Eng., Xiamen Univ., Xiamen, China; Dept. of Commun. Eng., Xiamen Univ., Xiamen, China; Dept. of Electron. Eng., Xiamen Univ., Xiamen, China; Dept. of Commun. Eng., Xiamen Univ., Xiamen, China","IEEE Transactions on Intelligent Transportation Systems","","2015","16","4","1951","1960","Since a vehicle logo is the clearest indicator of a vehicle manufacturer, most vehicle manufacturer recognition (VMR) methods are based on vehicle logo recognition. Logo recognition can be still a challenge due to difficulties in precisely segmenting the vehicle logo in an image and the requirement for robustness against various imaging situations simultaneously. In this paper, a convolutional neural network (CNN) system has been proposed for VMR that removes the requirement for precise logo detection and segmentation. In addition, an efficient pretraining strategy has been introduced to reduce the high computational cost of kernel training in CNN-based systems to enable improved real-world applications. A data set containing 11 500 logo images belonging to 10 manufacturers, with 10 000 for training and 1500 for testing, is generated and employed to assess the suitability of the proposed system. An average accuracy of 99.07% is obtained, demonstrating the high classification potential and robustness against various poor imaging situations.","","","10.1109/TITS.2014.2387069","National Natural Science Foundation of China; National Key Technology R&D Program; Fundamental Research Funds for the Central Universities; Research Fund for the Doctoral Program of Higher Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7031929","Convolutional neural networks (CNNs);deep learning;pretraining;vehicle logo recognition (VLR);Convolutional neural networks (CNNs);deep learning;pretraining;vehicle logo recognition (VLR)","Vehicles;Feature extraction;Kernel;Training;Image segmentation;Licenses;Image recognition","image classification;image segmentation;neural nets;object detection;object recognition;traffic engineering computing","vehicle logo recognition system;convolutional neural networks;pretraining strategy;vehicle manufacturer recognition;VMR;CNN-based systems;logo detection;logo segmentation;classification potential;poor imaging situations","","42","24","","","","","IEEE","IEEE Journals"
"Neuromorphic architectures for spiking deep neural networks","G. Indiveri; F. Corradi; N. Qiao","Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland","2015 IEEE International Electron Devices Meeting (IEDM)","","2015","","","4.2.1","4.2.4","We present a full custom hardware implementation of a deep neural network, built using multiple neuromorphic VLSI devices that integrate analog neuron and synapse circuits together with digital asynchronous logic circuits. The deep network comprises an event-based convolutional stage for feature extraction connected to a spike-based learning stage for feature classification. We describe the properties of the chips used to implement the network and present preliminary experimental results that validate the approach proposed.","","","10.1109/IEDM.2015.7409623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7409623","","Neurons;Integrated circuit modeling;Neuromorphics;Voltage control;Adaptation models;Feature extraction;Computer architecture","logic circuits;neural chips;neural nets;VLSI","neuromorphic architectures;spiking deep neural networks;multiple neuromorphic VLSI devices;asynchronous logic circuits;feature extraction;spike-based learning stage;feature classification","","39","15","","","","","IEEE","IEEE Conferences"
"The Video Recommendation System Based on DBN","C. Hongliang; Q. Xiaona","Sch. of Comput. & Commun. Eng., Univ. of Sci. & Technol. Beijing, Beijing, China; Sch. of Comput. & Commun. Eng., Univ. of Sci. & Technol. Beijing, Beijing, China","2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing","","2015","","","1016","1021","Video recommendation system provides users with suitable video for users to choose, which is an effective way to get a higher user satisfaction and user stickiness. Therefore, video websites pay much attention to it, as well as scholars. The existing recommendation algorithms are fused machine learning algorithms to video recommendation system. Such as some studies the SVM algorithm combined with a recommendation algorithm based on content, or use the BP neural network combined with collaborative filtering algorithm, to improve the algorithm accuracy. With the rapid development of Machine Learning, progresses in Deep Learning are considerable. Especially after the RBM training efficiency matter has been solved by the random sample, the reliability of multi-layer neural network is more clearly, also caused academic interest in depth of the neural network research. Compared to the original SVM model or Shallow Neural Network, Deep Neural Network has a more comprehensive structure, which leads to a better performance in function approximation and feature extraction. Apparently, if Deep Neural Network algorithm is deployed to recommend videos, a better accuracy will be achieved. This paper proposes a video recommendation system, which combines DBN with Collaborative Filtering algorithm, experimental results show that our algorithm achieves a better performance in accuracy compared with old ones.","","","10.1109/CIT/IUCC/DASC/PICOM.2015.154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363195","VideoRecommendationSystem;DBN;RBM;CF (Collaborative Filtering algorithm)","Motion pictures;Feature extraction;Collaboration;Training;Filtering;Neural networks;Filtering algorithms","backpropagation;collaborative filtering;recommender systems;support vector machines","video recommendation system;DBN;video Web sites;fused machine learning algorithms;SVM algorithm;BP neural network;collaborative filtering algorithm;deep learning;RBM training efficiency;multilayer neural network reliability;shallow neural network;feature extraction;function approximation;deep neural network algorithm","","7","10","","","","","IEEE","IEEE Conferences"
"Real-time full-body human gender recognition in (RGB)-D data","T. Linder; S. Wehner; K. O. Arras","Social Robotics Lab, Dept. of Computer Science, University of Freiburg, Germany; Social Robotics Lab, Dept. of Computer Science, University of Freiburg, Germany; Social Robotics Lab, Dept. of Computer Science, University of Freiburg, Germany","2015 IEEE International Conference on Robotics and Automation (ICRA)","","2015","","","3039","3045","Understanding social context is an important skill for robots that share a space with humans. In this paper, we address the problem of recognizing gender, a key piece of information when interacting with people and understanding human social relations and rules. Unlike previous work which typically considered faces or frontal body views in image data, we address the problem of recognizing gender in RGB-D data from side and back views as well. We present a large, gender-balanced, annotated, multi-perspective RGB-D dataset with full-body views of over a hundred different persons captured with both the Kinect v1 and Kinect v2 sensor. We then learn and compare several classifiers on the Kinect v2 data using a HOG baseline, two state-of-the-art deep-learning methods, and a recent tessellation-based learning approach. Originally developed for person detection in 3D data, the latter is able to learn the best selection, location and scale of a set of simple point cloud features. We show that for gender recognition, it outperforms the other approaches for both standing and walking people while being very efficient to compute with classification rates up to 150 Hz.","","","10.1109/ICRA.2015.7139616","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139616","","Three-dimensional displays;Robot sensing systems;Training;Legged locomotion;Accuracy;Support vector machines","human-robot interaction;image classification;image sensors;learning (artificial intelligence);object detection;object recognition","real-time full-body human gender recognition;RGB-D data;human social relations;human social rules;image data;multiperspective RGB-D dataset;gender-balanced RGB-D dataset;annotated RGB-D dataset;Kinect v2 sensor;Kinect v1 sensor;HOG baseline;deep-learning methods;tessellation-based learning approach;person detection;point cloud features","","6","27","","","","","IEEE","IEEE Conferences"
"Head detection based on convolutional neural network with multi-stage weighted feature","T. Rui; J. Fei; P. Cui; Y. Zhou; H. Fang","College of Field Engineering, PLA Univ. of Sci. & Tech., Nanjing 210007, China; College of Field Engineering, PLA Univ. of Sci. & Tech., Nanjing 210007, China; Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; Jiangsu Institute of Commerce, Nanjing 210007, China; College of Field Engineering, PLA Univ. of Sci. & Tech., Nanjing 210007, China","2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)","","2015","","","147","150","Human head detection is an important means of pedestrian detection and counting. By now, head detection is mainly based on outline, color and template which have low recognition rate and error tolerance. Recently, deep learning has become a research hotspot in the field of pattern recognition. As a model of deep learning, convolutional neural network (CNN) performs well in the areas of image recognition and speech analysis. In this paper, a new method based on CNN was proposed. This method uses a few new twists, such as multi-stage weighted feature and connections that skip layers to integrate global shape information and local motif information. The experimental results show that the proposed method performs a higher accuracy on head detection compared with the traditional ones'.","","","10.1109/ChinaSIP.2015.7230380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230380","human head detection;deep learning;multi-stage feature;convolutional neural network","","image recognition;neural nets;object detection;pedestrians;speech processing","convolutional neural network;multistage weighted feature;human head detection;pedestrian detection;pedestrian counting;pattern recognition;CNN;image recognition;speech analysis","","2","11","","","","","IEEE","IEEE Conferences"
"Annealed dropout trained maxout networks for improved LVCSR","S. J. Rennie; P. L. Dognin; X. Cui; V. Goel","IBM Thomas J. Watson Research Center, NY, USA; IBM Thomas J. Watson Research Center, NY, USA; IBM Thomas J. Watson Research Center, NY, USA; IBM Thomas J. Watson Research Center, NY, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","5181","5185","A significant barrier to progress in automatic speech recognition (ASR) capability is the empirical reality that techniques rarely “scale”-the yield of many apparently fruitful techniques rapidly diminishes to zero as the training criterion or decoder is strengthened, or the size of the training set is increased. Recently we showed that annealed dropout-a regularization procedure which gradually reduces the percentage of neurons that are randomly zeroed out during DNN training-leads to substantial word error rate reductions in the case of small to moderate training data amounts, and acoustic models trained based on the cross-entropy (CE) criterion [1]. In this paper we show that deep Maxout networks trained using annealed dropout can substantially improve the quality of commercial-grade LVCSR systems even when the acoustic model is trained with sequence-level training criterion, and on large amounts of data.","","","10.1109/ICASSP.2015.7178959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178959","Maxout Networks;Deep Neural Networks;Deterministic Annealing;Dropout Training;Model aggregation","Training;Annealing;Acoustics;Data models;Schedules;Topology;Training data","learning (artificial intelligence);neural nets;speech recognition","annealed dropout trained maxout networks;improved LVCSR;automatic speech recognition;training criterion;decoder;training set size;regularization procedure;DNN training;deep neural nets;word error rate reduction;acoustic models;cross entropy criterion;deep maxout networks;Large Vocabulary Continuous Speech Recognition","","2","16","","","","","IEEE","IEEE Conferences"
"A deep convolutional neural network based on nested residue number system","H. Nakahara; T. Sasao","Ehime University, Japan; Meiji University, Japan","2015 25th International Conference on Field Programmable Logic and Applications (FPL)","","2015","","","1","6","A pre-trained deep convolutional neural network (DCNN) is the feed-forward computation perspective which is widely used for the embedded vision systems. In the DCNN, the 2D convolutional operation occupies more than 90% of the computation time. Since the 2D convolutional operation performs massive multiply-accumulation (MAC) operations, conventional realizations could not implement a fully parallel DCNN. The RNS decomposes an integer into a tuple of L integers by residues of moduli set. Since no pair of modulus have a common factor with any other, the conventional RNS decomposes the MAC unit into circuits with different sizes. It means that the RNS could not utilize resources of an FPGA with uniform size. In this paper, we propose the nested RNS (NRNS), which recursively decompose the RNS. It can decompose the MAC unit into circuits with small sizes. In the DCNN using the NRNS, a 48-bit MAC unit is decomposed into 4-bit ones realized by look-up tables of the FPGA. In the system, we also use binary to NRNS converters and NRNS to binary converters. The binary to NRNS converter is realized by on-chip BRAMs, while the NRNS to binary one is realized by DSP blocks and BRAMs. Thus, a balanced usage of FPGA resources leads to a high clock frequency with less hardware. The ImageNet DCNN using the NRNS is implemented on a Xilinx Virtex VC707 evaluation board. As for the performance per area GOPS (Giga operations per second) per a slice, the proposed one is 5.86 times better than the existing best realization.","","","10.1109/FPL.2015.7293933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293933","","Table lookup;Field programmable gate arrays;Convolution;Kernel;Neural networks;Clocks;Dynamic range","computer vision;digital signal processing chips;embedded systems;field programmable gate arrays;learning (artificial intelligence);neural nets;residue number systems;table lookup","nested residue number system;pretrained deep convolutional neural network;DCNN;feed-forward computation perspective;embedded vision systems;2D convolutional operation;massive multiply-accumulation operations;RNS;FPGA;MAC unit;look-up tables;binary converters;on-chip BRAMs;DSP blocks;ImageNet DCNN;Xilinx Virtex VC707 evaluation board;GOPS;giga operations per second","","26","26","","","","","IEEE","IEEE Conferences"
"Semantic Image Segmentation via Deep Parsing Network","Z. Liu; X. Li; P. Luo; C. Loy; X. Tang","Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1377","1385","This paper addresses semantic image segmentation by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. It has several appealing properties. First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing works as its special cases. Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where a single DPN model yields a new state-of-the-art segmentation accuracy of 77.5%.","","","10.1109/ICCV.2015.162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410519","","Context;Semantics;Computational modeling;Graphics processing units;Image segmentation;Labeling;Computational efficiency","approximation theory;convolution;graphics processing units;image segmentation;iterative methods;learning (artificial intelligence);Markov processes;neural nets;semantic networks","semantic image segmentation;deep parsing network;DPN;Markov random field;MRF;high-order relation;label context mixture;iterative algorithm;convolutional neural network;CNN;mean field algorithm;MF approximation;graphical processing unit;GPU","","183","39","","","","","IEEE","IEEE Conferences"
"Integration of articulatory knowledge and voicing features based on DNN/HMM for Mandarin speech recognition","Ying-Wei Tan; Wen-Ju Liu; Wei Jiang; Hao Zheng","Department of National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, China; Department of National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, China; Department of National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, China; Department of National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, China","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","Speech production knowledge has been used to enhance the phonetic representation and the performance of automatic speech recognition (ASR) systems successfully. Representations of speech production make simple explanations for many phenomena observed in speech. These phenomena can not be easily analyzed from either acoustic signal or phonetic transcription alone. One of the most important aspects of speech production knowledge is the use of articulatory knowledge, which describes the smooth and continuous movements in the vocal tract. In this paper, we present a new articulatory model to provide available information for rescoring the speech recognition lattice hypothesis. The articulatory model consists of a feature front-end, which computes a voicing feature based on a spectral harmonics correlation (SHC) function, and a back-end based on the combination of deep neural networks (DNNs) and hidden Markov models (HMMs). The voicing features are incorporated with standard Mel frequency cepstral coefficients (MFCCs) using heteroscedastic linear discriminant analysis (HLDA) to compensate the speech recognition accuracy rates. Moreover, the advantages of two different models are taken into account by the algorithm, which retains deep learning properties of DNNs, while modeling the articulatory context powerfully through HMMs. Mandarin speech recognition experiments show the proposed method achieves significant improvements in speech recognition performance over the system using MFCCs alone.","","","10.1109/IJCNN.2015.7280396","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280396","","Hidden Markov models;Production;Speech","acoustic signal processing;cepstral analysis;correlation methods;hidden Markov models;learning (artificial intelligence);natural language processing;neural nets;speech recognition","articulatory knowledge;voicing features;Mandarin speech recognition;speech production knowledge;phonetic representation;automatic speech recognition systems;ASR systems;speech production representations;acoustic signal;phonetic transcription;speech recognition lattice hypothesis;articulatory model;feature front-end;spectral harmonics correlation function;SHC function;deep neural networks;DNNs;hidden Markov models;HMMs;Mel frequency cepstral coefficients;MFCCs;heteroscedastic linear discriminant analysis;HLDA;deep learning properties","","","41","","","","","IEEE","IEEE Conferences"
"Compression Artifacts Reduction by a Deep Convolutional Network","C. Dong; Y. Deng; C. C. Loy; X. Tang","Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","576","584","Lossy compression introduces complex compression artifacts, particularly the blocking artifacts, ringing effects and blurring. Existing algorithms either focus on removing blocking artifacts and produce blurred output, or restores sharpened images that are accompanied with ringing effects. Inspired by the deep convolutional networks (DCN) on super-resolution, we formulate a compact and efficient network for seamless attenuation of different compression artifacts. We also demonstrate that a deeper model can be effectively trained with the features learned in a shallow network. Following a similar ""easy to hard"" idea, we systematically investigate several practical transfer settings and show the effectiveness of transfer learning in low level vision problems. Our method shows superior performance than the state-of-the-arts both on the benchmark datasets and the real-world use cases (i.e. Twitter).","","","10.1109/ICCV.2015.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410430","","Image coding;Feature extraction;Image restoration;Image resolution;Noise measurement;Image reconstruction;Transform coding","image coding;image resolution;image restoration;neural nets","compression artifacts reduction;deep convolutional network;lossy compression;complex compression artifacts;blocking artifacts;ringing effect;blurring;image restoration;image superresolution;transfer learning;low level vision problems","","162","35","","","","","IEEE","IEEE Conferences"
"Towards a deep feature-action architecture for robot homing","A. Altahhan","Computing Department of Coventry University, CV1 5FB, UK","2015 IEEE 7th International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)","","2015","","","205","209","This paper describes a model for robot navigation that uses an architecture similar to an actor-critic reinforcement learning architecture. Contrary to the abundance of models that use two neural networks one for the actor and one for the critic, this model sets up the actor as a layer seconded by another layer which deduce the value function. Therefore, the effect is to have similar to a critic outcome combined with the actor in one network. Hence, the model paves the way for a deep reinforcement learning architecture for future work The reward signal is back propagated through the critic then the actor. At the same time, the features layer have been deeply trained by applying a simple PCA on the whole set of images histograms acquired during the first running episode. The model is then able to shrink the whole architecture to fit a new reduced features dimension. Initial experimental result on real robot shows that the agent accomplished good level of accuracy and efficacy in reaching the goal.","","","10.1109/ICCIS.2015.7274621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7274621","","Decision support systems;Conferences;Random access memory;World Wide Web","backpropagation;mobile robots;navigation;neurocontrollers;principal component analysis","deep feature-action architecture;robot homing;robot navigation;PCA;image histogram;reward signal;value function;neural network;actor-critic reinforcement learning architecture","","","16","","","","","IEEE","IEEE Conferences"
"On Efficiency of Semantic Relation Extraction through Low-dimensional Distributed Representations for Substrings","Z. Jin; C. Shibata; J. Sun; K. Tago","Sch. of Comput. Sci., Tokyo Univ. of Technol., Tokyo, Japan; Sch. of Comput. Sci., Tokyo Univ. of Technol., Tokyo, Japan; Nat. Inst. of Inf., Tokyo, Japan; Sch. of Comput. Sci., Tokyo Univ. of Technol., Tokyo, Japan","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","1749","1754","By virtue of recent developments in machine learning techniques, higher-level information can now to be extracted from big data. To analyze big data, efficient and smart representations of data achieved by using sufficiently fast algorithms, as well as highly accurate results, are important. In this paper, we focus on extracting multiple semantic relations using light-weight processing through the efficient low-dimensional expression of substrings in text data. We propose an approach to build features for relation classification consisting of only low-dimensional vectors representing substrings between two words, called substring vectors. The experimental results show that, using efficient low-dimensional representations of data and at a small computational cost, our approach achieves a sufficiently high accuracy that is better than most existing approaches. In addition, through experiments, we ensured that mapping substrings to a sufficiently low dimensional space yields better results in terms of both accuracy and efficiency.","","","10.1109/HPCC-CSS-ICESS.2015.267","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336424","deep learning;neural network language model;word vectors","Semantics;Silicon;Accuracy;Artificial neural networks;Computational modeling;Data mining;Big data","Big Data;data analysis;learning (artificial intelligence);pattern classification;string matching;text analysis","semantic relation extraction efficiency;low-dimensional distributed representation;substring low-dimensional expression;machine learning technique;higher-level information;Big Data analysis;text data;relation classification;low-dimensional vectors;substring vectors;substring mapping","","","14","","","","","IEEE","IEEE Conferences"
"Deep multimodal semantic embeddings for speech and images","D. Harwath; J. Glass","MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, Massachusetts, 02139, U.S.A; MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, Massachusetts, 02139, U.S.A","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","237","244","In this paper, we present a model which takes as input a corpus of images with relevant spoken captions and finds a correspondence between the two modalities. We employ a pair of convolutional neural networks to model visual objects and speech signals at the word level, and tie the networks together with an embedding and alignment model which learns a joint semantic space over both modalities. We evaluate our model using image search and annotation tasks on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000 spoken captions using Amazon Mechanical Turk.","","","10.1109/ASRU.2015.7404800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404800","Neural networks;multimodal semantic embeddings","Spectrogram;Semantics;Visualization;Speech;Neural networks;Image segmentation;Natural languages","image processing;learning (artificial intelligence);neural nets;speech recognition","deep multimodal semantic embeddings;speech embedding;image embedding;convolutional neural networks;visual object model;speech signal model;embedding model;alignment model;image search task;image annotation task;Flickr8k dataset;Amazon Mechanical Turk","","10","22","","","","","IEEE","IEEE Conferences"
"Decision tree based data classification for marine wireless communication","R. A. Roy; J. P. Nair; E. Sherly","IIITM-K, Technopark, Trivandrum, India; IIITM-K, Technopark, Trivandrum, India; IIITM-K, Technopark, Trivandrum, India","2015 International Conference on Computing and Network Communications (CoCoNet)","","2015","","","633","638","Wireless data communication along with data classification techniques has got wider acceptance in various marine wireless applications. This paper exploits the power of machine learning algorithm to classify wireless communication dataset for effective decision making in marine sector. Fishing is among the most risky of professions in the world because once out on the sea, the fishermen are subject to various oceanographic conditions. The unreliable communication between the fishing fleets and to the shore is a serious problem when they face emergency situations like bad weather, border attacks, natural calamities etc. This paper is intended to develop an algorithm to determine the most influential parameters by considering signal strength, wind speed etc. which helps to track, classify and disseminate information to the fishing fleets while they are in deep sea. A decision tree based classification is proposed to find the best node based on the signal strength and the environmental conditions and the scenario has been simulated using NS2 platform. An ensemble based learning algorithm with bagging and adaptive boosting in C4.5 is also employed for improving the performance. The performance comparison has been done and the result shows that the boosted decision tree algorithm has got highest classification accuracy of 95.73%.","","","10.1109/CoCoNet.2015.7411255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7411255","Data classification;marine wireless communication;data mining;mobility network;decision tree;NS2;End-to-End packet delivery;ensemble learning;bagging;adaptive boosting","Decision trees;Classification algorithms;Training;Ocean temperature;Data mining;Wireless communication;Wind speed","decision trees;learning (artificial intelligence);marine communication;marine engineering;pattern classification","data classification;marine wireless communication;machine learning algorithm;marine sector decision making;decision tree based classification;signal strength;environmental conditions;NS2 platform;ensemble based learning algorithm;bagging;adaptive boosting;C4.5;boosted decision tree algorithm;classification accuracy","","1","17","","","","","IEEE","IEEE Conferences"
"Learning Cross Space Mapping via DNN Using Large Scale Click-Through Logs","W. Yu; K. Yang; Y. Bai; H. Yao; Y. Rui","Harbin Institute of Technology, Harbin, Heilongjiang; Microsoft Research, Beijing, China; Harbin Institute of Technology, Harbin, Heilongjiang; Harbin Institute of Technology, Harbin, Heilongjiang; Microsoft Research, Beijing, China","IEEE Transactions on Multimedia","","2015","17","11","2000","2007","The gap between low-level visual signals and high-level semantics has been progressively bridged by continuous development of deep neural network (DNN). With recent progress of DNN, almost all image classification tasks have achieved new records of accuracy. To extend the ability of DNN to image retrieval tasks, we proposed a unified DNN model for image-query similarity calculation by simultaneously modeling image and query in one network. The unified DNN is named the cross space mapping (CSM) model, which contains two parts, a convolutional part and a query-embedding part. The image and query are mapped to a common vector space via these two parts respectively, and image-query similarity is naturally defined as an inner product of their mappings in the space. To ensure good generalization ability of the DNN, we learn weights of the DNN from a large number of click-through logs which consists of 23 million clicked image-query pairs between 1 million images and 11.7 million queries. Both the qualitative results and quantitative results on an image retrieval evaluation task with 1000 queries demonstrate the superiority of the proposed method.","","","10.1109/TMM.2015.2480340","National Natural Science Foundation of China; Key Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273936","Cross space mapping;deep neural network;image retrieval","Image retrieval;Training;Visualization;Semantics;Neural networks;Feature extraction;Joints","image classification;image retrieval;neural nets","cross space mapping;DNN;large scale click-through logs;low-level visual signals;high-level semantics;deep neural network;image classification tasks;image retrieval tasks;image-query similarity calculation;CSM model;query-embedding part;common vector space;image-query similarity;image retrieval evaluation task","","3","30","","","","","IEEE","IEEE Journals"
"Trust inference in online social networks","A. Papaoikonomou; M. Kardara; T. Varvarigou","National Technical University of Athens, 9 Iroon Polytechniou Str., 15773, Zografou, Greece; National Technical University of Athens, 9 Iroon Polytechniou Str., 15773, Zografou, Greece; National Technical University of Athens, 9 Iroon Polytechniou Str., 15773, Zografou, Greece","2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","","2015","","","600","604","We study the problem of trust inference in signed social networks, in which, in addition to rating items, users can also indicate their disposition towards each other through directional signed links. We explore the problem in a semi-supervised setting, where given a small fraction of signed edges we classify the remaining edges by leveraging contextual information (i.e. the users' ratings). In order to model user behavior, we use deep learning algorithms i.e. a variation of Restricted Boltzmann machine and Autoencoders for user encoding and edge classification respectively. We evaluate our approach on a large-scale real-world dataset and show that it outperforms state-of-the art methods.","","","10.1145/2808797.2809418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403603","signed social networks;edge classification;trust;restricted boltzmann machines;autoencoders","Social network services;Machine learning;Binary codes;Encoding;Training;Classification algorithms;Art","Boltzmann machines;inference mechanisms;learning (artificial intelligence);security of data;social networking (online)","trust inference;online social network;contextual information;deep learning algorithm;restricted Boltzmann machine;autoencoder;user encoding;edge classification","","1","19","","","","","IEEE","IEEE Conferences"
"Convolutional neural network for 3D object recognition based on RGB-D dataset","J. Wang; J. Lu; W. Chen; X. Wu","School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, China","2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA)","","2015","","","34","39","Object recognition is fundamental to some high-level computer vision tasks such as image segmentation, object tracking and behavior analysis. The main objective of object recognition is to answer whether a specified object exists in a given image, so extracting representative features from images and training a right classifier become the key techniques in this area. In this paper, we use convolutional neural network model to learn features from RGB-D dataset which are then given to a linear SVM classifier to classify objects. As the number of images in RGB-D dataset is not big enough to retrain a deep neural network with high feature extraction accuracy, we fine-tune the caffe model which was trained on approximately 1.2 million RGB images from ImageNet database. While the framework of depth image is intrinsically different form RGB image, we transform the depth image into three channels and use the same method with the RGB image to extract features. We can achieve a classification accuracy of 91.35% which is much better than the state of the art.","","","10.1109/ICIEA.2015.7334080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334080","Convolutional Neural Networks;fine-tuning;three-channel;SVM;RGB-D object dataset","Feature extraction;Kernel;Training;Neural networks;Accuracy;Machine learning;Object recognition","computer vision;feature extraction;image classification;image colour analysis;image segmentation;learning (artificial intelligence);neural nets;object recognition;object tracking;support vector machines","3D object recognition;RGB-D dataset;high-level computer vision;image segmentation;object tracking;behavior analysis;representative feature extraction;classifier training;convolutional neural network model;feature learning;linear SVM classifier;object classification;deep neural network;caffe model fine-tuning;RGB images;ImageNet database;depth image","","7","19","","","","","IEEE","IEEE Conferences"
"Weather classification with deep convolutional neural networks","M. Elhoseiny; S. Huang; A. Elgammal","Rutgers University, Piscataway, NJ, 08854, USA; Chongqing University, Chongqing, 400044, P.R.C; Rutgers University, Piscataway, NJ, 08854, USA","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","3349","3353","In this paper, we study weather classification from images using Convolutional Neural Networks (CNNs). Our approach outperforms the state of the art by a huge margin in the weather classification task. Our approach achieves 82.2% normalized classification accuracy instead of 53.1% for the state of the art (i.e., 54.8% relative improvement). We also studied the behavior of all the layers of the Convolutional Neural Networks, we adopted, and interesting findings are discussed.","","","10.1109/ICIP.2015.7351424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351424","Deep Learning;Weather Classification;Image Classification;Convolutional Neural Networks;Image Convolutional Activation Feature","Meteorology;Training;Neural networks;Clouds;Support vector machines;Sensors;Testing","convolution;geophysics computing;image classification;meteorology;neural nets","weather classification;image classification;convolutional neural network;CNN","","21","16","","","","","IEEE","IEEE Conferences"
"Selective and compressive sensing for energy-efficient implantable neural decoding","A. Wang; C. Song; X. Xu; F. Lin; Z. Jin; W. Xu","CSE Dept., SUNY at Buffalo, NY, USA; CSE Dept., SUNY at Buffalo, NY, USA; CSE Dept., SUNY at Buffalo, NY, USA; CSE Dept., SUNY at Buffalo, NY, USA; ECE Dept., SUNY at Binghamton, NY, USA; CSE Dept., SUNY at Buffalo, NY, USA","2015 IEEE Biomedical Circuits and Systems Conference (BioCAS)","","2015","","","1","4","The spike classification is a critical step in implantable neural decoding. The energy efficiency issue in the sensor node is a big challenge in the entire system. Compressive sensing (CS) provides a potential way to tackle this problem. However, the overhead of signal reconstruction constrains the compression in sensor node and analysis in remote server. In this paper, we design a new selective CS architecture for wireless implantable neural decoding. We implement all the signal analysis on the compressed domain. To achieve better energy efficiency, we propose a two-stage classification procedure, including a coarse-grained screening module with softmax regression and a fine-grained analysis module based on deep learning. The screening module completes the low-effort classification task in the front-end and transmits the compressed data of high-effort task to remote server for fine-grained analysis. Experimental results indicate that our selective CS architecture can gain more than 50% energy savings, yet keeping the high accuracy as state-of-the-art CS architectures.","","","10.1109/BioCAS.2015.7348375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348375","","Wireless communication;Decoding;Servers;Wireless sensor networks;Quantization (signal);Machine learning;Compressed sensing","compressed sensing;electroencephalography;learning (artificial intelligence);medical signal processing;neurophysiology;regression analysis;signal classification;signal reconstruction;wireless sensor networks","compressive sensing;energy-efficient implantable neural decoding;spike classification;energy efficiency;sensor node;signal reconstruction;remote server;selective CS architecture;wireless implantable neural decoding;signal analysis;compressed domain;two-stage classification procedure;coarse-grained screening module;softmax regression;fine-grained analysis module;deep learning;classification task;high-effort task","","5","10","","","","","IEEE","IEEE Conferences"
"An Intention-Topic Model Based on Verbs Clustering and Short Texts Topic Mining","T. Lu; S. Hou; Z. Chen; L. Cui; L. Zhang","Sch. of Inf. Sci. & Eng., Univ. of Jinan, Jinan, China; Libr. of Rizhao Polytech., Rizhao, China; Sch. of Inf. Sci. & Eng., Univ. of Jinan, Jinan, China; NA; Sch. of Inf. Sci. & Eng., Univ. of Jinan, Jinan, China","2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing","","2015","","","837","842","Microblog, Twitter, status messages, the classified information website and so on are experiencing explosive growth with the development of web2.0, people prefer to use short texts to express their intentions and activities. Yet, when people submit some requirements through short texts, they hope to get a feedback which can help them to solve their problems rather than relevant content. Sometimes people need corresponding intention rather than similar content. However, current researches cannot solve the problem well. In this paper, we propose an intentiontopic model: Verb-Biterm Topic Model(V-BTM), which aims at corresponding intention matching. Intention is expressed by verbs and topic is expressed by BTM. Intention is the action of people want to express and topic is the goal of the intention. The key of the model is that people tend to express their intention with verbs and tend to express the topic with non-verb. In this model, firstly, we distinguish intentions with the verb clustering with the help of word2vec which is a deep learning tool. Secondly, we mine the topic using Biterm Topic Model(BTM) on the data without verbs. We carry out experiments on real-world short text collections. The results demonstrate that our approach can get better verb clustering and mine more coherent topics. Furthermore, the new model can be the base of our future researches.","","","10.1109/CIT/IUCC/DASC/PICOM.2015.124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363164","","Data mining;Clustering algorithms;Algorithm design and analysis;Machine learning;Data models;Semantics;Probability distribution","data mining;learning (artificial intelligence);pattern clustering;pattern matching;text analysis","V-BTM;real-world short text collections;short text topic mining;deep learning tool;word2vec;verb clustering;intention matching;Verb-Biterm topic model;intention-topic model;Web 2.0;classified information Web site;status messages;Twitter;microblog","","1","27","","","","","IEEE","IEEE Conferences"
"Neuromorphic hybrid RRAM-CMOS RBM architecture","M. Suri; V. Parmar; A. Kumar; D. Querlioz; F. Alibart","Department of Electrical Engineering, Indian Institute of Technology - Delhi, India; Department of Electrical Engineering, Indian Institute of Technology - Delhi, India; Department of Electrical Engineering, Indian Institute of Technology - Delhi, India; Institut IEF-CNRS, Orsay, France; IEMN- CNRS, 596652, Villeneuv d'Ascq, France","2015 15th Non-Volatile Memory Technology Symposium (NVMTS)","","2015","","","1","6","Restricted Boltzmann Machines (RBMs) offer a key methodology to implement Deep Learning paradigms. This paper presents a novel approach for realizing a hybrid RRAM-CMOS RBM architecture. In our proposed hybrid RBM architecture, HfOx based (filamentary-type switching) RRAM devices are extensively used to implement: (i) Synapses (ii) Internal neuron-state storage and (iii) Stochastic neuron activation function. To validate the proposed scheme we simulated our RBM architecture for classification and reconstruction of hand-written digits on a reduced MNIST dataset of 6000 images. Contrastive-divergence (CD) specially optimized for RRAM devices was used to drive the synaptic weight update mechanism. Total required size of the RRAM matrix in the simulated application is of the order of ~ 0.4 Mb. Peak classification accuracy of 92 %, and an average accuracy of ~ 89 % was obtained over 100 training epochs. Average number of RRAM switching events was ~ 14 million/per epoch.","","","10.1109/NVMTS.2015.7457484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457484","RBM;OXRAM;MNIST;Deep Learning;Stochasticity;cycle to cycle variability","Neurons;Hafnium compounds;Computer architecture;Resistance;Switches;Integrated circuit modeling;Hardware","Boltzmann machines;CMOS memory circuits;handwriting recognition;neural chips;resistive RAM","RRAM switching events;classification accuracy;RRAM matrix;synaptic weight update mechanism;contrastive-divergence;reduced MNIST dataset;hand-written digit classification;hand-written digit reconstruction;stochastic neuron activation function;internal neuron-state storage;synapses;filamentary-type switching;hafnium oxide-based RRAM devices;deep learning paradigm;restricted Boltzmann machines;neuromorphic hybrid RRAM-CMOS RBM architecture","","12","12","","","","","IEEE","IEEE Conferences"
"Fast Convolution Operations on Many-Core Architectures","S. Li; Y. Zhang; C. Xiang; L. Shi","State Key Lab. of Comput. Archit., Inst. of Comput. Technol., Beijing, China; State Key Lab. of Comput. Archit., Inst. of Comput. Technol., Beijing, China; Sch. of Inf. Eng., Zhengzhou Univ., Zhengzhou, China; Sch. of Inf. Eng., Zhengzhou Univ., Zhengzhou, China","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","316","323","Convolution operations have been widely used in many important application domains, such as deep learning and computer vision, in which convolution is always the most time-consuming part. High computational throughput and memory bandwidth make many-core architectures the promising targets to accelerate these applications. In this paper, we implement and optimize different convolution operations, including 1D convolution, 2D convolution and multi-channel 2D convolution executed in mini-batch mode, on both GPU and Intel MIC many-core architectures. We find out that the performance bottleneck of 1D and 2D convolutions is on registers rather than local memory or L1/L2 cache, and therefore, register tiling is used to improve the performance. In addition, we present a novel solution for multi-channel 2D convolution, in which convolution is conducted on images directly instead of being translated to matrix multiplication, and the data reuse of the algorithm is fully exploited. We further summarize the parameters of autotuning for multichannel 2D convolution and prune the search space based on heuristics. The experimental results show that, for the large filter size, our solution gets up to 33% performance improvement over cuDNN-v2 and up to 28% over clBLASbased implementation, on GTX TITAN and AMD W8000 respectively. On Intel MIC, our solution gets up to 25% of the theoretical peak performance.","","","10.1109/HPCC-CSS-ICESS.2015.94","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336182","Convolution;GPU;Intel MIC;OpenCL;Deep learning;Computer vision","Convolution;Instruction sets;Registers;Computer architecture;Microwave integrated circuits;Filtering algorithms;Neural networks","computer vision;graphics processing units;matrix multiplication;multiprocessing systems","manycore architecture;convolution operation;memory bandwidth;multichannel 2D convolution;GPU architecture;Intel MIC many-core architecture;matrix multiplication;GTX TITAN;AMD W8000;computer vision;deep learning","","4","11","","","","","IEEE","IEEE Conferences"
"Convolutional Neural Networks in Automatic Recognition of Trans-differentiated Neural Progenitor Cells under Bright-Field Microscopy","B. Jiang; X. Wang; J. Luo; X. Zhang; Y. Xiong; H. Pang","Guangzhou Inst. of Biomed. & Health, Guangzhou, China; Guangzhou Inst. of Biomed. & Health, Guangzhou, China; Guangzhou Inst. of Biomed. & Health, Guangzhou, China; Guangzhou Inst. of Biomed. & Health, Guangzhou, China; Guangzhou Inst. of Biomed. & Health, Guangzhou, China; Guangzhou Inst. of Biomed. & Health, Guangzhou, China","2015 Fifth International Conference on Instrumentation and Measurement, Computer, Communication and Control (IMCCC)","","2015","","","122","126","The study of cell morphology changes leads the investigation of the cell fate decision and its function. Bright-field imaging analysis allow us to use a labeling free and non-invasive approach to measure the morphological dynamics during cellular reprogramming, which includes induced pluripotent stem cells (iPSCs), and trans-differentiated neural progenitor cells (NPCs) from somatic cell source. However, the traditional method to study the NPC differentiation and its related function involves staining, and cell lysis, which can not materialized further for the clinical uses. In order to automatically, non-invasively, non-labelled analyze and cultivate cells, a system classifying NPCs under bright-field microscopic imaging is necessary. In this paper, we propose a novel recognition system based on convolutional neural networks, which could pre-process images and classify NPCs and non-NPCs. Experimental results prove that the proposed system provides a new tool for fundamental research in iPSCs and NPCs based generation medicine.","","","10.1109/IMCCC.2015.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405812","machine learning;deep learning;convolutional neural networks;trans-differentiated neural progenitor cells;non-invasive;non-labelled;bright-field microscopy","Microscopy;Feature extraction;Image recognition;Machine learning;Biological neural networks;Electronic mail;Morphology","cellular biophysics;image classification;medical image processing;neural nets;optical microscopy","automatic recognition;trans-differentiated neural progenitor cells;bright-field microscopic imaging analysis;convolutional neural networks;image pre-processing;NPC classification;nonNPC classification","","6","17","","","","","IEEE","IEEE Conferences"
"An In-Depth Context-Awareness Framework for Pervasive Video Cloud","W. Zhang; P. Duan; L. Chen","Dept. of Software Eng., China Univ. of Pet., Qingdao, China; Dept. of Software Eng., China Univ. of Pet., Qingdao, China; Dept. of Software Eng., China Univ. of Pet., Qingdao, China","2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)","","2015","","","543","549","We claim that context-awareness for big data should be more in-depth than that of classical one, due to complexities of big data. Intelligent video data processing based on video cloud plays an important role for some applications such as public security and transportation. The existing work on context-awareness can not work properly on pervasive video cloud due to the intrinsic complexities of big video data. Therefore, in this paper we propose an in-depth context-awareness framework for pervasive video cloud in order to know the underlying contexts in big video data, based on deep learning techniques. We have conducted initial evaluations to show the effectiveness of the proposed approach, including the prediction of workload for cloud nodes, and the recognition of targets in the video at real time.","","","10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518289","","Vehicles;Context;Cloud computing;Machine learning;Image recognition;Feature extraction;Sparks","Big Data;cloud computing;learning (artificial intelligence);object detection;ubiquitous computing;video signal processing","context-awareness framework;pervasive video cloud;big video data;deep learning techniques;workload prediction;cloud nodes;target recognition","","","22","","","","","IEEE","IEEE Conferences"
"A context-sensitive-chunk BPTT approach to training deep LSTM/BLSTM recurrent neural networks for offline handwriting recognition","K. Chen; Z. Yan; Q. Huo","Department of Electronics Science and Technology, University of Science and Technology of China, Hefei, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","411","415","We propose a context-sensitive-chunk based back-propagation through time (BPTT) approach to training deep (bidirectional) long short-term memory ((B)LSTM) recurrent neural networks (RNN) that splits each training sequence into chunks with appended contextual observations for character modeling of offline handwriting recognition. Using short context-sensitive chunks in both training and recognition brings following benefits: (1) the learned (B)LSTM will model mainly local character image dependency and the effect of long-range language model information reflected in training data is reduced; (2) mini-batch based training on GPU can be made more efficient; (3) low-latency BLSTM-based handwriting recognition is made possible by incurring only a delay of a short chunk rather than a whole sentence. Our approach is evaluated on IAM offline handwriting recognition benchmark task and performs better than the previous state-of-the-art BPTT-based approaches.","","","10.1109/ICDAR.2015.7333794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333794","","Hidden Markov models;Training;Recurrent neural networks;Context modeling;Computational modeling;Buildings","backpropagation;graphics processing units;handwriting recognition;recurrent neural nets","context-sensitive-chunk BPTT approach;deep BLSTM recurrent neural network;deep LSTM recurrent neural network;offline handwriting recognition;context-sensitive-chunk based backpropagation-through-time approach;context-sensitive-chunk-based BPTT approach;recurrent neural network;RNN;character modeling;local character image dependency;language model information;GPU;BLSTM-based handwriting recognition;lAM offline handwriting recognition benchmark","","11","26","","","","","IEEE","IEEE Conferences"
"Sparse low-rank fusion based deep features for missing modality face recognition","M. Shao; Z. Ding; Y. Fu","Department of Electrical and Computer Engineering, Northeastern University, USA; Department of Electrical and Computer Engineering, Northeastern University, USA; Department of Electrical and Computer Engineering, Northeastern University, USA","2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)","","2015","1","","1","6","Multi-modality data recently attract more and more research attention. In this paper, we concentrate on a very interesting problem - image classification with missing modality. Specifically, only images in one modality as well as a relevant auxiliary database are accessible during the training phase, which is significantly different from general image classification under the same modality. To this end, we propose a novel framework integrating multiple deep autoencoders with bagging strategy. For each autoencoder, we generate its input by randomly sampling data from other modality and the auxiliary database, and enforce its output to lie in a common feature space through Robust PCA. Finally, a novel sparse low-rank feature fusion approach is proposed in the test phase to integrate multiple features learned from different autoencoders, followed by a decision voting. Extensive experiments on two databases, i.e., BUAA-NIRVIS, Oulu-CASIA NIRVIS databases demonstrate the effectiveness of the proposed framework when there is only one modality available for training.","","","10.1109/FG.2015.7163103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163103","","Databases;Bagging;Training;Robustness;Testing;Principal component analysis;Visualization","face recognition;feature extraction;image classification;image fusion;principal component analysis;sampling methods","sparse low-rank fusion based deep features;missing modality face recognition;multimodality data;auxiliary database;training phase;image classification;multiple deep autoencoders;bagging strategy;random data sampling;robust PCA;sparse low-rank feature fusion approach;decision voting;BUAA-NIRVIS;Oulu-CASIA;NIRVIS databases","","1","29","","","","","IEEE","IEEE Conferences"
"Learning temporal features using LSTM-CNN architecture for face anti-spoofing","Z. Xu; S. Li; W. Deng","Beijing University of Posts and Telecommunication, No 10, Xitucheng Road, Haidian District, Beijing, PR China; Beijing University of Posts and Telecommunication, No 10, Xitucheng Road, Haidian District, Beijing, PR China; Beijing University of Posts and Telecommunication, No 10, Xitucheng Road, Haidian District, Beijing, PR China","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","","2015","","","141","145","Temporal features is important for face anti-spoofing. Unfortunately existing methods have limitations to explore such temporal features. In this work, we propose a deep neural network architecture combining Long Short-Term Memory (LSTM) units with Convolutional Neural Networks (CNN). Our architecture works well for face anti-spoofing by utilizing the LSTM units' ability of finding long relation from its input sequences as well as extracting local and dense features through convolution operations. Our best model shows significant performance improvement over general CNN architecture (5.93% vs. 7.34%), and hand-crafted features (5.93% vs. 10.00%) on CASIA dataset.","","","10.1109/ACPR.2015.7486482","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486482","","Face;Computer architecture;Feature extraction;Logic gates;Microprocessors;Neural networks;Video sequences","face recognition;feature extraction;neural nets","temporal features;LSTM-CNN architecture;face anti-spoofing;deep neural network architecture;long short-term memory units;convolutional neural networks;feature extraction;CASIA dataset","","25","20","","","","","IEEE","IEEE Conferences"
"Improving activity recognition with context information","L. Zhang; X. Wu; D. Luo","Key Lab of Machine Perception (Ministry of Education), Speech and Hearing Research Center, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Key Lab of Machine Perception (Ministry of Education), Speech and Hearing Research Center, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Key Lab of Machine Perception (Ministry of Education), Speech and Hearing Research Center, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China","2015 IEEE International Conference on Mechatronics and Automation (ICMA)","","2015","","","1241","1246","Activity recognition driven by sensor data has been heavily focused on in recent years, especially as wearable sensors become more common and popular personal equipments. As known, recognizing individual activities is one of the typical machine learning applications, which includes several basic phases, such as data collection, feature extraction, model training and performance evaluation. Originally, after data being collected and preprocessed, features are manually extracted for model training. As this may lead to a time-consuming and boring job, feature learning approaches were investigated and received a great success, especially when Deep Neural Networks, a powerful model for feature representation, were employed into this task. That is, classifier could be established with raw data. Those features, no matter manually selected or learned from raw data, depend on separate frames that were split from previous collected long data. Since human activity represented by sensor data is actually a time series signal, context information plays an important role, how to take advantage of knowledge implied in the inter-frames becomes significant. Unlike previous attempt that only enlarges the length of each frame, in this research, a new method that models context information with neighboring frames is investigated for activity recognition tasks. Based on the Daily and Sports Activities, a publicly available dataset, experiments are performed with three typical classifiers for activity recognition. The results show that the proposed method could significantly improve the recognition performance and the more the context information are considered, the higher the recognition accuracy will be.","","","10.1109/ICMA.2015.7237663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7237663","activity recognition;sensor data;accelerometer;context information","Feature extraction;Context;Accelerometers;Support vector machines;Acceleration;Training;Standards","feature extraction;feature selection;image classification;image motion analysis;learning (artificial intelligence);neural nets","activity recognition;context information;sensor data;wearable sensors;personal equipments;machine learning applications;data collection;feature extraction;model training;performance evaluation;feature learning;deep neural networks;feature representation;classifier;features selection;human activity;time series signal;neighboring frames;daily activities;sports activities;recognition performance","","4","20","","","","","IEEE","IEEE Conferences"
"Proteus: A scalable, flexible and extensible multi-classifier framework","D. Winiarski; Y. Coady","Department of Computer Science, University of Victoria, British Columbia, Canada; Department of Computer Science, University of Victoria, British Columbia, Canada","2015 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)","","2015","","","501","506","Though the popularity and demand for machine learning infrastructures is soaring in this age of “big data”, general purpose configuration and deployment strategies are still in their infancy. This paper presents Proteus, a flexible and extensible framework allowing different machine learning algorithms to be introduced in a plug-and-play manner in order to be evaluated. Proteus enables domain experts to more easily compare, contrast, and even combine results from classifiers including Deep Learning, GLM, GBM, Naive Bayes, Random Forest, SVM and Linear Regression. Leveraging this design, it is easier to explore the possibility that a combination of multiple classifiers may be the best approach to guaranteeing high accuracy. A case study involving 6 months of mouse-movement data from 5 patients with a Clinical Dementia Rating (CDR) of 0 (control group) and 5 patients with a CDR of 0.5 (considered a high impairment level) identifies the costs and benefits of this engineering effort towards a scalable, flexible and extensible architecture for multi-classifier analysis.","","","10.1109/PACRIM.2015.7334888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334888","","Pipelines;Algorithm design and analysis;Sparks;Sensors;Computer architecture;Libraries;Data analysis","learning (artificial intelligence);medical computing;regression analysis;support vector machines","Proteus;multiclassifier framework;machine learning infrastructures;deep learning;GLM;GBM;naive Bayes;random forest;linear regression;clinical dementia rating;multiclassifier analysis","","","7","","","","","IEEE","IEEE Conferences"
"Chinese microblog sentiment classification based on convolution neural network with content extension method","X. Sun; F. Gao; C. Li; F. Ren","School of Computer and Information, Hefei University of Technology, Hefei, China 230009; School of Computer and Information, Hefei University of Technology, Hefei, China 230009; IFLYTEK Co. Ltd., Hefei, China 230088; Tokushima University, Tokushima, Japan 770800","2015 International Conference on Affective Computing and Intelligent Interaction (ACII)","","2015","","","408","414","Related research for sentiment analysis on Chinese microblog is aiming at analyzing the emotion of posters. This paper presents a content extension method that combines post with its' comments into a microblog conversation for sentiment analysis. A new convolutional auto encoder which can extract contextual sentiment information from microblog conversation of the post is proposed. Furthermore, a DBN model, which is composed by several layers of RBM(Restricted Boltzmann Machine) stacked together, is implemented to extract some higher level feature for short text of a post. These RBM layers can encoder observed short text to learn hidden structures or semantics information for better feature representation. A ClassRBM (Classification RBM) layer, which is stacked on top of RBM layers, is adapted to achieve the final sentiment classification. The experiment results demonstrate that, with proper structure and parameter, the performance of the proposed deep learning method on sentiment classification is better than state-of-the-art surface learning models such as SVM or NB, which also proves that DBN is suitable for short-length document classification with the proposed feature dimensionality extension method.","","","10.1109/ACII.2015.7344603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344603","DBN;RBM;ClassRBM;Social Network;Microblog Conversation;Sentiment Analysis","Feature extraction;Semantics;Sentiment analysis;Context;Visualization;Neural networks;Data mining","Boltzmann machines;emotion recognition;feature extraction;learning (artificial intelligence);natural language processing;neural nets;pattern classification;text analysis;Web sites","Chinese Microblog sentiment classification;convolution neural network;content extension method;sentiment analysis;poster emotion;microblog conversation;contextual sentiment information extraction;DBN model;restricted Boltzmann machine;feature representation;classification RBM layer;ClassRBM layer;deep learning method;surface learning models;document classification;feature dimensionality extension method","","5","27","","","","","IEEE","IEEE Conferences"
"Comparative analysis of bagging, stacking and random subspace algorithms","P. Shrivastava; M. Shukla","Computer Science and Information Technology, Jayoti Vidyapeeth Women's University, Jaipur, India; Computer Science and Engineering, Sunder deep group of Institution, Ghaziabad, U.P., India","2015 International Conference on Green Computing and Internet of Things (ICGCIoT)","","2015","","","511","516","Data mining is a powerful new technology and is an important area of science and engineering. In this paper show that the comparing results using bagging, stacking and random subspace algorithms on forest fire data set in to WEKA data mining suite. We compare better results of these methods and improve classification accuracy. Performance results show that the classifiers built. These classifiers are more accurate than that produced by the classification methods. Finally, we are explaining the combining technique for increasing accuracy on the data set is presented. Experimental results are based on minimum time and minimum error rates.","","","10.1109/ICGCIoT.2015.7380518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7380518","Data mining;Random subspace;Bagging;Stacking;WEKA Tool;Forest fire data set;Machine learning;combining classifier","Stacking;Bagging;Algorithm design and analysis;Software algorithms;Monitoring;Biomedical monitoring;Software","data mining;pattern classification;random processes","stacking algorithms;random subspace algorithms;bagging algorithms;forest fire data set;data mining technology;WEKA data mining suite;classification accuracy;classifiers;minimum time;minimum error rates","","","26","","","","","IEEE","IEEE Conferences"
"Accurate prediction of docked protein structure similarity using neural networks and restricted Boltzmann machines","R. Farhoodi; B. Akbal-Delibas; N. Haspel","Department of Computer Science, University of Massachusetts Boston, USA 02125; Department of Computer Science, University of Massachusetts Boston, USA 02125; Department of Computer Science, University of Massachusetts Boston, USA 02125","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","1296","1303","One of the major challenges for protein-protein docking is to accurately discriminate native-like structures from false-positives. While there is an agreement on the existence of a relationship between various favorable intermolecular interactions (e.g., Van der Waals, electrostatic, desolvation forces, etc.) and the similarity of a conformation to its native structure, the exact nature of this relationship is not clear. Different docking algorithms often formulate this relationship as a weighted sum of selected terms and calibrate their weights against a training set to evaluate and rank candidate complexes. Despite improvement in the predictive abilities of recent docking methods, even state-of-the-art methods often fail to predict the binding of many complexes and still output a large number of false positive complexes. We propose a novel machine learning approach that not only ranks candidate structures relative to each other, but also predicts how similar each candidate is to the native conformation. We trained a two-layer neural network, a deep neural network and a network of Restricted Boltzmann Machines against extensive datasets of unbound complexes. We tested these methods with a set of candidate structures. Our method is able to predict the RMSDs of unbound docked complexes with a very small, often <; 1.5Å error margin.","","","10.1109/BIBM.2015.7359866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359866","Protein docking and refinement;scoring functions;RMSD prediction;machine learning;neural networks","Training","bioinformatics;Boltzmann machines;learning (artificial intelligence);molecular biophysics;proteins","docked protein structure similarity prediction;protein-protein docking;native-like structures;intermolecular interactions;docking algorithms;weighted sum;false positive complexes;machine learning approach;native conformation;two-layer neural network;deep neural network;restricted Boltzmann machines;unbound complexes;unbound docked complexes","","2","33","","","","","IEEE","IEEE Conferences"
"Robust Real-Time Load Profile Encoding and Classification Framework for Efficient Power Systems Operation","E. D. Varga; S. F. Beretka; C. Noce; G. Sapienza","Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Schneider Electric DMS NS, Novi Sad, Serbia; Enel, Italy; Enel, Italy","IEEE Transactions on Power Systems","","2015","30","4","1897","1904","Neatly represented and properly classified load profiles are fundamental to many control optimization techniques of modern power systems, especially in a distribution area. This paper presents a novel load profile management software framework for boosting the efficiency of power systems operation. The proposed framework encodes and classifies load profiles in real-time. Imperfections as well as time-shifts in the input (measured power consumption levels) are tolerated by the suggested system, thus always providing accurate, fast and reliable output. The framework's fully component based structure allows easy customizations of the encoding as well as the classification engines. The default encoding engine is based on an artificial neural network, a variant known as a deep learning auto-encoder comprised from stacked sparse auto-encoders. The default classifier engine is based on an implementation of a locality sensitive hashing algorithm. The developed methodology was tested on the real case of a set of anonymous customers supplied by a power distribution company. The paper also contains an elaboration about the experiences gained during the design, implementation and testing phase of this system as well as a detailed engineering use case of the framework's applicability.","","","10.1109/TPWRS.2014.2354552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898891","Classification algorithms;load modeling;multidimensional systems;multi-layer neural network;multilevel systems;real-time systems;unsupervised learning","Encoding;Real-time systems;Biological neural networks;Training;Feature extraction;Vectors;Engines","cryptography;encoding;file organisation;learning (artificial intelligence);neural nets;optimisation;power distribution reliability;power engineering computing;power system management","robust real-time load profile encoding;power system operation;control optimization technique;load profile management software framework;classification engine framework;artificial neural network;deep learning autoencoder;stacked sparse autoencoder;locality sensitive hashing algorithm;power distribution company;power consumption","","25","41","","","","","IEEE","IEEE Journals"
"Learned features versus engineered features for semantic video indexing","M. Budnik; E. Gutierrez-Gomez; B. Safadi; G. Quénot","Univ. Grenoble Alpes, LIG, F-38000 Grenoble, France; Univ. Grenoble Alpes, LIG, F-38000 Grenoble, France; Univ. Grenoble Alpes, LIG, F-38000 Grenoble, France; Univ. Grenoble Alpes, LIG, F-38000 Grenoble, France","2015 13th International Workshop on Content-Based Multimedia Indexing (CBMI)","","2015","","","1","6","In this paper, we compare “traditional” engineered (hand-crafted) features (or descriptors) and learned features for content-based semantic indexing of video documents. Learned (or semantic) features are obtained by training classifiers for other target concepts on other data. These classifiers are then applied to the current collection. The vector of classification scores is the new feature used for training a classifier for the current target concepts on the current collection. If the classifiers used on the other collection are of the Deep Convolutional Neural Network (DCNN) type, it is possible to use as a new feature not only the score values provided by the last layer but also the intermediate values corresponding to the output of all the hidden layers. We made an extensive comparison of the performance of such features with traditional engineered ones as well as with combinations of them. The comparison was made in the context of the TRECVid semantic indexing task. Our results confirm those obtained for still images: features learned from other training data generally outperform engineered features for concept recognition. Additionally, we found that directly training SVM classifiers using these features does significantly better than partially retraining the DCNN for adapting it to the new data. We also found that, even though the learned features performed better that the engineered ones, the fusion of both of them perform significantly better, indicating that engineered features are still useful, at least in this case.","","","10.1109/CBMI.2015.7153637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153637","","Semantics;Feature extraction;Training;Indexing;Training data;Histograms;Visualization","convolution;document image processing;feature extraction;image classification;indexing;neural nets;support vector machines;video retrieval;video signal processing","semantic video indexing;traditional engineered features;hand-crafted features;hand-crafted descriptors;learned features;content-based semantic indexing;video documents;semantic features;classifiers training;vector;classification scores;deep convolutional neural network;DCNN;TRECVid semantic indexing task;still images;SVM classifiers","","6","34","","","","","IEEE","IEEE Conferences"
"Dimensionality reduction by supervised locality analysis","L. Zhang; P. Peng; X. Xiang; X. Zhen","College of Information and Communication Engineering, Harbin Engineering University, Harbin, China; College of Information and Communication Engineering, Harbin Engineering University, Harbin, China; College of Information and Communication Engineering, Harbin Engineering University, Harbin, China; Department of Medical Biophysics, University of Western Ontario, London, ON, Canada","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","1488","1492","High-dimensional feature representations have recently been widely used for image classification, which not only induce large storage requirement and high computational complexity, but also tend to be lack of discrimination due to redundant and noisy features. In this paper, we propose a novel algorithm named supervised locality analysis (SLA) for dimensionality reduction. In contrast to conventional dimensionality reduction methods, the proposed SLA incorporates supervision into locality analysis by fully exploring multi-class distributions, which can handle the non-linear data structure while preserving intrinsic discriminative information. The obtained compact and highly discriminative features by the SLA is enables more accurate and efficient classification. Moreover, the SLA can be used for supervised dimensionality reduction of both handcrafted and deep learning based features. We have conduced experiments to evaluate the proposed SLA on three datasets for image classification. The SLA has produced state-of-the-art performance and largely outperformed widely-used dimensionality reduction methods.","","","10.1109/ICIP.2015.7351048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351048","Dimensionality reduction;manifold learning;locality analysis;image classification","Algorithm design and analysis;Principal component analysis;Optimization;Yttrium;Linear programming;Manifolds;Silicon","image classification;image representation;learning (artificial intelligence)","supervised locality analysis;high-dimensional feature representations;image classification;multiclass distributions;supervised dimensionality reduction;deep learning based features","","","21","","","","","IEEE","IEEE Conferences"
"A Convolutional Neural Network for Leaves Recognition Using Data Augmentation","C. Zhang; P. Zhou; C. Li; L. Liu","Sch. of Electron. Inf. & Commun., Huazhong Univ. of Sci. & Technol., Wuhan, China; Sch. of Electron. Inf. & Commun., Huazhong Univ. of Sci. & Technol., Wuhan, China; DNN (Deep Neural Network) Lab., Beijing JingDong Century Trade. Ltd., Beijing, China; TipDM Intell. Technol., Wuhan, China","2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing","","2015","","","2143","2150","Recently, convolutional neural networks (ConvNets) have achieved marvellous results in different field of recognition, especially in computer vision. In this paper, a seven-layer ConvNet using data augmentation is proposed for leaves recognition. First, we implement multiform transformations (e.g., rotation and translation etc.) to enlarge the dataset without changing their labels. This novel technique recently makes tremendous contribution to the performance of ConvNets as it is able to reduce the over-fitting degree and enhance the generalization ability of the ConvNet. Moreover, in order to get the shapes of leaves, we sharpen all the images with a random parameter. This method is similar to the edge detection, which has been proved useful in the image classification. Then we train a deep convolutional neural network to classify the augmented leaves data with three groups of test set and finally find that the method is quite feasible and effective. The accuracy achieved by our algorithm outperforms other methods for supervised learning on the popular leaf dataset Flavia.","","","10.1109/CIT/IUCC/DASC/PICOM.2015.318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363364","","Neural networks;Feature extraction;Training;Kernel;Blades;Electronic mail;Shape","computer vision;data analysis;edge detection;image classification;learning (artificial intelligence);neural nets","convolutional neural networks;leaves recognition;data augmentation;seven-layer ConvNet;computer vision;multiform transformations;rotation;translation;generalization ability enhancement;over-fitting degree reduction;random parameter;edge detection;image classification;supervised learning;leaf dataset Flavia","","12","26","","","","","IEEE","IEEE Conferences"
"Deeply Learned Rich Coding for Cross-Dataset Facial Age Estimation","Z. Kuang; C. Huang; W. Zhang","NA; NA; NA","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","338","343","We propose a method for leveraging publicly available labeled facial age datasets to estimate age from unconstrained face images at the ChaLearn Looking at People (LAP) challenge 2015 [9]. We first learn discriminative age related representation on multiple publicly available age datasets using deep Convolutional Neural Networks (CNN). Training CNN is supervised by rich binary codes, and thus modeled as a multi-label classification problem. The codes represent different age group partitions at multiple granularities, and also gender information. We then train a regressor from deep representation to age on the small training dataset provided by LAP organizer by fusing random forest and quadratic regression with local adjustment. Finally, we evaluate the proposed method on the provided testing data. It obtains the performance of 0.287, and ranks the 3rd place in the challenge. The experimental results demonstrate that the proposed deep representation is insensitive to cross-dataset bias, and thus generalizable to new datasets collected from other sources.","","","10.1109/ICCVW.2015.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406401","","Estimation;Face;Training;Binary codes;Feature extraction;Convolutional codes;Convolution","binary codes;convolution;face recognition;neural nets;regression analysis;visual databases","deeply learned rich coding;cross-dataset facial age estimation;labeled facial age datasets;unconstrained face images;ChaLearn Looking at People challenge 2015;LAP challenge 2015;multiple publicly available age datasets;deep convolutional neural networks;CNN;multilabel classification problem;binary codes;age group partitions;multiple granularities;random forest;quadratic regression;local adjustment","","14","31","","","","","IEEE","IEEE Conferences"
"Beyond Classification: Latent User Interests Profiling from Visual Contents Analysis","L. Yang; C. Hsieh; D. Estrin","NA; NA; NA","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","","2015","","","1410","1416","User preference profiling is an important task in modern online social networks (OSN). With the proliferation of image-centric social platforms, such as Pinterest, visual contents have become one of the most informative data streams for understanding user preferences. Traditional approaches usually treat visual content analysis as a general classification problem where one or more labels are assigned to each image. Although such an approach simplifies the process of image analysis, it misses the rich context and visual cues that play an important role in people's perception of images. In this paper, we explore the possibilities of learning a user's latent visual preferences directly from image contents. We propose a distance metric learning method based on Deep Convolutional Neural Networks (CNN) to directly extract similarity information from visual contents and use the derived distance metric to mine individual users' fine-grained visual preferences. Through our preliminary experiments using data from 5,790 Pinterest users, we show that even for the images within the same category, each user possesses distinct and individually-identifiable visual preferences that are consistent over their lifetime. Our results underscore the untapped potential of finer-grained visual preference profiling in understanding users' preferences.","","","10.1109/ICDMW.2015.160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395835","visual preference;personalization;siamese CNN","Visualization;Measurement;Feature extraction;Pins;Training;Silicon;Computer science","image classification;learning (artificial intelligence);neural nets;social networking (online)","latent user interest profiling;user preference profiling;online social networks;OSN;image-centric social platforms;Pinterest;informative data streams;visual content analysis;image analysis;image perception;user latent visual preference learning;image contents;distance metric learning method;deep-convolutional neural networks;CNN;similarity information extraction;user fine-grained visual preference mining;fine-grained visual preference profiling","","7","35","","","","","IEEE","IEEE Conferences"
"Matchability Prediction for Full-Search Template Matching Algorithms","A. Penate-Sanchez; L. Porzi; F. Moreno-Noguer","Inst. de Rob`otica i Inform`atica Ind., UPC, Barcelona, Spain; Fond. Bruno Kessler, Trento, Italy; Inst. de Rob`otica i Inform`atica Ind., UPC, Barcelona, Spain","2015 International Conference on 3D Vision","","2015","","","353","361","While recent approaches have shown that it is possible to do template matching by exhaustively scanning the parameter space, the resulting algorithms are still quite demanding. In this paper we alleviate the computational load of these algorithms by proposing an efficient approach for predicting the match ability of a template, before it is actually performed. This avoids large amounts of unnecessary computations. We learn the match ability of templates by using dense convolutional neural network descriptors that do not require ad-hoc criteria to characterize a template. By using deep learning descriptions of patches we are able to predict match ability over the whole image quite reliably. We will also show how no specific training data is required to solve problems like panorama stitching in which you usually require data from the scene in question. Due to the highly parallelizable nature of this tasks we offer an efficient technique with a negligible computational cost at test time.","","","10.1109/3DV.2015.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335503","","Prediction algorithms;Detectors;Robustness;Search problems;Approximation algorithms;Approximation methods;Computational efficiency","image matching;learning (artificial intelligence);neural nets","template matchability prediction;full-search template matching algorithms;parameter space scanning;computational load;template matchability learning;dense-convolutional neural network descriptors;deep learning descriptions;panorama stitching;scene data","","6","34","","","","","IEEE","IEEE Conferences"
"The development and analysis of quality of ""Batik Detector"" as a learning media for Indonesia Batik motifs Android based in Indonesian School of Singapore","H. D. Hermawan; F. Arifin","Informatics Engineering Education, Yogyakarta State University, Yogyakarta, Indonesia; Informatics Engineering Education, Yogyakarta State University, Yogyakarta, Indonesia","2015 International Conference on Science and Technology (TICST)","","2015","","","281","287","Batik is one of the Indonesian cultural heritages which contains symbol and deep philosophy of human life. But today, the young generation of Indonesia does not understand and recognize the diversity of Indonesian batik motifs. With sophisticated advance technology, it is become media for developer to create an application that can present the Indonesian batik motifs. The purpose of this research is to develop and analyse the quality of Batik Detector application as Android-based learning media for Indonesian Batik motifs at Singapore Indonesian School. Application development model used is Waterfall Model which consists of analysis, design, implementation and testing. Application development used development tools that are Unity 3D, mono-develop euphoria and markerless tracking augmented reality. The result of quality analysis for Batik Detector in performance efficiency aspect meets the Little Eye standard that was 15%. Functional suitability aspect was 100%, and met compatibility aspect. The feasibility level application based on opinions of media expert gained an average 4.31 in the category of highly proper, material expert was 100% in the category of valid, and from the users (usability) was 82.47%. It can be concluded that ""Batik Detector"" application is very feasible as learning media for Indonesian Batik motifs.","","","10.1109/TICST.2015.7369371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7369371","learning media;batik;batik motifs;Android","Decision support systems;Reliability;Usability;Testing","Android (operating system);courseware;history;mobile computing","Indonesian School of Singapore;Indonesian cultural heritages;philosophy;human life;Indonesian batik motifs;Android-based learning media;Singapore Indonesian School;application development model;waterfall model;Unity 3D;mono-develop euphoria;markerless tracking augmented reality;Batik Detector","","1","12","","","","","IEEE","IEEE Conferences"
"A sensor tagging approach for reusing building blocks of knowledge in learning classifier systems","L. Chen; P. Lee; T. Hsiao","NA; NA; NA","2015 IEEE Congress on Evolutionary Computation (CEC)","","2015","","","2953","2960","During the last decade, the extraction and reuse of building blocks of knowledge for the learning process of Extended Classifier System (XCS) in Multiplexer (MUX) problem domain have been demonstrate feasible by using Code Fragment (CF) (i.e. a tree-based structure ordinarily used in the field of Genetic Programming (GP)) as the representation of classifier conditions (the resulting system was called XCSCFC). However, the use of the tree-based structure may lead to the bloating problem and increase in time complexity when the tree grows deep. Therefore, we proposed a novel representation of classifier conditions for the XCS, named Sensory Tag (ST). The XCS with the ST as the input representation is called XCSSTC. The experiments of the proposed method were conducted in the MUX problem domain. The results indicate that the XCSSTC is capable of reusing building blocks of knowledge in the MUX problems. The current study also discussed about two different aspects of reusing of building blocks of knowledge. Specifically, we proposed the “attribution selection” part and the “logical relation between the attributes” part.","","","10.1109/CEC.2015.7257256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7257256","Extended Classifier System (XCS);Building Blocks;Sensory Tag;Hash table;Scalability;Pattern Recognition","Sociology;Statistics;Accuracy;Multiplexing;Encoding;Indexes;Impedance matching","feature selection;learning (artificial intelligence);pattern classification;trees (mathematics)","sensor tagging approach;knowledge building block;learning process;extended classifier system;XCS;code fragment;CF;tree-based structure;attribution selection","","3","26","","","","","IEEE","IEEE Conferences"
"Deep Matrix Factorization for social image tag refinement and assignment","Z. Li; J. Tang","School of Computer Science, Nanjing University of Science and Technology, No. 200, Xiaolingwei Road, China 210094; School of Computer Science, Nanjing University of Science and Technology, No. 200, Xiaolingwei Road, China 210094","2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP)","","2015","","","1","6","The number of images associated with user-provided tags has increased dramatically in recent years. User-provided tags are incomplete, subjective and noisy. In this work, we focus on the problem of image tag refinement and assignment. Different from previous work, we propose a novel Deep Matrix Factorization (DMF) algorithm, which uncovers the latent image representations and tag representations embedded in the latent subspace by exploiting the weakly-supervised tagging information and visual information. Due to the well-known semantic gap, the hidden representations of images are learned by a hierarchical model, which are progressively transformed from the visual feature space. It can naturally embed new images into the subspace using the learned deep architecture. Besides, to remove the noisy or redundant visual features, a sparse model is imposed on the transformation matrix of the first layer in the deep architecture. Finally, a unified optimization problem with a well-defined objective function is developed to formulate the proposed problem. Extensive experiments on real-world social image databases are conducted on the tasks of image tag refinement and assignment. Encouraging results are achieved with comparison to the state-of-the-art algorithms, which demonstrates the effectiveness of the proposed method.","","","10.1109/MMSP.2015.7340796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340796","","Visualization;Noise measurement;Semantics;Data models;Tagging;Optimization;Image representation","image representation;matrix decomposition;optimisation","deep matrix factorization;social image tag refinement and assignment;DMF algorithm;social image databases;unified optimization problem;transformation matrix;semantic gap;tag representations;latent image representations","","5","27","","","","","IEEE","IEEE Conferences"
"A hybrid Deep belief network approach for Financial distress prediction","Z. Lanbouri; S. Achchab","National School for Computer Science and Systems analysis, Mohammed V University of Rabat, Morocco; National School for Computer Science and Systems analysis, Mohammed V University of Rabat, Morocco","2015 10th International Conference on Intelligent Systems: Theories and Applications (SITA)","","2015","","","1","6","After the subprime crisis in 2008, an efficient Financial Distress Prediction (FDP) model has become necessary. Many research works have attempted to provide a model using statistical or intelligent methods. In this respect, this paper adopts a two-stage hybrid model that integrates Deep Learning and Support Vector Machine as a FDP modeling method. Local receptive fields is a technique used in order to select the nodes for each layer of our deep network. Then, stacked Restricted Boltzmann Machine is applied to form a Deep Belief Network as pre-training. Subsequently, Support Vector Machine follows for classification. An experiment over a sample of French firms offers accuracy details about this method. The proposed model actually provides a result of 76,8% instances that are correctly classified. Henceforth, the new technique could prevent Financial distress before it happens. As such, investors, managers, banks, decision makers and others could benefit from its significant impact.","","","10.1109/SITA.2015.7358416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358416","","Support vector machines;Business;Artificial neural networks;Training;Machine learning;Measurement;Correlation","","","","","19","","","","","IEEE","IEEE Conferences"
"DeepPano: Deep Panoramic Representation for 3-D Shape Recognition","B. Shi; S. Bai; Z. Zhou; X. Bai","School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, P. R. China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, P. R. China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, P. R. China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, P. R. China","IEEE Signal Processing Letters","","2015","22","12","2339","2343","This letter introduces a robust representation of 3-D shapes, named DeepPano, learned with deep convolutional neural networks (CNN). Firstly, each 3-D shape is converted into a panoramic view, namely a cylinder projection around its principle axis. Then, a variant of CNN is specifically designed for learning the deep representations directly from such views. Different from typical CNN, a row-wise max-pooling layer is inserted between the convolution and fully-connected layers, making the learned representations invariant to the rotation around a principle axis. Our approach achieves state-of-the-art retrieval/classification results on two large-scale 3-D model datasets (ModelNet-10 and ModelNet-40), outperforming typical methods by a large margin.","","","10.1109/LSP.2015.2480802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273863","3-D shape;classification;convolutional neural networks;panorama;retrieval","Shape;Three-dimensional displays;Solid modeling;Convolution;Feature extraction;Design automation;Neural networks","image recognition;image representation;neural nets;shape recognition","DeepPano;deep panoramic representation;3D shape recognition;robust representation;convolutional neural networks;CNN;cylinder projection;principle axis","","125","24","","","","","IEEE","IEEE Journals"
"Sign language recognition using real-sense","J. Huang; W. Zhou; H. Li; W. Li","Department of Electronic Engineering and Information Science, USTC; Department of Electronic Engineering and Information Science, USTC; Department of Electronic Engineering and Information Science, USTC; Department of Electronic Engineering and Information Science, USTC","2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)","","2015","","","166","170","Sign Language Recognition (SLR) targets on facilitating the communication between deaf-mute people and ordinary people. This task is very challenging due to the complexity and large variations in hand postures. Some methods require user wear sensor gloves which can detect the position and angle of finger articulations. Others use RGB-D camera like Kinect to track hands and rely on complex algorithms to segment hands from background. However, all these methods have its own disadvantages. Sensor-based methods are not natural as the user must wear cumbersome instruments while camera-based methods have to design extra algorithms to track and segment hands from complex background. To address these problems, we propose a novel method for SLR which involves the use of the Real-Sense. It is a camera device which can detect and track the location of hands in a natural way. More powerful, it provides the 3D coordinates of finger joints in real time. We build a deep neural network (DNN) based on Real-Sense to recognize different signs. The DNN takes the 3D coordinates of finger joints as input directly without using any handcrafted features. The reason is that DNN, as a deep model, is capable of learning suitable features for recognition from raw data. In experiment, to demonstrate the effectiveness of Real-Sense, we collect two datasets by Real-Sense and Kinect respectively, then build DNNs based on each dataset for recognition. To validate the powerfulness of DNN, we compare the performance of DNN and support vector machine (SVM) on the same dataset.","","","10.1109/ChinaSIP.2015.7230384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230384","Sign language recognition;Real-Sense;deep neural network","Gesture recognition;Assistive technology;Training;Three-dimensional displays;Neural networks;Support vector machines;Image recognition","cameras;data gloves;neural nets;object detection;object tracking;sign language recognition","sign language recognition;SLR;sensor gloves;position detection;angle of finger articulations;RGB-D camera;Kinect;hand tracking;sensor-based methods;deep neural network;DNN;3D coordinates;real-sense;3D finger joint coordinates;support vector machine;SVM","","18","13","","","","","IEEE","IEEE Conferences"
"Analyzing the Transferability of Collective Inference Models Across Networks","R. Niu; S. Moreno; J. Neville","NA; NA; NA","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","","2015","","","908","916","Collective inference models have recently been used to significantly improve the predictive accuracy of node classifications in network domains. However, these methods have generally assumed a fully labeled network is available for learning. There has been relatively little work on transfer learning methods for collective classification, i.e., to exploit labeled data in one network domain to learn a collective classification model to apply in another network. While there has been some work on transfer learning for link prediction and node classification, the proposed methods focus on developing algorithms to adapt the models without a deep understanding of how the network structure impacts transferability. Here we make the key observation that collective classification models are generally composed of local model templates that are rolled out across a heterogeneous network to construct a larger model for inference. Thus, the transferability of a model could depend on similarity of the local model templates and/or the global structure of the data networks. In this work, we study the performance of basic relational models when learned on one network and transferred to another network to apply collective inference. We show, using both synthetic and real data experiments, that transferability of models depends on both the graph structure and local model parameters. Moreover, we show that a probability calibration process (that removes bias due to propagation errors in collective inference) improves transferability.","","","10.1109/ICDMW.2015.192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395764","","Data models;Adaptation models;Correlation;Predictive models;Learning systems;Inference algorithms;Yttrium","inference mechanisms;learning (artificial intelligence);pattern classification;probability","transferability;collective inference models;node classifications;network domains;transfer learning;collective classification;probability calibration process","","","19","","","","","IEEE","IEEE Conferences"
"AI: Fears of 'playing God' [Control & Automation Artificial Intelligence]","E. Gent","NA","Engineering & Technology","","2015","10","2","76","79","Man's desire to build something in his own image has a long literary history. From the animated statues of Greek myths, to the clay Golem of Jewish folklore, to Mary Shelley's Frankenstein, alchemists, mystics and mad scientists have long been engaged in the quest for artificial sentience. Google has spearheaded this drive with a particular focus on start-ups specialising in 'deep learning', a sub-branch of machine learning that excels in areas considered crucial to the development of with Al, such as computer vision, speech recognition and natural language processing. But with Facebook announcing the creation of an Al lab in 2013, IBM making steady progress with its Watson Al system, and both Microsoft and Chinese search giant Baidu heavily investing in Al, competition in the field is heating up. The Al arms race has understandably caught the attention of the media and, as is often the case with game-changing technologies, many have been quick to foresee doom. Concerns over the potential existential threat to humankind posed by super-intelligent Al have been aired by influential voices including physicist Stephen Hawking and the SpaceX and Tesla founder Elon Musk, who recently announced a £10m donation to fund research into making Al safe.","","","10.1049/et.2015.0210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7098617","","","artificial intelligence","artificial sentience;artificial intelligence;Google;deep-learning;machine learning;Al development;computer vision;speech recognition;natural language processing;Facebook;Al lab;IBM;Watson Al system;Microsoft;Baidu;humankind threat;super-intelligent Al;Al safety","","","","","","","","IET","IET Journals"
"DeepEdge: A multi-scale bifurcated deep network for top-down contour detection","G. Bertasius; J. Shi; L. Torresani","University of Pennsylvania, Philadelphia, 19104, United States; University of Pennsylvania, Philadelphia, 19104, United States; Dartmouth College, Hanover, NH 03755, United States","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","4380","4389","Contour detection has been a fundamental component in many image segmentation and object detection systems. Most previous work utilizes low-level features such as texture or saliency to detect contours and then use them as cues for a higher-level task such as object detection. However, we claim that recognizing objects and predicting contours are two mutually related tasks. Contrary to traditional approaches, we show that we can invert the commonly established pipeline: instead of detecting contours with low-level cues for a higher-level recognition task, we exploit object-related features as high-level cues for contour detection.","","","10.1109/CVPR.2015.7299067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299067","","Feature extraction;Computer architecture;Image edge detection;Convolutional codes;Object detection;Machine learning;Training","edge detection;image segmentation;image texture;object detection","DeepEdge;multiscale bifurcated deep network;top-down contour detection;image segmentation;object detection systems;image texture;image saliency;object-related features","","108","34","","","","","IEEE","IEEE Conferences"
"Pedestrian detection based on deep convolutional neural network with ensemble inference network","H. Fukui; T. Yamashita; Y. Yamauchi; H. Fujiyoshi; H. Murase","Chubu University, 1200 Matsumoto cho, Kasugai, Aichi, Japan; Chubu University, 1200 Matsumoto cho, Kasugai, Aichi, Japan; Chubu University, 1200 Matsumoto cho, Kasugai, Aichi, Japan; Chubu University, 1200 Matsumoto cho, Kasugai, Aichi, Japan; Nagoya University, Furo cho, Chikusa ku, Nagoya, Aichi, Japan","2015 IEEE Intelligent Vehicles Symposium (IV)","","2015","","","223","228","Pedestrian detection is an active research topic for driving assistance systems. To install pedestrian detection in a regular vehicle, however, there is a need to reduce its cost and ensure high accuracy. Although many approaches have been developed, vision-based methods of pedestrian detection are best suited to these requirements. In this paper, we propose the methods based on Convolutional Neural Networks (CNN) that achieves high accuracy in various fields. To achieve such generalization, our CNN-based method introduces Random Dropout and Ensemble Inference Network (EIN) to the training and classification processes, respectively. Random Dropout selects units that have a flexible rate, instead of the fixed rate in conventional Dropout. EIN constructs multiple networks that have different structures in fully connected layers. The proposed methods achieves comparable performance to state-of-the-art methods, even though the structure of the proposed methods are considerably simpler.","","","10.1109/IVS.2015.7225690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7225690","","Feature extraction;Training;Benchmark testing;Accuracy;Robustness;Convolution;Machine learning","computer vision;driver information systems;inference mechanisms;neural nets;object detection;pedestrians","pedestrian detection;deep convolutional neural network;ensemble inference network;driving assistance systems;vision-based methods;CNN-based method;random dropout;classification process;training process","","16","30","","","","","IEEE","IEEE Conferences"
"Image retrieval based on convolutional neural network and kernel-based supervised hashing","T. Peng; Y. Zhao; S. Ke","Department of Computer Science and Engineering, Henan Institute of Engineering, Zhengzhou, China; Institute of Information System Engineering, Information Engineering University, Zhengzhou, China; Institute of Information System Engineering, Information Engineering University, Zhengzhou, China","2015 8th International Congress on Image and Signal Processing (CISP)","","2015","","","544","549","With the increasing amount of image data, the present image retrieval methods have several problems, such as the steps of the visual features coding are fixed, lack of learning ability, low expression ability of the features, high dimension of the features, which restrict the retrieval performance severely. Aiming at these problems, an image retrieval method based on convolutional neural network and kernel-based supervised hashing is proposed. Firstly, we use the learning ability of convolutional neural network to mine the internal implication relation of the images and extract the deep features. Then, introduce the kernel-based supervised hashing and train the high-dimension deep features with the supervised information, map the high-dimensional features to the low-dimensional compact binary codes. Finally, image retrieval on the mass image datasets is accomplished effectively in low-dimensional hamming space. The experimental results on ImageNet-1000 and Caltech-256 demonstrate that our method can enhance the expression ability of the image features effectively, and reduce the dimensionality of the high-dimension image features, the image retrieval performance is superior to the state-of-the-art methods.","","","10.1109/CISP.2015.7407939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407939","deep learning;image retrieval;convolutional neural network;approximate nearest neighbor;kernel-based supervised hashing","Neural networks;Image retrieval;Convolutional codes;Training;Convolution;Feature extraction;Semantics","convolution;cryptography;file organisation;image retrieval","convolutional neural network;kernel-based supervised hashing;image retrieval methods;learning ability","","","29","","","","","IEEE","IEEE Conferences"
"A restricted Boltzmann machine based two-lead electrocardiography classification","Y. Yan; X. Qin; Y. Wu; N. Zhang; J. Fan; L. Wang","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences","2015 IEEE 12th International Conference on Wearable and Implantable Body Sensor Networks (BSN)","","2015","","","1","9","An restricted Boltzmann machine learning algorithm were proposed in the two-lead heart beat classification problem. ECG classification is a complex pattern recognition problem. The unsupervised learning algorithm of restricted Boltzmann machine is ideal in mining the massive unlabelled ECG wave beats collected in the heart healthcare monitoring applications. A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. In this paper a deep belief network was constructed and the RBM based algorithm was used in the classification problem. Under the recommended twelve classes by the ANSI/AAMI EC57: 1998/(R)2008 standard as the waveform labels, the algorithm was evaluated on the two-lead ECG dataset of MIT-BIH and gets the performance with accuracy of 98.829%. The proposed algorithm performed well in the two-lead ECG classification problem, which could be generalized to multi-lead unsupervised ECG classification or detection problems.","","","10.1109/BSN.2015.7299399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299399","big data;electrocardiography classification;restricted Boltzmann machine;deep belief network","Electrocardiography;Heart beat;Feature extraction;Accuracy;Training;Data models;Signal processing algorithms","Boltzmann machines;diseases;electrocardiography;learning (artificial intelligence);medical signal processing;signal classification;stochastic processes","restricted Boltzmann machine;two-lead ECG classification;RBM learning algorithm;two-lead heart beat classification;complex pattern recognition;massive unlabelled ECG wave beat;heart healthcare monitoring;stochastic artificial neural network;probability distribution;two-lead ECG dataset;MIT-BIH arrhythmia database;multilead unsupervised ECG classification","","15","31","","","","","IEEE","IEEE Conferences"
"Adapting New Categories for Food Recognition with Deep Representation","S. Ao; C. X. Ling","NA; NA","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","","2015","","","1196","1203","Learning to classify new (target) data in a different domain is always an interesting and challenging task in data mining. The classifier could suffer the dataset bias when predicting the new categories from target domain. Many adaptation methods have been proposed to adjust this bias but are limited to using data either from similar categories or requiring a large number of labeled examples from the target domain. Automatically adapting and recognizing new food categories is a very practical task in daily life. In this paper, we propose a new method that can alleviate the dataset bias for food image recognition. To obtain less biased feature representation from the food images, we fine-tuned GoogLeNet as our deep feature extractor and achieve state-of-the-art performance on the Food-101 dataset. Using the deep representation, our method can learn efficient classifiers with fewer labeled examples. More specifically, our method employs an external classifier for adaptation, called ""negative classifier"".Experiment results show that utilizing the parameters of the negative classifier, our method can achieve better performance and converge faster to adapt the new categories.","","","10.1109/ICDMW.2015.203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395804","","Feature extraction;Training;Adaptation models;Image recognition;Data mining;Conferences;Computer science","data mining;feature extraction;food processing industry;food products;image recognition;image representation;pattern classification","food image recognition;data mining;feature representation;GoogLeNet;feature extractor;deep representation;negative classifier","","6","29","","","","","IEEE","IEEE Conferences"
"Audio event recognition based on DBN features from multiple filter-bank representations","Feng Guo; Xiaoou Chen; Deshun Yang","Institute of Computer Science & Technology, Peking University, 128 Zhongguancun North Street, Haidian District, Beijing 100871, China; Institute of Computer Science & Technology, Peking University, 128 Zhongguancun North Street, Haidian District, Beijing 100871, China; Institute of Computer Science & Technology, Peking University, 128 Zhongguancun North Street, Haidian District, Beijing 100871, China","2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP)","","2015","","","1","6","In the audio event classification or detection research field, the representation of the audio itself is important. Many researchers tried to apply Deep Belief Network (DBN) to learn new representations of the audio. The mel filter-bank feature, which is obtained based on mel scale, is commonly used as the low level representation of the audio in the pre-processing procedure of DBN. However, the mel bands used in mel filter-bank feature may not be sufficient for the comprehensive representation of the diverse audio events in the real world and then it will make it difficult for DBN to learn good audio features. In this paper, two steps are taken to explore and tackle the problem. In the first step, we conduct a comparison of the effects among different arrangements of frequency bands to DBN feature learning in the audio event recognition. Here the arrangements of frequency bands include mel bands, bark bands, linear bands and pyramid bands. In the second step, in order to utilize the different classification capabilities of the DBN features on different audio events, we adopt the Adaboost algorithm to fuse them. We conduct the experiments on real datasets collected from findsound website, and the results verifies that our proposed audio event classification system, which uses diverse features selected by Adaboost from all sets of DBN features, outperforms the one using only one kind of DBN feature set.","","","10.1109/MMSP.2015.7340807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340807","","Feature extraction;Event detection;Training;Machine learning;Multimedia communication;Streaming media;Fuses","audio signal processing;belief networks;learning (artificial intelligence);signal classification","audio event recognition;DBN features;multiple filter-bank representations;audio event classification system;Adaboost algorithm;pyramid bands;linear bands;bark bands;mel bands;mel filter-bank feature;deep belief network;audio event classification","","","","","","","","IEEE","IEEE Conferences"
"Conditional Random Fields as Recurrent Neural Networks","S. Zheng; S. Jayasumana; B. Romera-Paredes; V. Vineet; Z. Su; D. Du; C. Huang; P. H. S. Torr","NA; NA; NA; NA; NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1529","1537","Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.","","","10.1109/ICCV.2015.179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410536","","Labeling;Image segmentation;Semantics;Graphical models;Machine learning;Training;Computer vision","backpropagation;Gaussian processes;image segmentation;neural nets;probability;random processes","conditional random field;recurrent neural network;pixel-level labelling task;semantic image segmentation;image understanding;deep learning technique;convolutional neural network;CNN;CRF;probabilistic graphical modelling;Gaussian pairwise potential;mean-field approximate inference;back-propagation algorithm","","766","58","","","","","IEEE","IEEE Conferences"
"Emphysema discrimination from raw HRCT images by convolutional neural networks","E. M. Karabulut; T. Ibrikci","Gaziantep University, Computer Programming Department, 27310, Sahinbey, Gaziantep, Turkey; Cukurova University, Electrical-Electronics Engineering Department, 01330, Balcalı, Adana, Turkey","2015 9th International Conference on Electrical and Electronics Engineering (ELECO)","","2015","","","705","708","Emphysema is a chronic lung disease that causes breathlessness. HRCT is the reliable way of visual demonstration of emphysema in patients. The fact that dangerous and widespread nature of the disease require immediate attention of a doctor with a good degree of specialized anatomical knowledge. This necessitates the development of computer-based automatic identification system. This study aims to investigate the deep learning solution for discriminating emphysema subtypes by using raw pixels of input HRCT images of lung. Convolutional Neural Network (CNN) is used as the deep learning method for experiments carried out in the Caffe deep learning framework. As a result, promising percentage of accuracy is obtained besides low processing time.","","","10.1109/ELECO.2015.7394441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7394441","","Graphics processing units;Machine learning;Lungs;Training;Artificial neural networks;Mathematical model;Diseases","computerised tomography;diseases;lung;neural nets","emphysema discrimination;HRCT images;convolutional neural network;chronic lung disease;computer-based automatic identification system;Caffe deep learning framework","","4","16","","","","","IEEE","IEEE Conferences"
"Malaysia traffic sign recognition with convolutional neural network","M. M. Lau; K. H. Lim; A. A. Gopalai","Curtin Sarawak Research Institute, Curtin University Sarawak Malaysia, Malaysia; Curtin Sarawak Research Institute, Curtin University Sarawak Malaysia, Malaysia; School of Engineering, Monash University Malaysia, Malaysia","2015 IEEE International Conference on Digital Signal Processing (DSP)","","2015","","","1006","1010","Traffic sign recognition system is an important subsystem in advanced driver assistance systems (ADAS) that assisting a driver to detect a critical driving scenario and subsequently making an immediate decision. Recently, deep architecture neural network is popular because it adapts well in various kind of scenarios, even those which were not used during training. Therefore, a deep architecture neural network is implemented to perform traffic sign classification in order to improve the traffic sign recognition rate. A comparative study for a deep and shallow architecture neural network is presented in this paper. Deep and shallow architecture neural network refer to convolutional neural network (CNN) and radial basis function neural network (RBFNN) respectively. In the simulation result, two types of training modes had been compared i.e. incremental training and batch training. Experimental results show that incremental training mode trains faster than batch training mode. The performance of the convolutional neural network is evaluated with the Malaysian traffic sign database and achieves 99% of the recognition rate.","","","10.1109/ICDSP.2015.7252029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7252029","Advance driver assistance system;Traffic sign recognition;Radial basis function neural network;Convolutional neural network","Training;Computer architecture;Biological neural networks;Vehicles;Testing;Roads","driver information systems;image classification;learning (artificial intelligence);object recognition;radial basis function networks","Malaysia traffic sign recognition system;convolutional neural network;advanced driver assistance systems;ADAS;deep architecture neural network;traffic sign classification;traffic sign recognition rate improvement;shallow architecture neural network;CNN;radial basis function neural network;RBFNN;Malaysian traffic sign database;incremental training mode","","14","23","","","","","IEEE","IEEE Conferences"
"Visual object tracking via deep neural network","T. Xu; X. Wu","School of IoT Engineering, Jiangnan University, Wuxi, 214122, China; School of IoT Engineering, Jiangnan University, Wuxi, 214122, China","2015 IEEE First International Smart Cities Conference (ISC2)","","2015","","","1","6","Visual tracking is a fundamental research problem in computer vision field. In this paper, we propose an approach to incorporate visual prior into visual object tracking via deep neural network. Visual prior knowledge is expressed as the parameters of a stacked denoising autoencoder, which is trained from a large collection of natural images. By utilizing natural images, we can obtain generic image features which are more robust against variations. Then we design a classifier for tracking using the same structure as the stacked denoising autoencoder, tracking is then carried out under a particle filter framework by determining the current target's location and updating the parameters. In addition, in order to alleviate the computational burden caused by deep structure, an adaptive updating mechanism is proposed. As a result, we apply a general-to-special strategy for our stacked denoising autoencoder tracker (SDAT), the learned visual prior provides a reasonable initial value for parameters of the neural network, and the deep structure of our tracker is robust to appearance variations. Experiments over 50 challenging videos indicate the effectiveness and robustness of our tracker, and the resulting tracker is outstanding especially against variations with the existing state-of-the-art methods.","","","10.1109/ISC2.2015.7366162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7366162","Object tracking;visual prior;stacked denoising autoencoder","Target tracking;Visualization;Noise reduction;Robustness;Training;Neural networks","image denoising;neural nets;object tracking;particle filtering (numerical methods)","computer vision field;visual object tracking;deep neural network;visual prior knowledge;natural images;generic image features;classifier;particle filter framework;adaptive updating mechanism;stacked denoising autoencoder tracker;SDAT","","1","27","","","","","IEEE","IEEE Conferences"
"Enhanced image classification with a fast-learning shallow convolutional neural network","M. D. McDonnell; T. Vladusich","Computational and Theoretical Neuroscience Laboratory, Institute for Telecommunications Research, School of Information Technology and Mathematical Sciences, University of South Australia, Mawson Lakes, 5095, Australia; Computational and Theoretical Neuroscience Laboratory, Institute for Telecommunications Research, School of Information Technology and Mathematical Sciences, University of South Australia, Mawson Lakes, 5095, Australia","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","7","We present a neural network architecture and training method designed to enable very rapid training and low implementation complexity. Due to its training speed and the absence of iteratively-tuned parameters, the method has strong potential for applications requiring frequent retraining or online training. The approach is characterized by (a) convolutional filters based on biologically inspired visual processing filters, (b) randomly-valued classifier-stage input weights, (c) use of least squares regression to train the classifier output weights in a single batch, and (d) linear classifier-stage output units. We demonstrate the efficacy of the method by applying it to image classification. Our results match existing state-of-the-art results on the MNIST (0.37% error) and NORB-small (2.2% error) image classification databases, but with very fast training times compared to standard deep network approaches. The network's performance on the Google Street View House Number (SVHN) (4% error) database is also competitive with state-of-the art methods.","","","10.1109/IJCNN.2015.7280796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280796","","Databases","filtering theory;image classification;learning (artificial intelligence);least squares approximations;neural nets;regression analysis","fast-learning shallow convolutional neural network;neural network architecture;implementation complexity;convolutional filters;biologically inspired visual processing filters;randomly-valued classifier-stage input weights;least squares regression;linear classifier-stage output units;MNIST;Google Street View House Number;SVHN database;NORB-small image classification databases","","29","35","","","","","IEEE","IEEE Conferences"
"Discovery of the relations between genetic polymorphism and adverse drug reactions","Z. Liang; G. Zhang; J. X. Huang","School of Information Technology, York University, Toronto, ON, M3J1P3, Canada; School of Automation, Guangdong University of Technology, China; School of Computer Science, Central China Normal University, Wuhan, China","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","543","548","The genetic polymorphism of Cytochrome P450 (CYP 450) is considered as one of the main causes for adverse drug reactions (ADRs). In order to explore the latent correlations between ADRs and the genetic polymorphism, a new model is proposed in which both the inputs of the genetic locuses (i.e.CYP2D6*2, CYP2D6*10, CYP2D6*14, CYP1A2*1C and CYP1A2*1F) and occurrence as probabilistic distribution. A generative model is proposed to describe the joint distributions of occurrence of ADRs and the diversity of genetic sub-types of the input variables. The new algorithm is developed based on Generative Stochastic Networks (GSN) model. A Markov chain from a training data set is applied for the learning as a transition operator to simulate a probabilistic distribution. The transition distribution is conditional on the previous step of the chain thus it is able to perform learning at a much lower cost than the conventional maximal likelihood method. The experiment results show that the newly algorithm is more effective than the available conventional methods.","","","10.1109/BIBM.2015.7359741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359741","generative stochastic networks;deep learning;Markov chain transition;genetic polymorphism;adverse drug reaction","Genetics","drugs;genetics;learning (artificial intelligence);Markov processes","genetic polymorphism;adverse drug reactions;Cytochrome P450;genetic locuses;probabilistic distribution;joint distributions;Generative Stochastic Networks;GSN model;Markov chain;training data set;transition operator;transition distribution;conventional maximal likelihood method","","1","32","","","","","IEEE","IEEE Conferences"
"Photo-real talking head with deep bidirectional LSTM","B. Fan; L. Wang; F. K. Soong; L. Xie","School of Computer Science, Northwestern Polytechnical University, Xi'an, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4884","4888","Long short-term memory (LSTM) is a specific recurrent neural network (RNN) architecture that is designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we propose to use deep bidirectional LSTM (BLSTM) for audio/visual modeling in our photo-real talking head system. An audio/visual database of a subject's talking is firstly recorded as our training data. The audio/visual stereo data are converted into two parallel temporal sequences, i.e., contextual label sequences obtained by forced aligning audio against text, and visual feature sequences by applying active-appearance-model (AAM) on the lower face region among all the training image samples. The deep BLSTM is then trained to learn the regression model by minimizing the sum of square error (SSE) of predicting visual sequence from label sequence. After testing different network topologies, we interestingly found the best network is two BLSTM layers sitting on top of one feed-forward layer on our datasets. Compared with our previous HMM-based system, the newly proposed deep BLSTM-based one is better on both objective measurement and subjective A/B test.","","","10.1109/ICASSP.2015.7178899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178899","BLSTM;RNN;AAM;talking head","Hidden Markov models;Visualization;Face;Active appearance model;Shape;Speech","audio databases;audio signal processing;face recognition;feature extraction;feedforward neural nets;image sequences;recurrent neural nets;regression analysis;speech synthesis;stereo image processing;visual databases","photo-real talking head system;deep bidirectional LSTM;long short-term memory;recurrent neural network architecture;parallel temporal sequences;audio modeling;visual modeling;audio database;visual database;audio stereo data;visual stereo data;forced aligning audio;contextual label sequences;visual feature sequences;active-appearance-model;AAM;regression model;sum-of-square error minimization;SSE;feed-forward layer;visual speech synthesis","","25","22","","","","","IEEE","IEEE Conferences"
"Leveraging Datasets with Varying Annotations for Face Alignment via Deep Regression Network","J. Zhang; M. Kan; S. Shan; X. Chen","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","3801","3809","Facial landmark detection, as a vital topic in computer vision, has been studied for many decades and lots of datasets have been collected for evaluation. These datasets usually have different annotations, e.g., 68-landmark markup for LFPW dataset, while 74-landmark markup for GTAV dataset. Intuitively, it is meaningful to fuse all the datasets to predict a union of all types of landmarks from multiple datasets (i.e., transfer the annotations of each dataset to all other datasets), but this problem is nontrivial due to the distribution discrepancy between datasets and incomplete annotations of all types for each dataset. In this work, we propose a deep regression network coupled with sparse shape regression (DRN-SSR) to predict the union of all types of landmarks by leveraging datasets with varying annotations, each dataset with one type of annotation. Specifically, the deep regression network intends to predict the union of all landmarks, and the sparse shape regression attempts to approximate those undefined landmarks on each dataset so as to guide the learning of the deep regression network for face alignment. Extensive experiments on two challenging datasets, IBUG and GLF, demonstrate that our method can effectively leverage the multiple datasets with different annotations to predict the union of all types of landmarks.","","","10.1109/ICCV.2015.433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410790","","Shape;Face;Silicon;Robustness;Predictive models;Correlation;Neural networks","computer vision;neural nets;object detection;regression analysis","GLF dataset;IBUG dataset;face alignment;DRN-SSR;sparse shape regression;GTAV dataset;LFPW dataset;computer vision;facial landmark detection;deep regression network;face alignment annotation","","11","40","","","","","IEEE","IEEE Conferences"
"Distance metric learning for kernel density-based acoustic model under limited training data conditions","V. H. Do; X. Xiao; E. S. Chng; H. Li","School of Computer Engineering, Nanyang Technological University, Singapore; Temasek Laboratories@NTU, Nanyang Technological University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore","2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","","2015","","","54","58","Kernel density model works well for limited training data in acoustic modeling. In this paper, we improve the kernel density-based acoustic model for low resource language speech recognition. In our previous study, we demonstrated the effectiveness of the kernel density-based acoustic model on discriminative features such as cross-lingual bottleneck features. In this paper, we propose to learn a Mahalanobis-based distance, which is equivalent to a full rank linear feature transformation, to minimize training data frame classification error. Experimental results on the Wall Street Journal (WSJ) task show that the proposed Mahalanobis-based distance learning results in significant improvements over the Euclidean distance. The kernel density acoustic model with the Mahalanobis-based distance also outperforms deep neural network acoustic model significantly in limited training data cases.","","","10.1109/APSIPA.2015.7415373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415373","","Hidden Markov models;Kernel;Training data;Acoustics;Measurement;Data models;Training","acoustic signal processing;natural language processing;signal classification;speech recognition","distance metric learning;kernel density-based acoustic model;limited training data conditions;acoustic modeling;language speech recognition;full rank linear feature transformation;training data frame classification error;Wall Street Journal task;WSJ task;Mahalanobis-based distance learning;kernel density acoustic model","","1","29","","","","","IEEE","IEEE Conferences"
"From generic to specific deep representations for visual recognition","H. Azizpour; A. S. Razavian; J. Sullivan; A. Maki; S. Carlsson","KTH (Royal Institute of Technology), 114 28 Stockholm, Sweden; KTH (Royal Institute of Technology), 114 28 Stockholm, Sweden; KTH (Royal Institute of Technology), 114 28 Stockholm, Sweden; KTH (Royal Institute of Technology), 114 28 Stockholm, Sweden; KTH (Royal Institute of Technology), 114 28 Stockholm, Sweden","2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","","2015","","","36","45","Evidence is mounting that ConvNets are the best representation learning method for recognition. In the common scenario, a ConvNet is trained on a large labeled dataset and the feed-forward units activation, at a certain layer of the network, is used as a generic representation of an input image. Recent studies have shown this form of representation to be astoundingly effective for a wide range of recognition tasks. This paper thoroughly investigates the transferability of such representations w.r.t. several factors. It includes parameters for training the network such as its architecture and parameters of feature extraction. We further show that different visual recognition tasks can be categorically ordered based on their distance from the source task. We then show interesting results indicating a clear correlation between the performance of tasks and their distance from the source task conditioned on proposed factors. Furthermore, by optimizing these factors, we achieve state-of-the-art performances on 16 visual recognition tasks.","","","10.1109/CVPRW.2015.7301270","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301270","","Visualization;Sun;Training;Positron emission tomography;Computer vision;Standards;Image recognition","feature extraction;image representation","deep representations;ConvNets;representation learning method;visual recognition tasks;feature extraction;image representation;feedforward unit activation;labeled dataset","","128","48","","","","","IEEE","IEEE Conferences"
"Talking heads synthesis from audio with deep neural networks","T. Shimba; R. Sakurai; H. Yamazoe; J. Lee","Graduate School of Information Science and Engineering, Ritsumeikan University, 1-1-1 Kusatsu, Shiga, Japan; College of Information Science and Engineering, Ritsumeikan University, 1-1-1 Kusatsu, Shiga, Japan; College of Information Science and Engineering, Ritsumeikan University, 1-1-1 Kusatsu, Shiga, Japan; College of Information Science and Engineering, Ritsumeikan University, 1-1-1 Kusatsu, Shiga, Japan","2015 IEEE/SICE International Symposium on System Integration (SII)","","2015","","","100","105","Talking heads synthesis with expressions from speech is proposed in this paper. Talking heads synthesis can be considered as a learning problem of sequence-to-sequence mapping, which consists of audio as input and video as output. To synthesize talking heads, we use SAVEE database which consists of videos of multiple sentences speeches recorded from front of face. Audiovisual data can be considered as two parallel sequential data of audio and visual features and it is composed of continuous value. Thus, audio and visual features of our dataset are represented by a regression model. In this research, the regression model is trained with long short-term memory (LSTM) by minimizing mean squared error (MSE). Then, audio features are used as input and visual features are used as target of LSTM. Thereby, talking heads are synthesized from speech. Our method is proposed to use lower level audio features than phonemes and it enables to synthesize talking heads with expressions while existing researches which use phonemes as audio features only can synthesize neutral expression talking heads. With SAVEE database, we achieved the minimum MSE 17.03 on our testing dataset. In experiment, we use mel-frequency cepstral coefficient (MFCC), AMFCC and A2 MFCC with energy as audio feature and active appearance model (AAM) on entire face region as visual feature.","","","10.1109/SII.2015.7404961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404961","","Active appearance model;Speech;Visualization;Mel frequency cepstral coefficient;Face;Shape;Feature extraction","computer animation;feature extraction;mean square error methods;neural nets;regression analysis;speech processing","deep neural networks;learning problem;sequence-to-sequence mapping;SAVEE database;sentence speech;audio-visual data;parallel sequential data;visual features;audio features;regression model;long short-term memory;LSTM;minimizing mean squared error;MSE;neutral expression talking head synthesis;mel-frequency cepstral coefficient;MFCC","","3","10","","","","","IEEE","IEEE Conferences"
"Supporting Metacognition in Online, Professional Graduate Courses","J. Waters; S. Gasson","NA; NA","2015 48th Hawaii International Conference on System Sciences","","2015","","","91","100","This paper presents a model for metacognitive learning evaluation in a community of inquiry and discusses how it may be used to incentivize effective learning in online graduate courses. We identify patterns of behavior and peer-learner interactions that indicate social engagement in community model building, based on an extensive, qualitative analysis of online course discussions and student interaction data. We explore how metacognitive learning develops through socially-situated knowledge construction and demonstrate how community interactions result in deep learning. An extrinsic reward structure is suggested, in the form of a grading rubric that appears to encourage students to experiment with behaviors that provide the intrinsic rewards of metacognitive learning.","","","10.1109/HICSS.2015.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7069669","Metacognition;reflective learning;social engagement;online learning","Communities;Guidelines;Context;Best practices;Buildings;Information systems;Message systems","cognition;computer aided instruction;educational courses;Internet","metacognition;online professional graduate courses;metacognitive learning evaluation;peer-learner interactions;social engagement;community model building;socially-situated knowledge construction;extrinsic reward structure;grading rubric","","","40","","","","","IEEE","IEEE Conferences"
"Adaptive decision-level fusion for Fongbe phoneme classification using fuzzy logic and Deep Belief Networks","F. A. A. Laleye; E. C. Ezin; C. Motamed","Unité de Recherche en Informatique et Sciences Appliquées, Institut de Mathématiques et de Sciences Physiques, Université d'Abomey-Calavi, BP 613, Porto-Novo, Bénin; Unité de Recherche en Informatique et Sciences Appliquées, Institut de Mathématiques et de Sciences Physiques, Université d'Abomey-Calavi, BP 613, Porto-Novo, Bénin; Laboratoire d'Informatique Signal et Image de la Côte d'Opale, Université du Littoral Côte d'Opale, 50 rue F. Buisson, BP 719, 62228 Calais Cedex, France","2015 12th International Conference on Informatics in Control, Automation and Robotics (ICINCO)","","2015","01","","15","24","In this paper, we compare three approaches for decision fusion in a phoneme classification problem. We especially deal with decision-level fusion from Naive Bayes and Learning Vector Quantization (LVQ) classifiers that were trained and tested by three speech analysis techniques: Mel-frequency Cepstral Coefficients (MFCC), Relative Spectral Transform - Perceptual Linear Prediction (Rasta-PLP) and Perceptual Linear Prediction (PLP). Optimal decision making is performed with the non-parametric and parametric methods. We investigated the performance of both decision methods with a third proposed approach using fuzzy logic. The work discusses the classification of an African language phoneme namely Fongbe language and all experiments were performed on its dataset. After classification and the decision fusion, the overall decision fusion performance is obtained on test data with the proposed approach using fuzzy logic whose classification accuracies are 95,54% for consonants and 83,97% for vowels despite the lower execution time of Deep Belief Networks.","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350439","Decision Fusion;Fuzzy Logic;Deep Belief Networks;Phoneme Classification;Naive Bayes;LVQ;Fongbe Language","Speech;Fuzzy logic;Speech recognition;Mel frequency cepstral coefficient;Pragmatics;Neural networks","","","","","38","","","","","IEEE","IEEE Conferences"
"Object detection by labeling superpixels","J. Yan; Y. Yu; X. Zhu; Z. Lei; S. Z. Li","National Laboratory of Pattern Recognition, Chinese Academy of Sciences, China; Institute of Deep Learning, Baidu Research, USA; National Laboratory of Pattern Recognition, Chinese Academy of Sciences, China; National Laboratory of Pattern Recognition, Chinese Academy of Sciences, China; National Laboratory of Pattern Recognition, Chinese Academy of Sciences, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","5107","5116","Object detection is often conducted by object proposal generation and classification sequentially. This paper handles object detection in a superpixel oriented manner instead of the proposal oriented. Specially, this paper takes object detection as a multi-label superpixel labeling problem by minimizing an energy function. It uses the data cost term to capture the appearance, smooth cost term to encode the spatial context and label cost term to favor compact detection. The data cost is learned through a convolutional neural network and the parameters in the labeling model are learned through a structural SVM. Compared with proposal generation and classification based methods, the proposed superpixel labeling method can naturally detect objects missed by proposal generation step and capture the global image context to infer the overlapping objects. The proposed method shows its advantage in Pascal VOC and ImageNet. Notably, it performs better than the ImageNet ILSVRC2014 winner GoogLeNet (45.0% V.S. 43.9% in mAP) with much shallower and fewer CNNs.","","","10.1109/CVPR.2015.7299146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299146","","Proposals;Labeling;Object detection;Context;Support vector machines;Image color analysis;Training","image classification;neural nets;object detection;support vector machines","object detection;object proposal generation;object proposal classification;superpixel oriented manner;proposal oriented manner;multilabel superpixel labeling problem;energy function;data cost term;smooth cost term;label cost term;compact detection;convolutional neural network;structural SVM;superpixel labeling method;proposal generation step;global image context;overlapping object;Pascal VOC;ImageNet ILSVRC2014;GoogLeNet","","39","62","","","","","IEEE","IEEE Conferences"
"Accurate Human-Limb Segmentation in RGB-D Images for Intelligent Mobility Assistance Robots","S. Chandra; S. Tsogkas; I. Kokkinos","NA; NA; NA","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","436","442","Mobility impairment is one of the biggest challenges faced by elderly people in today's society. The inability to move about freely poses severe restrictions on their independence and general quality of life. This work is dedicated to developing intelligent robotic platforms that assist users to move without requiring a human attendant. This work was done in the context of an EU project involved in developing an intelligent robot for elderly user assistance. The robot is equipped with a Kinect sensor, and the vision component of the project has the responsibility of locating the user, estimating the user's pose, and recognizing gestures by the user. All these goals can take advantage of a method that accurately segments human-limbs in the colour (RGB) and depth (D) images captured by the Kinect sensor. We exploit recent advances in deep-learning to develop a system that performs accurate semantic segmentation of human limbs using colour and depth images. Our novel technical contributions are the following: 1) we describe a scheme for manual annotation of videos, that eliminates the need to annotate segmentation masks in every single frame, 2) we extend a state of the art deep learning system for semantic segmentation, to exploit diverse RGB and depth data, in a single framework for training and testing, 3) we evaluate different variants of our system and demonstrate promising performance, as well the contribution of diverse data, on our in-house Human-Limb dataset. Our method is very efficient, running at 8 frames per second on a GPU.","","","10.1109/ICCVW.2015.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406413","","Image segmentation;Semantics;Robots;Videos;Head;Training;Testing","assisted living;geriatrics;gesture recognition;graphics processing units;handicapped aids;image segmentation;intelligent robots;learning (artificial intelligence);pose estimation","human-limb segmentation;RGB-D image;intelligent mobility assistance robot;elderly people;intelligent robotic platform;EU project;Kinect sensor;gesture recognition;pose estimation;deep learning system;GPU","","8","27","","","","","IEEE","IEEE Conferences"
"Cloud based virtualization for a calorie measurement e-health mobile application","S. V. B. Peddi; A. Yassine; S. Shirmohammadi","Distributed and Collaborative Virtual Environments Research Laboratory, University of Ottawa, Canada; Distributed and Collaborative Virtual Environments Research Laboratory, University of Ottawa, Canada; Distributed and Collaborative Virtual Environments Research Laboratory, University of Ottawa, Canada","2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","","2015","","","1","6","Smartphones have transformed people approach towards technology. It not only empowers them to use it for communication but also for tracking and maintaining health-fitness via mobile applications. With the increasing number of complex mobile applications, the mobile, with its limited resources (in terms of the computational power and storage capacity) cannot efficiently run these applications autonomously. Similarly our e-health mobile application (Eat Healthy Stay Healthy) requires a platform to run highly computational intensive algorithms like the deep learning (for recognizing food images) and calorie measurement would need higher processing power to perform optimally. We propose a cloud based virtualization model that provides our e-health application with the required computational power that it needs to perform efficiently and at the same time would also give it the flexibility to make use of the various cloud resources. Our model comprises of concepts like virtual swap between various mobile sessions that assist the system for faster processing and intelligent decision mechanism for distributing the task of image processing to cloud servers. By implementing intelligent decision mechanism, the final calorie computation significantly improved by 20.5% while implementing deep learning in cloud.","","","10.1109/ICMEW.2015.7169853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169853","Cloud based Virtualization model;Mobile e-Health Application;Virtual Swap;Mobile Cloud Computing;Intelligent Decision Mechanism in Cloud","Androids;Humanoid robots;Time measurement;Servers","cloud computing;image recognition;learning (artificial intelligence);medical information systems;mobile computing;smart phones;virtualisation","cloud based virtualization;calorie measurement e-health mobile application;smart phones;health fitness;complex mobile applications;computational power;storage capacity;Eat Healthy Stay Healthy;computational intensive algorithms;deep learning;food image recognition;cloud resources;virtual swap;mobile sessions;intelligent decision mechanism;task distribution;image processing;cloud servers","","1","12","","","","","IEEE","IEEE Conferences"
"Optimize real-valued RBM with Bidirectional Autoencoder","Q. Feng; L. Chen; C. L. P. Chen","Authors are in the Department of Computer and Information Science, University of Macau; Authors are in the Department of Computer and Information Science, University of Macau; Authors are in the Department of Computer and Information Science, University of Macau","2015 International Conference on Fuzzy Theory and Its Applications (iFUZZY)","","2015","","","22","27","Deep learning increasingly attracted attention after the fast training method of Restricted Boltzmann Machine(RBM) is proposed[1]. Many researches directly constructed deep architecture with stack RBMs to learn the representation of the data, few studied the optimization method to get good RBM parameters. Here proposes a new optimization method for real-valued RBM by minimizing the reconstructed error. Firstly, build and initialize Bidirectional Autoencoder(Bi-Ae). Secondly, minimize the cost function with Stochastic Gradient Descent (SGD) to get the parameters. Thirdly, convert the Bi-AE into RBM with the most suitable parameters. Experiments are executed on the MNIST dataset. Compared with PSO and likelihood maximum optimization methods, the reconstructed errors of the proposed method is 4.02% smaller than the result from error[1], which is advanced.","","","10.1109/iFUZZY.2015.7391888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391888","RBM Optimization;Bidirectional Autoencoder;SGD;Back Propagation","Training;Data models;Training data;Brain modeling;Cost function;Feeds","Boltzmann machines;data structures;encoding;learning (artificial intelligence);maximum likelihood estimation;particle swarm optimisation;pattern classification","bidirectional autoencoder;real-valued RBM parameter;deep learning;restricted Boltzmann machine;Bi-Ae;stochastic gradient descent;SGD;MNIST dataset;PSO;likelihood maximum optimization method;data representation","","","26","","","","","IEEE","IEEE Conferences"
"A Study on Apparent Age Estimation","Y. Zhu; Y. Li; G. Mu; G. Guo","Lane Dept. of CSEE, West Virginia Univ., Morgantown, WV, USA; Lane Dept. of CSEE, West Virginia Univ., Morgantown, WV, USA; Lane Dept. of CSEE, West Virginia Univ., Morgantown, WV, USA; Lane Dept. of CSEE, West Virginia Univ., Morgantown, WV, USA","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","267","273","Age estimation from facial images is an important problem in computer vision and pattern recognition. Typically the goal is to predict the chronological age of a person given his or her face picture. It is seldom to study a related problem, that is, how old does a person look like from?the face photo? It is called apparent age estimation. A key difference between apparent age estimation and the traditional age estimation is that the age labels are annotated by human assessors rather than the real chronological age. The challenge for apparent age estimation is that there are?not many face images available with annotated age labels. Further, the annotated age labels for each face photo may not be consistent among different assessors. We study the problem of apparent age estimation by addressing the issues from different aspects, such as how to utilize a large number of face images without apparent age labels to learn a face representation using the deep neural networks, how to tune the deep networks using a limited number of examples with apparent age labels, and how well the machine learning methods can perform to estimate apparent ages. The apparent age data is from the ChaLearn Looking At People (LAP) challenge 2015. Using the protocol and time frame given by the challenge competition, we have achieved an error of 0.294835 on the final evaluation, and our result has been ranked the 3rd place in this competition.","","","10.1109/ICCVW.2015.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406392","","Estimation;Face;Databases;Training;Neural networks;Data models;Computer architecture","age issues;computer vision;face recognition;image representation;learning (artificial intelligence);neural nets","apparent age estimation;facial images;computer vision;pattern recognition;age label annotation;face photo;deep neural networks;machine learning methods;face representation;ChaLearn Looking At People challenge 2015","","30","24","","","","","IEEE","IEEE Conferences"
"Mobile gesture-based iPhone user authentication","K. Khare; T. Moh","Department of Computer Science, San Jose State University, San Jose, CA; Department of Computer Science, San Jose State University, San Jose, CA","2015 IEEE International Conference on Big Data (Big Data)","","2015","","","1615","1621","Efforts have been made to introduce an extra layer of security on mobile devices, including a good amount of research initiated in the behavioral biometrics domain. However, all prior research approaches for mobile gesture-based authentication has been carried out uni-directionally. Despite of the fact that there are many devices with their own configurations, the study of mobile authentication based on behavioral biometrics has been done only with the Android operating system and devices. In this paper, a novel approach to identifying the owner of a mobile device based on Behavioral Biometrics Mobile Gestures Recognition is presented. This research takes the first step towards implementing behavioral biometrics identification for iOS based iPhone devices. In this research work, it is shown that a user can be identified as the true owner or an imposter of such a device based on the interactive behavior and gestures of the user. In this way continuous identification or authentication of an owner can be done based on the interaction of the user and the device. It is shown that a continuous authentication mechanism can be established using a self-learning model based on machine learning classification approaches such as Random Forests, Gradient Boosting Machine, Deep Learning, and Naive Bayes. The results in this paper show that, with behavioral biometrics, automated user authentication mechanism, EER (Equal Error Rate) can be improved to around 27%, clearly demonstrating that the chances of authenticating the user are good.","","","10.1109/BigData.2015.7363929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363929","Behavioral;Biometrics;Machine Learning;Authentication;Gestures;iOS;iPhone","Authentication;Biometrics (access control);Shape;Keyboards;Mobile communication;Mobile handsets","Android (operating system);behavioural sciences computing;biometrics (access control);gesture recognition;iOS (operating system);learning (artificial intelligence);mobile computing;security of data;smart phones","equal error rate;EER;machine learning classification approach;iOS based iPhone device;behavioral biometrics mobile gesture recognition;Android device;Android operating system;behavioral biometrics domain;mobile device;mobile gesture-based iPhone user authentication","","","17","","","","","IEEE","IEEE Conferences"
"Low-resource keyword search strategies for tamil","N. F. Chen; C. Ni; I. Chen; S. Sivadas; V. T. Pham; H. Xu; X. Xiao; T. S. Lau; S. J. Leow; B. P. Lim; C. Leung; L. Wang; C. Lee; A. Goh; E. S. Chng; B. Ma; H. Li","Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Georgia Institute of Technology, USA; Institute for Infocomm Research, A*STAR, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Georgia Institute of Technology, USA; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","5366","5370","We propose strategies for a state-of-the-art keyword search (KWS) system developed by the SINGA team in the context of the 2014 NIST Open Keyword Search Evaluation (OpenKWS14) using conversational Tamil provided by the IARPA Babel program. To tackle low-resource challenges and the rich morphological nature of Tamil, we present highlights of our current KWS system, including: (1) Submodular optimization data selection to maximize acoustic diversity through Gaussian component indexed N-grams; (2) Keywordaware language modeling; (3) Subword modeling of morphemes and homophones.","","","10.1109/ICASSP.2015.7178996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178996","Spoken term detection (STD);keyword spotting;under-resourced languages;active learning;unsupervised learning;semi-supervised learning;inflective languages;agglutinative languages;morphology;deep neural network (DNN)","Speech;Keyword search;Acoustics;Speech recognition;Data models;Training;Optimization","Gaussian processes;linguistics;optimisation;speech recognition","low-resource keyword search strategy;state-of-the-art KWS system;NIST Open Keyword Search Evaluation;conversational Tamil;IARPA Babel program;submodular optimization data selection;Gaussian component indexed N-gram;keyword aware language model;morpheme subword model;homophone subword model;speech recognition","","15","31","","","","","IEEE","IEEE Conferences"
"The Power of Seeing: Experiences using video as a deep-sea engagement and education tool","M. Hoeberechts; D. Owens; D. J. Riddell; A. D. Robertson","Ocean Networks Canada, University of Victoria, BC, Canada; Ocean Networks Canada, University of Victoria, BC, Canada; Ocean Networks Canada, University of Victoria, BC, Canada; Ocean Networks Canada, University of Victoria, BC, Canada","OCEANS 2015 - MTS/IEEE Washington","","2015","","","1","9","This paper describes initiatives underway at Ocean Networks Canada (ONC) in using video data as a tool for public engagement and education: live video streams from cameras on the seafloor, citizen science using video data, audience participation in deep-sea expeditions, and K-12 engagement through the Ocean Sense program. Live and archived video attract the majority of user traffic on ONC's website and can be leveraged to direct the viewers to other content and messaging, enhancing their engagement with the deep-sea environment. Public interest in scientific discovery creates a user base for citizen science initiatives, while educational audiences can be connected to both realtime and asynchronous learning materials. The power of live connections is also harnessed during research expeditions, which can be extended from the ship and the seafloor directly into the classroom.","","","10.23919/OCEANS.2015.7404592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404592","","Streaming media;Cameras;Oceans;Observatories;Education;Arctic;Underwater cables","educational technology;video streaming","deep-sea engagement;education tool;ocean networks Canada;public engagement;video streams;audience participation;educational audiences","","1","52","","","","","IEEE","IEEE Conferences"
"Improved Classification and Reconstruction by Introducing Independence and Randomization in Deep Neural Networks","G. Hiranandani; H. Karnick","Adobe Res., Bangalore, India; Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Kanpur, Kanpur, India","2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA)","","2015","","","1","8","This paper deals with a novel way of improving classification as well as reconstructions obtained from deep neural networks. The underlying ideas that have been used throughout are Independence and Randomization. The idea is to expose the inherent properties of neural network architectures and to make simpler models that are easy to implement rather than creating highly fine-tuned and complex neural network architectures. For the most basic type of deep neural network i.e. fully connected, it has been shown that dividing the data into independent components and training each component separately not only reduces the parameters to be learned but also the training is more efficient. And if the predictions are fused appropriately the overall accuracy also increases. Using the orthogonality of LAB colour space, it is shown that L,A and B components trained separately produce better reconstructions than RGB components taken together which in turn produce better reconstructions than LAB components taken together. Based on a similar approach, randomization has been injected into the networks so as to make different networks as independent as possible. Again fusing predictions appropriately increases accuracy. The best error on MNIST's test data set was 1.91% which is a drop by 1.05% in comparison to architectures that we created similar to [1]. As the technique is architecture independent it can be applied to other networks - for example CNNs or RNNs.","","","10.1109/DICTA.2015.7371270","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371270","","Image reconstruction;Biological neural networks;Computer architecture;Training;Neurons;Vegetation","image classification;image colour analysis;image reconstruction;neural nets","deep neural networks;neural network architecture;LAB colour space;data classification;data reconstruction","","","9","","","","","IEEE","IEEE Conferences"
"Scalable energy-efficient, low-latency implementations of trained spiking Deep Belief Networks on SpiNNaker","E. Stromatias; D. Neil; F. Galluppi; M. Pfeiffer; S. Liu; S. Furber","Advanced Processor Technologies Group, School of Computer Science, University of Manchester, M13 9PL, United Kingdom; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Winterthurerstrasse 190, CH-8057, Switzerland; Equipe de Vision et Calcul Naturel, Vision Institute, Université Pierre et Marie Curie, UMR S968 Inserm, UPMC, CNRS UMR 7210, CHNO des Quinze-Vingts, Paris, France; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Winterthurerstrasse 190, CH-8057, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Winterthurerstrasse 190, CH-8057, Switzerland; Advanced Processor Technologies Group, School of Computer Science, University of Manchester, M13 9PL, United Kingdom","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","Deep neural networks have become the state-of-the-art approach for classification in machine learning, and Deep Belief Networks (DBNs) are one of its most successful representatives. DBNs consist of many neuron-like units, which are connected only to neurons in neighboring layers. Larger DBNs have been shown to perform better, but scaling-up poses problems for conventional CPUs, which calls for efficient implementations on parallel computing architectures, in particular reducing the communication overhead. In this context we introduce a realization of a spike-based variation of previously trained DBNs on the biologically-inspired parallel SpiNNaker platform. The DBN on SpiNNaker runs in real-time and achieves a classification performance of 95% on the MNIST handwritten digit dataset, which is only 0.06% less than that of a pure software implementation. Importantly, using a neurally-inspired architecture yields additional benefits: during network run-time on this task, the platform consumes only 0.3 W with classification latencies in the order of tens of milliseconds, making it suitable for implementing such networks on a mobile platform. The results in this paper also show how the power dissipation of the SpiNNaker platform and the classification latency of a network scales with the number of neurons and layers in the network and the overall spike activity rate.","","","10.1109/IJCNN.2015.7280625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280625","","Neurons;Topology;MATLAB;Clocks","belief networks;neural nets;parallel architectures;power aware computing","deep belief network;DBN;parallel SpiNNaker platform;neuron-like unit;neurally-inspired architecture;power dissipation;classification latency","","20","30","","","","","IEEE","IEEE Conferences"
"A water quality assessment method based on sparse autoencoder","Y. Yuan; K. Jia","College of Electronic Information and Control Engineering, Beijing University of Technology, Beijing, China; College of Electronic Information and Control Engineering, Beijing University of Technology, Beijing, China","2015 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","","2015","","","1","4","Water quality assessment is very important for monitoring water sources and main canal, which is beneficial to offer strategies for the management of water quality and environment. This paper proposes a water quality assessment method based on a sparse autoencoder network. In the proposed approach, a representation model is firstly learned via a sparse autoencoder trained by unlabeled water monitoring data acquired from DanJiangKou reservoir, then a softmax classifier is trained using a small set of labeled classification data based on the China Surface Water Environmental Quality Standard (GB3838-2002) expressed by the sparse autoencoder. The combined model is finally used to evaluate the water quality. Experimental results show that the proposed method in this paper is of high robustness and accuracy of water quality assessment, and has a good prospect of practical applications.","","","10.1109/ICSPCC.2015.7338853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7338853","water quality assessment;sparse autoencoder;softmax;deep learning","Decision support systems;Indexes;Organizations;Data preprocessing;Feature extraction;Training;Testing","codecs;environmental monitoring (geophysics);quality management;reservoirs;water quality","water quality assessment method;sparse autoencoder;water sources monitoring;sparse autoencoder network;representation model;water monitoring data;DanJiangKou reservoir;China;surface water environmental quality standard","","1","16","","","","","IEEE","IEEE Conferences"
"Fast aircraft detection in satellite images based on convolutional neural networks","H. Wu; H. Zhang; J. Zhang; F. Xu","Institute of Software Chinese Academy of Sciences, China; Institute of Software Chinese Academy of Sciences, China; Institute of Software Chinese Academy of Sciences, China; Institute of Software Chinese Academy of Sciences, China","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","4210","4214","Aircraft detection in satellite images is generally difficult due to the variations of aircraft type, pose, size and complex background. In this paper, we propose a new aircraft detection framework based on objectiveness detection techniques (e.g., BING) and Convolutional Neural Networks (CNN). The advantages are two folds. On one hand, we first introduce the CNN for aircraft detection, as CNN can learn rich features from the raw data automatically and has yielded a state-of-the-art performance in many object detection tasks. On the other hand, the use of candidate object regions proposed by BING achieves a high object detection rate and saves time simultaneously. Experimental results show that the proposed method is fast and effective to detect aircrafts in complex airport scenes. We also construct a dataset for aircraft detection obtained from Google Earth.","","","10.1109/ICIP.2015.7351599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351599","Deep learning;aircraft detection;convolutional neural networks;BING;objectness","Aircraft;Proposals;Object detection;Feature extraction;Satellites;Aircraft manufacture;Military aircraft","aerospace computing;aircraft;convolution;neural nets;object detection","aircraft detection;satellite images;convolutional neural networks;objectiveness detection techniques;object detection tasks;complex airport scenes;Google Earth;BING","","17","13","","","","","IEEE","IEEE Conferences"
"FACE2GPS: Estimating geographic location from facial features","M. T. Islam; S. Workman; N. Jacobs","Department of Computer Science, University of Kentucky; Department of Computer Science, University of Kentucky; Department of Computer Science, University of Kentucky","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","1608","1612","The facial appearance of a person is a product of many factors, including their gender, age, and ethnicity. Methods for estimating these latent factors directly from an image of a face have been extensively studied for decades. We extend this line of work to include estimating the location where the image was taken. We propose a deep network architecture for making such predictions and demonstrate its superiority to other approaches in an extensive set of quantitative experiments on the GeoFaces dataset. Our experiments show that in 26% of the cases the ground truth location is the topmost prediction, and if we allow ourselves to consider the top five predictions, the accuracy increases to 47%. In both cases, the deep learning based approach significantly outperforms random chance as well as another baseline method.","","","10.1109/ICIP.2015.7351072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351072","facial features;image localization","Face;Cities and towns;Neural networks;Facial features;Computer architecture;Probability distribution;Face recognition","face recognition;gender issues","deep learning-based approach;GeoFace dataset;deep network architecture;facial feature;geographic location estimation;FACE2GPS","","2","34","","","","","IEEE","IEEE Conferences"
"Social restricted Boltzmann Machine: Human behavior prediction in health social networks","N. Phan; D. Dou; B. Piniewski; D. Kil","University of Oregon, Eugene, OR, USA; University of Oregon, Eugene, OR, USA; PeaceHealth Laboratories, Vancouver, WA, USA; HealthMantic Inc., Los Altos, CA, USA","2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","","2015","","","424","431","Modeling and predicting human behaviors, such as the activity level and intensity, is the key to prevent the cascades of obesity, and help spread wellness and healthy behavior in a social network. The user diversity, dynamic behaviors, and hidden social influences make the problem more challenging. In this work, we propose a deep learning model named Social Restricted Boltzmann Machine (SRBM) for human behavior modeling and prediction in health social networks. In the proposed SRBM model, we naturally incorporate self-motivation, implicit and explicit social influences, and environmental events together into three layers which are historical, visible, and hidden layers. The interactions among these behavior determinants are naturally simulated through parameters connecting these layers together. The contrastive divergence and back-propagation algorithms are employed for training the model. A comprehensive experiment on real and synthetic data has shown the great effectiveness of our deep learning model compared with conventional methods.","","","10.1145/2808797.2809307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403573","","Social network services;Predictive models;Obesity;Machine learning;Laboratories;Data models","backpropagation;behavioural sciences computing;Boltzmann machines;health care;medical computing;social networking (online)","social restricted Boltzmann machine;human behavior prediction;health social networks;human behavior modeling;user diversity;dynamic behaviors;hidden social influences;deep learning model;SRBM;contrastive divergence;back-propagation algorithms","","2","25","","","","","IEEE","IEEE Conferences"
"Intelligent bearing fault monitoring system using support vector machine and wavelet packet decomposition for induction motors","H. O. Vishwakarma; K. S. Sajan; B. Maheshwari; Y. D. Dhiman","NA; NA; NA; NA","2015 International Conference on Power and Advanced Control Engineering (ICPACE)","","2015","","","339","343","In this paper an intelligent condition monitoring of induction motor based on the wavelet packet decomposition and time domain features have been presented. The classification has been done using the support vector machine (SVM) on the basis of statistical learning theory. The data has been collected on a 10 HP induction motor in the lab having different bearing defects using piezoelectric type accelerometer. The signal is then processed to extract the time domain and wavelet features. Wavelet packet decomposition is used to extract the features from time-frequency domain. In this work, 3rd level wavelet packet decomposition has been considered. The experimental results shows that the classification of the bearing faults of the induction motor based on wavelet packet decomposition and time domain features and pattern recognition using support vector machine provides a new approach for intelligent bearing fault diagnosis of induction motor. GUI using MATLAB is developed for the work to make it more users friendly.","","","10.1109/ICPACE.2015.7274969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7274969","Bearing faults;Kernel function;Segmentation;SVM;Wavelet Packet Decomposition","Feature extraction;Time-frequency analysis;Support vector machines;Reliability;Frequency conversion;Time-domain analysis;Wavelet domain","condition monitoring;induction motors;pattern recognition;support vector machines","intelligent bearing fault monitoring system;support vector machine;wavelet packet decomposition;induction motors;intelligent condition monitoring;time domain features;SVM;statistical learning theory;piezoelectric type accelerometer;wavelet features;time-frequency domain;pattern recognition;GUI;MATLAB;power 10 hp","","3","20","","","","","IEEE","IEEE Conferences"
"Memory and Information Processing in Neuromorphic Systems","G. Indiveri; S. Liu","Inst. of Neuroinf., Univ. & ETH Zurich, Zurich, Switzerland; Inst. of Neuroinf., Univ. & ETH Zurich, Zurich, Switzerland","Proceedings of the IEEE","","2015","103","8","1379","1397","A striking difference between brain-inspired neuromorphic processors and current von Neumann processor architectures is the way in which memory and processing is organized. As information and communication technologies continue to address the need for increased computational power through the increase of cores within a digital processor, neuromorphic engineers and scientists can complement this need by building processor architectures where memory is distributed with the processing. In this paper, we present a survey of brain-inspired processor architectures that support models of cortical networks and deep neural networks. These architectures range from serial clocked implementations of multineuron systems to massively parallel asynchronous ones and from purely digital systems to mixed analog/digital systems which implement more biological-like models of neurons and synapses together with a suite of adaptation and learning mechanisms analogous to the ones found in biological nervous systems. We describe the advantages of the different approaches being pursued and present the challenges that need to be addressed for building artificial neural processing systems that can display the richness of behaviors seen in biological systems.","","","10.1109/JPROC.2015.2444094","European Union European Research Council; EU Future and Emerging Technologies; Swiss National Foundation Stochastic Event Inference Processor; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7159144","Convolutional networks;deep neural networks (DNNs);event-based computation;learning;massively parallel;memristor;neuromorphic computing;plasticity;VLSI;spike-timing-dependent plasticity (STDP);spiking neural network (SNN);von Neumann bottleneck","Neuromorphics;Computer architecture;Neurons;Biological neural networks;Field programmable gate arrays;Program processors;Information processing;Memory management;Brain modeling","neural nets;storage management","information processing;brain-inspired processor architectures;cortical networks;deep neural networks;multineuron systems serial clocked implementations;massively parallel asynchronous systems;purely digital systems;mixed analog-digital systems;biological nervous systems;artificial neural processing systems;memory processing","","195","167","","","","","IEEE","IEEE Journals"
"Distributed sparse HMAX model","Yulong Wang; Qingtian Zhang; Xiaolin Hu","Tsinghua National Laboratory for Information Science and Technology (TNList), Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology (TNList), Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology (TNList), Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China","2015 Chinese Automation Congress (CAC)","","2015","","","740","745","HMAX is a neuroscience-inspired deep learning model, which consists of alternating S layers and C layers, mimicking the functional mechanism of the ventral visual pathway of primates. Recently, sparse coding is introduced to the framework of HMAX for learning the S layer bases, leading to semantic features of natural images. However, when processing large scale datasets, the sparse HMAX is beset with large time and memory consumption during both training and testing. In this paper, we present a distributed algorithm to parallelize the sparse coding in S layers. Experiments on a cluster with eight nodes verify that the algorithm can also learn higher level semantic representations of objects but with much higher efficiency than the original model. The acceleration rate is linearly proportional to the number of nodes, which shows the good scalability of the proposed method.","","","10.1109/CAC.2015.7382596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382596","","Visualization;Prototypes;Memory management","compressed sensing;image coding;learning (artificial intelligence)","distributed sparse HMAX model;neuroscience-inspired deep learning model;alternating S layers;alternating C layers;primates ventral visual pathway;sparse coding;natural image semantic features","","","26","","","","","IEEE","IEEE Conferences"
"Evolving engineering education for social innovation and humanitarian impact — Lessons learned across a range of models","M. K. Ravel; B. Linder; W. C. Oakes; C. B. Zoltowski","Affordable Design and Entrepreneurship Program, Olin College of Engineering, Needham, MA, U.S.A.; Affordable Design and Entrepreneurship Program, Olin College of Engineering, Needham, MA, U.S.A.; EPICS Program, Purdue University, W. Lafayette, IN, U.S.A.; EPICS Program, Purdue University, W. Lafayette, IN, U.S.A.","2015 IEEE Global Humanitarian Technology Conference (GHTC)","","2015","","","169","176","Engineering and science education is on a trajectory in which core domain knowledge is complemented by the economic and human dimensions of technology. Adding these dimensions can attract a broader range of students to technical careers while also producing more socially conscious innovators. There is growing interest in learning models that can combine technology and community engagement for exposing students to economic and human impacts. This paper outlines lessons learned from two different institutions with programs giving students deep experiences in community-based, technical design projects across both domestic and international environments. One program has grown within a large established university for almost two decades with an emphasis on engineering applied towards community-based design, and has scaled to over 20 universities. The other program emphasizes global collaboration and has been running for five years at a small, private engineering college with a focus on the intersection of engineering, entrepreneurship and society. Highlighting common elements of the two programs gives insights into how to introduce and sustain such education models. We present the lessons learned in critical areas such as curriculum and credit, institutional context, community partnering, faculty development, student preparation and assessment, development processes, project selection, project operations, team organization, advisor roles, and mentoring.","","","10.1109/GHTC.2015.7343969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7343969","engineering;education;learning models;social innovation;entrepreneurship;community engagement;humanitarian impact;social ventures;faculty development;project selection;team formation;student preparation","Entrepreneurship;Organizations;Thumb;Engineering education;Biological system modeling","computer aided instruction;educational institutions;socio-economic effects","engineering education;social innovation;humanitarian impact;science education;community-based project;technical design project;global collaboration;private engineering college;entrepreneurship;society;institutional context;community partnering;faculty development;student preparation;project selection;project operation;team organization","","","33","","","","","IEEE","IEEE Conferences"
"Machine learning for transient discovery in Pan-STARRS1 difference imaging","D. E. Wright; S. J. Smartt; K. W. Smith; P. Miller; R. Kotak; A. Rest; W. S. Burgett; K. C. Chambers; H. Flewelling; K. W. Hodapp; M. Huber; R. Jedicke; N. Kaiser; N. Metcalfe; P. A. Price; J. L. Tonry; R. J. Wainscoat; C. Waters","NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","Monthly Notices of the Royal Astronomical Society","","2015","449","1","451","466","Efficient identification and follow-up of astronomical transients is hindered by the need for humans to manually select promising candidates from data streams that contain many false positives. These artefacts arise in the difference images that are produced by most major ground-based time-domain surveys with large format CCD cameras. This dependence on humans to reject bogus detections is unsustainable for next generation all-sky surveys and significant effort is now being invested to solve the problem computationally. In this paper, we explore a simple machine learning approach to real–bogus classification by constructing a training set from the image data of ∼32 000 real astrophysical transients and bogus detections from the Pan-STARRS1 Medium Deep Survey. We derive our feature representation from the pixel intensity values of a 20 × 20 pixel stamp around the centre of the candidates. This differs from previous work in that it works directly on the pixels rather than catalogued domain knowledge for feature design or selection. Three machine learning algorithms are trained (artificial neural networks, support vector machines and random forests) and their performances are tested on a held-out subset of 25 per cent of the training data. We find the best results from the random forest classifier and demonstrate that by accepting a false positive rate of 1 per cent, the classifier initially suggests a missed detection rate of around 10 per cent. However, we also find that a combination of bright star variability, nuclear transients and uncertainty in human labelling means that our best estimate of the missed detection rate is approximately 6 per cent.","","","10.1093/mnras/stv292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8211945","methods: data analysis;methods: statistical;techniques: image processing;surveys;supernovae: general","","","","","","","","","","","OUP","OUP Journals"
"Discovering the essence of Software Engineering an integrated game-based approach based on the SEMAT Essence specification","J. Pieper","IACS - Institute of Applied Computer Science, University of Applied Sciences Stralsund, Germany","2015 IEEE Global Engineering Education Conference (EDUCON)","","2015","","","939","947","Software processes and Software Engineering (SE) methods belong to those knowledge areas which are challenging to be taught intuitively accessible. The specification “Kernel and Language for Software Engineering Methods (Essence)” [1] claims to deliver an approach to consolidate all essential dimensions of SE-endeavors into an universal compact and actionable kernel. This paper describes the characteristics of the Essence specification with respect to its suitability for use in academic SE education where students get introduced to the world of SE methods and software processes. To enable a deep understanding of the Essence concepts in an academic setting a suitable approach is needed. The integrated approach presented in this paper introduces students stepwise into the concepts of Essence. It lets them explore the concepts in a virtual simulated game environment and finally deploy them in real world SE endeavors. Thereby an efficient and engaging learning arrangement supports the active construction of knowledge. It encourages active exploration, enables the viewing of the learning object from different perspectives and promotes articulation and reflection in social interchange early in the learning process. Key objectives of this approach are to sensitize students for the diversity of dimensions that have to be taken into account in a SE endeavor, to provide a valuable guidance for using SE methods inside and outside of their curriculum and to enable students to transfer their newly acquired knowledge to other contexts.","","","10.1109/EDUCON.2015.7096086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7096086","software engineering;software engineering education;software engineering practices;software engineering methods;software process;game-based learning;digital gamebased learning;simulation;software process simulation;SEMAT;Essence","Kernel;Software engineering;Context;Software systems;Engineering education;Conferences","computer aided instruction;computer games;computer science education;formal specification","software engineering;integrated game-based approach;SEMAT Essence specification;software processes;SE methods;academic SE education;virtual simulated game environment;social interchange;learning process","","5","22","","","","","IEEE","IEEE Conferences"
"Weakly-Supervised Structured Output Learning with Flexible and Latent Graphs Using High-Order Loss Functions","G. Carneiro; T. Peng; C. Bayer; N. Navab","Australian Centre for Visual Technol., Univ. of Adelaide, Adelaide, SA, Australia; Comput. Aided Med. Procedures, Tech. Univ. Munchen, Munich, Germany; Dept. of Radiat. Oncology, Tech. Univ. Munchen, Munich, Germany; Comput. Aided Med. Procedures, Tech. Univ. Munchen, Munich, Germany","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","648","656","We introduce two new structured output models that use a latent graph, which is flexible in terms of the number of nodes and structure, where the training process minimises a high-order loss function using a weakly annotated training set. These models are developed in the context of microscopy imaging of malignant tumours, where the estimation of the number and proportion of classes of microcirculatory supply units (MCSU) is important in the assessment of the efficacy of common cancer treatments (an MCSU is a region of the tumour tissue supplied by a microvessel). The proposed methodologies take as input multimodal microscopy images of a tumour, and estimate the number and proportion of MCSU classes. This estimation is facilitated by the use of an underlying latent graph (not present in the manual annotations), where each MCSU is represented by a node in this graph, labelled with the MCSU class and image location. The training process uses the manual weak annotations available, consisting of the number of MCSU classes per training image, where the training objective is the minimisation of a high-order loss function based on the norm of the error between the manual and estimated annotations. One of the models proposed is based on a new flexible latent structure support vector machine (FLSSVM) and the other is based on a deep convolutional neural network (DCNN) model. Using a dataset of 89 weakly annotated pairs of multimodal images from eight tumours, we show that the quantitative results from DCNN are superior, but the qualitative results from FLSSVM are better and both display high correlation values regarding the number and proportion of MCSU classes compared to the manual annotations.","","","10.1109/ICCV.2015.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410438","","Tumors;Training;Manuals;Cancer;Support vector machines;Microscopy;Computer vision","cancer;graph theory;medical image processing;neural nets;support vector machines;tumours","weakly-supervised structured output learning;flexible graph;latent graph;high-order loss function;microscopy imaging;malignant tumour;microcirculatory supply units;cancer treatment;flexible latent structure support vector machine;FLSSVM;deep convolutional neural network;DCNN model","","3","37","","","","","IEEE","IEEE Conferences"
"Building context-dependent DNN acoustic models using Kullback-Leibler divergence-based state tying","G. Gosztolya; T. Grósz; L. Tóth; D. Imseng","MTA-SZTE Research Group on Artificial Intelligence, Szeged, Hungary; MTA-SZTE Research Group on Artificial Intelligence, Szeged, Hungary; MTA-SZTE Research Group on Artificial Intelligence, Szeged, Hungary; Idiap Research Institute, Martigny, Switzerland","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4570","4574","Deep neural network (DNN) based speech recognizers have recently replaced Gaussian mixture (GMM) based systems as the state-of-the-art. HMM/DNN systems have kept many refinements of the HMM/GMM framework, even though some of these may be suboptimal for them. One such example is the creation of context-dependent tied states, for which an efficient decision tree state tying method exists. The tied states used to train DNNs are usually obtained using the same tying algorithm, even though it is based on likelihoods of Gaussians. In this paper, we investigate an alternative state clustering method that uses the Kullback-Leibler (KL) divergence of DNN output vectors to build the decision tree. It has already been successfully applied within the framework of KL-HMM systems, and here we show that it is also beneficial for HMM/DNN hybrids. In a large vocabulary recognition task we report a 4% relative word error rate reduction using this state clustering method.","","","10.1109/ICASSP.2015.7178836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178836","Speech recognition;deep neural networks;state tying;Kullback-Leibler divergence","Hidden Markov models;Speech;Artificial neural networks;Context","acoustic signal processing;decision trees;Gaussian distribution;hidden Markov models;learning (artificial intelligence);pattern clustering;speech recognition","context-dependent DNN acoustic models;Kullback-Leibler divergence-based state tying method;deep neural network based speech recognizers;decision tree state tying method;state clustering method;DNN output vectors;KL-HMM systems;vocabulary recognition task;relative word error rate reduction","","1","19","","","","","IEEE","IEEE Conferences"
"Feature enhancement based on generative-discriminative hybrid approach with gmms and DNNS for noise robust speech recognition","M. Fujimoto; T. Nakatani","NTT Communication Science Laboratories, NTT Corporation, Japan; NTT Communication Science Laboratories, NTT Corporation, Japan","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","5019","5023","This paper presents a technique that combines generative and discriminative approaches with Gaussian mixture models (GMMs) and deep neural networks (DNNs) for model-based feature enhancement. Typical model-based feature enhancement employs a generative model approach. The enhanced features are obtained by using the weighted sum of linear transformations given by each Gaussian component contained in GMMs and corresponding posterior probabilities. The computation of posterior probabilities is a crucial factor for this kind of feature enhancement, and can also be formulated as the class discrimination problem of observed noisy features. The prominent discriminability of DNNs is a well-known solution to this discrimination problem. Therefore, we propose the use of DNNs for computing posterior probabilities. The proposed method incorporates the benefit of the discriminative approach into the generative approach. For AURORA2 task evaluations, the proposed method provided noticeable improvements compared with results obtained using the conventional generative model approach.","","","10.1109/ICASSP.2015.7178926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178926","feature enhancement;generative-discriminative hybrid approach;deep neural networks;unsupervised modeling","Speech;Speech recognition;Estimation;Computational modeling;Noise reduction","feature extraction;Gaussian processes;mixture models;probability;speech recognition;unsupervised learning","model-based feature enhancement;generative-discriminative hybrid approach;GMM;Gaussian mixture models;DNN;deep neural networks;noise robust speech recognition;weighted linear transformation sum;Gaussian component;posterior probability computation;discrimination problem;AURORA2 task evaluations;unsupervised modeling","","5","28","","","","","IEEE","IEEE Conferences"
"Better Exploiting OS-CNNs for Better Event Recognition in Images","L. Wang; Z. Wang; S. Guo; Y. Qiao","Shenzhen Key Lab. of CVPR, Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Key Lab. of CVPR, Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Key Lab. of CVPR, Shenzhen Inst. of Adv. Technol., Shenzhen, China; Shenzhen Key Lab. of CVPR, Shenzhen Inst. of Adv. Technol., Shenzhen, China","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","287","294","Event recognition from still images is one of the most important problems for image understanding. However, compared with object recognition and scene recognition, event recognition has received much less research attention in computer vision community. This paper addresses the problem of cultural event recognition in still images and focuses on applying deep learning methods on this problem. In particular, we utilize the successful architecture of Object-Scene Convolutional Neural Networks (OS-CNNs) to perform event recognition. OS-CNNs are composed of object nets and scene nets, which transfer the learned representations from the pre-trained models on large-scale object and scene recognition datasets, respectively. We propose four types of scenarios to explore OS-CNNs for event recognition by treating them as either ""end-to-end event predictors"" or ""generic feature extractors"". Our experimental results demonstrate that the global and local representations of OS-CNNs are complementary to each other. Finally, based on our investigation of OS-CNNs, we come up with a solution for the cultural event recognition track at the ICCV ChaLearn Looking at People (LAP) challenge 2015. Our team secures the third place at this challenge and our result is very close to the best performance.","","","10.1109/ICCVW.2015.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406395","","Image recognition;Feature extraction;Cultural differences;Training;Object recognition;Neural networks;Computer vision","computer vision;feature extraction;learning (artificial intelligence);neural nets;object recognition","OS-CNN;event recognition;still images;image understanding;object recognition;scene recognition;computer vision community;cultural event recognition;deep learning methods;object-scene convolutional neural networks;end-to-end event predictors;generic feature extractors;ICCV ChaLearn Looking at People;LAP challenge","","5","28","","","","","IEEE","IEEE Conferences"
"Web-scale training for face identification","Y. Taigman; M. Yang; M. Ranzato; L. Wolf","Facebook AI Research, Menlo Park, CA 94025, USA; Facebook AI Research, Menlo Park, CA 94025, USA; Facebook AI Research, Menlo Park, CA 94025, USA; Tel Aviv University, Israel","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","2746","2754","Scaling machine learning methods to very large datasets has attracted considerable attention in recent years, thanks to easy access to ubiquitous sensing and data from the web. We study face recognition and show that three distinct properties have surprising effects on the transferability of deep convolutional networks (CNN): (1) The bottleneck of the network serves as an important transfer learning regularizer, and (2) in contrast to the common wisdom, performance saturation may exist in CNN's (as the number of training samples grows); we propose a solution for alleviating this by replacing the naive random subsampling of the training set with a bootstrapping process. Moreover, (3) we find a link between the representation norm and the ability to discriminate in a target domain, which sheds lights on how such networks represent faces. Based on these discoveries, we are able to improve face recognition accuracy on the widely used LFW benchmark, both in the verification (1:1) and identification (1:N) protocols, and directly compare, for the first time, with the state of the art Commercially-Off-The-Shelf system and show a sizable leap in performance.","","","10.1109/CVPR.2015.7298891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298891","","Face;Training;Benchmark testing;Entropy;Face recognition;Accuracy;Protocols","face recognition;image representation;Internet;learning (artificial intelligence);neural nets","Web-scale training;face identification;machine learning methods;face recognition;deep convolutional networks;CNN;face representation;LFW benchmark;verification protocol;identification protocol;commercially-off-the-shelf system","","74","27","","","","","IEEE","IEEE Conferences"
"A Dynamic Convolutional Layer for short rangeweather prediction","B. Klein; L. Wolf; Y. Afek","The Blavatnik School of Computer Science, Tel Aviv University, Israel; The Blavatnik School of Computer Science, Tel Aviv University, Israel; The Blavatnik School of Computer Science, Tel Aviv University, Israel","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","4840","4848","We present a new deep network layer called “Dynamic Convolutional Layer” which is a generalization of the convolutional layer. The conventional convolutional layer uses filters that are learned during training and are held constant during testing. In contrast, the dynamic convolutional layer uses filters that will vary from input to input during testing. This is achieved by learning a function that maps the input to the filters. We apply the dynamic convolutional layer to the application of short range weather prediction and show performance improvements compared to other baselines.","","","10.1109/CVPR.2015.7299117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299117","","Radar imaging;Weather forecasting;Training;Computer architecture;Convolutional codes;Image generation","geophysics computing;learning (artificial intelligence);neural nets;weather forecasting","short-range weather prediction;deep-network layer;filter learning;dynamic convolutional layer;performance improvement","","12","28","","","","","IEEE","IEEE Conferences"
"Design environment for hardware generation of SLFF neural network topologies with ELM training capability","J. Martínez-Villena; J. Francés-Víllora; A. Rosado-Muñoz; M. Bataller-Mompeán; J. Guerrero-Martínez; M. Wegrzyn; M. Adamski","GPDS. Dpt. Electronic Engineering. ETSE-School of Engineering. Universität de Valencia. 46100 Burjassot, Valencia. Spain; GPDS. Dpt. Electronic Engineering. ETSE-School of Engineering. Universität de Valencia. 46100 Burjassot, Valencia. Spain; GPDS. Dpt. Electronic Engineering. ETSE-School of Engineering. Universität de Valencia. 46100 Burjassot, Valencia. Spain; GPDS. Dpt. Electronic Engineering. ETSE-School of Engineering. Universität de Valencia. 46100 Burjassot, Valencia. Spain; GPDS. Dpt. Electronic Engineering. ETSE-School of Engineering. Universität de Valencia. 46100 Burjassot, Valencia. Spain; Institute of Computer Engineering and Electronics ul. Podgórna 50. 65-246 Zielona Góra, University of Zielona Góra. Poland; Institute of Computer Engineering and Electronics ul. Podgórna 50. 65-246 Zielona Góra, University of Zielona Góra. Poland","2015 IEEE 13th International Conference on Industrial Informatics (INDIN)","","2015","","","868","875","Extreme Learning Machine (ELM) is a noniterative training method suited for Single Layer Feed Forward Neural Networks (SLFF-NN). Typically, a hardware neural network is trained before implementation in order to avoid additional on-chip occupation, delay and performance degradation. However, ELM provides fixed-time learning capability and simplifies the process of re-training a neural network once implemented in hardware. This is an important issue in many applications where input data are continuously changing and a new training process must be launched very often, providing self-adaptation. This work describes a general SLFF-NN design environment to assist in the definition of neural network hardware implementation parameters including real-time ELM training. The software design environment uses initial user-provided input data with information about the type of problem: sample dataset and validated results, input fields, accuracy; and, together with simulation tools, recommends the optimum configuration for the neural topology and automatically generates synthesizable code for the hardware implementation tool. This is possible due to the design of parameter-dependent synthesis code and optimal hardware architecture design for both neural network and ELM training. Results show all the steps required to follow a successful design flow from the software tool to the final running device and, as an application example, the FPGA implementation for realtime detection of brain area in electrode positioning during a Deep Brain Stimulation (DBS) surgery is shown.","","","10.1109/INDIN.2015.7281850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7281850","","Random access memory;Hardware;Training;Biological neural networks;Topology;Neurons;Artificial neural networks","feedforward neural nets;field programmable gate arrays;learning (artificial intelligence);software tools;topology","hardware generation;single layer feed forward neural network;SLFF neural network topology;SLFF-NN design environment;extreme learning machine;ELM training capability;neural network hardware implementation;parameter-dependent synthesis code;hardware architecture design;software tool;FPGA implementation;deep brain stimulation;DBS surgery","","","20","","","","","IEEE","IEEE Conferences"
"Accurate Segmentation of Cervical Cytoplasm and Nuclei Based on Multiscale Convolutional Network and Graph Partitioning","Y. Song; L. Zhang; S. Chen; D. Ni; B. Lei; T. Wang","Shenzhen University; Shenzhen University; Shenzhen University; Shenzhen University; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen, China; Shenzhen University","IEEE Transactions on Biomedical Engineering","","2015","62","10","2421","2433","In this paper, a multiscale convolutional network (MSCN) and graph-partitioning-based method is proposed for accurate segmentation of cervical cytoplasm and nuclei. Specifically, deep learning via the MSCN is explored to extract scale invariant features, and then, segment regions centered at each pixel. The coarse segmentation is refined by an automated graph partitioning method based on the pretrained feature. The texture, shape, and contextual information of the target objects are learned to localize the appearance of distinctive boundary, which is also explored to generate markers to split the touching nuclei. For further refinement of the segmentation, a coarse-to-fine nucleus segmentation framework is developed. The computational complexity of the segmentation is reduced by using superpixel instead of raw pixels. Extensive experimental results demonstrate that the proposed cervical nucleus cell segmentation delivers promising results and outperforms existing methods.","","","10.1109/TBME.2015.2430895","National Natural Science Foundation of China; 48th Scientific Research Foundation for the Returned Overseas Chinese Scholars; National Natural Science Foundation of Guangdong Province; Shenzhen Key Basic Research; Shenzhen-Hong Kong Innovation Circle Funding; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7103332","Cervical segmentation;multi-scale convolutional network;graph-partitioning;touching-cell splitting;coarse to fine;Cervical segmentation;coarse to fine;graph partitioning;multiscale convolutional network (MSCN);touching-cell splitting","Shape;Image segmentation;Computer architecture;Feature extraction;Microprocessors;Image edge detection;Image color analysis","biomedical optical imaging;cancer;cellular biophysics;computational complexity;feature extraction;image segmentation;image texture;learning (artificial intelligence);medical image processing","cervical cytoplasm segmentation;multiscale convolutional network;MSCN;deep learning;scale invariant feature extraction;coarse segmentation;automated graph partitioning method;pretrained feature;texture information;shape information;contextual information;target objects;coarse-to-fine nucleus segmentation framework;computational complexity;superpixel;raw pixels;cervical nucleus cell segmentation","Cell Nucleus;Cervix Uteri;Cytoplasm;Female;Histocytochemistry;Humans;Image Processing, Computer-Assisted;Microscopy","86","51","","","","","IEEE","IEEE Journals"
"Age and gender classification using convolutional neural networks","G. Levi; T. Hassncer","Department of Mathematics and Computer Science, The Open University of Israel, Israel; Department of Mathematics and Computer Science, The Open University of Israel, Israel","2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","","2015","","","34","42","Automatic age and gender classification has become relevant to an increasing amount of applications, particularly since the rise of social platforms and social media. Nevertheless, performance of existing methods on real-world images is still significantly lacking, especially when compared to the tremendous leaps in performance recently reported for the related task of face recognition. In this paper we show that by learning representations through the use of deep-convolutional neural networks (CNN), a significant increase in performance can be obtained on these tasks. To this end, we propose a simple convolutional net architecture that can be used even when the amount of learning data is limited. We evaluate our method on the recent Adience benchmark for age and gender estimation and show it to dramatically outperform current state-of-the-art methods.","","","10.1109/CVPRW.2015.7301352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301352","","Face;Benchmark testing;Training;Estimation;Face recognition;Computer architecture;Neurons","face recognition;image classification;image representation;learning (artificial intelligence);neural net architecture;social networking (online);visual databases","automatic age classification;automatic gender classification;social platforms;social media;real-world images;face recognition;representation learning;deep-convolutional neural networks;CNN;convolutional net architecture;Adience benchmark","","254","56","","","","","IEEE","IEEE Conferences"
"Augmenting Strong Supervision Using Web Data for Fine-Grained Categorization","Z. Xu; S. Huang; Y. Zhang; D. Tao","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2524","2532","We propose a new method for fine-grained object recognition that employs part-level annotations and deep convolutional neural networks (CNNs) in a unified framework. Although both schemes have been widely used to boost recognition performance, due to the difficulty in acquiring detailed part annotations, strongly supervised fine-grained datasets are usually too small to keep pace with the rapid evolution of CNN architectures. In this paper, we solve this problem by exploiting inexhaustible web data. The proposed method improves classification accuracy in two ways: more discriminative CNN feature representations are generated using a training set augmented by collecting a large number of part patches from weakly supervised web images, and more robust object classifiers are learned using a multi-instance learning algorithm jointly on the strong and weak datasets. Despite its simplicity, the proposed method delivers a remarkable performance improvement on the CUB200-2011 dataset compared to baseline part-based R-CNN methods, and achieves the highest accuracy on this dataset even in the absence of test image annotations.","","","10.1109/ICCV.2015.290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410647","","Feature extraction;Training;Detectors;Robustness;Training data;Proposals;Computer architecture","feature extraction;image classification;image retrieval;Internet;learning (artificial intelligence);neural net architecture;object recognition;performance evaluation","strong supervision augmentation;Web data;fine-grained categorization;fine-grained object recognition;part-level annotations;deep convolutional neural networks;recognition performance;strongly supervised fine-grained datasets;CNN architectures;classification accuracy improvement;discriminative CNN feature representations;weakly supervised Web images;robust object classifiers;multiinstance learning algorithm;performance improvement;CUB200-2011 dataset","","20","35","","","","","IEEE","IEEE Conferences"
"Modeling Social Influence on Activity-Travel Behaviors Using Artificial Transportation Systems","S. Chen; Z. Liu; D. Shen","State Key Lab. of Manage. & Control for Complex Syst., Inst. of Autom., Beijing, China; Technol. on Inf. Syst. Eng. Lab., Nat. Univ. of Defense Technol., Changsha, China; Res. Center for Comput. Experiments & Parallel Syst., Nat. Univ. of Defense Technol., Changsha, China","IEEE Transactions on Intelligent Transportation Systems","","2015","16","3","1576","1581","A deep understanding of people's activity-travel behaviors is critical and essential for effective travel demand forecasting and management. Although it is acknowledged that social interactions play an important role in people's decision-making behaviors, our understanding of how they shape and impact activity-travel behaviors of people is still limited. Therefore, for the first time, this paper introduces social learning into artificial transportation systems (ATSs) to model their influence on activity-travel behaviors. Based on a specified ATS, three types of universal social interactions (i.e., imitation, conformity, and experience sharing on social networks) are modeled and studied. The results indicate that our models can make artificial agents learn to decide the best behavior, form habitual choices, and emerge fashion gradually.","","","10.1109/TITS.2014.2342279","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6907939","Activity-travel behaviors;artificial transportation systems (ATSs);social interactions;social learning;social networks;Activity-travel behaviors;artificial transportation systems (ATSs);social interactions;social learning;social networks","Computational modeling;Social network services;Sociology;Statistics;Roads","behavioural sciences;decision making;demand forecasting;transportation","activity-travel behaviors;artificial transportation systems;travel demand forecasting;travel demand management;social interactions;decision-making behaviors;social learning;ATS;universal social interactions;habitual choices","","3","35","","","","","IEEE","IEEE Journals"
"Robust Sound Event Classification Using Deep Neural Networks","I. McLoughlin; H. Zhang; Z. Xie; Y. Song; W. Xiao","National Engineering Laboratory of Speech and Language Information Processing, The University of Science and Technology of China, Hefei,PRC; National Engineering Laboratory of Speech and Language Information Processing, The University of Science and Technology of China, Hefei, PRC; National Engineering Laboratory of Speech and Language Information Processing, The University of Science and Technology of China, Hefei, PRC; National Engineering Laboratory of Speech and Language Information Processing, The University of Science and Technology of China, Hefei, PRC; European Research Center, Huawei Technologies Duesseldorf GmbH, Munich, Germany","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2015","23","3","540","552","The automatic recognition of sound events by computers is an important aspect of emerging applications such as automated surveillance, machine hearing and auditory scene understanding. Recent advances in machine learning, as well as in computational models of the human auditory system, have contributed to advances in this increasingly popular research field. Robust sound event classification, the ability to recognise sounds under real-world noisy conditions, is an especially challenging task. Classification methods translated from the speech recognition domain, using features such as mel-frequency cepstral coefficients, have been shown to perform reasonably well for the sound event classification task, although spectrogram-based or auditory image analysis techniques reportedly achieve superior performance in noise. This paper outlines a sound event classification framework that compares auditory image front end features with spectrogram image-based front end features, using support vector machine and deep neural network classifiers. Performance is evaluated on a standard robust classification task in different levels of corrupting noise, and with several system enhancements, and shown to compare very well with current state-of-the-art classification techniques.","","","10.1109/TASLP.2015.2389618","Huawei Technologies; Fundamental Research Funds for the Central Universities China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7003973","Auditory event detection;machine hearing","Vectors;Feature extraction;Support vector machines;Spectrogram;Speech;Speech processing;Auditory system","acoustic signal processing;feature extraction;neural nets;signal classification;support vector machines","sound event classification;deep neural network;DNN;auditory image front end feature;spectrogram image-based front end feature;support vector machine","","82","43","","","","","IEEE","IEEE Journals"
"EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding","Y. Miao; M. Gowayyed; F. Metze","Language Technologies Institute, School of Computer Science, Carnegie Mellon University; Language Technologies Institute, School of Computer Science, Carnegie Mellon University; Language Technologies Institute, School of Computer Science, Carnegie Mellon University","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","167","174","The performance of automatic speech recognition (ASR) has improved tremendously due to the application of deep neural networks (DNNs). Despite this progress, building a new ASR system remains a challenging task, requiring various resources, multiple training stages and significant expertise. This paper presents our Eesen framework which drastically simplifies the existing pipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen involves learning a single recurrent neural network (RNN) predicting context-independent targets (phonemes or characters). To remove the need for pre-generated frame labels, we adopt the connectionist temporal classification (CTC) objective function to infer the alignments between speech and label sequences. A distinctive feature of Eesen is a generalized decoding approach based on weighted finite-state transducers (WFSTs), which enables the efficient incorporation of lexicons and language models into CTC decoding. Experiments show that compared with the standard hybrid DNN systems, Eesen achieves comparable word error rates (WERs), while at the same time speeding up decoding significantly.","","","10.1109/ASRU.2015.7404790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404790","Recurrent neural network;connectionist temporal classification;end-to-end ASR","Hidden Markov models;Decoding;Acoustics;Training;Recurrent neural networks;Computational modeling;Speech","pattern classification;recurrent neural nets;speech recognition","EESEN framework;end-to-end speech recognition;deep RNN model;recurrent neural networks;WFST-based decoding;CTC objective function;connectionist temporal classification;weighted finite-state transducer;WER;word error rate","","168","38","","","","","IEEE","IEEE Conferences"
"Image sentiment analysis using deep convolutional neural networks with domain specific fine tuning","S. Jindal; S. Singh","Department of Information & Communication Technology, Manipal Institute of Technology, Manipal University, Manipal-576104, India; Department of Information & Communication Technology, Manipal Institute of Technology, Manipal University, Manipal-576104, India","2015 International Conference on Information Processing (ICIP)","","2015","","","447","451","Images are the easiest medium through which people can express their emotions on social networking sites. Social media users are increasingly using images and videos to express their opinions and share their experiences. Sentiment analysis of such large scale visual content can help better extract user sentiments toward events or topics, such as those in image tweets, so that prediction of sentiment from visual content is complementary to textual sentiment analysis. Significant progress has been made with this technology, however, there is little research focus on the picture sentiments. In this work, an image sentiment prediction framework is built with Convolutional Neural Networks (CNN). Specifically, this framework is pretrained on a large scale data for object recognition to further perform transfer learning. Extensive experiments were conducted on manually labeled Flickr image dataset. To make use of such labeled data, we employ a progressive strategy of domain specific fine tuning of the deep network. The results show that the proposed CNN training can achieve better performance in image sentiment analysis than competing networks.","","","10.1109/INFOP.2015.7489424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489424","","Visualization;Sentiment analysis;Neural networks;Tuning;Training;Flickr;Image classification","image processing;neural nets;object detection;social networking (online)","image sentiment analysis;deep convolutional neural networks;domain specific fine tuning;social networking sites;social media users;visual content;textual sentiment analysis;image sentiment prediction framework;CNN;object recognition;Flickr image dataset","","10","15","","","","","IEEE","IEEE Conferences"
"How to develop cloud security awareness","E. S. Ruboczki","Doctoral School on Safety and Security Sciences, Óbudai University, Budapest, Hungary","2015 IEEE 10th Jubilee International Symposium on Applied Computational Intelligence and Informatics","","2015","","","323","326","Development of Cloud Computing it is essential to develop the awareness of usage. Crowd of people use any day, on any device, from anywhere. It is necessary to handle those wide scale of users who want to keep their data in safe, and preserve their identity in the cyberspace. Cloud Computing Service Providers have to evolve their platforms as simply as it can be - but it should be given more responsibility for the customers. In an enterprise workflow the rights and policy of cloud using it could be teachable, transmissible in a very ordered way. But can we measure the effectiveness of these trainings? What type of trainings or lessons should be suitable for a company? And what type of trainings or lessons should be suitable for the employees? In my essay I observe the type of teaching, and try to scan the advantages and disadvantages of the types. With deep experiences in personal trainings, online trainings and E-learning in enterprise environment could be measured the results of the effectiveness of these knowledge transferring. Nowadays there is another way to reach the students in an easier and different way to take the advantage of addicting of play games - this another way is the gamification. The author's research to wield in the Cloud Computing teaching, especially the security of cloud and develop the cloud awareness of usage.","","","10.1109/SACI.2015.7208221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208221","Cloud Computing;Cloud Security;Cloud Awareness;Gamification;Education","Training;Security;Cloud computing;Electronic learning;Companies;Games","cloud computing;computer based training;computer games;computer science education;security of data;teaching","cloud security awareness;teaching;gamification;personal trainings;online trainings;E-learning;enterprise environment;cloud computing","","","7","","","","","IEEE","IEEE Conferences"
"Acoustic Features for Recognizing Musical Artist Influence","B. G. Morton; Y. E. Kim","Music & Entertainment Technol. Lab., Drexel Univ., Philadelphia, PA, USA; Music & Entertainment Technol. Lab., Drexel Univ., Philadelphia, PA, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","1117","1122","Musicologists have been interested in the topic of influence between composers for years and have developed methods and heuristics for recognizing influence in classical music. While these methods work well for music where the score is the primary source of information, this type of analysis is not well suited for modern popular music where the audio recording itself is arguably the primary representation. This paper presents two audio content-based systems for influence recognition: a system using a spectral representation (Constant-q transform) and support vector machines and another system that obtains features by using a deep belief network and then logistic regression for classification. The system using the spectral representation provides a baseline for future comparisons and evidence to support the idea that influence recognition can be performed using information extracted from the audio signal. The other system attempts to improve performance by using a deep belief network to learn features useful for influence recognition by mapping data extracted from the audio signal to labeled influence data. A dataset of about 77,000 30-second audio clips, consisting of retail previews of popular music tracks was gathered for this work. These songs were chosen from expertly-labeled influence relationship information gathered by the editors of the AllMusic guide.","","","10.1109/ICMLA.2015.136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424469","","Music;Feature extraction;Data models;Transforms;Support vector machines;Data mining","acoustic signal processing;audio signal processing;belief networks;feature extraction;music;signal classification;signal representation;spectral analysis;support vector machines","acoustic features;musical artist influence recognition;musicologists;classical music;musical score;audio recording;audio content-based systems;spectral representation;constant-q transform;support vector machines;deep belief network;logistic regression;classification;audio signal;audio clips;music tracks;AllMusic guide","","","17","","","","","IEEE","IEEE Conferences"
"Neural signal processing and closed-loop control algorithm design for an implanted neural recording and stimulation system","L. Hamilton; M. McConley; K. Angermueller; D. Goldberg; M. Corba; L. Kim; J. Moran; P. D. Parks; S. Chin; A. S. Widge; D. D. Dougherty; E. N. Eskandar","Charles Stark Draper Laboratory, Cambridge, Massachusetts, USA; Charles Stark Draper Laboratory, Cambridge, Massachusetts, USA; Charles Stark Draper Laboratory, Cambridge, Massachusetts, USA; Charles Stark Draper Laboratory, Cambridge, Massachusetts, USA; Charles Stark Draper Laboratory, Cambridge, Massachusetts, USA; Charles Stark Draper Laboratory, Cambridge, Massachusetts, USA; Charles Stark Draper Laboratory, Cambridge, Massachusetts, USA; Charles Stark Draper Laboratory, Cambridge, Massachusetts, USA; Charles Stark Draper Laboratory, Cambridge, Massachusetts, USA; Department of Psychiatry, Massachusetts General Hospital and Harvard Medical School, Boston, USA; Department of Psychiatry, Massachusetts General Hospital and Harvard Medical School, Boston, USA; Department of Neurological Surgery, Massachusetts General Hospital and Harvard Medical School, Boston, USA","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","7831","7836","A fully autonomous intracranial device is built to continually record neural activities in different parts of the brain, process these sampled signals, decode features that correlate to behaviors and neuropsychiatric states, and use these features to deliver brain stimulation in a closed-loop fashion. In this paper, we describe the sampling and stimulation aspects of such a device. We first describe the signal processing algorithms of two unsupervised spike sorting methods. Next, we describe the LFP time-frequency analysis and feature derivation from the two spike sorting methods. Spike sorting includes a novel approach to constructing a dictionary learning algorithm in a Compressed Sensing (CS) framework. We present a joint prediction scheme to determine the class of neural spikes in the dictionary learning framework; and, the second approach is a modified OSort algorithm which is implemented in a distributed system optimized for power efficiency. Furthermore, sorted spikes and time-frequency analysis of LFP signals can be used to generate derived features (including cross-frequency coupling, spike-field coupling). We then show how these derived features can be used in the design and development of novel decode and closed-loop control algorithms that are optimized to apply deep brain stimulation based on a patient's neuropsychiatric state. For the control algorithm, we define the state vector as representative of a patient's impulsivity, avoidance, inhibition, etc. Controller parameters are optimized to apply stimulation based on the state vector's current state as well as its historical values. The overall algorithm and software design for our implantable neural recording and stimulation system uses an innovative, adaptable, and reprogrammable architecture that enables advancement of the state-of-the-art in closed-loop neural control while also meeting the challenges of system power constraints and concurrent development with ongoing scientific research designed to define brain network connectivity and neural network dynamics that vary at the individual patient level and vary over time.","","","10.1109/EMBC.2015.7320207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320207","Neural Stimulation;Neuropsychiatric Disorders;Closed-loop Control;Decode;Signal Processing","Signal processing algorithms;Algorithm design and analysis;Dictionaries;Software algorithms;Signal processing;Real-time systems;Base stations","brain;closed loop systems;compressed sensing;medical signal processing;neurophysiology;time-frequency analysis","neural signal processing;closed-loop control algorithm design;implanted neural recording;autonomous intracranial device;neural activity;neuropsychiatric state;brain stimulation system;unsupervised spike sorting method;LFP time-frequency analysis;feature derivation;dictionary learning algorithm;compressed sensing framework;neural spikes;modified OSort algorithm;power efficiency;cross-frequency coupling;spike-field coupling;closed-loop neural control;brain network connectivity;neural network dynamics","Algorithms;Brain;Deep Brain Stimulation;Humans;Implantable Neurostimulators;Signal Processing, Computer-Assisted;Software","3","9","","","","","IEEE","IEEE Conferences"
"Deep convolutional network neocognitron: Improved Interpolating-Vector","K. Fukushima; H. Shouno","Fuzzy Logic Systems Institute, Iizuka, Fukuoka 820-0067, Japan; University of Electro-Communications, Chofu, Tokyo 182-8585, Japan","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","The neocognitron is a multi-layered convolutional network that can be trained to recognize visual patterns robustly. In the intermediate layers of the neocognitron, local features are extracted from input patterns. In the highest (or deepest) layers of the network, the method of Interpolating-Vector is used for classifying patterns based on the features extracted by the intermediate layers. During the learning, several reference vectors for each class are created from a set of training vectors. To recognize an input vector, we measure distances (based on similarities) between the input vector and planes that are spanned by every trio of reference vectors of the same class. The class name of the nearest plane is taken as the result of classification. To reduce the computational cost, we propose to search the nearest plane, not among all possible combinations of three reference vectors, but only among trios that contain the nearest reference vector. For reducing the computational cost, it is also important to represent the large number of training vectors accurately with a compact set of reference vectors. To create a compact set of reference vectors, the learning is carried out in two steps. In the first step, reference vectors are just chosen from vectors in the training set. We start modifying reference vectors (namely, fine tuning of connections) from the second step after an enough number of reference vectors have been chosen. The effectiveness of the proposed method for recognizing hand-written digits is demonstrated by computer simulation.","","","10.1109/IJCNN.2015.7280514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280514","","Training","feature extraction;interpolation;neural nets;pattern classification;vectors","deep convolutional network neocognitron;interpolating-vector;multilayered convolutional network;visual pattern recognition;intermediate layers;local feature extraction;pattern classification;reference vectors;distance measurement;computational cost reduction;nearest reference vector;hand-written digit recognition;computer simulation","","3","18","","","","","IEEE","IEEE Conferences"
"Hybrid DNN-Latent structured SVM acoustic models for continuous speech recognition","S. Ravuri","International Computer Science Institute, Berkeley, CA, University of California - Berkeley, Berkeley, CA","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","37","44","In this work, we propose Deep Neural Network (DNN)-Latent Structured Support Vector Machine (LSSVM) Acoustic Models as replacement for more standard sequence-discriminative trained DNN-HMM hybrid acoustic models. Compared to existing methods, approaches based on margin maximization, as is considered in this work, enjoy better theoretical justification. In addition to a max-margin based criteria, we also extend the Structured SVM model to include latent variables in the model to account for uncertainty in state alignments. Introducing latent structure allows for better sample complexity, often requiring 33% to 66% fewer utterances to converge compared to alternate criteria. On an 8-hour independent test set of conversational speech, the proposed method decreases word error rate by 9% relative to a cross-entropy trained hybrid system, while the best existing system decreases the word error rate by 6.5% relative.","","","10.1109/ASRU.2015.7404771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404771","Structured SVM;Deep Learning;Sequence-Discriminative Training;Large Margin;Acoustic Modeling","Hidden Markov models;Support vector machines;Training;Acoustics;Decoding;Speech recognition;Neural networks","acoustic signal processing;entropy;neural nets;optimisation;speech recognition;support vector machines","hybrid DNN-latent structured SVM acoustic models;continuous speech recognition;deep-neural network latent structured support vector machine acoustic models;standard sequence-discriminative trained DNN-HMM hybrid acoustic models;max-margin based criteria;structured SVM model;latent variables;state alignment uncertainty;latent structure;utterances;conversational speech;word error rate;cross-entropy trained hybrid system","","","33","","","","","IEEE","IEEE Conferences"
"A supervised method using convolutional neural networks for retinal vessel delineation","Q. Li; L. Xie; Q. Zhang; S. Qi; P. Liang; H. Zhang; T. Wang","National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen 518060; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen 518060; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen 518060; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen 518060; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen 518060; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen 518060; National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen 518060","2015 8th International Congress on Image and Signal Processing (CISP)","","2015","","","418","422","Retinal vessel delineation is a hot research topic owing to its importance in a lot of clinic application. Several methods have been proposed in the past decades. Here we will present a new supervised method for retinal vessel segmentation. The method is designed to explore the complex relationship between retinal images and their corresponding vessel label maps. Specifically, in order to build a model describing the direct transformation from retinal image to vessel map, we introduce a deep convolutional neural network (abbreviation as CNN), which has strong enough induction ability. For the purpose of constructing the whole vessel probability map, we also design a synthesis method. Our method shows better performance on DRIVE dataset than state-of-the-art of reported approaches in the light of sensitivity (abbreviation as Se), specificity (abbreviation as Sp) and accuracy (abbreviation as Acc). Our proposed method has great potential to be applied in existing computer-assisted diagnostic system of ophthalmologic diseases. Meanwhile, the method may offer a novel, general computing framework for segmentation in other fields.","","","10.1109/CISP.2015.7407916","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407916","CNN;retinal image;deep learning;vessel delineation","Image segmentation;Training;Retinal vessels;Measurement;Neural networks;Feature extraction","blood vessels;eye;feedforward neural nets;image segmentation;medical image processing;patient diagnosis;probability","supervised method;convolutional neural networks;retinal vessel delineation;clinic application;retinal vessel segmentation;retinal images;vessel label maps;deep convolutional neural network;vessel probability map;synthesis method;DRIVE dataset;computer-assisted diagnostic system;ophthalmologic diseases;general computing framework","","5","9","","","","","IEEE","IEEE Conferences"
"Exploring multi-channel features for denoising-autoencoder-based speech enhancement","S. Araki; T. Hayashi; M. Delcroix; M. Fujimoto; K. Takeda; T. Nakatani","NTT Communication Science Laboratories, NTT Corporation, 2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan; NTT Communication Science Laboratories, NTT Corporation, 2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan; NTT Communication Science Laboratories, NTT Corporation, 2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan; NTT Communication Science Laboratories, NTT Corporation, 2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan; Department of Media Science, Nagoya University, 1 Furo-cho, Chikusa-ku, Nagoya-shi, Aichi 464-0814, Japan; NTT Communication Science Laboratories, NTT Corporation, 2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","116","120","This paper investigates a multi-channel denoising autoencoder (DAE)-based speech enhancement approach. In recent years, deep neural network (DNN)-based monaural speech enhancement and robust automatic speech recognition (ASR) approaches have attracted much attention due to their high performance. Although multi-channel speech enhancement usually outperforms single channel approaches, there has been little research on the use of multi-channel processing in the context of DAE. In this paper, we explore the use of several multi-channel features as DAE input to confirm whether multi-channel information can improve performance. Experimental results show that certain multi-channel features outperform both a monaural DAE and a conventional time-frequency-mask-based speech enhancement method.","","","10.1109/ICASSP.2015.7177943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177943","Deep learning;denoising autoencoder;multi-channel noise suppression;PASCAL ‘CHiME’ challenge","Noise reduction;Training;Testing;Filter banks;Artificial neural networks","interference suppression;speech coding;speech enhancement","multi-channel denoising autoencoder-based speech enhancement approach;multi-channel DAE-based speech enhancement approach;deep neural network-based monaural speech enhancement;DNN-based monaural speech enhancement;robust automatic speech recognition approaches;ASR;multi-channel speech enhancement;multi-channel processing;multi-channel information","","28","25","","","","","IEEE","IEEE Conferences"
"Voice Conversion Using RNN Pre-Trained by Recurrent Temporal Restricted Boltzmann Machines","T. Nakashika; T. Takiguchi; Y. Ariki","Graduate School of System Informatics, Kobe University, Nada, Japan; Organization of Advanced Science and Technology, Kobe University, Nada, Japan; Organization of Advanced Science and Technology, Kobe University, Nada, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2015","23","3","580","587","This paper presents a voice conversion (VC) method that utilizes the recently proposed probabilistic models called recurrent temporal restricted Boltzmann machines (RTRBMs). One RTRBM is used for each speaker, with the goal of capturing high-order temporal dependencies in an acoustic sequence. Our algorithm starts from the separate training of one RTRBM for a source speaker and another for a target speaker using speaker-dependent training data. Because each RTRBM attempts to discover abstractions to maximally express the training data at each time step, as well as the temporal dependencies in the training data, we expect that the models represent the linguistic-related latent features in high-order spaces. In our approach, we convert (match) features of emphasis for the source speaker to those of the target speaker using a neural network (NN), so that the entire network (consisting of the two RTRBMs and the NN) acts as a deep recurrent NN and can be fine-tuned. Using VC experiments, we confirm the high performance of our method, especially in terms of objective criteria, relative to conventional VC methods such as approaches based on Gaussian mixture models and on NNs.","","","10.1109/TASLP.2014.2379589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6980455","Deep Learning;recurrent neural network;recurrent temporal restricted Boltzmann machine (RTRBM);speaker specific features;voice conversion","Artificial neural networks;Vectors;Speech;Acoustics;Training;Training data;Data models","acoustic signal processing;Boltzmann machines;Gaussian processes;mixture models;probability;recurrent neural nets;speaker recognition","voice conversion;RNN;VC method;probabilistic model;recurrent temporal restricted Boltzmann machines;RTRBM;high-order temporal dependency;acoustic sequence;source speaker;target speaker;speaker-dependent training data;linguistic-related latent feature;high-order spaces;neural network;deep recurrent NN;Gaussian mixture model","","28","42","","","","","IEEE","IEEE Journals"
"Leveraging appearance priors in non-rigid registration, with application to manipulation of deformable objects","S. H. Huang; J. Pan; G. Mulcaire; P. Abbeel","Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, USA; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, USA; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, USA; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, USA","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","2015","","","878","885","Manipulation of deformable objects is a widely applicable but challenging task in robotics. One promising nonparametric approach for this problem is trajectory transfer, in which a non-rigid registration is computed between the starting scene of the demonstration and the scene at test time. This registration is extrapolated to find a function from ℝ3 to ℝ3, which is then used to warp the demonstrated robot trajectory to generate a proposed trajectory to execute in the test scene. In prior work [1] [2], only depth information from the scenes has been used to compute this warp function. This approach ignores appearance information, but there are situations in which using both shape and appearance information is necessary for finding high quality non-rigid warp functions. In this paper, we describe an approach to learn relevant appearance information about deformable objects using deep learning, and use this additional information to improve the quality of non-rigid registration between demonstration and test scenes. Our method better registers areas of interest on deformable objects that are crucial for manipulation, such as rope crossings and towel corners and edges. We experimentally validate our approach in both simulation and in the real world, and show that the utilization of appearance information leads to a significant improvement in both selecting the best matching demonstration scene for a given test scene, and finding a high quality non-rigid registration between those two scenes.","","","10.1109/IROS.2015.7353475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353475","","Trajectory;Robots;Shape;Machine learning;Three-dimensional displays;Neural networks;Context","extrapolation;image registration;robot vision;trajectory control","appearance prior;nonrigid registration;deformable object manipulation;robotics;nonparametric approach;trajectory transfer;warp function;deep learning","","12","38","","","","","IEEE","IEEE Conferences"
"Human action recognition system for elderly and children care using three stream ConvNet","C. Huang; C. Wang; J. Wang","Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan, R.O.C.; Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan, R.O.C.; Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan, R.O.C.","2015 International Conference on Orange Technologies (ICOT)","","2015","","","5","9","Because of the change of family structure and population ageing, elderly and children care is become a very important issue in modern society. When adults are busy working, they have no time to care elderly and children who standalone in the home. This paper proposes an elderly and children care system to solve this important problem. The proposed intelligent surveillance system is based on action recognition technique of image processing. In this paper, a three stream convolution neural network is proposed for recognize human actions such as fall floor and baby craw. If the system detect abnormal activities are occurred, it will raise alarm and notice family members. In the experiment, there are totally 21 categories activities are collected from HMDB-51 dataset, UCF-101 dataset and Internet. The proposed system achieves 93.42% recognition rate of selected actions.","","","10.1109/ICOT.2015.7498476","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498476","action recognition;three stream ConvNet;convolutional neural network;deep learning;spatial;temporal;moving","Senior citizens;Feature extraction;Streaming media;Neural networks;Computer vision;Optical imaging;Image motion analysis","health care;image motion analysis;neural nets;object recognition;video surveillance","human action recognition system;elderly care;children care;three stream ConvNet;intelligent surveillance system;image processing;three stream convolution neural network","","7","19","","","","","IEEE","IEEE Conferences"
"Speech controlled robotics using Artificial Neural Network","N. Joshi; A. Kumar; P. Chakraborty; R. Kala","Robotics and Artificial Intelligence Laboratory, Indian Institute of Information Technology, Allahabad, India; Robotics and Artificial Intelligence Laboratory, Indian Institute of Information Technology, Allahabad, India; Robotics and Artificial Intelligence Laboratory, Indian Institute of Information Technology, Allahabad, India; Robotics and Artificial Intelligence Laboratory, Indian Institute of Information Technology, Allahabad, India","2015 Third International Conference on Image Information Processing (ICIIP)","","2015","","","526","530","Humanoid robots nowadays are able to interact with humans in realtime environments. It is always desirable that robots should have similar capacity to humans for their auditory information processing and taking actions based on the same. With this concept, in this paper we proposed a nature inspired algorithm where recognition is done by using Artificial Neural Network and and the recognized word command is used to actuate the HOAP-2 Humanoid robot In this paper we demonstrate a prototype system to control a humanoid robot using speech. The prototype can be customized for different types of tasks depending upon the utility of the robot.","","","10.1109/ICIIP.2015.7414829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7414829","Speech Recognition;Artificial Neural Network;Deep Learning;HOAP-2","Artificial neural networks;Robot sensing systems;Testing","audio signal processing;humanoid robots;human-robot interaction;neural nets;speech recognition","speech controlled robotics;artificial neural network;auditory information processing;speech recognition;HOAP-2 humanoid robot","","1","12","","","","","IEEE","IEEE Conferences"
"Categorizing Big Video Data on the Web: Challenges and Opportunities","Y. Jiang","Sch. of Comput. Sci., Fudan Univ., Shanghai, China","2015 IEEE International Conference on Multimedia Big Data","","2015","","","13","15","Video categorization is a very important problem with many applications like content search and organization, smart content-aware advertising, open-source intelligence analysis, etc. This paper discusses selected representative research progresses in categorizing big video data, with a focus on the user-generated videos on the Internet. We identify two major challenges in this vibrant field and envision promising directions that deserve in-depth future investigations. The discussions in this paper are brief but hopefully useful for quickly understanding the current progress and knowing where we should go in the next couple of years.","","","10.1109/BigMM.2015.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153768","Video Categorization;Deep Learning","Benchmark testing;Neural networks;Multimedia communication;Streaming media;Semantics;Acoustics;Feature extraction","Big Data;Internet;public domain software;video retrieval","big video data categorization;content search;smart content-aware advertising;open-source intelligence analysis;user-generated videos;Internet","","2","24","","","","","IEEE","IEEE Conferences"
"Fault diagnosis method study in roller bearing based on wavelet transform and stacked auto-encoder","T. Junbo; L. Weining; A. Juneng; W. Xueqian","Center of Intelligent Control and Telescience, Tsinghua University, Beijing 100084; Center of Intelligent Control and Telescience, Tsinghua University, Beijing 100084; State Key Laboratory of Acoustics, Chinese Academy of Sciences, Beijing, 100190; Center of Intelligent Control and Telescience, Tsinghua University, Beijing 100084","The 27th Chinese Control and Decision Conference (2015 CCDC)","","2015","","","4608","4613","Considering the nonlinear and non-stationary characteristics of fault vibration signal in the roller bearing system, an intelligent fault diagnosis model based on wavelet transform and stacked auto-encoder is proposed. This paper firstly uses the combination of digital wavelet frame (DWF) and nonlinear soft threshold method to de-noise fault vibration signal. Then stacked auto-encoder is taken to extract the fault signal feature, which is regarded as the input of BP network classifier. The output results of BP network classifier represent fault categories. In addition, neural network ensemble method is also adopted to greatly improve the recognition rate of fault diagnosis.","","","10.1109/CCDC.2015.7162738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7162738","wavelet transform;stacked auto-encoder;roller bearing;fault diagnosis;deep learning","Feature extraction;Vibrations;Fault diagnosis;Wavelet transforms;Neural networks;Noise reduction","backpropagation;fault diagnosis;feature extraction;neural nets;rolling bearings;signal classification;signal denoising;vibrational signal processing;wavelet transforms","nonlinear characteristics;nonstationary characteristics;fault vibration signal;roller bearing system;intelligent fault diagnosis model;wavelet transform;stacked autoencoder;digital wavelet frame;nonlinear soft threshold method;fault vibration signal denoising;fault signal feature extraction;BP network classifier;fault categories;neural network ensemble method;fault diagnosis recognition rate;roller bearing","","14","10","","","","","IEEE","IEEE Conferences"
"Multilingual representations for low resource speech recognition and keyword search","J. Cui; B. Kingsbury; B. Ramabhadran; A. Sethy; K. Audhkhasi; X. Cui; E. Kislal; L. Mangu; M. Nussbaum-Thom; M. Picheny; Z. Tüske; P. Golik; R. Schlüter; H. Ney; M. J. F. Gales; K. M. Knill; A. Ragni; H. Wang; P. Woodland","IBM Watson, 1101 Kitchawan Rd, Yorktown Heights, NY, 10598, U.S.A.; IBM Watson, 1101 Kitchawan Rd, Yorktown Heights, NY, 10598, U.S.A.; IBM Watson, 1101 Kitchawan Rd, Yorktown Heights, NY, 10598, U.S.A.; IBM Watson, 1101 Kitchawan Rd, Yorktown Heights, NY, 10598, U.S.A.; IBM Watson, 1101 Kitchawan Rd, Yorktown Heights, NY, 10598, U.S.A.; IBM Watson, 1101 Kitchawan Rd, Yorktown Heights, NY, 10598, U.S.A.; IBM Watson, 1101 Kitchawan Rd, Yorktown Heights, NY, 10598, U.S.A.; IBM Watson, 1101 Kitchawan Rd, Yorktown Heights, NY, 10598, U.S.A.; IBM Watson, 1101 Kitchawan Rd, Yorktown Heights, NY, 10598, U.S.A.; IBM Watson, 1101 Kitchawan Rd, Yorktown Heights, NY, 10598, U.S.A.; Computer Science Department, RWTH Aachen University, 52056 Aachen, Germany; Computer Science Department, RWTH Aachen University, 52056 Aachen, Germany; Computer Science Department, RWTH Aachen University, 52056 Aachen, Germany; Computer Science Department, RWTH Aachen University, 52056 Aachen, Germany; Cambridge University Engineering Department, Trumpington Street, Cambridge, CB2 1PZ, UK; Cambridge University Engineering Department, Trumpington Street, Cambridge, CB2 1PZ, UK; Cambridge University Engineering Department, Trumpington Street, Cambridge, CB2 1PZ, UK; Cambridge University Engineering Department, Trumpington Street, Cambridge, CB2 1PZ, UK; Cambridge University Engineering Department, Trumpington Street, Cambridge, CB2 1PZ, UK","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","259","266","This paper examines the impact of multilingual (ML) acoustic representations on Automatic Speech Recognition (ASR) and keyword search (KWS) for low resource languages in the context of the OpenKWS15 evaluation of the IARPA Babel program. The task is to develop Swahili ASR and KWS systems within two weeks using as little as 3 hours of transcribed data. Multilingual acoustic representations proved to be crucial for building these systems under strict time constraints. The paper discusses several key insights on how these representations are derived and used. First, we present a data sampling strategy that can speed up the training of multilingual representations without appreciable loss in ASR performance. Second, we show that fusion of diverse multilingual representations developed at different LORELEI sites yields substantial ASR and KWS gains. Speaker adaptation and data augmentation of these representations improves both ASR and KWS performance (up to 8.7% relative). Third, incorporating un-transcribed data through semi-supervised learning, improves WER and KWS performance. Finally, we show that these multilingual representations significantly improve ASR and KWS performance (relative 9% for WER and 5% for MTWV) even when forty hours of transcribed audio in the target language is available. Multilingual representations significantly contributed to the LORELEI KWS systems winning the OpenKWS15 evaluation.","","","10.1109/ASRU.2015.7404803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404803","Multilingual Representation;Hierarchical Deep Neural Network;Keyword Search;BABEL","Training;Training data;Keyword search;Context;Data models;Acoustics;Neural networks","learning (artificial intelligence);natural language processing;speech recognition","LORELEI KWS systems;multilingual representations;semisupervised learning;data augmentation;speaker adaptation;LORELEI sites;ASR performance;data sampling strategy;Swahili KWS systems;Swahili ASR systems;IARPA Babel program;OpenKWS15 evaluation;low resource languages;automatic speech recognition;multilingual acoustic representations;keyword search;low resource speech recognition","","15","47","","","","","IEEE","IEEE Conferences"
"Query Specific Rank Fusion for Image Retrieval","S. Zhang; M. Yang; T. Cour; K. Yu; D. N. Metaxas","Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC; AI Research, Facebook Inc., 1601 Willow Road, Menlo Park, CA; Google Inc., Mountain View, CA; Institute of Deep Learning, Baidu Inc., Beijing, P.R., China; Department of Computer Science, Rutgers the State University of New Jersey, Division of Computer and Information Sciences, Room C324, Hill Center Bldg for the Mathematical Sciences, Piscataway, NJ","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2015","37","4","803","815","Recently two lines of image retrieval algorithms demonstrate excellent scalability: 1) local features indexed by a vocabulary tree, and 2) holistic features indexed by compact hashing codes. Although both of them are able to search visually similar images effectively, their retrieval precision may vary dramatically among queries. Therefore, combining these two types of methods is expected to further enhance the retrieval precision. However, the feature characteristics and the algorithmic procedures of these methods are dramatically different, which is very challenging for the feature-level fusion. This motivates us to investigate how to fuse the ordered retrieval sets, i.e., the ranks of images, given by multiple retrieval methods, to boost the retrieval precision without sacrificing their scalability. In this paper, we model retrieval ranks as graphs of candidate images and propose a graph-based query specific fusion approach, where multiple graphs are merged and reranked by conducting a link analysis on a fused graph. The retrieval quality of an individual method is measured on-the-fly by assessing the consistency of the top candidates' nearest neighborhoods. Hence, it is capable of adaptively integrating the strengths of the retrieval methods using local or holistic features for different query images. This proposed method does not need any supervision, has few parameters, and is easy to implement. Extensive and thorough experiments have been conducted on four public datasets, i.e., the UKbench, Corel-5K, Holidays and the large-scale San Francisco Landmarks datasets. Our proposed method has achieved very competitive performance, including state-of-the-art results on several data sets, e.g., the N-S score 3.83 for UKbench.","","","10.1109/TPAMI.2014.2346201","NEC Laboratories America; University of North Carolina at Charlotte; Oak Ridge Associated Universities; Ralph E. Powe Junior Faculty Enhancement Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6873347","Large-scale image retrieval;vocabulary tree;hashing;graph-based fusion;query specific fusion","Vocabulary;Image retrieval;Visualization;Image edge detection;Fuses;Scalability","image fusion;image retrieval;trees (mathematics)","query specific rank fusion;image retrieval algorithms;vocabulary tree;compact hashing codes;feature-level fusion;ordered retrieval set fusion;multiple retrieval methods;graph-based query specific fusion approach;link analysis;top candidate nearest neighborhoods;UKbench public datasets;Corel-5K public datasets;Holidays public datasets;large-scale San Francisco landmarks datasets","","94","47","","","","","IEEE","IEEE Journals"
"RRAM-Based Analog Approximate Computing","B. Li; P. Gu; Y. Shan; Y. Wang; Y. Chen; H. Yang","Department of Electronic Engineering, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Baidu Research-Institute for Deep Learning, Baidu Inc., Beijing, China; Department of Electronic Engineering, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA, USA; Department of Electronic Engineering, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2015","34","12","1905","1917","Approximate computing is a promising design paradigm for better performance and power efficiency. In this paper, we propose a power efficient framework for analog approximate computing with the emerging metal-oxide resistive switching random-access memory (RRAM) devices. A programmable RRAM-based approximate computing unit (RRAM-ACU) is introduced first to accelerate approximated computation, and an approximate computing framework with scalability is then proposed on top of the RRAM-ACU. In order to program the RRAM-ACU efficiently, we also present a detailed configuration flow, which includes a customized approximator training scheme, an approximator-parameter-to-RRAM-state mapping algorithm, and an RRAM state tuning scheme. Finally, the proposed RRAM-based computing framework is modeled at system level. A predictive compact model is developed to estimate the configuration overhead of RRAM-ACU and help explore the application scenarios of RRAM-based analog approximate computing. The simulation results on a set of diverse benchmarks demonstrate that, compared with a x86-64 CPU at 2 GHz, the RRAM-ACU is able to achieve 4.06-196.41× speedup and power efficiency of 24.59-567.98 GFLOPS/W with quality loss of 8.72% on average. And the implementation of hierarchical model and X application demonstrates that the proposed RRAM-based approximate computing framework can achieve 12.8× power efficiency than its pure digital implementation counterparts (CPU, graphics processing unit, and field- programmable gate arrays).","","","10.1109/TCAD.2015.2445741","973 Project; National Natural Science Foundation of China; Brain Inspired Computing Research, Tsinghua University; Tsinghua University Initiative Scientific Research Program; Importation and Development of High-Caliber Talents Project of Beijing Municipal Institutions, National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7123597","RRAM;approximate computing;power efficiency;neural network;Approximate computing;neural network;power efficiency;resistive random-access memory (RRAM)","Approximation methods;Resistance;Training;Arrays;Computational modeling;Hardware;Approximation algorithms","resistive RAM","power efficient framework;metal-oxide resistive switching random-access memory devices;approximate computing unit;ACU;configuration flow;training scheme;approximator-parameter-to-RRAM-state mapping algorithm;hierarchical model;CPU;graphics processing unit;field-programmable gate arrays;FPGA;frequency 2 GHz","","42","42","","","","","IEEE","IEEE Journals"
"On constructing and analysing an interpretable brain model for the DNN based on hidden activity patterns","K. C. Sim","Computer Science Department, School of Computing, National University of Singapore, Singapore","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","22","29","Deep Neural Network (DNN) has been well received as a powerful machine learning model in a wide range of pattern classification tasks. Despite its superior performance in handling complex real-world problems, DNNs have been used pretty much as a black box, without offering much insights in terms of how and why high quality classification performance has been achieved. To address this problem, this paper studies the DNN hidden unit activities and presents a novel interpretable DNN visualisation technique that projects the hidden units of the DNN onto a meaningful 2-dimensional subspace. The projected points are displayed with colours to reflect the activation values for the purpose of visualisation. In this paper, the proposed technique is used to visualise two DNN acoustic models trained on the multi-condition data from the Aurora 4 corpus. The technique is able to produce a two dimensional representation of the DNN ""brain"" with interpretable regions. It also accentuates the effect of how the behaviour of the hidden units changes across different layers.","","","10.1109/ASRU.2015.7404769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404769","deep neural network;visualisation;interpretability","Acoustics;Entropy;Brain modeling;Shape;Speech;Visualization;Training","data visualisation;neural nets;pattern classification","interpretable brain model;deep neural network;hidden activity patterns;machine learning model;pattern classification tasks;DNN visualisation technique;DNN acoustic models;Aurora 4 corpus","","3","24","","","","","IEEE","IEEE Conferences"
"A Hybrid Vertex Outlier Detection Method Based on Distributed Representation and Local Outlier Factor","Z. Li; L. Zeng","NA; NA","2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)","","2015","","","512","516","Outlier detection is a basic task in network analysis, which is useful in many applications such as intrusion detection, criminal investigation, and information filtering. In this paper we proposed a hybrid outlier detection methods in complex networks based on Vertex Distributed Representation and Local Outlier Factor, with the aim to find abnormal vertexes that are apart from the group or community in complex networks. The proposed outlier detection method based on Vertex Distributed Representation (VDR) and Local Outlier Factor (LOF) is named as VDR-LOF. VDR-LOF maps vertexes or edges into a density continuous real-valued space, and then uses LOF algorithm to detection the outliers. We conducted experiments on American College Football Network and Enron Email Network, visualized the original networks and its corresponding feature map in 2D space, then we found the vertex outliers in the network.","","","10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518283","Outlier Detection;Distributed Representation;Local Outlier Factor;Deep Learning","Electronic mail;Visualization;Complex networks;Image edge detection;Feature extraction;Machine learning;Linear programming","complex networks;network theory (graphs)","hybrid vertex outlier detection method;local outlier factor;network analysis;intrusion detection;criminal investigation;information filtering;vertex distributed representation;complex networks;VDR-LOF;density continuous real-valued space;LOF algorithm;American College Football Network;Enron Email Network","","","29","","","","","IEEE","IEEE Conferences"
"Unsupervised hierarchical convolutional sparse auto-encoder for high spatial resolution imagery scene classification","Xiaobing Han; Yanfei Zhong; Bei Zhao; Liangpei Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, 430079, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, 430079, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, 430079, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, 430079, China","2015 11th International Conference on Natural Computation (ICNC)","","2015","","","42","46","Recently, efficiently representing the scenes from a large volume of high spatial resolution (HSR) images is a critical problem to be solved. Traditional scene classification problems were solved by utilizing the spatial, spectral and structural features of the HSR images separately or jointly, which lacks considering all those features of the images integrally and automatically. In this paper, we propose an efficient hierarchical convolutional sparse auto-encoder (HCSAE) algorithm considering all the features of the images integrally for scene classification, which adopts the unsupervised hierarchical idea based on the single-hierarchy convolutional sparse auto-encoder (CSAE). Compared with the single-hierarchy CSAE, HCSAE can extract more robust and efficient features containing abundant detail and structural information in the higher hierarchy for scene classification. To further improve the calculation performance and reduce the over-fitting of the network, a “dropout” strategy is adopted in this paper. The experimental results were confirmed by the UC Merced dataset consisting of 21 land-use categories, and showed that HCSAE performs better than the traditional scene classification methods and the single-hierarchy CSAE algorithm.","","","10.1109/ICNC.2015.7377963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7377963","Deep Learning;hierarchical convolutional sparse autoencoder (HCSAE);HSR images;scene classification;dropout","Feature extraction;Convolution;Data mining;Encoding;Machine learning;Decoding;Robustness","convolutional codes;geophysical image processing;image classification;image coding;image resolution","unsupervised hierarchical convolutional sparse auto-encoder;high-spatial resolution imagery scene classification;high-spatial resolution images;HSR images;scene classification problem;spatial feature;spectral feature;structural feature;image feature;HCSAE algorithm;unsupervised hierarchical idea;dropout strategy;UC Merced dataset;single-hierarchy CSAE algorithm","","","19","","","","","IEEE","IEEE Conferences"
"On using heterogeneous data for vehicle-based speech recognition: A DNN-based approach","X. Feng; B. Richardson; S. Amman; J. Glass","MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA, 02139; Ford Motor Company, USA; Ford Motor Company, USA; MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA, 02139","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4385","4389","Most automatic speech recognition (ASR) systems incorporate a single source of information about their input, namely, features and transformations derived from the speech signal. However, in many applications, e.g., vehicle-based speech recognition, sensor data and environmental information are often available to complement audio information. In this paper, we show how these data can be used to improve hybrid DNN-HMM ASR systems for a vehicle-based speech recognition task. Feature fusion is accomplished by augmenting acoustic features with additional side information before being presented to the DNN acoustic model. The additional features are extracted from the vehicle speed, HVAC status, windshield wiper status, and vehicle type. This supplementary information improves the DNNs ability to discriminate phonetic events in an environment-aware way without having to make any modification to the DNN training algorithms. Experimental results show that heterogeneous data are effective irrespective of whether cross-entropy or sequence training is used. For CE training, a WER reduction of 6.3% is obtained, while sequential training reduces it by 5.5%.","","","10.1109/ICASSP.2015.7178799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178799","Noise Robustness;Deep Neural Network;Additional Feature for ASR;Condition-aware DNN","Hidden Markov models;Robustness;Mel frequency cepstral coefficient;Speech;Computational modeling;Vehicles","entropy;learning (artificial intelligence);neural nets;speech processing;speech recognition;vehicles","heterogeneous data;vehicle-based speech recognition;automatic speech recognition;speech signal;sensor data;DNN-HMM ASR system;feature fusion;acoustic feature augmentation;DNN acoustic model;vehicle speed;HVAC status;windshield wiper status;vehicle type;phonetic event discrimination;DNN training algorithm;cross-entropy;sequence training;WER reduction;deep neural network","","6","19","","","","","IEEE","IEEE Conferences"
"DRN: Bringing Greedy Layer-Wise Training into Time Dimension","X. Li; X. Jia; H. Li; H. Xiao; J. Gao; A. Zhang","Dept. of Comput. Sci. & Eng., State Univ. of New York at Buffalo, Buffalo, NY, USA; Dept. of Comput. Sci. & Eng., State Univ. of New York at Buffalo, Buffalo, NY, USA; Dept. of Comput. Sci. & Eng., State Univ. of New York at Buffalo, Buffalo, NY, USA; Dept. of Comput. Sci. & Eng., State Univ. of New York at Buffalo, Buffalo, NY, USA; Dept. of Comput. Sci. & Eng., State Univ. of New York at Buffalo, Buffalo, NY, USA; Dept. of Comput. Sci. & Eng., State Univ. of New York at Buffalo, Buffalo, NY, USA","2015 IEEE International Conference on Data Mining","","2015","","","859","864","Sequential data modeling has received growing interests due to its impact on real world problems. Sequential data is ubiquitous - financial transactions, advertise conversions and disease evolution are examples of sequential data. A long-standing challenge in sequential data modeling is how to capture the strong hidden correlations among complex features in high volumes. The sparsity and skewness in the features extracted from sequential data also add to the complexity of the problem. In this paper, we address these challenges from both discriminative and generative perspectives, and propose novel stochastic learning algorithms to model nonlinear variances from static time frames and their transitions. The proposed model, Deep Recurrent Network (DRN), can be trained in an unsupervised fashion to capture transitions, or in a discriminative fashion to conduct sequential labeling. We analyze the conditional independence of each functional module and tackle the diminishing gradient problem by developing a two-pass training algorithm. Extensive experiments on both simulated and real-world dynamic networks show that the trained DRN outperforms all baselines in the sequential classification task and obtains excellent performance in the regression task.","","","10.1109/ICDM.2015.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373402","","Training;Hidden Markov models;Data models;Computational modeling;Mathematical model;Data mining;Heuristic algorithms","feature extraction;pattern classification;recurrent neural nets;regression analysis;unsupervised learning","sequential data modeling;financial transactions;advertise conversions;disease evolution;complex feature extraction;generative perspectives;stochastic learning algorithms;nonlinear variances;static time frames;deep recurrent network;unsupervised training;discriminative perspectives;sequential labeling;two-pass training algorithm;simulated dynamic networks;real-world dynamic networks;trained DRN;sequential classification task;regression task","","","25","","","","","IEEE","IEEE Conferences"
"On the use of statistical semantics for metadata-based social image retrieval","N. Rekabsaz; R. Bierig; B. Ionescu; A. Hanbury; M. Lupu","Information and Software Engineering Group, Vienna University of Technology, A-1040 Vienna, Austria; Information and Software Engineering Group, Vienna University of Technology, A-1040 Vienna, Austria; LAPI, University Politehnica of Bucharest, 061071 Romania; Information and Software Engineering Group, Vienna University of Technology, A-1040 Vienna, Austria; Information and Software Engineering Group, Vienna University of Technology, A-1040 Vienna, Austria","2015 13th International Workshop on Content-Based Multimedia Indexing (CBMI)","","2015","","","1","4","We revisit text-based image retrieval for social media, exploring the opportunities offered by statistical semantics. We assess the performance and limitation of several complementary corpus-based semantic text similarity methods in combination with word representations. We compare results with state-of-the-art text search engines. Our deep learning-based semantic retrieval methods show a statistically significant improvement in comparison to a best practice Solr search engine, at the expense of a significant increase in processing time. We provide a solution for reducing the semantic processing time up to 48% compared to the standard approach, while achieving the same performance.","","","10.1109/CBMI.2015.7153634","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153634","","Semantics;Image retrieval;Indexing;Context;Encyclopedias;Correlation","image retrieval;learning (artificial intelligence);social networking (online);statistical analysis;text analysis","statistical semantics;metadata-based social image retrieval;text-based image retrieval;social media;corpus-based semantic text similarity methods;word representations;deep learning-based semantic retrieval methods;Solr search engine","","4","17","","","","","IEEE","IEEE Conferences"
"Sleep stages classification using shallow classifiers","E. P. Giri; A. M. Arymurthy; M. I. Fanany; S. K. Wijaya","Faculty of Computer Science, University of Indonesia, Depok, West Java, Indonesia; Faculty of Computer Science, University of Indonesia, Depok, West Java, Indonesia; Faculty of Computer Science, University of Indonesia, Depok, West Java, Indonesia; Department of Physics, Faculty of Mathematics and Natural Sciences, University of Indonesia, Depok, West Java, Indonesia","2015 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","","2015","","","297","301","A person with sleep disorder such as apnea will stop breathing for a while during sleep. If frequently occurs, sleep disorder is dangerous for health. An early step for diagnosing apnea is by classifying the sleep stages during sleep. This study explores some shallow classifiers and their feasibility applied to sleep data. Recently, a sleep stages classification system that use deep unsupervised features learning representations have been proposed [9]. In our view, an adequate study on this problem using shallow classifiers still need to be investigated. This study, using some of the data on [9], focuses on evaluating some shallow classifier to the sleep stages classification problem. This study evaluates five classifiers: SVM, Neural Network, Classification Tree, k-Nearest Neighborhood (k-NN), and Naive Bayes. Experiment result shows that neural network gives best performance for sleep stage classification problem. Compared to the SVM (the 2-nd rank of accuracy on S000 data), the neural network is also more efficient than SVM in term of computational time and memory requirement.","","","10.1109/ICACSIS.2015.7415162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415162","sleep stage classification;classifier;neural network;SVM;EEG","Support vector machines;Silicon;Bayes methods;Neural networks;Robustness;Electrooculography;Electromyography","medical computing;medical disorders;neural nets;patient diagnosis;pattern classification;sleep;support vector machines;unsupervised learning","sleep stage classification system;shallow classifier;sleep disorder;apnea diagnosis;deep unsupervised features learning;support vector machine;SVM;neural network;classification tree;k-nearest neighborhood;k-NN algorithm;naive Bayes","","2","14","","","","","IEEE","IEEE Conferences"
"A computational model for predicting local distortion visibility via convolutional neural network trainedon natural scenes","M. M. Alam; P. Patil; M. T. Hagan; D. M. Chandler","School of Electrical and Computer Engineering, Oklahoma State University, USA; School of Electrical and Computer Engineering, Oklahoma State University, USA; School of Electrical and Computer Engineering, Oklahoma State University, USA; Department of Electrical and Electronic Engineering, Shizuoka University, Hamamatsu, Japan","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","3967","3971","A crucial requirement for modern image coding is the ability to accurately and efficiently predict the local visibility of coding artifacts. Such predictions could help guide the allocation of bits or the determination of quality for each spatial region. This paper presents a convolutional-neural-network-based (CNN-based) model to predict local distortion visibility in natural scenes. Although CNNs have recently emerged as a powerful tool for many computer vision applications due to its deep learning abilities and computational efficiency, CNNs have never been tested for predicting continuous values such as visibility thresholds. We optimized the model's parameters on our recently published large dataset on local masking in natural scenes [Alam et al., Journal of Vision, 2014]. Testing results demonstrate that our CNN-based model: (1) can indeed succeed in this task; (2) can more accurately predict thresholds than modern gain-control-based models; (3) is competitive in terms of prediction accuracy with a gain-control model tuned to the same dataset; and (4) is significantly more computationally efficient than modern gain-controls models.","","","10.1109/ICIP.2015.7351550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351550","Local distortion visibility;convolutional neural network;visual masking;contrast gain control","Computational modeling;Distortion;Predictive models;Convolution;Kernel;Biological system modeling;Computer architecture","computer vision;image coding;learning (artificial intelligence);natural scenes;neural nets","convolutional neural network;natural scene;computational model;image coding;convolutional-neural-network-based model;CNN-based model;local distortion visibility;computer vision application;deep learning;modern gain-control-based model","","2","27","","","","","IEEE","IEEE Conferences"
"DeepBag: Recognizing Handbag Models","Y. Wang; S. Li; A. C. Kot","Rapid-Rich Object SEarch (ROSE) Lab, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore; Rapid-Rich Object SEarch (ROSE) Lab, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore; Room S1-B1a-20, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore","IEEE Transactions on Multimedia","","2015","17","11","2072","2083","In this paper, we address the problem of branded handbag recognition. It is a challenging problem due to the non-rigid deformation, illumination changes, and inter-class similarity. We propose a novel framework based on deep convolutional neural network (CNN). Concretely, we propose a new CNN model, called feature selective joint classification - regression CNN (FSCR-CNN). Its advantages lie in two folds: 1) it alleviates the illumination changes by a feature selection strategy to focus on the color- nondiscriminative features in the network learning, and 2) rather than only targeting on the hard label (i.e., the handbag model), it also incorporates a soft label (i.e., a distribution measuring the similarity between the ground truth model and all the models to be trained) to construct the loss function for training CNN, which leads to a better classifier for handbags with large inter-class similarity. We evaluate the performance of our framework on a newly built branded handbag dataset. The results show that it performs favorably for recognizing handbags with 94.48% in accuracy. We also apply the proposed FSCR-CNN model in recognizing other fine-grained objects with state-of-the-art CNN architectures, which is able to achieve over 5% improvement in accuracy.","","","10.1109/TMM.2015.2480228","Singapore National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272090","Convolutional neural networks;feature selection;handbag recognition;soft label","Image recognition;Image color analysis;Power capacitors;Training;Object recognition;Lighting;Computer architecture","feature selection;feedforward neural nets;image classification;image colour analysis;learning (artificial intelligence);lighting;object recognition;regression analysis","DeepBag;branded handbag recognition problem;nonrigid deformation;illumination changes;interclass similarity;deep convolutional neural network;feature selective joint classification-regression CNN model;FSCR-CNN model;feature selection strategy;network learning;similarity distribution measurement;ground truth model;loss function","","5","63","","","","","IEEE","IEEE Journals"
"Inferencing in information extraction: Techniques and applications","D. Barbosa; H. Wang; C. Yu","University of Alberta, Edmonton, Canada; Google Inc., Mountain View, CA, USA; Google Inc., New York, USA","2015 IEEE 31st International Conference on Data Engineering","","2015","","","1534","1537","Information extraction at Web scale has become one of the most important research topics in data management since major commercial search engines started incorporating knowledge in their search results a couple of years ago [1]. Users increasingly expect structured knowledge as answers to their search needs. Using Bing as an example, the result page for “Lionel Messi” is full of structured knowledge facts, such as his birthday and awards. The research efforts towards improving the accuracy and coverage of such knowledge bases have led to significant advances in Information Extraction techniques [2], [3]. As the initial challenge of accurately extracting facts for popular entities are being addressed, more difficult challenges have emerged such as extending knowledge coverage to long tail entities and domains, understanding interestingness and usefulness of facts within a given context, and addressing information-seeking needs more directly and accurately. In this tutorial, we will survey the recent research efforts and provide an introduction to the techniques that address those challenges, and the applications that benefit from the adoption of those techniques. In particular, this tutorial will focus on a variety of techniques that can be broadly viewed as knowledge inferencing, i.e., combining multiple data sources and extraction techniques to verify existing knowledge and derive new knowledge. More specifically, we focus on four main categories of inferencing techniques: 1) deep natural language processing using machine learning techniques, 2) data cleaning using integrity constraints, 3) large-scale probabilistic reasoning, and 4) leveraging human expertise for domain knowledge extraction.","","","10.1109/ICDE.2015.7113420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7113420","","Information retrieval;Data mining;Tutorials;Knowledge based systems;Knowledge engineering;Google;Cleaning","data integrity;inference mechanisms;information retrieval;Internet;learning (artificial intelligence);natural language processing;search engines","knowledge inferencing;information extraction techniques;Web scale;data management;commercial search engines;Bing;Lionel Messi;fact extraction;knowledge coverage;information-seeking needs;multiple data sources;deep natural language processing;machine learning techniques;data cleaning;integrity constraints;large-scale probabilistic reasoning;human expertise leverage;domain knowledge extraction","","1","25","","","","","IEEE","IEEE Conferences"
"Material recognition in the wild with the Materials in Context Database","S. Bell; P. Upchurch; N. Snavely; K. Bala","Department of Computer Science, Cornell University, USA; Department of Computer Science, Cornell University, USA; Department of Computer Science, Cornell University, USA; Department of Computer Science, Cornell University, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3479","3487","Recognizing materials in real-world images is a challenging task. Real-world materials have rich surface texture, geometry, lighting conditions, and clutter, which combine to make the problem particularly difficult. In this paper, we introduce a new, large-scale, open dataset of materials in the wild, the Materials in Context Database (MINC), and combine this dataset with deep learning to achieve material recognition and segmentation of images in the wild. MINC is an order of magnitude larger than previous material databases, while being more diverse and well-sampled across its 23 categories. Using MINC, we train convolutional neural networks (CNNs) for two tasks: classifying materials from patches, and simultaneous material recognition and segmentation in full images. For patch-based classification on MINC we found that the best performing CNN architectures can achieve 85.2% mean class accuracy. We convert these trained CNN classifiers into an efficient fully convolutional framework combined with a fully connected conditional random field (CRF) to predict the material at every pixel in an image, achieving 73.1% mean class accuracy. Our experiments demonstrate that having a large, well-sampled dataset such as MINC is crucial for real-world material recognition and segmentation.","","","10.1109/CVPR.2015.7298970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298970","","Skin;Fabrics;Hair;Plastics;Robots;Pipelines;Ceramics","image classification;image segmentation;learning (artificial intelligence);neural net architecture;random processes;visual databases","material recognition;real-world images;real-world materials;surface texture;geometry;lighting conditions;clutter;large-scale open dataset;materials in the wild;materials in context database;MINC;deep learning;images segmentation;material databases;convolutional neural networks;materials classification;patch-based classification;CNN architectures;mean class accuracy;CNN classifiers;fully convolutional framework;fully connected conditional random field;CRF;image pixel","","104","34","","","","","IEEE","IEEE Conferences"
"A review on speech separation using NMF and its extensions","T. Pham; Y. Lee; Y. Chen; J. Wang","Department of Computer Science and Information Engineering, National Central University, Taiwan; Department of Computer Science and Information Engineering, National Central University, Taiwan; Department of Computer Science and Information Engineering, National Central University, Taiwan; Department of Computer Science and Information Engineering, National Central University, Taiwan","2015 International Conference on Orange Technologies (ICOT)","","2015","","","26","29","Speech separation aims to estimate the target signals produced by individual speech sources from a mixture signal. In this paper, we especially review on data-driven separation methods, where algorithms will be enhanced to produce better dictionary learning which considers the geometric of input data and efficiently performs separation mixture. We review the existing algorithms using non-negative matrix factorization, sparse coding, mixture local dictionary, group lasso, and graph regularization to produce knowledge bases. We also review the extension of NMF by incorporating two state-of-art techniques i.e. bilevel optimization and deep neural network.","","","10.1109/ICOT.2015.7498486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498486","single channel source separation;non-negative matrix factorization;graph regularization;group lasso;bilevel optimization","Speech;Dictionaries;Training;Sparse matrices;Source separation;Transforms;Bayes methods","graph theory;learning (artificial intelligence);matrix decomposition;neural nets;optimisation;source separation;speech coding","speech separation;NMF;target signal estimation;speech sources;data-driven separation method;dictionary learning;separation mixture;nonnegative matrix factorization;sparse coding;mixture local dictionary;group lasso;graph regularization;bilevel optimization;deep neural network","","","24","","","","","IEEE","IEEE Conferences"
"HARF: Hierarchy-Associated Rich Features for Salient Object Detection","W. Zou; N. Komodakis","Coll. of Inf. Eng., Shenzhen Univ., Shenzhen, China; Ecole des Ponts ParisTech, Univ. Paris-Est, Marne-la-Vallée, France","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","406","414","The state-of-the-art salient object detection models are able to perform well for relatively simple scenes, yet for more complex ones, they still have difficulties in highlighting salient objects completely from background, largely due to the lack of sufficiently robust features for saliency prediction. To address such an issue, this paper proposes a novel hierarchy-associated feature construction framework for salient object detection, which is based on integrating elementary features from multi-level regions in a hierarchy. Furthermore, multi-layered deep learning features are introduced and incorporated as elementary features into this framework through a compact integration scheme. This leads to a rich feature representation, which is able to represent the context of the whole object/background and is much more discriminative as well as robust for salient object detection. Extensive experiments on the most widely used and challenging benchmark datasets demonstrate that the proposed approach substantially outperforms the state-of-the-art on salient object detection.","","","10.1109/ICCV.2015.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410411","","Feature extraction;Object detection;Image segmentation;Robustness;Computational modeling;Visualization;Biological system modeling","feature extraction;image representation;learning (artificial intelligence);object detection","HARF;hierarchy-associated rich features;salient object detection;saliency prediction;hierarchy-associated feature construction framework;elementary features;multilevel region;multilayered deep learning features;compact integration scheme;feature representation","","18","44","","","","","IEEE","IEEE Conferences"
"Show and tell: A neural image caption generator","O. Vinyals; A. Toshev; S. Bengio; D. Erhan","Google, USA; Google, USA; Google, USA; Google, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3156","3164","Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.","","","10.1109/CVPR.2015.7298935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298935","","Logic gates;Measurement;Training;Visualization;Recurrent neural networks;Google","computer vision;learning (artificial intelligence);natural language processing;recurrent neural nets","neural image caption generator;image content description;artificial intelligence;computer vision;natural language processing;generative model;deep recurrent architecture;machine translation;natural sentence generation;language fluency;learning;BLEU-1 score;Pascal dataset;Flickr30k;SBU;COCO dataset","","928","34","","","","","IEEE","IEEE Conferences"
"Short-term load forecasting for smart water and gas grids: A comparative evaluation","M. Fagiani; S. Squartini; R. Bonfigli; F. Piazza","Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy; Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy; Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy; Department of Information Engineering, Università Politecnica delle Marche, Ancona, Italy","2015 IEEE 15th International Conference on Environment and Electrical Engineering (EEEIC)","","2015","","","1198","1203","Moving from a recent publication of Fagiani et al. [1], short-term predictions of water and natural gas consumption are performed exploiting state-of-the-art techniques. Specifically, for two datasets, the performance of Support Vector Regression (SVR), Extreme Learning Machine (ELM), Genetic Programming (GP), Artificial Neural Networks (ANNs), Echo State Networks (ESNs), and Deep Belief Networks (DBNs) are compared adopting common evaluation criteria. Concerning the datasets, the Almanac of Minutely Power Dataset (AMPds) is used to compute predictions with domestic consumption, 2 year of recordings, and to perform further evaluations with the available heterogeneous data, such as energy and temperature. Whereas, predictions of building consumption are performed with the datasets recorded at the Department for International Development (DFID). In addition, the results achieved for the previous release of the AMPds, 1 year of recordings, are also reported, in order to evaluate the impact of seasonality in forecasting performance. Finally, the achieved results validate the suitability of ANN, SVR and ELM approaches for prediction applications in small-grid scenario. Specifically, for the domestic consumption the best performance are achieved by SVR and ANN, for natural gas and water, respectively. Whereas, the ANN shows the best results for both water and natural gas forecasting in building scenario.","","","10.1109/EEEIC.2015.7165339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165339","short-term load forecasting;heterogeneous data forecasting;computational intelligence;domestic and building consumption forecasting;smart water and gas grids","Natural gas;Artificial neural networks;Neurons;Forecasting;Support vector machines;Buildings;Biological neural networks","belief networks;genetic algorithms;learning (artificial intelligence);load forecasting;neural nets;production engineering computing;smart power grids","load forecasting;smart water;gas grids;support vector regression;SVR;extreme learning machine;genetic programming;artificial neural networks;ANN;echo state networks;deep belief networks;Almanac minutely power dataset","","1","29","","","","","IEEE","IEEE Conferences"
"XFace: A Face Recognition System for Android Mobile Phones","J. Hu; L. Peng; L. Zheng","Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China; Dept. of Electron. Eng., Tsinghua Univ. Beijing, Beijing, China; Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China","2015 IEEE 3rd International Conference on Cyber-Physical Systems, Networks, and Applications","","2015","","","13","18","With the rapid development of mobile computing, biometric identification technologies including face recognition on smart phones become increasingly attractive. There are two main challenges of face recognition for mobile computing: one is the problem of the variations of face images acquired in unconstrained environment, the other is the computing limitation of mobile phone platform. This paper presents the design and implementation of an open source face recognition system named XFace for Android mobile phone platform, which provides a feasible solution to meet these challenges. The proposed face recognition approach includes face detection, eye detection, preprocessing for ROI (Region of Interest), LBP (Local Binary Pattern) feature extraction, feature dimensionality reduction based on PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis), and minimum distance classifier. The implementation of the proposed approach is mainly based on Open CV (Open Source Computer Vision) SDK. The system framework and user interface are also presented. Experimental results on practical face image samples collected on Android mobile phones proved the effectiveness of the proposed method. In future work, it is possible to integrate deep learning based face recognition method to the XFace system.","","","10.1109/CPSNA.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272678","Face Recognition;Face Detection;Eigenfaces;Fisherfaces;Android Application","Face;Face recognition;Smart phones;Training;Cameras;Accuracy","Android (operating system);face recognition;feature extraction;image classification;learning (artificial intelligence);mobile computing;object detection;principal component analysis;public domain software;smart phones","mobile computing;biometric identification technologies;smart phones;face images;unconstrained environment;mobile phone platform;open source face recognition system;XFace;Android mobile phone platform;face detection;eye detection;ROI preprocessing;region of interest;LBP feature extraction;local binary pattern;feature dimensionality reduction;PCA;principal component analysis;LDA;linear discriminant analysis;minimum distance classifier;OpenCV SDK;open source computer vision;deep learning based face recognition method","","6","9","","","","","IEEE","IEEE Conferences"
"DjiNN and Tonic: DNN as a service and its implications for future warehouse scale computers","J. Hauswald; Y. Kang; M. A. Laurenzano; Q. Chen; C. Li; T. Mudge; R. G. Dreslinski; J. Mars; L. Tang","Clarity Lab, University of Michigan - Ann Arbor, USA; Clarity Lab, University of Michigan - Ann Arbor, USA; Clarity Lab, University of Michigan - Ann Arbor, USA; Clarity Lab, University of Michigan - Ann Arbor, USA; Clarity Lab, University of Michigan - Ann Arbor, USA; Clarity Lab, University of Michigan - Ann Arbor, USA; Clarity Lab, University of Michigan - Ann Arbor, USA; Clarity Lab, University of Michigan - Ann Arbor, USA; Clarity Lab, University of Michigan - Ann Arbor, USA","2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)","","2015","","","27","40","As applications such as Apple Siri, Google Now, Microsoft Cortana, and Amazon Echo continue to gain traction, webservice companies are adopting large deep neural networks (DNN) for machine learning challenges such as image processing, speech recognition, natural language processing, among others. A number of open questions arise as to the design of a server platform specialized for DNN and how modern warehouse scale computers (WSCs) should be outfitted to provide DNN as a service for these applications. In this paper, we present DjiNN, an open infrastructure for DNN as a service in WSCs, and Tonic Suite, a suite of 7 end-to-end applications that span image, speech, and language processing. We use DjiNN to design a high throughput DNN system based on massive GPU server designs and provide insights as to the varying characteristics across applications. After studying the throughput, bandwidth, and power properties of DjiNN and Tonic Suite, we investigate several design points for future WSC architectures. We investigate the total cost of ownership implications of having a WSC with a disaggregated GPU pool versus a WSC composed of homogeneous integrated GPU servers. We improve DNN throughput by over 120× for all but one application (40× for Facial Recognition) on an NVIDIA K40 GPU. On a GPU server composed of 8 NVIDIA K40s, we achieve near-linear scaling (around 1000× throughput improvement) for 3 of the 7 applications. Through our analysis, we also find that GPU-enabled WSCs improve total cost of ownership over CPU-only designs by 4-20×, depending on the composition of the workload.","","","10.1145/2749469.2749472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284053","","Servers;Graphics processing units;Throughput;Neural networks;Neurons;Libraries;Face","graphics processing units;learning (artificial intelligence);neural nets","DjiNN;warehouse scale computers;deep neural networks;machine learning;Tonic suite;WSC architectures;homogeneous integrated GPU servers;NVIDIA K40 GPU","","30","48","","","","","IEEE","IEEE Conferences"
"Activation models for biologically grounded visual perception in robotics","K. M. Varadarajan; M. Vincze","Karthik Mahesh Varadarajan and Markus Vincze are at the Technical University of Vienna, Austria; Karthik Mahesh Varadarajan and Markus Vincze are at the Technical University of Vienna, Austria","2015 20th International Conference on Methods and Models in Automation and Robotics (MMAR)","","2015","","","511","516","The field of computer vision, in general, is focused on achieving maximal computational efficiency with little priority towards mimicking biological vision. Biologically grounded vision algorithms such as Poggio's layered features and Deep Learning Networks, on the other hand, are geared towards achieving recognition performance and predictable artefacts similar to the human vision system. Nevertheless, there is a dearth of computer vision algorithms and associated robotic systems that incorporate human-like spatio-temporal behaviors in visual processing, which has been conveniently modeled in symbolic systems such as ACT-R. In this paper, we explore the possibility of grounding a recently developed semantic cognitive vision theory - the k-TR theory of affordances for visual perception, using biological models of visual cue activation, thus attempting a marriage between one family of computer vision processing algorithms and ACT-R based spatio-temporal behavior models. Various biological effects such as frequency and recency of feature activation, cognitive concept linkages and spreading activation, partial matching of concepts and visual features, effect of noise at various levels of perception and cognitive analysis, are taken into account. While it can be seen that the performance of such a cognitive k-TR system grounded in biological/brain activation models is inferior in recall performance to an infinite memory, infinite time computational k-TR system, the temporal characteristics in terms of recall times and memory management of the biologically grounded system are superior to the purely computational model when deployed in a semantic context. The implementation of such a system is thus an important step towards the development of human-like behavior and performance in cognitive robots.","","","10.1109/MMAR.2015.7283928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7283928","Biological vision;Activation model;Recall;Memory;Cognitive models;k-TR","Field-flow fractionation","cognition;cognitive systems;learning (artificial intelligence);robot vision;storage management;visual perception","biologically grounded visual perception;robotics;mimicking biological vision;biologically grounded vision algorithms;Poggio layered features;deep learning networks;human vision system;computer vision algorithms;associated robotic systems;human-like spatio-temporal behaviors;visual processing;symbolic systems;semantic cognitive vision theory;k-TR theory;biological models;visual cue activation;computer vision processing algorithms;ACT-R based spatio-temporal behavior models;feature activation;cognitive concept linkages;spreading activation;partial matching;visual features;cognitive analysis;cognitive k-TR system;biological activation models;brain activation models;infinite memory;infinite time computational k-TR system;memory management;biologically grounded system;cognitive robots","","","9","","","","","IEEE","IEEE Conferences"
"A Century of Portraits: A Visual Historical Record of American High School Yearbooks","S. Ginosar; K. Rakelly; S. Sachs; B. Yin; A. A. Efros","NA; NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","652","658","Many details about our world are not captured in written records because they are too mundane or too abstract to describe in words. Fortunately, since the invention of the camera, an ever-increasing number of photographs capture much of this otherwise lost information. This plethora of artifacts documenting our ""visual culture"" is a treasure trove of knowledge as yet untapped by historians. We present a dataset of 37,921 frontal-facing American high school yearbook photos that allow us to use computation to glimpse into the historical visual record too voluminous to be evaluated manually. The collected portraits provide a constant visual frame of reference with varying content. We can therefore use them to consider issues such as a decade's defining style elements, or trends in fashion and social norms over time. We demonstrate that our historical image dataset may be used together with weakly-supervised data-driven techniques to perform scalable historical analysis of large image corpora with minimal human effort, much in the same way that large text corpora together with natural language processing revolutionized historians' workflow. Furthermore, we demonstrate the use of our dataset in dating grayscale portraits using deep learning methods.","","","10.1109/ICCVW.2015.87","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406436","","Visualization;Hair;Sociology;Statistics;Market research;Cameras;Standards","history;learning (artificial intelligence);natural language processing;painting","dating grayscale portrait;deep learning method;natural language processing;weakly-supervised data-driven technique;visual culture;American High School Yearbooks;visual historical record","","7","27","","","","","IEEE","IEEE Conferences"
"Improving image recognition by hierarchical model and denoising","Fuqiang Chen; Yan Wu","College of Electronics and Information Engineering Tongji University, Shanghai, China 201804; College of Electronics and Information Engineering Tongji University, Shanghai, China 201804","2015 11th International Conference on Natural Computation (ICNC)","","2015","","","977","981","In this study, a novel method for image recognition based on deep learning algorithm and image denoising is proposed. It is based on Bernoulli process factor analysis denoising method and tiled convolutional neural network. The images are first denoised using Bernoulli process factor analysis, and then the denoised images are transmitted to tiled convolutional neural network for robust feature extraction. Lastly, support vector machine is used for classification. The experiments implemented on the benchmark dataset CIFAR-10 shows the effectiveness of our proposed method, which performs better than our previously proposed method, CDAE-SVM (contractive denoising autoencoder + SVM).","","","10.1109/ICNC.2015.7378124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7378124","Image denoising;Tiled convolutional neural network;Bernoulli process factor analysis;Topologic independent component analysis","Feature extraction;Neural networks;Support vector machines;Image recognition;Image denoising;Robustness;Noise reduction","feature extraction;image classification;image denoising;learning (artificial intelligence);neural nets;support vector machines","image recognition;hierarchical model;deep learning algorithm;image denoising;Bernoulli process factor analysis denoising method;tiled convolutional neural network;robust feature extraction;support vector machine;classification;CIFAR-10 dataset","","","27","","","","","IEEE","IEEE Conferences"
"Intercell-Interference Cancellation and Neural Network Transmit Power Optimization for MIMO Channels","M. A. Wijaya; K. Fukawa; H. Suzuki","Tokyo Inst. of Technol., Tokyo, Japan; Tokyo Inst. of Technol., Tokyo, Japan; Tokyo Inst. of Technol., Tokyo, Japan","2015 IEEE 82nd Vehicular Technology Conference (VTC2015-Fall)","","2015","","","1","5","Recent demand for capacity increase in mobile communications urges the development of both multiple- input multiple-output (MIMO) and small cells. However, overlapping coverage areas between neighboring small cells results in interference-limited networks, where the capacity of the network is severely degraded without superior intercell interference management (IIM) techniques. As one of the superior IIM techniques, the power control intercell interference coordination (ICIC) optimizes downlink transmit powers in the network. This paper proposes an IIM scheme consisting of both neural networks (NNs) power control on the transmitter side and interference cancellation (IC) on the receiver side. Computer simulations examine networks of MIMO systems with the power optimization using some ICIC algorithms: NN, the greedy search, the belief propagation (BP), and the maximum power, combined with IC on the receiver side. In addition, the deep learning, a breakthrough technique for NN with multi-layer structures, is also applied to mobile communications. The results show that the performance of NN is very close to that of the greedy search and superior to that of BP. Complexity of NN is much less than that of BP, and thus NN is suitable for IIM. It is also demonstrated that employing IC provides high capacity gain.","","","10.1109/VTCFall.2015.7390988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390988","","Artificial neural networks;Integrated circuits;MIMO;Power control;Optimization;Interference;Training","cellular radio;computational complexity;interference suppression;learning (artificial intelligence);MIMO communication;neural nets;radio receivers;radiofrequency interference;telecommunication computing;telecommunication power management;wireless channels","intercell interference cancellation;neural network transmit power optimization;MIMO channel;mobile communication;multiple input multiple output;small cell;interference-limited network;intercell interference management technique;IIM technique;power control intercell interference coordination;ICIC;NN power control;belief propagation;greedy search;deep learning;breakthrough technique;multilayer structure","","10","10","","","","","IEEE","IEEE Conferences"
"Discriminative learning of apparel features","R. Rothe; M. Ristin; M. Dantone; L. Van Gool","Computer Vision Laboratory, D-ITET, ETH Zürich, Switzerland; Computer Vision Laboratory, D-ITET, ETH Zürich, Switzerland; Computer Vision Laboratory, D-ITET, ETH Zürich, Switzerland; Computer Vision Laboratory, D-ITET, ETH Zürich, Switzerland","2015 14th IAPR International Conference on Machine Vision Applications (MVA)","","2015","","","5","9","Fashion is a major segment in e-commerce with growing importance and a steadily increasing number of products. Since manual annotation of apparel items is very tedious, the product databases need to be organized automatically, e.g. by image classification. Common image classification approaches are based on features engineered for general purposes which perform poorly on specific images of apparel. We therefore propose to learn discriminative features based on a small set of annotated images. We experimentally evaluate our method on a dataset with 30,000 images containing apparel items, and compare it to other engineered and learned sets of features. The classification accuracy of our features is significantly superior to designed HOG and SIFT features (43.7% and 16.1% relative improvement, respectively). Our method allows for fast feature extraction and training, is easy to implement and, unlike deep convolutional networks, does not require powerful dedicated hardware.","","","10.1109/MVA.2015.7153120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153120","","Accuracy;Feature extraction;Training;Visualization;Databases;Glass;Histograms","electronic commerce;feature extraction;image classification","discriminative learning;apparel features;manual annotation;image classification;apparel image;discriminative features;annotated images;feature classification accuracy;HOG feature;SIFT feature;feature extraction;feature training;e-commerce","","","17","","","","","IEEE","IEEE Conferences"
"VRank: Voting system on Ranking model for human age estimation","T. Lim; K. Hua; H. Wang; K. Zhao; M. Hu; W. Cheng","Research Center for Information Technology Innovation (CITI), Academia Sinica, Taipei, 115 Taiwan; CSIE, National Taiwan University of Science and Technology, Taipei, 106 Taiwan; Research Center for Information Technology Innovation (CITI), Academia Sinica, Taipei, 115 Taiwan; Research Center for Information Technology Innovation (CITI), Academia Sinica, Taipei, 115 Taiwan; CSIE, National Cheng Kung University, Tainan, 701 Taiwan; Research Center for Information Technology Innovation (CITI), Academia Sinica, Taipei, 115 Taiwan","2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP)","","2015","","","1","6","Ranking algorithms have proven the potential for human age estimation. Currently, a common paradigm is to compare the input face with reference faces of known age to generate a ranking relation whereby the first-rank reference is exploited for labeling the input face. In this paper, we proposed a framework to improve upon the typical ranking model, called Voting system on Ranking model (VRank), by leveraging relational information (comparative relations, i.e. if the input face is younger or older than each of the references) to make a more robust estimation. Our approach has several advantages: firstly, comparative relations can be explicitly involved to benefit the estimation task; secondly, few incorrect comparisons will not influence much the accuracy of the result, making this approach more robust than the conventional approach; finally, we propose to incorporate the deep learning architecture for training, which extracts robust facial features for increasing the effectiveness of classification. In comparison to the best results from the state-of-the-art methods, the VRank showed a significant outperformance on all the benchmarks, with a relative improvement of 5.74% ~ 69.45% (FG-NET), 19.09% ~ 68.71% (MORPH), and 0.55% ~ 17.73% (IoG).","","","10.1109/MMSP.2015.7340789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340789","","Bismuth;Estimation;Face;Support vector machines;Robustness;Machine learning;Feature extraction","estimation theory;feature extraction","VRank;voting system;ranking model;human age estimation;first-rank reference;relational information;robust estimation;estimation task;deep learning architecture;robust facial features","","2","30","","","","","IEEE","IEEE Conferences"
"Brushstroke based sparse hybrid convolutional neural networks for author classification of Chinese ink-wash paintings","M. Sun; D. Zhang; J. Ren; Z. Wang; J. S. Jin","School of Computer Science and Technology, Tianjin University, Tianjin, China; School of Computer Science and Technology, Tianjin University, Tianjin, China; Centre for excellence in Signal and Image Processing, University of Strathclyde, Glasgow, U.K; School of Computer Software, Tianjin University, Tianjin, China; School of Computer Software, Tianjin University, Tianjin, China","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","626","630","A novel stroke based sparse hybrid convolutional neural networks (CNNs) method is proposed for author classification of Chinese ink-wash paintings (IWPs). As Chinese IWPs usually have many authors in several art styles, this differs from real images or western paintings and has led to a big challenge. In our work, we classify Chinese IWPs of different artists by analyzing a set of automatically extracted brushstrokes. A sparse hybrid CNNs in a deep-learning framework is then proposed to extract brushstroke features to replace the commonly used handcrafted ones such as edge, color, intensity and texture. Using 120 IWPs from six famous artists, promising results have been shown in successfully classifying authors in comparison to two other state-of-the-art approaches.","","","10.1109/ICIP.2015.7350874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350874","Brushstroke analysis;Chinese ink-wash painting;image classification;sparse coding;convolutional neural networks","Painting;Feature extraction;Image edge detection;Machine learning;Neural networks;Art;Convolution","image classification;neural nets","brushstroke based sparse hybrid convolutional neural networks;author classification;Chinese ink-wash paintings;sparse hybrid CNN method;Chinese IWP;western paintings;deep-learning framework;brushstroke features","","6","24","","","","","IEEE","IEEE Conferences"
"Robust Optimization for Deep Regression","V. Belagiannis; C. Rupprecht; G. Carneiro; N. Navab","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2830","2838","Convolutional Neural Networks (ConvNets) have successfully contributed to improve the accuracy of regression-based methods for computer vision tasks such as human pose estimation, landmark localization, and object detection. The network optimization has been usually performed with L2 loss and without considering the impact of outliers on the training process, where an outlier in this context is defined by a sample estimation that lies at an abnormal distance from the other training sample estimations in the objective space. In this work, we propose a regression model with ConvNets that achieves robustness to such outliers by minimizing Tukey's biweight function, an M-estimator robust to outliers, as the loss function for the ConvNet. In addition to the robust loss, we introduce a coarse-to-fine model, which processes input images of progressively higher resolutions for improving the accuracy of the regressed values. In our experiments, we demonstrate faster convergence and better generalization of our robust loss function for the tasks of human pose estimation and age estimation from face images. We also show that the combination of the robust loss function with the coarse-to-fine model produces comparable or better results than current state-of-the-art approaches in four publicly available human pose estimation datasets.","","","10.1109/ICCV.2015.324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410681","","Training;Robustness;Machine learning;Convergence;Minimization","computer vision;convergence;face recognition;image resolution;neural nets;optimisation;pose estimation;regression analysis","regression model;ConvNets;convolutional neural networks;biweight function;M-estimator;image resolution;convergence;loss function;human pose estimation;face image age estimation;computer vision","","46","50","","","","","IEEE","IEEE Conferences"
"Quantum-Behaved Particle Swarm Optimization with Cooperative Coevolution for Large Scale Optimization","N. Tian","Dept. of Educ. Technol., Jiangnan Univ., Wuxi, China","2015 14th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES)","","2015","","","82","85","Quantum-behaved particle swarm optimization (QPSO) has successfully been applied to unimodal and multimodal optimization problems. However, with the emerging and popular of big data and deep machine learning, QPSO encounters limitations with high dimensions. In this paper, QPSO with cooperative co evolution (QPSO_CC) is used to decompose the high dimensional problems into several lower dimensional problems and optimize them separately. The numerical experimental results show that QPSO_CC has comparative or even better performance than other algorithms.","","","10.1109/DCABES.2015.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429562","large scale;quantum-behaved particle swarm optimization;cooperative coevolution;domain decompositimponent","Particle swarm optimization;Optimization;Context;Quantum computing;Sun;Genetic algorithms;Benchmark testing","particle swarm optimisation;quantum computing","quantum-behaved particle swarm optimization;cooperative coevolution;large scale optimization;unimodal optimization problems;multimodal optimization problems;big data;deep machine learning","","1","18","","","","","IEEE","IEEE Conferences"
"Component-model based detection and recognition of road vehicles","Min Wang; Liangwei Jiang; Wenjie Lu; Aifen Fang","Key Laboratory of Ministry of Public Security for Road Traffic Safety, Wuxi, China; Key Laboratory of Ministry of Public Security for Road Traffic Safety, Wuxi, China; Key Laboratory of Ministry of Public Security for Road Traffic Safety, Wuxi, China; Traffic Management Research Institute of Ministry of Public Security, Wuxi, China","2015 IEEE International Conference on Progress in Informatics and Computing (PIC)","","2015","","","449","453","Feature extraction and recognition of a vehicle is always a popular research spot in the intelligent transportation system (ITS) area. Based on this technology, many applications, such as the license plate recognition, brand recognition, driving behavior understanding and so on, can be realized to improve the transportation management-control level. Unlike normal task-oriented vehicle recognition methods, a new feature extraction and recognition framework based on a component-model for vehicles is introduced in this paper, which extracts features from vehicle components with a coarse-to-fine mechanism. This kind of deep learned visual feature can be used for vehicle detection, license plate recognition and brand recognition. Furthermore, a component dataset including 110 different brands of vehicles is built up for evaluation. The proposed method obtained a good performance in the experimental result, which is significant for the practical application.","","","10.1109/PIC.2015.7489887","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489887","feature extraction;vehicle recognition;component segmantation;intelligent transportation","Vehicles;Visualization;Licenses;Silicon","feature extraction;intelligent transportation systems;object detection;object recognition;road vehicles","component-model based detection;component-model based recognition;road vehicles;feature extraction;feature recognition;vehicle components;coarse-to-fine mechanism;deep learned visual feature;vehicle detection;license plate recognition;brand recognition;intelligent transportation system;ITS","","1","12","","","","","IEEE","IEEE Conferences"
"Learning lightness from human judgement on relative reflectance","T. Narihira; M. Maire; S. X. Yu","UC Berkeley, USA; TTI Chicago, USA; UC Berkeley, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","2965","2973","We develop a new approach to inferring lightness, the perceived reflectance of surfaces, from a single image. Classic methods view this problem from the perspective of intrinsic image decomposition, where an image is separated into reflectance and shading components. Rather than reason about reflectance and shading together, we learn to directly predict lightness differences between pixels. Large-scale training from human judgement data on relative reflectance, and patch representations built using deep networks, provide the foundation for our model. Benchmarked on the Intrinsic Images in the Wild dataset [4], our local lightness model achieves on-par performance with the state-of-the-art global lightness model, which incorporates multiple shading/reflectance priors and simultaneous reasoning between pairs of pixels in a dense conditional random field formulation.","","","10.1109/CVPR.2015.7298915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298915","","","image colour analysis;image representation;neural nets","human judgement;relative reflectance;surface reflectance;image decomposition;shading components;lightness differences;pixels;large-scale training;patch representations;deep networks;Wild dataset;local lightness model;global lightness model","","20","26","","","","","IEEE","IEEE Conferences"
"Automatic assessment of English learner pronunciation using discriminative classifiers","M. Nicolao; A. V. Beeston; T. Hain","Speech and Hearing Research Group, Department of Computer Science, University of Sheffield, UK; Speech and Hearing Research Group, Department of Computer Science, University of Sheffield, UK; Speech and Hearing Research Group, Department of Computer Science, University of Sheffield, UK","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","5351","5355","This paper presents a novel system for automatic assessment of pronunciation quality of English learner speech, based on deep neural network (DNN) features and phoneme specific discriminative classifiers. DNNs trained on a large corpus of native and non-native learner speech are used to extract phoneme posterior probabilities. A part of the corpus includes per phone teacher annotations, which allows training of two Gaussian Mixture Models (GMM), representing correct pronunciations and typical error patterns. The likelihood ratio is then obtained for each observed phone. Several models were evaluated on a large corpus of English-learning students, with a variety of skill levels, and aged 13 upwards. The cross-correlation of the best system and average human annotator reference scores is 0.72, with miss and false alarm rate around 19%. Automatic assessment is 81.6% correct with a high degree of confidence. The new approach significantly outperforms spectral distance based baseline systems.","","","10.1109/ICASSP.2015.7178993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178993","Pronunciation assessment;Computer-Assisted Language Learning;DNN-GMM;binary classifier","Training;Speech;Feature extraction;Regression tree analysis;Acoustics;Nickel;Neural networks","Gaussian processes;mixture models;neural nets;speech recognition","automatic assessment;english learner pronunciation;pronunciation quality;english learner speech;deep neural network;DNN features;phoneme specific discriminative classifiers;nonnative learner speech;phoneme posterior probability;per phone teacher annotations;Gaussian mixture models;GMM;likelihood ratio;english-learning students;human annotator reference scores","","3","10","","","","","IEEE","IEEE Conferences"
"Improving HMM/DNN in ASR of under-resourced languages using probabilistic sampling","M. Song; Q. Zhang; J. Pan; Y. Yan","The Key Laboratory of Speech Acoustics and Content Understanding, Chinese Academy of Sciences, Beijing, 100190, China; The Key Laboratory of Speech Acoustics and Content Understanding, Chinese Academy of Sciences, Beijing, 100190, China; The Key Laboratory of Speech Acoustics and Content Understanding, Chinese Academy of Sciences, Beijing, 100190, China; The Key Laboratory of Speech Acoustics and Content Understanding, Chinese Academy of Sciences, Beijing, 100190, China","2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)","","2015","","","20","24","In HMM/DNN automatic speech recognition (ASR) systems, the DNNs model the posterior probabilities for triphone states. However, triphone states are unevenly distributed. In this situation, the training algorithm tends to converge to a local optimum more related to states with rich data than states with poor data. Thus, the imbalance of the training data decreases the ASR performances, especially for under-resourced languages. To deal with this issue, we explore a resampling technique, called “probabilistic sampling”, which can be seen as a linear smoothing between the original sampling and the uniform sampling. The effectiveness of the probabilistic sampling has been studied in two under-resourced ASR experiments. With the probabilistic sampling, the first experiment got a 6.3% relative phone error rate (PER) reduction compared to the conventional DNN baseline; the second experiment used shared-hidden-layer multilingual DNN as the baseline, and obtained a 4.9% relative PER reduction.","","","10.1109/ChinaSIP.2015.7230354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230354","Automatic speech recognition;HM-M/DNN hybrid;under-resourced languages;probabilistic sampling","Probabilistic logic;Hidden Markov models;Training;Training data;Speech recognition;Speech;Acoustics","hidden Markov models;learning (artificial intelligence);neural nets;probability;signal sampling;smoothing methods;speech recognition","ASR system;underresourced language;probabilistic sampling;HMM-DNN automatic speech recognition system;posterior probability;triphone state;training algorithm;training data imbalance;resampling technique;linear smoothing;uniform sampling;phone error rate;PER reduction;shared-hidden-layer multilingual DNN;deep neural network","","","17","","","","","IEEE","IEEE Conferences"
"High performance offline handwritten Chinese character recognition using GoogLeNet and directional feature maps","Z. Zhong; L. Jin; Z. Xie","School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","846","850","Just like its great success in solving many computer vision problems, the convolutional neural networks (CNN) provided new end-to-end approach to handwritten Chinese character recognition (HCCR) with very promising results in recent years. However, previous CNNs so far proposed for HCCR were neither deep enough nor slim enough. We show in this paper that, a deeper architecture can benefit HCCR a lot to achieve higher performance, meanwhile can be designed with less parameters. We also show that the traditional feature extraction methods, such as Gabor or gradient feature maps, are still useful for enhancing the performance of CNN. We design a streamlined version of GoogLeNet [13], which was original proposed for image classification in recent years with very deep architecture, for HCCR (denoted as HCCR-GoogLeNet). The HCCR-GoogLeNet we used is 19 layers deep but involves with only 7.26 million parameters. Experiments were conducted using the ICDAR 2013 offline HCCR competition dataset. It has been shown that with the proper incorporation with traditional directional feature maps, the proposed single and ensemble HCCR-GoogLeNet models achieve new state of the art recognition accuracy of 96.35% and 96.74%, respectively, outperforming previous best result with significant gap.","","","10.1109/ICDAR.2015.7333881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333881","Deep learning;convolutional neural networks;classifier ensemble;handwritten Chinese character recognition","Image recognition;Feature extraction;Handwriting recognition;Art;Databases;Gabor filters;Standards","feature extraction;handwritten character recognition;image classification;natural language processing;neural nets","high performance offline handwritten Chinese character recognition;directional feature maps;convolutional neural networks;feature extraction;image classification;ICDAR 2013 offline HCCR competition dataset;directional feature maps;ensemble HCCR-GoogLeNet model;single HCCR-GoogLeNet model","","76","23","","","","","IEEE","IEEE Conferences"
"Network-structured discussions for collaborative concept mapping and peer learning","S. Rafaeli; C. Kent","NA; NA","IBM Journal of Research and Development","","2015","59","6","7:1","7:13","Online discussion is an essential tool for the collaborative construction of knowledge in learning communities. LMSs (Learning Management Systems) include such collaborative tools to enable knowledge sharing. Common topology and design constructs of online collaboration are often expressed as forum threads; however, forums or bulletin boards are not primarily designed for social constructivist learning, in which knowledge develops collaboratively in a methodical process that links new items to existing ones. In this paper, we present a preliminary concept and hypothesize that social constructivist learning can be implemented through a semantic network topology of a discussion platform. To that end, we developed Ligilo, a peer-learning, online platform, in which each content item is conveyed as a node in a semantic network constructed of posts. Analysis of this platform consists of three parts. First, we review the theoretical background of networked-structured discussions in learning. Then, we describe the architecture of Ligilo, focusing on two aspects: (1) the learning experience as a process of contributing new information items by relating to existing items and (2) a deep, behavioral analytics framework as a teacher's guide. Both these aspects are based on semantic network topology. We conclude the paper by describing some early cases of peer learning involving networked-structured discussions.","","","10.1147/JRD.2015.2461091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7330113","","Collaboration;Semantics;Peer-to-peer computing;Cognition;Network topology;Collaborative work;Knowledge engineering","","","","4","63","","","","","IBM","IBM Journals"
"Tracking the student's performance in Web-based education using Scrum methodology","M. Mahalakshmi; M. Sundararajan","St.Peter's University, Avadi, Chennai, India; Dept. of Computer Science, Govt. Arts College for Men, Nandanam, Chennai, India","2015 International Conference on Computing and Communications Technologies (ICCCT)","","2015","","","379","382","The Internet, communication and mobile technologies are emerging as predominant paradigm in the learning of students. Various technologies have created numerous opportunities for the learners to explore in the learning landscape. Even though traditional teaching and learning are still popular in India, the scope for Web based education is positive. It is commonly believed that the Web based education is poised to penetrate deep into the educational system in India in near future. While global web-based education system faces different subjects, ideas and different kind of talented student's, the biggest challenge is communication and tracking the performance of student's. To deal with this challenge, we can use scrum. Scrum is the essence and one of the powerful agile methodologies. This paper presents about scrum and scrum backlog.","","","10.1109/ICCCT2.2015.7292779","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7292779","Scrum framework;Proposed Framework;Comparison;Artifacts;Conclusion","Education;Software;Collaborative work;Planning;Scrum (Software development);Monitoring;Communications technology","computer aided instruction;educational administrative data processing;Internet;mobile learning;teaching","student performance tracking;Web-based education;Scrum methodology;Internet;mobile technologies;teaching;India;Web based education","","3","9","","","","","IEEE","IEEE Conferences"
"Human Action Recognition Using Factorized Spatio-Temporal Convolutional Networks","L. Sun; K. Jia; D. Yeung; B. E. Shi","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","4597","4605","Human actions in video sequences are three-dimensional (3D) spatio-temporal signals characterizing both the visual appearance and motion dynamics of the involved humans and objects. Inspired by the success of convolutional neural networks (CNN) for image classification, recent attempts have been made to learn 3D CNNs for recognizing human actions in videos. However, partly due to the high complexity of training 3D convolution kernels and the need for large quantities of training videos, only limited success has been reported. This has triggered us to investigate in this paper a new deep architecture which can handle 3D signals more effectively. Specifically, we propose factorized spatio-temporal convolutional networks (FstCN) that factorize the original 3D convolution kernel learning as a sequential process of learning 2D spatial kernels in the lower layers (called spatial convolutional layers), followed by learning 1D temporal kernels in the upper layers (called temporal convolutional layers). We introduce a novel transformation and permutation operator to make factorization in FstCN possible. Moreover, to address the issue of sequence alignment, we propose an effective training and inference strategy based on sampling multiple video clips from a given action video sequence. We have tested FstCN on two commonly used benchmark datasets (UCF-101 and HMDB-51). Without using auxiliary training videos to boost the performance, FstCN outperforms existing CNN based methods and achieves comparable performance with a recent method that benefits from using auxiliary training videos.","","","10.1109/ICCV.2015.522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410879","","Kernel;Convolution;Three-dimensional displays;Video sequences;Training;Visualization;Computer architecture","image classification;image motion analysis;image sequences;learning (artificial intelligence);neural nets;object recognition;video signal processing","human action recognition;factorized spatio-temporal convolutional network;video sequences;3D spatio-temporal signal;visual appearance;motion dynamics;convolutional neural network;CNN;image classification;FstCN;3D convolution kernel learning;spatial convolutional layer;temporal convolutional layer;inference strategy","","141","35","","","","","IEEE","IEEE Conferences"
"Speech acoustic modeling from raw multichannel waveforms","Y. Hoshen; R. J. Weiss; K. W. Wilson","Hebrew University of Jerusalem, Israel; Google Inc, New York, USA; Google Inc, New York, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4624","4628","Standard deep neural network-based acoustic models for automatic speech recognition (ASR) rely on hand-engineered input features, typically log-mel filterbank magnitudes. In this paper, we describe a convolutional neural network - deep neural network (CNN-DNN) acoustic model which takes raw multichannel waveforms as input, i.e. without any preceding feature extraction, and learns a similar feature representation through supervised training. By operating directly in the time domain, the network is able to take advantage of the signal's fine time structure that is discarded when computing filterbank magnitude features. This structure is especially useful when analyzing multichannel inputs, where timing differences between input channels can be used to localize a signal in space. The first convolutional layer of the proposed model naturally learns a filterbank that is selective in both frequency and direction of arrival, i.e. a bank of bandpass beamformers with an auditory-like frequency scale. When trained on data corrupted with noise coming from different spatial locations, the network learns to filter them out by steering nulls in the directions corresponding to the noise sources. Experiments on a simulated multichannel dataset show that the proposed acoustic model outperforms a DNN that uses log-mel filterbank magnitude features under noisy and reverberant conditions.","","","10.1109/ICASSP.2015.7178847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178847","Automatic speech recognition;acoustic modeling;convolutional neural networks;beamforming","Indexes;Acoustics;Training;Speech;Computational modeling;Feature extraction;Speech enhancement","channel bank filters;neural nets;speech recognition","speech acoustic modeling;raw multichannel waveforms;standard deep neural network-based acoustic models;automatic speech recognition;ASR;log-mel filterbank magnitudes;convolutional neural network - deep neural network acoustic model;feature extraction;feature representation;bandpass beamformers","","55","20","","","","","IEEE","IEEE Conferences"
"Local Convolutional Features with Unsupervised Training for Image Retrieval","M. Paulin; M. Douze; Z. Harchaoui; J. Mairal; F. Perronin; C. Schmid","NA; NA; NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","91","99","Patch-level descriptors underlie several important computer vision tasks, such as stereo-matching or content-based image retrieval. We introduce a deep convolutional architecture that yields patch-level descriptors, as an alternative to the popular SIFT descriptor for image retrieval. The proposed family of descriptors, called Patch-CKN, adapt the recently introduced Convolutional Kernel Network (CKN), an unsupervised framework to learn convolutional architectures. We present a comparison framework to benchmark current deep convolutional approaches along with Patch-CKN for both patch and image retrieval, including our novel ""RomePatches"" dataset. Patch-CKN descriptors yield competitive results compared to supervised CNN alternatives on patch and image retrieval.","","","10.1109/ICCV.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410376","","Image retrieval;Kernel;Detectors;Computer architecture;Lighting;Three-dimensional displays;Pipelines","computer vision;content-based retrieval;convolution;image retrieval;stereo image processing;unsupervised learning","local convolutional features;unsupervised training;patch-level descriptors;computer vision tasks;stereo-matching;content-based image retrieval;deep convolutional architecture;Patch-CKN descriptors;convolutional kernel network;patch retrieval;RomePatches dataset","","60","47","","","","","IEEE","IEEE Conferences"
"Clinical subthalamic nucleus prediction from high-field brain MRI","J. Kim; Y. Duchin; G. Sapiro; J. Vitek; N. Harel","Department of Electrical and Computer Engineering, Duke University, Durham, USA; Center for Magnetic Resonance Research, University of Minnesota, Minneapolis, USA; Department of Electrical and Computer Engineering, Duke University, Durham, USA; Department of Neurology, University of Minnesota, Minneapolis, USA; Center for Magnetic Resonance Research, University of Minnesota, Minneapolis, USA","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","","2015","","","1264","1267","The subthalamic nucleus (STN) within the sub-cortical region of the Basal ganglia is a crucial targeting structure for Parkinson's Deep brain stimulation (DBS) surgery. Volumetric segmentation of such small and complex structure, which is elusive in clinical MRI protocols, is thereby a pre-requisite process for reliable DBS direct targeting. While direct visualization of the STN is facilitated with advanced ultrahigh-field MR imaging (7 Tesla), such high fields are not always clinically available. In this paper, we aim at the automatic prediction of the STN region on clinical low-field MRI, exploiting dependencies between the STN and its adjacent structures, learned from ultrahigh-field MRI. We present a framework based on a statistical shape model to learn such shape relationship on high quality MR data sets. This allows for an accurate prediction and visualization of the STN structure, given detectable predictors on the low-field MRI. Experimental results on Parkinson's patients demonstrate that the proposed approach enables accurate estimation of the STN on clinical 1.5T MRI.","","","10.1109/ISBI.2015.7164104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164104","Deep brain stimulation;subthalamic nucleus;statistical shape models;high-field MRI","Shape;Magnetic resonance imaging;Training;Predictive models;Image segmentation;Satellite broadcasting","biomedical MRI;brain;diseases;image segmentation;medical image processing;physiological models;statistical analysis;surgery","clinical subthalamic nucleus prediction;high-field brain MRI;subcortical region;basal ganglia;Parkinson's Deep brain stimulation surgery;volumetric segmentation;clinical MRI protocols;DBS direct targeting;direct visualization;ultrahigh-field MR imaging;clinical low-field MRI;adjacent structures;ultrahigh-field MRI;statistical shape model;shape relationship;high quality MR data;STN structure prediction;STN structure visualization;magnetic flux density 1.5 T;magnetic flux density 7 T","","2","20","","","","","IEEE","IEEE Conferences"
"Driving posture recognition by convolutional neural networks","Chao Yan; B. Zhang; F. Coenen","Department of Computer Science & Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, 215123, China; Department of Computer Science & Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou, 215123, China; Department of Computer Science, The University of Liverpool, UK","2015 11th International Conference on Natural Computation (ICNC)","","2015","","","680","685","Driver fatigue and inattention have long been recognized as the main contributing factors in traffic accidents. Development of intelligent driver assistance systems with embeded functionality of driver vigilance monitoring is therefore an urgent and challenging task. This paper presents a novel system which applies convolutional neural network to automatically learn and predict four driving postures. The main idea is to monitor driver hand position with discriminative information extracted to predict safe/unsafe driving posture. In comparison to previous approaches, convolutional neural networks (CNN) can automatically learn discriminative features directly from raw images. In our works, a CNN model was first pre-trained by an unsupervised feature learning called using sparse filtering, and subsequently fine-tuned with four classes of labeled data. The Approach was verified using the Southeast University Driving-Posture Dataset, which comprised of video clips covering four driving postures, including normal driving, responding to a cell phone call, eating and smoking. Compared to other popular approaches with different image descriptor and classification, our method achieves the best performance with a overall accuracy of 99.78%.","","","10.1109/ICNC.2015.7378072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7378072","Driving posture recognition;Driving assistance system;Deep learning;Convolutional neural network","Feature extraction;Neural networks;Training;Vehicles;Computer architecture;Monitoring;Convolution","accidents;driver information systems;fatigue;image recognition;knowledge based systems;neural nets;video signal processing","driving posture recognition;convolutional neural networks;driver fatigue;driver inattention;traffic accidents;intelligent driver assistance systems;driver vigilance monitoring;CNN;sparse filtering;southeast university driving-posture dataset;video clips","","4","34","","","","","IEEE","IEEE Conferences"
"An ensemble of invariant features for person re-identification","S. Chen; Y. Lee; J. Hwang; Y. Hung; J. Yoo","Department of Computer Science & Information Engineering, National Taiwan University, 1, Sec. 4, Roosevelt Road, Taipei 10617, Taiwan; Department of Electrical Engineering, University of Washington, 185 W Stevens way NE, Seattle, 98195, USA; Department of Electrical Engineering, University of Washington, 185 W Stevens way NE, Seattle, 98195, USA; Department of Computer Science & Information Engineering, National Taiwan University, 1, Sec. 4, Roosevelt Road, Taipei 10617, Taiwan; SW Content Research Laboratory, ETRI, 218 Gajeong-ro, Yuseong-gu, Daejeon 305-700, S. Korea","2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP)","","2015","","","1","6","We propose an ensemble of invariant features for person re-identification. The proposed method requires no domain learning and can effectively overcome the issues created by the variations of human poses and viewpoint between a pair of different cameras. Our ensemble model utilizes both holistic and region-based features. To avoid the misalignment problem, the test human object sample is used to generate multiple virtual samples, by applying slight geometric distortion. The holistic features are extracted from a publically available pre-trained deep convolutional neural network. On the other hand, the region-based features are based on our proposed Two-Way Gaussian Mixture Model Fitting and the Completed Local Binary Pattern texture representations. To make better generalization during the matching without additional learning processes for the feature aggregation, the ensemble scheme combines all three feature distances using distances normalization. The proposed framework achieves robustness against partial occlusion, pose and viewpoint changes. In addition, the experimental results show that our method exceeds the state of the art person re-identification performance based on the challenging benchmark 3DPeS.","","","10.1109/MMSP.2015.7340791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340791","","Feature extraction;Histograms;Image color analysis;Probes;Training;Torso;Three-dimensional displays","Gaussian processes;image texture;learning (artificial intelligence);mixture models;neural nets","person re-identification;invariant features ensemble;misalignment problem avoidance;geometric distortion;feature extraction;deep convolutional neural network;two-way Gaussian mixture model fitting;completed local binary pattern texture representations;partial occlusion","","","20","","","","","IEEE","IEEE Conferences"
"A hybrid ensemble learning approach to star–galaxy classification","E. J. Kim; R. J. Brunner; M. Carrasco Kind","NA; NA; NA","Monthly Notices of the Royal Astronomical Society","","2015","453","1","507","521","There exist a variety of star–galaxy classification techniques, each with their own strengths and weaknesses. In this paper, we present a novel meta-classification framework that combines and fully exploits different techniques to produce a more robust star–galaxy classification. To demonstrate this hybrid, ensemble approach, we combine a purely morphological classifier, a supervised machine learning method based on random forest, an unsupervised machine learning method based on self-organizing maps, and a hierarchical Bayesian template-fitting method. Using data from the CFHTLenS survey (Canada–France–Hawaii Telescope Lensing Survey), we consider different scenarios: when a high-quality training set is available with spectroscopic labels from DEEP2 (Deep Extragalactic Evolutionary Probe Phase 2 ), SDSS (Sloan Digital Sky Survey), VIPERS (VIMOS Public Extragalactic Redshift Survey), and VVDS (VIMOS VLT Deep Survey), and when the demographics of sources in a low-quality training set do not match the demographics of objects in the test data set. We demonstrate that our Bayesian combination technique improves the overall performance over any individual classification method in these scenarios. Thus, strategies that combine the predictions of different classifiers may prove to be optimal in currently ongoing and forthcoming photometric surveys, such as the Dark Energy Survey and the Large Synoptic Survey Telescope.","","","10.1093/mnras/stv1608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8200654","methods: data analysis;methods: statistical;surveys;stars: statistics;galaxies: statistics","","","","","1","","","","","","OUP","OUP Journals"
"Denoising AutoEncoder in Neural Networks with Modified Elliott Activation Function and Sparsity-Favoring Cost Function","H. Burhani; W. Feng; G. Hu","Depts. of Comput. & Inf. Syst. & Math., Trent Univ., Peterborough, ON, Canada; Depts. of Comput. & Inf. Syst. & Math., Trent Univ., Peterborough, ON, Canada; Dept. of Comput. Sci., Central Michigan Univ., Mount Pleasant, MI, USA","2015 3rd International Conference on Applied Computing and Information Technology/2nd International Conference on Computational Science and Intelligence","","2015","","","343","348","Neural networks (NN) are architectures and algorithms for machine learning. They are quite powerful for tasks like classification, clustering, and pattern recognition. Large neural networks can be considered a universal function that can approximate any function, and hence are effective at learning from the training data, not only the useful information but also the noise in the training data. However, as the number of neurons and the number of hidden layers grow, the number of connections in the network increases exponentially, and the over fitting problem becomes more severe biased towards noise. Various methods have been proposed to address this problem such as AutoEncoder, Dropout, DropConnect, and Factored Mean training. In this paper, we propose a denoising autoencoder approach using a modified Elliott activation function and a cost function that favors sparsity in the input data. Preliminary experiments using the modified algorithm on several real data sets showed that the proposed approach performed well.","","","10.1109/ACIT-CSI.2015.67","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336086","Deep neural networks;denoising autoencoder;activation function;cost function","Neurons;Biological neural networks;Training;Noise reduction;Cost function;Machine learning algorithms","neural nets;transfer functions","neural networks;sparsity-favoring cost function;denoising autoencoder;modified Elliott activation function;input data sparsity","","1","18","","","","","IEEE","IEEE Conferences"
"Comparative analysis of Kannada phoneme recognition using different classifiers","Akhila K S; R. Kumaraswamy","Dept. of Electronics and Communication, Siddaganga Institute of Technology, Tumakuru, India; Dept. of Electronics and Communication, Siddaganga Institute of Technology, Tumakuru, India","2015 International Conference on Trends in Automation, Communications and Computing Technology (I-TACT-15)","","2015","","","1","6","Information retrieval from audio and speech is very important in the present digital world. Phonetic search (phoneme level search) is an efficient technique for searching words or phrases from audio and speech recordings. In this paper, a baseline phoneme recognition system for Kannada language is developed using Deep Belief Networks (DBNs). Phonemes are segmented from broadcast/read mode Kannada speech. 16 MFCC features are extracted from each speech frame. These features are used as input to the recognizer. DBNs are relatively new area of machine learning. The learning procedure of DBN has two steps, an unsupervised pre-training stage and fine-tuning stage. The performance of DBN for recognition of 25 Kannada phonemes is compared with the conventional methods of speech recognition such as, Multi-Layer Feed Forward Neural Networks (ML-FFNNs) and Support Vector Machines (SVMs). Experimental results show that DBNs yield a high performance as compared to other techniques with Phoneme Error Rate (PER) of 23.6 %. In another experiment conducted, shows that DBN's performance is influenced by number of hidden units in the hidden layer chosen.","","","10.1109/ITACT.2015.7492683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7492683","Kannada speech;Phoneme recognition;Deep Belief Networks;Multi-Layer Feed Forward Neural Networks;Support Vector Machines;Phoneme Error Rate","Speech recognition;Speech;Hidden Markov models;Neural networks;Support vector machines;Acoustics;Feature extraction","","","","","13","","","","","IEEE","IEEE Conferences"
"From captions to visual concepts and back","H. Fang; S. Gupta; F. Iandola; R. K. Srivastava; L. Deng; P. Dollár; J. Gao; X. He; M. Mitchell; J. C. Platt; C. L. Zitnick; G. Zweig","Microsoft Research, Beijing 100080, China; Microsoft Research, Beijing 100080, China; Microsoft Research, Beijing 100080, China; Microsoft Research, Beijing 100080, China; Microsoft Research, Beijing 100080, China; Microsoft Research, Beijing 100080, China; Microsoft Research, Beijing 100080, China; Microsoft Research, Beijing 100080, China; Microsoft Research, Beijing 100080, China; Microsoft Research, Beijing 100080, China; Microsoft Research, Beijing 100080, China; Microsoft Research, Beijing 100080, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","1473","1482","This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time.","","","10.1109/CVPR.2015.7298754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298754","","Training;Detectors;Semantics;Visualization;Neural networks;Training data;Measurement","feature extraction;learning (artificial intelligence);object detection","image descriptions generation;visual detectors;language models;multimodal similarity models;image captions;multiple instance learning;part-of-speech;word detector;maximum-entropy language model;word usage statistics;sentence-level features;Microsoft COCO benchmark;BLEU-4 score","","301","51","","","","","IEEE","IEEE Conferences"
"Look and Think Twice: Capturing Top-Down Visual Attention with Feedback Convolutional Neural Networks","C. Cao; X. Liu; Y. Yang; Y. Yu; J. Wang; Z. Wang; Y. Huang; L. Wang; C. Huang; W. Xu; D. Ramanan; T. S. Huang","NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2956","2964","While feedforward deep convolutional neural networks (CNNs) have been a great success in computer vision, it is important to note that the human visual cortex generally contains more feedback than feedforward connections. In this paper, we will briefly introduce the background of feedbacks in the human visual cortex, which motivates us to develop a computational feedback mechanism in deep neural networks. In addition to the feedforward inference in traditional neural networks, a feedback loop is introduced to infer the activation status of hidden layer neurons according to the ""goal"" of the network, e.g., high-level semantic labels. We analogize this mechanism as ""Look and Think Twice."" The feedback networks help better visualize and understand how deep neural networks work, and capture visual attention on expected objects, even in images with cluttered background and multiple objects. Experiments on ImageNet dataset demonstrate its effectiveness in solving tasks such as image classification and object localization.","","","10.1109/ICCV.2015.338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410695","","Neurons;Visualization;Biological neural networks;Feedforward neural networks;Semantics;Feedback loop;Logic gates","computer vision;convolution;feedback;feedforward neural nets;inference mechanisms;learning (artificial intelligence);semantic networks","top-down visual attention;feedback convolutional neural network;CNN;computer vision;computational feedback mechanism;deep neural network;feedforward inference;feedback loop;high-level semantic label","","66","34","","","","","IEEE","IEEE Conferences"
"An unsupervised approach to the semantic description of the sound quality of violins","M. Buccoli; M. Zanoni; F. Setragno; F. Antonacci; A. Sarti","Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano Piazza Leonardo da Vinci 32 - 20133 Milano, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano Piazza Leonardo da Vinci 32 - 20133 Milano, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano Piazza Leonardo da Vinci 32 - 20133 Milano, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano Piazza Leonardo da Vinci 32 - 20133 Milano, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano Piazza Leonardo da Vinci 32 - 20133 Milano, Italy","2015 23rd European Signal Processing Conference (EUSIPCO)","","2015","","","2004","2008","In this study we propose a set of semantic musical descriptors that can be used for describing the timbre of violins. The proposed semantic model follows a dimensional approach, which allows us to express the degree of intensity of each descriptor. A set of recordings of a number of violins (among them, Stradivari, Amati and Guarnieri instruments) were annotated with the descriptors through questionnaires. The recordings are processed with deep learning techniques, to learn salient features from the audio signal in an unsupervised fashion. In this study we propose an automatic annotation procedure based on a set of regression functions that model each semantic descriptor using the learned set of features.","","","10.1109/EUSIPCO.2015.7362735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362735","High-level music descriptor;violin;timbre;sound quality","Semantics;Instruments;Training;Neurons;Feature extraction;Europe;Signal processing","audio signal processing;feature extraction;learning (artificial intelligence);musical acoustics;musical instruments;regression analysis","violin sound quality;semantic musical descriptors;violins timbre;learning techniques;salient features;audio signal;unsupervised fashion;automatic annotation;regression function","","","24","","","","","IEEE","IEEE Conferences"
"Constrained Convolutional Neural Networks for Weakly Supervised Segmentation","D. Pathak; P. Krähenbühl; T. Darrell","Univ. of California, Berkeley, Berkeley, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1796","1804","We present an approach to learn a dense pixel-wise labeling from image-level tags. Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network (CNN) classifier. We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a CNN. Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization. The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks. Extensive experiments demonstrate the generality of our new learning framework. The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation. We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm.","","","10.1109/ICCV.2015.209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410566","","Optimization;Image segmentation;Labeling;Neural networks;Standards;Semantics;Convolutional codes","image segmentation;learning (artificial intelligence);neural nets","constrained convolutional neural networks;dense pixel-wise labeling;image-level tags;CNN classifier;loss formulation;stochastic gradient descent optimization;learning framework;weakly supervised semantic image segmentation","","122","37","","","","","IEEE","IEEE Conferences"
"RGB-D object recognition and pose estimation based on pre-trained convolutional neural network features","M. Schwarz; H. Schulz; S. Behnke","Rheinische Friedrich-Wilhelms-Universität Bonn, Computer Science Institute VI, Autonomous Intelligent Systems, Friedrich-Ebert-Allee 144, 53113, Germany; Rheinische Friedrich-Wilhelms-Universität Bonn, Computer Science Institute VI, Autonomous Intelligent Systems, Friedrich-Ebert-Allee 144, 53113, Germany; Rheinische Friedrich-Wilhelms-Universität Bonn, Computer Science Institute VI, Autonomous Intelligent Systems, Friedrich-Ebert-Allee 144, 53113, Germany","2015 IEEE International Conference on Robotics and Automation (ICRA)","","2015","","","1329","1335","Object recognition and pose estimation from RGB-D images are important tasks for manipulation robots which can be learned from examples. Creating and annotating datasets for learning is expensive, however. We address this problem with transfer learning from deep convolutional neural networks (CNN) that are pre-trained for image categorization and provide a rich, semantically meaningful feature set. We incorporate depth information, which the CNN was not trained with, by rendering objects from a canonical perspective and colorizing the depth channel according to distance from the object center. We evaluate our approach on the Washington RGB-D Objects dataset, where we find that the generated feature set naturally separates classes and instances well and retains pose manifolds. We outperform state-of-the-art on a number of subtasks and show that our approach can yield superior results when only little training data is available.","","","10.1109/ICRA.2015.7139363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139363","","Feature extraction;Image color analysis;Training;Support vector machines;Estimation;Pipelines;Accuracy","learning (artificial intelligence);manipulators;neural nets;object recognition;pose estimation;visual databases","pose estimation;convolutional neural network features;object recognition;RGB-D images;manipulation robots;transfer learning;CNN;image categorization;canonical perspective;depth channel;object center;Washington RGB-D objects dataset;pose manifolds","","106","19","","","","","IEEE","IEEE Conferences"
"Median Filtering Forensics Based on Convolutional Neural Networks","J. Chen; X. Kang; Y. Liu; Z. J. Wang","School of Information Science and Technology, Sun Yat-sen University, Guangzhou, China; School of Information Science and Technology, Sun Yat-sen University, Guangzhou, China; School of Information Science and Technology, Sun Yat-sen University, Guangzhou, China; Electrical and Computer Engineering Department, University of British Columbia, Vancouver, Canada","IEEE Signal Processing Letters","","2015","22","11","1849","1853","Median filtering detection has recently drawn much attention in image editing and image anti-forensic techniques. Current image median filtering forensics algorithms mainly extract features manually. To deal with the challenge of detecting median filtering from small-size and compressed image blocks, by taking into account of the properties of median filtering, we propose a median filtering detection method based on convolutional neural networks (CNNs), which can automatically learn and obtain features directly from the image. To our best knowledge, this is the first work of applying CNNs in median filtering image forensics. Unlike conventional CNN models, the first layer of our CNN framework is a filter layer that accepts an image as the input and outputs its median filtering residual (MFR). Then, via alternating convolutional layers and pooling layers to learn hierarchical representations, we obtain multiple features for further classification. We test the proposed method on several experiments. The results show that the proposed method achieves significant performance improvements, especially in the cut-and-paste forgery detection.","","","10.1109/LSP.2015.2438008","National Science Foundation of China; 973 Program; NSF of Guangdong province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7113799","Convolutional neural networks;deep learning;hierarchical representations;median filtering forensics","Filtering;Convolution;Forensics;Feature extraction;Kernel;Image coding;Image edge detection","digital forensics;feature extraction;image classification;image coding;image filtering;image representation;median filters;neural nets","image editing;image antiforensic techniques;image median filtering forensics algorithms;feature extraction;small-size compressed image blocks;CNN framework;median filtering residual;MFR;convolutional layers;pooling layers;hierarchical representation;image classification;cut-and-paste forgery detection;convolutional neural networks","","117","24","","","","","IEEE","IEEE Journals"
"Representation models in single channel source separation","M. Zöhrer; F. Pernkopf","Signal Processing and Speech Communication Lab, Graz University of Technology, Austria; Signal Processing and Speech Communication Lab, Graz University of Technology, Austria","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","713","717","Model-based single-channel source separation (SCSS) is an ill-posed problem requiring source-specific prior knowledge. In this paper, we use representation learning and compare general stochastic networks (GSNs), Gauss Bernoulli restricted Boltzmann machines (GBRBMs), conditional Gauss Bernoulli restricted Boltzmann machines (CGBRBMs), and higher order contractive autoencoders (HCAEs) for modeling the source-specific knowledge. In particular, these models learn a mapping from speech mixture spectrogram representations to single-source spectrogram representations, i.e. we apply them as filter for the speech mixture. In the test case, the individual source spectrograms of both models are inferred and the softmask for re-synthesis of the time signals is determined thereof. We evaluate the deep architectures on data of the 2nd CHiME speech separation challenge and provide results for a speaker dependent, a speaker independent, a matched noise condition and an unmatched noise condition task. Our experiments show the best PESQ and overall perceptual score on average for GSNs in all four tasks.","","","10.1109/ICASSP.2015.7178062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178062","single channel source separation;deep neural networks;general stochastic network;representation models","Speech;Noise;Training;Spectrogram;Stochastic processes;Data models;Source separation","acoustic filters;acoustic noise;acoustic radiators;acoustic signal processing;Boltzmann machines;signal representation;source separation;speech coding;speech synthesis;stochastic processes","representation models;model-based single-channel source separation;representation learning;source-specific knowledge;speech mixture spectrogram representation;general stochastic networks;Gauss Bernoulli restricted Boltzmann machines;conditional Gauss Bernoulli restricted Boltzmann machines;higher order contractive autoencoders;single-source spectrogram representations;filter;time signal resynthesis;2nd CHiME speech separation;matched noise condition;unmatched noise condition task;perceptual score","","3","36","","","","","IEEE","IEEE Conferences"
"Software-Defined Networking for RSU Clouds in Support of the Internet of Vehicles","M. A. Salahuddin; A. Al-Fuqaha; M. Guizani","Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA; Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA; Computer Science and Engineering, Qatar University, Doha, Qatar","IEEE Internet of Things Journal","","2015","2","2","133","144","We propose a novel roadside unit (RSU) cloud, a vehicular cloud, as the operational backbone of the vehicle grid in the Internet of Vehicles (IoV). The architecture of the proposed RSU cloud consists of traditional and specialized RSUs employing software-defined networking (SDN) to dynamically instantiate, replicate, and/or migrate services. We leverage the deep programmability of SDN to dynamically reconfigure the services hosted in the network and their data forwarding information to efficiently serve the underlying demand from the vehicle grid. We then present a detailed reconfiguration overhead analysis to reduce reconfigurations, which are costly for service providers. We use the reconfiguration cost analysis to design and formulate an integer linear programming (ILP) problem to model our novel RSU cloud resource management (CRM). We begin by solving for the Pareto optimal frontier (POF) of nondominated solutions, such that each solution is a configuration that minimizes either the number of service instances or the RSU cloud infrastructure delay, for a given average demand. Then, we design an efficient heuristic to minimize the reconfiguration costs. A fundamental contribution of our heuristic approach is the use of reinforcement learning to select configurations that minimize reconfiguration costs in the network over the long term. We perform reconfiguration cost analysis and compare the results of our CRM formulation and heuristic. We also show the reduction in reconfiguration costs when using reinforcement learning in comparison to a myopic approach. We show significant improvement in the reconfigurations costs and infrastructure delay when compared to purist service installations.","","","10.1109/JIOT.2014.2368356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949072","Internet of Vehicles;Cloud Resource Management;RSU Cloud;Intelligent Transportation Systems;Vehicular Ad hoc Networks;Software Defined Networking;Cloud resource management (CRM);intelligent transportation systems (ITS);Internet of Vehicles (IoV);roadside unit (RSU) cloud;software-defined networking (SDN);vehicular ad hoc networks (VANETs)","Vehicles;Cloud computing;Delays;Resource management;Internet of Things;Control systems;Computer architecture","cloud computing;heuristic programming;integer programming;Internet of Things;learning (artificial intelligence);linear programming;minimisation;Pareto optimisation;road vehicles;software defined networking;software radio;vehicular ad hoc networks","Internet of Vehicles;IoV;vehicle grid;software defined networking;SDN;data forwarding information;reconfiguration cost analysis;integer linear programming problem;ILP problem;RSU cloud resource management;roadside unit CRM;Pareto optimal frontier;POF;heuristic approach;reinforcement learning;reconfiguration cost minimization;myopic approach;vehicular ad hoc network","","100","32","","","","","IEEE","IEEE Journals"
"Fully convolutional networks for semantic segmentation","J. Long; E. Shelhamer; T. Darrell","UC Berkeley, USA; UC Berkeley, USA; UC Berkeley, USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3431","3440","Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.","","","10.1109/CVPR.2015.7298965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298965","","Semantics;Training;Convolution;Image segmentation;Computer architecture;Deconvolution;Adaptation models","image classification;image segmentation;inference mechanisms;learning (artificial intelligence)","fully convolutional networks;semantic segmentation;visual models;pixels-to-pixels;inference;learning;contemporary classification networks;PASCAL VOC;NYUDv2;SIFT flow","","4798","39","","","","","IEEE","IEEE Conferences"
"An investigation of augmenting speaker representations to improve speaker normalisation for DNN-based speech recognition","H. Huang; K. C. Sim","School of Computing, National University of Singapore, Republic of Singapore, 117417; School of Computing, National University of Singapore, Republic of Singapore, 117417","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4610","4613","The conventional short-term interval features used by the Deep Neural Networks (DNNs) lack the ability to learn longer term information. This poses a challenge for training a speaker-independent (SI) DNN since the short-term features do not provide sufficient information for the DNN to estimate the real robust factors of speaker-level variations. The key to this problem is to obtain a sufficiently robust and informative speaker representation. This paper compares several speaker representations. Firstly, a DNN speaker classifier is used to extract the bottleneck features as the speaker representation, called the Bottleneck Speaker Vector (BSV). To further improve the robustness of this representation, a first-order Bottleneck Speaker Super Vector (BSSV) is also proposed, where the BSV is expanded into a super vector space by incorporating the phoneme posterior probabilities. Finally, a more fine-grain speaker representation based on the FMLLR-shifted features is examined. The experimental results on the WSJ0 and WSJ1 datasets show that the proposed speaker representations are useful in normalising the speaker effects for robust DNN-based automatic speech recognition. The best performance is achieved by augmenting both the BSSV and the FMLLR-shifted representations, yielding 10.0% - 15.3% relatively performance gains over the SI DNN baseline.","","","10.1109/ICASSP.2015.7178844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178844","speaker normalisation;augmented speaker representation;deep neural network;speech recognition","Feature extraction;Acoustics;Training;Speech;Hidden Markov models;Speech recognition;Silicon","feature extraction;neural nets;probability;speech recognition","speaker representations;speaker normalisation;deep neural networks;speaker-independent DNN;short-term features;DNN speaker classifier;feature extraction;first-order bottleneck speaker super vector;BSSV;phoneme posterior probabilities;FMLLR-shifted features;robust DNN-based automatic speech recognition","","18","12","","","","","IEEE","IEEE Conferences"
"Human activity recognition with HMM-DNN model","L. Zhang; X. Wu; D. Luo","Key Lab of Machine Perception (Ministry of Education), Speech and Hearing Research Center, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Key Lab of Machine Perception (Ministry of Education), Speech and Hearing Research Center, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Key Lab of Machine Perception (Ministry of Education), Speech and Hearing Research Center, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China","2015 IEEE 14th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)","","2015","","","192","197","Activity recognition commonly made use of hidden Markov models (HMMs) to exploit temporal dependencies between activities. The emission distribution of HMMs could be represented by generative models, such as Gaussian mixture models (GMMs), or discriminative models, such as random forest (RF). These models, especially discriminative ones, needed to manually extract features from the sensor data, which relied on the experience of the researchers, and usually was a time-consuming task when complicated features are extracted. Furthermore, with these methods, the process of quantization of the sensor data, i.e., manual feature extraction, might lose much useful information and thus led to a performance debasement. In this paper, we recommend deep neural networks (DNNs) for modeling the emission distribution of HMMs, which automatically learn features suitable for classification from the raw sensor data and then estimate the posterior probabilities of the HMM states. We collected a dataset of daily activities and based on which experiments were performed to compare our HMM-DNN model with both HMM-GMM and HMM-RF. The results illustrated that HMM-DNN outperformed both HMM-GMM and HMM-RF.","","","10.1109/ICCI-CC.2015.7259385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7259385","activity recognition;deep neural networks;hidden Markov models;sensor data;accelerometer","Hidden Markov models;Radio frequency;Accuracy;Legged locomotion;Manuals;Markov processes","feature extraction;Gaussian processes;hidden Markov models;mixture models;neural nets","human activity recognition;HMM-DNN model;hidden Markov models;temporal dependencies;emission distribution;Gaussian mixture models;GMM;generative models;discriminative models;random forest;RF;manual feature extraction;deep neural networks;raw sensor data;posterior probabilities","","7","14","","","","","IEEE","IEEE Conferences"
"Single trial prediction of normal and excessive cognitive load through EEG feature fusion","P. Bashivan; M. Yeasin; G. M. Bidelman","Department of Electrical and Computer Engineering, The University of Memphis, TN, USA; Department of Electrical and Computer Engineering, The University of Memphis, TN, USA; Institute for Intelligent Systems and School of Communication Sciences & Disorders, The University of Memphis, TN, USA","2015 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)","","2015","","","1","5","Detection of subtle changes in cognitive states (e.g., cognitive overload) or epistemic state of mind remains a challenge. As they typically lack visible expressions, indirect methods like analysis of facial expressions are ineffective at best. Towards solving such problem, we present a statistical approach to predict cognitive load from single trial electrophysiological recordings of brain activity (i.e., EEG). We evaluated the utility of two commonly used sets of features, namely, wavelet entropy and band-specific power (theta, alpha, and beta) to predict levels of cognitive load. We show that performance of the model (i.e., support vector machine) could be improved by feature fusion (such as wavelet entropy and spectral power features together) and also integrating nonlinear representations learned through deep belief networks. Our results demonstrate predictions of cognitive load across four different levels with an overall accuracy of 92% during execution of a memory task.","","","10.1109/SPMB.2015.7405422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405422","EEG;cognitive load;wavelet entropy;support vector machines;deep belief networks;random forest","Electroencephalography;Support vector machines;Entropy;Electrodes;Brain modeling;Kernel;Vegetation","belief networks;cognition;electroencephalography;entropy;medical signal processing;neurophysiology;sensor fusion;support vector machines;wavelet transforms","normal cognitive load;excessive cognitive load;EEG feature fusion;epistemic mind state;facial expression analysis;single trial electrophysiological recordings;brain activity;wavelet entropy;band-specific power;nonlinear representations;deep belief networks;memory task;support vector machine","","1","30","","","","","IEEE","IEEE Conferences"
"Utilizing Latent Semantic Word Representations for Automated Essay Scoring","C. Jin; B. He","Sch. of Comput. & Control Eng., Univ. of Chinese Acad. of Sci., Beijing, China; Sch. of Comput. & Control Eng., Univ. of Chinese Acad. of Sci., Beijing, China","2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)","","2015","","","1101","1108","Automated essay scoring (AES) utilizes a set of features to measure the writing quality of essays. However, due to the limits of the existing natural language processing techniques, current AES systems are only capable of making use of shallow text features such as the essay length and the number of the clause. In this paper, we argue that the current AES systems can be further improved by taking into account the latent semantic features. To this end, on top of the commonly used shallow features, we propose three deep semanitc features based on Continuous Bag-of-Words Model (CBOW) and Recursive Auto encoder Model. We use Support Vector Machine for Ranking (SVMrank) to learn a rating model and test the performance of three new features. Experiments on the publicly available English essay dataset, Automated Student Assessment Prize (ASAP), show that our proposed features are beneficial to automated essay scoring.","","","10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518381","Automated essay scoring;Deep neural network;Semantic word representations","Semantics;Natural language processing;Feature extraction;Grammar;Syntactics;Complexity theory","natural language processing;neural nets;support vector machines;text analysis;word processing","latent semantic word representations;automated essay scoring;essay writing quality;natural language processing techniques;AES systems;shallow text features;latent semantic features;deep semanitc features;continuous bag-of-words model;CBOW;recursive autoencoder model;support vector machine for ranking;SVMrank;publicly available English essay dataset;Automated Student Assessment Prize;ASAP","","1","16","","","","","IEEE","IEEE Conferences"
"Investigating online low-footprint speaker adaptation using generalized linear regression and click-through data","Y. Zhao; J. Li; J. Xue; Y. Gong","Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4310","4314","To develop speaker adaptation algorithms for deep neural network (DNN) that are suitable for large-scale online deployment, it is desirable that the adaptation model be represented in a compact form and learned in an unsupervised fashion. In this paper, we propose a novel low-footprint adaptation technique for DNN that adapts the DNN model through node activation functions. The approach introduces slope and bias parameters in the sigmoid activation functions for each speaker, allowing the adaptation model to be stored in a small-sized storage space. We show that this adaptation technique can be formulated in a linear regression fashion, analogous to other speak adaptation algorithms that apply additional linear transformations to the DNN layers. We further investigate semi-supervised online adaptation by making use of the user click-through data as a supervision signal. The proposed method is evaluated on short message dictation and voice search tasks in supervised, unsupervised, and semi-supervised setups. Compared with the singular value decomposition (SVD) bottleneck adaptation, the proposed adaptation method achieves comparable accuracy improvements with much smaller footprint.","","","10.1109/ICASSP.2015.7178784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178784","automatic speech recognition;deep neural network;speaker adaptation;low footprint","Adaptation models;Neural networks;Speech recognition;Speech;Hidden Markov models;Training;Silicon","neural nets;regression analysis;speech recognition","online low-footprint speaker adaptation;generalized linear regression;click-through data;deep neural network;node activation functions;sigmoid activation functions;linear regression fashion;linear transformations;short message dictation;voice search tasks;singular value decomposition","","18","27","","","","","IEEE","IEEE Conferences"
"Recurrent convolutional neural network for object recognition","Ming Liang; Xiaolin Hu","State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Tsinghua University, Beijing 100084, China; State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Tsinghua University, Beijing 100084, China","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3367","3375","In recent years, the convolutional neural network (CNN) has achieved great success in many computer vision tasks. Partially inspired by neuroscience, CNN shares many properties with the visual system of the brain. A prominent difference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abundant. Inspired by this fact, we propose a recurrent CNN (RCNN) for object recognition by incorporating recurrent connections into each convolutional layer. Though the input is static, the activities of RCNN units evolve over time so that the activity of each unit is modulated by the activities of its neighboring units. This property enhances the ability of the model to integrate the context information, which is important for object recognition. Like other recurrent neural networks, unfolding the RCNN through time can result in an arbitrarily deep network with a fixed number of parameters. Furthermore, the unfolded network has multiple paths, which can facilitate the learning process. The model is tested on four benchmark object recognition datasets: CIFAR-10, CIFAR-100, MNIST and SVHN. With fewer trainable parameters, RCNN outperforms the state-of-the-art models on all of these datasets. Increasing the number of parameters leads to even better performance. These results demonstrate the advantage of the recurrent structure over purely feed-forward structure for object recognition.","","","10.1109/CVPR.2015.7298958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298958","","Feeds;Computational modeling;Convolution","computer vision;feedforward neural nets;learning (artificial intelligence);object recognition;recurrent neural nets","object recognition;recurrent convolutional neural network;convolutional neural network;computer vision task;brain;feed-forward architecture;recurrent CNN;RCNN;arbitrarily deep network;CIFAR-10;CIFAR-100;MNIST;SVHN","","203","54","","","","","IEEE","IEEE Conferences"
"Wide-Area Image Geolocalization with Aerial Reference Imagery","S. Workman; R. Souvenir; N. Jacobs","Univ. of Kentucky, Lexington, KY, USA; Univ. of North Carolina at Charlotte, Charlotte, NC, USA; Univ. of Kentucky, Lexington, KY, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","3961","3969","We propose to use deep convolutional neural networks to address the problem of cross-view image geolocalization, in which the geolocation of a ground-level query image is estimated by matching to georeferenced aerial images. We use state-of-the-art feature representations for ground-level images and introduce a cross-view training approach for learning a joint semantic feature representation for aerial images. We also propose a network architecture that fuses features extracted from aerial images at multiple spatial scales. To support training these networks, we introduce a massive database that contains pairs of aerial and ground-level images from across the United States. Our methods significantly out-perform the state of the art on two benchmark datasets. We also show, qualitatively, that the proposed feature representations are discriminative at both local and continental spatial scales.","","","10.1109/ICCV.2015.451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410808","","Feature extraction;Training;Geology;Semantics;Databases;Neural networks","feature extraction;geographic information systems;geophysical image processing;image matching;image retrieval;learning (artificial intelligence);neural net architecture;visual databases","wide-area image geolocalization;aerial reference imagery;deep convolutional neural networks;cross-view image geolocalization;ground-level query image;georeferenced aerial image matching;state-of-the-art feature representations;cross-view training;joint semantic feature representation;network architecture;feature extraction;feature fusion;image database;United States;local spatial scales;continental spatial scales","","39","45","","","","","IEEE","IEEE Conferences"
"Cloud based sports analytics using semantic web tools and technologies","K. Mahmood; H. Takahashi","Department of Computer Science & Engineering, Oakland University, Rochester, MI, USA 48307; Office of Research Innovation and Commercialization, Greenwich University, DK-10, St. 38, Darakshan, D.H.A Phase VI, Karachi, Pakistan","2015 IEEE 4th Global Conference on Consumer Electronics (GCCE)","","2015","","","431","433","The ""variety"" aspect of Big data is of immense significance since it accommodates the numerous formats of information - Structured, Unstructured and Semi-structured This paper presents novel idea of cloud based video analytics using speech processing, Natural Language Processing and semantic web technologies, along with analysis of sensory data obtained from players body sensors. The long term goal of our proposed system based on deep learning is to give answers like what is the best combination of players for next N minutes of the game by performing detailed analysis on variety of big data.","","","10.1109/GCCE.2015.7398708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398708","Semantic web;Soccer;Video Analytics","Sensors;Games;Speech;Big data;Semantic Web;Speech processing;Tagging","Big Data;body sensor networks;cloud computing;learning (artificial intelligence);natural language processing;semantic Web;speech processing;sport;video signal processing","body sensor;natural language processing;speech processing;cloud-based video analytics;Big data;semantic Web technology;semantic Web tool;cloud-based sport analytics","","2","12","","","","","IEEE","IEEE Conferences"
"Steerable second order intensity features for pedestrian detection","S. K. Tan; T. Cham; J. Wu","School of Computer Engineering, Nanyang Technological University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore; Nanjing University, China","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","","2015","","","311","315","In this paper we consider how second order intensity features can be used to boost the accuracies of SVM-based pedestrian detectors, which remain much faster than recently popular deep learning approaches. In particular, we demonstrate that combining second order information features, corresponding to Hessians of patch-wise image intensities, with HOG and LBP features leads to more than 10% improvement in accuracy for frequently used and difficult datasets. In addition, we present a framework to visualize the responses of the linear SVM classifier at different locations and for different feature types, which enables a comprehensive and detailed analysis of failure modes of the current HOG-LBP detector. An interesting observation made is that the weight patterns of the Hessian-based features change when combined with HOG and LBP features as compared to using Hessians only, and that these changes can be understood from an intuitive perspective as detailed in our paper.","","","10.1109/ACPR.2015.7486516","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486516","","Feature extraction;Shape;Detectors;Support vector machines;Eigenvalues and eigenfunctions;Image edge detection;Histograms","image classification;learning (artificial intelligence);object detection;pedestrians;support vector machines","steerable second order intensity features;SVM-based pedestrian detectors;second order information features;patch-wise image intensities;HOG features;LBP features;linear SVM classifier;failure mode analysis;weight patterns;Hessian-based features","","","24","","","","","IEEE","IEEE Conferences"
"Ensemble unsupervised feature selection based on permutation and R-value","Xiaomei Wang; Xiaohui Lin; Xin Huang; Yuansheng Yang","School of Computer Science and Technology, Dalian University of Technology, China; School of Computer Science and Technology, Dalian University of Technology, China; School of Computer Science and Technology, Dalian University of Technology, China; School of Computer Science and Technology, Dalian University of Technology, China","2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","","2015","","","795","800","Selecting the informative features from the high dimensional data can improve the performance of the classification and get a deep understanding of the problems. A non-problem related feature contains little information and has little influence on the data distribution. By permuting the feature and calculating the data distribution difference, how much information the feature contains could be measured. In this paper, we propose an unsupervised feature selection method (EUFSPR), which combines the ensemble technique, clustering, permutation and data distribution evaluation techniques to measure the feature importance. Clustering is adopted to get the sample groups and the data distribution is evaluated by the overlapping areas. Eight gene expression microarray datasets are utilized to demonstrate the effectiveness of the proposed method over the unsupervised feature selection methods and supervised feature selection methods.","","","10.1109/FSKD.2015.7382044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382044","unsupervised feature selection;ensemble technique;clustering;permutation","Clustering algorithms;Indexes;Support vector machines;Classification algorithms;Unsupervised learning;Computer science;Algorithm design and analysis","bioinformatics;feature selection;pattern classification;pattern clustering;unsupervised learning","ensemble unsupervised feature selection method;permutation method;R-value;high-dimensional data;performance improvement;data classification;nonproblem related feature;data distribution;EUFSPR;pattern clustering;data distribution evaluation techniques;feature importance measure;gene expression microarray datasets;supervised feature selection methods","","1","34","","","","","IEEE","IEEE Conferences"
"CNN-based shot boundary detection and video annotation","W. Tong; L. Song; X. Yang; H. Qu; R. Xie","Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University","2015 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting","","2015","","","1","5","With the explosive growth of video data, content-based video analysis and management technologies such as indexing, browsing and retrieval have drawn much attention. Video shot boundary detection (SBD) is usually the first and important step for those technologies. Great efforts have been made to improve the accuracy of SBD algorithms. However, most works are based on signal rather than interpretable features of frames. In this paper, we propose a novel video shot boundary detection framework based on interpretable TAGs learned by Convolutional Neural Networks (CNNs). Firstly, we adopt a candidate segment selection to predict the positions of shot boundaries and discard most non-boundary frames. This preprocessing method can help to improve both accuracy and speed of the SBD algorithm. Then, cut transition and gradual transition detections which are based on the interpretable TAGs are conducted to identify the shot boundaries in the candidate segments. Afterwards, we synthesize the features of frames in a shot and get semantic labels for the shot. Experiments on TRECVID 2001 test data show that the proposed scheme can achieve a better performance compared with the state-of-the-art schemes. Besides, the semantic labels obtained by the framework can be used to depict the content of a shot.","","","10.1109/BMSB.2015.7177222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177222","Retrieval and indexing;shot boundary detection;deep learning;convolutional neural networks;video coding and processing","Semantics;Feature extraction;Computed tomography;Mathematical model;Indexing;Accuracy;Neural networks","neural nets;video coding","convolutional neural networks;CNN;video annotation;video data;content-based video analysis;management technologies;video shot boundary detection;TRECVID 2001 test data;semantic labels","","11","13","","","","","IEEE","IEEE Conferences"
"Multi-modal Convolutional Neural Networks for Activity Recognition","S. Ha; J. Yun; S. Choi","NA; NA; NA","2015 IEEE International Conference on Systems, Man, and Cybernetics","","2015","","","3017","3022","Convolutional neural network (CNN), which comprises one or more convolutional and pooling layers followed by one or more fully-connected layers, has gained popularity due to its ability to learn fruitful representations from images or speeches, capturing local dependency and slight-distortion invariance. CNN has recently been applied to the problem of activity recognition, where 1D kernels are applied to capture local dependency over time in a series of observations measured at inertial sensors (3-axis accelerometers and gyroscopes). In this paper we present a multi-modal CNN where we use 2D kernels in both convolutional and pooling layers, to capture local dependency over time as well as spatial dependency over sensors. Experiments on benchmark datasets demonstrate the high performance of our multi-modal CNN, compared to several state of the art methods.","","","10.1109/SMC.2015.525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379657","Activity recognition;convolutional neural networks;deep learning","Kernel;Convolution;Sensor phenomena and characterization;Neural networks;Intelligent sensors;Feature extraction","feedforward neural nets;pattern recognition","multimodal convolutional neural networks;activity recognition;CNN;inertial sensors;convolutional layer;pooling layer","","21","14","","","","","IEEE","IEEE Conferences"
"Novel hierarchical Cellular Simultaneous Recurrent neural Network for object detection","M. Alam; L. Vidyaratne; K. M. Iftekharuddin","Vision Lab at Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA 23529, USA; Vision Lab at Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA 23529, USA; Vision Lab at Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA 23529, USA","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","7","Large scale feed forward neural networks have seen intense application in many computer vision problems. However, these networks can get hefty and computationally intensive with increasing complexity of the task. This work, for the first time in literature, introduces a Cellular Simultaneous Recurrent Network (CSRN) based hierarchical neural network for object detection. CSRN has shown to be more effective to solving complex tasks such as maze traversal and image processing when compared to generic feed forward networks. While deep neural networks (DNN) have exhibited excellent performance in object detection and recognition, such hierarchical structure has largely been absent in neural networks with recurrency. This work attempts to introduce deep hierarchy in CSRN for object detection. We propose a novel CSRN based DNN feature extractor that utilizes highly efficient filters derived from CSRNs used in the hidden convolutional layer. Experiments using a face detection task show that the CSRN based DNN offers comparable performance to the state-of-the-art convolutional neural network (CNN), while utilizing only as few as five filters in each of its hidden layers. In comparison, the CNN requires from a few to thousands of filters in each of its hidden layers for the same task. We also demonstrate the flexibility of the proposed architecture by introducing hybridization concept to the network to enhance its scale invariance. The hybrid scale invariant architecture is tested with randomly scaled object image dataset for face detection. Finally, we show that the CSRN based hybrid DNN performance is also comparable to that of the hybrid CNN.","","","10.1109/IJCNN.2015.7280480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280480","Cellular Simultaneous Recurrent Network (CSRN);Hierarchical Recurrent Neural Network;Unsupervised Learning;Feature extraction;Auto-encoder","Feature extraction;Face;Biological neural networks;Training;Computer architecture;Glass","computer vision;face recognition;feedforward neural nets;image filtering;object detection;object recognition;recurrent neural nets","hierarchical cellular simultaneous recurrent neural network;object detection;feed forward neural networks;computer vision;cellular simultaneous recurrent network;CSRN;hierarchical neural network;deep neural networks;DNN;object recognition;hidden convolutional layer;face detection task;filters;hybridization concept;hybrid scale invariant architecture;object image dataset","","3","22","","","","","IEEE","IEEE Conferences"
"Network-on-chip: Current issues and challenges","M. S. Gaur; V. Laxmi; M. Zwolinski; M. Kumar; N. Gupta; Ashish","Computer Engineering, Malaviya National Institute of Technology MNIT, Jaipur, India; Computer Engineering, Malaviya National Institute of Technology MNIT, Jaipur, India; Electronic Systems Design Group University of Southampton, High field, Southampton SO17 1BJ; Malaviya National Institute of Technology MNIT, Jaipur, India; Malaviya National Institute of Technology MNIT, Jaipur, India; Malaviya National Institute of Technology MNIT, Jaipur, India","2015 19th International Symposium on VLSI Design and Test","","2015","","","1","3","Due to the shrinking transistor sizes, the density of ICs roughly doubles every year as predicted by Moore's law. These advancements in the VLSI integration densities towards the nano scale era, witnessed a paradigm shift from computation centric designs to communication centric designs incorporating very large number of simple cores. Plenty of traditional interconnect schemes like point to point, buses and crossbars are available to interconnect small number of cores. While achieving fast and efficient communication with point to point communication schemes, wire density is a barrier for adapting them to many core architectures. Moreover, buses are simpler in design, they suffer from the scalability and arbitration issues along with bandwidth bottleneck as the number of cores increases. Similarly area and power requirements of a crossbar limits its applicability. Hence, in many core architectures like Chip Multiprocessors (CMP) and Multi processor System-on-Chip (MPSoCs), emerge the need of an efficient communication infrastructure as traditional solutions fails to handle the communication challenges. Network-on-Chip (NoC), a scalable and modular design approach, has been proposed as a promising alternative to traditional bus based architectures for inter-core communication. NoC has also been accepted in industy (Tilera's TILE-Gx72, TILE64TM [1] processors and Intel's terascale processor [2]. NoCs are an attractive alternative for the traditional shared-buses or dedicated wires due to many reasons. First, NoCs represent a scalable solution to on-chip communication paradigm, because they provide scalable bandwidth at low power and area overheads. Second, NoCs are very efficient in terms of use of wiring and multiplexing many traffic flows on the same channels providing quality of service and higher bandwidth. Finally, on-chip networks with regular topologies have short interconnects that can be optimized and reused using regular iterative blocks, thus making the verification process easy. For on-chip networks, two-dimensional (2D) mesh is the most preferred topology choice due to its regularity, scalability, and perfect physical layout on an actual chip. This tutorial shall focus on NoC routing algorithms, their implementations and issues. The main parameters of the network which are affected by the routing algorithm include fault-tolerance, quality of service, communication performance (throughput and latency) and power consumption. The following are the main objective of this tutorial ; Introduction to NoC [3]: In this part, we briefly discuss about various design parameters of NoC such as topology, switching, flow control, routing and comparison with existing mechanisms. : Routing Taxonomy [4]: In this part, we present classification of various routing algorithms. : Deadlock and Livelock freedom in Routing: One of current issue in NoC routing is the use of acyclic channel dependency graph (ACDG) for deadlock freedom prohibiting certain routing turns. Thus, ACDG reduces the degree of adaptiveness. In this section, we discuss various turn models [5] and how these turn model can be improved to increase adaptivity while maintaining deadlock freedom. : Routing Implementations for NoC: Denser integration advancements make the chip more prone to failures (deep sub-micron effects, manufacturing effects etc). Furthermore these failures may disrupt the regularity of 2D meshes, leading to an irregular set of topologies generated from regular 2D meshes. Under this condition, solutions of regular 2D meshes may no longer work due to irregular topology. In this section, we discuss state-of-art routing implementation techniques [6]-[8] used for irregular 2D mesh under different failures. : Learning methods to handle congestion in Routing: Reinforcement Learning (RL) is a machine learning paradigm that has been widely applied in many areas. The Q-Learning has been used in NOC to learn the network traffic and make the routing decisions accordingly. At each node, a table is used to store the values that represent the congestion level of each link and these values are updated after every packet transfer. Although, Q-Learning has improved network performance but there are many challenges which we would discuss in this section. : Brief hands on tool chain for NoC simulation shall also provide towards the end.","","","10.1109/ISVDAT.2015.7208160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208160","","Routing;System-on-chip;Topology;Computer architecture;Electronic mail;Very large scale integration;Bandwidth","network routing;network-on-chip","network-on-chip;chip multiprocessors;multi processor system-on-chip;inter-core communication;two-dimensional mesh;NoC routing algorithms;fault-tolerance;quality of service;communication performance;power consumption;acyclic channel dependency graph;reinforcement learning","","9","8","","","","","IEEE","IEEE Conferences"
"Acoustic model training based on node-wise weight boundary model increasing speed of discrete neural networks","R. Takeda; K. Komatani; K. Nakadai","Osaka University, The Institute of Scientific and Industrial Research 8-1, Mihogaoka, Ibaraki, Osaka 567-0047, Japan; Osaka University, The Institute of Scientific and Industrial Research 8-1, Mihogaoka, Ibaraki, Osaka 567-0047, Japan; Honda Research Institute Japan Co., Ltd., Wako, Saitama 351-0114, Japan","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","52","58","Our purpose is to realize discrete neural networks (NNs), whose some parameters are discretized, as a low-resource and fast NNs for acoustic models. Two essential problems should be tackled for its realization; 1) the reduction of discretization errors and 2) the implementation method for fast processing. We propose a new parameter training algorithm for 1) and an implementation using look-up table (LUT) on general-purpose CPUs for 2), respectively. The former can set proper boundaries of discretization at each node of NNs, resulting in the reduction of discretization error. The latter can reduce the memory usage of NNs within the cache size of CPU by encoding parameters of NNs. Experiments with 2-bit discrete NNs showed that our algorithm maintained almost the same word accuracy as 8-bit discrete NNs and achieved a 40% increase in speed of the NN's forward calculation.","","","10.1109/ASRU.2015.7404773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404773","Deep Neural Network;Acoustic Model;Quantization;Discretization","Artificial neural networks;Training;Table lookup;Hidden Markov models;Computational modeling;Quantization (signal)","acoustic signal processing;learning (artificial intelligence);neural nets;table lookup","acoustic model training;node-wise weight boundary model;discrete neural networks;discretization error reduction;parameter training algorithm;look-up table;general-purpose CPU","","","17","","","","","IEEE","IEEE Conferences"
"Ex-situ training of dense memristor crossbar for neuromorphic applications","R. Hasan; C. Yakopcic; T. M. Taha","Department of Electrical and Computer Engineering, University of Dayton, Ohio, USA; Department of Electrical and Computer Engineering, University of Dayton, Ohio, USA; Department of Electrical and Computer Engineering, University of Dayton, Ohio, USA","Proceedings of the 2015 IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH´15)","","2015","","","75","81","This study proposes a technique for programming a dense memristor crossbar array without isolation transistors (0T1M) in order to achieve ex-situ training of a neural network. Programming memristors to a specific resistance level requires an iterative process needing the reading of individual memristor resistances due to memristor device stochasticity. This paper presents a circuit to read individual resistances from a 0T1M crossbar and a method to map neuron synaptic weights into a novel neural circuit to enable ex-situ training. The results show that we are able to train the resistances in a 0T1M crossbar and that the 0T1M system is about 93% smaller in area than 1T1M systems.","","","10.1109/NANOARCH.2015.7180590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180590","Memristor crossbars;deep neural networks;pattern recognition;low power system","Decision support systems;Nanoscale devices;GSM;Erbium","iterative methods;learning (artificial intelligence);memristor circuits;neural nets","ex-situ training;neuromorphic applications;dense memristor crossbar array;isolation transistors;neural network;resistance level;iterative process;memristor resistances;memristor device stochasticity;0T1M crossbar;neuron synaptic weights;neural circuit","","10","23","","","","","IEEE","IEEE Conferences"
"High-performance Swahili keyword search with very limited language pack: The THUEE system for the OpenKWS15 evaluation","M. Cai; Z. Lv; C. Lu; J. Kang; L. Hui; Z. Zhang; J. Liu","Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","215","222","This paper presents the Swahili keyword search system developed by the THUEE team for the OpenKWS15 evaluation, which is conducted by NIST under the IARPA Babel program. There are several highlights in the development of the system, including automatic generation of the pronunciation lexicon, aggressive data augmentation, the multilingual bottleneck feature extractor trained from 6 languages, text selection from web data for language model training, semi-supervised training for acoustic models and language models, out-of-vocabulary keyword detection using morphemes and a rich diversity of the systems for combination. A wide variety of acoustic modeling techniques are explored and compared. Up to 12 different individual systems are used for combination. The system achieves the state-of-the-art performance in the required condition of the evaluation.","","","10.1109/ASRU.2015.7404797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404797","Speech recognition;keyword search;deep neural network (DNN);low-resource;acoustic model","Training;Data models;Hidden Markov models;Keyword search;Acoustics;Tuning;Training data","acoustic signal processing;feature extraction;learning (artificial intelligence);natural language processing;speech recognition","ASR;automatic speech recognition;morphemes;out-of-vocabulary keyword detection;language models;acoustic models;semisupervised training;language model training;text selection;multilingual bottleneck feature extractor;data augmentation;automatic pronunciation lexicon generation;IARPA Babel program;NIST;OpenKWS15 evaluation;THUEE team;Swahili keyword search system","","4","42","","","","","IEEE","IEEE Conferences"
"Automatic annotating SRRs from web databases using Naive Bayes approach","N. P. Rane; D. D. Patil","K. K. W. I. E. E. R., Nashik, India; S. S. G. B. C. O. E. T., Bhusawal, India","2015 International Conference on Computer, Communication and Control (IC4)","","2015","","","1","6","The number of web databases are increasing progressively day-by-day and are web accessible through HTML- based form. The data units that are encoded in the search result web page are in a particular structure and unstructured format and are needed for human browsing for applications like, comparison shopping, to rate a web resource, deep web collection etc. So, for machine processing, it is needed to extract all data units at one place and assign semantic labels accurately to the retrieved data units. In this paper, an automatic annotation system is introduced in which the data units from SRRs are extracted, automatic semantic labels are obtained from the data units extracted and the values of the attributes are aligned accurately under the semantic label. To improve the automatic generation of annotations, Naive Bayes machine learning classifier is used. Our results obtained shows that use of proposed system contributes to achieve accurate results.","","","10.1109/IC4.2015.7375679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375679","Data Extraction;Data Alignment;Data Annotation;Search Result Records;Naive Bayes","Data mining;Feature extraction;Semantics;Databases;Web pages;Search engines;Visualization","information retrieval;Internet;learning (artificial intelligence);pattern classification;search engines","automatic annotating SRRs;Web databases;HTML- based form;data unit extraction;Web page;search result records;automatic semantic labels;automatic annotation generation;Naive Bayes machine learning classifier","","","10","","","","","IEEE","IEEE Conferences"
"ItHELPS: Iterative high-accuracy error localization in post-silicon","V. Bertacco; W. Bonkowski","Department of Computer Science and Engineering, University of Michigan; Department of Computer Science and Engineering, University of Michigan","2015 33rd IEEE International Conference on Computer Design (ICCD)","","2015","","","196","199","The increasing complexity of modern digital circuits has exacerbated the challenge of verifying the functionality of these systems. To further compound the issue, shrinking time-to-market constraints place increased pressure on attaining correct devices in short amounts of time. As a result, more and more of the burden of validation has shifted to the post-silicon stage, when the first silicon prototypes of a design become available. This validation phase brings much faster test execution speeds, at the cost of a very limited ability of diagnosing bugs. To further compound the problem, intermittent failures are not uncommon, due to the physical nature of the device under validation. In this work we propose ItHELPS, a solution to identify the timing of a bug manifestation and the root signals responsible for it in industry-size complex digital designs. We employ a synergistic approach based on a machine-learning solution (DBSCAN) paired with an adaptive refinement analysis, capable of narrowing the location of a failure down to a handful of signals, possibly buried deep within the design hierarchy. We find experimentally that our approach outperforms the accuracy of prior state-of-the-art solutions by two orders of magnitude.","","","10.1109/ICCD.2015.7357103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357103","","Algorithm design and analysis;Monitoring;Silicon;Prototypes;Computer bugs;Software;Iterative methods","circuit complexity;electronic engineering computing;failure analysis;integrated circuit design;integrated circuit testing;learning (artificial intelligence)","design hierarchy;adaptive refinement analysis;DBSCAN;machine learning solution;industry-size complex digital designs;root signals;bug manifestation;intermittent failures;digital circuit complexity;iterative high-accuracy error localization in post-silicon;ItHELPS","","2","13","","","","","IEEE","IEEE Conferences"
"Transformation-Invariant Convolutional Jungles","D. Laptev; J. M. Buhmann","ETH Zurich, Switzerland; ETH Zurich, Switzerland","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","3043","3051","Many Computer Vision problems arise from information processing of data sources with nuisance variances like scale, orientation, contrast, perspective foreshortening or - in medical imaging - staining and local warping. In most cases these variances can be stated a priori and can be used to improve the generalization of recognition algorithms. We propose a novel supervised feature learning approach, which efficiently extracts information from these constraints to produce interpretable, transformation-invariant features. The proposed method can incorporate a large class of transformations, e.g., shifts, rotations, change of scale, morphological operations, non-linear distortions, photometric transformations, etc. These features boost the discrimination power of a novel image classification and segmentation method, which we call Transformation-Invariant Convolutional Jungles (TICJ). We test the algorithm on two benchmarks in face recognition and medical imaging, where it achieves state of the art results, while being computationally significantly more efficient than Deep Neural Networks.","","","10.1109/CVPR.2015.7298923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298923","","Kernel;Neural networks;Optimization;Computer vision;Feature extraction;Training;Image segmentation","computer vision;feature extraction;image classification;image segmentation;learning (artificial intelligence)","transformation-invariant convolutional jungles;computer vision;recognition algorithms;supervised feature learning approach;information extraction;interpretable feature;transformation-invariant feature;image classification;image segmentation;TICJ;face recognition;medical imaging","","6","29","","","","","IEEE","IEEE Conferences"
"Differentiable pooling for unsupervised speaker adaptation","P. Swietojanski; S. Renals","Centre for Speech Technology Research, University of Edinburgh, EH8 9AB, UK; Centre for Speech Technology Research, University of Edinburgh, EH8 9AB, UK","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4305","4309","This paper proposes a differentiable pooling mechanism to perform model-based neural network speaker adaptation. The proposed technique learns a speaker-dependent combination of activations within pools of hidden units, was shown to work well unsupervised, and does not require speaker-adaptive training. We have conducted a set of experiments on the TED talks data, as used in the IWSLT evaluations. Our results indicate that the approach can reduce word error rates (WERs) on standard IWSLT test sets by about 5-11% relative compared to speaker-independent systems and was found complementary to the recently proposed learning hidden units contribution (LHUC) approach, reducing WER by 6-13% relative. Both methods were also found to work well when adapting with small amounts of unsupervised data - 10 seconds is able to decrease the WER by 5% relative compared to the baseline speaker independent system.","","","10.1109/ICASSP.2015.7178783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178783","Differentiable pooling;Speaker Adaptation;Deep Neural Networks;TED;LHUC","Adaptation models;Artificial neural networks;Training;Lead","loudspeakers","unsupervised speaker adaptation;differentiable pooling mechanism;model-based neural network speaker adaptation;TED;IWSLT evaluations;word error rates;WER;speaker-independent systems;learning hidden units contribution;LHUC","","15","41","","","","","IEEE","IEEE Conferences"
"SNAPS: Semantic network traffic analysis through projection and selection","B. C. M. Cappers; J. J. van Wijk","Department of Mathematics and Computer Science, Eindhoven University of Technology, The Netherlands; Department of Mathematics and Computer Science, Eindhoven University of Technology, The Netherlands","2015 IEEE Symposium on Visualization for Cyber Security (VizSec)","","2015","","","1","8","Most network traffic analysis applications are designed to discover malicious activity by only relying on high-level flow-based message properties. However, to detect security breaches that are specifically designed to target one network (e.g., Advanced Persistent Threats), deep packet inspection and anomaly detection are indispensible. In this paper, we focus on how we can support experts in discovering whether anomalies at message level imply a security risk at network level. In SNAPS (Semantic Network traffic Analysis through Projection and Selection), we provide a bottom-up pixel-oriented approach for network traffic analysis where the expert starts with low-level anomalies and iteratively gains insight in higher level events through the creation of multiple selections of interest in parallel. The tight integration between visualization and machine learning enables the expert to iteratively refine anomaly scores, making the approach suitable for both post-traffic analysis and online monitoring tasks. To illustrate the effectiveness of this approach, we present example explorations on two real-world data sets for the detection and understanding of potential Advanced Persistent Threats in progress.","","","10.1109/VIZSEC.2015.7312768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312768","Anomaly detection;network traffic analysis;multivariate analysis;streaming data;interaction;parse data analysis","Protocols;Data visualization;Context;Image color analysis;Semantics;Histograms;Payloads","iterative methods;learning (artificial intelligence);semantic networks;telecommunication computing;telecommunication security;telecommunication traffic","SNAPS;malicious activity;high-level flow-based message properties;security breaches;advanced persistent threats;packet inspection;anomaly detection;network level security risk;semantic network traffic analysis through projection and selection;bottom-up pixel-oriented approach;network traffic analysis;low-level anomalies;machine learning;post-traffic analysis;online monitoring tasks","","12","27","","","","","IEEE","IEEE Conferences"
"Beyond human recognition: A CNN-based framework for handwritten character recognition","L. Chen; S. Wang; W. Fan; J. Sun; S. Naoi","Fujitsu Research & Development Center, Beijing, China; Fujitsu Research & Development Center, Beijing, China; Fujitsu Research & Development Center, Beijing, China; Fujitsu Research & Development Center, Beijing, China; Fujitsu Research & Development Center, Beijing, China","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","","2015","","","695","699","Because of the various appearance (different writers, writing styles, noise, etc.), the handwritten character recognition is one of the most challenging task in pattern recognition. Through decades of research, the traditional method has reached its limit while the emergence of deep learning provides a new way to break this limit. In this paper, a CNN-based handwritten character recognition framework is proposed. In this framework, proper sample generation, training scheme and CNN network structure are employed according to the properties of handwritten characters. In the experiments, the proposed framework performed even better than human on handwritten digit (MNIST) and Chinese character (CASIA) recognition. The advantage of this framework is proved by these experimental results.","","","10.1109/ACPR.2015.7486592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486592","","Training;Distortion;Character recognition;Machine learning;Error analysis;Neurons","handwritten character recognition;neural nets;pattern recognition","human recognition;CNN-based framework;handwritten character recognition;pattern recognition;sample generation;training scheme","","28","23","","","","","IEEE","IEEE Conferences"
"Convolutional Channel Features","B. Yang; J. Yan; Z. Lei; S. Z. Li","Nat. Lab. of Pattern Recognition, Inst. of Autom., Beijing, China; Nat. Lab. of Pattern Recognition, Inst. of Autom., Beijing, China; Nat. Lab. of Pattern Recognition, Inst. of Autom., Beijing, China; Nat. Lab. of Pattern Recognition, Inst. of Autom., Beijing, China","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","82","90","Deep learning methods are powerful tools but often suffer from expensive computation and limited flexibility. An alternative is to combine light-weight models with deep representations. As successful cases exist in several visual problems, a unified framework is absent. In this paper, we revisit two widely used approaches in computer vision, namely filtered channel features and Convolutional Neural Networks (CNN), and absorb merits from both by proposing an integrated method called Convolutional Channel Features (CCF). CCF transfers low-level features from pre-trained CNN models to feed the boosting forest model. With the combination of CNN features and boosting forest, CCF benefits from the richer capacity in feature representation compared with channel features, as well as lower cost in computation and storage compared with end-to-end CNN methods. We show that CCF serves as a good way of tailoring pre-trained CNN models to diverse tasks without fine-tuning the whole network to each task by achieving state-of-the-art performances in pedestrian detection, face detection, edge detection and object proposal generation.","","","10.1109/ICCV.2015.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410375","","Feature extraction;Boosting;Computational modeling;Image edge detection;Object detection;Support vector machines;Proposals","computer vision;feature extraction;neural nets;object detection","pedestrian detection;face detection;edge detection;object proposal generation;boosting forest model;CCF;CNN;convolutional neural networks;filtered channel features;computer vision;deep learning methods;convolutional channel features","","140","57","","","","","IEEE","IEEE Conferences"
"Effective training of convolutional networks using noisy Web images","P. D. Vo; A. Ginsca; H. Le Borgne; A. Popescu","CEA, LIST, Vision and Content Engineering Laboratory, France; CEA, LIST, Vision and Content Engineering Laboratory, France; CEA, LIST, Vision and Content Engineering Laboratory, France; CEA, LIST, Vision and Content Engineering Laboratory, France","2015 13th International Workshop on Content-Based Multimedia Indexing (CBMI)","","2015","","","1","6","Deep convolutional networks have recently shown very interesting performance in a variety of computer vision tasks. Besides network architecture optimization, a key contribution to their success is the availability of training data. Network training is usually done with manually validated data but this approach has a significant cost and poses a scalability problem. Here we introduce an innovative pipeline that combines weakly-supervised image reranking methods and network fine-tuning to effectively train convolutional networks from noisy Web collections. We evaluate the proposed training method versus the conventional supervised training on cross-domain classification tasks. Results show that our method outperforms the conventional method in all of the three datasets. Our findings open opportunities for researchers and practitioners to use convolutional networks with inexpensive training cost.","","","10.1109/CBMI.2015.7153607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153607","","Training;Databases;Noise measurement;Accuracy;Noise;Support vector machines;Training data","convolution;image classification;learning (artificial intelligence);optimisation","noisy Web images;deep convolutional networks;computer vision tasks;network architecture optimization;training data;network training;weakly-supervised image reranking methods;network fine-tuning;noisy Web collections;conventional supervised training;cross-domain classification tasks","","","22","","","","","IEEE","IEEE Conferences"
"AttentionNet: Aggregating Weak Directions for Accurate Object Detection","D. Yoo; S. Park; J. Lee; A. S. Paek; I. S. Kweon","NA; NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2659","2667","We present a novel detection method using a deep convolutional neural network (CNN), named AttentionNet. We cast an object detection problem as an iterative classification problem, which is the most suitable form of a CNN. AttentionNet provides quantized weak directions pointing a target object and the ensemble of iterative predictions from AttentionNet converges to an accurate object boundary box. Since AttentionNet is a unified network for object detection, it detects objects without any separated models from the object proposal to the post bounding-box regression. We evaluate AttentionNet by a human detection task and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC 2007/2012 with an 8-layered architecture only.","","","10.1109/ICCV.2015.305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410662","","Proposals;Object detection;Training;Agriculture;Computer vision;Computer architecture;Predictive models","convolution;learning (artificial intelligence);neural nets;object detection","AttentionNet;object detection;deep convolutional neural network;CNN","","33","28","","","","","IEEE","IEEE Conferences"
"Coupled Dictionaries for Exemplar-Based Speech Enhancement and Automatic Speech Recognition","D. Baby; T. Virtanen; J. F. Gemmeke; H. Van hamme","KU Leuven, Speech Processing Research Group, Electrical Engineering Department (ESAT), Leuven, Belgium; Department of Signal Processing, Tampere University of Technology, Tampere, Finland; KU Leuven, Speech Processing Research Group, Electrical Engineering Department (ESAT), Leuven, Belgium; KU Leuven, Speech Processing Research Group, Electrical Engineering Department (ESAT), Leuven, Belgium","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2015","23","11","1788","1799","Exemplar-based speech enhancement systems work by decomposing the noisy speech as a weighted sum of speech and noise exemplars stored in a dictionary and use the resulting speech and noise estimates to obtain a time-varying filter in the full-resolution frequency domain to enhance the noisy speech. To obtain the decomposition, exemplars sampled in lower dimensional spaces are preferred over the full-resolution frequency domain for their reduced computational complexity and the ability to better generalize to unseen cases. But the resulting filter may be sub-optimal as the mapping of the obtained speech and noise estimates to the full-resolution frequency domain yields a low-rank approximation. This paper proposes an efficient way to directly compute the full-resolution frequency estimates of speech and noise using coupled dictionaries: an input dictionary containing atoms from the desired exemplar space to obtain the decomposition and a coupled output dictionary containing exemplars from the full-resolution frequency domain. We also introduce modulation spectrogram features for the exemplar-based tasks using this approach. The proposed system was evaluated for various choices of input exemplars and yielded improved speech enhancement performances on the AURORA-2 and AURORA-4 databases. We further show that the proposed approach also results in improved word error rates (WERs) for the speech recognition tasks using HMM-GMM and deep-neural network (DNN) based systems.","","","10.1109/TASLP.2015.2450491","European Commission; IWT-SBO; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7138598","Exemplar-based;modulation envelope;noise robust automatic speech recognition;non-negative sparse coding","Speech;Noise;Dictionaries;Speech enhancement;Discrete Fourier transforms;Modulation","approximation theory;computational complexity;filtering theory;Gaussian processes;hidden Markov models;learning (artificial intelligence);mixture models;neural nets;signal denoising;speech enhancement;speech recognition","DNN based systems;deep-neural network based systems;HMM-GMM;AURORA-4 database;AURORA-2 database;modulation spectrogram features;coupled output dictionary;input dictionary;low-rank approximation;computational complexity reduction;full-resolution frequency domain;time-varying filter;noise exemplars;weighted sum-of-speech;noisy speech enhancement;automatic speech recognition;exemplar-based speech enhancement systems","","12","49","","","","","IEEE","IEEE Journals"
"A new approach for automated detection of behavioral task onset for patients with Parkinson's disease using subthalamic nucleus local field potentials","N. Zaker; J. J. Zhang; S. Hanrahan; J. Nedrud; A. O. Hebb","Department of Electrical and Computer Engineering, University of Denver, CO 80210; Department of Electrical and Computer Engineering, University of Denver, CO 80210; Colorado Neurological Institute, Englewood, CO 80113; Colorado Neurological Institute, Englewood, CO 80113; Colorado Neurological Institute, Englewood, CO 80113","2015 49th Asilomar Conference on Signals, Systems and Computers","","2015","","","780","784","We present a new automated onset detection approach for behavioral tasks of patients with Parkinson's disease (PD) using Local Field Potential (LFP) signals collected during Deep Brain Stimulation (DBS) implantation surgeries. Using time-frequency signal processing methods, features are extracted and clustered in the feature space. A supervised Discrete Hidden Markov Models (DHMM) is employed and merged with Support Vector Machines (SVM) in a two-layer classifier to boost up the detection rate. According to our experimental results, the proposed approach can detect the onset of behaviors using LFP signals collected during DBS surgery with the accuracy of 84% while the acceptable delay is set to 1500 ms.","","","10.1109/ACSSC.2015.7421240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7421240","","Hidden Markov models;Feature extraction;Satellite broadcasting;Time-frequency analysis;Support vector machines;Training;Electric potential","diseases;feature extraction;hidden Markov models;learning (artificial intelligence);medical signal processing;nucleus;patient care;signal classification;support vector machines;time-frequency analysis","automated behavioral task onset detection approach;Parkinson disease;subthalamic nucleus local field potentials;local field potential signals;LFP signals;deep brain stimulation implantation surgeries;DBS implantation surgeries;time-frequency signal processing methods;feature extraction;feature clustering;feature space;supervised discrete hidden Markov models;DHMM;support vector machines;SVM;two-layer classifier","","","17","","","","","IEEE","IEEE Conferences"
"Building Handwriting Recognizers by Leveraging Skeletons of Both Offline and Online Samples","X. Zhang; M. Wang; L. Wang; Q. Huo; H. Li","Harbin Institute of Technology, 150001, China; Shanghai Jiao Tong University, 200240, China; Microsoft Research Asia, Beijing 100080, China; Microsoft Research Asia, Beijing 100080, China; Harbin Institute of Technology, 150001, China","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","406","410","We present an approach to leveraging both offline and online handwriting samples to build a single recognizer for recognizing both offline and online handwritings. Given a training set of offline handwriting samples and another set of online handwriting samples, a skeleton is derived first from each offline handwriting sample via vectorization. Then both the skeleton samples and online handwriting samples are normalized and rendered by using the same method to generate a combined training set of skeleton images. Finally a handwriting recognizer based on Deep Bidirectional Long Short-Term Memory (DBLSTM) and Hidden Markov Model (HMM) is built from the skeleton images. In recognition, a preprocessing step consistent with that in training is applied to an unknown offline or online handwriting sample to derive a skeleton image, which is recognized by the hybrid DBLSTM-HMM handwriting recognition system accordingly. We have built such a recognizer by using IAM benchmark databases of offline and online English handwritings plus an internal online handwriting corpus, which outperforms the recognizers built from either offline or online handwriting samples only.","","","10.1109/ICDAR.2015.7333793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333793","","Handwriting recognition;Training;Image recognition;Yttrium;Text recognition","handwriting recognition;hidden Markov models;learning (artificial intelligence);natural language processing","vectorization;online handwriting sample normalization;online handwriting sample rendering;combined training set;skeleton images;handwriting recognizer;deep bidirectional long short-term memory;hidden Markov model;preprocessing step;DBLSTM-HMM handwriting recognition system;lAM benchmark databases;online English handwritings;offline English handwriting sample;internal online handwriting corpus","","2","41","","","","","IEEE","IEEE Conferences"
"Recognition of Haptic Interaction Patterns in Dyadic Joint Object Manipulation","C. E. Madan; A. Kucukyilmaz; T. M. Sezgin; C. Basdogan","College of Engineering, Koc University, Istanbul, Sariyer, Turkey; Electrical and Electronics Department, Imperial College London, Exhibition Road, London, United Kingdom; College of Engineering, Koc University, Istanbul, Sariyer, Turkey; College of Engineering, Koc University, Istanbul, Sariyer, Turkey","IEEE Transactions on Haptics","","2015","8","1","54","66","The development of robots that can physically cooperate with humans has attained interest in the last decades. Obviously, this effort requires a deep understanding of the intrinsic properties of interaction. Up to now, many researchers have focused on inferring human intents in terms of intermediate or terminal goals in physical tasks. On the other hand, working side by side with people, an autonomous robot additionally needs to come up with in-depth information about underlying haptic interaction patterns that are typically encountered during human-human cooperation. However, to our knowledge, no study has yet focused on characterizing such detailed information. In this sense, this work is pioneering as an effort to gain deeper understanding of interaction patterns involving two or more humans in a physical task. We present a labeled human-human-interaction dataset, which captures the interaction of two humans, who collaboratively transport an object in an haptics-enabled virtual environment. In the light of information gained by studying this dataset, we propose that the actions of cooperating partners can be examined under three interaction types: In any cooperative task, the interacting humans either 1) work in harmony, 2) cope with conflicts, or 3) remain passive during interaction. In line with this conception, we present a taxonomy of human interaction patterns; then propose five different feature sets, comprising force-, velocity-and power-related information, for the classification of these patterns. Our evaluation shows that using a multi-class support vector machine (SVM) classifier, we can accomplish a correct classification rate of 86 percent for the identification of interaction patterns, an accuracy obtained by fusing a selected set of most informative features by Minimum Redundancy Maximum Relevance (mRMR) feature selection method.","","","10.1109/TOH.2014.2384049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991578","classifier design and evaluation;feature evaluation and selection;haptic collaboration;haptic interfaces;haptics-enabled virtual environments;interaction patterns;machine learning;pattern recognition;physical human-X interaction;realistic haptic human-robot interaction;support vector machine classification;Behavior recognition;Behavior recognition;classifier design and evaluation;feature evaluation and selection;haptic collaboration;haptic interfaces;haptics-enabled virtual environments;interaction patterns;machine learning;pattern recognition;physical human-X interaction;realistic haptic human-robot interaction;support vector machine classification","Robots;Haptic interfaces;Pattern recognition;Joints;Force;Hip;Virtual environments","haptic interfaces;human-robot interaction;support vector machines","haptic interaction pattern recognition;dyadic joint object manipulation;robot development;intrinsic properties;physical tasks;autonomous robot;human-human cooperation;interaction patterns;human-human-interaction dataset;human interaction patterns;support vector machine;SVM classifier;minimum redundancy maximum relevance;mRMR feature selection method","Cooperative Behavior;Humans;Man-Machine Systems;Models, Biological;Pattern Recognition, Automated;Support Vector Machine;Task Performance and Analysis;Touch","12","29","","","","","IEEE","IEEE Journals"
"A Methodology for Automatically 3D Geological Modeling Based on Geophysical Data Grids","X. Yu; Y. Xu","NA; NA","2015 8th International Conference on Intelligent Computation Technology and Automation (ICICTA)","","2015","","","40","43","Using 3D visualization models to exhibit geological structure has become a trend in geological studies. Compared to 2D geological mapping, 3D geological modeling is dependent on more geological sampling information. In many cases, however, the geological sampling information is difficult to acquire by drilling (especially for deep subsurface information). Geophysical methods (e.g., Gravity, seismic, and electric) have become the major tools in geological modeling. Because the geophysical data are recorded in a data grid, people must extract the geological information from various data grids acquired through different geophysical methods and subsequently integrate the information to manually construct a 3D geological model. This approach usually causes inconvenience and inefficiencies in practice. Therefore, we propose a methodology of automatically 3D geological modeling based on geophysical data grids. The method first constructs visualization models from different geophysical data grids and subsequently integrates these models for interpretation using mapping rules learned from physical properties of rock samples measured in a laboratory and finally converts the interpreted visualization model to a 3D geological model. With the application in the practical work, the result demonstrates that the methodology can effectively solve problems of 3D geological modeling in the case of enriched geophysical data lacking sufficient geological sampling information.","","","10.1109/ICICTA.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7473231","data visualization;3D geological modeling;machine learning","Geology;Data models;Solid modeling;Three-dimensional displays;Data visualization;Geophysical measurements;Data mining","data visualisation;geology;geophysical techniques;geophysics computing;rocks","automatically 3D geological modeling;geophysical data grids;3D visualization models;geological sampling information;geophysical methods;rock physical properties","","","16","","","","","IEEE","IEEE Conferences"
"Stereoscopy and haptics human eye AR app","C. Soto-Fernandez; H. Vega; A. Uribe-Quevedo; N. Jaimes; B. Kapralos","Mechatronics Engineering, Mil. Nueva Granada University, Bogota, Colombia; Multimedia Engineering, Mil. Nueva Granada University, Bogota, Colombia; Industrial Engineering, Mil. Nueva Granada University, Bogota, Colombia; School of Medicine, Mil. Nueva Granada University, Bogota, Colombia; Business and Information Technology, University of Ontario, Oshawa, Canada","2015 IEEE Games Entertainment Media Conference (GEM)","","2015","","","1","2","The study of anatomy using virtual reality provides examination and navigation features otherwise difficult in real life due to the size and location of what it is being studied. When studying the human eye common medical school resources include illustrations, pictures, mockups and practices on cadavers and advanced simulators. Current technological trends are seeing the employment of 3D technologies such as virtual reality and augmented reality as complementary tools to promote learning skills. The human eye is a spherical structure yet when presented in virtual training environments, the eye is typically lacking stereoscopic features making it difficult for the trainees to develop a deep understanding of the eye's anatomical structure. In this project we discuss the development of a stereoscopic augmented reality app. The app incorporates haptics, and employs traditional means of eye learning and examination using game elements to promote engagement.","","","10.1109/GEM.2015.7377255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7377255","","Haptic interfaces;Three-dimensional displays;Solid modeling;Servers;Education;Electronic mail;Virtual reality","augmented reality;visual perception","haptics human eye AR app;virtual reality;examination features;navigation features;human eye common medical school resources;illustrations;pictures;mockups;practices;cadavers;advanced simulators;3D technologies;augmented reality;complementary tools;learning skills;virtual training environments;eye anatomical structure;stereoscopic app;eye learning;game elements","","","8","","","","","IEEE","IEEE Conferences"
"Variable step-size MLMS algorithm for digital predistortion in wideband OFDM systems","F. Zhang; Y. Wang; B. Ai","Xidian University, Xi'an 710071, China; Xidian University, Xi'an 710071, China; Beijing Jiaotong University, Beijing 100044, China","IEEE Transactions on Consumer Electronics","","2015","61","1","10","15","As one of the most promising linearization techniques, adaptive Digital Predistortion (PD) has been widely utilized in modern wireless communication systems for improving the efficiency of Power Amplifier (PA). In view of the non-stationary signal environment for the wideband PAs, an efficient indirect learning adaptive PD is proposed in the paper based on the Memory Polynomial Model (MPM). The coefficients of the proposed PD can be effectively identified by the Modified Least Mean Square (MLMS) learning algorithm. In addition, more stable convergence and lower steady-state error can be achieved simultaneously for the PAs with deep memory effects by adopting the variable step-size parameter. Theoretical analysis results regarding the learning stability, convergence behavior, and selection criteria of initial settings are derived. Simulations demonstrate that MLMS outperforms traditional LMS, Normalized LMS (NLMS), and Generalized Normalized Gradient Descent (GNGD) algorithms in terms of the Normalized Mean Square Error (NMSE) and out-of-band Power Spectral Density (PSD) under the noisy feedback condition for the wideband PAs1.","","","10.1109/TCE.2015.7064105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064105","Digital predistortion;Power amplifier;Variable step-size;Modified least mean square","Least squares approximations;Noise measurement;Convergence;Predistortion;Wideband;OFDM;Algorithm design and analysis","gradient methods;least mean squares methods;OFDM modulation;power amplifiers","variable step-size MLMS algorithm;wideband OFDM systems;linearization techniques;adaptive digital predistortion;power amplifier;nonstationary signal environment;memory polynomial model;modified least mean square learning algorithm;steady-state error;variable step-size parameter;learning stability;normalized LMS algorithm;generalized normalized gradient descent algorithms;normalized mean square error;GNGD algorithms;NMSE;out-of-band power spectral density;PSD","","8","12","","","","","IEEE","IEEE Journals"
"Sign Language Recognition using 3D convolutional neural networks","Jie Huang; Wengang Zhou; Houqiang Li; Weiping Li","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China","2015 IEEE International Conference on Multimedia and Expo (ICME)","","2015","","","1","6","Sign Language Recognition (SLR) targets on interpreting the sign language into text or speech, so as to facilitate the communication between deaf-mute people and ordinary people. This task has broad social impact, but is still very challenging due to the complexity and large variations in hand actions. Existing methods for SLR use hand-crafted features to describe sign language motion and build classification models based on those features. However, it is difficult to design reliable features to adapt to the large variations of hand gestures. To approach this problem, we propose a novel 3D convolutional neural network (CNN) which extracts discriminative spatial-temporal features from raw video stream automatically without any prior knowledge, avoiding designing features. To boost the performance, multi-channels of video streams, including color information, depth clue, and body joint positions, are used as input to the 3D CNN in order to integrate color, depth and trajectory information. We validate the proposed model on a real dataset collected with Microsoft Kinect and demonstrate its effectiveness over the traditional approaches based on hand-crafted features.","","","10.1109/ICME.2015.7177428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177428","Sign Language Recognition;3D Convolutional Neural Networks;Deep Learning","Three-dimensional displays;Feature extraction;Gesture recognition;Assistive technology;Hidden Markov models;Convolution;Trajectory","handicapped aids;image colour analysis;neural nets;sign language recognition;social sciences;video signal processing","sign language recognition;3D convolutional neural networks;SLR targets;deaf-mute people;ordinary people;social impact;hand gestures;CNN;video streams;color information;depth clue;body joint positions;Microsoft Kinect","","36","17","","","","","IEEE","IEEE Conferences"
"A Novel Fast Approach for Convolutional Networks with Small Filters Based on GPU","W. Jiang; Y. Chen; H. Jin; B. Luo; Y. Chi","Services Comput. Technol. & Syst. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China; Services Comput. Technol. & Syst. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China; Services Comput. Technol. & Syst. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China; Services Comput. Technol. & Syst. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China; Services Comput. Technol. & Syst. Lab., Huazhong Univ. of Sci. & Technol., Wuhan, China","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","278","283","Recently, convolutional networks have achieved great successes in the field of computer vision. In order to improve the efficiency of convolutional networks, large amount of solutions focusing on training algorithms and parallelism strategies have been proposed. In this paper, a novel algorithm based on look-up table is proposed to speed up convolutional networks with small filters by applying GPU. By transforming multiplication operations in the convolution computation to some table-based summation operations, the overhead of convolution computation can be reduced largely. The process of creating table and looking up table is very appropriate for parallelization on GPU. Experiment results show that the proposed approaches can improve the speed of convolution computation by 20%-30%, compared with state-of-the-art existing works.","","","10.1109/HPCC-CSS-ICESS.2015.218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336176","Deep Learning;Convolutional Network;Parallel Computation;GPU","Convolution;Graphics processing units;Training;Computational modeling;Filtering algorithms;Kernel;Digital filters","graphics processing units;parallel processing;table lookup","convolutional network;GPU;computer vision;parallelism strategy;convolution computation;table-based summation operation;look-up table","","3","15","","","","","IEEE","IEEE Conferences"
"Automated anatomical landmark detection ondistal femur surface using convolutional neural network","D. Yang; S. Zhang; Z. Yan; C. Tan; K. Li; D. Metaxas","CBIM, Rutgers University, Piscataway, NJ, US; Department of Computer Science, University of North Carolina at Charlotte, NC, US; CBIM, Rutgers University, Piscataway, NJ, US; CBIM, Rutgers University, Piscataway, NJ, US; Department of Industrial and Systems Engineering, Rutgers University, Piscataway, NJ, US; CBIM, Rutgers University, Piscataway, NJ, US","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","","2015","","","17","21","Accurate localization of the anatomical landmarks on distal femur bone in the 3D medical images is very important for knee surgery planning and biomechanics analysis. However, the landmark identification process is often conducted manually or by using the inserted auxiliaries, which is time-consuming and lacks of accuracy. In this paper, an automatic localization method is proposed to determine positions of initial geometric landmarks on femur surface in the 3D MR images. Based on the results from the convolutional neural network (CNN) classifiers and shape statistics, we use the narrow-band graph cut optimization to achieve the 3D segmentation of femur surface. Finally, the anatomical landmarks are located on the femur according to the geometric cues of surface mesh. Experiments demonstrate that the proposed method is effective, efficient, and reliable to segment femur and locate the anatomical landmarks.","","","10.1109/ISBI.2015.7163806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163806","Deep learning;anatomical landmark detection;convolutional neural network;graph cut;mesh curvature","Three-dimensional displays;Biomedical imaging;Image segmentation;Shape;Training;Bones;Neural networks","biomechanics;biomedical MRI;bone;graph theory;image classification;medical image processing;neural nets;optimisation;surgery","automated anatomical landmark detection;distal femur surface;convolutional neural network;distal femur bone;3D medical images;knee surgery planning;biomechanics analysis;landmark identification process;automatic localization method;3D MR images;magnetic resonance images;CNN classifiers;shape statistics;narrow-band graph cut optimization;3D segmentation;surface mesh","","11","20","","","","","IEEE","IEEE Conferences"
"Interpolation aided fuzzy image classification","Yongfeng Zhang; C. Shang; Q. Shen","Department of Computer Science, Institute of Mathematics, Physics and Computer Science, Aberystwyth University, UK; Department of Computer Science, Institute of Mathematics, Physics and Computer Science, Aberystwyth University, UK; Department of Computer Science, Institute of Mathematics, Physics and Computer Science, Aberystwyth University, UK","2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)","","2015","","","1","7","This paper presents a novel application of interpolation in supporting fuzzy image classification. The recently introduced Deep Spatio-Temporal Inference Network (DeSTIN) is employed to carry out limited original feature extraction. A simple but effective linear interpolation is then used to artificially increase the dimensionality of the extracted feature sets for accurate classification, without incurring heavy computational cost. In particular, Fuzzy-Rough Nearest Neighbour (FRNN) and Fuzzy Ownership Nearest Neighbour (FRNN-O) are each utilised for image classification. The work is tested against the popular MNIST dataset of handwritten digits [1]. Experimental results indicate that the proposed approach is highly promising.","","","10.1109/FUZZ-IEEE.2015.7337903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7337903","","Feature extraction;Interpolation;Time complexity;Computer architecture;Tin;Computer science;Machine learning","feature extraction;fuzzy set theory;image classification;interpolation;rough set theory","linear interpolation aided fuzzy image classification;deep spatio-temporal inference network;DeSTIN;feature extraction;fuzzy-rough nearest neighbour;fuzzy ownership nearest neighbour;FRNN-O;MNIST;handwritten digits","","","18","","","","","IEEE","IEEE Conferences"
"Facial landmark detection via pose-induced auto-encoder networks","Y. Chen; W. Luo; J. Yang","School of Computer Science and Engineering, Nanjing University of Science and Technology; School of Computer Science and Engineering, Nanjing University of Science and Technology; School of Computer Science and Engineering, Nanjing University of Science and Technology","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","2115","2119","Facial landmark detection is an important issue in a face recognition system. However, human faces in wild conditions often present large variations in shape due to different poses, occlusions or expressions, which makes it a difficult task. Instead of learning the same map for all images, we propose a Pose-Induced Auto-encoder Networks (PIAN) approach which uses different pose-induced networks for landmark estimation under different pose conditions. Firstly, we build a network to simultaneously get the initial landmark and pose estimation. Then, different networks which are induced by the estimated pose are built for local search where a component-based searching method is explored. By using the pose inducing strategy, the initial estimation is reliable and it helps reduce the variations in local patches. This makes the component-based search feasible and more accurate than previous searching methods. Experiments show that our method outperforms the state-of-the-art algorithms especially in terms of fine estimation of landmarks.","","","10.1109/ICIP.2015.7351174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351174","Facial landmark;Pose-induced;Deep Auto-encoder","Face;Shape;Databases;Training;Neural networks","estimation theory;face recognition;object detection;pose estimation","facial landmark detection;pose-induced auto-encoder networks;face recognition system;occlusions;PIAN approach;landmark estimation;pose estimation;component-based searching;pose inducing strategy","","2","26","","","","","IEEE","IEEE Conferences"
"Landing performance simulation of an asteroid landing mechanism","Z. Zhao; D. Li; B. Yuan; S. Gao; J. Zhao","Beijing Key Laboratory of Intelligent Space Robotic Systems, Technology and Applications, Beijing Institute of Spacecraft System Engineering, 100092, China; Beijing Key Laboratory of Intelligent Space Robotic Systems, Technology and Applications, Beijing Institute of Spacecraft System Engineering, 100092, China; Beijing Key Laboratory of Intelligent Space Robotic Systems, Technology and Applications, Beijing Institute of Spacecraft System Engineering, 100092, China; Beijing Key Laboratory of Intelligent Space Robotic Systems, Technology and Applications, Beijing Institute of Spacecraft System Engineering, 100092, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, 150080, China","2015 IEEE International Conference on Information and Automation","","2015","","","1914","1919","Asteroid landing is a new research field in deep space exploration, which has important scientific and economic values in learning the origin of the solar system and exploring the asteroid. It is microgravity and little media properties are obtained on the asteroid surface, thus good landing performance is demanded to realize safe landing of the landing mechanism. Landing mechanism in the paper has three legs, and the damping device is located between the landing gear and the equipment base. Simulation method is introduced to analyze the landing performance of the landing mechanism. Landing performance of the landing mechanism on 30° landing slope with the largest landing velocity is simulated in three typical landing modes, and the landing performance is reflected by stability time of the landing gear and overloading acceleration of the equipment base. Landing simulation has important significance in learning landing performance and guiding the landing of the landing mechanism.","","","10.1109/ICInfA.2015.7279601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279601","Asteroid;landing mechanism;landing performance;simulation","Force;Damping;Acceleration;Gears;Loading;Stability analysis;Mathematical model","aerospace safety;aerospace simulation;asteroids;damping;entry, descent and landing (spacecraft);gears;solar system;stability","asteroid landing mechanism;landing performance simulation;deep space exploration;solar system;microgravity;asteroid surface;damping device;landing gear;landing velocity simulation;stability","","","13","","","","","IEEE","IEEE Conferences"
"DexROV: Enabling effective dexterous ROV operations in presence of communication latency","J. Gancet; D. Urbina; P. Letier; M. Ilzkovitz; P. Weiss; F. Gauch; B. Chemisky; G. Antonelli; G. Casalino; G. Indiveri; A. Birk; M. F. Pfingsthorn; S. Calinon; A. Turetta; C. Walen; L. Guilpain","Space Applications Services NV., Zaventem, Belgium; Space Applications Services NV., Zaventem, Belgium; Space Applications Services NV., Zaventem, Belgium; Space Applications Services NV., Zaventem, Belgium; Comex SA., Marseille, France; Comex SA., Marseille, France; Comex SA., Marseille, France; Interuniversity Center of Integrated Systems for the Marine Environment (ISME) Genoa, Italy; Interuniversity Center of Integrated Systems for the Marine Environment (ISME) Genoa, Italy; Interuniversity Center of Integrated Systems for the Marine Environment (ISME) Genoa, Italy; Jacobs University Bremen, Bremen, Germany; Jacobs University Bremen, Bremen, Germany; Idiap Research Institute, Martigny, Switzerland; Graal Tech SRL, Genova, Italy; EJR-Quartz BV, Leiden, Netherlands; EJR-Quartz BV, Leiden, Netherlands","OCEANS 2015 - Genova","","2015","","","1","6","Subsea interventions in the oil & gas industry as well as in other domains such as archaeology or geological surveys are demanding and costly activities for which robotic solutions are often deployed in addition or in substitution to human divers - contributing to risks and costs cutting. The operation of ROVs (Remotely Operated Vehicles) nevertheless requires significant off-shore dedicated manpower to handle and operate the robotic platform and the supporting vessel. In order to reduce the footprint of operations, DexROV proposes to implement and evaluate novel operation paradigms with safer, more cost effective and time efficient ROV operations. As a keystone of the proposed approach, manned support will in a large extent be delocalized within an onshore ROV control center, possibly at a large distance from the actual operations, relying on satellite communications. The proposed scheme also makes provision for advanced dexterous manipulation and semi-autonomous capabilities, leveraging human expertise when deemed useful. The outcomes of the project will be integrated and evaluated in a series of tests and evaluation campaigns, culminating with a realistic deep sea (1,300 meters) trial in the Mediterranean sea.","","","10.1109/OCEANS-Genova.2015.7271691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271691","ROV;long range teleoperation;communication latencies;force feedback;real time simulation;machine learning;3D perception;3D modelling;autonomy;dexterous manipulation","Force feedback;Three-dimensional displays;Navigation;Exoskeletons;Manipulators;Sensors;Robustness","dexterous manipulators;oceanographic techniques;remotely operated vehicles","DexROV;effective dexterous ROV operations;communication latency;remotely operated vehicles;satellite communications;realistic deep sea trial;Mediterranean sea","","5","29","","","","","IEEE","IEEE Conferences"
"Gene expression profiles based Human cancer diseases classification","H. Salem; G. Attiya; N. El-Fishawy","Communications & Computer Department, Faculty of Engineering, Delta University, Mansoura, Egypt; Computer Science & Engineering Department, Faculty of Electronic Engineering, Menoufia University, Menouf, Egypt; Computer Science & Engineering Department, Faculty of Electronic Engineering, Menoufia University, Menouf, Egypt","2015 11th International Computer Engineering Conference (ICENCO)","","2015","","","181","187","Cancers are a large family of diseases that involve abnormal cell growth with the potential to spread to other parts of the body. A cancer disease in any of its forms represents a major cause of death worldwide. In cancer diagnosis, classification of different tumor types is of the greatest significance. Accuracy for prediction of various tumor types gives better treatment and minimization of toxicity on patients. Accordingly, creating methodologies that can effectively differentiate between cancer subtypes is essential. This paper presents a new methodology to classify Human cancer diseases based on the gene expression profiles. The proposed methodology combines both Information gain (IG) and Deep Genetic Algorithm (DGA). It first uses IG for feature selection, then uses Genetic Algorithm (GA) for feature reduction and finally uses Genetic Programming (GP) for cancer types' classification. The proposed system is evaluated by classifying cancer diseases in seven cancer datasets and the results are compared with most recent approaches.","","","10.1109/ICENCO.2015.7416345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7416345","Cancer Diagnosis/Classification;Decision Support System;Gene Expression;Genetic Algorithm;Feature Selection;Information Gain;Machine Learning","Tumors;Lungs;Cancer;Colon;Diseases;Morphology","cancer;cellular biophysics;feature selection;genetic algorithms;genetics;medical computing;patient treatment;pattern classification","gene expression profiles;human cancer diseases classification;abnormal cell growth;cancer diagnosis;tumor type classification;patient treatment;toxicity minimization;information gain;deep genetic algorithm;DGA;feature selection;feature reduction;genetic programming;cancer types classification","","4","43","","","","","IEEE","IEEE Conferences"
"Automatic script identification in the wild","B. Shi; C. Yao; C. Zhang; X. Guo; F. Huang; X. Bai","School of EIC, Huazhong University of Science and Technology, Wuhan, China 430074; School of EIC, Huazhong University of Science and Technology, Wuhan, China 430074; School of EIC, Huazhong University of Science and Technology, Wuhan, China 430074; Tecent, Shanghai, China 200233; Tecent, Shanghai, China 200233; School of EIC, Huazhong University of Science and Technology, Wuhan, China 430074","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","531","535","With the rapid increase of transnational communication and cooperation, people frequently encounter multilingual scenarios in various situations. In this paper, we are concerned with a relatively new problem: script identification at word or line levels in natural scenes. A large-scale dataset with a great quantity of natural images and 10 types of widely-used languages is constructed and released. In allusion to the challenges in script identification in real-world scenarios, a deep learning based algorithm is proposed. The experiments on the proposed dataset demonstrate that our algorithm achieves superior performance, compared with conventional image classification or script identification methods, including as the original CNN architecture, LLC and GLCM.","","","10.1109/ICDAR.2015.7333818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333818","","Support vector machines;Correlation","image classification;recurrent neural nets;text detection","automatic script identification;natural scenes;deep learning based algorithm;image classification;CNN architecture;LLC;GLCM;convolutional neural network;text images;locality-constrained linear coding;gray-level co-occurrence matrix","","12","31","","","","","IEEE","IEEE Conferences"
"Memory access patterns: the missing piece of the multi-GPU puzzle","T. Ben-Nun; E. Levy; A. Barak; E. Rubin","Dept. of Comput. Sci., Hebrew Univ. of Jerusalem, Jerusalem, Israel; Dept. of Comput. Sci., Hebrew Univ. of Jerusalem, Jerusalem, Israel; Dept. of Comput. Sci., Hebrew Univ. of Jerusalem, Jerusalem, Israel; Dept. of Comput. Sci., Hebrew Univ. of Jerusalem, Jerusalem, Israel","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","","2015","","","1","12","With the increased popularity of multi-GPU nodes in modern HPC clusters, it is imperative to develop matching programming paradigms for their efficient utilization. In order to take advantage of the local GPUs and the low-latency high-throughput interconnects that link them, programmers need to meticulously adapt parallel applications with respect to load balancing, boundary conditions and device synchronization. This paper presents MAPS-Multi, an automatic multi-GPU partitioning framework that distributes the workload based on the underlying memory access patterns. The framework consists of host- and device-level APIs that allow programs to efficiently run on a variety of GPU and multi-GPU architectures. The framework implements several layers of code optimization, device abstraction, and automatic inference of inter-GPU memory exchanges. The paper demonstrates that the performance of MAPS-Multi achieves near-linear scaling on fundamental computational operations, as well as real-world applications in deep learning and multivariate analysis.","","","10.1145/2807591.2807611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832793","","Instruction sets;Graphics processing units;Kernel;Programming;Containers;Memory management;Optimization","application program interfaces;file organisation;graphics processing units;parallel processing;pattern matching;resource allocation;synchronisation","memory access patterns;multiGPU puzzle;modern HPC clusters;matching programming paradigms;low latency high throughput;parallel applications;load balancing;boundary conditions;device synchronization;automatic multiGPU partitioning;MAPS-Multi;host level API;device level API;multiGPU architectures;code optimization;device abstraction;automatic inference;deep learning;multivariate analysis","","6","32","","","","","IEEE","IEEE Conferences"
"Improving concurrency and asynchrony in multithreaded MPI applications using software offloading","K. Vaidyanathan; D. D. Kalamkar; K. Pamnany; J. R. Hammond; P. Balaji; D. Das; J. Park; B. Joó","NA; NA; NA; NA; NA; NA; NA; NA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","","2015","","","1","12","We present a new approach for multithreaded communication and asynchronous progress in MPI applications, wherein we offload communication processing to a dedicated thread. The central premise is that given the rapidly increasing core counts on modern systems, the improvements in MPI performance arising from dedicating a thread to drive communication outweigh the small loss of resources for application computation, particularly when overlap of communication and computation can be exploited. Our approach allows application threads to make MPI calls concurrently, enqueuing these as communication tasks to be processed by a dedicated communication thread. This not only guarantees progress for such communication operations, but also reduces load imbalance. Our implementation additionally significantly reduces the overhead of mutual exclusion seen in existing implementations for applications using MPI_THREAD_MULTIPLE. Our technique requires no modification to the application, and we demonstrate significant performance improvement (up to 2X) for QCD, 1-D FFT and deep learning CNN applications.","","","10.1145/2807591.2807602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832804","","Instruction sets;Message systems;Parallel processing;Data transfer;Government;Standards","application program interfaces;concurrency control;message passing;multi-threading;resource allocation","concurrency;asynchrony;multithreaded MPI application;software offloading;multithreaded communication;asynchronous progress;communication processing;MPI performance;concurrent MPI calls;communication operation;load imbalance reduction;mutual exclusion overhead;MPI_THREAD_MULTIPLE;QCD;1D FFT;deep learning CNN application","","8","35","","","","","IEEE","IEEE Conferences"
"Detection of exudates in fundus photographs using convolutional neural networks","P. Prentašić; S. Lončarić","University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, Image Processing Group 10000, Croatia; University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, Image Processing Group 10000, Croatia","2015 9th International Symposium on Image and Signal Processing and Analysis (ISPA)","","2015","","","188","192","Diabetic retinopathy is one of the leading causes of preventable blindness in the developed world. Early diagnosis of diabetic retinopathy enables timely treatment and in order to achieve it a major effort will have to be invested into screening programs and especially into automated screening programs. Detection of exudates is very important for early diagnosis of diabetic retinopathy. Deep neural networks have proven to be a very promising machine learning technique, and have shown excellent results in different compute vision problems. In this paper we show that convolutional neural networks can be effectively used in order to detect exudates in color fundus photographs.","","","10.1109/ISPA.2015.7306056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306056","","Diabetes;Optical imaging;Convolution;Retinopathy;Optical sensors;Retina;Neural networks","biomedical optical imaging;colour photography;diseases;eye;image colour analysis;medical image processing;neural nets;object detection","exudate detection;convolutional neural networks;diabetic retinopathy;preventable blindness;early diagnosis;deep neural networks;machine learning technique;color fundus photographs","","11","31","","","","","IEEE","IEEE Conferences"
"Full-rank linear-chain NeuroCRF for sequence labeling","M. Rondeau; Y. Su","McGill University, Canada; Nuance Communications, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","5281","5285","Inspired by the success of deep neural network-hidden Markov model (DNN-HMM) in acoustic modeling for automatic speech recognition, a number of researchers from various fields have independently proposed the idea of combining DNN and conditional random fields (CRFs). Despite their subtle differences, this class of models is collectively referred to as “NeuroCRF” in this paper. We focus our attention on applying a linear-chain NeuroCRF to the fundamental and ubiquitous problem of sequence labeling in natural language processing with distributed word representations. We question the necessity of previous works' use of the neural network to learn a low-rank emission feature matrix, added to a transition feature matrix. By modeling a full-rank feature matrix directly, we show that statistically significant gains can be achieved on the CoNLL-2000 syntactic chunking task, without harming performance on tasks with low dependencies between consecutive labels, such as the CoNLL-2003 named entity recognition task.","","","10.1109/ICASSP.2015.7178979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178979","Neural networks;spoken language understanding;conditional random fields","Training;Artificial neural networks;Hidden Markov models;Labeling;Syntactics;Feature extraction","hidden Markov models;natural language processing;neural nets;speech recognition","sequence labeling;full-rank linear-chain NeuroCRF;conditional random fields;deep neural network;hidden Markov model;DNN-HMM;acoustic modeling;automatic speech recognition;ubiquitous problem;natural language processing;distributed word representations;low-rank emission feature matrix;transition feature matrix;CoNLL-2000 syntactic chunking task;entity recognition task","","1","16","","","","","IEEE","IEEE Conferences"
"Illumination invariant face recognition using convolutional neural networks","N. P. Ramaiah; E. P. Ijjina; C. K. Mohan","Department of Computer Science and Engineering, Indian Institute of Technology Hyderabad, Andhra Pradesh, India 502205; Department of Computer Science and Engineering, Indian Institute of Technology Hyderabad, Andhra Pradesh, India 502205; Department of Computer Science and Engineering, Indian Institute of Technology Hyderabad, Andhra Pradesh, India 502205","2015 IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems (SPICES)","","2015","","","1","4","Face is one of the most widely used biometric in security systems. Despite its wide usage, face recognition is not a fully solved problem due to the challenges associated with varying illumination conditions and pose. In this paper, we address the problem of face recognition under non-uniform illumination using deep convolutional neural networks (CNN). The ability of a CNN to learn local patterns from data is used for facial recognition. The symmetry of facial information is exploited to improve the performance of the system by considering the horizontal reflections of the facial images. Experiments conducted on Yale facial image dataset demonstrate the efficacy of the proposed approach.","","","10.1109/SPICES.2015.7091490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091490","biometrics;facial recognition;convolutional neural networks;non-uniform illumination","Face recognition;Lighting;Face;Neural networks;Training;Pattern analysis","biometrics (access control);face recognition;neural nets;security","Yale facial image dataset;horizontal reflections;CNN;deep convolutional neural networks;nonuniform illumination;security systems;biometric;illumination invariant face recognition","","10","22","","","","","IEEE","IEEE Conferences"
"Scotopic Visual Recognition","B. Chen; P. Perona","NA; NA","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","659","662","Recognition from a small number of photons is important for biomedical imaging, security, astronomy and many other fields. We develop a framework that allows a machine to classify objects as quickly as possible, hence requiring as few photons as possible, while maintaining the error rate below an acceptable threshold. The framework also allows for a dynamic speed versus accuracy tradeoff. Given a generative model of the scene, the optimal tradeoff can be obtained from a self-recurrent deep neural network. The generative model may also be learned from the data. We find that MNIST classification performance from less than 1 photon per pixel is comparable to that obtained from images in normal lighting conditions. Classification on CIFAR10 requires 10 photon per pixel to stay within 1% the normal-light performance.","","","10.1109/ICCVW.2015.88","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406437","","Photonics;Visualization;Computational modeling;Imaging;Error analysis;Time factors;Erbium","image classification;object recognition;recurrent neural nets","scotopic visual recognition;photons;object classification;self-recurrent deep neural network;MNIST classification;lighting condition","","1","18","","","","","IEEE","IEEE Conferences"
"Content linking for UGC based on Word Embedding model","Zhiqiao Gao; Lei Li; Liyuan Mao; Dezhu He; Chao Xue","Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China","2015 11th International Conference on Heterogeneous Networking for Quality, Reliability, Security and Robustness (QSHINE)","","2015","","","149","154","There are huge amounts of User Generated Contents (UGCs) consisting of authors' articles of different themes and readers' comments in social networks every day. Generally, an article often gives rise to thousands of readers' comments, which are related to specific points of the originally published article or previous comments. Hence it has suggested the urgent need for automated methods to implement the content linking task, which can also help other related applications, such as information retrieval, summarization and content management. So far content linking is still a relatively new issue. Because of the unsatisfactory of traditional ways based on feature extraction, we look forward to using deeper textual semantic analysis. The Word Embedding model based on deep learning has performed well in Natural Language Processing (NLP), especially in mining deep semantic information recently. Therefore, we study further on the Word Embedding model trained by different neural network models from which we can learn the structure, principles and training ways of the neural network based language models in more depth to complete deep semantic feature extraction. With the aid of the semantic features, we expect to put forward a new method for content linking between comments and their original articles in social networks, and finally verify the validity of the proposed method through experiments and comparison with traditional ways based on feature extraction.","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332559","Content Linking;Word Embedding model;UGC","Irrigation;Joining processes;Mathematical model;Cities and towns;Semantics;Roads","feature extraction;natural language processing;social networking (online)","content linking;word embedding model;user generated contents;UGC;social networks;automated methods;content management;information retrieval;feature extraction;natural language processing;NLP;semantic information;neural network models;language models;deep semantic feature extraction","","","12","","","","","IEEE","IEEE Conferences"
"Environmental sound classification with convolutional neural networks","K. J. Piczak","Institute of Electronic Systems, Warsaw University of Technology","2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)","","2015","","","1","6","This paper evaluates the potential of convolutional neural networks in classifying short audio clips of environmental sounds. A deep model consisting of 2 convolutional layers with max-pooling and 2 fully connected layers is trained on a low level representation of audio data (segmented spectrograms) with deltas. The accuracy of the network is evaluated on 3 public datasets of environmental and urban recordings. The model outperforms baseline implementations relying on mel-frequency cepstral coefficients and achieves results comparable to other state-of-the-art approaches.","","","10.1109/MLSP.2015.7324337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324337","environmental sound;convolutional neural networks;classification","Neural networks;Training;Accuracy;Convolution;Convolutional codes;Yttrium;Pattern recognition","audio signal processing;cepstral analysis;neural nets;signal classification","environmental sound classification;convolutional neural network;audio clip;convolutional layer;max-pooling;low level representation;audio data;segmented spectrogram;public dataset;environmental recording;urban recording;baseline implementation;mel-frequency cepstral coefficient","","136","39","","","","","IEEE","IEEE Conferences"
"DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving","C. Chen; A. Seff; A. Kornhauser; J. Xiao","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2722","2730","Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments. We also train a model for car distance estimation on the KITTI dataset. Results show that our direct perception approach can generalize well to real driving images. Source code and data are available on our project website.","","","10.1109/ICCV.2015.312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410669","","Roads;Games;Automobiles;Training;Neural networks;Robots;Testing","computer games;computer vision;neural nets;road traffic;traffic engineering computing;virtual reality","KITTI dataset;car distance estimation;virtual environments;video game;convolutional neural network;direct perception representation;behavior reflex;traffic state;road state;mediated perception approaches;vision-based autonomous driving systems","","273","22","","","","","IEEE","IEEE Conferences"
"Effective insect recognition using a stacked autoencoder with maximum correntropy criterion","Yu Qi; G. T. Cinar; V. M. A. Souza; G. E. A. P. A. Batista; Yueming Wang; J. C. Principe","Qiushi Academy for Advanced Studies, Zhejiang University, Hangzhou, China; Computational NeuroEngineering Laboratory, University of Florida, Gainesville, USA; Institute of Mathematics and Computer Science, University of São Paulo, São Carlos, Brazil; Institute of Mathematics and Computer Science, University of São Paulo, São Carlos, Brazil; Qiushi Academy for Advanced Studies, Zhejiang University, Hangzhou, China; Computational NeuroEngineering Laboratory, University of Florida, Gainesville, USA","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","7","Throughout the history, insects had been intimately connected to humanity, in both positive and negative ways. Insects play an important part in crop pollination, on the other hand, some of them spread diseases that kill millions of people every year. Effective control of harmful insects while having little impact to beneficial insects and environment is extremely important. Recently, an intelligent trap that uses laser sensors was presented to control the population of target insects. The device could record and analyze sensor signals when an insect passes through the trap and make quick decisions whether to catch it or not. The effectiveness of the trap relies on the correct choice of classification algorithm to perform the insect detection. In this paper, we propose to use a deep neural network with maximum correntropy criterion (MCC) for reliable classification of insects in real-time. Experimental results show that, deep networks are effective for learning stable features from brief insect passage signals. By replacing the mean square error cost with MCC, the robustness of autoencoders against noise is improved significantly and robust features could be learned. The method is tested on five species of insects and a total of 5325 passages. High classification accuracy of 92.1% is achieved. Compared with previously applied methods, better classification performance is obtained using only 10% of the computation time. Therefore, our method is efficient and reliable for online insect detection.","","","10.1109/IJCNN.2015.7280418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280418","","Robustness;Insects;Optimization","biology computing;neural nets;signal classification","insect recognition;stacked autoencoder;deep neural network;maximum correntropy criterion;insect classification;MCC","","1","14","","","","","IEEE","IEEE Conferences"
"An analysis of convolutional neural networks for speech recognition","J. Huang; J. Li; Y. Gong","Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","4989","4993","Despite the fact that several sites have reported the effectiveness of convolutional neural networks (CNNs) on some tasks, there is no deep analysis regarding why CNNs perform well and in which case we should see CNNs' advantage. In the light of this, this paper aims to provide some detailed analysis of CNNs. By visualizing the localized filters learned in the convolutional layer, we show that edge detectors in varying directions can be automatically learned. We then identify four domains we think CNNs can consistently provide advantages over fully-connected deep neural networks (DNNs): channel-mismatched training-test conditions, noise robustness, distant speech recognition, and low-footprint models. For distant speech recognition, a CNN trained on 1000 hours of Kinect distant speech data obtains relative 4% word error rate reduction (WERR) over a DNN of a similar size. To our knowledge, this is the largest corpus so far reported in the literature for CNNs to show its effectiveness. Lastly, we establish that the CNN structure combined with maxout units is the most effective model under small-sizing constraints for the purpose of deploying small-footprint models to devices. This setup gives relative 9.3% WERR from DNNs with sigmoid units.","","","10.1109/ICASSP.2015.7178920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178920","Convolutional neural networks;DNN;low footprint models;maxout units","Speech recognition;Hidden Markov models;Speech;Neural networks;Convolution;Feature extraction;Training data","convolution;edge detection;filters;neural nets;speech recognition","convolutional neural networks;CNN;localized filters;edge detectors;deep neural networks;DNN;channel-mismatched training-test conditions;noise robustness;distant speech recognition;low-footprint models;Kinect distant speech data;word error rate reduction;WERR;sigmoid units;time 1000 hour","","29","24","","","","","IEEE","IEEE Conferences"
"FaceNet: A unified embedding for face recognition and clustering","F. Schroff; D. Kalenichenko; J. Philbin","Google Inc., USA; Google Inc., USA; Google Inc., USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","815","823","Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure offace similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings asfeature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-artface recognition performance using only 128-bytes perface. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result [15] by 30% on both datasets.","","","10.1109/CVPR.2015.7298682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298682","","Face;Face recognition;Training;Accuracy;Artificial neural networks;Standards;Principal component analysis","convolution;data mining;face recognition;image matching;neural nets;optimisation;pattern clustering","FaceNet embedding;face recognition;face clustering;deep convolutional network;embedding optimization;face patch matching;online triplet mining method","","1644","23","","","","","IEEE","IEEE Conferences"
"Empirical research on class environment evaluation","Huang Sui; Zhan Dong Hua","Dept. of Computer Science, Jinan University, Guangzhou, Guangdong Province, China; Zheng Xian Primary School, Zhuhai, Guangdong Province, China","2015 4th International Conference on Computer Science and Network Technology (ICCSNT)","","2015","01","","225","228","Researches have revealed the deep impact of class environment on student learning outcomes and proposed a serial of methods for survey and taxonomy. But these efforts suffer from two issues: (1) They emphasize the holistic measurement and evaluation of class environment, but ignore the individual effects in the class. (2) They do not consider interpersonal behaviors among the students and teachers. In this paper, we address these two challenges as follows. First, we introduce a half anonymous survey instrument to assist individual opinion data collection. Second, we adopt social network analysis to explore relationships among students. An empirical class test was conducted to verify the method. Experimental results have demonstrated the ability of our approach to discover the class environment problems with high reliability and veracity.","","","10.1109/ICCSNT.2015.7490741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490741","class environment;social network analysis;half anonymous survey;measurment and evaluation","Social network services;Psychology;Instruments;Statistical analysis;Computer science;Taxonomy;Analytical models","educational administrative data processing;social networking (online)","empirical research;class environment evaluation;student learning outcomes;holistic measurement;individual opinion data collection;social network analysis;empirical class test","","","8","","","","","IEEE","IEEE Conferences"
"Going deeper with convolutions","C. Szegedy; Wei Liu; Yangqing Jia; P. Sermanet; S. Reed; D. Anguelov; D. Erhan; V. Vanhoucke; A. Rabinovich","Google Inc., USA; University of North Carolina, Chapel Hill, USA; Google Inc., USA; Google Inc., USA; University of Michigan, Ann Arbor, USA; Google Inc., USA; Google Inc., USA; Google Inc., USA; Magic Leap Inc., USA","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","1","9","We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.","","","10.1109/CVPR.2015.7298594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298594","","Computer architecture;Convolutional codes;Sparse matrices;Neural networks;Visualization;Object detection;Computer vision","convolution;decision making;feature extraction;Hebbian learning;image classification;neural net architecture;resource allocation","convolutional neural network architecture;resource utilization;architectural decision;Hebbian principle;object classification;object detection","","6748","21","","","","","IEEE","IEEE Conferences"
"Classifying phonological categories in imagined and articulated speech","S. Zhao; F. Rudzicz","Department of Computer Science, University of Toronto, Canada; Department of Computer Science, University of Toronto, Canada","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","992","996","This paper presents a new dataset combining 3 modalities (EEG, facial, and audio) during imagined and vocalized phonemic and single-word prompts. We pre-process the EEG data, compute features for all 3 modalities, and perform binary classification of phonological categories using a combination of these modalities. For example, a deep-belief network obtains accuracies over 90% on identifying consonants, which is significantly more accurate than two baseline support vector machines. We also classify between the different states (resting, stimuli, active thinking) of the recording, achieving accuracies of 95%. These data may be used to learn multimodal relationships, and to develop silent-speech and brain-computer interfaces.","","","10.1109/ICASSP.2015.7178118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178118","Phonological categories;electroencephalography;speech articulation;deep-belief networks","Electroencephalography;Presses;Tin","brain-computer interfaces;electroencephalography;feature extraction;medical signal processing;signal classification;speech processing;support vector machines","EEG dataset;facial dataset;audio dataset;binary classification;phonological categories;support vector machines;silent-speech;brain-computer interfaces","","14","20","","","","","IEEE","IEEE Conferences"
"Audio super-resolution using concatenative resynthesis","M. I. Mandel; Y. S. Cho","Brooklyn College, CUNY, Computer & Information Science, Brooklyn, NY 11210; The Ohio State University, Computer Science & Engineering, Columbus, OH, 43210","2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)","","2015","","","1","5","This paper utilizes a recently introduced non-linear dictionary-based denoising system in another voice mapping task, that of transforming low-bandwidth, low-bitrate speech into high-bandwidth, high-quality speech. The system uses a deep neural network as a learned nonlinear comparison function to drive unit selection in a concatenative synthesizer based on clean recordings. This neural network is trained to predict whether a given clean audio segment from the dictionary could be transformed into a given segment of the degraded observation. Speaker-dependent experiments on the small-vocabulary CHiME2-GRID corpus show that this model is able to resynthesize high quality clean speech from degraded observations. Preliminary listening tests show that the system is able to improve subjective speech quality evaluations by up to 50 percentage points, while a similar system based on non-negative matrix factorization and trained on the same data produces no significant improvement.","","","10.1109/WASPAA.2015.7336890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336890","bandwidth expansion;concatenative synthesis;deep neural networks;nonparametric;speech","Speech;Dictionaries;Neural networks;Speech processing;Bandwidth;Packet loss","audio recording;audio signal processing;matrix decomposition;neural nets;signal denoising;speech processing","audio superresolution;concatenative resynthesis;nonlinear dictionary-based denoising system;voice mapping task;low-bitrate speech;concatenative synthesizer;audio segment;CHiME2-GRID corpus;speech quality evaluations;nonnegative matrix factorization;deeep neural network","","4","26","","","","","IEEE","IEEE Conferences"
"An Efficient Method for Document Categorization Based on Word2vec and Latent Semantic Analysis","R. Ju; P. Zhou; C. H. Li; L. Liu","NA; NA; NA; NA","2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing","","2015","","","2276","2283","Document categorization is the process of classifying documents from many mixed documents automatically, and the main problem is how to express document content in vector space completely. This paper proposes a new model named Latent Semantic Analysis (LSA) + word2vec to categorize documents. This is the first attempt of combining word2vec with LSA at document categorization and it can map document to vector space under the premise of keeping document contents fully. At first, we create a term by document matrix and the element of which is decided by Term Frequency-Inverse Document Frequency (TF-IDF) weighting and word vector trained by word2vec. This matrix is a 3-dimensional matrix and it can describe the meaning of every word and the content of every document exactly. Secondly, Singular Value Decomposition (SVD) is executed on the matrix and lower computational complexity is gained from this. The model is named LSA + word2vec. Then, document vector gained from the new model are put into Convolutional Neural Network (CNN) to train. CNN is an efficient deep learning algorithm, which improves the accuracy of classification greatly. We evaluate the performance based on the 20newsgroups corpus. The results show that our new model achieves better effects on document categorization tasks, and the accuracy made about 15% improvement than traditional methods, such as LSA and Vector Space Model (VSM).","","","10.1109/CIT/IUCC/DASC/PICOM.2015.336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363382","document categorization;word2vec;LSA;CNN","Computational modeling;Neural networks;Semantics;Electronic mail;Matrix decomposition;Feature extraction;Computational complexity","document handling;learning (artificial intelligence);neural nets;singular value decomposition","VSM;vector space model;CNN;convolutional neural network;document vector;SVD;singular value decomposition;TF-IDF;term frequency inverse document frequency;document matrix;document contents;LSA;vector space;document classification;latent semantic analysis;Word2vec;document categorization","","7","26","","","","","IEEE","IEEE Conferences"
"Computing education in K-12 schools: A review of the literature","V. Garneli; M. N. Giannakos; K. Chorianopoulos","Department of Informatics, Ionian University, Corfu, Greece; Department of Computer and Information Science, Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Department of Informatics, Ionian University, Corfu, Greece","2015 IEEE Global Engineering Education Conference (EDUCON)","","2015","","","543","551","During the last few years, the focus of computer science education (CSE) in primary and secondary schools (shortly K-12) have reached a significant turning point. This study reviews the published papers on the field of K-12 computing education in order to summarize the findings, guide future studies and give reflections for the major achievements in the area of CSE in K-12 schools. 47 peer-reviewed articles were collected from a systematic literature search and analyzed, based on a categorization of their main elements. Programming tools, educational context, and instructional methods are the main examined categories of this research. Results of this survey show the direction of CSE in schools research during the last years and summarized the benefits as well as the challenges. In particular, we analyzed the selected papers from the perspective of the various instructional methods aiming at introducing and enhancing learning, using several programming tools and educational context in K-12 CSE. Despite the challenges, the findings suggest that implementing computing lessons in K-12 education could be an enjoyable and effective learning experience. In addition, we suggest ways to facilitate deep learning and deal with various implications of the formal and informal education. Encouraging students to create their own projects or solve problems should be a significant part of the learning process.","","","10.1109/EDUCON.2015.7096023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7096023","computer science education;computer programming;programming pedagogy;educational context;programming tools;K-12 Education","Programming profession;Education;Context;Games;Visualization;Computer languages","computer aided instruction;computer science education;educational institutions","computer science education;primary schools;secondary schools;K-12 computing education;CSE;K-12 schools;programming tools;educational context;instructional methods;computing lesson implementation;formal education;informal education;learning process","","8","69","","","","","IEEE","IEEE Conferences"
"3D real-time tracking, following and imaging of white sharks with an Autonomous Underwater Vehicle","A. L. Kukulya; R. Stokey; R. Littlefield; F. Jaffre; E. M. H. Padilla; G. Skomal","Applied Ocean Physics & Engineering Dept., Woods Hole Oceanographic Institution, Woods Hole, MA; Applied Ocean Physics & Engineering Dept., Woods Hole Oceanographic Institution, Woods Hole, MA; Applied Ocean Physics & Engineering Dept., Woods Hole Oceanographic Institution, Woods Hole, MA; Applied Ocean Physics & Engineering Dept., Woods Hole Oceanographic Institution, Woods Hole, MA; Pelagios Kakunja A.C. La Paz, B.C.S., México; Massachusetts Marine Fisheries, New Bedford, MA","OCEANS 2015 - Genova","","2015","","","1","6","Little is known about deep-water predatory attacks and behavior of white sharks (Carcharodon carcharias). Revealing how ocean predators operate and forage within their environment is fundamental to the protection of the species and their ecosystem. It is very difficult to quantify habitat use and behavior of large marine animals that may range widely and are not easy to observe directly, such as sharks. Studies of shark foraging ecology and feeding behavior are extremely difficult as predation events are rarely witnessed. Also, these studies often cannot identify the habitat in which a shark feeds, making definition of critical habitats difficult [1]. Currently, satellite and acoustic tags are used to follow the migration of white sharks, however this method limits information acquisition about detailed behaviors therefore leaving gaps in scientists understanding of dynamic movement of marine animals. Recent developments in fine-scale spatial 3D tracking and imaging of large sharks with an Autonomous Underwater Vehicle (AUV) have given scientists a never before seen view into hunting and foraging behaviors of these animals in the wild [2]. While tracking pelagic predators is no longer a novel idea, improved imaging sensors and navigation capabilities continue to evolve thus making observations of behaviors in deep water even more possible. Significant improvements have been made in hardware and software capabilities from lessons learned while tracking basking sharks and white sharks in Cape Cod in 2012 using a specially modified Remote Environmental Monitoring UnitS REMUS AUV known as SharkCam developed in the Oceanographic Systems Lab (OSL) at the Woods Hole Oceanographic Institution (WHOI. The capabilities and field results from a second expedition taken place near Guadalupe Island, Mexico from November 2013 are presented in this paper. The REMUS SharkCam system consists of a 100-meter depth rated vehicle outfitted with a circular Ultra Short BaseLine (USBL) receiver array for omni-directional tracking of a tagged animal. The vehicle interrogates the tag, and the round trip travel time of the response is then used to determine the range to the animal. This response is then beam-formed to determine the bearing relative to the vehicle, and the vehicle's compass is used to transform this into an absolute bearing. From this, the location in earth coordinates (latitude/longitude) can be determined. A second response from the tag is time delayed proportional to depth. The time delta between the two responses is used to determine the depth of the animal. This combination allows precise location of the tagged animal in three-dimensional space never before possible from an underwater vehicle. In the 2012 field trials we conducted to track great white sharks off of Chatham, MA, it was found that maintaining a solid track on the shark proved to be challenging. There were two hypotheses as to why this was the case. First, when the shark towed the transponder, the orientation of the transponder changed from its normal upright orientation to horizontal, and the torroidal beam pattern of the bottom mounted transducer become vertical, not horizontal. The vehicle was in an acoustic dead zone. The second hypothesis was that when the shark moved quickly, the Doppler shift of the signal caused the beamformer signal match to fail. A significant improvement to real-time tracking was the development of a shipboard tracking system (STS) that enabled operators to track the tagged shark and the REMUS AUV separately, allowing for long term tracking of the animal if the AUV needed to be recovered due to battery depletion or mechanical faults. It also enabled operators to know how well the system was working in real time by providing depth, range and bearing back to the ship.","","","10.1109/OCEANS-Genova.2015.7271546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271546","SharkCam;A UV;Transponder;Tracking","Vehicles;Cameras;Animals;Transponders;Tracking;Real-time systems;Batteries","autonomous underwater vehicles;image processing;object tracking;oceanographic techniques","3D real-time tracking;white sharks;autonomous underwater vehicle;3D imaging;hunting;foraging behaviors;animals;navigation;deep water;Cape Cod;Remote Environmental Monitoring Units;SharkCam;Oceanographic Systems Lab","","1","11","","","","","IEEE","IEEE Conferences"
"Ocean observing/monitoring systems: Some past and present applications in Azores waters","M. R. Pinho; A. M. Martins; H. M. Silva","Department of Oceanography and Fisheries, University of the Azores, Horta, Azores, Portugal; Department of Oceanography and Fisheries, University of the Azores, Horta, Azores, Portugal; Department of Oceanography and Fisheries, University of the Azores, Horta, Azores, Portugal","2015 3rd Experiment International Conference (exp.at'15)","","2015","","","183","186","The Ocean is vast and of difficult access. Recent technological advances have brought us to a new era in ocean research (physical / biological / chemical / geological) one in which an Integrated network of Ocean Observing Systems (IOOS), involving strong developments in systems engineering and informatics, provides researchers with a continuous scientific presence in the ocean. These initiatives are worldwide accepted as vital tools for tracking, predicting, managing, and adapting to changes in Ocean and Coastal systems, by delivering data and information needed, so that decision-makers can take action to improve safety, enhance the economy, and protect the environment. Some of these IOOS systems incorporate broad themes identified by users and stakeholders such as: e.g. Marine Operations and Commerce, Coastal Hazards, Economic Development, Climate Variability and Change, Marine Forecasting, and Ecosystems, Fisheries and Water Quality, Public Outreach and Education, and more recently, Ocean Mining. The Azores (NE Atlantic) islands are located in an open-ocean area characterized by deep-sea ecosystems. Therefore, a regional IOOS can play a significant importance not only for science in general, but foremost to address regional/national/international needs/interests for ocean information, by gathering specific data on key coastal and ocean variables, while ensuring timely and sustained dissemination and availability of these data. However IOOS require heavy (and expensive) technology that, in some cases, is still in developing process in the Azores through experimentation. In this presentation we provide an overview of the multiple uses of ocean observing systems in the region. Rather than a comprehensive approach of this subject, a few examples are presented in relation to research carried out by the Department of Oceanography and Fisheries on different environments (open ocean, deep sea, coastal areas). In this presentation we will try to link the results of this research to learning processes in which the Internet is a common tool used.","","","10.1109/EXPAT.2015.7463262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7463262","Ocean;observing and monitoring;sensors and infrastructures;remote regions;Azores islands","Oceans;Sea measurements;Sensors;Meteorology;Aquaculture;Monitoring;Satellites","environmental monitoring (geophysics);oceanographic techniques","ocean observing-monitoring system;Azores water;ocean research;Integrated Network of Ocean Observing System;systems engineering;Ocean and Coastal system;decision maker;IOOS system;Marine Operations and Commerce;coastal hazard;economic development;Climate Variability and Change;marine forecasting;water quality;Public Outreach and Education;ocean mining;Azores island;deep-sea ecosystem;ocean information;Department of Oceanography and Fisheries","","","21","","","","","IEEE","IEEE Conferences"
"Recurrent convolutional neural networks for object-class segmentation of RGB-D video","M. S. Pavel; H. Schulz; S. Behnke","Universität Bonn, Computer Science Institute VI, Friedrich-Ebert-Allee 144, 53113, Germany; Universität Bonn, Computer Science Institute VI, Friedrich-Ebert-Allee 144, 53113, Germany; Universität Bonn, Computer Science Institute VI, Friedrich-Ebert-Allee 144, 53113, Germany","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","Object-class segmentation is a computer vision task which requires labeling each pixel of an image with the class of the object it belongs to. Deep convolutional neural networks (DNN) are able to learn and exploit local spatial correlations required for this task. They are, however, restricted by their small, fixed-sized filters, which limits their ability to learn long-range dependencies. Recurrent Neural Networks (RNN), on the other hand, do not suffer from this restriction. Their iterative interpretation allows them to model long-range dependencies by propagating activity. This property might be especially useful when labeling video sequences, where both spatial and temporal long-range dependencies occur. In this work, we propose novel RNN architectures for object-class segmentation. We investigate three ways to consider past and future context in the prediction process by comparing networks that process the frames one by one with networks that have access to the whole sequence. We evaluate our models on the challenging NYU Depth v2 dataset for object-class segmentation and obtain competitive results.","","","10.1109/IJCNN.2015.7280820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280820","","Computational modeling","correlation methods;image colour analysis;image segmentation;neural net architecture;recurrent neural nets;video signal processing","recurrent convolutional neural networks;RGB-D video;object-class segmentation;computer vision task;image pixel labeling;deep convolutional neural networks;DNN;local spatial correlations;fixed-sized filters;video sequences labeling;spatial long-range dependencies;temporal long-range dependencies;RNN architectures;prediction process;NYU Depth v2 dataset","","8","23","","","","","IEEE","IEEE Conferences"
"Phrase-based clause extraction for open information extraction system","A. Romadhony; D. H. Widyantoro; A. Purwarianti","School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia; School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia; School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia","2015 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","","2015","","","155","162","Recent development of variety and volume of information circulating in the Internet has prompted the emergence of a new paradigm in information extraction, namely the Open Information Extraction (Open IE). An evaluation of several existing Open IE systems shows a good performance on precision. However, improvement is still needed to boost the recall. A relation between entity pair in simple sentence is detected easier by the Open IE system rather than in complex sentence. In this paper, we propose a clause extraction approach employing phrase feature and requiring no learning, focusing on the entity pair. The proposed approach needs less computational cost than the previous work that employing deep parse feature or requiring learning. The experimental result shows that by extracting simpler clause, the performance of Open IE system increases. The average of best F-measure achieved in the evaluation on three benchmark datasets is 0.62, outperforms the previous work.","","","10.1109/ICACSIS.2015.7415184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415184","Open IE;entity-focused;text simplification;clause extraction","Feature extraction;Information retrieval;Data mining;Syntactics;Grammar;Internet;Semantics","Internet;text analysis","phrase-based clause extraction;open information extraction system;Internet;Open IE systems;phrase feature;deep parse feature;text simplication","","2","23","","","","","IEEE","IEEE Conferences"
"Arousal Recognition Using Audio-Visual Features and FMRI-Based Brain Response","J. Han; X. Ji; X. Hu; L. Guo; T. Liu","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; Cortical Architecture Imaging and Discovery Lab, Department of Computer Science, The University of Georgia, Athens, GA","IEEE Transactions on Affective Computing","","2015","6","4","337","347","As the indicator of emotion intensity, arousal is a significant clue for users to find their interested content. Hence, effective techniques for video arousal recognition are highly required. In this paper, we propose a novel framework for recognizing arousal levels by integrating low-level audio-visual features derived from video content and human brain's functional activity in response to videos measured by functional magnetic resonance imaging (fMRI). At first, a set of audio-visual features which have been demonstrated to be correlated with video arousal are extracted. Then, the fMRI-derived features that convey the brain activity of comprehending videos are extracted based on a number of brain regions of interests (ROIs) identified by a universal brain reference system. Finally, these two sets of features are integrated to learn a joint representation by using a multimodal deep Boltzmann machine (DBM). The learned joint representation can be utilized as the feature for training classifiers. Due to the fact that fMRI scanning is expensive and time-consuming, our DBM fusion model has the ability to predict the joint representation of the videos without fMRI scans. The experimental results on a video benchmark demonstrated the effectiveness of our framework and the superiority of integrated features.","","","10.1109/TAFFC.2015.2411280","National Science Foundation of China; National Science Foundation of China; NIH; US National Science Foundation; US National Science Foundation; US National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056522","Arousal recognition;affective computing;affective computing;fMRI-derived features;multimodal DBM;Arousal recognition;affective computing;fMRI-derived features;multimodal DBM","Feature extraction;Electroencephalography;Motion pictures;Emotion recognition;Brain models;Streaming media;Multimedia communication;Sentiment analysis;Behavioral science","biomedical MRI;Boltzmann machines;emotion recognition;feature extraction;video signal processing","arousal recognition;audio-visual features;FMRI-based brain response;emotion intensity;video content;human brain functional activity;functional magnetic resonance imaging;fMRI-derived feature extraction;brain region-of-interest;multimodal deep Boltzmann machine;DBM;video representation","","14","53","","","","","IEEE","IEEE Journals"
"Prediction gradients for feature extraction and analysis from convolutional neural networks","H. Z. Lo; J. P. Cohen; W. Ding","Department of Computer Science, University of Massachusetts Boston, Massachusetts, United States; Department of Computer Science, University of Massachusetts Boston, Massachusetts, United States; Department of Computer Science, University of Massachusetts Boston, Massachusetts, United States","2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)","","2015","1","","1","6","Despite their impact on computer vision and face recognition, the inner workings of deep convolutional neural networks (CNNs) have traditionally been regarded as uninterpretable. We demonstrate this to be false by proposing prediction gradients to understand how neural networks encode concepts into individual units. In constrast, existing efforts to understand convolutional nets focus on visualizing units and classes in pixel space, often using optimization. Our method for calculating prediction gradients is very efficient, and provides an effective technique to rank and quantify importance of internal units and their learned features based on the unit's relevance to any prediction. We use prediction gradients to analyse the features learned by a CNN on a standard face recognition data set. Our analysis identifies strong patterns of activation which are unique for each identity. In addition, we validate the rating produced by prediction gradients to remove the most important features of the network, knocking out their respective units in the network, and demonstrating detrimental effects on network prediction. Our experiments validate the utility of the prediction gradient in understanding the importance and relationships between units inside a convolutional neural network.","","","10.1109/FG.2015.7163154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163154","","Neural networks;Visualization;Face;Feature extraction;Face recognition;Training;Optimization","computer vision;data visualisation;face recognition;feature extraction;gradient methods;neural nets;optimisation","prediction gradients;feature extraction;computer vision;CNNs;deep convolutional neural networks;pixel space visualizing units;standard face recognition data set;network prediction","","1","16","","","","","IEEE","IEEE Conferences"
"Face Recognition Despite Wearing Glasses","A. Liang; C. S. N. Pathirage; C. Wang; W. Liu; L. Li; J. Duan","Dept. of Comput., Curtin Univ., Perth, WA, Australia; Dept. of Comput., Curtin Univ., Perth, WA, Australia; Dept. of Comput., Curtin Univ., Perth, WA, Australia; Dept. of Comput., Curtin Univ., Perth, WA, Australia; Dept. of Comput., Curtin Univ., Perth, WA, Australia; Sch. of Comput. Sci., Univ. of Nottingham, Nottingham, UK","2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA)","","2015","","","1","8","In this paper we address the challenge of performing face recognition on human faces that are wearing glasses. This is a common problem for face recognition and automatic identity checking at airports, as passengers frequently forget to remove their glasses when passing through customs. In order to solve this problem, we first propose an automatic glasses presence detection model based on the tree-pictorial-structured face detection model and such model can detect the presence of glasses and further assign landmarks on the rim, hinge, and bridge of the glasses on frontal faces. Experimental results show that the glasses detection rate is highly satisfactory for various face databases. Secondly, based on the landmarks on glasses, we apply the non-local colour total variation (CTV) inpainting approach in an attempt to remove the glasses; also, we apply the deep learning technique to further remove the traces of glasses and light reflection on lenses by regarding them as noises. Finally, experiments for face recognition after glasses removal are conducted by using some typical approaches and the results show that our glasses removal framework can improve face recognition accuracy significantly.","","","10.1109/DICTA.2015.7371260","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371260","","Glass;Face;Face recognition;Databases;Shape;Training;Machine learning","face recognition;feature extraction;image colour analysis;image restoration;iterative methods;optimisation;trees (mathematics)","face recognition;automatic identity checking;automatic glass presence detection model;tree-pictorial-structured face detection model;colour total variation;CTV inpainting approach;iterative optimization","","1","35","","","","","IEEE","IEEE Conferences"
"Word embedding based retrieval model for similar cases recommendation","Yifei Zhao; Jing Wang; Feiyue Wang","The State Key Laboratory of Management and Control for Complex Systems, CASIA, Beijing 100190, China; The State Key Laboratory of Management and Control for Complex Systems, CASIA, Beijing 100190, China; The State Key Laboratory of Management and Control for Complex Systems, CASIA, Beijing 100190, China","2015 Chinese Automation Congress (CAC)","","2015","","","2268","2272","Similar cases recommendation is more and more popular in the internet inquiry. There have been lots of cases which have been solved perfectly, and recommending them to similar inquiries can not only save the patients' waiting time, but also giving more good references. However, the inquiry platform cannot understand the diversity of description, i.e. the same meaning with different description. This may shield some cases with very high quality answers. In this paper, based on deep learning, we proposed a retrieval model combining word embedding with language models. We use word embedding to solve the problem of description diversity, and then recommend the similar cases for the inquiries. The experiments are based on the data from ask.39.net, and the results show that our methods outperform the state-of-art methods.","","","10.1109/CAC.2015.7382881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382881","internet inquiry;case recommendation;word embedding;data mining","Semantics;Machine learning;Estimation;Computational modeling;Internet;Medical services;Mathematical model","data mining;information retrieval;Internet;medical information systems","word embedding based retrieval model;similar case recommendation;Internet inquiry;patient waiting time;description diversity","","2","27","","","","","IEEE","IEEE Conferences"
"Contractive Rectifier Networks for Nonlinear Maximum Margin Classification","S. An; M. Hayat; S. H. Khan; M. Bennamoun; F. Boussaid; F. Sohel","Sch. of Comput. Sci. & Software Eng., Univ. of Western Australia, Crawley, WA, Australia; Sch. of Comput. Sci. & Software Eng., Univ. of Western Australia, Crawley, WA, Australia; Sch. of Comput. Sci. & Software Eng., Univ. of Western Australia, Crawley, WA, Australia; Sch. of Comput. Sci. & Software Eng., Univ. of Western Australia, Crawley, WA, Australia; Sch. of Electr. Electron. & Comput. Eng., Univ. of Western Australia, Crawley, WA, Australia; Sch. of Comput. Sci. & Software Eng., Univ. of Western Australia, Crawley, WA, Australia","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2515","2523","To find the optimal nonlinear separating boundary with maximum margin in the input data space, this paper proposes Contractive Rectifier Networks (CRNs), wherein the hidden-layer transformations are restricted to be contraction mappings. The contractive constraints ensure that the achieved separating margin in the input space is larger than or equal to the separating margin in the output layer. The training of the proposed CRNs is formulated as a linear support vector machine (SVM) in the output layer, combined with two or more contractive hidden layers. Effective algorithms have been proposed to address the optimization challenges arising from contraction constraints. Experimental results on MNIST, CIFAR-10, CIFAR-100 and MIT-67 datasets demonstrate that the proposed contractive rectifier networks consistently outperform their conventional unconstrained rectifier network counterparts.","","","10.1109/ICCV.2015.289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410646","","Support vector machines;Training;Neurons;Australia;Aerospace electronics;Nonlinear distortion","image classification;neural nets;nonlinear programming;object recognition;support vector machines","object classification;deep learning network;optimization;SVM;support vector machine;contraction mapping;hidden-layer transformation;nonlinear maximum margin classification;CRN;contractive rectifier network","","3","41","","","","","IEEE","IEEE Conferences"
"Disaggregation of appliances energy profile in households and data visualization of abnormal behavior","R. Vápeník; M. Kovalčík; P. Fecil'ak; F. Jakab","Department of Computer and Informatics, Faculty of Electrical Engineering and Informatics, Technical University of Košice, Košice, Slovakia; Department of Computer and Informatics, Faculty of Electrical Engineering and Informatics, Technical University of Košice, Košice, Slovakia; Department of Computer and Informatics, Faculty of Electrical Engineering and Informatics, Technical University of Košice, Košice, Slovakia; Department of Computer and Informatics, Faculty of Electrical Engineering and Informatics, Technical University of Košice, Košice, Slovakia","2015 13th International Conference on Emerging eLearning Technologies and Applications (ICETA)","","2015","","","1","6","Main goal of this paper is a system created for the purpose of disaggregation appliances from the energy profile in the households and the evaluation of unusual activities carried out in a smart home appliances with the help of self-organizing maps (SOM), or individual model views for educational purpose. The paper focuses on the analysis of existing methods and techniques of the problem. The core of the paper is the study of the various methods of disaggregation and describes the uses of the disaggregation of existing systems. After the deep investigation of issue, the next step is to select two different methods of disaggregation, which will be implemented to achieve the objectives of the paper. Second part of this paper is evaluation of activities can be achieved by calculation which is dependent on the models views, or by SOM, which is learned from standard activities. Evaluated by this unusual activity appliance. The system is controlled through a simple graphical user interface. This paper was created as learning model for using describe method to programming in various language.","","","10.1109/ICETA.2015.7558521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558521","","Home appliances;Neurons;Switches;Self-organizing feature maps;Monitoring;Sensors;Standards","data visualisation;domestic appliances;graphical user interfaces;home computing;power engineering computing;self-organising feature maps","learning model;graphical user interface;standard activities;educational purpose;individual model views;SOM;self-organizing maps;smart home appliances;households;energy profile;disaggregation appliances;abnormal behavior;data visualization","","","13","","","","","IEEE","IEEE Conferences"
"Scene text recognition with deeper convolutional neural networks","Y. Zhang; W. Wang; L. Wang; L. Wang","Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China; Fujitsu R&D Center Co., Ltd. China","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","2384","2388","Scene text recognition plays an important role in many applications such as video indexing and house number localization in maps. Recently, some feature learning methods have been proposed to handle this problem, which often exploit deep architectures with no more than 5 layers and relatively large receptive fields. Meanwhile, to avoid model overfitting, they generally take advantage of large amount of additional data. Inspired by the great success of GoogleLeNet with a deeper network and VGG networks with smaller receptive fields in the ImageNet competition, in this paper, we adopt a much deeper network with up to 15 layers and smaller receptive fields (3×3) to learn better features for scene text recognition. Particularly, even without additional training data, our model can achieve better performance. Experiments on scene text datasets (ICDAR 2003, SVT, Chars74K) demonstrate that our method achieves the state-of-the-art performance on character classification and competitive performance on cropped word recognition.","","","10.1109/ICIP.2015.7351229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351229","scene text recognition;convolutional neural networks;receptive field","Text recognition;Feature extraction;Training;Testing;Agriculture;Data models;Training data","neural nets;text detection","deeper convolutional neural networks;scene text recognition;video indexing;house number localization;feature learning methods;GoogleLeNet;VGG networks;ImageNet competition;scene text datasets;character classification;cropped word recognition","","1","22","","","","","IEEE","IEEE Conferences"
"Leveraging big data for grasp planning","D. Kappler; J. Bohg; S. Schaal","Autonomous Motion Department at the Max-Planck-Institute for Intelligent Systems, Tübingen, Germany; Autonomous Motion Department at the Max-Planck-Institute for Intelligent Systems, Tübingen, Germany; Autonomous Motion Department at the Max-Planck-Institute for Intelligent Systems, Tübingen, Germany","2015 IEEE International Conference on Robotics and Automation (ICRA)","","2015","","","4304","4311","We propose a new large-scale database containing grasps that are applied to a large set of objects from numerous categories. These grasps are generated in simulation and are annotated with different grasp stability metrics. We use a descriptive and efficient representation of the local object shape at which each grasp is applied. Given this data, we present a two-fold analysis: (i) We use crowdsourcing to analyze the correlation of the metrics with grasp success as predicted by humans. The results show that the metric based on physics simulation is a more consistent predictor for grasp success than the standard υ-metric. The results also support the hypothesis that human labels are not required for good ground truth grasp data. Instead the physics-metric can be used to generate datasets in simulation that may then be used to bootstrap learning in the real world. (ii) We apply a deep learning method and show that it can better leverage the large-scale database for prediction of grasp success compared to logistic regression. Furthermore, the results suggest that labels based on the physics-metric are less noisy than those from the υ-metric and therefore lead to a better classification performance.","","","10.1109/ICRA.2015.7139793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139793","","Databases;Robots;Shape;Three-dimensional displays;Stability analysis;Noise measurement","Big Data;control engineering computing;database management systems;grippers;planning (artificial intelligence)","Big Data;grasp planning;large-scale database;physics simulation;physics-metric;learning method;logistic regression","","64","38","","","","","IEEE","IEEE Conferences"
"Saliency Map Generation by the Convolutional Neural Network for Real-Time Traffic Light Detection Using Template Matching","V. John; K. Yoneda; Z. Liu; S. Mita","Intelligent Information Processing Laboratory, Toyota Technological Institute, Nagoya, Japan; Research Centre for Smart Vehicles, Toyota Technological Institute, Nagoya, Japan; Intelligent Information Processing Laboratory, Toyota Technological Institute, Nagoya, Japan; Research Centre for Smart Vehicles, Toyota Technological Institute, Nagoya, Japan","IEEE Transactions on Computational Imaging","","2015","1","3","159","173","A critical issue in autonomous vehicle navigation and advanced driver assistance systems (ADAS) is the accurate real-time detection of traffic lights. Typically, vision-based sensors are used to detect the traffic light. However, the detection of traffic lights using computer vision, image processing, and learning algorithms is not trivial. The challenges include appearance variations, illumination variations, and reduced appearance information in low illumination conditions. To address these challenges, we present a visual camera-based real-time traffic light detection algorithm, where we identify the spatially constrained region-of-interest in the image containing the traffic light. Given, the identified region-of-interest, we achieve high traffic light detection accuracy with few false positives, even in adverse environments. To perform robust traffic light detection in varying conditions with few false positives, the proposed algorithm consists of two steps, an offline saliency map generation and a real-time traffic light detection. In the offline step, a convolutional neural network, i.e., a deep learning framework, detects and recognizes the traffic lights in the image using region-of-interest information provided by an onboard GPS sensor. The detected traffic light information is then used to generate the saliency maps with a modified multidimensional density-based spatial clustering of applications with noise (M-DBSCAN) algorithm. The generated saliency maps are indexed using the vehicle GPS information. In the real-time step, traffic lights are detected by retrieving relevant saliency maps and performing template matching by using colour information. The proposed algorithm is validated with the datasets acquired in varying conditions and different countries, e.g., USA, Japan, and France. The experimental results report a high detection accuracy with negligible false positives under varied illumination conditions. More importantly, an average computational time of 10 ms/frame is achieved. A detailed parameter analysis is conducted and the observations are summarized and reported in this paper.","","","10.1109/TCI.2015.2480006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272062","Traffic Light Detection;Convolutional Neural Network;DBSCAN;Saliency Maps;Traffic Light Detection;Convolutional Neural Network;DBSCAN;Saliency Maps","Image color analysis;Vehicles;Clustering algorithms;Real-time systems;Imaging;Lighting;Accuracy","image colour analysis;image denoising;image matching;lighting;neural nets;object detection;road traffic;traffic engineering computing","saliency map generation;convolutional neural network;realtime traffic light detection;template matching;autonomous vehicle navigation;advanced driver assistance systems;ADAS;vision-based sensors;computer vision;image processing;learning algorithms;region-of-interest identification;M-DBSCAN algorithm;multidimensional density-based spatial clustering of applications with noise algorithm;vehicle GPS information;Global Positioning System;colour information;illumination condition","","21","50","","","","","IEEE","IEEE Journals"
"A study on effects of implicit and explicit language model information for DBLSTM-CTC based handwriting recognition","Q. Liu; L. Wang; Q. Huo","ACM Honored Class, Zhiyuan College, Shanghai Jiao Tong University, 200240, China; Microsoft Research Asia, Beijing 100080, China; Microsoft Research Asia, Beijing 100080, China","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","461","465","Deep Bidirectional Long Short-Term Memory (DBLSTM) with a Connectionist Temporal Classification (CTC) output layer has been established as one of the state-of-the-art solutions for handwriting recognition. It is well-known that the DBLSTM trained by using a CTC objective function will learn both local character image dependency for character modeling and long-range contextual dependency for implicit language modeling. In this paper, we study the effects of implicit and explicit language model information for DBLSTM-CTC based handwriting recognition by comparing the performance of using or without using an explicit language model in decoding. It is observed that even using one million lines of training sentences to train the DBLSTM, using an explicit language model is still helpful. To deal with such a large-scale training problem, a GPU-based training tool has been developed for CTC training of DBLSTM by using a mini-batch based epochwise Back Propagation Through Time (BPTT) algorithm.","","","10.1109/ICDAR.2015.7333804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333804","","Hidden Markov models;Training;Decoding;Smoothing methods;Databases;Handwriting recognition;Text recognition","backpropagation;handwriting recognition;handwritten character recognition;image classification;image coding","DBLSTM-CTC based handwriting recognition;implicit language model information;explicit language model information;deep bidirectional long short-term memory;connectionist temporal classification output layer;DBLSTM training;CTC objective function;local character image dependency;character modeling;long-range contextual dependency;training sentences;large-scale training problem;CTC training;GPU-based training tool;mini-batch based epoch wise back propagation through time algorithm","","12","35","","","","","IEEE","IEEE Conferences"
"A robust hierarchical detection method for scene text based on convolutional neural networks","Hailiang Xu; Feng Su","State Key Laboratory for Novel Software Technology, Nanjing University, 210023, China; State Key Laboratory for Novel Software Technology, Nanjing University, 210023, China","2015 IEEE International Conference on Multimedia and Expo (ICME)","","2015","","","1","6","Detecting the text in natural scene images is often challenging due to the complexity and variety of text's appearance and its interaction with the scene context. In this paper, we present a novel hierarchical text detection method exploiting textual characteristics at both character and text line scales for improved accuracy. First, seed candidate characters are detected with discriminative deep convolutional features learned within the maximally stable extremal regions extracted from the image, and are further grown to localize other degraded candidate characters. Next, as a finer filtering of text in the richer text line context, the random forest classifier is exploited on statistical features of text line characterizing the geometrical and conformability properties of constituent character components, to predict the text and non-text label. The effectiveness of the proposed method is demonstrated by the state-of-the-art results achieved on the public datasets.","","","10.1109/ICME.2015.7177494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177494","Text detection;convolutional neural network;MSER;natural scene image;boosting","Training;Feature extraction;Context;Accuracy;Protocols;Robustness;Image color analysis","edge detection;feature extraction;filtering theory;neural nets;statistical analysis;text detection","robust hierarchical detection method;scene text detection method;convolutional neural networks;natural scene images;text appearance;hierarchical text detection method;textual characteristics;text line scales;seed candidate character detection;discriminative deep convolutional features;extremal region extraction;text filtering;random forest classifier;statistical features;conformability property;geometrical property;constituent character components","","","19","","","","","IEEE","IEEE Conferences"
"Multi-task Recurrent Neural Network for Immediacy Prediction","X. Chu; W. Ouyang; W. Yang; X. Wang","Dept. of Electron. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Electron. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Electron. Eng., Chinese Univ. of Hong Kong, Hong Kong, China; Dept. of Electron. Eng., Chinese Univ. of Hong Kong, Hong Kong, China","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","3352","3360","In this paper, we propose to predict immediacy for interacting persons from still images. A complete immediacy set includes interactions, relative distance, body leaning direction and standing orientation. These measures are found to be related to the attitude, social relationship, social interaction, action, nationality, and religion of the communicators. A large-scale dataset with 10,000 images is constructed, in which all the immediacy measures and the human poses are annotated. We propose a rich set of immediacy representations that help to predict immediacy from imperfect 1-person and 2-person pose estimation results. A multi-task deep recurrent neural network is constructed to take the proposed rich immediacy representation as input and learn the complex relationship among immediacy predictions multiple steps of refinement. The effectiveness of the proposed approach is proved through extensive experiments on the large scale dataset.","","","10.1109/ICCV.2015.383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410740","","Shoulder;Feature extraction;Recurrent neural networks;Correlation;Videos","data analysis;pose estimation;recurrent neural nets","immediacy prediction;still images;interactions;relative distance;body leaning direction;standing orientation;attitude;social relationship;social interaction;action;nationality;religion;large-scale dataset;imperfect 1-person pose estimation;2-person pose estimation;multitask deep recurrent neural network","","16","41","","","","","IEEE","IEEE Conferences"
"Gibbs sampling with low-power spiking digital neurons","S. Das; B. U. Pedroni; P. Merolla; J. Arthur; A. S. Cassidy; B. L. Jackson; D. Modha; G. Cauwenberghs; K. Kreutz-Delgado","ECE, UC San Diego, La Jolla, CA 92093; BioEng., UC San Diego, La Jolla, CA 92093; IBM Research Almaden, San Jose, CA 95120; IBM Research Almaden, San Jose, CA 95120; IBM Research Almaden, San Jose, CA 95120; IBM Research Almaden, San Jose, CA 95120; IBM Research Almaden, San Jose, CA 95120; BioEng., UC San Diego, La Jolla, CA 92093; ECE, UC San Diego, La Jolla, CA 92093","2015 IEEE International Symposium on Circuits and Systems (ISCAS)","","2015","","","2704","2707","Restricted Boltzmann Machines and Deep Belief Networks have been successfully used in a wide variety of applications including image classification and speech recognition. Inference and learning in these algorithms uses a Markov Chain Monte Carlo procedure called Gibbs sampling. A sigmoidal function forms the kernel of this sampler which can be realized from the firing statistics of noisy integrate-and-fire neurons on a neuromorphic VLSI substrate. This paper demonstrates such an implementation on an array of digital spiking neurons with stochastic leak and threshold properties for inference tasks and presents some key performance metrics for such a hardware-based sampler in both the generative and discriminative contexts.","","","10.1109/ISCAS.2015.7169244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169244","","Neurons;Substrates;Noise;Noise measurement;Neuromorphics;Stochastic processes;Biological neural networks","Boltzmann machines;Markov processes;Monte Carlo methods","Gibbs sampling;low-power spiking digital neurons;restricted Boltzmann machines;deep belief networks;image classification;speech recognition;Markov Chain Monte Carlo procedure;sigmoidal function forms;neuromorphic VLSI substrate;digital spiking neurons;hardware-based sampler","","9","9","","","","","IEEE","IEEE Conferences"
"Hierarchical Convolutional Features for Visual Tracking","C. Ma; J. Huang; X. Yang; M. Yang","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","3074","3082","Visual object tracking is challenging as target objects often undergo significant appearance changes caused by deformation, abrupt motion, background clutter and occlusion. In this paper, we exploit features extracted from deep convolutional neural networks trained on object recognition datasets to improve tracking accuracy and robustness. The outputs of the last convolutional layers encode the semantic information of targets and such representations are robust to significant appearance variations. However, their spatial resolution is too coarse to precisely localize targets. In contrast, earlier convolutional layers provide more precise localization but are less invariant to appearance changes. We interpret the hierarchies of convolutional layers as a nonlinear counterpart of an image pyramid representation and exploit these multiple levels of abstraction for visual tracking. Specifically, we adaptively learn correlation filters on each convolutional layer to encode the target appearance. We hierarchically infer the maximum response of each layer to locate targets. Extensive experimental results on a largescale benchmark dataset show that the proposed algorithm performs favorably against state-of-the-art methods.","","","10.1109/ICCV.2015.352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410709","","Visualization;Target tracking;Correlation;Feature extraction;Semantics;Spatial resolution;Robustness","correlation methods;feature extraction;image coding;image filtering;image representation;image resolution;neural nets;object tracking","convolutional layer;correlation filters;image pyramid representation;spatial resolution;semantic information encoding;object recognition datasets;deep convolutional neural networks;feature extraction;visual object tracking;hierarchical convolutional features","","553","39","","","","","IEEE","IEEE Conferences"
"Speaker location and microphone spacing invariant acoustic modeling from raw multichannel waveforms","T. N. Sainath; R. J. Weiss; K. W. Wilson; A. Narayanan; M. Bacchiani; Andrew","Google, Inc., New York, NY, USA; Google, Inc., New York, NY, USA; Google, Inc., New York, NY, USA; Google, Inc., New York, NY, USA; Google, Inc., New York, NY, USA; Google, Inc., New York, NY, USA","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","30","36","Multichannel ASR systems commonly use separate modules to perform speech enhancement and acoustic modeling. In this paper, we present an algorithm to do multichannel enhancement jointly with the acoustic model, using a raw waveform convolutional LSTM deep neural network (CLDNN). We will show that our proposed method offers ~5% relative improvement in WER over a log-mel CLDNN trained on multiple channels. Analysis shows that the proposed network learns to be robust to varying angles of arrival for the target speaker, and performs as well as a model that is given oracle knowledge of the true location. Finally, we show that training such a network on inputs captured using multiple (linear) array configurations results in a model that is robust to a range of microphone spacings.","","","10.1109/ASRU.2015.7404770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404770","","Array signal processing;Microphone arrays;Convolution;Training;Reverberation","neural nets;speaker recognition;speech enhancement","array configuration;log-mel CLDNN;word error rate;WER;convolutional LSTM deep neural network;multichannel enhancement;speech enhancement;automatic speech recognition systems;multichannel ASR systems;microphone spacing invariant acoustic modeling;speaker location","","17","20","","","","","IEEE","IEEE Conferences"
"Multimodal embedding fusion for robust speaker role recognition in video broadcast","M. Rouvier; S. Delecraz; B. Favre; M. Bendris; F. Bechet","Aix-Marseille Université, CNRS, LIF, Marseille, France; Aix-Marseille Université, CNRS, LIF, Marseille, France; Aix-Marseille Université, CNRS, LIF, Marseille, France; Aix-Marseille Université, CNRS, LIF, Marseille, France; Aix-Marseille Université, CNRS, LIF, Marseille, France","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","","2015","","","383","389","Person role recognition in video broadcasts consists in classifying people into roles such as anchor, journalist, guest, etc. Existing approaches mostly consider one modality, either audio (speaker role recognition) or image (shot role recognition), firstly because of the non-synchrony between both modalities, and secondly because of the lack of a video corpus annotated in both modalities. Deep Neural Networks (DNN) approaches offer the ability to learn simultaneously feature representations (embeddings) and classification functions. This paper presents a multimodal fusion of audio, text and image embeddings spaces for speaker role recognition in asynchronous data. Monomodal embeddings are trained on exogenous data and fine-tuned using a DNN on 70 hours of French Broadcasts corpus for the target task. Experiments on the REPERE corpus show the benefit of the embeddings level fusion compared to the monomodal embeddings systems and to the standard late fusion method.","","","10.1109/ASRU.2015.7404820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404820","Speaker role recognition;multimodal speaker embeddings;broadcast News","Feature extraction;Image recognition;Visualization;Acoustics;Speech;Neural networks;Support vector machines","broadcasting;feature extraction;image classification;image fusion;image representation;neural nets;speaker recognition;video signal processing","multimodal embedding fusion;robust speaker role recognition;person role recognition;video broadcasts;video corpus;deep neural network approach;DNN approach;feature representations;classification functions;image embeddings spaces;French Broadcasts corpus;REPERE corpus;monomodal embedding systems","","2","33","","","","","IEEE","IEEE Conferences"
"Using Big Data Analytics for Authorship Authentication of Arabic Tweets","J. Albadarneh; B. Talafha; M. Al-Ayyoub; B. Zaqaibeh; M. Al-Smadi; Y. Jararweh; E. Benkhelifa","Jordan Univ. of Sci. & Technol., Irbid, Jordan; Jordan Univ. of Sci. & Technol., Irbid, Jordan; Jordan Univ. of Sci. & Technol., Irbid, Jordan; Appl. Sci. Univ., Bahrain; Jordan Univ. of Sci. & Technol., Irbid, Jordan; Jordan Univ. of Sci. & Technol., Irbid, Jordan; Staffordshire Univ., Stafford, UK","2015 IEEE/ACM 8th International Conference on Utility and Cloud Computing (UCC)","","2015","","","448","452","Authorship authentication of a certain text is concerned with correctly attributing it to its author based on its contents. It is a very important problem with deep root in history as many classical texts have doubtful attributions. The information age and ubiquitous use of the Internet is further complicating this problem and adding more dimensions to it. We are interested in the modern version of this problem where the text whose authorship needs authentication is an online text found in online social networks. Specifically, we are interested in the authorship authentication of tweets. This is not the only challenging aspect we consider here. Another challenging aspect is the language of the tweets. Most current works and existing tools support English. We chose to focus on the very important, yet largely understudied, Arabic language. Finally, we add another challenging aspect to the problem at hand by addressing it at a very large scale. We present our effort to employ big data analytics to address the authorship authentication problem of Arabic tweets. We start by crawling a dataset of more than 53K tweets distributed across 20 authors. We then use preprocessing steps to clean the data and prepare it for analysis. The next step is to compute the feature vectors of each tweet. We use the Bag-Of-Words (BOW) approach and compute the weights using the Term Frequency-Inverse Document Frequency (TF-IDF). Then, we feed the dataset to a Naive Bayes classifier implemented on a parallel and distributed computing framework known as Hadoop. To the best of our knowledge, none of the previous works on authorship authentication of Arabic text addressed the unique challenges associated with (1) tweets and (2) large-scale datasets. This makes our work unique on many levels. The results show that the testing accuracy is not very high (61.6%), which is expected in the very challenging setting that we consider.","","","10.1109/UCC.2015.80","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7431455","","Authentication;Electronic mail;Algorithm design and analysis;Support vector machines;Big data;Clustering algorithms;Machine learning algorithms","Big Data;natural language processing;social networking (online);text analysis","BOW approach;term frequency-inverse document frequency;TF-IDF;Naive Bayes classifier;distributed computing framework;Hadoop;Arabic text;Bag-Of-Words approach;authorship authentication problem;Arabic language;online social networks;online text;Internet;ubiquitous use;information age;classical texts;Arabic tweets;Big Data analytics","","1","29","","","","","IEEE","IEEE Conferences"
"Notice of Removal: Question Answering Search engine short review and road-map to future QA Search Engine","A. D. Kadam; S. D. Joshi; S. V. Shinde; S. P. Medhane","Dept. Info. Tech, BVDUCOEP, India; Dept. comp, BVDUCOEP, India; Dept. Info. Tech, BVDUCOEP, India; Dept. Info. Tech, BVDUCOEP, India","2015 International Conference on Electrical, Electronics, Signals, Communication and Optimization (EESCO)","","2015","","","1","8","Search Engines are based on bivalent logic and probability theory with lack of real world conceptual knowledge, higher precision and reasoning ability. Ranked links or snippets are less effective with big data Analysis and web of information. State of art Search Engine Google endeavor to retrieve good quality document, Msn Engine i.e. Bing endeavor decision boosting. Advanced algorithms account in user intents semantics and societal patterns on Web with recommendation system as product (recommendation Engines). Search engines have advanced from Text based to voice based (Dialogue based) to Image based (multimedia as input Question to search). Major advancement analysis of search Engine Enhancement, search Engines have advanced from traditional data base retrieval machine to web based machines from horizontal engines to Vertical Engines (Naukari.com, Quickr.com) with dedicated crawler Technology at heart. Information of web is structured instructed and posses a huge challenge to big data analytics. All though Retrieval results are optimistic yet they Lack in ability to interpret user Question. Precise answering from relevant Informatics is Search Engine Enhancement. Time complexity and memory are Algorithmic and Machine design parameters that support optimized search result. Question Answering Search Engine (QA Engine) is machine with deductive reasoning capability ability to amalgamate information from various knowledge datasets. QA Engine is front linear area in advanced information retrieval techniques, state of art technology to future of search engines, expert systems. QA search engines have advanced from Shallow Technique (keyword technique) to template based Structured Knowledge processing Engines, profile based engines, and context based machines to cross language machines to Multimedia QA Engines. Community based QA Yahoo answer, stack overflow to specialized search engines like ask.com, qura.com, true knowledge. Question Answering system have been embedded in Google Search Engine(Google Question Answering). IBM's Watson, a cognitive machine Thinking machine like Humans is decision support Engine developed under Deep QA project with advanced Natural language processing Information Retrieval with deep mining, a machine learning model that learns over time with reasoning model at base. With advent of android platform web based services like Apple Siri, Android Assistant questioner technology is art that assists. Enhancement future search engines with analysis (thinking), cognitive ability. This manuscript I present in Architecture on Question answering Search Engine that is search Engine Enhancement with Finite State Machine which facilitate answering question to time complexity The abstract, methodology, algorithmic analysis, of 20 research paper facilitate a future research and integration of proposed Architecture of Question Answering Search Engine.","","","10.1109/EESCO.2015.7253949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7253949","","","","","","","17","","","","","IEEE","IEEE Conferences"
"Developing a teaching framework to support software inspection","N. Gazerani; R. Ahmad; S. Forouzani; N. Gazerani","Department of software engineering, University of Malaya, Kuala Lumpur, Malaysia; Department of software engineering, University of Malaya, Kuala Lumpur, Malaysia; Department of software engineering, University of Malaya, Kuala Lumpur, Malaysia; Department of computing, UCSI University, Kuala Lumpur, Malaysia","2015 2nd International Conference on Knowledge-Based Engineering and Innovation (KBEI)","","2015","","","84","90","The objective of inspection process is to reduce the cost by finding and removing defects earlier. In recent years, there have been a number of attempts to further increase inspection efficiency by the introduction of tool support and resulting in a number of prototype systems. However, many software engineers suffer from lack of background knowledge and experience on software inspections and its techniques. The purpose of the study is to investigate the possible ways to teach software inspection processes in both sides of concepts and practice to Software engineering students. So, we offered a teaching framework to make software engineering students have a deep understanding on software inspection and to improve their practical abilities. The framework consists of three parts: general guidelines, specific guidelines and learning activity. In addition, there are two parts of general guidelines: Software inspection concept and Software inspection technique. Specific guidelines include Software inspection process and applying technique on Software inspection process. The third part includes conceptual samples, collaboration in practice, work sample and assessment for improving practical skills and abilities. Based on the teaching framework, a software support tool is designed and developed using interactive feature such as combining text, sound, graphic, images and animation. It is evaluated by software engineering students in University of Malaya to show positive impact of framework on teaching software inspections.","","","10.1109/KBEI.2015.7436026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436026","software inspection;guidelines and teaching framework","Decision support systems;Guidelines;Education;Inspection;Software;Industries;Engineering profession","computer aided instruction;computer animation;computer science education;educational institutions;graphical user interfaces;interactive systems;software quality;teaching;text analysis","collaboration-in-practice;work-sample;conceptual samples;interactive feature;University of Malaya;animation feature;image feature;graphic feature;sound feature;text feature;practical skills improvement;learning activity;specific guidelines;general guidelines;student practical ability improvement;software engineering students;software techniques;software inspection;teaching framework","","","6","","","","","IEEE","IEEE Conferences"
"Collaboration, distribution and culture - challenges for communication","H. Jaakkola; J. Henno; B. Thalheim; J. Mäkelä","Tampere University of Technology/Pori Department, Finland; Tallinn University of Technology/Information Technology, Estonia; Christian-Albrechts-University Kiel, Computer Science Institute, Germany; University of Lapland/Method Sciences, Rovaniemi, Finland","2015 38th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","","2015","","","657","664","Work, to an increasing amount, is based on collaboration between different partners; collaboration emphasizes the importance of communication between the collaborating parties. Increasingly, work is also becoming distributed and carried out in different geographical locations; distribution underlines the importance of managing and organizing work. The third important aspect characterizing current work is globalization; this refers to the multicultural characteristics of work and the need to understand the behavioral patterns of different (national) cultures. The paper addresses these three challenges related to the current work context. The approach points out the importance of a deep understanding of the characteristics of work - collaboration, distribution, and cultural diversity. Adaptive learning provides one potential solution to the challenges.","","","10.1109/MIPRO.2015.7160354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160354","","Teamwork;Complexity theory;Cultural differences;Organizations;Context;Computers","cultural aspects;globalisation;information systems;organisational aspects;software engineering","geographical locations;globalization;multicultural characteristics;culture behavioral patterns;cultural diversity;work collaboration;work distribution;adaptive learning;work management;software engineering;information systems","","3","31","","","","","IEEE","IEEE Conferences"
"Recipe recognition with large multimodal food dataset","Xin Wang; D. Kumar; N. Thome; M. Cord; F. Precioso","Sorbonne Universités, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, France; University of Waterloo, Vision and Image Processing (VIP) Lab, Ontario, Canada; Sorbonne Universités, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, France; Sorbonne Universités, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, France; Universités Nice Sophia Antipolis, UMR 7271, I3S, F-06900, France","2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","","2015","","","1","6","This paper deals with automatic systems for image recipe recognition. For this purpose, we compare and evaluate leading vision-based and text-based technologies on a new very large multimodal dataset (UPMC Food-101) containing about 100,000 recipes for a total of 101 food categories. Each item in this dataset is represented by one image plus textual information. We present deep experiments of recipe recognition on our dataset using visual, textual information and fusion. Additionally, we present experiments with text-based embedding technology to represent any food word in a semantical continuous space. We also compare our dataset features with a twin dataset provided by ETHZ university: we revisit their data collection protocols and carry out transfer learning schemes to highlight similarities and differences between both datasets. Finally, we propose a real application for daily users to identify recipes. This application is a web search engine that allows any mobile device to send a query image and retrieve the most relevant recipes in our dataset.","","","10.1109/ICMEW.2015.7169757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169757","","Visualization;Accuracy;Training;Feature extraction;Protocols;HTML;Google","computer vision;food technology;image fusion;image recognition;image retrieval;Internet;search engines;text analysis","multimodal food dataset;image recipe recognition;vision-based technology;text-based technology;multimodal dataset;UPMC Food-101;food categories;text-based embedding technology;semantical continuous space;data collection protocols;transfer learning schemes;Web search engine;mobile device;query image;relevant recipe retrieval","","12","20","","","","","IEEE","IEEE Conferences"
"PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization","A. Kendall; M. Grimes; R. Cipolla","NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2938","2946","We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 3 degrees accuracy for large scale outdoor scenes and 0.5m and 5 degrees accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show that the PoseNet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples.","","","10.1109/ICCV.2015.336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410693","","Cameras;Training;Neural networks;Robot vision systems;Simultaneous localization and mapping;Real-time systems;Quaternions","convolution;image motion analysis;neural nets;real-time systems;regression analysis","PoseNet;real-time 6-DOF camera relocalization;monocular six degree of freedom relocalization system;single RGB image;convolutional neural network;large scale outdoor scenes;image plane regression problems;transfer learning;large scale classification data;motion blur;camera intrinsics;point based SIFT registration;pose feature","","283","29","","","","","IEEE","IEEE Conferences"
"Picture: A probabilistic programming language for scene perception","T. D. Kulkarni; P. Kohli; J. B. Tenenbaum; V. Mansinghka","MIT, Cambridge, 02139, United States; Microsoft Research, Beijing 100080, China; MIT, Cambridge, 02139, United States; MIT, Cambridge, 02139, United States","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","4390","4399","Recent progress on probabilistic modeling and statistical learning, coupled with the availability of large training datasets, has led to remarkable progress in computer vision. Generative probabilistic models, or “analysis-by-synthesis” approaches, can capture rich scene structure but have been less widely applied than their discriminative counterparts, as they often require considerable problem-specific engineering in modeling and inference, and inference is typically seen as requiring slow, hypothesize-and-test Monte Carlo methods. Here we present Picture, a probabilistic programming language for scene understanding that allows researchers to express complex generative vision models, while automatically solving them using fast general-purpose inference machinery. Picture provides a stochastic scene language that can express generative models for arbitrary 2D/3D scenes, as well as a hierarchy of representation layers for comparing scene hypotheses with observed images by matching not simply pixels, but also more abstract features (e.g., contours, deep neural network activations). Inference can flexibly integrate advanced Monte Carlo strategies with fast bottom-up data-driven methods. Thus both representations and inference strategies can build directly on progress in discriminatively trained systems to make generative vision more robust and efficient. We use Picture to write programs for 3D face analysis, 3D human pose estimation, and 3D object reconstruction - each competitive with specially engineered baselines.","","","10.1109/CVPR.2015.7299068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299068","","Probabilistic logic;Proposals;Shape;Cameras;Analytical models;Rendering (computer graphics);Nose","computer vision;image matching;image reconstruction;inference mechanisms;Monte Carlo methods;pose estimation;programming languages;statistical analysis","Picture language;probabilistic programming language;scene perception;probabilistic modeling;statistical learning;computer vision;analysis-by-synthesis approach;hypothesize-and-test Monte Carlo methods;scene understanding;generative vision models;fast general-purpose inference machinery;stochastic scene language;image matching;3D face analysis;3D human pose estimation;3D object reconstruction","","29","52","","","","","IEEE","IEEE Conferences"
"A Multichannel Spectrum Sensing Fusion Mechanism for Cognitive Radio Networks: Design and Application to IEEE 802.22 WRANs","N. Tadayon; S. Aïssa","Institut National de la Recherche Scientifique (INRS), University of Quebec, Montreal, QC, Canada; Institut National de la Recherche Scientifique (INRS), University of Quebec, Montreal, QC, Canada","IEEE Transactions on Cognitive Communications and Networking","","2015","1","4","359","371","The IEEE 802.22 is a new cognitive radio standard that is aimed at extending wireless outreach to rural areas. Known as wireless regional area networks, and designed based on the not-to-interfere spectrum sharing model, WRANs are channelized and centrally controlled networks working on the under-utilized UHF/VHF TV bands to establish communication with remote users, so-called customer premises equipment (CPEs). Despite the importance of reliable and interference-free operation in these frequencies, spectrum sensing fusion mechanisms suggested in IEEE 802.22 are rudimentary and fail to satisfy the stringent mandated sensing requirements. Other deep-rooted shortcomings are performance nonuniformity over different signal-to-noise-ratio regimes, unbalanced performance, instability, and lack of flexibility. Inspired by these observations, in this paper, we propose a distributed spectrum sensing technique for WRANs, named multichannel learning-based distributed sensing fusion mechanism (MC-LDS). MC-LDS is demonstrated to be self-trained, stable, and to compensate for fault reports through its inherent reward-penalty approach. Moreover, MC-LDS exhibits a better uniform performance in all traffic regimes, is fair (reduces the false-alarm/misdetection gap), adjustable (works with several degrees of freedom), and bandwidth efficient (opens transmission opportunities for more CPEs). Simulation results and comparisons unanimously corroborate that MC-LDS outperforms IEEE 802.22 recommended algorithms, i.e., the AND, OR, and VOTING rules.","","","10.1109/TCCN.2016.2543732","Discovery Grant from the Natural Sciences and Engineering Research Council (NSERC) of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7437430","Cognitive Radio;IEEE 802.22;WRAN;Distributed Spectrum Sensing;Data Fusion;Decision Combining;Cognitive Radio;IEEE 802.22;WRAN;Distributed Spectrum Sensing;Data Fusion;Decision Combining","Sensors;Standards;Diversity reception;Collaboration;Reliability;Cognitive radio;Wireless sensor networks","cognitive radio;radio spectrum management;sensor fusion;signal detection;telecommunication traffic;wireless regional area networks","traffic regimes;MC-LDS fusion mechanism;multichannel learning-based distributed sensing fusion mechanism;unbalanced performance;signal-to-noise-ratio regime;stringent mandated sensing requirements;reliable operation;interference-free operation;CPE;customer premises equipment;remote users;under-utilized UHF-VHF TV bands;centrally controlled network;channelized network;not-to-interfere spectrum sharing model;wireless regional area network;cognitive radio network;multichannel spectrum sensing fusion mechanism;IEEE 802.22 WRAN;cognitive radio standard","","6","32","","","","","IEEE","IEEE Journals"
"Acquisition of grounded models of adjectival modifiers supporting semantic composition and transfer to a physical interactive robot","N. Mavridis; S. B. Kundig; N. Kapellas","Informatics and Telematics Institute, NCSR Demokritos, Athens, Greece; Informatics and Telematics Institute, NCSR Demokritos, Athens, Greece; Informatics and Telematics Institute, NCSR Demokritos, Athens, Greece","2015 International Conference on Advanced Robotics (ICAR)","","2015","","","244","251","Compositionality is a property of natural language which is of prime importance: It enables humans to form and conceptualize potentially novel and complex ideas, by combining words. On the other hand, the symbol grounding problem examines the way meaning is anchored to entities external to language, such as sensory percepts and sensory-motor routines. In this paper we aim towards the exploration of the intersection of compositionality and symbol grounding. We thus propose a methodology for constructing empirically derived models of grounded meaning, which afford composition of grounded semantics. We illustrate our methodology for the case of adjectival modifiers. Grounded models of adjectively modified and unmodified colors are acquired through a specially designed procedure with 134 participants, and then computational models of the modifiers “dark” and “light” are derived. The generalization ability of these learnt models is quantitatively evaluated, and their usage is demonstrated in a real-world physical humanoid robot. We regard this as an important step towards extending empirical approaches for symbol grounding so that they can accommodate compositionality: a necessary step towards the deep understanding of natural language for situated embodied agents, such as sensor-enabled ambient intelligence and interactive robots.","","","10.1109/ICAR.2015.7251463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7251463","compositionality;symbol grounding;adjectival modifiers;interactive robots","Computational modeling;Image color analysis;Color;Grounding;Semantics;Robot sensing systems","ambient intelligence;humanoid robots;human-robot interaction;natural language processing","grounded model acquisition;adjectival modifiers;semantic composition;physical interactive robot;natural language;symbol grounding problem;sensory percepts;sensory-motor routines;compositionality;learnt models;real-world physical humanoid robot;sensor-enabled ambient intelligence;situated embodied agents","","","16","","","","","IEEE","IEEE Conferences"
"System and architecture level characterization of big data applications on big and little core server architectures","M. Malik; S. Rafatirah; A. Sasan; H. Homayoun","Department of Electrical and Computer Engineering, George Mason University, Fairfax, VA, USA; Department of Information Sciences and Technology, George Mason University, Fairfax, VA, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, VA, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, VA, USA","2015 IEEE International Conference on Big Data (Big Data)","","2015","","","85","94","Emerging Big Data applications require a significant amount of server computational power. Big data analytics applications rely heavily on specific deep machine learning and data mining algorithms, and exhibit high computational intensity, memory intensity, I/O intensity and control intensity. Big data applications require computing resources that can efficiently scale to manage massive amounts of diverse data. However, the rapid growth in the data yields challenges to process data efficiently using current server architectures such as big Xeon cores. Furthermore, physical design constraints, such as power and density, have become the dominant limiting factor for scaling out servers. Therefore recent work advocates the use of low-power embedded cores in servers such as little Atom to address these challenges. In this work, through methodical investigation of power and performance measurements, and comprehensive system level and micro-architectural analysis, we characterize emerging big data applications on big Xeon and little Atom-based server architecture. The characterization results across a wide range of real-world big data applications and various software stacks demonstrate how the choice of big vs little core-based server for energy-efficiency is significantly influenced by the size of data, performance constraints, and presence of accelerator. Furthermore, the microarchitecture-level analysis highlights where improvement is needed in big and little cores microarchitecture.","","","10.1109/BigData.2015.7363745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363745","Performance;Power;Characterization;Big Data;High-Performance server;Low-Power server;Accelerator","Big data;Servers;Data mining;Computer architecture;Microarchitecture;Atomic measurements;Real-time systems","Big Data;data mining;file servers;microcomputers;microprocessor chips","little cores microarchitecture;microarchitecture-level analysis highlights;energy efficiency;software stacks;Atom-based server architecture;big Xeon;micro-architectural analysis;low-power embedded cores;Xeon cores;diverse data;control intensity;I/O intensity;memory intensity;high computational intensity;data mining algorithms;machine learning;Big data analytics applications;server computational power;core server architectures;architecture level characterization","","22","41","","","","","IEEE","IEEE Conferences"
"Robust in-hand manipulation of variously sized and shaped objects","S. Funabashi; A. Schmitz; T. Sato; S. Somlor; S. Sugano","Sugano Lab, School of Creative Science and Engineering, Waseda University, Okubo 2-4-12, Shinjuku, Tokyo, 169-0072, Japan; Sugano Lab, School of Creative Science and Engineering, Waseda University, Okubo 2-4-12, Shinjuku, Tokyo, 169-0072, Japan; Sugano Lab, School of Creative Science and Engineering, Waseda University, Okubo 2-4-12, Shinjuku, Tokyo, 169-0072, Japan; Sugano Lab, School of Creative Science and Engineering, Waseda University, Okubo 2-4-12, Shinjuku, Tokyo, 169-0072, Japan; Department of Modern Mechanical Engineering, School of Creative Science and Engineering, Waseda University, Ookubo 3-4-1, Shinjuku, Tokyo, 169-8555, Japan","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","2015","","","257","263","Moving objects within the hand is challenging, especially if the objects are of various shape and size. In this paper we use machine learning to learn in-hand manipulation of such various sized and shaped objects. The TWENDY-ONE hand is used, which has various properties that makes it well suited for in-hand manipulation: a high number of actuated joints, passive degrees of freedom and soft skin, six-axis force/torque (F/T) sensors in each fingertip, and distributed tactile sensors in the skin. A dataglove is used to gather training samples for teaching the required behavior. The object size information is extracted from the initial grasping posture. After training a neural network, the robot is able to manipulate objects of untrained sizes and shape. The results show the importance of size and tactile information. Compared to interpolation control, the adaptability for the initial posture gap could be greatly extended. Final results show that with deep learning the number of required training sets can be drastically reduced.","","","10.1109/IROS.2015.7353383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353383","","Portable document format;IEEE Xplore","","","","8","","","","","","IEEE","IEEE Conferences"
"Auto-encoder based modeling of combustion system for circulating fluidized bed boiler","Y. Yiru; G. Yinghui; X. Jianyu","Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China","2015 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","","2015","","","1","4","Deep learning attract the interests of many researchers. Multidimensional algorithms require large data storage space. This paper proposes a modeling of the combustion system used for Circulating Fluidized Bed Boiler (CFBB), which is based on the method of auto-encoder of deep learning. The 20 dimensional input samples set is the input layer, and then the units of hidden layer are calculated. The data dimension is reduced through the auto-encoder, further, these data are as input of the RBF network. The modeling is carried out by the Radical Basis Function (RBF) neutral network. Compared with traditional methods, the auto-encoder is suitable for modeling. The samples are greatly reduced for the subsequent work. Numerical results provided in this paper validate the proposed model and method, as well as the validity of the conversion from the auto-encoder strategy.","","","10.1109/ICSPCC.2015.7338946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7338946","Circulating fluidized bed boiler (CFBB);auto-encoders;combustion system;modeling","Data models;Mathematical model;Neural networks;Combustion;Computational modeling;Training;Testing","boilers;combustion;fluidised beds;radial basis function networks","radial basis function neutral network;data dimension;CFBB;circulating fluidized bed boiler;combustion system;auto-encoder based modeling","","","7","","","","","IEEE","IEEE Conferences"
"On the location dependence of convolutional neural network features","S. Workman; N. Jacobs","Department of Computer Science, University of Kentucky, United States; Department of Computer Science, University of Kentucky, United States","2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","","2015","","","70","78","As the availability of geotagged imagery has increased, so has the interest in geolocation-related computer vision applications, ranging from wide-area image geolocalization to the extraction of environmental data from social network imagery. Encouraged by the recent success of deep convolutional networks for learning high-level features, we investigate the usefulness of deep learned features for such problems. We compare features extracted from various layers of convolutional neural networks and analyze their discriminative ability with regards to location. Our analysis spans several problem settings, including region identification, visualizing land cover in aerial imagery, and ground-image localization in regions without ground-image reference data (where we achieve state-of-the-art performance on a benchmark dataset). We present results on multiple datasets, including a new dataset we introduce containing hundreds of thousands of ground-level and aerial images in a large region centered around San Francisco.","","","10.1109/CVPRW.2015.7301385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301385","","Feature extraction;Neural networks;Visualization;Support vector machines;Geology;Databases;Principal component analysis","feature extraction;geophysical image processing;land cover;neural nets","feature extraction;convolutional neural networks;region identification;land cover visualization;ground-image localization;San Francisco;aerial imagery","","13","30","","","","","IEEE","IEEE Conferences"
"On the relationship between visual attributes and convolutional networks","V. Escorcia; J. C. Niebles; B. Ghanem","King Abdullah University of Science and Technology (KAUST), Saudi Arabia; Universidad del Norte, Colombia; King Abdullah University of Science and Technology (KAUST), Saudi Arabia","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","2015","","","1256","1264","One of the cornerstone principles of deep models is their abstraction capacity, i.e. their ability to learn abstract concepts from `simpler' ones. Through extensive experiments, we characterize the nature of the relationship between abstract concepts (specifically objects in images) learned by popular and high performing convolutional networks (conv-nets) and established mid-level representations used in computer vision (specifically semantic visual attributes). We focus on attributes due to their impact on several applications, such as object description, retrieval and mining, and active (and zero-shot) learning. Among the findings we uncover, we show empirical evidence of the existence of Attribute Centric Nodes (ACNs) within a conv-net, which is trained to recognize objects (not attributes) in images. These special conv-net nodes (1) collectively encode information pertinent to visual attribute representation and discrimination, (2) are unevenly and sparsely distribution across all layers of the conv-net, and (3) play an important role in conv-net based object recognition.","","","10.1109/CVPR.2015.7298730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298730","","Yttrium","computer vision;convolution;neural nets;object recognition;semantic networks","semantic visual attribute;convolutional network;computer vision;attribute centric node;ACN;object recognition","","39","24","","","","","IEEE","IEEE Conferences"
"Ariyaka: A PALI Alphabet Recognition Script","N. Gautam; R. S. Sharma; H. Garima","NA; NA; NA","2015 International Conference on Computational Intelligence and Communication Networks (CICN)","","2015","","","293","295","Despite the fact that handwriting recognition is playing an essential part in to days era as well as learning of deep-rooted writings which belongs to reign of Budhha and resembles to ancient scripts and artefacts that are not easy to grasp and holds our culture base. PALI is prop for every language used around 1 4 century B.C. And is written in numerous languages or scripts resembling Sinhala, Khmer, Burmese, Devanagari, Lao and many more which are having remarkable influence. Discussions in this paper displays need of ancient scripts to be recognized and Ariyaka is taken as base script here which is recognized by Modified OCR classification with an accuracy of 85.73%.","","","10.1109/CICN.2015.65","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546101","Archaeology;Handwriting Recognition;Pali;Ariyaka script;OCR","Optical character recognition software;Character recognition;Feature extraction;Handwriting recognition;Optical imaging;Image recognition;Computer science","document image processing;handwriting recognition;natural language processing;optical character recognition","Ariyaka;PALI alphabet recognition script;handwriting recognition;Budhha;ancient script;OCR classification","","","10","","","","","IEEE","IEEE Conferences"
"A new teaching tool to enhance power quality assessment","J. F. Martins; P. Pereira; A. J. Pires; V. F. Pires","FCT/UNL & CTS-UNINOVA, Monte da Caparica, Portugal; FCT/UNL & CTS-UNINOVA, Monte da Caparica, Portugal; ESTSetubal/IPS & CTS-UNINOVA, Setúbal, Portugal; ESTSetubal/IPS & INESC-ID Lisboa, Setúbal, Portugal","IECON 2015 - 41st Annual Conference of the IEEE Industrial Electronics Society","","2015","","","004158","004162","The study and assessment of Power Quality issues is nowadays a very important subject, particularly regarding Cyber-physical and Industrial Agents based systems, which are extremely sensitive to Power Quality disturbances. Giving students or engineers practical experience in this field requires a large investment from teaching institutions. This paper presents a laboratory device that emulates Power Quality disturbances in order to provide the required experimental expertize in the subject. It addresses limiting aspects such as harmonic distortion, flicker, sags, swells and transients. The developed system presents a good opportunity for technicians, even without deep knowledge on the field of power quality, to learn basic principles and be able to identify Power Quality events. Since the system is based on real data, represents a valuable approach giving trainees practical knowledge on the field.","","","10.1109/IECON.2015.7392748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7392748","Power Quality;power inverter;fault event generator","Power quality;Computers;Inverters;Monitoring;Harmonic distortion;Education;Harmonic analysis","computer aided instruction;harmonic distortion;multi-agent systems;power engineering computing;power engineering education;power supply quality;power system harmonics;teaching","teaching tool;power quality assessment enhancement;cyber-physical systems;industrial agents based systems;power quality disturbances;teaching institutions;laboratory device;harmonic distortion;flicker;sags;swells;transients;power inverter;fault event generator","","","16","","","","","IEEE","IEEE Conferences"
"Cybermatics Forum Keynotes","Bin Guo; Xing Xie; Zhangbing Zhou; Niu Jian-Wei; Bofeng Zhang; C. Leung; Weishan Zhang; Lu Liu","NA; NA; NA; NA; NA; NA; NA; NA","2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)","","2015","","","xcviii","xcviii","These keynotes discusses the following: Mobile Crowd Sensing and Computing: Challenges and Opportunities; Understanding User Mobility from Large Scale Human Behavioral Data; Assessment of Service Protocol Adaptability in Mediated Service Interactions; VINCE: Exploriting visible light sensing for smartphone-based NFC systems; Personalized Services Based on User Model in Social Networks; Big Data Mining and Computing in a Smart World; A Smart Realtime Video Cloud Platform Using Deep Learning; A Socio-ecological Service Discovery Model in Machine-to-Machine Communication Networks.","","","10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518202","","","behavioural sciences computing;mobile computing","trusted computing;autonomic computing;associated workshops;mobile crowd sensing;large scale human behavioral data;personalized services;social networks","","","","","","","","IEEE","IEEE Conferences"
"Five forces shaping the silicon world: Advanced sensing and intelligence in IoT and vision","C. Rowen","Design Group Cadence Design Systems, Inc.","2015 28th IEEE International System-on-Chip Conference (SOCC)","","2015","","","1","1","The cumulative improvement in digital silicon density, energy and performance has had an impressive quantitative impact on the world we live in. But new forces, embodied in radical changes in system applications, are rapidly disrupting traditional silicon architectures. In this talk we chart five of the major forces at work in silicon systems, and explore new categories of “things that sense and see”. Along the way, we visit some fundamental shifts taking place in low-energy processor cores, in vision DSPs, and in systems for “deep learning” that now exceed human capabilities.","","","10.1109/SOCC.2015.7406942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406942","","Silicon;IP networks;Companies;Computer architecture;Sensors;Artificial intelligence;Process control","digital signal processing chips;elemental semiconductors;integrated circuit design;low-power electronics;microprocessor chips;silicon","DSP;low-energy processor cores;IoT;advanced sensing;Si","","","","","","","","IEEE","IEEE Conferences"
"A Novel Recognition Method of Multimedia Data for Social Network","G. Chen; Y. Su; X. Ren; F. Xie","Commun. Univ. of China, Beijing, China; Xinhua News Agency, Beijing, China; Commun. Univ. of China, Beijing, China; Xinhua News Agency, Beijing, China","2015 3rd International Conference on Applied Computing and Information Technology/2nd International Conference on Computational Science and Intelligence","","2015","","","464","467","Due to the trend of diversification of media in social network, the deep learning of multimedia data is becoming more and more important. This paper analyzes the special tag information of image, audio and video data on the internet, and proposes an effective recognition method of multimedia data for social network. The method will provide a lot of data for the multimedia content analysis of social network.","","","10.1109/ACIT-CSI.2015.88","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336108","Social Network;Multimedia;Data Analysis","Streaming media;Multimedia communication;Social network services;Internet;HTML;Uniform resource locators;Web pages","Internet;multimedia computing;social networking (online)","recognition method;multimedia data;social network;Internet;multimedia content analysis","","3","6","","","","","IEEE","IEEE Conferences"
"Flowing ConvNets for Human Pose Estimation in Videos","T. Pfister; J. Charles; A. Zisserman","Dept. of Eng. Sci., Univ. of Oxford, Oxford, UK; Sch. of Comput., Univ. of Leeds, Leeds, UK; Dept. of Eng. Sci., Univ. of Oxford, Oxford, UK","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1913","1921","The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow. To this end we propose a network architecture with the following novelties: (i) a deeper network than previously investigated for regressing heatmaps, (ii) spatial fusion layers that learn an implicit spatial model, (iii) optical flow is used to align heatmap predictions from neighbouring frames, and (iv) a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map. We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, one that regresses joint coordinates directly, and one that predicts heatmaps without spatial fusion. The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset, and outperforms other deep methods that don't use a graphical model on the single-image FLIC benchmark (and also [5, 35] in the high precision region).","","","10.1109/ICCV.2015.222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410579","","Heating;Optical imaging;Videos;Training;Adaptive optics;Computer architecture","image sequences;neural nets;pose estimation;video signal processing","ConvNets;human pose estimation;temporal context;optical flow;spatial fusion layer;heatmap prediction;parametric pooling layer;pooled confidence map;video pose estimation","","127","40","","","","","IEEE","IEEE Conferences"
"Adapting off-the-shelf CNNs for word spotting & recognition","A. Sharma; Pramod Sankar K.","Xerox Research Centre India, Bengaluru, India; Xerox Research Centre India, Bengaluru, India","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","","2015","","","986","990","The word spotting approach is extremely useful for searching and annotating documents for which robust recognizers are unavailable. Traditionally, hand-designed features were used to represent the word images for spotting. In this paper, we learn a data-driven representation for word-images from Convolutional Neural Networks (CNNs). Previous approaches that learn deep neural networks for a particular task/dataset are difficult to design and train for generic word spotting. Instead, by “adapting” a CNN trained for a different problem, we show tremendous speedup in the training phase. Our experiments show that features extracted from an adapted-CNN handsomely outperform hand-designed features on both spotting and recognition tasks for printed (English and Telugu) and handwritten (IAM) document collections.","","","10.1109/ICDAR.2015.7333909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333909","","Bridges;Indexes","document image processing;image representation;natural language processing;neural nets","English;Telugu;convolutional neural networks;data driven representation;word images;hand designed features;robust recognizers;word spotting approach;adapting off-the-shelf CNN;word recognition;word spotting","","19","21","","","","","IEEE","IEEE Conferences"
"Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views","H. Su; C. R. Qi; Y. Li; L. J. Guibas","Stanford Univ., Stanford, CA, USA; Stanford Univ., Stanford, CA, USA; Stanford Univ., Stanford, CA, USA; Stanford Univ., Stanford, CA, USA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2686","2694","Object viewpoint estimation from 2D images is an essential task in computer vision. However, two issues hinder its progress: scarcity of training data with viewpoint annotations, and a lack of powerful features. Inspired by the growing availability of 3D models, we propose a framework to address both issues by combining render-based image synthesis and CNNs (Convolutional Neural Networks). We believe that 3D models have the potential in generating a large number of images of high variation, which can be well exploited by deep CNN with a high learning capacity. Towards this goal, we propose a scalable and overfit-resistant image synthesis pipeline, together with a novel CNN specifically tailored for the viewpoint estimation task. Experimentally, we show that the viewpoint estimation from our pipeline can significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.","","","10.1109/ICCV.2015.308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410665","","Three-dimensional displays;Solid modeling;Estimation;Training;Deformable models;Computational modeling;Pipelines","computer vision;neural nets;rendering (computer graphics);solid modelling","CNN;3D model view rendering;object viewpoint estimation;computer vision;render-based image synthesis;convolutional neural networks;overfit-resistant image synthesis pipeline;viewpoint estimation task","","182","40","","","","","IEEE","IEEE Conferences"
"Why Data Needs more Attention in Architecture Design - Experiences from Prototyping a Large-Scale Mobile App Ecosystem","M. Naab; S. Braun; T. Lenhart; S. Hess; A. Eitel; D. Magin; R. Carbon; F. Kiefer","Fraunhofer IESE, Kaiserslautern, Germany; Fraunhofer IESE, Kaiserslautern, Germany; Fraunhofer IESE, Kaiserslautern, Germany; Fraunhofer IESE, Kaiserslautern, Germany; Fraunhofer IESE, Kaiserslautern, Germany; Fraunhofer IESE, Kaiserslautern, Germany; ETIC John Deere, Kaiserslautern, Germany; ETIC John Deere, Kaiserslautern, Germany","2015 12th Working IEEE/IFIP Conference on Software Architecture","","2015","","","75","84","Data is of great importance in computer science and in particular in information systems and how data is treated has major impact on a system's quality attributes. Nevertheless, software architecture research, literature, and practice often neglect data and focus instead on other architectural topics like components and connectors or the management of architecture decisions in general. This paper contributes experiences from the prototyping of a large-scale mobile app ecosystem for the agricultural domain. Architectural drivers like multi-tenancy, different technical platforms and offline capability led to deep reasoning about data. In this paper, we describe the architectural decisions made around data in the app ecosystem and we present our lessons learned on technical aspects regarding data, but also on data modeling and general methodical aspects how to treat data in architecting. We want to share these experiences with the research community to stimulate more research on data in software architecture and we want to give practitioners usable hints for their daily work around data in constructing large information systems and ecosystems.","","","10.1109/WICSA.2015.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158506","software architecture;architecture design;data;mobile;app ecosystem;practical experiences","Computer architecture;Context;Synchronization;Databases;Data models;Ecosystems;Mobile communication","mobile computing;software architecture;software prototyping;software quality","architecture design;large-scale mobile app ecosystem prototyping;computer science;information systems;system quality attributes;software architecture;architecture decision management;agricultural domain;architectural drivers;data modeling","","9","29","","","","","IEEE","IEEE Conferences"
"Implementation of energy harvesting system for powering thermal gliders for long duration ocean research","C. D. Haldeman; O. Schofield; D. C. Webb; T. I. Valdez; J. A. Jones","Center for Ocean Observing Leadership Rutgers, The State University of New Jersey, New Brunswick, USA; Center for Ocean Observing Leadership Rutgers, The State University of New Jersey, New Brunswick, USA; Teledyne Webb Research Corporation, North Falmouth, MA USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, USA","OCEANS 2015 - MTS/IEEE Washington","","2015","","","1","5","The exploration of the Earth's oceans is aided by autonomous underwater vehicles (AUVs). AUVs in use today include floats and gliders; they can be deployed to profile salinity, temperature and pressure of the ocean at depths of up to 2 km. Both the floats and gliders typically control buoyancy by filling and deflating an external bladder with a hydraulic fluid delivered by an electrical pump. The operation time of an AUV is limited by energy storage. For floats, such as the Argo float, the operating duration is approximately 5 years with the capability to dive once every 10 days. For electric gliders, such as the deep G2 Slocum, the mission duration can be up to one year with lithium primary batteries. An energy storage system has been developed that can harvest energy from the temperature differences at various depths of the ocean. This system was demonstrated on an Argo style float and has been implemented in a thermal version of the Slocum glider. The energy harvesting system is based on a phase change material with a freeze thaw cycle that pressurizes hydraulic oil that is converted to electrical energy. The thermal Slocum glider does not use an electrical pump, but harvested thermal energy to control buoyancy. The goal for the thermal Slocum glider is for persistent ocean operation for a duration of up to 10 years. A thermal powered glider with an energy harvesting system as described can collect conductivity, temperature, and pressure data and deliver it to the National Data Buoy Center (NDBC) Glider Data Monitoring System and the World Meteorological Organization (WMO) Global Telecommunications System (GTS). Feeding into operational modeling centers such as the National Centers for Environmental Prediction (NCEP) and the U.S. Naval Observatory (NAVO), this data will enable advanced climate predictions over a timespan not currently achievable with present technology. Current testing of the thermal powered Slocum glider is to determine the durability of the technology and quantify the glider system design. Previous issues with this technology included energy storage system management and glider mechanical limitations. Our objective is to learn how to fly an energy harvesting thermal glider that interacts with the ocean environment efficiently. We would also like to establish the latitudinal range of operation. This thermal powered Slocumglider, dubbed Clark, after the famous explorer duo Lewis and Clark, has been deployed off of St. Thomas for flight dynamics and durability testing. The following paper will discuss the deployment and testing of the thermal powered Slocum glider. We will also discuss the advantages of ocean energy harvesting technology for oceanographic research.","","","10.23919/OCEANS.2015.7404559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404559","glider;thermal glider;thermal recharging","Batteries;Ocean temperature;Energy harvesting;Buoyancy;Electronic ballasts","autonomous underwater vehicles;energy harvesting;oceanographic techniques","energy harvesting system;thermal gliders;long duration ocean research;autonomous underwater vehicles;AUV;hydraulic fluid;electrical pump;Argo float;electric gliders;energy storage system;Slocum glider;phase change material;National Data Buoy Center;NDBC;glider data monitoring system;World Meteorological Organization;Global Telecommunications System;GTS;National Centers for Environmental Prediction;NCEP;flight dynamics;durability testing","","1","7","","","","","IEEE","IEEE Conferences"
"Seclius: An Information Flow-Based, Consequence-Centric Security Metric","S. A. Zonouz; R. Berthier; H. Khurana; W. H. Sanders; T. Yardley","Department of Electrical and Computer Engineering, University of Miami , Coral Gables; Department of Electrical and Computer Engineering, University of Illinois , Urbana; Integrated Security Technologies division, Honeywell Automation and Control Systems Lab, Minneapolis; Department of Electrical and Computer Engineering, University of Illinois , Urbana; Department of Electrical and Computer Engineering, University of Illinois , Urbana","IEEE Transactions on Parallel and Distributed Systems","","2015","26","2","562","573","It is critical to monitor IT systems that are part of energy delivery system infrastructure. The problem with intrusion detection systems (IDSes) is that they often produce thousands of alerts daily that must be dealt with by administrators manually. To provide situational awareness, detection systems usually employ (alert, priority) mappings that are either built in the IDS without consideration of the high-level mission objectives of the infrastructure, or manually defined by administrators through a time-consuming task that requires deep system-level expertise. In this paper, we present Seclius, an online security evaluation framework that translates low-level IDS alerts into a high-level system security measure and provides a ranking of past malicious events and affected system assets based on how crucial they are for the organization. Seclius significantly reduces human involvement by automatically learning system characteristics, providing a simple formalism that administrators can use to define security requirements. Experiments on a process control network with real vulnerabilities and a multistep attack show that Seclius can accurately report system security with low performance overhead and support the time-constrained security decision-making process that is necessary for critical infrastructure.","","","10.1109/TPDS.2013.162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547151","Intrusion detection systems;system security metric;information flow-based analysis","Security;Measurement;Organizations;Databases;Web servers;Probabilistic logic","security of data","Seclius framework;information flow-based consequence-centric security metric;IT systems;information systems;intrusion detection system;IDS;online security evaluation framework;high-level system security measure;time-constrained security decision-making process;critical infrastructure","","11","50","","","","","IEEE","IEEE Journals"
"Visual Madlibs: Fill in the Blank Description Generation and Question Answering","L. Yu; E. Park; A. C. Berg; T. L. Berg","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","2461","2469","In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images. Experiments using joint-embedding and deep learning methods show promising results on these tasks.","","","10.1109/ICCV.2015.283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410640","","Visualization;Natural languages;Context;Computer vision;Knowledge discovery;Explosions;Videos","image retrieval;natural language processing;question answering (information retrieval)","fill in the blank description generation;natural language descriptions;visual Madlibs dataset;fill-in-the-blank templates;targeted descriptions;description generation tasks;multiple-choice question-answering","","31","38","","","","","IEEE","IEEE Conferences"
"A graph digital signal processing method for semantic analysis","M. Trifan; B. Ionescu; C. Gadea; D. Ionescu","School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Ontario, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Ontario, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Ontario, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Ontario, Canada","2015 IEEE 10th Jubilee International Symposium on Applied Computational Intelligence and Informatics","","2015","","","187","192","This paper focuses on the problem of devising a computationally tractable procedure for representing the natural language understanding (NLU). It approaches this goal, by using distributional models of meaning through a method from graph-based digital signal processing (DSP) which only recently grabbed the attention of researchers from the field of natural language processing (NLP) related to big data analysis. The novelty of our approach lies in the combination of three domains: advances in deep learning algorithms for word representation, dependency parsing for modeling inter-word relations and convolution using orthogonal Hadamard codes for composing the two previous areas, generating a unique representation for the sentence. Two types of problems are resolved in a new unified way: sentence similarity given by the cos function of the corresponding vectors and question-answering where the query is matched to possible answers. This technique resembles the spread spectrum methods from telecommunication theory where multiple users share a common channel, and are able to communicate without interference. In the content of this paper the case of individual words play the role of users sharing the same sentence. Examples of the method application to a standard set of sentences, used for benchmarking the accuracy and the execution time is also given.","","","10.1109/SACI.2015.7208196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208196","distributional semantic composition;similarity;Hadamard matrix;CDMA;dependency parser;SICK corpus","Semantics;Signal processing algorithms;Digital signal processing;Error correction;Error correction codes;Correlation;Natural language processing","graph theory;natural language processing;question answering (information retrieval);signal processing","graph digital signal processing method;semantic analysis;natural language understanding;NLU;distributional models;graph based digital signal processing;DSP;NLP;big data analysis;orthogonal Hadamard codes;sentence similarity;question answering;spread spectrum methods;telecommunication theory","","","36","","","","","IEEE","IEEE Conferences"
"CLTune: A Generic Auto-Tuner for OpenCL Kernels","C. Nugteren; V. Codreanu","SURFsara HPC centre, Amsterdam, Netherlands; SURFsara HPC centre, Amsterdam, Netherlands","2015 IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip","","2015","","","195","202","This work presents CLTune, an auto-tuner for OpenCL kernels. It evaluates and tunes kernel performance of a generic, user-defined search space of possible parameter-value combinations. Example parameters include the OpenCL workgroup size, vector data-types, tile sizes, and loop unrolling factors. CLTune can be used in the following scenarios: 1) when there are too many tunable parameters to explore manually, 2) when performance portability across OpenCL devices is desired, or 3) when the optimal parameters change based on input argument values (e.g. matrix dimensions). The auto-tuner is generic, easy to use, open-source, and supports multiple search strategies including simulated annealing and particle swarm optimisation. CLTune is evaluated on two GPU case-studies inspired by the recent successes in deep learning: 2D convolution and matrix-multiplication (GEMM). For 2D convolution, we demonstrate the need for auto-tuning by optimizing for different filter sizes, achieving performance on-par or better than the state-of-the-art. For matrix-multiplication, we use CLTune to explore a parameter space of more than two-hundred thousand configurations, we show the need for device-specific tuning, and outperform the clBLAS library on NVIDIA, AMD and Intel GPUs.","","","10.1109/MCSoC.2015.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328205","OpenCL;Auto-Tuning;GPU;Convolution;GEMM","Kernel;Convolution;Performance evaluation;Tuners;Simulated annealing;Search problems","parallel programming;public domain software;software libraries","CLTune;auto-tuner;OpenCL kernels;open-source;GPU;2D convolution;matrix-multiplication","","21","22","","","","","IEEE","IEEE Conferences"
"What Makes an Object Memorable?","R. Dubey; J. Peterson; A. Khosla; M. Yang; B. Ghanem","NA; Univ. of California, Merced, Merced, CA, USA; NA; Univ. of California, Merced, Merced, CA, USA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1089","1097","Recent studies on image memorability have shed light on what distinguishes the memorability of different images and the intrinsic and extrinsic properties that make those images memorable. However, a clear understanding of the memorability of specific objects inside an image remains elusive. In this paper, we provide the first attempt to answer the question: what exactly is remembered about an image? We augment both the images and object segmentations from the PASCAL-S dataset with ground truth memorability scores and shed light on the various factors and properties that make an object memorable (or forgettable) to humans. We analyze various visual factors that may influence object memorability (e.g. color, visual saliency, and object categories). We also study the correlation between object and image memorability and find that image memorability is greatly affected by the memorability of its most memorable object. Lastly, we explore the effectiveness of deep learning and other computational approaches in predicting object memorability in images. Our efforts offer a deeper understanding of memorability in general thereby opening up avenues for a wide variety of applications.","","","10.1109/ICCV.2015.130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410487","","Visualization;Image segmentation;Image color analysis;Correlation;Games;Computer vision;Presses","image colour analysis;image segmentation","image memorability;object memorability;object segmentation;image segmentation;PASCAL-S dataset;ground truth memorability scores;visual factors;visual saliency;object categories;color","","21","40","","","","","IEEE","IEEE Conferences"
"Predicting Ball Ownership in Basketball from a Monocular View Using Only Player Trajectories","X. Wei; L. Sha; P. Lucey; P. Carr; S. Sridharan; I. Matthews","Disney Res., Pittsburgh, PA, USA; Disney Res., Pittsburgh, PA, USA; Disney Res., Pittsburgh, PA, USA; Disney Res., Pittsburgh, PA, USA; Queensland Univ. of Technol., Brisbane, QLD, Australia; Disney Res., Pittsburgh, PA, USA","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","780","787","Tracking objects like a basketball from a monocular view is challenging due to its small size, potential to move at high velocities as well as the high frequency of occlusion. However, humans with a deep knowledge of a game like basketball can predict with high accuracy the location of the ball even without seeing it due to the location and motion of nearby objects, as well as information of where it was last seen. Learning from tracking data is problematic however, due to the high variance in player locations. In this paper, we show that by simply ""permuting"" the multi-agent data we obtain a compact role-ordered feature which accurately predict the ball owner. We also show that our formulation can incorporate other information sources such as a vision-based ball detector to improve prediction accuracy.","","","10.1109/ICCVW.2015.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406455","","Entropy;Tracking;Probability distribution;Games;Detectors;Cameras;Training","computer vision;multi-agent systems;object tracking;sport","ball ownership prediction;basketball;monocular view;player trajectories;object tracking;ball location;tracking data;player locations;multiagent data;compact role-ordered feature;vision-based ball detector;prediction accuracy","","2","27","","","","","IEEE","IEEE Conferences"
"A novel denoising autoencoder assisted segmentation algorithm for cotton field","Yanan Li; Z. Cao; Y. Xiao; Hao Lu; Yanjun Zhu","School of Automation, Huazhong University of Science and Technology, Wuhan 430074, China; School of Automation, Huazhong University of Science and Technology, Wuhan 430074, China; School of Automation, Huazhong University of Science and Technology, Wuhan 430074, China; School of Automation, Huazhong University of Science and Technology, Wuhan 430074, China; School of Automation, Huazhong University of Science and Technology, Wuhan 430074, China","2015 Chinese Automation Congress (CAC)","","2015","","","588","593","Crop segmentation from the images captured in the outdoor field is a complex task in agriculture automation, let alone detecting some specific crops with one method. Cotton, as one of the four major economic crops, is of great significance to the development of the national economy. In this paper, a novel strategy based on the deep learning is utilized to establish the crop classifier in the RGB vector color space in order to realize the specific crop image segmentation. To the best of our knowledge, little research has been done to crop segmentation in the wild cotton field with digital cameras. To verify the performance of the proposed method, two specific crops (cotton plants and raw cotton) grown in the cotton field are demonstrated in this paper. Experiment results show that our method outperforms other state-of-art algorithms on cotton plants (raw cotton) segmentation in yielding the highest performance with the lowest mean square deviation. Moreover, the impact of different color spaces to the proposed method is compared. The proposed crop segmentation method can not only be used to green crop segmentation in the field, but segment specific crop like cotton.","","","10.1109/CAC.2015.7382568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382568","specific crop segmentation;denoising autoencoder;cotton","Cotton;Image segmentation;Image color analysis;Noise reduction;Training;Green products","agricultural engineering;crops;image capture;image denoising;image segmentation;inspection;mean square error methods","cotton field;image capture;crop classifier;RGB vector color space;crop image segmentation;lowest mean square deviation;denoising autoencoder","","","18","","","","","IEEE","IEEE Conferences"
"Automatic tag extraction from social media for visual labeling","S. Liu; T. Forss","Arcada University of Applied Sciences, Jan-Magnus Janssonin aukio 1, 00560 Helsinki, Finland; Arcada University of Applied Sciences, Jan-Magnus Janssonin aukio 1, 00560 Helsinki, Finland","2015 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)","","2015","01","","504","510","Visual labeling or automated visual annotation is of great importance to the efficient access and management of multimedia content. Many methods and techniques have been proposed for image annotation in the last decade and they have shown reasonable performance on standard datasets. Great progress has been made especially in recent couple of years with the development of deep learning models for image content analysis and extraction of content-based concept labels. However, concept objects labels are much more friendly to machine than to users. We consider that more relevant and user-friendly visual labels need to include “context” descriptors. In this study we explore the possibilities to leverage social media content as a resource for visual labeling. We developed a tag extraction system that applies heuristic rules and term weighting method to extract image tags from associated Tweet. The system retrieves tweet-image pairs from public Twitter accounts, analyzes the Tweet, and generates labels for the images. We elaborate on different visual labeling methods, tag analysis and tag refinement methods.","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7526962","Visual Labeling;Image Annotation;Social Media Analysis;Twitter","Visualization;Twitter;Labeling;Context;Tagging;Media;Organizations","","","","","25","","","","","IEEE","IEEE Conferences"
"Scene Intrinsics and Depth from a Single Image","E. Shelhamer; J. T. Barron; T. Darrell","NA; NA; NA","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","","2015","","","235","242","Intrinsic image decomposition factorizes an observed image into its physical causes. This is most commonly framed as a decomposition into reflectance and shading, although recent progress has made full decompositions into shape, illumination, reflectance, and shading possible. However, existing factorization approaches require depth sensing to initialize the optimization of scene intrinsics. Rather than relying on depth sensors, we show that depth estimated purely from monocular appearance can provide sufficient cues for intrinsic image analysis. Our full intrinsic pipeline regresses depth by a fully convolutional network then jointly optimizes the intrinsic factorization to recover the input image. This combination yields full decompositions by uniting feature learning through deep network regression with physical modeling through statistical priors and random field regularization. This work demonstrates the first pipeline for full intrinsic decomposition of scenes from a single color image input alone.","","","10.1109/ICCVW.2015.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406388","","Shape;Lighting;Pipelines;Optimization;Sensors;Training;Cognition","image colour analysis;image sensors;statistical analysis","scene intrinsics;intrinsic image decomposition;physical causes;factorization approaches;depth sensors;monocular appearance;physical modeling;statistical priors;random field regularization;single color image","","7","30","","","","","IEEE","IEEE Conferences"
"Web-Scale Image Clustering Revisited","Y. Avrithis; Y. Kalantidis; E. Anagnostopoulos; I. Z. Emiris","NA; NA; NA; NA","2015 IEEE International Conference on Computer Vision (ICCV)","","2015","","","1502","1510","Large scale duplicate detection, clustering and mining of documents or images has been conventionally treated with seed detection via hashing, followed by seed growing heuristics using fast search. Principled clustering methods, especially kernelized and spectral ones, have higher complexity and are difficult to scale above millions. Under the assumption of documents or images embedded in Euclidean space, we revisit recent advances in approximate k-means variants, and borrow their best ingredients to introduce a new one, inverted-quantized k-means (IQ-means). Key underlying concepts are quantization of data points and multi-index based inverted search from centroids to cells. Its quantization is a form of hashing and analogous to seed detection, while its updates are analogous to seed growing, yet principled in the sense of distortion minimization. We further design a dynamic variant that is able to determine the number of clusters k in a single run at nearly zero additional cost. Combined with powerful deep learned representations, we achieve clustering of a 100 million image collection on a single machine in less than one hour.","","","10.1109/ICCV.2015.176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410533","","Quantization (signal);Visualization;Distortion;Artificial neural networks;Search problems;Probabilistic logic;Metadata","approximation theory;data mining;document handling;image processing;minimisation;pattern clustering;search problems","IQ-means;data point quantization;multiindex based inverted search;distortion minimization;dynamic variant design;inverted-quantized k-means;approximate k-means variants;principled clustering methods;seed growing heuristics;hashing;seed detection;document mining;document clustering;duplicate detection;Web-scale image clustering","","11","39","","","","","IEEE","IEEE Conferences"
"Software quality research: From processes to model-based techniques","B. Peischl","Softnet Austria, Inffeldgasse 16b/II, 8010 Graz, Austria","2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","2015","","","1","6","In this article we state that cyber-physical systems and the Internet of Things pose challenges to software quality research. In the emerging real-time digital economy companies need to gain deep customer insight. The state of the art in modelbased systems and model-based testing allows software engineers for product-based and quantitative control of quality and for increased productivity. These gains can be invested to better understand the domain and business conditions. We argue that cyber-physical systems will pose an excellent basis for conducting collaborative research in model-based systems and model-based testing in the near future and report on lessons learnt in three areas of software (quality) research: (1) process-oriented quality, (2) model-based systems and (3) model-based testing.","","","10.1109/ICSTW.2015.7107475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7107475","","Industries;Unified modeling language;Testing;Collaboration;Software quality;Companies","groupware;Internet of Things;program testing;software quality","software quality research;model-based techniques;cyber-physical systems;Internet-of-things;real-time digital economy companies;model-based testing;model-based systems;product-based;quantitative control;collaborative research;process-oriented quality","","3","27","","","","","IEEE","IEEE Conferences"
"The neural-SIFT feature descriptor for visual vocabulary object recognition","S. Jansen; A. Shantia; M. A. Wiering","Institute of Artificial Intelligence and Cognitive Engineering, University of Groningen, The Netherlands; Institute of Artificial Intelligence and Cognitive Engineering, University of Groningen, The Netherlands; Institute of Artificial Intelligence and Cognitive Engineering, University of Groningen, The Netherlands","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","8","Recognizing the semantic content of an image is a challenging problem in computer vision. Many researchers attempt to apply local image descriptors to extract features from an image, but choosing the best type of feature to use is still an open problem. Some of these systems are only trained once using a fixed descriptor, like the Scale Invariant Feature Transform (SIFT). In most cases these algorithms show good performance, but they do not learn from their mistakes once training is completed. In this paper a continuous deep neural network feedback system is proposed which consists of an adaptive neural network feature descriptor, the bag of visual words approach and a neural classifier. Two initialization methods for the neural network feature descriptor were compared, one where it was trained on SIFT descriptor output and one where it was randomly initialized. After initial training, the system propagates the classification error from the neural network classifier through the entire pipeline, updating not only the classifier itself, but also the type of features to extract. Results show that for both initialization methods the feedback system increased accuracy substantially when regular training was not able to increase it any further. The proposed neural-SIFT feature descriptor performs better than the SIFT descriptor itself even with a limited number of training instances. Initializing on an existing feature descriptor is beneficial when not a lot of training samples are available. However, when there are a lot of training samples the system is able to construct a well-performing descriptor, solely based on classifier feedback.","","","10.1109/IJCNN.2015.7280660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280660","","Feature extraction;Visualization;Training","feature extraction;image classification;neural nets;object recognition;transforms","neural-SIFT feature descriptor;visual vocabulary object recognition;image semantic content;computer vision;feature extraction;scale invariant feature transform;adaptive neural network feature descriptor;visual words approach;initialization methods;classification error;neural network classifier;training samples;well-performing descriptor;classifier feedback","","1","29","","","","","IEEE","IEEE Conferences"
"Predicting Geo-informative Attributes in Large-Scale Image Collections Using Convolutional Neural Networks","S. Lee; H. Zhang; D. J. Crandall","Sch. of Inf. & Comput., Indiana Univ., Bloomington, IN, USA; Sch. of Inf. & Comput., Indiana Univ., Bloomington, IN, USA; Sch. of Inf. & Comput., Indiana Univ., Bloomington, IN, USA","2015 IEEE Winter Conference on Applications of Computer Vision","","2015","","","550","557","Geographic location is a powerful property for organizing large-scale photo collections, but only a small fraction of online photos are geo-tagged. Most work in automatically estimating geo-tags from image content is based on comparison against models of buildings or landmarks, or on matching to large reference collections of geotagged images. These approaches work well for frequently photographed places like major cities and tourist destinations, but fail for photos taken in sparsely photographed places where few reference photos exist. Here we consider how to recognize general geo-informative attributes of a photo, e.g. the elevation gradient, population density, demographics, etc. of where it was taken, instead of trying to estimate a precise geo-tag. We learn models for these attributes using a large (noisy) set of geo-tagged images from Flickr by training deep convolutional neural networks (CNNs). We evaluate on over a dozen attributes, showing that while automatically recognizing some attributes is very difficult, others can be automatically estimated with about the same accuracy as a human.","","","10.1109/WACV.2015.79","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045933","","Sociology;Statistics;Training;NASA;Economic indicators;Neural networks;Geospatial analysis","convolution;image matching;image retrieval;neural nets;photography;social networking (online)","geo-informative attribute;large-scale image collection;geographic location;large-scale photo collections;image content;geo-tagged image;Flickr;convolutional neural networks;CNN","","15","46","","","","","IEEE","IEEE Conferences"
"Compact Global Descriptors for Visual Search","V. Chandrasekhar; J. Lin; O. Morère; A. Veillard; H. Goh","Inst. for Infocomm Res., Singapore, Singapore; Inst. for Infocomm Res., Singapore, Singapore; Inst. for Infocomm Res., Singapore, Singapore; Univ. Pierre et Marie Curie, Paris, France; Inst. for Infocomm Res., Singapore, Singapore","2015 Data Compression Conference","","2015","","","333","342","The first step in an image retrieval pipeline consists of comparing global descriptors from a large database to find a short list of candidate matching images. The more compact the global descriptor, the faster the descriptors can be compared for matching. State-of-the-art global descriptors based on Fisher Vectors are represented with tens of thousands of floating point numbers. While there is significant work on compression of local descriptors, there is relatively little work on compression of high dimensional Fisher Vectors. We study the problem of global descriptor compression in the context of image retrieval, focusing on extremely compact binary representations: 64-1024 bits. Motivated by the remarkable success of deep neural networks in recent literature, we propose a compression scheme based on deeply stacked Restricted Boltzmann Machines (SRBM), which learn lower dimensional non-linear subspaces on which the data lie. We provide a thorough evaluation of several state-of-the-art compression schemes based on PCA, Locality Sensitive Hashing, Product Quantization and greedy bit selection, and show that the proposed compression scheme outperforms all existing schemes.","","","10.1109/DCC.2015.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7149290","global descriptors;feature compression;compact descriptors for visual search","Image coding;Training;Transform coding;Neural networks;Principal component analysis;Standards;Visualization","Boltzmann machines;data compression;image coding;image matching;image retrieval;principal component analysis;vectors;visual databases","compact global descriptors;visual search;image retrieval;large database;matching images;Fisher vectors;floating point numbers;local descriptors compression;global descriptor compression;compact binary representations;compression scheme;deeply stacked restricted Boltzmann machines;SRBM;nonlinear subspaces;PCA;locality sensitive hashing;product quantization;greedy bit selection;principal component analysis","","8","29","","","","","IEEE","IEEE Conferences"
"On the performance of ConvNet features for place recognition","N. Sünderhauf; S. Shirazi; F. Dayoub; B. Upcroft; M. Milford","ARC Centre of Excellence for Robotic Vision, Queensland University of Technology (QUT), Brisbane 4000, Australia; ARC Centre of Excellence for Robotic Vision, Queensland University of Technology (QUT), Brisbane 4000, Australia; ARC Centre of Excellence for Robotic Vision, Queensland University of Technology (QUT), Brisbane 4000, Australia; ARC Centre of Excellence for Robotic Vision, Queensland University of Technology (QUT), Brisbane 4000, Australia; ARC Centre of Excellence for Robotic Vision, Queensland University of Technology (QUT), Brisbane 4000, Australia","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","2015","","","4297","4304","After the incredible success of deep learning in the computer vision domain, there has been much interest in applying Convolutional Network (ConvNet) features in robotic fields such as visual navigation and SLAM. Unfortunately, there are fundamental differences and challenges involved. Computer vision datasets are very different in character to robotic camera data, real-time performance is essential, and performance priorities can be different. This paper comprehensively evaluates and compares the utility of three state-of-the-art ConvNets on the problems of particular relevance to navigation for robots; viewpoint-invariance and condition-invariance, and for the first time enables real-time place recognition performance using ConvNets with large maps by integrating a variety of existing (locality-sensitive hashing) and novel (semantic search space partitioning) optimization techniques. We present extensive experiments on four real world datasets cultivated to evaluate each of the specific challenges in place recognition. The results demonstrate that speed-ups of two orders of magnitude can be achieved with minimal accuracy degradation, enabling real-time performance. We confirm that networks trained for semantic place categorization also perform better at (specific) place recognition when faced with severe appearance changes and provide a reference for which networks and layers are optimal for different aspects of the place recognition problem.","","","10.1109/IROS.2015.7353986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353986","","Feature extraction;Robustness;Visualization;Real-time systems;Semantics;Computer vision","neural nets;object recognition;optimisation;robot vision;search problems;SLAM (robots)","semantic place categorization;optimization technique;semantic search space partitioning;locality-sensitive hashing;condition-invariance;viewpoint-invariance;SLAM;visual navigation;robotic field;convolutional network;computer vision;place recognition;ConvNet feature","","128","37","","","","","IEEE","IEEE Conferences"
"Enabling open access to LTE network components; the NITOS testbed paradigm","N. Makris; C. Zarafetas; S. Kechagias; T. Korakis; I. Seskar; L. Tassiulas","Informatics & Telematics Institute, CERTH, Greece; Informatics & Telematics Institute, CERTH, Greece; Dept. of ECE, University of Thessaly, Volos, Greece; Informatics & Telematics Institute, CERTH, Greece; WINLAB, Rutgers University, USA; Dept. of ECE, Yale Institute for Network Science, USA","Proceedings of the 2015 1st IEEE Conference on Network Softwarization (NetSoft)","","2015","","","1","6","The lessons already learned from the existing protocols operation are taken into deep consideration during the standardization activities of the potential technologies opted for the future 5th Generation mobile networks. Prior research on wireless technologies in general has clearly shown the need for open programmable experimental facilities which can be used for the implementation and evaluation of novel algorithms and ideas under real world settings, even directly comparable to existing technologies and methodologies. Nevertheless, provisioning of such testbed platforms mandates the respective tools which will enable access to the testbed resources and will expose the maximum possible flexibility in configuring them. In this work, we present our efforts in building such a facility, along with the tools and services that cope with such requirements. The facility upon which we build is the long-established NITOS wireless testbed, which is offering commercial as well as open source LTE components in a 24/7 basis.","","","10.1109/NETSOFT.2015.7116191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7116191","","WiMAX;Base stations;Internet;Flexible printed circuits;Protocols;Routing","5G mobile communication;Long Term Evolution","LTE network components;NITOS testbed paradigm;5th generation mobile networks","","17","18","","","","","IEEE","IEEE Conferences"
"The Impact of Subcultures in Cultural Algorithm Problem Solving","R. G. Reynolds; Y. A. Gawasmeh; A. Salaymeh","Comput. Sci. Dept., Wayne State Univ., Detroit, MI, USA; Comput. Sci. Dept., Wayne State Univ., Detroit, MI, USA; Comput. Sci. Dept., Wayne State Univ., Detroit, MI, USA","2015 IEEE Symposium Series on Computational Intelligence","","2015","","","1876","1884","Cultural Algorithms are computational models of social evolution based upon principle of Cultural Evolution. A Cultural Algorithm consists of a Belief Space consisting of a network of active and passive knowledge sources and a Population Space of agents. The agents are connected via a social fabric over which information used in agent problem solving is passed. The knowledge sources in the Belief Space compete with each other in order to influence the decision making of agents in the Population Space. Likewise, the problem solving experiences of agents in the Population Space are sent back to the Belief Space and used to update the knowledge sources there. It is a dual inheritance system in which both the Population and Belief spaces evolve in parallel. In this paper we compare three different social fabrics (homogeneous, heterogeneous and Sub-Cultures) over a wide range of problem complexities. The performances of these three different evolutionary approaches are compared relative to a variety of benchmark landscapes of varying entropy, from static to chaotic. We show that as the number of independent processes that are involved in the production of a landscape increases, the more advantageous subcultures are in directing the population to a solution. Such landscapes are often characteristic of deep learning problems in which patterns are generated by the interaction of many simple interactions. While sub-cultured approaches can emerge in a given problem, they do not have to. It is shown that for single layer generators for a landscape or image, sub-cultures do not effectively emerge since they are not needed to solve such problems.","","","10.1109/SSCI.2015.261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7376838","","Topology;Cultural differences;Sociology;Statistics;Wheels;Fabrics;Complexity theory","algorithm theory;evolutionary computation;multi-agent systems;problem solving","evolutionary approach;subculture social fabric;heterogeneous social fabric;homogeneous social fabric;dual inheritance system;agent decision making;agent population space;belief space;cultural evolution principle;cultural algorithm problem solving","","3","19","","","","","IEEE","IEEE Conferences"
"Guest Editorial: Deep Learning for Multimedia Computing","G. Qi; H. Larochelle; B. Huet; J. Luo; K. Yu","Department of Computer Science, University of Central Florida, Orlando, FL, USA; Department of Computer Science, Université de Sherbrooke, Sherbrooke, QC, Canada; Multimedia Communications Department, EURECOM, Biot, France; Department of Computer Science, University of Rochester, Rochester, NY, USA; Multimedia Department, Baidu Inc., Beijing, China","IEEE Transactions on Multimedia","","2015","17","11","1873","1874","The twenty papers in this special section aim at providing a forum to present recent advancements in deep learning research that directly concerns the multimedia community. Specifically, deep learning has successfully designed algorithms that can build deep nonlinear representations to mimic how the brain perceives and understands multimodal information, ranging from low-level signals like images and audios, to high-level semantic data like natural language. For multimedia research, it is especially important to develop deep networks to capture the dependencies between different genres of data, building joint deep representation for diverse modalities. ","","","10.1109/TMM.2015.2485538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7302125","","Special issues and sections;Machine learning;Multimedia communication;Neural networks;Natural language processing;Object recognition;Multimedia computing;Data models","","","","1","","","","","","IEEE","IEEE Journals"
"The sexy job in the next ten years will be statisticians","E. Moulines","Institut Télécom / Télécom ParisTech (ENST), Département TSI / CNRS UMR 5141","2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)","","2015","","","XXIX","XXXVI","These invited talks discuss the following: The sexy job in the next ten years will be statisticians; Fine Grained Visual Categorisation; Big Data Analytics vs Business Intelligence; The Rise of Open Big data & Analytics platform; Data Science for Social Good: How the Data Science Bowl Advanced the Field of Deep Learning and Ocean Science.","","","10.1109/DSAA.2015.7344777","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344777","","","Big Data;data analysis","data science;advanced analytics;visual categorisation;Big Data analytics;business intelligence;open Big Data;deep learning;ocean science","","","","","","","","IEEE","IEEE Conferences"
"[Title page i]","","","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","","2015","","","i","i","The following topics are dealt with: fuzzy systems; time series; statistical learning; Bayesian learning; classifier systems; support vector machines; machine learning; predictive modelling; feature selection; advanced learning methods; information retrieval; neural networks; deep learning; supervised learning; recommender systems; and formal modeling.","","","10.1109/ICMLA.2015.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424249","","","feature selection;fuzzy systems;information retrieval;learning (artificial intelligence);neural nets;pattern classification;recommender systems;support vector machines;time series","formal modeling;recommender systems;supervised learning;deep learning;neural networks;information retrieval;advanced learning methods;feature selection;predictive modelling;machine learning;support vector machines;classifier systems;Bayesian learning;statistical learning;time series;fuzzy systems","","","","","","","","IEEE","IEEE Conferences"
"How big data changes statistical machine learning","L. Bottou","Facebook AI Research, New York","2015 IEEE International Conference on Big Data (Big Data)","","2015","","","1","1","Summary form only given. This presentation illustrates how big data forces change on algorithmic techniques and the goals of machine learning, bringing along challenges and opportunities. 1. The theoretical foundations of statistical machine learning traditionally assume that training data is scarce. If one assumes instead that data is abundant and that the bottleneck is the computation time, stochastic algorithms with poor optimization performance become very attractive learning algorithms. These algorithms quickly became the backbone of large-scale machine learning and are the object of very active research. 2. Increasing the training set size cannot improve average errors indefinitely. However this diminishing returns problem vanishes if we measure instead the diversity of conditions in which the trained system performs well. In other words, big data is not an opportunity to increase the average accuracy, but an opportunity to increase coverage. Machine learning research must broaden its statistical framework in order to embrace all the (changing) aspects of real big data problems. Transfer learning, causal inference, and deep learning are successful steps in this direction.","","","10.1109/BigData.2015.7363712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363712","","","Big Data;inference mechanisms;learning (artificial intelligence)","Big Data;statistical machine learning;learning algorithms;causal inference;transfer learning;deep learning","","","","","","","","IEEE","IEEE Conferences"
"IAPR keynote lecture IV: Deep learning","Y. Bengio","University of Montreal, Canada","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","","2015","","","xx","xx","Deep learning has arisen around 2006 as a renewal of neural networks research allowing such models to have more layers. Theoretical investigations have shown that functions obtained as deep compositions of simpler functions (which includes both deep and recurrent nets) can express highly varying functions (with many ups and downs and different input regions that can be distinguished) much more efficiently (with fewer parameters) than otherwise. Empirical work in a variety of applications has demonstrated that, when well trained, such deep architectures can be highly successful, remarkably breaking through previous state-of-the-art in many areas, including speech recognition, object recognition, language models, and transfer learning. This talk will summarize the advances that have made these breakthroughs possible, and end with questions about some major challenges still ahead of researchers in order to continue our climb towards AI-level competence.","","","10.1109/ACPR.2015.7486451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486451","","","","","","","","","","","","IEEE","IEEE Conferences"
"Special Issue on Deep Learning for Multimedia Computing","","","IEEE Transactions on Multimedia","","2015","17","2","260","260","Describes the above-named upcoming special issue or section. May include topics to be covered or calls for papers.","","","10.1109/TMM.2015.2392921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011004","","","","","","","","","","","","IEEE","IEEE Journals"
"Special section on deep learning in medical applications","","","IEEE Transactions on Medical Imaging","","2015","34","8","1769","1769","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","","","10.1109/TMI.2015.2460431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7172577","","","","","","","","","","","","IEEE","IEEE Journals"
"Special section on deep learning in medical applications","","","IEEE Transactions on Medical Imaging","","2015","34","9","1990","1990","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","","","10.1109/TMI.2015.2474097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7229396","","","","","","","","","","","","IEEE","IEEE Journals"
"Special Issue on Deep Learning for Multimedia Computing","","","IEEE Transactions on Multimedia","","2015","17","1","143","143","Describes the above-named upcoming special issue or section. May include topics to be covered or calls for papers.","","","10.1109/TMM.2014.2384071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996063","","","","","","","","","","","","IEEE","IEEE Journals"
"Special Issue on Deep Learning for Multimedia Computing","","","IEEE Transactions on Multimedia","","2015","17","3","452","452","Describes the above-named upcoming special issue or section. May include topics to be covered or calls for papers.","","","10.1109/TMM.2015.2401853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7041298","","","","","","","","","","","","IEEE","IEEE Journals"
"Programme","","","2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)","","2015","06","","1","2","The following topics are dealt with: action unit occurence detection; facial expression recognition; deep learning; FACS action unit occurence; intensity estimation; facial action unit detection; discriminant multi-label manifold embedding; action unit intensity estimation; facial action unit intensity prediction; multitask metric learning; Kernel regression; cross dataset learning; person specific normalisation; automatic action unit detection and multi-kernel support vector machine.","","","10.1109/FG.2015.7284867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284867","","","data handling;face recognition;learning (artificial intelligence);regression analysis;support vector machines","action unit occurence detection;facial expression recognition;deep learning;FACS action unit occurence;intensity estimation;facial action unit detection;discriminant multi-label manifold embedding;action unit intensity estimation;facial action unit intensity prediction;multitask metric learning;Kernel regression;cross dataset learning;person specific normalisation;automatic action unit detection;multi-kernel support vector machine","","","","","","","","IEEE","IEEE Conferences"
"[Title page i]","","","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","","2015","","","i","i","The following topics are dealt with: software repository mining; interaction data; app mining; code review; ecosystems; API; architecture; bugs; computer musicians; bullies; gists; licenses; deep learning; and process mining.","","","10.1109/MSR.2015.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180054","","","application program interfaces;data mining;law;learning (artificial intelligence);program debugging;software engineering;source code (software)","software repository mining;interaction data;app mining;code review;ecosystems;API;architecture;bugs;computer musicians;bullies;gists;licenses;deep learning;process mining","","","","","","","","IEEE","IEEE Conferences"
"[Front cover]","","","2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","","2015","","","c1","c1","The following topics are dealt with: Computer Vision; Localization / Positioning; Motion Planning and Control; Kinematics and Control; SLAM; Navigation Technologies for Autonomous Underwater Vehicles; Intelligent Measurement, Diagnosis and Control; Intelligent hands and Eyes for Medical Robotics; Recognition; Aerial Robotics; Mechanism Design; Intelligent Mobile or Field Robot Technology; Deep Learning for Robotics and Computer Vision; Robot Navigation; Micro-/Nano-technology for microrobots and biomedical applications; Coexistence Space: Scene Understanding and Interaction; Humanoid Robots and Teleoperation; Robot Vision and Applications; HRI.","","","10.1109/URAI.2015.7358806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358806","","","autonomous underwater vehicles;computer vision;learning (artificial intelligence);path planning;robots","microrobots;biomedical applications;coexistence space;scene understanding;humanoid robots;teleoperation;robot vision;HRI;nanotechnology;microtechnology;robot navigation;deep learning;field robot technology;intelligent mobile robot technology;mechanism design;aerial robotics;recognition;medical robotics;intelligent eyes;intelligent hands;diagnosis;intelligent measurement;autonomous underwater vehicles;navigation technologies;SLAM;kinematics;control;motion planning;positioning;localization;computer vision;author index","","","","","","","","IEEE","IEEE Conferences"
"[Title page i]","","","2015 IEEE Conference on Collaboration and Internet Computing (CIC)","","2015","","","i","i","The following topics are dealt with: Internet computing; cloud computing; data analytics; access control; secure sharing; social media; Internet; data privacy; risk assessment; trusted computing; urban informatics; collaborative systems; services and content networks; adaptive systems and virtualization; deep learning; and query processing.","","","10.1109/CIC.2015.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7423047","","","authorisation;cloud computing;data analysis;data privacy;groupware;learning (artificial intelligence);query processing;social networking (online);trusted computing;virtualisation","Internet computing;query processing;deep learning;virtualization;adaptive systems;service networks;content networks;collaborative systems;urban informatics;trusted computing;risk assessment;data privacy;Internet;social media;secure sharing;access control;data analytics;cloud computing","","","","","","","","IEEE","IEEE Conferences"
"[Copyright notice]","","","2015 IEEE 10th Jubilee International Symposium on Applied Computational Intelligence and Informatics","","2015","","","1","1","The following topics are dealt with: Big Data; deep learning; space robotics; computer-aided patient evaluation; human-robot interaction; cryptography; inverted pendulum control; bidirectional flyback inverter; Web-based real-time collaboration; mobile robots; wireless sensor network; content-based image retrieval; PID controller; robot arms; PD control; digital signal processing; fuzzy automata; adaptive control; multiview computer vision; university education; cloud computing; mobile network; business process similarity; social network; distribution network; PageRank-based recommender system; intrusion detection; surgical robots; and robot car model.","","","10.1109/SACI.2015.7208269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208269","","","adaptive control;aerospace robotics;automata theory;Big Data;business data processing;cloud computing;computer vision;content-based retrieval;cryptography;distribution networks;engineering education;human-robot interaction;image retrieval;invertors;learning (artificial intelligence);manipulators;medical computing;medical robotics;mobile radio;mobile robots;nonlinear control systems;PD control;recommender systems;security of data;signal processing;social networking (online);three-term control;wireless sensor networks","distribution network;PageRank-based recommender system;intrusion detection;surgical robots;robot car model;social network;business process similarity;mobile network;cloud computing;university education;multiview computer vision;adaptive control;fuzzy automata;digital signal processing;PD control;robot arms;PID controller;content-based image retrieval;wireless sensor network;mobile robots;Web-based real-time collaboration;bidirectional flyback inverter;inverted pendulum control;cryptography;human-robot interaction;computer-aided patient evaluation;space robotics;deep learning;Big Data","","","","","","","","IEEE","IEEE Conferences"
"[Front cover]","","","2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES)","","2015","","","c1","c1","The following topics are dealt with: learning; Web; urban water-supply system; IP Core; DCI approach; real-time sensor network; linked open data source; process mining; image coding; deep neural network architecture; human computer interaction; social human-robot interaction; VANET; authorized V2V communication; MIMO system; surgical robotics; ontologies; genetic algorithm; image reconstruction; mobile robot; artificial neural network; fuzzy reasoning; heat exchanger; fuzzy controller design; mobile device; human machine interface design; decision support system; data mining technique; discrete-time SISO system; augmented reality; visual analysis; content management system; Androids; nonlinear MPC; collaborative filtering; recommendation; wireless sensor networks;; humidity control; temperature control and stability.","","","10.1109/INES.2015.7329762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7329762","","","augmented reality;collaborative filtering;content management;data mining;decision support systems;discrete time systems;fuzzy control;fuzzy reasoning;genetic algorithms;heat exchangers;human computer interaction;humidity control;image coding;image reconstruction;Internet;learning (artificial intelligence);medical robotics;MIMO systems;mobile robots;neural nets;nonlinear control systems;predictive control;public domain software;recommender systems;stability;surgery;temperature control;vehicular ad hoc networks;water supply;wireless sensor networks","mobile device;human machine interface design;decision support system;data mining technique;discrete-time SISO system;augmented reality;visual analysis;content management system;Androids;nonlinear MPC;collaborative filtering;recommendation;wireless sensor networks;humidity control;temperature control;stability;fuzzy controller design;heat exchanger;fuzzy reasoning;artificial neural network;mobile robot;image reconstruction;genetic algorithm;ontologies;surgical robotics;MIMO system;authorized V2V communication;VANET;social human-robot interaction;human computer interaction;deep neural network architecture;image coding;process mining;linked open data source;real-time sensor network;DCI approach;IP Core;urban water-supply system;Web;learning","","","","","","","","IEEE","IEEE Conferences"
"Keynote speech III: Computer go research — The challenges ahead","M. Müller","Department of Computing Science at the University of Alberta in Edmonton, Canada","2015 IEEE Conference on Computational Intelligence and Games (CIG)","","2015","","","18","18","Summary form only given. With the success of Monte Carlo Tree Search, the game of Go has become a focus of games research. Recently, deep convolutional neural networks have achieved human-level performance in predicting master moves. Even before that, machine learning techniques have been used very successfully as an automated way to improve the domain knowledge in Go programs. Go programs have now reached a level close to top amateur players. In order to challenge professional level players, we must combine the three pillars of modern Go programs - search, knowledge, and simulation - in a high performance system, possibly running on massively parallel hardware. This talk will summarize recent progress in this exciting field, and outline a research strategy for boosting the performance of Go programs to the next level.","","","10.1109/CIG.2015.7317659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7317659","","","computer games;learning (artificial intelligence);Monte Carlo methods;neural nets;tree searching","computer go research;monte carlo tree search;game of Go;games research;deep convolutional neural network;human-level performance;machine learning technique;Go program;amateur player;professional level player;high performance system;parallel hardware","","","","","","","","IEEE","IEEE Conferences"
"Silicon photonics coprocessors for energy efficient computing","B. Jalali; A. Mahjoubfar; D. Solli; M. Asghari; Y. Jiang; C. L. Chen","Electrical Engineering Department, UCLA, Los Angeles, CA; Electrical Engineering Department, UCLA, Los Angeles, CA; Electrical Engineering Department, UCLA, Los Angeles, CA; Electrical Engineering Department, UCLA, Los Angeles, CA; Electrical Engineering Department, UCLA, Los Angeles, CA; Electrical Engineering Department, UCLA, Los Angeles, CA","2015 2nd International Conference on Opto-Electronics and Applied Optics (IEM OPTRONIX)","","2015","","","1","2","We introduce three types of analog optical accelerators for enhancing the performance of electronic computing. These include (1) physical computing using complex optical dynamics in silicon for acceleration of scientific computing, (2) spectrotemporal processing and sparse coding using dispersion basis functions and (3) photonic computing primitives for performing nonlinear mathematical operations.","","","10.1109/OPTRONIX.2015.7345515","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345515","natural computing;analog computing;physical computing;complex dynamic systems;intelligent dynamic systems;analog optical co-processor;silicon photonics;photonic hardware accelerator;optical learning machines;photonic deep learning","","elemental semiconductors;nonlinear optics;optical computing;optical dispersion;silicon","silicon photonics coprocessors;energy efficient computing;analog optical accelerators;electronic computing;physical computing;complex optical dynamics;scientific computing;spectrotemporal processing;sparse coding;dispersion basis functions;photonic computing primitives;nonlinear mathematical operations","","","12","","","","","IEEE","IEEE Conferences"
"Session 4 overview: Processors: High-performance digital subcommittee","A. Inoue; J. L. Shin","Fujitsu Labs., Kanagawa, Japan; Oracle Corporation, Redwood Shores, CA","2015 IEEE International Solid-State Circuits Conference - (ISSCC) Digest of Technical Papers","","2015","","","68","69","As compute power is increasingly migrating to large data centers and the cloud, microprocessors face progressively more stringent design constraints. This year's processor session introduces three new ""big iron"" processors in 22nm/20nm technology providing higher performance and power efficiency. Increasing core counts and cache sizes, with adaptive power management techniques, are applied to optimize performance/W. A high-density microserver computing node is optimized for Big Data and shows its capability in handling a large number of threads. For future intelligent computing, such as deep learning and inference, a specialized processor is demonstrated. Other papers in this session describe adaptive techniques, as well as design optimizations to improve throughput and energy efficiency.","","","10.1109/ISSCC.2015.7062929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7062929","","","","","","","","","","","","IEEE","IEEE Conferences"
